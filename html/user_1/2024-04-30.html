<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-30</h1>
<h3>Title: Neural Modes: Self-supervised Learning of Nonlinear Modal Subspaces</h3>
<ul>
<li><strong>Authors: </strong>Jiahong Wang, Yinwei Du, Stelian Coros, Bernhard Thomaszewski</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17620">https://arxiv.org/abs/2404.17620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17620">https://arxiv.org/pdf/2404.17620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17620]] Neural Modes: Self-supervised Learning of Nonlinear Modal Subspaces(https://arxiv.org/abs/2404.17620)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We propose a self-supervised approach for learning physics-based subspaces for real-time simulation. Existing learning-based methods construct subspaces by approximating pre-defined simulation data in a purely geometric way. However, this approach tends to produce high-energy configurations, leads to entangled latent space dimensions, and generalizes poorly beyond the training set. To overcome these limitations, we propose a self-supervised approach that directly minimizes the system's mechanical energy during training. We show that our method leads to learned subspaces that reflect physical equilibrium constraints, resolve overfitting issues of previous methods, and offer interpretable latent space parameters.</li>
</ul>

<h3>Title: Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of  the Land</h3>
<ul>
<li><strong>Authors: </strong>Simone Scardapane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17625">https://arxiv.org/abs/2404.17625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17625">https://arxiv.org/pdf/2404.17625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17625]] Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of  the Land(https://arxiv.org/abs/2404.17625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This book is a self-contained introduction to the design of modern (deep) neural networks. Because the term "neural" comes with a lot of historical baggage, I prefer the simpler term "differentiable models" in the text. The focus of this 250-pages volume is on building efficient blocks for processing $n$D data, including convolutions, transformers, graph layers, and modern recurrent models (including linearized transformers and structured state-space models). Because the field is evolving quickly, I have tried to strike a good balance between theory and code, historical considerations and recent trends. I assume the reader has some exposure to machine learning and linear algebra, but I try to cover the preliminaries when necessary. The volume is a refined draft from a set of lecture notes for a course called Neural Networks for Data Science Applications that I teach in Sapienza. I do not cover many advanced topics (generative modeling, explainability, prompting, agents), which will be published over time in the companion website.</li>
</ul>

<h3>Title: Generative Dataset Distillation: Balancing Global Structure and Local  Details</h3>
<ul>
<li><strong>Authors: </strong>Longzhen Li, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17732">https://arxiv.org/abs/2404.17732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17732">https://arxiv.org/pdf/2404.17732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17732]] Generative Dataset Distillation: Balancing Global Structure and Local  Details(https://arxiv.org/abs/2404.17732)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new dataset distillation method that considers balancing global structure and local details when distilling the information from a large dataset into a generative model. Dataset distillation has been proposed to reduce the size of the required dataset when training models. The conventional dataset distillation methods face the problem of long redeployment time and poor cross-architecture performance. Moreover, previous methods focused too much on the high-level semantic attributes between the synthetic dataset and the original dataset while ignoring the local features such as texture and shape. Based on the above understanding, we propose a new method for distilling the original image dataset into a generative model. Our method involves using a conditional generative adversarial network to generate the distilled dataset. Subsequently, we ensure balancing global structure and local details in the distillation process, continuously optimizing the generator for more information-dense dataset generation.</li>
</ul>

<h3>Title: Causal Diffusion Autoencoders: Toward Counterfactual Generation via  Diffusion Probabilistic Models</h3>
<ul>
<li><strong>Authors: </strong>Aneesh Komanduri, Chen Zhao, Feng Chen, Xintao Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17735">https://arxiv.org/abs/2404.17735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17735">https://arxiv.org/pdf/2404.17735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17735]] Causal Diffusion Autoencoders: Toward Counterfactual Generation via  Diffusion Probabilistic Models(https://arxiv.org/abs/2404.17735)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images.</li>
</ul>

<h3>Title: High-quality Surface Reconstruction using Gaussian Surfels</h3>
<ul>
<li><strong>Authors: </strong>Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17774">https://arxiv.org/abs/2404.17774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17774">https://arxiv.org/pdf/2404.17774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17774]] High-quality Surface Reconstruction using Gaussian Surfels(https://arxiv.org/abs/2404.17774)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods.</li>
</ul>

<h3>Title: Temporal Scaling Law for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yizhe Xiong, Xiansheng Chen, Xin Ye, Hui Chen, Zijia Lin, Haoran Lian, Jianwei Niu, Guiguang Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17785">https://arxiv.org/abs/2404.17785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17785">https://arxiv.org/pdf/2404.17785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17785]] Temporal Scaling Law for Large Language Models(https://arxiv.org/abs/2404.17785)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) are widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed as Scaling Laws, have discovered that the loss of LLMs scales as power laws with model size, computational budget, and dataset size. However, the performance of LLMs throughout the training process remains untouched. In this paper, we propose the novel concept of Temporal Scaling Law and study the loss of LLMs from the temporal dimension. We first investigate the imbalance of loss on each token positions and develop a reciprocal-law across model scales and training stages. We then derive the temporal scaling law by studying the temporal patterns of the reciprocal-law parameters. Results on both in-distribution (IID) data and out-of-distribution (OOD) data demonstrate that our temporal scaling law accurately predicts the performance of LLMs in future training stages. Moreover, the temporal scaling law reveals that LLMs learn uniformly on different token positions, despite the loss imbalance. Experiments on pre-training LLMs in various scales show that this phenomenon verifies the default training paradigm for generative language models, in which no re-weighting strategies are attached during training. Overall, the temporal scaling law provides deeper insight into LLM pre-training.</li>
</ul>

<h3>Title: Dynamical Mode Recognition of Coupled Flame Oscillators by Supervised  and Unsupervised Learning Approaches</h3>
<ul>
<li><strong>Authors: </strong>Weiming Xu, Tao Yang, Peng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17801">https://arxiv.org/abs/2404.17801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17801">https://arxiv.org/pdf/2404.17801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17801]] Dynamical Mode Recognition of Coupled Flame Oscillators by Supervised  and Unsupervised Learning Approaches(https://arxiv.org/abs/2404.17801)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Combustion instability in gas turbines and rocket engines, as one of the most challenging problems in combustion research, arises from the complex interactions among flames, which are also influenced by chemical reactions, heat and mass transfer, and acoustics. Identifying and understanding combustion instability is essential to ensure the safe and reliable operation of many combustion systems, where exploring and classifying the dynamical behaviors of complex flame systems is a core take. To facilitate fundamental studies, the present work concerns dynamical mode recognition of coupled flame oscillators made of flickering buoyant diffusion flames, which have gained increasing attention in recent years but are not sufficiently understood. The time series data of flame oscillators are generated by fully validated reacting flow simulations. Due to limitations of expertise-based models, a data-driven approach is adopted. In this study, a nonlinear dimensional reduction model of variational autoencoder (VAE) is used to project the simulation data onto a 2-dimensional latent space. Based on the phase trajectories in latent space, both supervised and unsupervised classifiers are proposed for datasets with well known labeling and without, respectively. For labeled datasets, we establish the Wasserstein-distance-based classifier (WDC) for mode recognition; for unlabeled datasets, we develop a novel unsupervised classifier (GMM-DTWC) combining dynamic time warping (DTW) and Gaussian mixture model (GMM). Through comparing with conventional approaches for dimensionality reduction and classification, the proposed supervised and unsupervised VAE-based approaches exhibit a prominent performance for distinguishing dynamical modes, implying their potential extension to dynamical mode recognition of complex combustion problems.</li>
</ul>

<h3>Title: Meta In-Context Learning Makes Large Language Models Better Zero and  Few-Shot Relation Extractors</h3>
<ul>
<li><strong>Authors: </strong>Guozheng Li, Peng Wang, Jiajun Liu, Yikai Guo, Ke Ji, Ziyu Shang, Zijie Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17807">https://arxiv.org/abs/2404.17807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17807">https://arxiv.org/pdf/2404.17807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17807]] Meta In-Context Learning Makes Large Language Models Better Zero and  Few-Shot Relation Extractors(https://arxiv.org/abs/2404.17807)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Relation extraction (RE) is an important task that aims to identify the relationships between entities in texts. While large language models (LLMs) have revealed remarkable in-context learning (ICL) capability for general zero and few-shot learning, recent studies indicate that current LLMs still struggle with zero and few-shot RE. Previous studies are mainly dedicated to design prompt formats and select good examples for improving ICL-based RE. Although both factors are vital for ICL, if one can fundamentally boost the ICL capability of LLMs in RE, the zero and few-shot RE performance via ICL would be significantly improved. To this end, we introduce \textsc{Micre} (\textbf{M}eta \textbf{I}n-\textbf{C}ontext learning of LLMs for \textbf{R}elation \textbf{E}xtraction), a new meta-training framework for zero and few-shot RE where an LLM is tuned to do ICL on a diverse collection of RE datasets (i.e., learning to learn in context for RE). Through meta-training, the model becomes more effectively to learn a new RE task in context by conditioning on a few training examples with no parameter updates or task-specific templates at inference time, enabling better zero and few-shot task generalization. We experiment \textsc{Micre} on various LLMs with different model scales and 12 public RE datasets, and then evaluate it on unseen RE benchmarks under zero and few-shot settings. \textsc{Micre} delivers comparable or superior performance compared to a range of baselines including supervised fine-tuning and typical in-context learning methods. We find that the gains are particular significant for larger model scales, and using a diverse set of the meta-training RE datasets is key to improvements. Empirically, we show that \textsc{Micre} can transfer the relation semantic knowledge via relation label name during inference on target RE datasets.</li>
</ul>

<h3>Title: Recall, Retrieve and Reason: Towards Better In-Context Relation  Extraction</h3>
<ul>
<li><strong>Authors: </strong>Guozheng Li, Peng Wang, Wenjun Ke, Yikai Guo, Ke Ji, Ziyu Shang, Jiajun Liu, Zijie Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17809">https://arxiv.org/abs/2404.17809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17809">https://arxiv.org/pdf/2404.17809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17809]] Recall, Retrieve and Reason: Towards Better In-Context Relation  Extraction(https://arxiv.org/abs/2404.17809)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Relation extraction (RE) aims to identify relations between entities mentioned in texts. Although large language models (LLMs) have demonstrated impressive in-context learning (ICL) abilities in various tasks, they still suffer from poor performances compared to most supervised fine-tuned RE methods. Utilizing ICL for RE with LLMs encounters two challenges: (1) retrieving good demonstrations from training examples, and (2) enabling LLMs exhibit strong ICL abilities in RE. On the one hand, retrieving good demonstrations is a non-trivial process in RE, which easily results in low relevance regarding entities and relations. On the other hand, ICL with an LLM achieves poor performance in RE while RE is different from language modeling in nature or the LLM is not large enough. In this work, we propose a novel recall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora (training examples) to enable relevant retrieving and reliable in-context reasoning. Specifically, we distill the consistently ontological knowledge from training datasets to let LLMs generate relevant entity pairs grounded by retrieval corpora as valid queries. These entity pairs are then used to retrieve relevant training examples from the retrieval corpora as demonstrations for LLMs to conduct better ICL via instruction tuning. Extensive experiments on different LLMs and RE datasets demonstrate that our method generates relevant and valid entity pairs and boosts ICL abilities of LLMs, achieving competitive or new state-of-the-art performance on sentence-level RE compared to previous supervised fine-tuning methods and ICL-based methods.</li>
</ul>

<h3>Title: ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired  Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Zhongze Wang, Haitao Zhao, Jingchao Peng, Lujian Yao, Kaijie Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17825">https://arxiv.org/abs/2404.17825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17825">https://arxiv.org/pdf/2404.17825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17825]] ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired  Image Dehazing(https://arxiv.org/abs/2404.17825)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Unpaired image dehazing (UID) holds significant research importance due to the challenges in acquiring haze/clear image pairs with identical backgrounds. This paper proposes a novel method for UID named Orthogonal Decoupling Contrastive Regularization (ODCR). Our method is grounded in the assumption that an image consists of both haze-related features, which influence the degree of haze, and haze-unrelated features, such as texture and semantic information. ODCR aims to ensure that the haze-related features of the dehazing result closely resemble those of the clear image, while the haze-unrelated features align with the input hazy image. To accomplish the motivation, Orthogonal MLPs optimized geometrically on the Stiefel manifold are proposed, which can project image features into an orthogonal space, thereby reducing the relevance between different features. Furthermore, a task-driven Depth-wise Feature Classifier (DWFC) is proposed, which assigns weights to the orthogonal features based on the contribution of each channel's feature in predicting whether the feature source is hazy or clear in a self-supervised fashion. Finally, a Weighted PatchNCE (WPNCE) loss is introduced to achieve the pulling of haze-related features in the output image toward those of clear images, while bringing haze-unrelated features close to those of the hazy input. Extensive experiments demonstrate the superior performance of our ODCR method on UID.</li>
</ul>

<h3>Title: Evaluation of Few-Shot Learning for Classification Tasks in the Polish  Language</h3>
<ul>
<li><strong>Authors: </strong>Tsimur Hadeliya, Dariusz Kajtoch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17832">https://arxiv.org/abs/2404.17832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17832">https://arxiv.org/pdf/2404.17832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17832]] Evaluation of Few-Shot Learning for Classification Tasks in the Polish  Language(https://arxiv.org/abs/2404.17832)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We introduce a few-shot benchmark consisting of 7 different classification tasks native to the Polish language. We conducted an empirical comparison with 0 and 16 shots between fine-tuning, linear probing, SetFit, and in-context learning (ICL) using various pre-trained commercial and open-source models. Our findings reveal that ICL achieves the best performance, with commercial models like GPT-3.5 and GPT-4 attaining the best performance. However, there remains a significant 14 percentage points gap between our best few-shot learning score and the performance of HerBERT-large fine-tuned on the entire training dataset. Among the techniques, SetFit emerges as the second-best approach, closely followed by linear probing. We observed the worst and most unstable performance with non-linear head fine-tuning. Results for ICL indicate that continual pre-training of models like Mistral-7b or Llama-2-13b on Polish corpora is beneficial. This is confirmed by the improved performances of Bielik-7b and Trurl-13b, respectively. To further support experiments in few-shot learning for Polish, we are releasing handcrafted templates for the ICL.</li>
</ul>

<h3>Title: VANER: Leveraging Large Language Model for Versatile and Adaptive  Biomedical Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Junyi Biana, Weiqi Zhai, Xiaodi Huang, Jiaxuan Zheng, Shanfeng Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17835">https://arxiv.org/abs/2404.17835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17835">https://arxiv.org/pdf/2404.17835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17835]] VANER: Leveraging Large Language Model for Versatile and Adaptive  Biomedical Named Entity Recognition(https://arxiv.org/abs/2404.17835)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prevalent solution for BioNER involves using representation learning techniques coupled with sequence labeling. However, such methods are inherently task-specific, demonstrate poor generalizability, and often require dedicated model for each dataset. To leverage the versatile capabilities of recently remarkable large language models (LLMs), several endeavors have explored generative approaches to entity extraction. Yet, these approaches often fall short of the effectiveness of previouly sequence labeling approaches. In this paper, we utilize the open-sourced LLM LLaMA2 as the backbone model, and design specific instructions to distinguish between different types of entities and datasets. By combining the LLM's understanding of instructions with sequence labeling techniques, we use mix of datasets to train a model capable of extracting various types of entities. Given that the backbone LLMs lacks specialized medical knowledge, we also integrate external entity knowledge bases and employ instruction tuning to compel the model to densely recognize carefully curated entities. Our model VANER, trained with a small partition of parameters, significantly outperforms previous LLMs-based models and, for the first time, as a model based on LLM, surpasses the majority of conventional state-of-the-art BioNER systems, achieving the highest F1 scores across three datasets.</li>
</ul>

<h3>Title: Revisiting Multimodal Emotion Recognition in Conversation from the  Perspective of Graph Spectrum</h3>
<ul>
<li><strong>Authors: </strong>Tao Meng, Fuchen Zhang, Yuntao Shou, Wei Ai, Nan Yin, Keqin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17862">https://arxiv.org/abs/2404.17862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17862">https://arxiv.org/pdf/2404.17862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17862]] Revisiting Multimodal Emotion Recognition in Conversation from the  Perspective of Graph Spectrum(https://arxiv.org/abs/2404.17862)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Efficiently capturing consistent and complementary semantic features in a multimodal conversation context is crucial for Multimodal Emotion Recognition in Conversation (MERC). Existing methods mainly use graph structures to model dialogue context semantic dependencies and employ Graph Neural Networks (GNN) to capture multimodal semantic features for emotion recognition. However, these methods are limited by some inherent characteristics of GNN, such as over-smoothing and low-pass filtering, resulting in the inability to learn long-distance consistency information and complementary information efficiently. Since consistency and complementarity information correspond to low-frequency and high-frequency information, respectively, this paper revisits the problem of multimodal emotion recognition in conversation from the perspective of the graph spectrum. Specifically, we propose a Graph-Spectrum-based Multimodal Consistency and Complementary collaborative learning framework GS-MCC. First, GS-MCC uses a sliding window to construct a multimodal interaction graph to model conversational relationships and uses efficient Fourier graph operators to extract long-distance high-frequency and low-frequency information, respectively. Then, GS-MCC uses contrastive learning to construct self-supervised signals that reflect complementarity and consistent semantic collaboration with high and low-frequency signals, thereby improving the ability of high and low-frequency information to reflect real emotions. Finally, GS-MCC inputs the collaborative high and low-frequency information into the MLP network and softmax function for emotion prediction. Extensive experiments have proven the superiority of the GS-MCC architecture proposed in this paper on two benchmark data sets.</li>
</ul>

<h3>Title: Noisy Node Classification by Bi-level Optimization based Multi-teacher  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yujing Liu, Zongqian Wu, Zhengyu Lu, Ci Nie, Guoqiu Wen, Ping Hu, Xiaofeng Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17875">https://arxiv.org/abs/2404.17875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17875">https://arxiv.org/pdf/2404.17875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17875]] Noisy Node Classification by Bi-level Optimization based Multi-teacher  Distillation(https://arxiv.org/abs/2404.17875)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Previous graph neural networks (GNNs) usually assume that the graph data is with clean labels for representation learning, but it is not true in real applications. In this paper, we propose a new multi-teacher distillation method based on bi-level optimization (namely BO-NNC), to conduct noisy node classification on the graph data. Specifically, we first employ multiple self-supervised learning methods to train diverse teacher models, and then aggregate their predictions through a teacher weight matrix. Furthermore, we design a new bi-level optimization strategy to dynamically adjust the teacher weight matrix based on the training progress of the student model. Finally, we design a label improvement module to improve the label quality. Extensive experimental results on real datasets show that our method achieves the best results compared to state-of-the-art methods.</li>
</ul>

<h3>Title: Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling</h3>
<ul>
<li><strong>Authors: </strong>Di Wu, Shicai Fan, Xue Zhou, Li Yu, Yuzhong Deng, Jianxiao Zou, Baihong Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17900">https://arxiv.org/abs/2404.17900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17900">https://arxiv.org/pdf/2404.17900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17900]] Unsupervised Anomaly Detection via Masked Diffusion Posterior Sampling(https://arxiv.org/abs/2404.17900)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, anomaly</a></li>
<li><strong>Abstract: </strong>Reconstruction-based methods have been commonly used for unsupervised anomaly detection, in which a normal image is reconstructed and compared with the given test image to detect and locate anomalies. Recently, diffusion models have shown promising applications for anomaly detection due to their powerful generative ability. However, these models lack strict mathematical support for normal image reconstruction and unexpectedly suffer from low reconstruction quality. To address these issues, this paper proposes a novel and highly-interpretable method named Masked Diffusion Posterior Sampling (MDPS). In MDPS, the problem of normal image reconstruction is mathematically modeled as multiple diffusion posterior sampling for normal images based on the devised masked noisy observation model and the diffusion-based normal image prior under Bayesian framework. Using a metric designed from pixel-level and perceptual-level perspectives, MDPS can effectively compute the difference map between each normal posterior sample and the given test image. Anomaly scores are obtained by averaging all difference maps for multiple posterior samples. Exhaustive experiments on MVTec and BTAD datasets demonstrate that MDPS can achieve state-of-the-art performance in normal image reconstruction quality as well as anomaly detection and localization.</li>
</ul>

<h3>Title: SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Manav Nitin Kapadnis, Sohan Patnaik, Abhilash Nandy, Sourjyadip Ray, Pawan Goyal, Debdoot Sheet</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17912">https://arxiv.org/abs/2404.17912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17912">https://arxiv.org/pdf/2404.17912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17912]] SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision  Language Models(https://arxiv.org/abs/2404.17912)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large Language Models (MLLMs) can automate the creation of accurate and coherent radiological reports. Existing methods often hallucinate details in text-based reports that don't accurately reflect the image content. To mitigate this, we introduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework. We employ a unique self-supervised loss that leverages similarity between pooled image representations and the contextual representations of the generated radiological text, alongside the standard Causal Language Modeling objective, to refine image-text representations. This allows the model to scrutinize and align the generated text through dynamic interaction between a given image and the generated text, therefore reducing hallucination and continuously enhancing nuanced report generation. SERPENT-VLM outperforms existing baselines such as LLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and Radiology Objects in COntext (ROCO) datasets, and also proves to be robust against noisy images. A qualitative case study emphasizes the significant advancements towards more sophisticated MLLM frameworks for R2Gen, opening paths for further research into self-supervised refinement in the medical imaging domain.</li>
</ul>

<h3>Title: Accurate and fast anomaly detection in industrial processes and IoT  environments</h3>
<ul>
<li><strong>Authors: </strong>Simone Tonini (1), Andrea Vandin (1), Francesca Chiaromonte (1 and 2), Daniele Licari (3), Fernando Barsacchi (4) ((1) L'EMbeDS and Institute of Economics, Sant'Anna School of Advanced Studies, Pisa, (2) Dept. of Statistics, The Pennsylvania State University, (3) L'EMbeDS, Sant'Anna School of Advanced Studies, (4) A. Celli Group, Lucca)</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17925">https://arxiv.org/abs/2404.17925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17925">https://arxiv.org/pdf/2404.17925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17925]] Accurate and fast anomaly detection in industrial processes and IoT  environments(https://arxiv.org/abs/2404.17925)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We present a novel, simple and widely applicable semi-supervised procedure for anomaly detection in industrial and IoT environments, SAnD (Simple Anomaly Detection). SAnD comprises 5 steps, each leveraging well-known statistical tools, namely; smoothing filters, variance inflation factors, the Mahalanobis distance, threshold selection algorithms and feature importance techniques. To our knowledge, SAnD is the first procedure that integrates these tools to identify anomalies and help decipher their putative causes. We show how each step contributes to tackling technical challenges that practitioners face when detecting anomalies in industrial contexts, where signals can be highly multicollinear, have unknown distributions, and intertwine short-lived noise with the long(er)-lived actual anomalies. The development of SAnD was motivated by a concrete case study from our industrial partner, which we use here to show its effectiveness. We also evaluate the performance of SAnD by comparing it with a selection of semi-supervised methods on public datasets from the literature on anomaly detection. We conclude that SAnD is effective, broadly applicable, and outperforms existing approaches in both anomaly detection and runtime.</li>
</ul>

<h3>Title: Spatio-Temporal Side Tuning Pre-trained Foundation Models for  Video-based Pedestrian Attribute Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xiao Wang, Qian Zhu, Jiandong Jin, Jun Zhu, Futian Wang, Bo Jiang, Yaowei Wang, Yonghong Tian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17929">https://arxiv.org/abs/2404.17929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17929">https://arxiv.org/pdf/2404.17929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17929]] Spatio-Temporal Side Tuning Pre-trained Foundation Models for  Video-based Pedestrian Attribute Recognition(https://arxiv.org/abs/2404.17929)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Existing pedestrian attribute recognition (PAR) algorithms are mainly developed based on a static image, however, the performance is unreliable in challenging scenarios, such as heavy occlusion, motion blur, etc. In this work, we propose to understand human attributes using video frames that can fully use temporal information by fine-tuning a pre-trained multi-modal foundation model efficiently. Specifically, we formulate the video-based PAR as a vision-language fusion problem and adopt a pre-trained foundation model CLIP to extract the visual features. More importantly, we propose a novel spatiotemporal side-tuning strategy to achieve parameter-efficient optimization of the pre-trained vision foundation model. To better utilize the semantic information, we take the full attribute list that needs to be recognized as another input and transform the attribute words/phrases into the corresponding sentence via split, expand, and prompt operations. Then, the text encoder of CLIP is utilized for embedding processed attribute descriptions. The averaged visual tokens and text tokens are concatenated and fed into a fusion Transformer for multi-modal interactive learning. The enhanced tokens will be fed into a classification head for pedestrian attribute prediction. Extensive experiments on two large-scale video-based PAR datasets fully validated the effectiveness of our proposed framework. The source code of this paper is available at https://github.com/Event-AHU/OpenPAR.</li>
</ul>

<h3>Title: Critical Review for One-class Classification: recent advances and the  reality behind them</h3>
<ul>
<li><strong>Authors: </strong>Toshitaka Hayashi, Dalibor Cimr, Hamido Fujita, Richard Cimler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17931">https://arxiv.org/abs/2404.17931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17931">https://arxiv.org/pdf/2404.17931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17931]] Critical Review for One-class Classification: recent advances and the  reality behind them(https://arxiv.org/abs/2404.17931)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This paper offers a comprehensive review of one-class classification (OCC), examining the technologies and methodologies employed in its implementation. It delves into various approaches utilized for OCC across diverse data types, such as feature data, image, video, time series, and others. Through a systematic review, this paper synthesizes promi-nent strategies used in OCC from its inception to its current advance-ments, with a particular emphasis on the promising application. Moreo-ver, the article criticizes the state-of-the-art (SOTA) image anomaly de-tection (AD) algorithms dominating one-class experiments. These algo-rithms include outlier exposure (binary classification) and pretrained model (multi-class classification), conflicting with the fundamental con-cept of learning from one class. Our investigation reveals that the top nine algorithms for one-class CIFAR10 benchmark are not OCC. We ar-gue that binary/multi-class classification algorithms should not be com-pared with OCC.</li>
</ul>

<h3>Title: Random Walk on Pixel Manifolds for Anomaly Segmentation of Complex  Driving Scenes</h3>
<ul>
<li><strong>Authors: </strong>Zelong Zeng, Kaname Tomite</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17961">https://arxiv.org/abs/2404.17961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17961">https://arxiv.org/pdf/2404.17961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17961]] Random Walk on Pixel Manifolds for Anomaly Segmentation of Complex  Driving Scenes(https://arxiv.org/abs/2404.17961)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In anomaly segmentation for complex driving scenes, state-of-the-art approaches utilize anomaly scoring functions to calculate anomaly scores. For these functions, accurately predicting the logits of inlier classes for each pixel is crucial for precisely inferring the anomaly score. However, in real-world driving scenarios, the diversity of scenes often results in distorted manifolds of pixel embeddings in embedding space. This effect is not conducive to directly using the pixel embeddings for the logit prediction during inference, a concern overlooked by existing methods. To address this problem, we propose a novel method called Random Walk on Pixel Manifolds (RWPM). RWPM utilizes random walks to reveal the intrinsic relationships among pixels to refine the pixel embeddings. The refined pixel embeddings alleviate the distortion of manifolds, improving the accuracy of anomaly scores. Our extensive experiments show that RWPM consistently improve the performance of the existing anomaly segmentation methods and achieve the best results. Code: \url{https://github.com/ZelongZeng/RWPM}.</li>
</ul>

<h3>Title: A Method of Moments Embedding Constraint and its Application to  Semi-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Michael Majurski, Sumeet Menon, Parniyan Farvardin, David Chapman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17978">https://arxiv.org/abs/2404.17978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17978">https://arxiv.org/pdf/2404.17978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17978]] A Method of Moments Embedding Constraint and its Application to  Semi-Supervised Learning(https://arxiv.org/abs/2404.17978)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Discriminative deep learning models with a linear+softmax final layer have a problem: the latent space only predicts the conditional probabilities $p(Y|X)$ but not the full joint distribution $p(Y,X)$, which necessitates a generative approach. The conditional probability cannot detect outliers, causing outlier sensitivity in softmax networks. This exacerbates model over-confidence impacting many problems, such as hallucinations, confounding biases, and dependence on large datasets. To address this we introduce a novel embedding constraint based on the Method of Moments (MoM). We investigate the use of polynomial moments ranging from 1st through 4th order hyper-covariance matrices. Furthermore, we use this embedding constraint to train an Axis-Aligned Gaussian Mixture Model (AAGMM) final layer, which learns not only the conditional, but also the joint distribution of the latent space. We apply this method to the domain of semi-supervised image classification by extending FlexMatch with our technique. We find our MoM constraint with the AAGMM layer is able to match the reported FlexMatch accuracy, while also modeling the joint distribution, thereby reducing outlier sensitivity. We also present a preliminary outlier detection strategy based on Mahalanobis distance and discuss future improvements to this strategy. Code is available at: \url{https://github.com/mmajurski/ssl-gmm}</li>
</ul>

<h3>Title: Enhancing Pre-Trained Generative Language Models with Question Attended  Span Extraction on Machine Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Lin Ai, Zheng Hui, Zizhou Liu, Julia Hirschberg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.17991">https://arxiv.org/abs/2404.17991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.17991">https://arxiv.org/pdf/2404.17991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.17991]] Enhancing Pre-Trained Generative Language Models with Question Attended  Span Extraction on Machine Reading Comprehension(https://arxiv.org/abs/2404.17991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine Reading Comprehension (MRC) poses a significant challenge in the field of Natural Language Processing (NLP). While mainstream MRC methods predominantly leverage extractive strategies using encoder-only models such as BERT, generative approaches face the issue of out-of-control generation -- a critical problem where answers generated are often incorrect, irrelevant, or unfaithful to the source text. To address these limitations in generative models for MRC, we introduce the Question-Attended Span Extraction (QASE) module. Integrated during the fine-tuning phase of pre-trained generative language models (PLMs), QASE significantly enhances their performance, allowing them to surpass the extractive capabilities of advanced Large Language Models (LLMs) such as GPT-4. Notably, these gains in performance do not come with an increase in computational demands. The efficacy of the QASE module has been rigorously tested across various datasets, consistently achieving or even surpassing state-of-the-art (SOTA) results.</li>
</ul>

<h3>Title: Implicit Generative Prior for Bayesian Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yijia Liu, Xiao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18008">https://arxiv.org/abs/2404.18008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18008">https://arxiv.org/pdf/2404.18008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18008]] Implicit Generative Prior for Bayesian Neural Networks(https://arxiv.org/abs/2404.18008)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Predictive uncertainty quantification is crucial for reliable decision-making in various applied domains. Bayesian neural networks offer a powerful framework for this task. However, defining meaningful priors and ensuring computational efficiency remain significant challenges, especially for complex real-world applications. This paper addresses these challenges by proposing a novel neural adaptive empirical Bayes (NA-EB) framework. NA-EB leverages a class of implicit generative priors derived from low-dimensional distributions. This allows for efficient handling of complex data structures and effective capture of underlying relationships in real-world datasets. The proposed NA-EB framework combines variational inference with a gradient ascent algorithm. This enables simultaneous hyperparameter selection and approximation of the posterior distribution, leading to improved computational efficiency. We establish the theoretical foundation of the framework through posterior and classification consistency. We demonstrate the practical applications of our framework through extensive evaluations on a variety of tasks, including the two-spiral problem, regression, 10 UCI datasets, and image classification tasks on both MNIST and CIFAR-10 datasets. The results of our experiments highlight the superiority of our proposed framework over existing methods, such as sparse variational Bayesian and generative models, in terms of prediction accuracy and uncertainty quantification.</li>
</ul>

<h3>Title: DM-Align: Leveraging the Power of Natural Language Instructions to Make  Changes to Images</h3>
<ul>
<li><strong>Authors: </strong>Maria Mihaela Trusca, Tinne Tuytelaars, Marie-Francine Moens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18020">https://arxiv.org/abs/2404.18020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18020">https://arxiv.org/pdf/2404.18020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18020]] DM-Align: Leveraging the Power of Natural Language Instructions to Make  Changes to Images(https://arxiv.org/abs/2404.18020)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-based semantic image editing assumes the manipulation of an image using a natural language instruction. Although recent works are capable of generating creative and qualitative images, the problem is still mostly approached as a black box sensitive to generating unexpected outputs. Therefore, we propose a novel model to enhance the text-based control of an image editor by explicitly reasoning about which parts of the image to alter or preserve. It relies on word alignments between a description of the original source image and the instruction that reflects the needed updates, and the input image. The proposed Diffusion Masking with word Alignments (DM-Align) allows the editing of an image in a transparent and explainable way. It is evaluated on a subset of the Bison dataset and a self-defined dataset dubbed Dream. When comparing to state-of-the-art baselines, quantitative and qualitative results show that DM-Align has superior performance in image editing conditioned on language instructions, well preserves the background of the image and can better cope with long text instructions.</li>
</ul>

<h3>Title: Exposing Text-Image Inconsistency Using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mingzhen Huang, Shan Jia, Zhou Zhou, Yan Ju, Jialing Cai, Siwei Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18033">https://arxiv.org/abs/2404.18033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18033">https://arxiv.org/pdf/2404.18033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18033]] Exposing Text-Image Inconsistency Using Diffusion Models(https://arxiv.org/abs/2404.18033)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the battle against widespread online misinformation, a growing problem is text-image inconsistency, where images are misleadingly paired with texts with different intent or meaning. Existing classification-based methods for text-image inconsistency can identify contextual inconsistencies but fail to provide explainable justifications for their decisions that humans can understand. Although more nuanced, human evaluation is impractical at scale and susceptible to errors. To address these limitations, this study introduces D-TIIL (Diffusion-based Text-Image Inconsistency Localization), which employs text-to-image diffusion models to localize semantic inconsistencies in text and image pairs. These models, trained on large-scale datasets act as ``omniscient" agents that filter out irrelevant information and incorporate background knowledge to identify inconsistencies. In addition, D-TIIL uses text embeddings and modified image regions to visualize these inconsistencies. To evaluate D-TIIL's efficacy, we introduce a new TIIL dataset containing 14K consistent and inconsistent text-image pairs. Unlike existing datasets, TIIL enables assessment at the level of individual words and image regions and is carefully designed to represent various inconsistencies. D-TIIL offers a scalable and evidence-based approach to identifying and localizing text-image inconsistency, providing a robust framework for future research combating misinformation.</li>
</ul>

<h3>Title: Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View  Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Li, Jiawei Mo, Ying Wang, Chethan Parameshwara, Xiaohan Fei, Ashwin Swaminathan, CJ Taylor, Zhuowen Tu, Paolo Favaro, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18065">https://arxiv.org/abs/2404.18065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18065">https://arxiv.org/pdf/2404.18065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18065]] Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View  Diffusion Model(https://arxiv.org/abs/2404.18065)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an effective two-stage approach named Grounded-Dreamer to generate 3D assets that can accurately follow complex, compositional text prompts while achieving high fidelity by using a pre-trained multi-view diffusion model. Multi-view diffusion models, such as MVDream, have shown to generate high-fidelity 3D assets using score distillation sampling (SDS). However, applied naively, these methods often fail to comprehend compositional text prompts, and may often entirely omit certain subjects or parts. To address this issue, we first advocate leveraging text-guided 4-view images as the bottleneck in the text-to-3D pipeline. We then introduce an attention refocusing mechanism to encourage text-aligned 4-view image generation, without the necessity to re-train the multi-view diffusion model or craft a high-quality compositional 3D dataset. We further propose a hybrid optimization strategy to encourage synergy between the SDS loss and the sparse RGB reference images. Our method consistently outperforms previous state-of-the-art (SOTA) methods in generating compositional 3D assets, excelling in both quality and accuracy, and enabling diverse 3D from the same text prompt.</li>
</ul>

<h3>Title: CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with  Fine-tuned Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Zhengpeng Shi, Haoran Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18085">https://arxiv.org/abs/2404.18085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18085">https://arxiv.org/pdf/2404.18085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18085]] CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with  Fine-tuned Large Language Model(https://arxiv.org/abs/2404.18085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Domain-Specific Chinese Relation Extraction (DSCRE) aims to extract relations between entities from domain-specific Chinese text. Despite the rapid development of PLMs in recent years, especially LLMs, DSCRE still faces three core challenges: complex network structure design, poor awareness, and high consumption of fine-tuning. Given the impressive performance of large language models (LLMs) in natural language processing, we propose a new framework called CRE-LLM. This framework is based on fine-tuning open-source LLMs, such as Llama-2, ChatGLM2, and Baichuan2. CRE-LLM enhances the logic-awareness and generative capabilities of the model by constructing an appropriate prompt and utilizing open-source LLMs for instruction-supervised fine-tuning. And then it directly extracts the relations of the given entities in the input textual data, which improving the CRE approach. To demonstrate the effectiveness of the proposed framework, we conducted extensive experiments on two domain-specific CRE datasets, FinRE and SanWen. The experimental results show that CRE-LLM is significantly superior and robust, achieving state-of-the-art (SOTA) performance on the FinRE dataset. This paper introduces a novel approach to domain-specific relation extraction (DSCRE) tasks that are semantically more complex by combining LLMs with triples. Our code is publicly available.</li>
</ul>

<h3>Title: Generative AI for Visualization: State of the Art and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, Wei Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18144">https://arxiv.org/abs/2404.18144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18144">https://arxiv.org/pdf/2404.18144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18144]] Generative AI for Visualization: State of the Art and Future Directions(https://arxiv.org/abs/2404.18144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) has witnessed remarkable progress in recent years and demonstrated impressive performance in various generation tasks in different domains such as computer vision and computational design. Many researchers have attempted to integrate GenAI into visualization framework, leveraging the superior generative capacity for different operations. Concurrently, recent major breakthroughs in GenAI like diffusion model and large language model have also drastically increase the potential of GenAI4VIS. From a technical perspective, this paper looks back on previous visualization studies leveraging GenAI and discusses the challenges and opportunities for future research. Specifically, we cover the applications of different types of GenAI methods including sequence, tabular, spatial and graph generation techniques for different tasks of visualization which we summarize into four major stages: data enhancement, visual mapping generation, stylization and interaction. For each specific visualization sub-task, we illustrate the typical data and concrete GenAI algorithms, aiming to provide in-depth understanding of the state-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based on the survey, we discuss three major aspects of challenges and research opportunities including evaluation, dataset, and the gap between end-to-end GenAI and generative algorithms. By summarizing different generation algorithms, their current applications and limitations, this paper endeavors to provide useful insights for future GenAI4VIS research.</li>
</ul>

<h3>Title: Masked Attention as a Mechanism for Improving Interpretability of Vision  Transformers</h3>
<ul>
<li><strong>Authors: </strong>Clément Grisi, Geert Litjens, Jeroen van der Laak</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18152">https://arxiv.org/abs/2404.18152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18152">https://arxiv.org/pdf/2404.18152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18152]] Masked Attention as a Mechanism for Improving Interpretability of Vision  Transformers(https://arxiv.org/abs/2404.18152)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision Transformers are at the heart of the current surge of interest in foundation models for histopathology. They process images by breaking them into smaller patches following a regular grid, regardless of their content. Yet, not all parts of an image are equally relevant for its understanding. This is particularly true in computational pathology where background is completely non-informative and may introduce artefacts that could mislead predictions. To address this issue, we propose a novel method that explicitly masks background in Vision Transformers' attention mechanism. This ensures tokens corresponding to background patches do not contribute to the final image representation, thereby improving model robustness and interpretability. We validate our approach using prostate cancer grading from whole-slide images as a case study. Our results demonstrate that it achieves comparable performance with plain self-attention while providing more accurate and clinically meaningful attention heatmaps.</li>
</ul>

<h3>Title: Exploring the Robustness of In-Context Learning with Noisy Labels</h3>
<ul>
<li><strong>Authors: </strong>Chen Cheng, Xinzhi Yu, Haodong Wen, Jinsong Sun, Guanzhang Yue, Yihao Zhang, Zeming Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18191">https://arxiv.org/abs/2404.18191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18191">https://arxiv.org/pdf/2404.18191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18191]] Exploring the Robustness of In-Context Learning with Noisy Labels(https://arxiv.org/abs/2404.18191)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recently, the mysterious In-Context Learning (ICL) ability exhibited by Transformer architectures, especially in large language models (LLMs), has sparked significant research interest. However, the resilience of Transformers' in-context learning capabilities in the presence of noisy samples, prevalent in both training corpora and prompt demonstrations, remains underexplored. In this paper, inspired by prior research that studies ICL ability using simple function classes, we take a closer look at this problem by investigating the robustness of Transformers against noisy labels. Specifically, we first conduct a thorough evaluation and analysis of the robustness of Transformers against noisy labels during in-context learning and show that they exhibit notable resilience against diverse types of noise in demonstration labels. Furthermore, we delve deeper into this problem by exploring whether introducing noise into the training set, akin to a form of data augmentation, enhances such robustness during inference, and find that such noise can indeed improve the robustness of ICL. Overall, our fruitful analysis and findings provide a comprehensive understanding of the resilience of Transformer models against label noises during ICL and provide valuable insights into the research on Transformers in natural language processing. Our code is available at https://github.com/InezYu0928/in-context-learning.</li>
</ul>

<h3>Title: Paint by Inpaint: Learning to Add Image Objects by Removing Them First</h3>
<ul>
<li><strong>Authors: </strong>Navve Wasserman, Noam Rotstein, Roy Ganz, Ron Kimmel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18212">https://arxiv.org/abs/2404.18212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18212">https://arxiv.org/pdf/2404.18212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18212]] Paint by Inpaint: Learning to Add Image Objects by Removing Them First(https://arxiv.org/abs/2404.18212)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image editing has advanced significantly with the introduction of text-conditioned diffusion models. Despite this progress, seamlessly adding objects to images based on textual instructions without requiring user-provided input masks remains a challenge. We address this by leveraging the insight that removing objects (Inpaint) is significantly simpler than its inverse process of adding them (Paint), attributed to the utilization of segmentation mask datasets alongside inpainting models that inpaint within these masks. Capitalizing on this realization, by implementing an automated and extensive pipeline, we curate a filtered large-scale image dataset containing pairs of images and their corresponding object-removed versions. Using these pairs, we train a diffusion model to inverse the inpainting process, effectively adding objects into images. Unlike other editing datasets, ours features natural target images instead of synthetic ones; moreover, it maintains consistency between source and target by construction. Additionally, we utilize a large Vision-Language Model to provide detailed descriptions of the removed objects and a Large Language Model to convert these descriptions into diverse, natural-language instructions. We show that the trained model surpasses existing ones both qualitatively and quantitatively, and release the large-scale dataset alongside the trained models for the community.</li>
</ul>

<h3>Title: From Persona to Personalization: A Survey on Role-Playing Language  Agents</h3>
<ul>
<li><strong>Authors: </strong>Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18231">https://arxiv.org/abs/2404.18231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18231">https://arxiv.org/pdf/2404.18231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18231]] From Persona to Personalization: A Survey on Role-Playing Language  Agents(https://arxiv.org/abs/2404.18231)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.</li>
</ul>

<h3>Title: Fisher Information Improved Training-Free Conditional Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Kaiyu Song, Hanjiang Lai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18252">https://arxiv.org/abs/2404.18252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18252">https://arxiv.org/pdf/2404.18252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18252]] Fisher Information Improved Training-Free Conditional Diffusion Model(https://arxiv.org/abs/2404.18252)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, the diffusion model with the training-free methods has succeeded in conditional image generation tasks. However, there is an efficiency problem because it requires calculating the gradient with high computational cost, and previous methods make strong assumptions to solve it, sacrificing generalization. In this work, we propose the Fisher information guided diffusion model (FIGD). Concretely, we introduce the Fisher information to estimate the gradient without making any additional assumptions to reduce computation cost. Meanwhile, we demonstrate that the Fisher information ensures the generalization of FIGD and provides new insights for training-free methods based on the information theory. The experimental results demonstrate that FIGD could achieve different conditional generations more quickly while maintaining high quality.</li>
</ul>

<h3>Title: SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement  Learning Policies</h3>
<ul>
<li><strong>Authors: </strong>Amir Samadi, Konstantinos Koufos, Kurt Debattista, Mehrdad Dianati</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18326">https://arxiv.org/abs/2404.18326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18326">https://arxiv.org/pdf/2404.18326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18326]] SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement  Learning Policies(https://arxiv.org/abs/2404.18326)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While Deep Reinforcement Learning (DRL) has emerged as a promising solution for intricate control tasks, the lack of explainability of the learned policies impedes its uptake in safety-critical applications, such as automated driving systems (ADS). Counterfactual (CF) explanations have recently gained prominence for their ability to interpret black-box Deep Learning (DL) models. CF examples are associated with minimal changes in the input, resulting in a complementary output by the DL model. Finding such alternations, particularly for high-dimensional visual inputs, poses significant challenges. Besides, the temporal dependency introduced by the reliance of the DRL agent action on a history of past state observations further complicates the generation of CF examples. To address these challenges, we propose using a saliency map to identify the most influential input pixels across the sequence of past observed states by the agent. Then, we feed this map to a deep generative model, enabling the generation of plausible CFs with constrained modifications centred on the salient regions. We evaluate the effectiveness of our framework in diverse domains, including ADS, Atari Pong, Pacman and space-invaders games, using traditional performance metrics such as validity, proximity and sparsity. Experimental results demonstrate that this framework generates more informative and plausible CFs than the state-of-the-art for a wide range of environments and DRL agents. In order to foster research in this area, we have made our datasets and codes publicly available at https://github.com/Amir-Samadi/SAFE-RL.</li>
</ul>

<h3>Title: MultiMAE-DER: Multimodal Masked Autoencoder for Dynamic Emotion  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Peihao Xiang, Chaohao Lin, Kaida Wu, Ou Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18327">https://arxiv.org/abs/2404.18327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18327">https://arxiv.org/pdf/2404.18327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18327]] MultiMAE-DER: Multimodal Masked Autoencoder for Dynamic Emotion  Recognition(https://arxiv.org/abs/2404.18327)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to processing multimodal data for dynamic emotion recognition, named as the Multimodal Masked Autoencoder for Dynamic Emotion Recognition (MultiMAE-DER). The MultiMAE-DER leverages the closely correlated representation information within spatiotemporal sequences across visual and audio modalities. By utilizing a pre-trained masked autoencoder model, the MultiMAEDER is accomplished through simple, straightforward finetuning. The performance of the MultiMAE-DER is enhanced by optimizing six fusion strategies for multimodal input sequences. These strategies address dynamic feature correlations within cross-domain data across spatial, temporal, and spatiotemporal sequences. In comparison to state-of-the-art multimodal supervised learning models for dynamic emotion recognition, MultiMAE-DER enhances the weighted average recall (WAR) by 4.41% on the RAVDESS dataset and by 2.06% on the CREMAD. Furthermore, when compared with the state-of-the-art model of multimodal self-supervised learning, MultiMAE-DER achieves a 1.86% higher WAR on the IEMOCAP dataset.</li>
</ul>

<h3>Title: Exploring the Limits of Fine-grained LLM-based Physics Inference via  Premise Removal Interventions</h3>
<ul>
<li><strong>Authors: </strong>Jordan Meadows, Tamsin James, Andre Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18384">https://arxiv.org/abs/2404.18384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18384">https://arxiv.org/pdf/2404.18384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18384]] Exploring the Limits of Fine-grained LLM-based Physics Inference via  Premise Removal Interventions(https://arxiv.org/abs/2404.18384)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Language models can hallucinate when performing complex and detailed mathematical reasoning. Physics provides a rich domain for assessing mathematical reasoning capabilities where physical context imbues the use of symbols which needs to satisfy complex semantics (\textit{e.g.,} units, tensorial order), leading to instances where inference may be algebraically coherent, yet unphysical. In this work, we assess the ability of Language Models (LMs) to perform fine-grained mathematical and physical reasoning using a curated dataset encompassing multiple notations and Physics subdomains. We improve zero-shot scores using synthetic in-context examples, and demonstrate non-linear degradation of derivation quality with perturbation strength via the progressive omission of supporting premises. We find that the models' mathematical reasoning is not physics-informed in this setting, where physical context is predominantly ignored in favour of reverse-engineering solutions.</li>
</ul>

<h3>Title: PKU-AIGIQA-4K: A Perceptual Quality Assessment Database for Both  Text-to-Image and Image-to-Image AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Jiquan Yuan, Fanyi Yang, Jihe Li, Xinyan Cao, Jinming Che, Jinlong Lin, Xixin Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18409">https://arxiv.org/abs/2404.18409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18409">https://arxiv.org/pdf/2404.18409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18409]] PKU-AIGIQA-4K: A Perceptual Quality Assessment Database for Both  Text-to-Image and Image-to-Image AI-Generated Images(https://arxiv.org/abs/2404.18409)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, image generation technology has rapidly advanced, resulting in the creation of a vast array of AI-generated images (AIGIs). However, the quality of these AIGIs is highly inconsistent, with low-quality AIGIs severely impairing the visual experience of users. Due to the widespread application of AIGIs, the AI-generated image quality assessment (AIGIQA), aimed at evaluating the quality of AIGIs from the perspective of human perception, has garnered increasing interest among scholars. Nonetheless, current research has not yet fully explored this field. We have observed that existing databases are limited to images generated from single scenario settings. Databases such as AGIQA-1K, AGIQA-3K, and AIGCIQA2023, for example, only include images generated by text-to-image generative models. This oversight highlights a critical gap in the current research landscape, underscoring the need for dedicated databases catering to image-to-image scenarios, as well as more comprehensive databases that encompass a broader range of AI-generated image scenarios. Addressing these issues, we have established a large scale perceptual quality assessment database for both text-to-image and image-to-image AIGIs, named PKU-AIGIQA-4K. We then conduct a well-organized subjective experiment to collect quality labels for AIGIs and perform a comprehensive analysis of the PKU-AIGIQA-4K database. Regarding the use of image prompts during the training process, we propose three image quality assessment (IQA) methods based on pre-trained models that include a no-reference method NR-AIGCIQA, a full-reference method FR-AIGCIQA, and a partial-reference method PR-AIGCIQA. Finally, leveraging the PKU-AIGIQA-4K database, we conduct extensive benchmark experiments and compare the performance of the proposed methods and the current IQA methods.</li>
</ul>

<h3>Title: Mixture-of-Instructions: Comprehensive Alignment of a Large Language  Model through the Mixture of Diverse System Prompting Instructions</h3>
<ul>
<li><strong>Authors: </strong>Bowen Xu, Shaoyu Wu, Kai Liu, Lulu Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18410">https://arxiv.org/abs/2404.18410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18410">https://arxiv.org/pdf/2404.18410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18410]] Mixture-of-Instructions: Comprehensive Alignment of a Large Language  Model through the Mixture of Diverse System Prompting Instructions(https://arxiv.org/abs/2404.18410)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the proliferation of large language models (LLMs), the comprehensive alignment of such models across multiple tasks has emerged as a critical area of research. Existing alignment methodologies primarily address single task, such as multi-turn dialogue, coding, mathematical problem-solving, and tool usage. However, AI-driven products that leverage language models usually necessitate a fusion of these abilities to function effectively in real-world scenarios. Moreover, the considerable computational resources required for proper alignment of LLMs underscore the need for a more robust, efficient, and encompassing approach to multi-task alignment, ensuring improved generative performance. In response to these challenges, we introduce a novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction concatenation combined with diverse system prompts to boost the alignment efficiency of language models. We have also compiled a diverse set of seven benchmark datasets to rigorously evaluate the alignment efficacy of the MoI-enhanced language model. Our methodology was applied to the open-source Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This enhanced model demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks.</li>
</ul>

<h3>Title: U-Nets as Belief Propagation: Efficient Classification, Denoising, and  Diffusion in Generative Hierarchical Models</h3>
<ul>
<li><strong>Authors: </strong>Song Mei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18444">https://arxiv.org/abs/2404.18444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18444">https://arxiv.org/pdf/2404.18444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18444]] U-Nets as Belief Propagation: Efficient Classification, Denoising, and  Diffusion in Generative Hierarchical Models(https://arxiv.org/abs/2404.18444)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>U-Nets are among the most widely used architectures in computer vision, renowned for their exceptional performance in applications such as image segmentation, denoising, and diffusion modeling. However, a theoretical explanation of the U-Net architecture design has not yet been fully established. This paper introduces a novel interpretation of the U-Net architecture by studying certain generative hierarchical models, which are tree-structured graphical models extensively utilized in both language and image domains. With their encoder-decoder structure, long skip connections, and pooling and up-sampling layers, we demonstrate how U-Nets can naturally implement the belief propagation denoising algorithm in such generative hierarchical models, thereby efficiently approximating the denoising functions. This leads to an efficient sample complexity bound for learning the denoising function using U-Nets within these models. Additionally, we discuss the broader implications of these findings for diffusion models in generative hierarchical models. We also demonstrate that the conventional architecture of convolutional neural networks (ConvNets) is ideally suited for classification tasks within these models. This offers a unified view of the roles of ConvNets and U-Nets, highlighting the versatility of generative hierarchical models in modeling complex data distributions across language and image domains.</li>
</ul>

<h3>Title: Enabling Efficient and Flexible Interpretability of Data-driven Anomaly  Detection in Industrial Processes with AcME-AD</h3>
<ul>
<li><strong>Authors: </strong>Valentina Zaccaria, Chiara Masiero, David Dandolo, Gian Antonio Susto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18525">https://arxiv.org/abs/2404.18525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18525">https://arxiv.org/pdf/2404.18525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18525]] Enabling Efficient and Flexible Interpretability of Data-driven Anomaly  Detection in Industrial Processes with AcME-AD(https://arxiv.org/abs/2404.18525)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>While Machine Learning has become crucial for Industry 4.0, its opaque nature hinders trust and impedes the transformation of valuable insights into actionable decision, a challenge exacerbated in the evolving Industry 5.0 with its human-centric focus. This paper addresses this need by testing the applicability of AcME-AD in industrial settings. This recently developed framework facilitates fast and user-friendly explanations for anomaly detection. AcME-AD is model-agnostic, offering flexibility, and prioritizes real-time efficiency. Thus, it seems suitable for seamless integration with industrial Decision Support Systems. We present the first industrial application of AcME-AD, showcasing its effectiveness through experiments. These tests demonstrate AcME-AD's potential as a valuable tool for explainable AD and feature-based root cause analysis within industrial environments, paving the way for trustworthy and actionable insights in the age of Industry 5.0.</li>
</ul>

<h3>Title: IncidentResponseGPT: Generating Traffic Incident Response Plans with  Generative Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Artur Grigorev, Khaled Saleh, Yuming Ou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18550">https://arxiv.org/abs/2404.18550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18550">https://arxiv.org/pdf/2404.18550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18550]] IncidentResponseGPT: Generating Traffic Incident Response Plans with  Generative Artificial Intelligence(https://arxiv.org/abs/2404.18550)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Traffic congestion due to road incidents poses a significant challenge in urban environments, leading to increased pollution, economic losses, and traffic congestion. Efficiently managing these incidents is imperative for mitigating their adverse effects; however, the complexity of urban traffic systems and the variety of potential incidents represent a considerable obstacle. This paper introduces IncidentResponseGPT, an innovative solution designed to assist traffic management authorities by providing rapid, informed, and adaptable traffic incident response plans. By integrating a Generative AI platform with real-time traffic incident reports and operational guidelines, our system aims to streamline the decision-making process in responding to traffic incidents. The research addresses the critical challenges involved in deploying AI in traffic management, including overcoming the complexity of urban traffic networks, ensuring real-time decision-making capabilities, aligning with local laws and regulations, and securing public acceptance for AI-driven systems. Through a combination of text analysis of accident reports, validation of AI recommendations through traffic simulation, and implementation of transparent and validated AI systems, IncidentResponseGPT offers a promising approach to optimizing traffic flow and reducing congestion in the face of traffic incidents. The relevance of this work extends to traffic management authorities, emergency response teams, and municipal bodies, all integral stakeholders in urban traffic control and incident management. By proposing a novel solution to the identified challenges, this research aims to develop a framework that not only facilitates faster resolution of traffic incidents but also minimizes their overall impact on urban traffic systems.</li>
</ul>

<h3>Title: SIDBench: A Python Framework for Reliably Assessing Synthetic Image  Detection Methods</h3>
<ul>
<li><strong>Authors: </strong>Manos Schinas, Symeon Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18552">https://arxiv.org/abs/2404.18552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18552">https://arxiv.org/pdf/2404.18552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18552]] SIDBench: A Python Framework for Reliably Assessing Synthetic Image  Detection Methods(https://arxiv.org/abs/2404.18552)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generative AI technology offers an increasing variety of tools for generating entirely synthetic images that are increasingly indistinguishable from real ones. Unlike methods that alter portions of an image, the creation of completely synthetic images presents a unique challenge and several Synthetic Image Detection (SID) methods have recently appeared to tackle it. Yet, there is often a large gap between experimental results on benchmark datasets and the performance of methods in the wild. To better address the evaluation needs of SID and help close this gap, this paper introduces a benchmarking framework that integrates several state-of-the-art SID models. Our selection of integrated models was based on the utilization of varied input features, and different network architectures, aiming to encompass a broad spectrum of techniques. The framework leverages recent datasets with a diverse set of generative models, high level of photo-realism and resolution, reflecting the rapid improvements in image synthesis technology. Additionally, the framework enables the study of how image transformations, common in assets shared online, such as JPEG compression, affect detection performance. SIDBench is available on https://github.com/mever-team/sidbench and is designed in a modular manner to enable easy inclusion of new datasets and SID models.</li>
</ul>

<h3>Title: FashionSD-X: Multimodal Fashion Garment Synthesis using Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Kumar Singh, Ioannis Patras</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18591">https://arxiv.org/abs/2404.18591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18591">https://arxiv.org/pdf/2404.18591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18591]] FashionSD-X: Multimodal Fashion Garment Synthesis using Latent Diffusion(https://arxiv.org/abs/2404.18591)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid evolution of the fashion industry increasingly intersects with technological advancements, particularly through the integration of generative AI. This study introduces a novel generative pipeline designed to transform the fashion design process by employing latent diffusion models. Utilizing ControlNet and LoRA fine-tuning, our approach generates high-quality images from multimodal inputs such as text and sketches. We leverage and enhance state-of-the-art virtual try-on datasets, including Multimodal Dress Code and VITON-HD, by integrating sketch data. Our evaluation, utilizing metrics like FID, CLIP Score, and KID, demonstrates that our model significantly outperforms traditional stable diffusion models. The results not only highlight the effectiveness of our model in generating fashion-appropriate outputs but also underscore the potential of diffusion models in revolutionizing fashion design workflows. This research paves the way for more interactive, personalized, and technologically enriched methodologies in fashion design and representation, bridging the gap between creative vision and practical application.</li>
</ul>

<h3>Title: Anywhere: A Multi-Agent Framework for Reliable and Diverse  Foreground-Conditioned Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Tianyidan Xie, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Zili Yi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18598">https://arxiv.org/abs/2404.18598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18598">https://arxiv.org/pdf/2404.18598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18598]] Anywhere: A Multi-Agent Framework for Reliable and Diverse  Foreground-Conditioned Image Inpainting(https://arxiv.org/abs/2404.18598)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes. However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as "over-imagination", inconsistency between foreground and background, and limited diversity. In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues. Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models. This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer. The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts. In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image. The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed. Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating "over-imagination", resolving foreground-background discrepancies, and enhancing diversity. It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results.</li>
</ul>

<h3>Title: CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial  Animation Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Liang, Wenlin Zhuang, Tianyong Wang, Guangxing Geng, Guangyue Geng, Haifeng Xia, Siyu Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18604">https://arxiv.org/abs/2404.18604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18604">https://arxiv.org/pdf/2404.18604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18604]] CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial  Animation Generation(https://arxiv.org/abs/2404.18604)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Speech-driven 3D facial animation technology has been developed for years, but its practical application still lacks expectations. The main challenges lie in data limitations, lip alignment, and the naturalness of facial expressions. Although lip alignment has seen many related studies, existing methods struggle to synthesize natural and realistic expressions, resulting in a mechanical and stiff appearance of facial animations. Even with some research extracting emotional features from speech, the randomness of facial movements limits the effective expression of emotions. To address this issue, this paper proposes a method called CSTalk (Correlation Supervised) that models the correlations among different regions of facial movements and supervises the training of the generative model to generate realistic expressions that conform to human facial motion patterns. To generate more intricate animations, we employ a rich set of control parameters based on the metahuman character model and capture a dataset for five different emotions. We train a generative network using an autoencoder structure and input an emotion embedding vector to achieve the generation of user-control expressions. Experimental results demonstrate that our method outperforms existing state-of-the-art methods.</li>
</ul>

<h3>Title: FlexiFilm: Long Video Generation with Flexible Conditions</h3>
<ul>
<li><strong>Authors: </strong>Yichen Ouyang, jianhao Yuan, Hao Zhao, Gaoang Wang, Bo zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18620">https://arxiv.org/abs/2404.18620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18620">https://arxiv.org/pdf/2404.18620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18620]] FlexiFilm: Long Video Generation with Flexible Conditions(https://arxiv.org/abs/2404.18620)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating long and consistent videos has emerged as a significant yet challenging problem. While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation. This results in prominent temporal inconsistency and overexposure. Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation. Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure. Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses. Project page: https://y-ichen.github.io/FlexiFilm-Page/</li>
</ul>

<h3>Title: Convergence Properties of Score-Based Models using Graduated  Optimisation for Linear Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Pascal Fernsel, Željko Kereta, Alexander Denker</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18699">https://arxiv.org/abs/2404.18699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18699">https://arxiv.org/pdf/2404.18699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18699]] Convergence Properties of Score-Based Models using Graduated  Optimisation for Linear Inverse Problems(https://arxiv.org/abs/2404.18699)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The incorporation of generative models as regularisers within variational formulations for inverse problems has proven effective across numerous image reconstruction tasks. However, the resulting optimisation problem is often non-convex and challenging to solve. In this work, we show that score-based generative models (SGMs) can be used in a graduated optimisation framework to solve inverse problems. We show that the resulting graduated non-convexity flow converge to stationary points of the original problem and provide a numerical convergence analysis of a 2D toy example. We further provide experiments on computed tomography image reconstruction, where we show that this framework is able to recover high-quality images, independent of the initial value. The experiments highlight the potential of using SGMs in graduated optimisation frameworks.</li>
</ul>

<h3>Title: Towards Dog Bark Decoding: Leveraging Human Speech Processing for  Automated Bark Classification</h3>
<ul>
<li><strong>Authors: </strong>Artem Abzaliev, Humberto Pérez Espinosa, Rada Mihalcea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18739">https://arxiv.org/abs/2404.18739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18739">https://arxiv.org/pdf/2404.18739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18739]] Towards Dog Bark Decoding: Leveraging Human Speech Processing for  Automated Bark Classification(https://arxiv.org/abs/2404.18739)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Similar to humans, animals make extensive use of verbal and non-verbal forms of communication, including a large range of audio signals. In this paper, we address dog vocalizations and explore the use of self-supervised speech representation models pre-trained on human speech to address dog bark classification tasks that find parallels in human-centered tasks in speech recognition. We specifically address four tasks: dog recognition, breed identification, gender classification, and context grounding. We show that using speech embedding representations significantly improves over simpler classification baselines. Further, we also find that models pre-trained on large human speech acoustics can provide additional performance boosts on several tasks.</li>
</ul>

<h3>Title: Evaluating the Effectiveness of Video Anomaly Detection in the Wild:  Online Learning and Inference for Real-world Deployment</h3>
<ul>
<li><strong>Authors: </strong>Shanle Yao, Ghazal Alinezhad Noghre, Armin Danesh Pazho, Hamed Tabkhi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18747">https://arxiv.org/abs/2404.18747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18747">https://arxiv.org/pdf/2404.18747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18747]] Evaluating the Effectiveness of Video Anomaly Detection in the Wild:  Online Learning and Inference for Real-world Deployment(https://arxiv.org/abs/2404.18747)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Video Anomaly Detection (VAD) identifies unusual activities in video streams, a key technology with broad applications ranging from surveillance to healthcare. Tackling VAD in real-life settings poses significant challenges due to the dynamic nature of human actions, environmental variations, and domain shifts. Many research initiatives neglect these complexities, often concentrating on traditional testing methods that fail to account for performance on unseen datasets, creating a gap between theoretical models and their real-world utility. Online learning is a potential strategy to mitigate this issue by allowing models to adapt to new information continuously. This paper assesses how well current VAD algorithms can adjust to real-life conditions through an online learning framework, particularly those based on pose analysis, for their efficiency and privacy advantages. Our proposed framework enables continuous model updates with streaming data from novel environments, thus mirroring actual world challenges and evaluating the models' ability to adapt in real-time while maintaining accuracy. We investigate three state-of-the-art models in this setting, focusing on their adaptability across different domains. Our findings indicate that, even under the most challenging conditions, our online learning approach allows a model to preserve 89.39% of its original effectiveness compared to its offline-trained counterpart in a specific target domain.</li>
</ul>

<h3>Title: Flow AM: Generating Point Cloud Global Explanations by Latent Alignment</h3>
<ul>
<li><strong>Authors: </strong>Hanxiao Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18760">https://arxiv.org/abs/2404.18760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18760">https://arxiv.org/pdf/2404.18760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18760]] Flow AM: Generating Point Cloud Global Explanations by Latent Alignment(https://arxiv.org/abs/2404.18760)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although point cloud models have gained significant improvements in prediction accuracy over recent years, their trustworthiness is still not sufficiently investigated. In terms of global explainability, Activation Maximization (AM) techniques in the image domain are not directly transplantable due to the special structure of the point cloud models. Existing studies exploit generative models to yield global explanations that can be perceived by humans. However, the opacity of the generative models themselves and the introduction of additional priors call into question the plausibility and fidelity of the explanations. In this work, we demonstrate that when the classifier predicts different types of instances, the intermediate layer activations are differently activated, known as activation flows. Based on this property, we propose an activation flow-based AM method that generates global explanations that can be perceived without incorporating any generative model. Furthermore, we reveal that AM based on generative models fails the sanity checks and thus lack of fidelity. Extensive experiments show that our approach dramatically enhances the perceptibility of explanations compared to other AM methods that are not based on generative models. Our code is available at: https://github.com/Explain3D/FlowAM</li>
</ul>

<h3>Title: From Density to Geometry: YOLOv8 Instance Segmentation for Reverse  Engineering of Optimized Structures</h3>
<ul>
<li><strong>Authors: </strong>Thomas Rochefort-Beaudoin, Aurelian Vadean, Sofiane Achiche, Niels Aage</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18763">https://arxiv.org/abs/2404.18763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18763">https://arxiv.org/pdf/2404.18763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18763]] From Density to Geometry: YOLOv8 Instance Segmentation for Reverse  Engineering of Optimized Structures(https://arxiv.org/abs/2404.18763)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper introduces YOLOv8-TO, a novel approach for reverse engineering of topology-optimized structures into interpretable geometric parameters using the YOLOv8 instance segmentation model. Density-based topology optimization methods require post-processing to convert the optimal density distribution into a parametric representation for design exploration and integration with CAD tools. Traditional methods such as skeletonization struggle with complex geometries and require manual intervention. YOLOv8-TO addresses these challenges by training a custom YOLOv8 model to automatically detect and reconstruct structural components from binary density distributions. The model is trained on a diverse dataset of both optimized and random structures generated using the Moving Morphable Components method. A custom reconstruction loss function based on the dice coefficient of the predicted geometry is used to train the new regression head of the model via self-supervised learning. The method is evaluated on test sets generated from different topology optimization methods, including out-of-distribution samples, and compared against a skeletonization approach. Results show that YOLOv8-TO significantly outperforms skeletonization in reconstructing visually and structurally similar designs. The method showcases an average improvement of 13.84% in the Dice coefficient, with peak enhancements reaching 20.78%. The method demonstrates good generalization to complex geometries and fast inference times, making it suitable for integration into design workflows using regular workstations. Limitations include the sensitivity to non-max suppression thresholds. YOLOv8-TO represents a significant advancement in topology optimization post-processing, enabling efficient and accurate reverse engineering of optimized structures for design exploration and manufacturing.</li>
</ul>

<h3>Title: ConPro: Learning Severity Representation for Medical Images using  Contrastive Learning and Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hong Nguyen, Hoang Nguyen, Melinda Chang, Hieu Pham, Shrikanth Narayanan, Michael Pazzani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18831">https://arxiv.org/abs/2404.18831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18831">https://arxiv.org/pdf/2404.18831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18831]] ConPro: Learning Severity Representation for Medical Images using  Contrastive Learning and Preference Optimization(https://arxiv.org/abs/2404.18831)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Understanding the severity of conditions shown in images in medical diagnosis is crucial, serving as a key guide for clinical assessment, treatment, as well as evaluating longitudinal progression. This paper proposes Con- PrO: a novel representation learning method for severity assessment in medical images using Contrastive learningintegrated Preference Optimization. Different from conventional contrastive learning methods that maximize the distance between classes, ConPrO injects into the latent vector the distance preference knowledge between various severity classes and the normal class. We systematically examine the key components of our framework to illuminate how contrastive prediction tasks acquire valuable representations. We show that our representation learning framework offers valuable severity ordering in the feature space while outperforming previous state-of-the-art methods on classification tasks. We achieve a 6% and 20% relative improvement compared to a supervised and a self-supervised baseline, respectively. In addition, we derived discussions on severity indicators and related applications of preference comparison in the medical domain.</li>
</ul>

<h3>Title: It's Difficult to be Neutral -- Human and LLM-based Sentiment Annotation  of Patient Comments</h3>
<ul>
<li><strong>Authors: </strong>Petter Mæhlum, David Samuel, Rebecka Maria Norman, Elma Jelin, Øyvind Andresen Bjertnæs, Lilja Øvrelid, Erik Velldal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18832">https://arxiv.org/abs/2404.18832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18832">https://arxiv.org/pdf/2404.18832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18832]] It's Difficult to be Neutral -- Human and LLM-based Sentiment Annotation  of Patient Comments(https://arxiv.org/abs/2404.18832)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Sentiment analysis is an important tool for aggregating patient voices, in order to provide targeted improvements in healthcare services. A prerequisite for this is the availability of in-domain data annotated for sentiment. This article documents an effort to add sentiment annotations to free-text comments in patient surveys collected by the Norwegian Institute of Public Health (NIPH). However, annotation can be a time-consuming and resource-intensive process, particularly when it requires domain expertise. We therefore also evaluate a possible alternative to human annotation, using large language models (LLMs) as annotators. We perform an extensive evaluation of the approach for two openly available pretrained LLMs for Norwegian, experimenting with different configurations of prompts and in-context learning, comparing their performance to human annotators. We find that even for zero-shot runs, models perform well above the baseline for binary sentiment, but still cannot compete with human annotators on the full dataset.</li>
</ul>

<h3>Title: A Survey on Vision Mamba: Models, Applications and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Rui Xu, Shu Yang, Yihui Wang, Bo Du, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18861">https://arxiv.org/abs/2404.18861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18861">https://arxiv.org/pdf/2404.18861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18861]] A Survey on Vision Mamba: Models, Applications and Challenges(https://arxiv.org/abs/2404.18861)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Mamba, a recent selective structured state space model, performs excellently on long sequence modeling tasks. Mamba mitigates the modeling constraints of convolutional neural networks and offers advanced modeling capabilities similar to those of Transformers, through global receptive fields and dynamic weighting. Crucially, it achieves this without incurring the quadratic computational complexity typically associated with Transformers. Due to its advantages over the former two mainstream foundation models, Mamba exhibits great potential to be a visual foundation model. Researchers are actively applying Mamba to various computer vision tasks, leading to numerous emerging works. To help keep pace with the rapid advancements in computer vision, this paper aims to provide a comprehensive review of visual Mamba approaches. This paper begins by delineating the formulation of the original Mamba model. Subsequently, our review of visual Mamba delves into several representative backbone networks to elucidate the core insights of the visual Mamba. We then categorize related works using different modalities, including image, video, point cloud, multi-modal, and others. Specifically, for image applications, we further organize them into distinct tasks to facilitate a more structured discussion. Finally, we discuss the challenges and future research directions for visual Mamba, providing insights for future research in this quickly evolving area. A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.</li>
</ul>

<h3>Title: Truth-value judgment in language models: belief directions are context  sensitive</h3>
<ul>
<li><strong>Authors: </strong>Stefan F. Schouten, Peter Bloem, Ilia Markov, Piek Vossen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18865">https://arxiv.org/abs/2404.18865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18865">https://arxiv.org/pdf/2404.18865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18865]] Truth-value judgment in language models: belief directions are context  sensitive(https://arxiv.org/abs/2404.18865)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent work has demonstrated that the latent spaces of large language models (LLMs) contain directions predictive of the truth of sentences. Multiple methods recover such directions and build probes that are described as getting at a model's "knowledge" or "beliefs". We investigate this phenomenon, looking closely at the impact of context on the probes. Our experiments establish where in the LLM the probe's predictions can be described as being conditional on the preceding (related) sentences. Specifically, we quantify the responsiveness of the probes to the presence of (negated) supporting and contradicting sentences, and score the probes on their consistency. We also perform a causal intervention experiment, investigating whether moving the representation of a premise along these belief directions influences the position of the hypothesis along that same direction. We find that the probes we test are generally context sensitive, but that contexts which should not affect the truth often still impact the probe outputs. Our experiments show that the type of errors depend on the layer, the (type of) model, and the kind of data. Finally, our results suggest that belief directions are (one of the) causal mediators in the inference process that incorporates in-context information.</li>
</ul>

<h3>Title: Learning Mixtures of Gaussians Using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Khashayar Gatmiry, Jonathan Kelner, Holden Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS, math.PR, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18869">https://arxiv.org/abs/2404.18869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18869">https://arxiv.org/pdf/2404.18869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18869]] Learning Mixtures of Gaussians Using Diffusion Models(https://arxiv.org/abs/2404.18869)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We give a new algorithm for learning mixtures of $k$ Gaussians (with identity covariance in $\mathbb{R}^n$) to TV error $\varepsilon$, with quasi-polynomial ($O(n^{\text{poly log}\left(\frac{n+k}{\varepsilon}\right)})$) time and sample complexity, under a minimum weight assumption. Unlike previous approaches, most of which are algebraic in nature, our approach is analytic and relies on the framework of diffusion models. Diffusion models are a modern paradigm for generative modeling, which typically rely on learning the score function (gradient log-pdf) along a process transforming a pure noise distribution, in our case a Gaussian, to the data distribution. Despite their dazzling performance in tasks such as image generation, there are few end-to-end theoretical guarantees that they can efficiently learn nontrivial families of distributions; we give some of the first such guarantees. We proceed by deriving higher-order Gaussian noise sensitivity bounds for the score functions for a Gaussian mixture to show that that they can be inductively learned using piecewise polynomial regression (up to poly-logarithmic degree), and combine this with known convergence results for diffusion models. Our results extend to continuous mixtures of Gaussians where the mixing distribution is supported on a union of $k$ balls of constant radius. In particular, this applies to the case of Gaussian convolutions of distributions on low-dimensional manifolds, or more generally sets with small covering number.</li>
</ul>

<h3>Title: A Survey on Diffusion Models for Time Series and Spatio-Temporal Data</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang, Lintao Ma, Yi Wang, Chenghao Liu, Bin Yang, Zenglin Xu, Jiang Bian, Shirui Pan, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18886">https://arxiv.org/abs/2404.18886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18886">https://arxiv.org/pdf/2404.18886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18886]] A Survey on Diffusion Models for Time Series and Spatio-Temporal Data(https://arxiv.org/abs/2404.18886)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, anomaly</a></li>
<li><strong>Abstract: </strong>The study of time series data is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series data and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.</li>
</ul>

<h3>Title: Hide and Seek: How Does Watermarking Impact Face Recognition?</h3>
<ul>
<li><strong>Authors: </strong>Yuguang Yao, Steven Grosz, Sijia Liu, Anil Jain</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18890">https://arxiv.org/abs/2404.18890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18890">https://arxiv.org/pdf/2404.18890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18890]] Hide and Seek: How Does Watermarking Impact Face Recognition?(https://arxiv.org/abs/2404.18890)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The recent progress in generative models has revolutionized the synthesis of highly realistic images, including face images. This technological development has undoubtedly helped face recognition, such as training data augmentation for higher recognition accuracy and data privacy. However, it has also introduced novel challenges concerning the responsible use and proper attribution of computer generated images. We investigate the impact of digital watermarking, a technique for embedding ownership signatures into images, on the effectiveness of face recognition models. We propose a comprehensive pipeline that integrates face image generation, watermarking, and face recognition to systematically examine this question. The proposed watermarking scheme, based on an encoder-decoder architecture, successfully embeds and recovers signatures from both real and synthetic face images while preserving their visual fidelity. Through extensive experiments, we unveil that while watermarking enables robust image attribution, it results in a slight decline in face recognition accuracy, particularly evident for face images with challenging poses and expressions. Additionally, we find that directly training face recognition models on watermarked images offers only a limited alleviation of this performance decline. Our findings underscore the intricate trade off between watermarking and face recognition accuracy. This work represents a pivotal step towards the responsible utilization of generative models in face recognition and serves to initiate discussions regarding the broader implications of watermarking in biometrics.</li>
</ul>

<h3>Title: Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face  of Environmental Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Laixi Shi, Eric Mazumdar, Yuejie Chi, Adam Wierman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18909">https://arxiv.org/abs/2404.18909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18909">https://arxiv.org/pdf/2404.18909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18909]] Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face  of Environmental Uncertainty(https://arxiv.org/abs/2404.18909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties. While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions. This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set. This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria. Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria. We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length.</li>
</ul>

<h3>Title: TheaterGen: Character Management with LLM for Consistent Multi-turn  Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18919">https://arxiv.org/abs/2404.18919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18919">https://arxiv.org/pdf/2404.18919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18919]] TheaterGen: Character Management with LLM for Consistent Multi-turn  Image Generation(https://arxiv.org/abs/2404.18919)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a "Screenwriter", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the "Rehearsal". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the "Final Performance". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.</li>
</ul>

<h3>Title: Stylus: Automatic Adapter Selection for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Luo, Justin Wong, Brandon Trabucco, Yanping Huang, Joseph E. Gonzalez, Zhifeng Chen, Ruslan Salakhutdinov, Ion Stoica</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18928">https://arxiv.org/abs/2404.18928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18928">https://arxiv.org/pdf/2404.18928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18928]] Stylus: Automatic Adapter Selection for Diffusion Models(https://arxiv.org/abs/2404.18928)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters-most of which are highly customized with insufficient descriptions. This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
