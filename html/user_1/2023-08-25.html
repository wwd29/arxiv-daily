<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation. (arXiv:2308.12350v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12350">http://arxiv.org/abs/2308.12350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12350]] Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation(http://arxiv.org/abs/2308.12350)</code></li>
<li>Summary: <p>Translating images from a source domain to a target domain for learning
target models is one of the most common strategies in domain adaptive semantic
segmentation (DASS). However, existing methods still struggle to preserve
semantically-consistent local details between the original and translated
images. In this work, we present an innovative approach that addresses this
challenge by using source-domain labels as explicit guidance during image
translation. Concretely, we formulate cross-domain image translation as a
denoising diffusion process and utilize a novel Semantic Gradient Guidance
(SGG) method to constrain the translation process, conditioning it on the
pixel-wise source labels. Additionally, a Progressive Translation Learning
(PTL) strategy is devised to enable the SGG method to work reliably across
domains with large gaps. Extensive experiments demonstrate the superiority of
our approach over state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12453">http://arxiv.org/abs/2308.12453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12453]] Augmenting medical image classifiers with synthetic data from latent diffusion models(http://arxiv.org/abs/2308.12453)</code></li>
<li>Summary: <p>While hundreds of artificial intelligence (AI) algorithms are now approved or
cleared by the US Food and Drugs Administration (FDA), many studies have shown
inconsistent generalization or latent bias, particularly for underrepresented
populations. Some have proposed that generative AI could reduce the need for
real data, but its utility in model development remains unclear. Skin disease
serves as a useful case study in synthetic image generation due to the
diversity of disease appearance, particularly across the protected attribute of
skin tone. Here we show that latent diffusion models can scalably generate
images of skin disease and that augmenting model training with these data
improves performance in data-limited settings. These performance gains saturate
at synthetic-to-real image ratios above 10:1 and are substantially smaller than
the gains obtained from adding real images. As part of our analysis, we
generate and analyze a new dataset of 458,920 synthetic images produced using
several generation strategies. Our results suggest that synthetic data could
serve as a force-multiplier for model development, but the collection of
diverse real-world data remains the most important step to improve medical AI
algorithms.
</p></li>
</ul>

<h3>Title: Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion. (arXiv:2308.12469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12469">http://arxiv.org/abs/2308.12469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12469]] Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion(http://arxiv.org/abs/2308.12469)</code></li>
<li>Summary: <p>Producing quality segmentation masks for images is a fundamental problem in
computer vision. Recent research has explored large-scale supervised training
to enable zero-shot segmentation on virtually any image style and unsupervised
training to enable segmentation without dense annotations. However,
constructing a model capable of segmenting anything in a zero-shot manner
without any annotations is still challenging. In this paper, we propose to
utilize the self-attention layers in stable diffusion models to achieve this
goal because the pre-trained stable diffusion model has learned inherent
concepts of objects within its attention layers. Specifically, we introduce a
simple yet effective iterative merging process based on measuring KL divergence
among attention maps to merge them into valid segmentation masks. The proposed
method does not require any training or language dependency to extract quality
segmentation for any images. On COCO-Stuff-27, our method surpasses the prior
unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%
in mean IoU.
</p></li>
</ul>

<h3>Title: DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition. (arXiv:2308.12501v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12501">http://arxiv.org/abs/2308.12501</a></li>
<li>Code URL: https://github.com/shiyin-lc/dd-gcn</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12501]] DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition(http://arxiv.org/abs/2308.12501)</code></li>
<li>Summary: <p>Graph Convolutional Networks (GCNs) have been widely used in skeleton-based
human action recognition. In GCN-based methods, the spatio-temporal graph is
fundamental for capturing motion patterns. However, existing approaches ignore
the physical dependency and synchronized spatio-temporal correlations between
joints, which limits the representation capability of GCNs. To solve these
problems, we construct the directed diffusion graph for action modeling and
introduce the activity partition strategy to optimize the weight sharing
mechanism of graph convolution kernels. In addition, we present the
spatio-temporal synchronization encoder to embed synchronized spatio-temporal
semantics. Finally, we propose Directed Diffusion Graph Convolutional Network
(DD-GCN) for action recognition, and the experiments on three public datasets:
NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art
performance of our method.
</p></li>
</ul>

<h3>Title: APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency. (arXiv:2308.12605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12605">http://arxiv.org/abs/2308.12605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12605]] APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency(http://arxiv.org/abs/2308.12605)</code></li>
<li>Summary: <p>Diffusion models have exhibited promising progress in video generation.
However, they often struggle to retain consistent details within local regions
across frames. One underlying cause is that traditional diffusion models
approximate Gaussian noise distribution by utilizing predictive noise, without
fully accounting for the impact of inherent information within the input
itself. Additionally, these models emphasize the distinction between
predictions and references, neglecting information intrinsic to the videos. To
address this limitation, inspired by the self-attention mechanism, we propose a
novel text-to-video (T2V) generation network structure based on diffusion
models, dubbed Additional Perturbation for Latent noise with Adversarial
training (APLA). Our approach only necessitates a single video as input and
builds upon pre-trained stable diffusion networks. Notably, we introduce an
additional compact network, known as the Video Generation Transformer (VGT).
This auxiliary component is designed to extract perturbations from the inherent
information contained within the input, thereby refining inconsistent pixels
during temporal predictions. We leverage a hybrid architecture of transformers
and convolutions to compensate for temporal intricacies, enhancing consistency
between different frames within the video. Experiments demonstrate a noticeable
improvement in the consistency of the generated videos both qualitatively and
quantitatively.
</p></li>
</ul>

<h3>Title: Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12964">http://arxiv.org/abs/2308.12964</a></li>
<li>Code URL: https://github.com/naver-ai/densediffusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12964]] Dense Text-to-Image Generation with Attention Modulation(http://arxiv.org/abs/2308.12964)</code></li>
<li>Summary: <p>Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Self-Supervised Learning for Endoscopic Video Analysis. (arXiv:2308.12394v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12394">http://arxiv.org/abs/2308.12394</a></li>
<li>Code URL: https://github.com/royhirsch/endossl</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12394]] Self-Supervised Learning for Endoscopic Video Analysis(http://arxiv.org/abs/2308.12394)</code></li>
<li>Summary: <p>Self-supervised learning (SSL) has led to important breakthroughs in computer
vision by allowing learning from large amounts of unlabeled data. As such, it
might have a pivotal role to play in biomedicine where annotating data requires
a highly specialized expertise. Yet, there are many healthcare domains for
which SSL has not been extensively explored. One such domain is endoscopy,
minimally invasive procedures which are commonly used to detect and treat
infections, chronic inflammatory diseases or cancer. In this work, we study the
use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for
endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit
the power of SSL, we create sizable unlabeled endoscopic video datasets for
training MSNs. These strong image representations serve as a foundation for
secondary training with limited annotated datasets, resulting in
state-of-the-art performance in endoscopic benchmarks like surgical phase
recognition during laparoscopy and colonoscopic polyp characterization.
Additionally, we achieve a 50% reduction in annotated data size without
sacrificing performance. Thus, our work provides evidence that SSL can
dramatically reduce the need of annotated data in endoscopy.
</p></li>
</ul>

<h3>Title: MOFO: MOtion FOcused Self-Supervision for Video Understanding. (arXiv:2308.12447v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12447">http://arxiv.org/abs/2308.12447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12447]] MOFO: MOtion FOcused Self-Supervision for Video Understanding(http://arxiv.org/abs/2308.12447)</code></li>
<li>Summary: <p>Self-supervised learning (SSL) techniques have recently produced outstanding
results in learning visual representations from unlabeled videos. Despite the
importance of motion in supervised learning techniques for action recognition,
SSL methods often do not explicitly consider motion information in videos. To
address this issue, we propose MOFO (MOtion FOcused), a novel SSL method for
focusing representation learning on the motion area of a video, for action
recognition. MOFO automatically detects motion areas in videos and uses these
to guide the self-supervision task. We use a masked autoencoder which randomly
masks out a high proportion of the input sequence; we force a specified
percentage of the inside of the motion area to be masked and the remainder from
outside. We further incorporate motion information into the finetuning step to
emphasise motion in the downstream task. We demonstrate that our motion-focused
innovations can significantly boost the performance of the currently leading
SSL method (VideoMAE) for action recognition. Our method improves the recent
self-supervised Vision Transformer (ViT), VideoMAE, by achieving +2.6%, +2.1%,
+1.3% accuracy on Epic-Kitchens verb, noun and action classification,
respectively, and +4.7% accuracy on Something-Something V2 action
classification. Our proposed approach significantly improves the performance of
the current SSL method for action recognition, indicating the importance of
explicitly encoding motion in SSL.
</p></li>
</ul>

<h3>Title: Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects. (arXiv:2308.12590v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12590">http://arxiv.org/abs/2308.12590</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12590]] Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects(http://arxiv.org/abs/2308.12590)</code></li>
<li>Summary: <p>Learning 3D shape representation with dense correspondence for deformable
objects is a fundamental problem in computer vision. Existing approaches often
need additional annotations of specific semantic domain, e.g., skeleton poses
for human bodies or animals, which require extra annotation effort and suffer
from error accumulation, and they are limited to specific domain. In this
paper, we propose a novel self-supervised approach to learn neural implicit
shape representation for deformable objects, which can represent shapes with a
template shape and dense correspondence in 3D. Our method does not require the
priors of skeleton and skinning weight, and only requires a collection of
shapes represented in signed distance fields. To handle the large deformation,
we constrain the learned template shape in the same latent space with the
training shapes, design a new formulation of local rigid constraint that
enforces rigid transformation in local region and addresses local reflection
issue, and present a new hierarchical rigid constraint to reduce the ambiguity
due to the joint learning of template shape and correspondences. Extensive
experiments show that our model can represent shapes with large deformations.
We also show that our shape representation can support two typical
applications, such as texture transfer and shape editing, with competitive
performance. The code and models are available at
https://iscas3dv.github.io/deformshape
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Overcoming General Knowledge Loss with Selective Parameter Finetuning. (arXiv:2308.12462v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12462">http://arxiv.org/abs/2308.12462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12462]] Overcoming General Knowledge Loss with Selective Parameter Finetuning(http://arxiv.org/abs/2308.12462)</code></li>
<li>Summary: <p>Foundation models encompass an extensive knowledge base and offer remarkable
transferability. However, this knowledge becomes outdated or insufficient over
time. The challenge lies in updating foundation models to accommodate novel
information while retaining their original ability. In this paper, we present a
novel approach to achieving continual model updates by effecting localized
modifications to a small subset of parameters. Guided by insights gleaned from
prior analyses of foundational models, we first localize a specific layer for
model refinement and then introduce an importance scoring mechanism designed to
update only the most crucial weights. Our method is exhaustively evaluated on
foundational vision-language models, measuring its efficacy in both learning
new information and preserving pre-established knowledge across a diverse
spectrum of continual learning tasks, including Aircraft, Birdsnap CIFAR-100,
CUB, Cars, and GTSRB. The results show that our method improves the existing
continual learning methods by 0.5\% - 10\% on average, and reduces the loss of
pre-trained knowledge from around 5\% to 0.97\%. Comprehensive ablation studies
substantiate our method design, shedding light on the contributions of each
component to controllably learning new knowledge and mitigating the forgetting
of pre-trained knowledge.
</p></li>
</ul>

<h3>Title: Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12950">http://arxiv.org/abs/2308.12950</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12950]] Code Llama: Open Foundation Models for Code(http://arxiv.org/abs/2308.12950)</code></li>
<li>Summary: <p>We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained
on sequences of 16k tokens and show improvements on inputs with up to 100k
tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support
infilling based on surrounding content. Code Llama reaches state-of-the-art
performance among open models on several code benchmarks, with scores of up to
53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python
7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform
every other publicly available model on MultiPL-E. We release Code Llama under
a permissive license that allows for both research and commercial use.
</p></li>
</ul>

<h3>Title: FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12305">http://arxiv.org/abs/2308.12305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12305]] FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning(http://arxiv.org/abs/2308.12305)</code></li>
<li>Summary: <p>Recently, foundation models have exhibited remarkable advancements in
multi-modal learning. These models, equipped with millions (or billions) of
parameters, typically require a substantial amount of data for finetuning.
However, collecting and centralizing training data from diverse sectors becomes
challenging due to distinct privacy regulations. Federated Learning (FL)
emerges as a promising solution, enabling multiple clients to collaboratively
train neural networks without centralizing their local data. To alleviate
client computation burdens and communication overheads, previous works have
adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a
small fraction of the model parameters are optimized and communicated during
federated communications. Nevertheless, most previous works have focused on a
single modality and neglected one common phenomenon, i.e., the presence of data
heterogeneity across the clients. Therefore, in this work, we propose a
finetuning framework tailored to heterogeneous multi-modal FL, called Federated
Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a
Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the
client local updates and applying Mutual Knowledge Distillation (MKD) for an
efficient knowledge transfer. FedDAT is the first approach that enables an
efficient distributed finetuning of foundation models for a variety of
heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we
conduct extensive experiments on four multi-modality FL benchmarks with
different types of data heterogeneity, where FedDAT substantially outperforms
the existing centralized PEFT methods adapted for FL.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Continual Zero-Shot Learning through Semantically Guided Generative Random Walks. (arXiv:2308.12366v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12366">http://arxiv.org/abs/2308.12366</a></li>
<li>Code URL: https://github.com/wx-zhang/igczsl</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12366]] Continual Zero-Shot Learning through Semantically Guided Generative Random Walks(http://arxiv.org/abs/2308.12366)</code></li>
<li>Summary: <p>Learning novel concepts, remembering previous knowledge, and adapting it to
future tasks occur simultaneously throughout a human's lifetime. To model such
comprehensive abilities, continual zero-shot learning (CZSL) has recently been
introduced. However, most existing methods overused unseen semantic information
that may not be continually accessible in realistic settings. In this paper, we
address the challenge of continual zero-shot learning where unseen information
is not provided during training, by leveraging generative modeling. The heart
of the generative-based methods is to learn quality representations from seen
classes to improve the generative understanding of the unseen visual space.
Motivated by this, we introduce generalization-bound tools and provide the
first theoretical explanation for the benefits of generative modeling to CZSL
tasks. Guided by the theoretical analysis, we then propose our learning
algorithm that employs a novel semantically guided Generative Random Walk (GRW)
loss. The GRW loss augments the training by continually encouraging the model
to generate realistic and characterized samples to represent the unseen space.
Our algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUN
datasets, surpassing existing CZSL methods by 3-7\%. The code has been made
available here \url{https://github.com/wx-zhang/IGCZSL}
</p></li>
</ul>

<h3>Title: FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features. (arXiv:2308.12380v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12380">http://arxiv.org/abs/2308.12380</a></li>
<li>Code URL: https://github.com/ihp-lab/fg-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12380]] FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features(http://arxiv.org/abs/2308.12380)</code></li>
<li>Summary: <p>Automatic detection of facial Action Units (AUs) allows for objective facial
expression analysis. Due to the high cost of AU labeling and the limited size
of existing benchmarks, previous AU detection methods tend to overfit the
dataset, resulting in a significant performance loss when evaluated across
corpora. To address this problem, we propose FG-Net for generalizable facial
action unit detection. Specifically, FG-Net extracts feature maps from a
StyleGAN2 model pre-trained on a large and diverse face image dataset. Then,
these features are used to detect AUs with a Pyramid CNN Interpreter, making
the training efficient and capturing essential local features. The proposed
FG-Net achieves a strong generalization ability for heatmap-based AU detection
thanks to the generalizable and semantic-rich features extracted from the
pre-trained generative model. Extensive experiments are conducted to evaluate
within- and cross-corpus AU detection with the widely-used DISFA and BP4D
datasets. Compared with the state-of-the-art, the proposed method achieves
superior cross-domain performance while maintaining competitive within-domain
performance. In addition, FG-Net is data-efficient and achieves competitive
performance even when trained on 1000 samples. Our code will be released at
\url{https://github.com/ihp-lab/FG-Net}
</p></li>
</ul>

<h3>Title: MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models. (arXiv:2308.12963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12963">http://arxiv.org/abs/2308.12963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12963]] MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models(http://arxiv.org/abs/2308.12963)</code></li>
<li>Summary: <p>Despite tremendous advancements in bird's-eye view (BEV) perception, existing
models fall short in generating realistic and coherent semantic map layouts,
and they fail to account for uncertainties arising from partial sensor
information (such as occlusion or limited coverage). In this work, we introduce
MapPrior, a novel BEV perception framework that combines a traditional
discriminative BEV perception model with a learned generative model for
semantic map layouts. Our MapPrior delivers predictions with better accuracy,
realism, and uncertainty awareness. We evaluate our model on the large-scale
nuScenes benchmark. At the time of submission, MapPrior outperforms the
strongest competing method, with significantly improved MMD and ECE scores in
camera- and LiDAR-based BEV perception.
</p></li>
</ul>

<h3>Title: Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities. (arXiv:2308.12833v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12833">http://arxiv.org/abs/2308.12833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12833]] Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities(http://arxiv.org/abs/2308.12833)</code></li>
<li>Summary: <p>Spurred by the recent rapid increase in the development and distribution of
large language models (LLMs) across industry and academia, much recent work has
drawn attention to safety- and security-related threats and vulnerabilities of
LLMs, including in the context of potentially criminal activities.
Specifically, it has been shown that LLMs can be misused for fraud,
impersonation, and the generation of malware; while other authors have
considered the more general problem of AI alignment. It is important that
developers and practitioners alike are aware of security-related problems with
such models. In this paper, we provide an overview of existing - predominantly
scientific - efforts on identifying and mitigating threats and vulnerabilities
arising from LLMs. We present a taxonomy describing the relationship between
threats caused by the generative capabilities of LLMs, prevention measures
intended to address such threats, and vulnerabilities arising from imperfect
prevention measures. With our work, we hope to raise awareness of the
limitations of LLMs in light of such security concerns, among both experienced
developers and novel users of such technologies.
</p></li>
</ul>

<h3>Title: Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12890">http://arxiv.org/abs/2308.12890</a></li>
<li>Code URL: https://github.com/oniani/llms-vote</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12890]] Large Language Models Vote: Prompting for Rare Disease Identification(http://arxiv.org/abs/2308.12890)</code></li>
<li>Summary: <p>The emergence of generative Large Language Models (LLMs) emphasizes the need
for accurate and efficient prompting approaches. LLMs are often applied in
Few-Shot Learning (FSL) contexts, where tasks are executed with minimal
training data. FSL has become popular in many Artificial Intelligence (AI)
subdomains, including AI for health. Rare diseases, affecting a small fraction
of the population, inherently require FSL techniques due to limited data
availability, though manual data collection and annotation is costly and
time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a
flexible prompting approach for improving the performance of LLM queries in FSL
settings. MVP works by prompting numerous LLMs to perform the same tasks and
then conducting a majority vote on the resulting outputs. This method achieves
improved results to any one model in the ensemble on one-shot rare disease
identification and classification tasks. We also release a novel rare disease
dataset for FSL, available to those who agreed to the MIMIC-IV Data Use
Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple
times, substantially increasing the time needed for manual annotation, and to
address this, we assess the feasibility of using JSON for automating generative
LLM evaluation.
</p></li>
</ul>

<h3>Title: PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning. (arXiv:2308.12454v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12454">http://arxiv.org/abs/2308.12454</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12454]] PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning(http://arxiv.org/abs/2308.12454)</code></li>
<li>Summary: <p>Recent advances of generative learning models are accompanied by the growing
interest in federated learning (FL) based on generative adversarial network
(GAN) models. In the context of FL, GAN can capture the underlying client data
structure, and regenerate samples resembling the original data distribution
without compromising the private raw data. Although most existing GAN-based FL
works focus on training a global model, Personalized FL (PFL) sometimes can be
more effective in view of client data heterogeneity in terms of distinct data
sample distributions, feature spaces, and labels. To cope with client
heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation
strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in
different scenarios. More specially, we first learn the similarity among
clients and then develop an weighted collaborative data aggregation. The
empirical results through the rigorous experimentation on several well-known
datasets demonstrate the effectiveness of PFL-GAN.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: REB: Reducing Biases in Representation for Industrial Anomaly Detection. (arXiv:2308.12577v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12577">http://arxiv.org/abs/2308.12577</a></li>
<li>Code URL: https://github.com/shuailyu/reb</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12577]] REB: Reducing Biases in Representation for Industrial Anomaly Detection(http://arxiv.org/abs/2308.12577)</code></li>
<li>Summary: <p>Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct
industrial anomaly detection in two stages: obtain feature representations with
a pre-trained CNN model and perform distance measures for defect detection.
However, the features are not fully exploited as they ignore domain bias and
the difference of local density in feature space, which limits the detection
performance. In this paper, we propose Reducing Biases (REB) in representation
by considering the domain bias of the pre-trained model and building a
self-supervised learning task for better domain adaption with a defect
generation strategy (DefectMaker) imitating the natural defects. Additionally,
we propose a local density KNN (LDKNN) to reduce the local density bias and
obtain effective anomaly detection. We achieve a promising result of 99.5\%
AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\% AUROC on
the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\% AUROC
to the state-of-the-art result. All results are obtained with smaller backbone
networks such as Vgg11 and Resnet18, which indicates the effectiveness and
efficiency of REB for practical industrial applications.
</p></li>
</ul>

<h3>Title: Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12563">http://arxiv.org/abs/2308.12563</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12563]] Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals(http://arxiv.org/abs/2308.12563)</code></li>
<li>Summary: <p>Mainstream unsupervised anomaly detection algorithms often excel in academic
datasets, yet their real-world performance is restricted due to the controlled
experimental conditions involving clean training data. Addressing the challenge
of training with noise, a prevalent issue in practical anomaly detection, is
frequently overlooked. In a pioneering endeavor, this study delves into the
realm of label-level noise within sensory time-series anomaly detection (TSAD).
This paper presents a novel and practical end-to-end unsupervised TSAD when the
training data are contaminated with anomalies. The introduced approach, called
TSAD-C, is devoid of access to abnormality labels during the training phase.
TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities
(aka noise) present in the training data, a Variable Dependency Modeling module
to capture both long-term intra- and inter-variable dependencies within the
decontaminated data that can be considered as a surrogate of the pure normal
data, and an Anomaly Scoring module to detect anomalies. Our extensive
experiments conducted on three widely used physiological datasets conclusively
demonstrate that our approach surpasses existing methodologies, thus
establishing a new state-of-the-art performance in the field.
</p></li>
</ul>

<h3>Title: Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection. (arXiv:2308.12612v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12612">http://arxiv.org/abs/2308.12612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12612]] Try with Simpler -- An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection(http://arxiv.org/abs/2308.12612)</code></li>
<li>Summary: <p>The rapid growth of deep learning (DL) has spurred interest in enhancing
log-based anomaly detection. This approach aims to extract meaning from log
events (log message templates) and develop advanced DL models for anomaly
detection. However, these DL methods face challenges like heavy reliance on
training data, labels, and computational resources due to model complexity. In
contrast, traditional machine learning and data mining techniques are less
data-dependent and more efficient but less effective than DL. To make log-based
anomaly detection more practical, the goal is to enhance traditional techniques
to match DL's effectiveness. Previous research in a different domain (linking
questions on Stack Overflow) suggests that optimized traditional techniques can
rival state-of-the-art DL methods. Drawing inspiration from this concept, we
conducted an empirical study. We optimized the unsupervised PCA (Principal
Component Analysis), a traditional technique, by incorporating lightweight
semantic-based log representation. This addresses the issue of unseen log
events in training data, enhancing log representation. Our study compared seven
log-based anomaly detection methods, including four DL-based, two traditional,
and the optimized PCA technique, using public and industrial datasets. Results
indicate that the optimized unsupervised PCA technique achieves similar
effectiveness to advanced supervised/semi-supervised DL methods while being
more stable with limited training data and resource-efficient. This
demonstrates the adaptability and strength of traditional techniques through
small yet impactful adaptations.
</p></li>
</ul>

<h3>Title: Low-count Time Series Anomaly Detection. (arXiv:2308.12925v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12925">http://arxiv.org/abs/2308.12925</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12925]] Low-count Time Series Anomaly Detection(http://arxiv.org/abs/2308.12925)</code></li>
<li>Summary: <p>Low-count time series describe sparse or intermittent events, which are
prevalent in large-scale online platforms that capture and monitor diverse data
types. Several distinct challenges surface when modelling low-count time
series, particularly low signal-to-noise ratios (when anomaly signatures are
provably undetectable), and non-uniform performance (when average metrics are
not representative of local behaviour). The time series anomaly detection
community currently lacks explicit tooling and processes to model and reliably
detect anomalies in these settings. We address this gap by introducing a novel
generative procedure for creating benchmark datasets comprising of low-count
time series with anomalous segments. Via a mixture of theoretical and empirical
analysis, our work explains how widely-used algorithms struggle with the
distribution overlap between normal and anomalous segments. In order to
mitigate this shortcoming, we then leverage our findings to demonstrate how
anomaly score smoothing consistently improves performance. The practical
utility of our analysis and recommendation is validated on a real-world dataset
containing sales data for retail stores.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Large Language Model as Autonomous Decision Maker. (arXiv:2308.12519v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12519">http://arxiv.org/abs/2308.12519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12519]] Large Language Model as Autonomous Decision Maker(http://arxiv.org/abs/2308.12519)</code></li>
<li>Summary: <p>While large language models (LLMs) exhibit impressive language understanding
and in-context learning abilities, their decision-making ability still heavily
relies on the guidance of task-specific expert knowledge when solving
real-world tasks. To unleash the potential of LLMs as autonomous decision
makers, this paper presents an approach JuDec to endow LLMs with the
self-judgment ability, enabling LLMs to achieve autonomous judgment and
exploration for decision making. Specifically, in JuDec, Elo-based
Self-Judgment Mechanism is designed to assign Elo scores to decision steps to
judge their values and utilities via pairwise comparisons between two solutions
and then guide the decision-searching process toward the optimal solution
accordingly. Experimental results on the ToolBench dataset demonstrate JuDec's
superiority over baselines, achieving over 10% improvement in Pass Rate on
diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT
API calls), highlighting its effectiveness and efficiency.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
