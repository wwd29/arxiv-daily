<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-09</h1>
<h3>Title: User to Video: A Model for Spammer Detection Inspired by Video Classification Technology</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Zhang, Zhou Yang, Yucai Pang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06233">https://arxiv.org/abs/2510.06233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06233">https://arxiv.org/pdf/2510.06233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06233]] User to Video: A Model for Spammer Detection Inspired by Video Classification Technology(https://arxiv.org/abs/2510.06233)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This article is inspired by video classification technology. If the user behavior subspace is viewed as a frame image, consecutive frame images are viewed as a video. Following this novel idea, a model for spammer detection based on user videoization, called UVSD, is proposed. Firstly, a user2piexl algorithm for user pixelization is proposed. Considering the adversarial behavior of user stances, the user is viewed as a pixel, and the stance is quantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed for transforming user behavior subspace into frame images. Low-rank dense vectorization of subspace user relations is performed using representation learning, while cutting and diffusion algorithms are introduced to complete the frame imageization. Finally, user behavior videos are constructed based on temporal features. Subsequently, a video classification algorithm is combined to identify the spammers. Experiments using publicly available datasets, i.e., WEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art methods.</li>
</ul>

<h3>Title: Evaluating Embedding Frameworks for Scientific Domain</h3>
<ul>
<li><strong>Authors: </strong>Nouman Ahmed, Ronin Wu, Victor Botev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06244">https://arxiv.org/abs/2510.06244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06244">https://arxiv.org/pdf/2510.06244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06244]] Evaluating Embedding Frameworks for Scientific Domain(https://arxiv.org/abs/2510.06244)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Finding an optimal word representation algorithm is particularly important in terms of domain specific data, as the same word can have different meanings and hence, different representations depending on the domain and context. While Generative AI and transformer architecture does a great job at generating contextualized embeddings for any given work, they are quite time and compute extensive, especially if we were to pre-train such a model from scratch. In this work, we focus on the scientific domain and finding the optimal word representation algorithm along with the tokenization method that could be used to represent words in the scientific domain. The goal of this research is two fold: 1) finding the optimal word representation and tokenization methods that can be used in downstream scientific domain NLP tasks, and 2) building a comprehensive evaluation suite that could be used to evaluate various word representation and tokenization algorithms (even as new ones are introduced) in the scientific domain. To this end, we build an evaluation suite consisting of several downstream tasks and relevant datasets for each task. Furthermore, we use the constructed evaluation suite to test various word representation and tokenization algorithms.</li>
</ul>

<h3>Title: RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases</h3>
<ul>
<li><strong>Authors: </strong>Khartik Uppalapati, Shakeel Abdulkareem, Bora Yimenicioglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06267">https://arxiv.org/abs/2510.06267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06267">https://arxiv.org/pdf/2510.06267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06267]] RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases(https://arxiv.org/abs/2510.06267)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion framework that generates realistic yet privacy-preserving synthetic electronic-health-record (EHR) trajectories for ultra-rare diseases. RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph comprising approximately 8 M typed edges. Meta-path scores extracted from this 8-million-edge KG modulate the per-token noise schedule in the forward stochastic differential equation, steering generation toward biologically plausible lab-medication-adverse-event co-occurrences while retaining score-based diffusion model stability. The reverse denoiser then produces timestamped sequences of lab-code, medication-code, and adverse-event-flag triples that contain no protected health information. On simulated ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean Discrepancy by 40 percent relative to an unguided diffusion baseline and by greater than 60 percent versus GAN counterparts, without sacrificing downstream predictive utility. A black-box membership-inference evaluation using the DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55 safe-release threshold and substantially better than the approximately 0.61 plus or minus 0.03 observed for non-KG baselines, demonstrating strong resistance to re-identification. These results suggest that integrating biomedical knowledge graphs directly into diffusion noise schedules can simultaneously enhance fidelity and privacy, enabling safer data sharing for rare-disease research.</li>
</ul>

<h3>Title: Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhiyang Zhang, Ningcong Chen, Xin Zhang, Yanhua Li, Shen Su, Hui Lu, Jun Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06291">https://arxiv.org/abs/2510.06291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06291">https://arxiv.org/pdf/2510.06291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06291]] Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation(https://arxiv.org/abs/2510.06291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The widespread use of GPS devices has driven advances in spatiotemporal data mining, enabling machine learning models to simulate human decision making and generate realistic trajectories, addressing both data collection costs and privacy concerns. Recent studies have shown the promise of diffusion models for high-quality trajectory generation. However, most existing methods rely on convolution based architectures (e.g. UNet) to predict noise during the diffusion process, which often results in notable deviations and the loss of fine-grained street-level details due to limited model capacity. In this paper, we propose Trajectory Transformer, a novel model that employs a transformer backbone for both conditional information embedding and noise prediction. We explore two GPS coordinate embedding strategies, location embedding and longitude-latitude embedding, and analyze model performance at different scales. Experiments on two real-world datasets demonstrate that Trajectory Transformer significantly enhances generation quality and effectively alleviates the deviation issues observed in prior approaches.</li>
</ul>

<h3>Title: BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression</h3>
<ul>
<li><strong>Authors: </strong>Cristian Meo, Varun Sarathchandran, Avijit Majhi, Shao Hung, Carlo Saccardi, Ruben Imhoff, Roberto Deidda, Remko Uijlenhoet, Justin Dauwels</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06293">https://arxiv.org/abs/2510.06293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06293">https://arxiv.org/pdf/2510.06293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06293]] BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression(https://arxiv.org/abs/2510.06293)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Predicting precipitation maps is a highly complex spatiotemporal modeling task, critical for mitigating the impacts of extreme weather events. Short-term precipitation forecasting, or nowcasting, requires models that are not only accurate but also computationally efficient for real-time applications. Current methods, such as token-based autoregressive models, often suffer from flawed inductive biases and slow inference, while diffusion models can be computationally intensive. To address these limitations, we introduce BlockGPT, a generative autoregressive transformer using batched tokenization (Block) method that predicts full two-dimensional fields (frames) at each time step. Conceived as a model-agnostic paradigm for video prediction, BlockGPT factorizes space-time by using self-attention within each frame and causal attention across frames; in this work, we instantiate it for precipitation nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI (Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet) models. The results show that BlockGPT achieves superior accuracy, event localization as measured by categorical metrics, and inference speeds up to 31x faster than comparable baselines.</li>
</ul>

<h3>Title: Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling</h3>
<ul>
<li><strong>Authors: </strong>Young D. Kwon, Abhinav Mehrotra, Malcolm Chadwick, Alberto Gil Ramos, Sourav Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06295">https://arxiv.org/abs/2510.06295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06295">https://arxiv.org/pdf/2510.06295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06295]] Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling(https://arxiv.org/abs/2510.06295)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>High-resolution (4K) image-to-image synthesis has become increasingly important for mobile applications. Existing diffusion models for image editing face significant challenges, in terms of memory and image quality, when deployed on resource-constrained devices. In this paper, we present MobilePicasso, a novel system that enables efficient image editing at high resolutions, while minimising computational cost and memory usage. MobilePicasso comprises three stages: (i) performing image editing at a standard resolution with hallucination-aware loss, (ii) applying latent projection to overcome going to the pixel space, and (iii) upscaling the edited image latent to a higher resolution with adaptive context-preserving tiling. Our user study with 46 participants reveals that MobilePicasso not only improves image quality by 18-48% but reduces hallucinations by 14-51% over existing methods. MobilePicasso demonstrates significantly lower latency, e.g., up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g., a mere 9% increase over prior work. Surprisingly, the on-device runtime of MobilePicasso is observed to be faster than a server-based high-resolution image editing model running on an A100 GPU.</li>
</ul>

<h3>Title: RGBD Gaze Tracking Using Transformer for Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Tobias J. Bauer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06298">https://arxiv.org/abs/2510.06298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06298">https://arxiv.org/pdf/2510.06298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06298]] RGBD Gaze Tracking Using Transformer for Feature Fusion(https://arxiv.org/abs/2510.06298)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Subject of this thesis is the implementation of an AI-based Gaze Tracking system using RGBD images that contain both color (RGB) and depth (D) information. To fuse the features extracted from the images, a module based on the Transformer architecture is used. The combination of RGBD input images and Transformers was chosen because it has not yet been investigated. Furthermore, a new dataset is created for training the AI models as existing datasets either do not contain depth information or only contain labels for Gaze Point Estimation that are not suitable for the task of Gaze Angle Estimation. Various model configurations are trained, validated and evaluated on a total of three different datasets. The trained models are then to be used in a real-time pipeline to estimate the gaze direction and thus the gaze point of a person in front of a computer screen. The AI model architecture used in this thesis is based on an earlier work by Lian et al. It uses a Generative Adversarial Network (GAN) to simultaneously remove depth map artifacts and extract head pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm on the same dataset, but we show that using no pre-trained GAN module leads to a mean Euclidean error of 30.1mm. Replacing the Transformer module with a Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the model with Transformer module achieves a mean angular error of 3.59° and without Transformer module 3.26°, whereas the fundamentally different model architecture used by the dataset authors Zhang et al. achieves a mean angular error of 2.04°. On the OTH-Gaze-Estimation dataset created for...</li>
</ul>

<h3>Title: SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation</h3>
<ul>
<li><strong>Authors: </strong>Shuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06303">https://arxiv.org/abs/2510.06303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06303">https://arxiv.org/pdf/2510.06303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06303]] SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation(https://arxiv.org/abs/2510.06303)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies the training efficiency of autoregressive models with the parallel inference capability of diffusion. Instead of costly end-to-end diffusion training, SDAR performs a lightweight paradigm conversion that transforms a well-trained autoregressive (AR) model into a blockwise diffusion model through brief, data-efficient adaptation. During inference, SDAR generates sequences autoregressively across blocks for global coherence while decoding all tokens within each block in parallel via a discrete diffusion process. Extensive experiments show that AR models remain substantially more compute-efficient than masked diffusion models, providing a strong foundation for adaptation. Building on this insight, SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Scaling studies across dense and Mixture-of-Experts architectures confirm that SDAR scales without compromise: larger models exhibit stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning and domain adaptability. Our 30B MoE model surpasses its AR counterpart on challenging scientific reasoning benchmarks such as GPQA and ChemBench, and gains further improvements under test-time scaling methods like majority voting and pass@k. Together, these results establish SDAR as a practical paradigm that combines the strengths of autoregression and diffusion for scalable, high-throughput reasoning.</li>
</ul>

<h3>Title: Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, Jinbin Bai, Qian Yu, Dengyang Jiang, Yuandong Pu, Haoxing Chen, Le Zhuo, Junjun He, Gen Luo, Tianbin Li, Ming Hu, Jin Ye, Shenglong Ye, Bo Zhang, Chang Xu, Wenhai Wang, Hongsheng Li, Guangtao Zhai, Tianfan Xue, Bin Fu, Xiaohong Liu, Yu Qiao, Yihao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06308">https://arxiv.org/abs/2510.06308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06308">https://arxiv.org/pdf/2510.06308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06308]] Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding(https://arxiv.org/abs/2510.06308)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: this https URL.</li>
</ul>

<h3>Title: Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks</h3>
<ul>
<li><strong>Authors: </strong>Moein E. Samadi, Andreas Schuppert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06349">https://arxiv.org/abs/2510.06349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06349">https://arxiv.org/pdf/2510.06349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06349]] Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks(https://arxiv.org/abs/2510.06349)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have rapidly advanced AI, raising the question of whether their decisions will ultimately surpass human strategies in real-world domains. The exponential, and possibly super-exponential, pace of AI development makes such analysis elusive. Nevertheless, many application areas that matter for daily life and society show only modest gains so far; a prominent case is diagnosing and treating dynamically evolving disease in intensive care. The common challenge is adapting complex systems to dynamic environments. Effective strategies must optimize outcomes in systems composed of strongly interacting functions while avoiding shared side effects; this requires reliable, self-adaptive modeling. These tasks align with building digital twins of highly complex systems whose mechanisms are not fully or quantitatively understood. It is therefore essential to develop methods for self-adapting AI models with minimal data and limited mechanistic knowledge. As this challenge extends beyond medicine, AI should demonstrate clear superiority in these settings before assuming broader decision-making roles. We identify the curse of dimensionality as a fundamental barrier to efficient self-adaptation and argue that monolithic foundation models face conceptual limits in overcoming it. As an alternative, we propose a decentralized architecture of interacting small agent networks (SANs). We focus on agents representing the specialized substructure of the system, where each agent covers only a subset of the full system functions. Drawing on mathematical results on the learning behavior of SANs and evidence from existing applications, we argue that swarm-learning in diverse swarms can enable self-adaptive SANs to deliver superior decision-making in dynamic environments compared with monolithic foundation models, though at the cost of reduced reproducibility in detail.</li>
</ul>

<h3>Title: TransFIRA: Transfer Learning for Face Image Recognizability Assessment</h3>
<ul>
<li><strong>Authors: </strong>Allen Tu, Kartik Narayan, Joshua Gleason, Jennifer Xu, Matthew Meyn, Tom Goldstein, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06353">https://arxiv.org/abs/2510.06353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06353">https://arxiv.org/pdf/2510.06353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06353]] TransFIRA: Transfer Learning for Face Image Recognizability Assessment(https://arxiv.org/abs/2510.06353)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face recognition in unconstrained environments such as surveillance, video, and web imagery must contend with extreme variation in pose, blur, illumination, and occlusion, where conventional visual quality metrics fail to predict whether inputs are truly recognizable to the deployed encoder. Existing FIQA methods typically rely on visual heuristics, curated annotations, or computationally intensive generative pipelines, leaving their predictions detached from the encoder's decision geometry. We introduce TransFIRA (Transfer Learning for Face Image Recognizability Assessment), a lightweight and annotation-free framework that grounds recognizability directly in embedding space. TransFIRA delivers three advances: (i) a definition of recognizability via class-center similarity (CCS) and class-center angular separation (CCAS), yielding the first natural, decision-boundary--aligned criterion for filtering and weighting; (ii) a recognizability-informed aggregation strategy that achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly doubling correlation with true recognizability, all without external labels, heuristics, or backbone-specific training; and (iii) new extensions beyond faces, including encoder-grounded explainability that reveals how degradations and subject-specific factors affect recognizability, and the first recognizability-aware body recognition assessment. Experiments confirm state-of-the-art results on faces, strong performance on body recognition, and robustness under cross-dataset shifts. Together, these contributions establish TransFIRA as a unified, geometry-driven framework for recognizability assessment -- encoder-specific, accurate, interpretable, and extensible across modalities -- significantly advancing FIQA in accuracy, explainability, and scope.</li>
</ul>

<h3>Title: Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data</h3>
<ul>
<li><strong>Authors: </strong>Rishabh Ranjan, Valter Hudovernik, Mark Znidar, Charilaos Kanatsoulis, Roshan Upendra, Mahmoud Mohammadi, Joe Meyer, Tom Palczewski, Carlos Guestrin, Jure Leskovec</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06377">https://arxiv.org/abs/2510.06377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06377">https://arxiv.org/pdf/2510.06377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06377]] Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data(https://arxiv.org/abs/2510.06377)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Pretrained transformers readily adapt to new sequence modeling tasks via zero-shot prompting, but relational domains still lack architectures that transfer across datasets and tasks. The core challenge is the diversity of relational data, with varying heterogeneous schemas, graph structures and functional dependencies. In this paper, we present the Relational Transformer (RT) architecture, which can be pretrained on diverse relational databases and directly applied to unseen datasets and tasks without task- or dataset-specific fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with table/column metadata, (ii) is pretrained via masked token prediction, and (iii) utilizes a novel \textit{Relational Attention} mechanism over columns, rows, and primary-foreign key links. Pretrained on RelBench datasets spanning tasks such as churn and sales forecasting, RT attains strong zero-shot performance, averaging 94% of fully supervised AUROC on binary classification tasks with a single forward pass of a 22M parameter model, as opposed to 84% for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample efficiency. Our experiments show that RT's zero-shot transfer harnesses task-table context, relational attention patterns and schema semantics. Overall, RT provides a practical path toward foundation models for relational data.</li>
</ul>

<h3>Title: Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Fan Zhou, Chang Tian, Tim Van de Cruys</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06386">https://arxiv.org/abs/2510.06386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06386">https://arxiv.org/pdf/2510.06386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06386]] Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion(https://arxiv.org/abs/2510.06386)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating stylistic text with specific attributes is a key problem in controllable text generation. Recently, diffusion models have emerged as a powerful paradigm for both visual and textual generation. Existing approaches can be broadly categorized into classifier-free guidance (CFG) and classifier guidance (CG) methods. While CFG effectively preserves semantic content, it often fails to provide effective attribute control. In contrast, CG modifies the denoising trajectory using classifier gradients, enabling better attribute alignment but incurring high computational costs during sampling and suffering from classifier generalization issues. In this work, we propose RegDiff, a regularized diffusion framework that leverages attribute features without requiring a pretrained classifier during sampling, thereby achieving controllable generation with reduced computational costs. Specifically, RegDiff employs a VAE-based encoder--decoder architecture to ensure reconstruction fidelity and a latent diffusion model trained with attribute supervision to enable controllable text generation. Attribute information is injected only during training. Experiments on five datasets spanning multiple stylistic attributes demonstrate that RegDiff outperforms strong baselines in generating stylistic texts. These results validate the effectiveness of RegDiff as an efficient solution for attribute-controllable text diffusion. Our code, datasets, and resources will be released upon publication at this https URL.</li>
</ul>

<h3>Title: Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Ali Baheri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06397">https://arxiv.org/abs/2510.06397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06397">https://arxiv.org/pdf/2510.06397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06397]] Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings(https://arxiv.org/abs/2510.06397)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Non-Euclidean foundation models increasingly place representations in curved spaces such as hyperbolic geometry. We show that this geometry creates a boundary-driven asymmetry that backdoor triggers can exploit. Near the boundary, small input changes appear subtle to standard input-space detectors but produce disproportionately large shifts in the model's representation space. Our analysis formalizes this effect and also reveals a limitation for defenses: methods that act by pulling points inward along the radius can suppress such triggers, but only by sacrificing useful model sensitivity in that same direction. Building on these insights, we propose a simple geometry-adaptive trigger and evaluate it across tasks and architectures. Empirically, attack success increases toward the boundary, whereas conventional detectors weaken, mirroring the theoretical trends. Together, these results surface a geometry-specific vulnerability in non-Euclidean models and offer analysis-backed guidance for designing and understanding the limits of defenses.</li>
</ul>

<h3>Title: Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Mert Kayaalp, Caner Turkmen, Oleksandr Shchur, Pedro Mercado, Abdul Fatir Ansari, Michael Bohlke-Schneider, Bernie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06419">https://arxiv.org/abs/2510.06419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06419">https://arxiv.org/pdf/2510.06419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06419]] Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting(https://arxiv.org/abs/2510.06419)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Is bigger always better for time series foundation models? With the question in mind, we explore an alternative to training a single, large monolithic model: building a portfolio of smaller, pretrained forecasting models. By applying ensembling or model selection over these portfolios, we achieve competitive performance on large-scale benchmarks using much fewer parameters. We explore strategies for designing such portfolios and find that collections of specialist models consistently outperform portfolios of independently trained generalists. Remarkably, we demonstrate that post-training a base model is a compute-effective approach for creating sufficiently diverse specialists, and provide evidences that ensembling and model selection are more compute-efficient than test-time fine-tuning.</li>
</ul>

<h3>Title: Breaking Precision Time: OS Vulnerability Exploits Against IEEE 1588</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Abdullah Soomro, Fatima Muhammad Anwar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06421">https://arxiv.org/abs/2510.06421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06421">https://arxiv.org/pdf/2510.06421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06421]] Breaking Precision Time: OS Vulnerability Exploits Against IEEE 1588(https://arxiv.org/abs/2510.06421)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The Precision Time Protocol (PTP), standardized as IEEE 1588, provides sub-microsecond synchronization across distributed systems and underpins critical infrastructure in telecommunications, finance, power systems, and industrial automation. While prior work has extensively analyzed PTP's vulnerability to network-based attacks, prompting the development of cryptographic protections and anomaly detectors, these defenses presume an uncompromised host. In this paper, we identify and exploit a critical blind spot in current threat models: kernel-level adversaries operating from within the host running the PTP stack. We present the first systematic study of kernel-rooted attacks on PTP, demonstrating how privileged attackers can manipulate system time by corrupting key interfaces without altering PTP network traffic. We implement three attack primitives, constant offset, progressive skew, and random jitter, using in-kernel payloads, and evaluate their impact on the widely used ptp4l and phc2sys daemons. Our experiments reveal that these attacks can silently destabilize clock synchronization, bypassing existing PTP security extensions. These findings highlight the urgent need to reconsider host-level trust assumptions and integrate kernel integrity into the design of secure time synchronization systems.</li>
</ul>

<h3>Title: Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization</h3>
<ul>
<li><strong>Authors: </strong>Eliot Shekhtman, Yichen Zhou, Ingvar Ziemann, Nikolai Matni, Stephen Tu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06434">https://arxiv.org/abs/2510.06434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06434">https://arxiv.org/pdf/2510.06434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06434]] Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization(https://arxiv.org/abs/2510.06434)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Learning from temporally-correlated data is a core facet of modern machine learning. Yet our understanding of sequential learning remains incomplete, particularly in the multi-trajectory setting where data consists of many independent realizations of a time-indexed stochastic process. This important regime both reflects modern training pipelines such as for large foundation models, and offers the potential for learning without the typical mixing assumptions made in the single-trajectory case. However, instance-optimal bounds are known only for least-squares regression with dependent covariates; for more general models or loss functions, the only broadly applicable guarantees result from a reduction to either i.i.d. learning, with effective sample size scaling only in the number of trajectories, or an existing single-trajectory result when each individual trajectory mixes, with effective sample size scaling as the full data budget deflated by the mixing-time. In this work, we significantly broaden the scope of instance-optimal rates in multi-trajectory settings via the Hellinger localization framework, a general approach for maximum likelihood estimation. Our method proceeds by first controlling the squared Hellinger distance at the path-measure level via a reduction to i.i.d. learning, followed by localization as a quadratic form in parameter space weighted by the trajectory Fisher information. This yields instance-optimal bounds that scale with the full data budget under a broad set of conditions. We instantiate our framework across four diverse case studies: a simple mixture of Markov chains, dependent linear regression under non-Gaussian noise, generalized linear models with non-monotonic activations, and linear-attention sequence models. In all cases, our bounds nearly match the instance-optimal rates from asymptotic normality, substantially improving over standard reductions.</li>
</ul>

<h3>Title: TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Piyush Dashpute, Niki Nezakati, Wolfgang Heidrich, Vishwanath Saragadam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06460">https://arxiv.org/abs/2510.06460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06460">https://arxiv.org/pdf/2510.06460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06460]] TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion(https://arxiv.org/abs/2510.06460)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Thermal images from low-cost cameras often suffer from low resolution, fixed pattern noise, and other localized degradations. Available datasets for thermal imaging are also limited in both size and diversity. To address these challenges, we propose a patch-based diffusion framework (TDiff) that leverages the local nature of these distortions by training on small thermal patches. In this approach, full-resolution images are restored by denoising overlapping patches and blending them using smooth spatial windowing. To our knowledge, this is the first patch-based diffusion framework that models a learned prior for thermal image restoration across multiple tasks. Experiments on denoising, super-resolution, and deblurring demonstrate strong results on both simulated and real thermal data, establishing our method as a unified restoration pipeline.</li>
</ul>

<h3>Title: Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zhantao Deng, Mériem Er-Rafik, Anna Sushko, Cécile Hébert, Pascal Fua</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06516">https://arxiv.org/abs/2510.06516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06516">https://arxiv.org/pdf/2510.06516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06516]] Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion(https://arxiv.org/abs/2510.06516)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Limited-angle electron tomography aims to reconstruct 3D shapes from 2D projections of Transmission Electron Microscopy (TEM) within a restricted range and number of tilting angles, but it suffers from the missing-wedge problem that causes severe reconstruction artifacts. Deep learning approaches have shown promising results in alleviating these artifacts, yet they typically require large high-quality training datasets with known 3D ground truth which are difficult to obtain in electron microscopy. To address these challenges, we propose TEMDiff, a novel 3D diffusion-based iterative reconstruction framework. Our method is trained on readily available volumetric FIB-SEM data using a simulator that maps them to TEM tilt series, enabling the model to learn realistic structural priors without requiring clean TEM ground truth. By operating directly on 3D volumes, TEMDiff implicitly enforces consistency across slices without the need for additional regularization. On simulated electron tomography datasets with limited angular coverage, TEMDiff outperforms state-of-the-art methods in reconstruction quality. We further demonstrate that a trained TEMDiff model generalizes well to real-world TEM tilts obtained under different conditions and can recover accurate structures from tilt ranges as narrow as 8 degrees, with 2-degree increments, without any retraining or fine-tuning.</li>
</ul>

<h3>Title: Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security</h3>
<ul>
<li><strong>Authors: </strong>Ali Naseh, Anshuman Suri, Yuefeng Peng, Harsh Chaudhari, Alina Oprea, Amir Houmansadr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06525">https://arxiv.org/abs/2510.06525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06525">https://arxiv.org/pdf/2510.06525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06525]] Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security(https://arxiv.org/abs/2510.06525)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI leaderboards are central to evaluating model capabilities, but remain vulnerable to manipulation. Among key adversarial objectives is rank manipulation, where an attacker must first deanonymize the models behind displayed outputs -- a threat previously demonstrated and explored for large language models (LLMs). We show that this problem can be even more severe for text-to-image leaderboards, where deanonymization is markedly easier. Using over 150,000 generated images from 280 prompts and 19 diverse models spanning multiple organizations, architectures, and sizes, we demonstrate that simple real-time classification in CLIP embedding space identifies the generating model with high accuracy, even without prompt control or historical data. We further introduce a prompt-level separability metric and identify prompts that enable near-perfect deanonymization. Our results indicate that rank manipulation in text-to-image leaderboards is easier than previously recognized, underscoring the need for stronger defenses.</li>
</ul>

<h3>Title: VUGEN: Visual Understanding priors for GENeration</h3>
<ul>
<li><strong>Authors: </strong>Xiangyi Chen, Théophane Vallaeys, Maha Elbayad, John Nguyen, Jakob Verbeek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06529">https://arxiv.org/abs/2510.06529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06529">https://arxiv.org/pdf/2510.06529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06529]] VUGEN: Visual Understanding priors for GENeration(https://arxiv.org/abs/2510.06529)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in Vision-Language Models (VLMs) have enabled unified understanding across text and images, yet equipping these models with robust image generation capabilities remains challenging. Existing approaches often rely on reconstruction-oriented autoencoders or complex bridging mechanisms, leading to misalignment between understanding and generation representations, or architectural complexity. In this work, we propose VUGEN, a novel framework that explicitly leverages VLM's pretrained visual understanding priors for efficient and high-quality image generation. Our approach first transforms the high-dimensional latent space of the VLM's native vision encoder into a lower-dimensional, tractable distribution that maximally preserves visual information. The VLM is then trained to sample within this reduced latent space, ensuring alignment with its visual understanding capabilities. Finally, a dedicated pixel decoder maps these generated latents back to the image space. We find that a VAE-free pixel diffusion decoder to be on par or better than commonly used complex latent diffusion decoders that internally rely on VAE latents. Extensive experiments demonstrate that VUGEN achieves superior image generation performance, improving DPG Bench from 71.17 to 74.32 and FID from 11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding capabilities.</li>
</ul>

<h3>Title: From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Thusitha Dayaratne, Ngoc Duy Pham, Viet Vo, Shangqi Lai, Sharif Abuadbba, Hajime Suzuki, Xingliang Yuan, Carsten Rudolph</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06530">https://arxiv.org/abs/2510.06530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06530">https://arxiv.org/pdf/2510.06530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06530]] From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond(https://arxiv.org/abs/2510.06530)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The quality and experience of mobile communication have significantly improved with the introduction of 5G, and these improvements are expected to continue beyond the 5G era. However, vulnerabilities in control-plane protocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS), pose significant security threats, such as Blind Denial of Service (DoS) attacks. Despite the availability of existing anomaly detection methods that leverage rule-based systems or traditional machine learning methods, these methods have several limitations, including the need for extensive training data, predefined rules, and limited explainability. Addressing these challenges, we propose a novel anomaly detection framework that leverages the capabilities of Large Language Models (LLMs) in zero-shot mode with unordered data and short natural language attack descriptions within the Open Radio Access Network (O-RAN) architecture. We analyse robustness to prompt variation, demonstrate the practicality of automating the attack descriptions and show that detection quality relies on the semantic completeness of the description rather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate the solution and provide an extensive comparison of open-source and proprietary LLM implementations to demonstrate superior performance in attack detection. We further validate the practicality of our framework within O-RAN's real-time constraints, illustrating its potential for detecting other Layer-3 attacks.</li>
</ul>

<h3>Title: Incoherence in goal-conditioned autoregressive models</h3>
<ul>
<li><strong>Authors: </strong>Jacek Karwowski, Raymond Douglas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06545">https://arxiv.org/abs/2510.06545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06545">https://arxiv.org/pdf/2510.06545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06545]] Incoherence in goal-conditioned autoregressive models(https://arxiv.org/abs/2510.06545)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate mathematically the notion of incoherence: a structural issue with reinforcement learning policies derived by naive goal-conditioning of autoregressive models. We focus on the process of re-training models on their own actions, that is, fine-tuning offline-learned policies with online RL. We prove that it decreases incoherence and leads to an improvement in return, and we aim to characterize the resulting trajectory of policies. By re-framing standard notions of control-as-inference and soft Q learning, we establish a three-way correspondence with two other ways of understanding the iterative re-training process: as folding the posterior into the reward and, in the deterministic case, as decreasing the temperature parameter; the correspondence has computational content via the training-inference trade-off. Through soft-conditioning generative models, we discuss the link between incoherence and the effective horizon.</li>
</ul>

<h3>Title: Auto-Stega: An Agent-Driven System for Lifelong Strategy Evolution in LLM-Based Text Steganography</h3>
<ul>
<li><strong>Authors: </strong>Jiuan Zhou, Yu Cheng, Yuan Xie, Zhaoxia Yin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06565">https://arxiv.org/abs/2510.06565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06565">https://arxiv.org/pdf/2510.06565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06565]] Auto-Stega: An Agent-Driven System for Lifelong Strategy Evolution in LLM-Based Text Steganography(https://arxiv.org/abs/2510.06565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid progress of LLMs, high quality generative text has become widely available as a cover for text steganography. However, prevailing methods rely on hand-crafted or pre-specified strategies and struggle to balance efficiency, imperceptibility, and security, particularly at high embedding rates. Accordingly, we propose Auto-Stega, an agent-driven self-evolving framework that is the first to realize self-evolving steganographic strategies by automatically discovering, composing, and adapting strategies at inference time; the framework operates as a closed loop of generating, evaluating, summarizing, and updating that continually curates a structured strategy library and adapts across corpora, styles, and task constraints. A decoding LLM recovers the information under the shared strategy. To handle high embedding rates, we introduce PC-DNTE, a plug-and-play algorithm that maintains alignment with the base model's conditional distribution at high embedding rates, preserving imperceptibility while enhancing security. Experimental results demonstrate that at higher embedding rates Auto-Stega achieves superior performance with gains of 42.2\% in perplexity and 1.6\% in anti-steganalysis performance over SOTA methods.</li>
</ul>

<h3>Title: Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Huang, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang, Kaixiang Ji, Weilong Chai, Jianxin Sun, Libin Wang, Yongjie Lv, Taozhi Huang, Jiajia Liu, Qingpei Guo, Ming Yang, Jingdong Chen, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06590">https://arxiv.org/abs/2510.06590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06590">https://arxiv.org/pdf/2510.06590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06590]] Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer(https://arxiv.org/abs/2510.06590)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Visual tokenization remains a core challenge in unifying visual understanding and generation within the autoregressive paradigm. Existing methods typically employ tokenizers in discrete latent spaces to align with the tokens from large language models, where the quantization errors can limit semantic expressiveness and degrade the capability of vision-language understanding. To address this, we introduce MingTok, a new family of visual tokenizers with a continuous latent space, for unified autoregressive generation and understanding. While understanding tasks favor discriminative high-dimensional features, generation tasks prefer compact low-level codes. Thus, to reconcile these competing demands, MingTok adopts a three-stage sequential architecture involving low-level encoding, semantic expansion, and visual reconstruction. Built on top of it, Ming-UniVision eliminates the need for task-specific visual representations, and unifies diverse vision-language tasks under a single autoregrsssive prediction paradigm. By formulating both understanding and generation as next-token prediction in a shared continuous space, it seamlessly supports multi-round, in-context tasks such as iterative understanding, generation and editing. Empirically, we find that using a unified continuous visual representation reconciles the competing requirements on the tokenizers by the understanding and generation tasks, thereby leading to state-of-the-art level performance across both domains. We hope our findings will facilitate unified visual tokenization in the continuous domain. Inference code and model weights are released to benefit community.</li>
</ul>

<h3>Title: SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Ayush Zenith, Arnold Zumbrun, Neel Raut, Jing Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06596">https://arxiv.org/abs/2510.06596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06596">https://arxiv.org/pdf/2510.06596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06596]] SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation(https://arxiv.org/abs/2510.06596)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The performance of machine learning models depends heavily on training data. The scarcity of large-scale, well-annotated datasets poses significant challenges in creating robust models. To address this, synthetic data generated through simulations and generative models has emerged as a promising solution, enhancing dataset diversity and improving the performance, reliability, and resilience of models. However, evaluating the quality of this generated data requires an effective metric. This paper introduces the Synthetic Dataset Quality Metric (SDQM) to assess data quality for object detection tasks without requiring model training to converge. This metric enables more efficient generation and selection of synthetic datasets, addressing a key challenge in resource-constrained object detection tasks. In our experiments, SDQM demonstrated a strong correlation with the mean Average Precision (mAP) scores of YOLOv11, a leading object detection model, while previous metrics only exhibited moderate or weak correlations. Additionally, it provides actionable insights for improving dataset quality, minimizing the need for costly iterative training. This scalable and efficient metric sets a new standard for evaluating synthetic data. The code for SDQM is available at this https URL</li>
</ul>

<h3>Title: Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jingran Xu, Yuanyuan Liu, Yanjie Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06611">https://arxiv.org/abs/2510.06611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06611">https://arxiv.org/pdf/2510.06611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06611]] Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction(https://arxiv.org/abs/2510.06611)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its widespread application is limited by prolonged scan times. Fast MRI reconstruction techniques effectively reduce acquisition duration by reconstructing high-fidelity MR images from undersampled k-space data. In recent years, deep learning-based methods have demonstrated remarkable progress in this field, with self-supervised and unsupervised learning approaches proving particularly valuable in scenarios where fully sampled data are difficult to obtain. This paper proposes a novel zero-shot self-supervised reconstruction framework named UnrollINR, which enables scan-specific MRI reconstruction without relying on external training data. The method adopts a physics-guided unrolled iterative reconstruction architecture and introduces Implicit Neural Representation (INR) as a regularization prior to effectively constrain the solution space. By combining a deep unrolled structure with the powerful implicit representation capability of INR, the model's interpretability and reconstruction performance are enhanced. Experimental results demonstrate that even at a high acceleration rate of 10, UnrollINR achieves superior reconstruction performance compared to the supervised learning method, validating the superiority of the proposed method.</li>
</ul>

<h3>Title: POME: Post Optimization Model Edit via Muon-style Projection</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Di Fu, Yang Luo, Zirui Zhu, Minhao Cheng, Cho-Jui Hsieh, Yang You</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06627">https://arxiv.org/abs/2510.06627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06627">https://arxiv.org/pdf/2510.06627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06627]] POME: Post Optimization Model Edit via Muon-style Projection(https://arxiv.org/abs/2510.06627)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce Post-Optimization Model Edit (POME), a new algorithm that enhances the performance of fine-tuned large language models using only their pretrained and fine-tuned checkpoints, without requiring extra data or further optimization. The core idea is to apply a muon-style projection to $\Delta W$, the difference between the fine-tuned and pretrained weights. This projection uses truncated singular value decomposition (SVD) to equalize the influence of dominant update directions and prune small singular values, which often represent noise. As a simple post-processing step, POME is completely decoupled from the training pipeline. It requires zero modifications and imposes no overhead, making it universally compatible with any optimizer or distributed framework. POME delivers consistent gains, boosting average performance by +2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from 7B foundation models to 72B RLHF-instructed models -- establishes it as a practical, zero-cost enhancement for any fine-tuning pipeline. Code is available at this https URL.</li>
</ul>

<h3>Title: Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shiye Su, Yuhui Zhang, Linqi Zhou, Rajesh Ranganath, Serena Yeung-Levy</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06634">https://arxiv.org/abs/2510.06634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06634">https://arxiv.org/pdf/2510.06634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06634]] Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling(https://arxiv.org/abs/2510.06634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modeling transformations between arbitrary data distributions is a fundamental scientific challenge, arising in applications like drug discovery and evolutionary simulation. While flow matching offers a natural framework for this task, its use has thus far primarily focused on the noise-to-data setting, while its application in the general distribution-to-distribution setting is underexplored. We find that in the latter case, where the source is also a data distribution to be learned from limited samples, standard flow matching fails due to sparse supervision. To address this, we propose a simple and computationally efficient method that injects stochasticity into the training process by perturbing source samples and flow interpolants. On five diverse imaging tasks spanning biology, radiology, and astronomy, our method significantly improves generation quality, outperforming existing baselines by an average of 9 FID points. Our approach also reduces the transport cost between input and generated samples to better highlight the true effect of the transformation, making flow matching a more practical tool for simulating the diverse distribution transformations that arise in science.</li>
</ul>

<h3>Title: Control-Augmented Autoregressive Diffusion for Data Assimilation</h3>
<ul>
<li><strong>Authors: </strong>Prakhar Srivastava, Farrin Marouf Sofian, Francesco Immorlano, Kushagra Pandey, Stephan Mandt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06637">https://arxiv.org/abs/2510.06637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06637">https://arxiv.org/pdf/2510.06637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06637]] Control-Augmented Autoregressive Diffusion for Data Assimilation(https://arxiv.org/abs/2510.06637)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments pretrained ARDMs with a lightweight controller network, trained offline by previewing future ARDM rollouts and learning stepwise controls that anticipate upcoming observations under a terminal cost objective. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), a setting where existing methods are often computationally prohibitive and prone to forecast drift under sparse observations. Our approach reduces DA inference to a single forward rollout with on-the-fly corrections, avoiding expensive adjoint computations and/or optimizations during inference. We demonstrate that our method consistently outperforms four state-of-the-art baselines in stability, accuracy, and physical fidelity across two canonical PDEs and six observation regimes. We will release code and checkpoints publicly.</li>
</ul>

<h3>Title: XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation</h3>
<ul>
<li><strong>Authors: </strong>Udbhav Bamba, Minghao Fang, Yifan Yu, Haizhong Zheng, Fan Lai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06672">https://arxiv.org/abs/2510.06672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06672">https://arxiv.org/pdf/2510.06672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06672]] XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation(https://arxiv.org/abs/2510.06672)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Reinforcement learning algorithms such as GRPO have driven recent advances in large language model (LLM) reasoning. While scaling the number of rollouts stabilizes training, existing approaches suffer from limited exploration on challenging prompts and leave informative feedback signals underexploited, due to context-independent rollout allocation across prompts (e.g., generating 16 rollouts per prompt) and relying heavily on sparse rewards. This paper presents XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy optimization through the principled lens of rollout exploration-exploitation. To enhance exploration, XRPO introduces a mathematically grounded rollout allocator that adaptively prioritizes prompts with higher potential for uncertainty reduction. It further addresses stagnation on zero-reward prompts through an in-context seeding strategy that injects curated exemplars, steering the model into more difficult reasoning trajectories. To strengthen exploitation, XRPO develops a group-relative, novelty-aware advantage sharpening mechanism that leverages sequence likelihoods to amplify low-probability yet correct responses, thereby extending the policy's reach beyond sparse rewards. Experiments across diverse math and coding benchmarks on both reasoning and non-reasoning models demonstrate that XRPO outperforms existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while accelerating training convergence by up to 2.7X.</li>
</ul>

<h3>Title: Heptapod: Language Modeling on Visual Signals</h3>
<ul>
<li><strong>Authors: </strong>Yongxin Zhu, Jiawei Chen, Yuanzhe Chen, Zhuo Chen, Dongya Jia, Jian Cong, Xiaobin Zhuang, Yuping Wang, Yuxuan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06673">https://arxiv.org/abs/2510.06673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06673">https://arxiv.org/pdf/2510.06673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06673]] Heptapod: Language Modeling on Visual Signals(https://arxiv.org/abs/2510.06673)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs \textbf{causal attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend of semantic tokenizers}. Our key innovation is \textit{next 2D distribution prediction}: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.</li>
</ul>

<h3>Title: SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jipeng Lyu, Jiahua Dong, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06694">https://arxiv.org/abs/2510.06694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06694">https://arxiv.org/pdf/2510.06694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06694]] SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis(https://arxiv.org/abs/2510.06694)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.</li>
</ul>

<h3>Title: A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking</h3>
<ul>
<li><strong>Authors: </strong>Gal Fadlon, Idan Arbiv, Nimrod Berman, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06699">https://arxiv.org/abs/2510.06699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06699">https://arxiv.org/pdf/2510.06699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06699]] A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking(https://arxiv.org/abs/2510.06699)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic time series data is critical for applications in healthcare, finance, and science. However, irregular sampling and missing values present significant challenges. While prior methods address these irregularities, they often yield suboptimal results and incur high computational costs. Recent advances in regular time series generation, such as the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable generative capabilities by transforming time series into image representations, making them a promising solution. However, extending ImagenTime to irregular sequences using simple masking introduces "unnatural" neighborhoods, where missing values replaced by zeros disrupt the learning process. To overcome this, we propose a novel two-step framework: first, a Time Series Transformer completes irregular sequences, creating natural neighborhoods; second, a vision-based diffusion model with masking minimizes dependence on the completed values. This approach leverages the strengths of both completion and masking, enabling robust and efficient generation of realistic time series. Our method achieves state-of-the-art performance, achieving a relative improvement in discriminative score by $70\%$ and in computational cost by $85\%$. Code is at this https URL.</li>
</ul>

<h3>Title: A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Eitan Klinger, Zihao Huang, Tran Minh Nguyen, Emma Jayeon Park, Yige Chen, Yang Gu, Qingyu Gao, Siliang Liu, Mengyang Qiu, Jungyeul Park</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06749">https://arxiv.org/abs/2510.06749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06749">https://arxiv.org/pdf/2510.06749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06749]] A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction(https://arxiv.org/abs/2510.06749)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluating grammatical error correction requires metrics that reflect the diversity of valid human corrections rather than privileging a single reference. Existing frameworks, largely edit-based and English-centric, rely on rigid alignments between system and reference edits, limiting their applicability in multilingual and generative settings. This paper introduces a formal framework for \textit{fluency-based multi-reference evaluation}, framing $n$-gram similarity as an aggregation problem over multiple legitimate corrections. Within this formulation, we instantiate GLEU through four aggregation strategies--\textsc{select-best}, \textsc{simple-average}, \textsc{weighted-average}, and \textsc{merged-counts}--and analyze their properties of boundedness, monotonicity, and sensitivity to reference variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora show that these strategies capture complementary aspects of fluency and coverage. The framework unifies multi-reference evaluation into a principled, fluency-oriented approach that incorporates linguistic diversity without penalizing legitimate variation.</li>
</ul>

<h3>Title: OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot</h3>
<ul>
<li><strong>Authors: </strong>Junhan Zhu, Hesong Wang, Mingluo Su, Zefang Wang, Huan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06751">https://arxiv.org/abs/2510.06751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06751">https://arxiv.org/pdf/2510.06751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06751]] OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot(https://arxiv.org/abs/2510.06751)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.</li>
</ul>

<h3>Title: Extreme Amodal Face Detection</h3>
<ul>
<li><strong>Authors: </strong>Changlin Song, Yunzhong Hou, Michael Randall Barnes, Rahul Shome, Dylan Campbell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06791">https://arxiv.org/abs/2510.06791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06791">https://arxiv.org/pdf/2510.06791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06791]] Extreme Amodal Face Detection(https://arxiv.org/abs/2510.06791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Extreme amodal detection is the task of inferring the 2D location of objects that are not fully visible in the input image but are visible within an expanded field-of-view. This differs from amodal detection, where the object is partially visible within the input image, but is occluded. In this paper, we consider the sub-problem of face detection, since this class provides motivating applications involving safety and privacy, but do not tailor our method specifically to this class. Existing approaches rely on image sequences so that missing detections may be interpolated from surrounding frames or make use of generative models to sample possible completions. In contrast, we consider the single-image task and propose a more efficient, sample-free approach that makes use of the contextual cues from the image to infer the presence of unseen faces. We design a heatmap-based extreme amodal object detector that addresses the problem of efficiently predicting a lot (the out-of-frame region) from a little (the image) with a selective coarse-to-fine decoder. Our method establishes strong results for this new task, even outperforming less efficient generative approaches.</li>
</ul>

<h3>Title: Overview of the Plagiarism Detection Task at PAN 2025</h3>
<ul>
<li><strong>Authors: </strong>André Greiner-Petter, Maik Fröbe, Jan Philip Wahle, Terry Ruas, Bela Gipp, Akiko Aizawa, Martin Potthast</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06805">https://arxiv.org/abs/2510.06805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06805">https://arxiv.org/pdf/2510.06805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06805]] Overview of the Plagiarism Detection Task at PAN 2025(https://arxiv.org/abs/2510.06805)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generative plagiarism detection task at PAN 2025 aims at identifying automatically generated textual plagiarism in scientific articles and aligning them with their respective sources. We created a novel large-scale dataset of automatically generated plagiarism using three large language models: Llama, DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation of this dataset, summarize and compare the results of all participants and four baselines, and evaluate the results on the last plagiarism detection task from PAN 2015 in order to interpret the robustness of the proposed approaches. We found that the current iteration does not invite a large variety of approaches as naive semantic similarity approaches based on embedding vectors provide promising results of up to 0.8 recall and 0.5 precision. In contrast, most of these approaches underperform significantly on the 2015 dataset, indicating a lack in generalizability.</li>
</ul>

<h3>Title: VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance</h3>
<ul>
<li><strong>Authors: </strong>Teng Wang, Haojun Jiang, Yuxuan Wang, Zhenguo Sun, Shiji Song, Gao Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06809">https://arxiv.org/abs/2510.06809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06809">https://arxiv.org/pdf/2510.06809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06809]] VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance(https://arxiv.org/abs/2510.06809)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Echocardiography is a critical tool for detecting heart diseases. Recently, ultrasound foundation models have demonstrated remarkable capabilities in cardiac ultrasound image analysis. However, obtaining high-quality ultrasound images is a prerequisite for accurate diagnosis. Due to the exceptionally high operational difficulty of cardiac ultrasound, there is a shortage of highly skilled personnel, which hinders patients from receiving timely examination services. In this paper, we aim to adapt the medical knowledge learned by foundation models from vast datasets to the probe guidance task, which is designed to provide real-time operational recommendations for junior sonographers to acquire high-quality ultrasound images. Moreover, inspired by the practice where experts optimize action decisions based on past explorations, we meticulously design a parameter-efficient Vision-Action Adapter (VA-Adapter) to enable foundation model's image encoder to encode vision-action sequences, thereby enhancing guidance performance. With built-in sequential reasoning capabilities in a compact design, the VA-Adapter enables a pre-trained ultrasound foundation model to learn precise probe adjustment strategies by fine-tuning only a small subset of parameters. Extensive experiments demonstrate that the VA-Adapter can surpass strong probe guidance models. Our code will be released after acceptance.</li>
</ul>

<h3>Title: Exposing Citation Vulnerabilities in Generative Engines</h3>
<ul>
<li><strong>Authors: </strong>Riku Mochizuki, Shusuke Komatsu, Souta Noguchi, Kazuto Ataka</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06823">https://arxiv.org/abs/2510.06823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06823">https://arxiv.org/pdf/2510.06823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06823]] Exposing Citation Vulnerabilities in Generative Engines(https://arxiv.org/abs/2510.06823)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We analyze answers generated by generative engines (GEs) from the perspectives of citation publishers and the content-injection barrier, defined as the difficulty for attackers to manipulate answers to user prompts by placing malicious content on the web. GEs integrate two functions: web search and answer generation that cites web pages using large language models. Because anyone can publish information on the web, GEs are vulnerable to poisoning attacks. Existing studies of citation evaluation focus on how faithfully answer content reflects cited sources, leaving unexamined which web sources should be selected as citations to defend against poisoning attacks. To fill this gap, we introduce evaluation criteria that assess poisoning threats using the citation information contained in answers. Our criteria classify the publisher attributes of citations to estimate the content-injection barrier thereby revealing the threat of poisoning attacks in current GEs. We conduct experiments in political domains in Japan and the United States (U.S.) using our criteria and show that citations from official party websites (primary sources) are approximately \(25\%\)--\(45\%\) in the U.S. and \(60\%\)--\(65\%\) in Japan, indicating that U.S. political answers are at higher risk of poisoning attacks. We also find that sources with low content-injection barriers are frequently cited yet are poorly reflected in answer content. To mitigate this threat, we discuss how publishers of primary sources can increase exposure of their web content in answers and show that well-known techniques are limited by language differences.</li>
</ul>

<h3>Title: StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jaeseok Jeong, Junho Kim, Gayoung Lee, Yunjey Choi, Youngjung Uh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06827">https://arxiv.org/abs/2510.06827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06827">https://arxiv.org/pdf/2510.06827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06827]] StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance(https://arxiv.org/abs/2510.06827)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the domain of text-to-image generation, diffusion models have emerged as powerful tools. Recently, studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content. However, existing methods often suffer from content leakage, where undesired elements of the visual style prompt are transferred along with the intended style. To address this issue, we 1) extend classifier-free guidance (CFG) to utilize swapping self-attention and propose 2) negative visual query guidance (NVQG) to reduce the transfer of unwanted contents. NVQG employs negative score by intentionally simulating content leakage scenarios that swap queries instead of key and values of self-attention layers from visual style prompts. This simple yet effective method significantly reduces content leakage. Furthermore, we provide careful solutions for using a real image as visual style prompts. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, reflecting the style of the references, and ensuring that resulting images match the text prompts. Our code is available \href{this https URL}{here}.</li>
</ul>

<h3>Title: Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series</h3>
<ul>
<li><strong>Authors: </strong>Iago Xabier Vázquez, Javier Sedano, Muhammad Afzal, Ángel Miguel García-Vico</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06910">https://arxiv.org/abs/2510.06910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06910">https://arxiv.org/pdf/2510.06910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06910]] Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series(https://arxiv.org/abs/2510.06910)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection is a key task across domains such as industry, healthcare, and cybersecurity. Many real-world anomaly detection problems involve analyzing multiple features over time, making time series analysis a natural approach for such problems. While deep learning models have achieved strong performance in this field, their trend to exhibit high energy consumption limits their deployment in resource-constrained environments such as IoT devices, edge computing platforms, and wearables. To address this challenge, this paper introduces the \textit{Vacuum Spiker algorithm}, a novel Spiking Neural Network-based method for anomaly detection in time series. It incorporates a new detection criterion that relies on global changes in neural activity rather than reconstruction or prediction error. It is trained using Spike Time-Dependent Plasticity in a novel way, intended to induce changes in neural activity when anomalies occur. A new efficient encoding scheme is also proposed, which discretizes the input space into non-overlapping intervals, assigning each to a single neuron. This strategy encodes information with a single spike per time step, improving energy efficiency compared to conventional encoding methods. Experimental results on publicly available datasets show that the proposed algorithm achieves competitive performance while significantly reducing energy consumption, compared to a wide set of deep learning and machine learning baselines. Furthermore, its practical utility is validated in a real-world case study, where the model successfully identifies power curtailment events in a solar inverter. These results highlight its potential for sustainable and efficient anomaly detection.</li>
</ul>

<h3>Title: DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning</h3>
<ul>
<li><strong>Authors: </strong>Ke Guo, Haochen Liu, Xiaojun Wu, Chen Lv</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06913">https://arxiv.org/abs/2510.06913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06913">https://arxiv.org/pdf/2510.06913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06913]] DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning(https://arxiv.org/abs/2510.06913)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Realistic traffic simulation is critical for the development of autonomous driving systems and urban mobility planning, yet existing imitation learning approaches often fail to model realistic traffic behaviors. Behavior cloning suffers from covariate shift, while Generative Adversarial Imitation Learning (GAIL) is notoriously unstable in multi-agent settings. We identify a key source of this instability: irrelevant interaction misguidance, where a discriminator penalizes an ego vehicle's realistic behavior due to unrealistic interactions among its neighbors. To address this, we propose Decomposed Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map and ego-neighbor components, filtering out misleading neighbor: neighbor and neighbor: map interactions. We further introduce a social PPO objective that augments ego rewards with distance-weighted neighborhood rewards, encouraging overall realism across agents. Integrated into a lightweight SMART-based backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark.</li>
</ul>

<h3>Title: LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zecheng Tang, Baibei Ji, Quantong Qiu, Haitian Wang, Xiaobo Liang, Juntao Li, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06915">https://arxiv.org/abs/2510.06915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06915">https://arxiv.org/pdf/2510.06915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06915]] LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling(https://arxiv.org/abs/2510.06915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.</li>
</ul>

<h3>Title: Label-frugal satellite image change detection with generative virtual exemplar learning</h3>
<ul>
<li><strong>Authors: </strong>Hichem Sahbi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06926">https://arxiv.org/abs/2510.06926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06926">https://arxiv.org/pdf/2510.06926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06926]] Label-frugal satellite image change detection with generative virtual exemplar learning(https://arxiv.org/abs/2510.06926)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Change detection is a major task in remote sensing which consists in finding all the occurrences of changes in multi-temporal satellite or aerial images. The success of existing methods, and particularly deep learning ones, is tributary to the availability of hand-labeled training data that capture the acquisition conditions and the subjectivity of the user (oracle). In this paper, we devise a novel change detection algorithm, based on active learning. The main contribution of our work resides in a new model that measures how important is each unlabeled sample, and provides an oracle with only the most critical samples (also referred to as virtual exemplars) for further labeling. These exemplars are generated, using an invertible graph convnet, as the optimum of an adversarial loss that (i) measures representativity, diversity and ambiguity of the data, and thereby (ii) challenges (the most) the current change detection criteria, leading to a better re-estimate of these criteria in the subsequent iterations of active learning. Extensive experiments show the positive impact of our label-efficient learning model against comparative methods.</li>
</ul>

<h3>Title: Generating Surface for Text-to-3D using 2D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Huanning Dong, Fan Li, Ping Kuang, Jianwen Min</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06967">https://arxiv.org/abs/2510.06967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06967">https://arxiv.org/pdf/2510.06967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06967]] Generating Surface for Text-to-3D using 2D Gaussian Splatting(https://arxiv.org/abs/2510.06967)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in Text-to-3D modeling have shown significant potential for the creation of 3D content. However, due to the complex geometric shapes of objects in the natural world, generating 3D content remains a challenging task. Current methods either leverage 2D diffusion priors to recover 3D geometry, or train the model directly based on specific 3D representations. In this paper, we propose a novel method named DirectGaussian, which focuses on generating the surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize conditional text generation models and the surface of a 3D object is rendered by 2D Gaussian splatting with multi-view normal and texture priors. For multi-view geometric consistency problems, DirectGaussian incorporates curvature constraints on the generated surface during optimization process. Through extensive experiments, we demonstrate that our framework is capable of achieving diverse and high-fidelity 3D content creation.</li>
</ul>

<h3>Title: VelLMes: A high-interaction AI-based deception framework</h3>
<ul>
<li><strong>Authors: </strong>Muris Sladić (1), Veronica Valeros (1), Carlos Catania (2), Sebastian Garcia (1) ((1) Czech Technical University in Prague, (2) CONICET, UNCuyo)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06975">https://arxiv.org/abs/2510.06975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06975">https://arxiv.org/pdf/2510.06975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06975]] VelLMes: A high-interaction AI-based deception framework(https://arxiv.org/abs/2510.06975)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>There are very few SotA deception systems based on Large Language Models. The existing ones are limited only to simulating one type of service, mainly SSH shells. These systems - but also the deception technologies not based on LLMs - lack an extensive evaluation that includes human attackers. Generative AI has recently become a valuable asset for cybersecurity researchers and practitioners, and the field of cyber-deception is no exception. Researchers have demonstrated how LLMs can be leveraged to create realistic-looking honeytokens, fake users, and even simulated systems that can be used as honeypots. This paper presents an AI-based deception framework called VelLMes, which can simulate multiple protocols and services such as SSH Linux shell, MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus VelLMes offers a variety of choices for deception design based on the users' needs. VelLMes is designed to be attacked by humans, so interactivity and realism are key for its performance. We evaluate the generative capabilities and the deception capabilities. Generative capabilities were evaluated using unit tests for LLMs. The results of the unit tests show that, with careful prompting, LLMs can produce realistic-looking responses, with some LLMs having a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception capabilities with 89 human attackers. The results showed that about 30% of the attackers thought that they were interacting with a real system when they were assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH Linux shell honeypot on the Internet to capture real-life attacks. Analysis of these attacks showed us that LLM honeypots simulating Linux shells can perform well against unstructured and unexpected attacks on the Internet, responding correctly to most of the issued commands.</li>
</ul>

<h3>Title: Revisiting Mixout: An Overlooked Path to Robust Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06982">https://arxiv.org/abs/2510.06982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06982">https://arxiv.org/pdf/2510.06982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06982]] Revisiting Mixout: An Overlooked Path to Robust Finetuning(https://arxiv.org/abs/2510.06982)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Finetuning vision foundation models often improves in-domain accuracy but comes at the cost of robustness under distribution shift. We revisit Mixout, a stochastic regularizer that intermittently replaces finetuned weights with their pretrained reference, through the lens of a single-run, weight-sharing implicit ensemble. This perspective reveals three key levers that govern robustness: the \emph{masking anchor}, \emph{resampling frequency}, and \emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i) replaces the fixed anchor with an exponential moving-average snapshot that adapts during training, and (ii) regulates masking period via an explicit resampling-frequency hyperparameter. Our sparse-kernel implementation updates only a small fraction of parameters with no inference-time overhead, enabling training on consumer-grade GPUs. Experiments on benchmarks covering covariate shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet, iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy beyond zero-shot performance while surpassing both Model Soups and strong parameter-efficient finetuning baselines under distribution shift.</li>
</ul>

<h3>Title: No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts</h3>
<ul>
<li><strong>Authors: </strong>Girolamo Macaluso, Lorenzo Mandelli, Mirko Bicchierai, Stefano Berretti, Andrew D. Bagdanov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06988">https://arxiv.org/abs/2510.06988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06988">https://arxiv.org/pdf/2510.06988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06988]] No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts(https://arxiv.org/abs/2510.06988)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model's generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.</li>
</ul>

<h3>Title: RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning</h3>
<ul>
<li><strong>Authors: </strong>Artur Horal, Daniel Pina, Henrique Paz, Iago Paulo, João Soares, Rafael Ferreira, Diogo Tavares, Diogo Glória-Silva, João Magalhães, David Semedo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06994">https://arxiv.org/abs/2510.06994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06994">https://arxiv.org/pdf/2510.06994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06994]] RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning(https://arxiv.org/abs/2510.06994)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents the vision, scientific contributions, and technical details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework, to audit the robustness of Large Language Models (LLMs) in AI-assisted software development. Our work is driven by three major research streams: (1) robust and systematic assessment of LLM conversational jailbreaks; (2) a diverse generative multi-turn attack suite, supporting compositional, realistic and goal-oriented jailbreak conversational strategies; and (3) a hierarchical attack planner, which adaptively plans, serializes, and triggers attacks tailored to specific LLM's vulnerabilities. Together, these contributions form a unified framework -- combining assessment, attack generation, and strategic planning -- to comprehensively evaluate and expose weaknesses in LLMs' robustness. Extensive evaluation is conducted to systematically assess and analyze the performance of the overall system and each component. Experimental results demonstrate that our multi-turn adversarial attack strategies can successfully lead state-of-the-art LLMs to produce unsafe generations, highlighting the pressing need for more research into enhancing LLM's robustness.</li>
</ul>

<h3>Title: Introspection in Learned Semantic Scene Graph Localisation</h3>
<ul>
<li><strong>Authors: </strong>Manshika Charvi Bissessur, Efimia Panagiotaki, Daniele De Martini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07053">https://arxiv.org/abs/2510.07053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07053">https://arxiv.org/pdf/2510.07053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07053]] Introspection in Learned Semantic Scene Graph Localisation(https://arxiv.org/abs/2510.07053)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This work investigates how semantics influence localisation performance and robustness in a learned self-supervised, contrastive semantic localisation framework. After training a localisation network on both original and perturbed maps, we conduct a thorough post-hoc introspection analysis to probe whether the model filters environmental noise and prioritises distinctive landmarks over routine clutter. We validate various interpretability methods and present a comparative reliability analysis. Integrated gradients and Attention Weights consistently emerge as the most reliable probes of learned behaviour. A semantic class ablation further reveals an implicit weighting in which frequent objects are often down-weighted. Overall, the results indicate that the model learns noise-robust, semantically salient relations about place definition, thereby enabling explainable registration under challenging visual and structural variations.</li>
</ul>

<h3>Title: LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish</h3>
<ul>
<li><strong>Authors: </strong>Fred Philippy, Laura Bernardy, Siwen Guo, Jacques Klein, Tegawendé F. Bissyandé</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07074">https://arxiv.org/abs/2510.07074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07074">https://arxiv.org/pdf/2510.07074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07074]] LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish(https://arxiv.org/abs/2510.07074)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Instruction tuning has become a key technique for enhancing the performance of large language models, enabling them to better follow human prompts. However, low-resource languages such as Luxembourgish face severe limitations due to the lack of high-quality instruction datasets. Traditional reliance on machine translation often introduces semantic misalignment and cultural inaccuracies. In this work, we address these challenges by creating a cross-lingual instruction tuning dataset for Luxembourgish, without resorting to machine-generated translations into it. Instead, by leveraging aligned data from English, French, and German, we build a high-quality dataset that preserves linguistic and cultural nuances. We provide evidence that cross-lingual instruction tuning not only improves representational alignment across languages but also the model's generative capabilities in Luxembourgish. This highlights how cross-lingual data curation can avoid the common pitfalls of machine-translated data and directly benefit low-resource language development.</li>
</ul>

<h3>Title: Accelerating Diffusion LLM Inference via Local Determinism Propagation</h3>
<ul>
<li><strong>Authors: </strong>Fanheng Kong, Jingyuan Zhang, Yahui Liu, Zirui Wu, Yu Tian, Victoria W., Guorui Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07081">https://arxiv.org/abs/2510.07081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07081">https://arxiv.org/pdf/2510.07081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07081]] Accelerating Diffusion LLM Inference via Local Determinism Propagation(https://arxiv.org/abs/2510.07081)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) represent a significant advancement in text generation, offering parallel token decoding capabilities. However, existing open-source implementations suffer from quality-speed trade-offs that impede their practical deployment. Conservative sampling strategies typically decode only the most confident token per step to ensure quality (i.e., greedy decoding), at the cost of inference efficiency due to repeated redundant refinement iterations--a phenomenon we term delayed decoding. Through systematic analysis of dLLM decoding dynamics, we characterize this delayed decoding behavior and propose a training-free adaptive parallel decoding strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built on two fundamental empirical principles: local determinism propagation centered on high-confidence anchors and progressive spatial consistency decay. By applying these principles, LocalLeap identifies anchors and performs localized relaxed parallel decoding within bounded neighborhoods, achieving substantial inference step reduction through early commitment of already-determined tokens without compromising output quality. Comprehensive evaluation on various benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput improvements and reduces decoding steps to just 14.2\% of the original requirement, achieving these gains with negligible performance impact. The source codes are available at: this https URL.</li>
</ul>

<h3>Title: DADO: A Depth-Attention framework for Object Discovery</h3>
<ul>
<li><strong>Authors: </strong>Federico Gonzalez, Estefania Talavera, Petia Radeva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07089">https://arxiv.org/abs/2510.07089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07089">https://arxiv.org/pdf/2510.07089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07089]] DADO: A Depth-Attention framework for Object Discovery(https://arxiv.org/abs/2510.07089)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Unsupervised object discovery, the task of identifying and localizing objects in images without human-annotated labels, remains a significant challenge and a growing focus in computer vision. In this work, we introduce a novel model, DADO (Depth-Attention self-supervised technique for Discovering unseen Objects), which combines an attention mechanism and a depth model to identify potential objects in images. To address challenges such as noisy attention maps or complex scenes with varying depth planes, DADO employs dynamic weighting to adaptively emphasize attention or depth features based on the global characteristics of each image. We evaluated DADO on standard benchmarks, where it outperforms state-of-the-art methods in object discovery accuracy and robustness without the need for fine-tuning.</li>
</ul>

<h3>Title: Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Mereu, Aidan Scannell, Yuxin Hou, Yi Zhao, Aditya Jitta, Antonio Dominguez, Luigi Acerbi, Amos Storkey, Paul Chang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07092">https://arxiv.org/abs/2510.07092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07092">https://arxiv.org/pdf/2510.07092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07092]] Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report(https://arxiv.org/abs/2510.07092)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>World models are a powerful paradigm in AI and robotics, enabling agents to reason about the future by predicting visual observations or compact latent states. The 1X World Model Challenge introduces an open-source benchmark of real-world humanoid interaction, with two complementary tracks: sampling, focused on forecasting future image frames, and compression, focused on predicting future discrete latent codes. For the sampling track, we adapt the video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned future frame prediction. We condition the video generation on robot states using AdaLN-Zero, and further post-train the model using LoRA. For the compression track, we train a Spatio-Temporal Transformer model from scratch. Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386 in the compression task, securing 1st place in both challenges.</li>
</ul>

<h3>Title: Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning</h3>
<ul>
<li><strong>Authors: </strong>Taylor Sorensen, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07105">https://arxiv.org/abs/2510.07105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07105">https://arxiv.org/pdf/2510.07105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07105]] Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning(https://arxiv.org/abs/2510.07105)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Many natural language processing (NLP) tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators. In this paper, we outline our system for modeling human variation. Our system leverages language models' (LLMs) in-context learning abilities, along with a two-step meta-learning training procedure for 1) post-training on many datasets requiring in-context learning and 2) specializing the model via in-context meta-learning to the particular data distribution of interest. We also evaluate the performance of our system submission to the Learning With Disagreements (LeWiDi) competition, where it was the overall winner on both tasks. Additionally, we perform an ablation study to measure the importance of each system component. We find that including rater examples in-context is crucial for our system's performance, dataset-specific fine-tuning is helpful on the larger datasets, post-training on other in-context datasets is helpful on one of the competition datasets, and that performance improves with model scale.</li>
</ul>

<h3>Title: GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics</h3>
<ul>
<li><strong>Authors: </strong>Guan-Yan Yang, Farn Wang, Kuo-Hui Yeh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07109">https://arxiv.org/abs/2510.07109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07109">https://arxiv.org/pdf/2510.07109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07109]] GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics(https://arxiv.org/abs/2510.07109)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Consumer electronics (CE) connected to the Internet of Things are susceptible to various attacks, including DDoS and web-based threats, which can compromise their functionality and facilitate remote hijacking. These vulnerabilities allow attackers to exploit CE for broader system attacks while enabling the propagation of malicious code across the CE network, resulting in device failures. Existing deep learning-based traffic anomaly detection systems exhibit high accuracy in traditional network environments but are often overly complex and reliant on static infrastructure, necessitating manual configuration and management. To address these limitations, we propose a scalable network model that integrates Software-defined Networking (SDN) and Compute First Networking (CFN) for next-generation CE networks. In this network model, we propose a Graph Neural Networks-based Network Anomaly Detection framework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN architecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph with dynamic traffic features, providing a holistic view of network security. The core of the framework is a GNN model (GSAGE) for graph representation learning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF) demonstrates superior performance compared to existing feature selection methods. Experimental evaluations on CE environment reveal that GNN-NAD achieves superior metrics in accuracy, recall, precision, and F1 score, even with small sample sizes, exceeding the performance of current network anomaly detection methods. This work advances the security and efficiency of next-generation intelligent CE networks.</li>
</ul>

<h3>Title: MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency</h3>
<ul>
<li><strong>Authors: </strong>Dongki Jung, Jaehoon Choi, Yonghan Lee, Sungmin Eum, Heesung Kwon, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07119">https://arxiv.org/abs/2510.07119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07119">https://arxiv.org/pdf/2510.07119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07119]] MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency(https://arxiv.org/abs/2510.07119)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Monocular 3D foundation models offer an extensible solution for perception tasks, making them attractive for broader 3D vision applications. In this paper, we propose MoRe, a training-free Monocular Geometry Refinement method designed to improve cross-view consistency and achieve scale alignment. To induce inter-frame relationships, our method employs feature matching between frames to establish correspondences. Rather than applying simple least squares optimization on these matched points, we formulate a graph-based optimization framework that performs local planar approximation using the estimated 3D points and surface normals estimated by monocular foundation models. This formulation addresses the scale ambiguity inherent in monocular geometric priors while preserving the underlying 3D structure. We further demonstrate that MoRe not only enhances 3D reconstruction but also improves novel view synthesis, particularly in sparse view rendering scenarios.</li>
</ul>

<h3>Title: Graph Conditioned Diffusion for Controllable Histopathology Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Sarah Cechnicka, Matthew Baugh, Weitong Zhang, Mischa Dombrowski, Zhe Li, Johannes C. Paetzold, Candice Roufosse, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07129">https://arxiv.org/abs/2510.07129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07129">https://arxiv.org/pdf/2510.07129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07129]] Graph Conditioned Diffusion for Controllable Histopathology Image Generation(https://arxiv.org/abs/2510.07129)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in Diffusion Probabilistic Models (DPMs) have set new standards in high-quality image synthesis. Yet, controlled generation remains challenging, particularly in sensitive areas such as medical imaging. Medical images feature inherent structure such as consistent spatial arrangement, shape or texture, all of which are critical for diagnosis. However, existing DPMs operate in noisy latent spaces that lack semantic structure and strong priors, making it difficult to ensure meaningful control over generated content. To address this, we propose graph-based object-level representations for Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding to each major structure in the image, encapsulating their individual features and relationships. These graph representations are processed by a transformer module and integrated into a diffusion model via the text-conditioning mechanism, enabling fine-grained control over generation. We evaluate this approach using a real-world histopathology use case, demonstrating that our generated data can reliably substitute for annotated patient data in downstream segmentation tasks. The code is available here.</li>
</ul>

<h3>Title: A multi-layered embedded intrusion detection framework for programmable logic controllers</h3>
<ul>
<li><strong>Authors: </strong>Rishabh Das. Aaron Werth, Tommy Morris</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07171">https://arxiv.org/abs/2510.07171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07171">https://arxiv.org/pdf/2510.07171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07171]] A multi-layered embedded intrusion detection framework for programmable logic controllers(https://arxiv.org/abs/2510.07171)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Industrial control system (ICS) operations use trusted endpoints like human machine interfaces (HMIs) and workstations to relay commands to programmable logic controllers (PLCs). Because most PLCs lack layered defenses, compromise of a trusted endpoint can drive unsafe actuator commands and risk safety-critical operation. This research presents an embedded intrusion detection system that runs inside the controller and uses header-level telemetry to detect and respond to network attacks. The system combines a semi-supervised anomaly detector and a supervised attack classifier. We evaluate the approach on a midstream oil-terminal testbed using three datasets collected during tanker-truck loading. The anomaly detector achieves zero missed attacks, corresponding to 0.998 Matthews correlation. The supervised stage attains 97.37 percent hold-out accuracy and 97.03 percent external accuracy. The embedded design adds a median of 2,031 microseconds of end-to-end latency and does not impact PLC's cycle time. The proposed architecture provides a multi-layer embedded security that meets the real-time requirements of an industrial system.</li>
</ul>

<h3>Title: MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yihao Zhi, Chenghong Li, Hongjie Liao, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Xiaodong Cun, Wensen Feng, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07190">https://arxiv.org/abs/2510.07190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07190">https://arxiv.org/pdf/2510.07190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07190]] MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis(https://arxiv.org/abs/2510.07190)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in video generation, powered by large-scale datasets and diffusion techniques, have shown that video diffusion models can function as implicit 4D novel view synthesizers. Nevertheless, current methods primarily concentrate on redirecting camera trajectory within the front view while struggling to generate 360-degree viewpoint changes. In this paper, we focus on human-centric subdomain and present MV-Performer, an innovative framework for creating synchronized novel view videos from monocular full-body captures. To achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset and incorporate an informative condition signal. Specifically, we use the camera-dependent normal maps rendered from oriented partial point clouds, which effectively alleviate the ambiguity between seen and unseen observations. To maintain synchronization in the generated videos, we propose a multi-view human-centric video diffusion model that fuses information from the reference video, partial rendering, and different viewpoints. Additionally, we provide a robust inference procedure for in-the-wild video cases, which greatly mitigates the artifacts induced by imperfect monocular depth estimation. Extensive experiments on three datasets demonstrate our MV-Performer's state-of-the-art effectiveness and robustness, setting a strong model for human-centric 4D novel view synthesis.</li>
</ul>

<h3>Title: Resolution scaling governs DINOv3 transfer performance in chest radiograph classification</h3>
<ul>
<li><strong>Authors: </strong>Soroosh Tayebi Arasteh, Mina Shaigan, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07191">https://arxiv.org/abs/2510.07191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07191">https://arxiv.org/pdf/2510.07191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07191]] Resolution scaling governs DINOv3 transfer performance in chest radiograph classification(https://arxiv.org/abs/2510.07191)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n>814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.</li>
</ul>

<h3>Title: EigenScore: OOD Detection using Covariance in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shirin Shoushtari, Yi Wang, Xiao Shi, M. Salman Asif, Ulugbek S. Kamilov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07206">https://arxiv.org/abs/2510.07206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07206">https://arxiv.org/pdf/2510.07206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07206]] EigenScore: OOD Detection using Covariance in Diffusion Models(https://arxiv.org/abs/2510.07206)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems in safety-sensitive domains. Diffusion models have recently emerged as powerful generative models, capable of capturing complex data distributions through iterative denoising. Building on this progress, recent work has explored their potential for OOD detection. We propose EigenScore, a new OOD detection method that leverages the eigenvalue spectrum of the posterior covariance induced by a diffusion model. We argue that posterior covariance provides a consistent signal of distribution shift, leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear spectral signature. We further provide analysis explicitly linking posterior covariance to distribution mismatch, establishing it as a reliable signal for OOD detection. To ensure tractability, we adopt a Jacobian-free subspace iteration method to estimate the leading eigenvalues using only forward evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance, with up to 5% AUROC improvement over the best baseline. Notably, it remains robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing diffusion-based methods often fail.</li>
</ul>

<h3>Title: Security-Robustness Trade-offs in Diffusion Steganography: A Comparative Analysis of Pixel-Space and VAE-Based Architectures</h3>
<ul>
<li><strong>Authors: </strong>Yuhua Xu, Wei Sun, Chengpei Tang, Jiaxing Lu, Jingying Zhou, Chen Gu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07219">https://arxiv.org/abs/2510.07219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07219">https://arxiv.org/pdf/2510.07219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07219]] Security-Robustness Trade-offs in Diffusion Steganography: A Comparative Analysis of Pixel-Space and VAE-Based Architectures(https://arxiv.org/abs/2510.07219)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current generative steganography research mainly pursues computationally expensive mappings to perfect Gaussian priors within single diffusion model architectures. This work introduces an efficient framework based on approximate Gaussian mapping governed by a scale factor calibrated through capacity-aware adaptive optimization. Using this framework as a unified analytical tool, systematic comparative analysis of steganography in pixel-space models versus VAE-based latent-space systems is conducted. The investigation reveals a pronounced architecture dependent security-robustness trade-off: pixel-space models achieve high security against steganalysis but exhibit fragility to channel distortions, while VAE-based systems like Stable Diffusion offer substantial robustness at the cost of security vulnerabilities. Further analysis indicates that the VAE component drives this behavior through opposing mechanisms where the encoder confers robustness via manifold regularization while the decoder introduces vulnerabilities by amplifying latent perturbations into detectable artifacts. These findings characterize the conflicting architectural roles in generative steganography and establish a foundation for future research.</li>
</ul>

<h3>Title: Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection</h3>
<ul>
<li><strong>Authors: </strong>Franco Javier Arellano, José Ignacio Orlando</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07277">https://arxiv.org/abs/2510.07277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07277">https://arxiv.org/pdf/2510.07277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07277]] Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection(https://arxiv.org/abs/2510.07277)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Diabetic Macular Edema (DME) is a leading cause of vision loss among patients with Diabetic Retinopathy (DR). While deep learning has shown promising results for automatically detecting this condition from fundus images, its application remains challenging due the limited availability of annotated data. Foundation Models (FM) have emerged as an alternative solution. However, it is unclear if they can cope with DME detection in particular. In this paper, we systematically compare different FM and standard transfer learning approaches for this task. Specifically, we compare the two most popular FM for retinal images--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different training regimes and evaluation settings in IDRiD, MESSIDOR-2 and OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do not consistently outperform fine-tuned CNNs in this task. In particular, an EfficientNet-B0 ranked first or second in terms of area under the ROC and precision/recall curves in most evaluation settings, with RETFound only showing promising results in OEFI. FLAIR, on the other hand, demonstrated competitive zero-shot performance, achieving notable AUC-PR scores when prompted appropriately. These findings reveal that FM might not be a good tool for fine-grained ophthalmic tasks such as DME detection even after fine-tuning, suggesting that lightweight CNNs remain strong baselines in data-scarce environments.</li>
</ul>

<h3>Title: Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07316">https://arxiv.org/abs/2510.07316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07316">https://arxiv.org/pdf/2510.07316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07316]] Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers(https://arxiv.org/abs/2510.07316)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.</li>
</ul>

<h3>Title: Temporal Prompting Matters: Rethinking Referring Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ci-Siang Lin, Min-Hung Chen, I-Jieh Liu, Chien-Yi Wang, Sifei Liu, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.07319">https://arxiv.org/abs/2510.07319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.07319">https://arxiv.org/pdf/2510.07319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.07319]] Temporal Prompting Matters: Rethinking Referring Video Object Segmentation(https://arxiv.org/abs/2510.07319)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
