<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-08</h1>
<h2>diffusion</h2>
<h3>Title: FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02433">http://arxiv.org/abs/2401.02433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02433]] FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients(http://arxiv.org/abs/2401.02433)</code></li>
<li>Summary: <p>With the rapid development of imaging sensor technology in the field of
remote sensing, multi-modal remote sensing data fusion has emerged as a crucial
research direction for land cover classification tasks. While diffusion models
have made great progress in generative models and image classification tasks,
existing models primarily focus on single-modality and single-client control,
that is, the diffusion process is driven by a single modal in a single
computing node. To facilitate the secure fusion of heterogeneous data from
clients, it is necessary to enable distributed multi-modal control, such as
merging the hyperspectral data of organization A and the LiDAR data of
organization B privately on each base station client. In this study, we propose
a multi-modal collaborative diffusion federated learning framework called
FedDiff. Our framework establishes a dual-branch diffusion model feature
extraction setup, where the two modal data are inputted into separate branches
of the encoder. Our key insight is that diffusion models driven by different
modalities are inherently complementary in terms of potential denoising steps
on which bilateral connections can be built. Considering the challenge of
private and efficient communication between multiple clients, we embed the
diffusion model into the federated learning communication structure, and
introduce a lightweight communication module. Qualitative and quantitative
experiments validate the superiority of our framework in terms of image quality
and conditional consistency.
</p></li>
</ul>

<h3>Title: VASE: Object-Centric Appearance and Shape Manipulation of Real Videos. (arXiv:2401.02473v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02473">http://arxiv.org/abs/2401.02473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02473]] VASE: Object-Centric Appearance and Shape Manipulation of Real Videos(http://arxiv.org/abs/2401.02473)</code></li>
<li>Summary: <p>Recently, several works tackled the video editing task fostered by the
success of large-scale text-to-image generative models. However, most of these
methods holistically edit the frame using the text, exploiting the prior given
by foundation diffusion models and focusing on improving the temporal
consistency across frames. In this work, we introduce a framework that is
object-centric and is designed to control both the object's appearance and,
notably, to execute precise and explicit structural modifications on the
object. We build our framework on a pre-trained image-conditioned diffusion
model, integrate layers to handle the temporal dimension, and propose training
strategies and architectural modifications to enable shape control. We evaluate
our method on the image-driven video editing task showing similar performance
to the state-of-the-art, and showcasing novel shape-editing capabilities.
Further details, code and examples are available on our project page:
https://helia95.github.io/vase-website/
</p></li>
</ul>

<h3>Title: Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02524">http://arxiv.org/abs/2401.02524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02524]] Comprehensive Exploration of Synthetic Data Generation: A Survey(http://arxiv.org/abs/2401.02524)</code></li>
<li>Summary: <p>Recent years have witnessed a surge in the popularity of Machine Learning
(ML), applied across diverse domains. However, progress is impeded by the
scarcity of training data due to expensive acquisition and privacy legislation.
Synthetic data emerges as a solution, but the abundance of released models and
limited overview literature pose challenges for decision-making. This work
surveys 417 Synthetic Data Generation (SDG) models over the last decade,
providing a comprehensive overview of model types, functionality, and
improvements. Common attributes are identified, leading to a classification and
trend analysis. The findings reveal increased model performance and complexity,
with neural network-based approaches prevailing, except for privacy-preserving
data generation. Computer vision dominates, with GANs as primary generative
models, while diffusion models, transformers, and RNNs compete. Implications
from our performance evaluation highlight the scarcity of common metrics and
datasets, making comparisons challenging. Additionally, the neglect of training
and computational costs in literature necessitates attention in future
research. This work serves as a guide for SDG model selection and identifies
crucial areas for future exploration.
</p></li>
</ul>

<h3>Title: Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss. (arXiv:2401.02677v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02677">http://arxiv.org/abs/2401.02677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02677]] Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss(http://arxiv.org/abs/2401.02677)</code></li>
<li>Summary: <p>Stable Diffusion XL (SDXL) has become the best open source text-to-image
model (T2I) for its versatility and top-notch image quality. Efficiently
addressing the computational demands of SDXL models is crucial for wider reach
and applicability. In this work, we introduce two scaled-down variants, Segmind
Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter
UNets, respectively, achieved through progressive removal using layer-level
losses focusing on reducing the model size while preserving generative quality.
We release these models weights at https://hf.co/Segmind. Our methodology
involves the elimination of residual networks and transformer blocks from the
U-Net structure of SDXL, resulting in significant reductions in parameters, and
latency. Our compact models effectively emulate the original SDXL by
capitalizing on transferred knowledge, achieving competitive results against
larger multi-billion parameter SDXL. Our work underscores the efficacy of
knowledge distillation coupled with layer-level losses in reducing model size
while preserving the high-quality generative capabilities of SDXL, thus
facilitating more accessible deployment in resource-constrained environments.
</p></li>
</ul>

<h3>Title: Diffbody: Diffusion-based Pose and Shape Editing of Human Images. (arXiv:2401.02804v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02804">http://arxiv.org/abs/2401.02804</a></li>
<li>Code URL: <a href="https://github.com/yutaokuyama/diffbody">https://github.com/yutaokuyama/diffbody</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02804]] Diffbody: Diffusion-based Pose and Shape Editing of Human Images(http://arxiv.org/abs/2401.02804)</code></li>
<li>Summary: <p>Pose and body shape editing in a human image has received increasing
attention. However, current methods often struggle with dataset biases and
deteriorate realism and the person's identity when users make large edits. We
propose a one-shot approach that enables large edits with identity
preservation. To enable large edits, we fit a 3D body model, project the input
image onto the 3D model, and change the body's pose and shape. Because this
initial textured body model has artifacts due to occlusion and the inaccurate
body shape, the rendered image undergoes a diffusion-based refinement, in which
strong noise destroys body structure and identity whereas insufficient noise
does not help. We thus propose an iterative refinement with weak noise, applied
first for the whole body and then for the face. We further enhance the realism
by fine-tuning text embeddings via self-supervised learning. Our quantitative
and qualitative evaluations demonstrate that our method outperforms other
existing methods across various datasets.
</p></li>
</ul>

<h3>Title: Generating Non-Stationary Textures using Self-Rectification. (arXiv:2401.02847v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02847">http://arxiv.org/abs/2401.02847</a></li>
<li>Code URL: <a href="https://github.com/xiaorongjun000/self-rectification">https://github.com/xiaorongjun000/self-rectification</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02847]] Generating Non-Stationary Textures using Self-Rectification(http://arxiv.org/abs/2401.02847)</code></li>
<li>Summary: <p>This paper addresses the challenge of example-based non-stationary texture
synthesis. We introduce a novel twostep approach wherein users first modify a
reference texture using standard image editing tools, yielding an initial rough
target for the synthesis. Subsequently, our proposed method, termed
"self-rectification", automatically refines this target into a coherent,
seamless texture, while faithfully preserving the distinct visual
characteristics of the reference exemplar. Our method leverages a pre-trained
diffusion network, and uses self-attention mechanisms, to gradually align the
synthesized texture with the reference, ensuring the retention of the
structures in the provided target. Through experimental validation, our
approach exhibits exceptional proficiency in handling non-stationary textures,
demonstrating significant advancements in texture synthesis when compared to
existing state-of-the-art techniques. Code is available at
https://github.com/xiaorongjun000/Self-Rectification
</p></li>
</ul>

<h3>Title: Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction. (arXiv:2401.02916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02916">http://arxiv.org/abs/2401.02916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02916]] Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction(http://arxiv.org/abs/2401.02916)</code></li>
<li>Summary: <p>Human trajectory forecasting is a critical challenge in fields such as
robotics and autonomous driving. Due to the inherent uncertainty of human
actions and intentions in real-world scenarios, various unexpected occurrences
may arise. To uncover latent motion patterns in human behavior, we introduce a
novel memory-based method, named Motion Pattern Priors Memory Network. Our
method involves constructing a memory bank derived from clustered prior
knowledge of motion patterns observed in the training set trajectories. We
introduce an addressing mechanism to retrieve the matched pattern and the
potential target distributions for each prediction from the memory bank, which
enables the identification and retrieval of natural motion patterns exhibited
by agents, subsequently using the target priors memory token to guide the
diffusion model to generate predictions. Extensive experiments validate the
effectiveness of our approach, achieving state-of-the-art trajectory prediction
accuracy. The code will be made publicly available.
</p></li>
</ul>

<h3>Title: Simple Hierarchical Planning with Diffusion. (arXiv:2401.02644v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02644">http://arxiv.org/abs/2401.02644</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02644]] Simple Hierarchical Planning with Diffusion(http://arxiv.org/abs/2401.02644)</code></li>
<li>Summary: <p>Diffusion-based generative methods have proven effective in modeling
trajectories with offline datasets. However, they often face computational
challenges and can falter in generalization, especially in capturing temporal
abstractions for long-horizon tasks. To overcome this, we introduce the
Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning
method combining the advantages of hierarchical and diffusion-based planning.
Our model adopts a "jumpy" planning strategy at the higher level, which allows
it to have a larger receptive field but at a lower computational cost -- a
crucial factor for diffusion-based planning methods, as we have empirically
verified. Additionally, the jumpy sub-goals guide our low-level planner,
facilitating a fine-tuning stage and further improving our approach's
effectiveness. We conducted empirical evaluations on standard offline
reinforcement learning benchmarks, demonstrating our method's superior
performance and efficiency in terms of training and planning speed compared to
the non-hierarchical Diffuser as well as other hierarchical planning methods.
Moreover, we explore our model's generalization capability, particularly on how
our method improves generalization capabilities on compositional
out-of-distribution tasks.
</p></li>
</ul>

<h3>Title: Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02683">http://arxiv.org/abs/2401.02683</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02683]] Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation(http://arxiv.org/abs/2401.02683)</code></li>
<li>Summary: <p>Denoising diffusion models have shown great potential in multiple research
areas. Existing diffusion-based generative methods on de novo 3D molecule
generation face two major challenges. Since majority heavy atoms in molecules
allow connections to multiple atoms through single bonds, solely using
pair-wise distance to model molecule geometries is insufficient. Therefore, the
first one involves proposing an effective neural network as the denoising
kernel that is capable to capture complex multi-body interatomic relationships
and learn high-quality features. Due to the discrete nature of graphs,
mainstream diffusion-based methods for molecules heavily rely on predefined
rules and generate edges in an indirect manner. The second challenge involves
accommodating molecule generation to diffusion and accurately predicting the
existence of bonds. In our research, we view the iterative way of updating
molecule conformations in diffusion process is consistent with molecular
dynamics and introduce a novel molecule generation method named
Geometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge,
we introduce a Dual-Track Transformer Network (DTN) to fully excevate global
spatial relationships and learn high quality representations which contribute
to accurate predictions of features and geometries. As for the second
challenge, we design Geometric-Facilitated Loss (GFLoss) which intervenes the
formation of bonds during the training period, instead of directly embedding
edges into the latent space. Comprehensive experiments on current benchmarks
demonstrate the superiority of GFMDiff.
</p></li>
</ul>

<h3>Title: Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02739">http://arxiv.org/abs/2401.02739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02739]] Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors(http://arxiv.org/abs/2401.02739)</code></li>
<li>Summary: <p>We propose denoising diffusion variational inference (DDVI), an approximate
inference algorithm for latent variable models which relies on diffusion models
as expressive variational posteriors. Our method augments variational
posteriors with auxiliary latents, which yields an expressive class of models
that perform diffusion in latent space by reversing a user-specified noising
process. We fit these models by optimizing a novel lower bound on the marginal
likelihood inspired by the wake-sleep algorithm. Our method is easy to
implement (it fits a regularized extension of the ELBO), is compatible with
black-box variational inference, and outperforms alternative classes of
approximate posteriors based on normalizing flows or adversarial networks. When
applied to deep latent variable models, our method yields the denoising
diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in
biology -- inferring latent ancestry from human genomes -- outperforming strong
baselines on the Thousand Genomes dataset.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: DHGCN: Dynamic Hop Graph Convolution Network for Self-supervised Point Cloud Learning. (arXiv:2401.02610v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02610">http://arxiv.org/abs/2401.02610</a></li>
<li>Code URL: <a href="https://github.com/jinec98/dhgcn">https://github.com/jinec98/dhgcn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02610]] DHGCN: Dynamic Hop Graph Convolution Network for Self-supervised Point Cloud Learning(http://arxiv.org/abs/2401.02610)</code></li>
<li>Summary: <p>Recent works attempt to extend Graph Convolution Networks (GCNs) to point
clouds for classification and segmentation tasks. These works tend to sample
and group points to create smaller point sets locally and mainly focus on
extracting local features through GCNs, while ignoring the relationship between
point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network
(DHGCN) for explicitly learning the contextual relationships between the
voxelized point parts, which are treated as graph nodes. Motivated by the
intuition that the contextual information between point parts lies in the
pairwise adjacent relationship, which can be depicted by the hop distance of
the graph quantitatively, we devise a novel self-supervised part-level hop
distance reconstruction task and design a novel loss function accordingly to
facilitate training. In addition, we propose the Hop Graph Attention (HGA),
which takes the learned hop distance as input for producing attention weights
to allow edge features to contribute distinctively in aggregation. Eventually,
the proposed DHGCN is a plug-and-play module that is compatible with
point-based backbone networks. Comprehensive experiments on different backbones
and tasks demonstrate that our self-supervised method achieves state-of-the-art
performance. Our source code is available at: https://github.com/Jinec98/DHGCN.
</p></li>
</ul>

<h3>Title: Fus-MAE: A cross-attention-based data fusion approach for Masked Autoencoders in remote sensing. (arXiv:2401.02764v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02764">http://arxiv.org/abs/2401.02764</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02764]] Fus-MAE: A cross-attention-based data fusion approach for Masked Autoencoders in remote sensing(http://arxiv.org/abs/2401.02764)</code></li>
<li>Summary: <p>Self-supervised frameworks for representation learning have recently stirred
up interest among the remote sensing community, given their potential to
mitigate the high labeling costs associated with curating large satellite image
datasets. In the realm of multimodal data fusion, while the often used
contrastive learning methods can help bridging the domain gap between different
sensor types, they rely on data augmentations techniques that require expertise
and careful design, especially for multispectral remote sensing data. A
possible but rather scarcely studied way to circumvent these limitations is to
use a masked image modelling based pretraining strategy. In this paper, we
introduce Fus-MAE, a self-supervised learning framework based on masked
autoencoders that uses cross-attention to perform early and feature-level data
fusion between synthetic aperture radar and multispectral optical data - two
modalities with a significant domain gap. Our empirical findings demonstrate
that Fus-MAE can effectively compete with contrastive learning strategies
tailored for SAR-optical data fusion and outperforms other masked-autoencoders
frameworks trained on a larger corpus.
</p></li>
</ul>

<h3>Title: Locally Adaptive Neural 3D Morphable Models. (arXiv:2401.02937v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02937">http://arxiv.org/abs/2401.02937</a></li>
<li>Code URL: <a href="https://github.com/michaeltrs/lamm">https://github.com/michaeltrs/lamm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02937]] Locally Adaptive Neural 3D Morphable Models(http://arxiv.org/abs/2401.02937)</code></li>
<li>Summary: <p>We present the Locally Adaptive Morphable Model (LAMM), a highly flexible
Auto-Encoder (AE) framework for learning to generate and manipulate 3D meshes.
We train our architecture following a simple self-supervised training scheme in
which input displacements over a set of sparse control vertices are used to
overwrite the encoded geometry in order to transform one training sample into
another. During inference, our model produces a dense output that adheres
locally to the specified sparse geometry while maintaining the overall
appearance of the encoded object. This approach results in state-of-the-art
performance in both disentangling manipulated geometry and 3D mesh
reconstruction. To the best of our knowledge LAMM is the first end-to-end
framework that enables direct local control of 3D vertex geometry in a single
forward pass. A very efficient computational graph allows our network to train
with only a fraction of the memory required by previous methods and run faster
during inference, generating 12k vertex meshes at $&gt;$60fps on a single CPU
thread. We further leverage local geometry control as a primitive for higher
level editing operations and present a set of derivative capabilities such as
swapping and sampling object parts. Code and pretrained models can be found at
https://github.com/michaeltrs/LAMM.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02458">http://arxiv.org/abs/2401.02458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02458]] Data-Centric Foundation Models in Computational Healthcare: A Survey(http://arxiv.org/abs/2401.02458)</code></li>
<li>Summary: <p>The advent of foundation models (FMs) as an emerging suite of AI techniques
has struck a wave of opportunities in computational healthcare. The interactive
nature of these models, guided by pre-training data and human instructions, has
ignited a data-centric AI paradigm that emphasizes better data
characterization, quality, and scale. In healthcare AI, obtaining and
processing high-quality clinical data records has been a longstanding
challenge, ranging from data quantity, annotation, patient privacy, and ethics.
In this survey, we investigate a wide range of data-centric approaches in the
FM era (from model pre-training to inference) towards improving the healthcare
workflow. We discuss key perspectives in AI security, assessment, and alignment
with human values. Finally, we offer a promising outlook of FM-based analytics
to enhance the performance of patient outcome and clinical workflow in the
evolving landscape of healthcare and medicine. We provide an up-to-date list of
healthcare-related foundation models and datasets at
https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare .
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Image-based Deep Learning for Smart Digital Twins: a Review. (arXiv:2401.02523v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02523">http://arxiv.org/abs/2401.02523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02523]] Image-based Deep Learning for Smart Digital Twins: a Review(http://arxiv.org/abs/2401.02523)</code></li>
<li>Summary: <p>Smart Digital twins (SDTs) are being increasingly used to virtually replicate
and predict the behaviors of complex physical systems through continual data
assimilation enabling the optimization of the performance of these systems by
controlling the actions of systems. Recently, deep learning (DL) models have
significantly enhanced the capabilities of SDTs, particularly for tasks such as
predictive maintenance, anomaly detection, and optimization. In many domains,
including medicine, engineering, and education, SDTs use image data
(image-based SDTs) to observe and learn system behaviors and control their
behaviors. This paper focuses on various approaches and associated challenges
in developing image-based SDTs by continually assimilating image data from
physical systems. The paper also discusses the challenges involved in designing
and implementing DL models for SDTs, including data acquisition, processing,
and interpretation. In addition, insights into the future directions and
opportunities for developing new image-based DL approaches to develop robust
SDTs are provided. This includes the potential for using generative models for
data augmentation, developing multi-modal DL models, and exploring the
integration of DL with other technologies, including 5G, edge computing, and
IoT. In this paper, we describe the image-based SDTs, which enable broader
adoption of the digital twin DT paradigms across a broad spectrum of areas and
the development of new methods to improve the abilities of SDTs in replicating,
predicting, and optimizing the behavior of complex systems.
</p></li>
</ul>

<h3>Title: PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering. (arXiv:2401.02797v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02797">http://arxiv.org/abs/2401.02797</a></li>
<li>Code URL: <a href="https://github.com/jinlhe/pefomed">https://github.com/jinlhe/pefomed</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02797]] PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering(http://arxiv.org/abs/2401.02797)</code></li>
<li>Summary: <p>Multimodal large language models (MLLMs) represent an evolutionary expansion
in the capabilities of traditional large language models, enabling them to
tackle challenges that surpass the scope of purely text-based applications. It
leverages the knowledge previously encoded within these language models,
thereby enhancing their applicability and functionality in the reign of
multimodal contexts. Recent works investigate the adaptation of MLLMs to
predict free-form answers as a generative task to solve medical visual question
answering (Med-VQA) tasks. In this paper, we propose a parameter efficient
framework for fine-tuning MLLM specifically tailored to Med-VQA applications,
and empirically validate it on a public benchmark dataset. To accurately
measure the performance, we employ human evaluation and the results reveal that
our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v
model by a significant margin of 26% absolute accuracy on closed-ended
questions. The code will be available here: https://github.com/jinlHe/PeFoMed.
</p></li>
</ul>

<h3>Title: t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02576">http://arxiv.org/abs/2401.02576</a></li>
<li>Code URL: <a href="https://github.com/williamyue37/t-dgr">https://github.com/williamyue37/t-dgr</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02576]] t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making(http://arxiv.org/abs/2401.02576)</code></li>
<li>Summary: <p>Deep generative replay has emerged as a promising approach for continual
learning in decision-making tasks. This approach addresses the problem of
catastrophic forgetting by leveraging the generation of trajectories from
previously encountered tasks to augment the current dataset. However, existing
deep generative replay methods for continual learning rely on autoregressive
models, which suffer from compounding errors in the generated trajectories. In
this paper, we propose a simple, scalable, and non-autoregressive method for
continual learning in decision-making tasks using a generative model that
generates task samples conditioned on the trajectory timestep. We evaluate our
method on Continual World benchmarks and find that our approach achieves
state-of-the-art performance on the average success rate metric among continual
learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
</p></li>
</ul>

<h3>Title: H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02905">http://arxiv.org/abs/2401.02905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02905]] H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses(http://arxiv.org/abs/2401.02905)</code></li>
<li>Summary: <p>Discovering human cognitive and emotional states using multi-modal
physiological signals draws attention across various research applications.
Physiological responses of the human body are influenced by human cognition and
commonly used to analyze cognitive states. From a network science perspective,
the interactions of these heterogeneous physiological modalities in a graph
structure may provide insightful information to support prediction of cognitive
states. However, there is no clue to derive exact connectivity between
heterogeneous modalities and there exists a hierarchical structure of
sub-modalities. Existing graph neural networks are designed to learn on
non-hierarchical homogeneous graphs with pre-defined graph structures; they
failed to learn from hierarchical, multi-modal physiological data without a
pre-defined graph structure. To this end, we propose a hierarchical
heterogeneous graph generative network (H2G2-Net) that automatically learns a
graph structure without domain knowledge, as well as a powerful representation
on the hierarchical heterogeneous graph in an end-to-end fashion. We validate
the proposed method on the CogPilot dataset that consists of multi-modal
physiological signals. Extensive experiments demonstrate that our proposed
method outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems. (arXiv:2401.02450v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02450">http://arxiv.org/abs/2401.02450</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02450]] Locally Differentially Private Embedding Models in Distributed Fraud Prevention Systems(http://arxiv.org/abs/2401.02450)</code></li>
<li>Summary: <p>Global financial crime activity is driving demand for machine learning
solutions in fraud prevention. However, prevention systems are commonly
serviced to financial institutions in isolation, and few provisions exist for
data sharing due to fears of unintentional leaks and adversarial attacks.
Collaborative learning advances in finance are rare, and it is hard to find
real-world insights derived from privacy-preserving data processing systems. In
this paper, we present a collaborative deep learning framework for fraud
prevention, designed from a privacy standpoint, and awarded at the recent PETs
Prize Challenges. We leverage latent embedded representations of varied-length
transaction sequences, along with local differential privacy, in order to
construct a data release mechanism which can securely inform externally hosted
fraud and anomaly detection models. We assess our contribution on two
distributed data sets donated by large payment networks, and demonstrate
robustness to popular inference-time attacks, along with utility-privacy
trade-offs analogous to published work in alternative application domains.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task. (arXiv:2401.02909v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02909">http://arxiv.org/abs/2401.02909</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02909]] Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task(http://arxiv.org/abs/2401.02909)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are increasingly bringing advances to Natural
Language Processing. However, low-resource languages, those lacking extensive
prominence in datasets for various NLP tasks, or where existing datasets are
not as substantial, such as Portuguese, already obtain several benefits from
LLMs, but not to the same extent. LLMs trained on multilingual datasets
normally struggle to respond to prompts in Portuguese satisfactorily,
presenting, for example, code switching in their responses. This work proposes
a fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two
versions: 7B and 13B. We evaluate the performance of this model in
classification tasks using the zero-shot approach with in-context learning, and
compare it with other LLMs. Our main contribution is to bring an LLM with
satisfactory results in the Portuguese language, as well as to provide a model
that is free for research or commercial purposes.
</p></li>
</ul>

<h3>Title: Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks. (arXiv:2401.02921v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02921">http://arxiv.org/abs/2401.02921</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02921]] Towards ASR Robust Spoken Language Understanding Through In-Context Learning With Word Confusion Networks(http://arxiv.org/abs/2401.02921)</code></li>
<li>Summary: <p>In the realm of spoken language understanding (SLU), numerous natural
language understanding (NLU) methodologies have been adapted by supplying large
language models (LLMs) with transcribed speech instead of conventional written
text. In real-world scenarios, prior to input into an LLM, an automated speech
recognition (ASR) system generates an output transcript hypothesis, where
inherent errors can degrade subsequent SLU tasks. Here we introduce a method
that utilizes the ASR system's lattice output instead of relying solely on the
top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU
outcomes. Our in-context learning experiments, covering spoken question
answering and intent classification, underline the LLM's resilience to noisy
speech transcripts with the help of word confusion networks from lattices,
bridging the SLU performance gap between using the top ASR hypothesis and an
oracle upper bound. Additionally, we delve into the LLM's robustness to varying
ASR performance conditions and scrutinize the aspects of in-context learning
which prove the most influential.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
