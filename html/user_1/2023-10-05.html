<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02279">http://arxiv.org/abs/2310.02279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02279]] Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion(http://arxiv.org/abs/2310.02279)</code></li>
<li>Summary: <p>Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion
model sampling at the cost of sample quality but lack a natural way to
trade-off quality for speed. To address this limitation, we propose Consistency
Trajectory Model (CTM), a generalization encompassing CM and score-based models
as special cases. CTM trains a single neural network that can -- in a single
forward pass -- output scores (i.e., gradients of log-density) and enables
unrestricted traversal between any initial and final time along the Probability
Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables
the efficient combination of adversarial training and denoising score matching
loss to enhance performance and achieves new state-of-the-art FIDs for
single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at
64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes,
both deterministic and stochastic, involving long jumps along the ODE solution
trajectories. It consistently improves sample quality as computational budgets
increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the
score accommodates all diffusion model inference techniques, including exact
likelihood computation.
</p></li>
</ul>

<h3>Title: FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models. (arXiv:2310.02401v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02401">http://arxiv.org/abs/2310.02401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02401]] FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models(http://arxiv.org/abs/2310.02401)</code></li>
<li>Summary: <p>Text-to-image generative models based on latent diffusion models (LDM) have
demonstrated their outstanding ability in generating high-quality and
high-resolution images according to language prompt. Based on these powerful
latent diffusion models, various fine-tuning methods have been proposed to
achieve the personalization of text-to-image diffusion models such as artistic
style adaptation and human face transfer. However, the unauthorized usage of
data for model personalization has emerged as a prevalent concern in relation
to copyright violations. For example, a malicious user may use the fine-tuning
technique to generate images which mimic the style of a painter without his/her
permission. In light of this concern, we have proposed FT-Shield, a
watermarking approach specifically designed for the fine-tuning of
text-to-image diffusion models to aid in detecting instances of infringement.
We develop a novel algorithm for the generation of the watermark to ensure that
the watermark on the training images can be quickly and accurately transferred
to the generated images of text-to-image diffusion models. A watermark will be
detected on an image by a binary watermark detector if the image is generated
by a model that has been fine-tuned using the protected watermarked images.
Comprehensive experiments were conducted to validate the effectiveness of
FT-Shield.
</p></li>
</ul>

<h3>Title: EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods. (arXiv:2310.02426v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02426">http://arxiv.org/abs/2310.02426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02426]] EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods(http://arxiv.org/abs/2310.02426)</code></li>
<li>Summary: <p>A plethora of text-guided image editing methods have recently been developed
by leveraging the impressive capabilities of large-scale diffusion-based
generative models such as Imagen and Stable Diffusion. A standardized
evaluation protocol, however, does not exist to compare methods across
different types of fine-grained edits. To address this gap, we introduce
EditVal, a standardized benchmark for quantitatively evaluating text-guided
image editing methods. EditVal consists of a curated dataset of images, a set
of editable attributes for each image drawn from 13 possible edit types, and an
automated evaluation pipeline that uses pre-trained vision-language models to
assess the fidelity of generated images for each edit type. We use EditVal to
benchmark 8 cutting-edge diffusion-based editing methods including SINE, Imagic
and Instruct-Pix2Pix. We complement this with a large-scale human study where
we show that EditVall's automated evaluation pipeline is strongly correlated
with human-preferences for the edit types we considered. From both the human
study and automated evaluation, we find that: (i) Instruct-Pix2Pix, Null-Text
and SINE are the top-performing methods averaged across different edit types,
however {\it only} Instruct-Pix2Pix and Null-Text are able to preserve original
image properties; (ii) Most of the editing methods fail at edits involving
spatial operations (e.g., changing the position of an object). (iii) There is
no `winner' method which ranks the best individually across a range of
different edit types. We hope that our benchmark can pave the way to developing
more reliable text-guided image editing tools in the future. We will publicly
release EditVal, and all associated code and human-study templates to support
these research directions in https://deep-ml-research.github.io/editval/.
</p></li>
</ul>

<h3>Title: Generalization in diffusion models arises from geometry-adaptive harmonic representation. (arXiv:2310.02557v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02557">http://arxiv.org/abs/2310.02557</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02557]] Generalization in diffusion models arises from geometry-adaptive harmonic representation(http://arxiv.org/abs/2310.02557)</code></li>
<li>Summary: <p>High-quality samples generated with score-based reverse diffusion algorithms
provide evidence that deep neural networks (DNN) trained for denoising can
learn high-dimensional densities, despite the curse of dimensionality. However,
recent reports of memorization of the training set raise the question of
whether these networks are learning the "true" continuous density of the data.
Here, we show that two denoising DNNs trained on non-overlapping subsets of a
dataset learn nearly the same score function, and thus the same density, with a
surprisingly small number of training images. This strong generalization
demonstrates an alignment of powerful inductive biases in the DNN architecture
and/or training algorithm with properties of the data distribution. We analyze
these, demonstrating that the denoiser performs a shrinkage operation in a
basis adapted to the underlying image. Examination of these bases reveals
oscillating harmonic structures along contours and in homogeneous image
regions. We show that trained denoisers are inductively biased towards these
geometry-adaptive harmonic representations by demonstrating that they arise
even when the network is trained on image classes such as low-dimensional
manifolds, for which the harmonic basis is suboptimal. Additionally, we show
that the denoising performance of the networks is near-optimal when trained on
regular image classes for which the optimal basis is known to be
geometry-adaptive and harmonic.
</p></li>
</ul>

<h3>Title: SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D. (arXiv:2310.02596v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02596">http://arxiv.org/abs/2310.02596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02596]] SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D(http://arxiv.org/abs/2310.02596)</code></li>
<li>Summary: <p>It is inherently ambiguous to lift 2D results from pre-trained diffusion
models to a 3D world for text-to-3D generation. 2D diffusion models solely
learn view-agnostic priors and thus lack 3D knowledge during the lifting,
leading to the multi-view inconsistency problem. We find that this problem
primarily stems from geometric inconsistency, and avoiding misplaced geometric
structures substantially mitigates the problem in the final outputs. Therefore,
we improve the consistency by aligning the 2D geometric priors in diffusion
models with well-defined 3D shapes during the lifting, addressing the vast
majority of the problem. This is achieved by fine-tuning the 2D diffusion model
to be viewpoint-aware and to produce view-specific coordinate maps of
canonically oriented 3D objects. In our process, only coarse 3D information is
used for aligning. This "coarse" alignment not only resolves the multi-view
inconsistency in geometries but also retains the ability in 2D diffusion models
to generate detailed and diversified high-quality objects unseen in the 3D
datasets. Furthermore, our aligned geometric priors (AGP) are generic and can
be seamlessly integrated into various state-of-the-art pipelines, obtaining
high generalizability in terms of unseen shapes and visual appearance while
greatly alleviating the multi-view inconsistency problem. Our method represents
a new state-of-the-art performance with an 85+% consistency rate by human
evaluation, while many previous methods are around 30%. Our project page is
https://sweetdreamer3d.github.io/
</p></li>
</ul>

<h3>Title: MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02601">http://arxiv.org/abs/2310.02601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02601]] MagicDrive: Street View Generation with Diverse 3D Geometry Control(http://arxiv.org/abs/2310.02601)</code></li>
<li>Summary: <p>Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird's-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
</p></li>
</ul>

<h3>Title: On Memorization in Diffusion Models. (arXiv:2310.02664v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02664">http://arxiv.org/abs/2310.02664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02664]] On Memorization in Diffusion Models(http://arxiv.org/abs/2310.02664)</code></li>
<li>Summary: <p>Due to their capacity to generate novel and high-quality samples, diffusion
models have attracted significant research interest in recent years. Notably,
the typical training objective of diffusion models, i.e., denoising score
matching, has a closed-form optimal solution that can only generate training
data replicating samples. This indicates that a memorization behavior is
theoretically expected, which contradicts the common generalization ability of
state-of-the-art diffusion models, and thus calls for a deeper understanding.
Looking into this, we first observe that memorization behaviors tend to occur
on smaller-sized datasets, which motivates our definition of effective model
memorization (EMM), a metric measuring the maximum size of training data at
which a learned diffusion model approximates its theoretical optimum. Then, we
quantify the impact of the influential factors on these memorization behaviors
in terms of EMM, focusing primarily on data distribution, model configuration,
and training procedure. Besides comprehensive empirical results identifying the
influential factors, we surprisingly find that conditioning training data on
uninformative random labels can significantly trigger the memorization in
diffusion models. Our study holds practical significance for diffusion model
users and offers clues to theoretical research in deep generative models. Code
is available at https://github.com/sail-sg/DiffMemorize.
</p></li>
</ul>

<h3>Title: ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF. (arXiv:2310.02712v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02712">http://arxiv.org/abs/2310.02712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02712]] ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF(http://arxiv.org/abs/2310.02712)</code></li>
<li>Summary: <p>Recently, there has been a significant advancement in text-to-image diffusion
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into NeRF editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing NeRF editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding
real-world scenes into the latent space of the latent diffusion model (LDM)
through a unique refinement layer. This approach enables us to obtain a NeRF
backbone that is not only faster but also more amenable to editing compared to
traditional image space NeRF editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-NeRF achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.
</p></li>
</ul>

<h3>Title: Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models. (arXiv:2310.02848v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02848">http://arxiv.org/abs/2310.02848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02848]] Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models(http://arxiv.org/abs/2310.02848)</code></li>
<li>Summary: <p>Image inpainting aims to fill in the missing pixels with visually coherent
and semantically plausible content. Despite the great progress brought from
deep generative models, this task still suffers from i. the difficulties in
large-scale realistic data collection and costly model training; and ii. the
intrinsic limitations in the traditionally user-defined binary masks on objects
with unclear boundaries or transparent texture. In this paper, we propose
MagicRemover, a tuning-free method that leverages the powerful diffusion models
for text-guided image inpainting. We introduce an attention guidance strategy
to constrain the sampling process of diffusion models, enabling the erasing of
instructed areas and the restoration of occluded content. We further propose a
classifier optimization algorithm to facilitate the denoising stability within
less sampling steps. Extensive comparisons are conducted among our MagicRemover
and state-of-the-art methods including quantitative evaluation and user study,
demonstrating the significant improvement of MagicRemover on high-quality image
inpainting. We will release our code at https://github.com/exisas/Magicremover.
</p></li>
</ul>

<h3>Title: Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts. (arXiv:2310.02906v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02906">http://arxiv.org/abs/2310.02906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02906]] Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts(http://arxiv.org/abs/2310.02906)</code></li>
<li>Summary: <p>Image synthesis approaches, e.g., generative adversarial networks, have been
popular as a form of data augmentation in medical image analysis tasks. It is
primarily beneficial to overcome the shortage of publicly accessible data and
associated quality annotations. However, the current techniques often lack
control over the detailed contents in generated images, e.g., the type of
disease patterns, the location of lesions, and attributes of the diagnosis. In
this work, we adapt the latest advance in the generative model, i.e., the
diffusion model, with the added control flow using lesion-specific visual and
textual prompts for generating dermatoscopic images. We further demonstrate the
advantage of our diffusion model-based framework over the classical generation
models in both the image quality and boosting the segmentation performance on
skin lesions. It can achieve a 9% increase in the SSIM image quality measure
and an over 5% increase in Dice coefficients over the prior arts.
</p></li>
</ul>

<h3>Title: T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation. (arXiv:2310.02977v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02977">http://arxiv.org/abs/2310.02977</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02977]] T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation(http://arxiv.org/abs/2310.02977)</code></li>
<li>Summary: <p>Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.
</p></li>
</ul>

<h3>Title: Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples. (arXiv:2310.02988v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02988">http://arxiv.org/abs/2310.02988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02988]] Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples(http://arxiv.org/abs/2310.02988)</code></li>
<li>Summary: <p>While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes from existing
datasets. To address this challenge, we employ text-to-image diffusion models
to produce counterfactual examples for probing intserctional social biases at
scale. Our approach utilizes Stable Diffusion with cross attention control to
produce sets of counterfactual image-text pairs that are highly similar in
their depiction of a subject (e.g., a given occupation) while differing only in
their depiction of intersectional social attributes (e.g., race &amp; gender). We
conduct extensive experiments using our generated dataset which reveal the
intersectional social biases present in state-of-the-art VLMs.
</p></li>
</ul>

<h3>Title: Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day. (arXiv:2310.03015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03015">http://arxiv.org/abs/2310.03015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03015]] Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day(http://arxiv.org/abs/2310.03015)</code></li>
<li>Summary: <p>The task of novel view synthesis aims to generate unseen perspectives of an
object or scene from a limited set of input images. Nevertheless, synthesizing
novel views from a single image still remains a significant challenge in the
realm of computer vision. Previous approaches tackle this problem by adopting
mesh prediction, multi-plain image construction, or more advanced techniques
such as neural radiance fields. Recently, a pre-trained diffusion model that is
specifically designed for 2D image synthesis has demonstrated its capability in
producing photorealistic novel views, if sufficiently optimized on a 3D
finetuning task. Although the fidelity and generalizability are greatly
improved, training such a powerful diffusion model requires a vast volume of
training data and model parameters, resulting in a notoriously long time and
high computational costs. To tackle this issue, we propose Efficient-3DiM, a
simple but effective framework to learn a single-image novel-view synthesizer.
Motivated by our in-depth analysis of the inference process of diffusion
models, we propose several pragmatic strategies to reduce the training overhead
to a manageable scale, including a crafted timestep sampling strategy, a
superior 3D feature extractor, and an enhanced training scheme. When combined,
our framework is able to reduce the total training time from 10 days to less
than 1 day, significantly accelerating the training process under the same
computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive
experiments are conducted to demonstrate the efficiency and generalizability of
our proposed method.
</p></li>
</ul>

<h3>Title: Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models. (arXiv:2310.03020v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03020">http://arxiv.org/abs/2310.03020</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03020]] Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models(http://arxiv.org/abs/2310.03020)</code></li>
<li>Summary: <p>Zero-shot novel view synthesis (NVS) from a single image is an essential
problem in 3D object understanding. While recent approaches that leverage
pre-trained generative models can synthesize high-quality novel views from
in-the-wild inputs, they still struggle to maintain 3D consistency across
different views. In this paper, we present Consistent-1-to-3, which is a
generative framework that significantly mitigate this issue. Specifically, we
decompose the NVS task into two stages: (i) transforming observed regions to a
novel view, and (ii) hallucinating unseen regions. We design a scene
representation transformer and view-conditioned diffusion model for performing
these two stages respectively. Inside the models, to enforce 3D consistency, we
propose to employ epipolor-guided attention to incorporate geometry
constraints, and multi-view attention to better aggregate multi-view
information. Finally, we design a hierarchy generation paradigm to generate
long sequences of consistent views, allowing a full 360 observation of the
provided object image. Qualitative and quantitative evaluation over multiple
datasets demonstrate the effectiveness of the proposed mechanisms against
state-of-the-art approaches. Our project page is at
https://jianglongye.com/consistent123/
</p></li>
</ul>

<h3>Title: Stochastic force inference via density estimation. (arXiv:2310.02366v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02366">http://arxiv.org/abs/2310.02366</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02366]] Stochastic force inference via density estimation(http://arxiv.org/abs/2310.02366)</code></li>
<li>Summary: <p>Inferring dynamical models from low-resolution temporal data continues to be
a significant challenge in biophysics, especially within transcriptomics, where
separating molecular programs from noise remains an important open problem. We
explore a common scenario in which we have access to an adequate amount of
cross-sectional samples at a few time-points, and assume that our samples are
generated from a latent diffusion process. We propose an approach that relies
on the probability flow associated with an underlying diffusion process to
infer an autonomous, nonlinear force field interpolating between the
distributions. Given a prior on the noise model, we employ score-matching to
differentiate the force field from the intrinsic noise. Using relevant
biophysical examples, we demonstrate that our approach can extract
non-conservative forces from non-stationary data, that it learns equilibrium
dynamics when applied to steady-state data, and that it can do so with both
additive and multiplicative noise models.
</p></li>
</ul>

<h3>Title: SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02391">http://arxiv.org/abs/2310.02391</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02391]] SE(3)-Stochastic Flow Matching for Protein Backbone Generation(http://arxiv.org/abs/2310.02391)</code></li>
<li>Summary: <p>The computational design of novel protein structures has the potential to
impact numerous scientific disciplines greatly. Toward this goal, we introduce
$\text{FoldFlow}$ a series of novel generative models of increasing modeling
power based on the flow-matching paradigm over $3\text{D}$ rigid motions --
i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein
backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free
approach to learning deterministic continuous-time dynamics and matching
invariant target distributions on $\text{SE(3)}$. We next accelerate training
by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$,
leading to the construction of both more simple and stable flows. Finally, we
design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free
training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our
family of $\text{FoldFlow}$ generative models offer several key advantages over
previous approaches to the generative modeling of proteins: they are more
stable and faster to train than diffusion-based approaches, and our models
enjoy the ability to map any invariant source distribution to any invariant
target distribution over $\text{SE(3)}$. Empirically, we validate our FoldFlow
models on protein backbone generation of up to $300$ amino acids leading to
high-quality designable, diverse, and novel samples.
</p></li>
</ul>

<h3>Title: Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02505">http://arxiv.org/abs/2310.02505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02505]] Learning to Reach Goals via Diffusion(http://arxiv.org/abs/2310.02505)</code></li>
<li>Summary: <p>Diffusion models are a powerful class of generative models capable of mapping
random noise in high-dimensional spaces to a target manifold through iterative
denoising. In this work, we present a novel perspective on goal-conditioned
reinforcement learning by framing it within the context of diffusion modeling.
Analogous to the diffusion process, where Gaussian noise is used to create
random trajectories that walk away from the data manifold, we construct
trajectories that move away from potential goal states. We then learn a
goal-conditioned policy analogous to the score function. This approach, which
we call Merlin, can reach predefined or novel goals from an arbitrary initial
state without learning a separate value function. We consider three choices for
the noise model to replace Gaussian noise in diffusion - reverse play from the
buffer, reverse dynamics model, and a novel non-parametric approach. We
theoretically justify our approach and validate it on offline goal-reaching
tasks. Empirical results are competitive with state-of-the-art methods, which
suggests this perspective on diffusion for RL is a simple, scalable, and
effective direction for sequential decision-making.
</p></li>
</ul>

<h3>Title: Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders. (arXiv:2310.02508v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02508">http://arxiv.org/abs/2310.02508</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02508]] Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders(http://arxiv.org/abs/2310.02508)</code></li>
<li>Summary: <p>Three-dimensional native states of natural proteins display recurring and
hierarchical patterns. Yet, traditional graph-based modeling of protein
structures is often limited to operate within a single fine-grained resolution,
and lacks hourglass neural architectures to learn those high-level building
blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant
coarse-graining model that efficiently operates on all heavy atoms of standard
protein residues, while respecting their relevant symmetries. Our model departs
from current approaches that employ graph modeling, instead focusing on local
convolutional coarsening to model sequence-motif interactions in log-linear
length complexity. We train Ophiuchus on contiguous fragments of PDB monomers,
investigating its reconstruction capabilities across different compression
rates. We examine the learned latent space and demonstrate its prompt usage in
conformational interpolation, comparing interpolated trajectories to structure
snapshots from the PDBFlex dataset. Finally, we leverage denoising diffusion
probabilistic models (DDPM) to efficiently sample readily-decodable latent
embeddings of diverse miniproteins. Our experiments demonstrate Ophiuchus to be
a scalable basis for efficient protein modeling and generation.
</p></li>
</ul>

<h3>Title: MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02520">http://arxiv.org/abs/2310.02520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02520]] MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation(http://arxiv.org/abs/2310.02520)</code></li>
<li>Summary: <p>Health risk prediction is one of the fundamental tasks under predictive
modeling in the medical domain, which aims to forecast the potential health
risks that patients may face in the future using their historical Electronic
Health Records (EHR). Researchers have developed several risk prediction models
to handle the unique challenges of EHR data, such as its sequential nature,
high dimensionality, and inherent noise. These models have yielded impressive
results. Nonetheless, a key issue undermining their effectiveness is data
insufficiency. A variety of data generation and augmentation methods have been
introduced to mitigate this issue by expanding the size of the training data
set through the learning of underlying data distributions. However, the
performance of these methods is often limited due to their task-unrelated
design. To address these shortcomings, this paper introduces a novel,
end-to-end diffusion-based risk prediction model, named MedDiffusion. It
enhances risk prediction performance by creating synthetic patient data during
training to enlarge sample space. Furthermore, MedDiffusion discerns hidden
relationships between patient visits using a step-wise attention mechanism,
enabling the model to automatically retain the most vital information for
generating high-quality data. Experimental evaluation on four real-world
medical datasets demonstrates that MedDiffusion outperforms 14 cutting-edge
baselines in terms of PR-AUC, F1, and Cohen's Kappa. We also conduct ablation
studies and benchmark our model against GAN-based alternatives to further
validate the rationality and adaptability of our model design. Additionally, we
analyze generated data to offer fresh insights into the model's
interpretability.
</p></li>
</ul>

<h3>Title: Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02679">http://arxiv.org/abs/2310.02679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02679]] Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization(http://arxiv.org/abs/2310.02679)</code></li>
<li>Summary: <p>We tackle the problem of sampling from intractable high-dimensional density
functions, a fundamental task that often appears in machine learning and
statistics. We extend recent sampling-based approaches that leverage controlled
stochastic processes to model approximate samples from these target densities.
The main drawback of these approaches is that the training objective requires
full trajectories to compute, resulting in sluggish credit assignment issues
due to use of entire trajectories and a learning signal present only at the
terminal time. In this work, we present Diffusion Generative Flow Samplers
(DGFS), a sampling-based framework where the learning process can be tractably
broken down into short partial trajectory segments, via parameterizing an
additional "flow function". Our method takes inspiration from the theory
developed for generative flow networks (GFlowNets), allowing us to make use of
intermediate learning signals and benefit from off-policy exploration
capabilities. Through a variety of challenging experiments, we demonstrate that
DGFS results in more accurate estimates of the normalization constant than
closely-related prior methods.
</p></li>
</ul>

<h3>Title: Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space. (arXiv:2310.02970v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02970">http://arxiv.org/abs/2310.02970</a></li>
<li>Code URL: https://github.com/ebekkers/ponita</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02970]] Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space(http://arxiv.org/abs/2310.02970)</code></li>
<li>Summary: <p>Based on the theory of homogeneous spaces we derive \textit{geometrically
optimal edge attributes} to be used within the flexible message passing
framework. We formalize the notion of weight sharing in convolutional networks
as the sharing of message functions over point-pairs that should be treated
equally. We define equivalence classes of point-pairs that are identical up to
a transformation in the group and derive attributes that uniquely identify
these classes. Weight sharing is then obtained by conditioning message
functions on these attributes. As an application of the theory, we develop an
efficient equivariant group convolutional network for processing 3D point
clouds. The theory of homogeneous spaces tells us how to do group convolutions
with feature maps over the homogeneous space of positions $\mathbb{R}^3$,
position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$
itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to
the ability to represent directional information, which $\mathbb{R}^3$ methods
cannot, and it significantly enhances computational efficiency compared to
indexing features on the full SE$(3)$ group. We empirically support this claim
by reaching state-of-the-art results -- in accuracy and speed -- on three
different benchmarks: interatomic potential energy prediction, trajectory
forecasting in N-body systems, and generating molecules via equivariant
diffusion models.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: FroSSL: Frobenius Norm Minimization for Self-Supervised Learning. (arXiv:2310.02903v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02903">http://arxiv.org/abs/2310.02903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02903]] FroSSL: Frobenius Norm Minimization for Self-Supervised Learning(http://arxiv.org/abs/2310.02903)</code></li>
<li>Summary: <p>Self-supervised learning (SSL) is an increasingly popular paradigm for
representation learning. Recent methods can be classified as
sample-contrastive, dimension-contrastive, or asymmetric network-based, with
each family having its own approach to avoiding informational collapse. While
dimension-contrastive methods converge to similar solutions as
sample-contrastive methods, it can be empirically shown that some methods
require more epochs of training to converge. Motivated by closing this divide,
we present the objective function FroSSL which is both sample- and
dimension-contrastive up to embedding normalization. FroSSL works by minimizing
covariance Frobenius norms for avoiding collapse and minimizing mean-squared
error for augmentation invariance. We show that FroSSL converges more quickly
than a variety of other SSL methods and provide theoretical and empirical
support that this faster convergence is due to how FroSSL affects the
eigenvalues of the embedding covariance matrices. We also show that FroSSL
learns competitive representations on linear probe evaluation when used to
train a ResNet18 on the CIFAR-10, CIFAR-100, STL-10, and ImageNet datasets.
</p></li>
</ul>

<h3>Title: Dual Conic Proxies for AC Optimal Power Flow. (arXiv:2310.02969v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02969">http://arxiv.org/abs/2310.02969</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02969]] Dual Conic Proxies for AC Optimal Power Flow(http://arxiv.org/abs/2310.02969)</code></li>
<li>Summary: <p>In recent years, there has been significant interest in the development of
machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).
Although significant progress has been achieved in predicting high-quality
primal solutions, no existing learning-based approach can provide valid dual
bounds for AC-OPF. This paper addresses this gap by training optimization
proxies for a convex relaxation of AC-OPF. Namely, the paper considers a
second-order cone (SOC) relaxation of ACOPF, and proposes a novel dual
architecture that embeds a fast, differentiable (dual) feasibility recovery,
thus providing valid dual bounds. The paper combines this new architecture with
a self-supervised learning scheme, which alleviates the need for costly
training data generation. Extensive numerical experiments on medium- and
large-scale power grids demonstrate the efficiency and scalability of the
proposed methodology.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations. (arXiv:2310.02428v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02428">http://arxiv.org/abs/2310.02428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02428]] EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields for Atomistic Simulations(http://arxiv.org/abs/2310.02428)</code></li>
<li>Summary: <p>Equivariant graph neural networks force fields (EGraFFs) have shown great
promise in modelling complex interactions in atomic systems by exploiting the
graphs' inherent symmetries. Recent works have led to a surge in the
development of novel architectures that incorporate equivariance-based
inductive biases alongside architectural innovations like graph transformers
and message passing to model atomic interactions. However, thorough evaluations
of these deploying EGraFFs for the downstream task of real-world atomistic
simulations, is lacking. To this end, here we perform a systematic benchmarking
of 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet),
with the aim of understanding their capabilities and limitations for realistic
atomistic simulations. In addition to our thorough evaluation and analysis on
eight existing datasets based on the benchmarking literature, we release two
new benchmark datasets, propose four new metrics, and three new challenging
tasks. The new datasets and tasks evaluate the performance of EGraFF to
out-of-distribution data, in terms of different crystal structures,
temperatures, and new molecules. Interestingly, evaluation of the EGraFF models
based on dynamic simulations reveals that having a lower error on energy or
force does not guarantee stable or reliable simulation or faithful replication
of the atomic structures. Moreover, we find that no model clearly outperforms
other models on all datasets and tasks. Importantly, we show that the
performance of all the models on out-of-distribution datasets is unreliable,
pointing to the need for the development of a foundation model for force fields
that can be used in real-world simulations. In summary, this work establishes a
rigorous framework for evaluating machine learning force fields in the context
of atomic simulations and points to open research challenges within this
domain.
</p></li>
</ul>

<h3>Title: scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain. (arXiv:2310.02713v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02713">http://arxiv.org/abs/2310.02713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02713]] scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain(http://arxiv.org/abs/2310.02713)</code></li>
<li>Summary: <p>Single-cell RNA sequencing (scRNA-seq) has made significant strides in
unraveling the intricate cellular diversity within complex tissues. This is
particularly critical in the brain, presenting a greater diversity of cell
types than other tissue types, to gain a deeper understanding of brain function
within various cellular contexts. However, analyzing scRNA-seq data remains a
challenge due to inherent measurement noise stemming from dropout events and
the limited utilization of extensive gene expression information. In this work,
we introduce scHyena, a foundation model designed to address these challenges
and enhance the accuracy of scRNA-seq analysis in the brain. Specifically,
inspired by the recent Hyena operator, we design a novel Transformer
architecture called singe-cell Hyena (scHyena) that is equipped with a linear
adaptor layer, the positional encoding via gene-embedding, and a
{bidirectional} Hyena operator. This enables us to process full-length
scRNA-seq data without losing any information from the raw data. In particular,
our model learns generalizable features of cells and genes through pre-training
scHyena using the full length of scRNA-seq data. We demonstrate the superior
performance of scHyena compared to other benchmark methods in downstream tasks,
including cell type classification and scRNA-seq imputation.
</p></li>
</ul>

<h3>Title: Multiple Physics Pretraining for Physical Surrogate Models. (arXiv:2310.02994v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02994">http://arxiv.org/abs/2310.02994</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02994]] Multiple Physics Pretraining for Physical Surrogate Models(http://arxiv.org/abs/2310.02994)</code></li>
<li>Summary: <p>We introduce multiple physics pretraining (MPP), an autoregressive
task-agnostic pretraining approach for physical surrogate modeling. MPP
involves training large surrogate models to predict the dynamics of multiple
heterogeneous physical systems simultaneously by learning features that are
broadly useful across diverse physical tasks. In order to learn effectively in
this setting, we introduce a shared embedding and normalization strategy that
projects the fields of multiple systems into a single shared embedding space.
We validate the efficacy of our approach on both pretraining and downstream
tasks over a broad fluid mechanics-oriented benchmark. We show that a single
MPP-pretrained transformer is able to match or outperform task-specific
baselines on all pretraining sub-tasks without the need for finetuning. For
downstream tasks, we demonstrate that finetuning MPP-trained models results in
more accurate predictions across multiple time-steps on new physics compared to
training from scratch or finetuning pretrained video foundation models. We
open-source our code and model weights trained at multiple scales for
reproducibility and community experimentation.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02567">http://arxiv.org/abs/2310.02567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02567]] Improving Automatic VQA Evaluation Using Large Language Models(http://arxiv.org/abs/2310.02567)</code></li>
<li>Summary: <p>8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task.
</p></li>
</ul>

<h3>Title: Analyzing and Improving OT-based Adversarial Networks. (arXiv:2310.02611v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02611">http://arxiv.org/abs/2310.02611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02611]] Analyzing and Improving OT-based Adversarial Networks(http://arxiv.org/abs/2310.02611)</code></li>
<li>Summary: <p>Optimal Transport (OT) problem aims to find a transport plan that bridges two
distributions while minimizing a given cost function. OT theory has been widely
utilized in generative modeling. In the beginning, OT distance has been used as
a measure for assessing the distance between data and generated distributions.
Recently, OT transport map between data and prior distributions has been
utilized as a generative model. These OT-based generative models share a
similar adversarial training objective. In this paper, we begin by unifying
these OT-based adversarial methods within a single framework. Then, we
elucidate the role of each component in training dynamics through a
comprehensive analysis of this unified framework. Moreover, we suggest a simple
but novel method that improves the previously best-performing OT-based model.
Intuitively, our approach conducts a gradual refinement of the generated
distribution, progressively aligning it with the data distribution. Our
approach achieves a FID score of 2.51 on CIFAR-10, outperforming unified
OT-based adversarial approaches.
</p></li>
</ul>

<h3>Title: GETAvatar: Generative Textured Meshes for Animatable Human Avatars. (arXiv:2310.02714v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02714">http://arxiv.org/abs/2310.02714</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02714]] GETAvatar: Generative Textured Meshes for Animatable Human Avatars(http://arxiv.org/abs/2310.02714)</code></li>
<li>Summary: <p>We study the problem of 3D-aware full-body human generation, aiming at
creating animatable human avatars with high-quality textures and geometries.
Generally, two challenges remain in this field: i) existing methods struggle to
generate geometries with rich realistic details such as the wrinkles of
garments; ii) they typically utilize volumetric radiance fields and neural
renderers in the synthesis process, making high-resolution rendering
non-trivial. To overcome these problems, we propose GETAvatar, a Generative
model that directly generates Explicit Textured 3D meshes for animatable human
Avatar, with photo-realistic appearance and fine geometric details.
Specifically, we first design an articulated 3D human representation with
explicit surface modeling, and enrich the generated humans with realistic
surface details by learning from the 2D normal maps of 3D scan data. Second,
with the explicit mesh representation, we can use a rasterization-based
renderer to perform surface rendering, allowing us to achieve high-resolution
image generation efficiently. Extensive experiments demonstrate that GETAvatar
achieves state-of-the-art performance on 3D-aware human generation both in
appearance and geometry quality. Notably, GETAvatar can generate images at
512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving
upon previous methods by 2x. Our code and models will be available.
</p></li>
</ul>

<h3>Title: From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. (arXiv:2310.03003v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03003">http://arxiv.org/abs/2310.03003</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03003]] From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference(http://arxiv.org/abs/2310.03003)</code></li>
<li>Summary: <p>Large language models (LLMs) have exploded in popularity due to their new
generative capabilities that go far beyond prior state-of-the-art. These
technologies are increasingly being leveraged in various domains such as law,
finance, and medicine. However, these models carry significant computational
challenges, especially the compute and energy costs required for inference.
Inference energy costs already receive less attention than the energy costs of
training LLMs -- despite how often these large models are called on to conduct
inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see
increasing usage and deployment in various domains, a better understanding of
their resource utilization is crucial for cost-savings, scaling performance,
efficient hardware usage, and optimal inference strategies.
</p>
<p>In this paper, we describe experiments conducted to study the computational
and energy utilization of inference with LLMs. We benchmark and conduct a
preliminary analysis of the inference performance and inference energy costs of
different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta
AI on two generations of popular GPUs (NVIDIA V100 \&amp; A100) and two datasets
(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in
research and practice. We present the results of multi-node, multi-GPU
inference using model sharding across up to 32 GPUs. To our knowledge, our work
is the one of the first to study LLM inference performance from the perspective
of computational and energy resources at this scale.
</p></li>
</ul>

<h3>Title: Delta-AI: Local objectives for amortized inference in sparse graphical models. (arXiv:2310.02423v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02423">http://arxiv.org/abs/2310.02423</a></li>
<li>Code URL: https://github.com/gfnorg/delta-ai</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02423]] Delta-AI: Local objectives for amortized inference in sparse graphical models(http://arxiv.org/abs/2310.02423)</code></li>
<li>Summary: <p>We present a new algorithm for amortized inference in sparse probabilistic
graphical models (PGMs), which we call $\Delta$-amortized inference
($\Delta$-AI). Our approach is based on the observation that when the sampling
of variables in a PGM is seen as a sequence of actions taken by an agent,
sparsity of the PGM enables local credit assignment in the agent's policy
learning objective. This yields a local constraint that can be turned into a
local loss in the style of generative flow networks (GFlowNets) that enables
off-policy training but avoids the need to instantiate all the random variables
for each parameter update, thus speeding up training considerably. The
$\Delta$-AI objective matches the conditional distribution of a variable given
its Markov blanket in a tractable learned sampler, which has the structure of a
Bayesian network, with the same conditional distribution under the target PGM.
As such, the trained sampler recovers marginals and conditional distributions
of interest and enables inference of partial subsets of variables. We
illustrate $\Delta$-AI's effectiveness for sampling from synthetic PGMs and
training latent variable models with sparse factor structure.
</p></li>
</ul>

<h3>Title: GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature. (arXiv:2310.02442v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02442">http://arxiv.org/abs/2310.02442</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02442]] GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature(http://arxiv.org/abs/2310.02442)</code></li>
<li>Summary: <p>Generating diverse objects (e.g., images) using generative models (such as
GAN or VAE) has achieved impressive results in the recent years, to help solve
many design problems that are traditionally done by humans. Going beyond image
generation, we aim to find solutions to more general design problems, in which
both the diversity of the design and conformity of constraints are important.
Such a setting has applications in computer graphics, animation, industrial
design, material science, etc, in which we may want the output of the generator
to follow discrete/combinatorial constraints and penalize any deviation, which
is non-trivial with existing generative models and optimization solvers. To
address this, we propose GenCO, a novel framework that conducts end-to-end
training of deep generative models integrated with embedded combinatorial
solvers, aiming to uncover high-quality solutions aligned with nonlinear
objectives. While structurally akin to conventional generative models, GenCO
diverges in its role - it focuses on generating instances of combinatorial
optimization problems rather than final objects (e.g., images). This shift
allows finer control over the generated outputs, enabling assessments of their
feasibility and introducing an additional combinatorial loss component. We
demonstrate the effectiveness of our approach on a variety of generative tasks
characterized by combinatorial intricacies, including game level generation and
map creation for path planning, consistently demonstrating its capability to
yield diverse, high-quality solutions that reliably adhere to user-specified
combinatorial properties.
</p></li>
</ul>

<h3>Title: Dual-stage Flows-based Generative Modeling for Traceable Urban Planning. (arXiv:2310.02453v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02453">http://arxiv.org/abs/2310.02453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02453]] Dual-stage Flows-based Generative Modeling for Traceable Urban Planning(http://arxiv.org/abs/2310.02453)</code></li>
<li>Summary: <p>Urban planning, which aims to design feasible land-use configurations for
target areas, has become increasingly essential due to the high-speed
urbanization process in the modern era. However, the traditional urban planning
conducted by human designers can be a complex and onerous task. Thanks to the
advancement of deep learning algorithms, researchers have started to develop
automated planning techniques. While these models have exhibited promising
results, they still grapple with a couple of unresolved limitations: 1)
Ignoring the relationship between urban functional zones and configurations and
failing to capture the relationship among different functional zones. 2) Less
interpretable and stable generation process. To overcome these limitations, we
propose a novel generative framework based on normalizing flows, namely
Dual-stage Urban Flows (DSUF) framework. Specifically, the first stage is to
utilize zone-level urban planning flows to generate urban functional zones
based on given surrounding contexts and human guidance. Then we employ an
Information Fusion Module to capture the relationship among functional zones
and fuse the information of different aspects. The second stage is to use
configuration-level urban planning flows to obtain land-use configurations
derived from fused information. We design several experiments to indicate that
our framework can outperform compared to other generative models for the urban
planning task.
</p></li>
</ul>

<h3>Title: A Recipe for Improved Certifiable Robustness: Capacity and Data. (arXiv:2310.02513v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02513">http://arxiv.org/abs/2310.02513</a></li>
<li>Code URL: https://github.com/hukkai/liresnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02513]] A Recipe for Improved Certifiable Robustness: Capacity and Data(http://arxiv.org/abs/2310.02513)</code></li>
<li>Summary: <p>A key challenge, supported both theoretically and empirically, is that
robustness demands greater network capacity and more data than standard
training. However, effectively adding capacity under stringent Lipschitz
constraints has proven more difficult than it may seem, evident by the fact
that state-of-the-art approach tend more towards \emph{underfitting} than
overfitting. Moreover, we posit that a lack of careful exploration of the
design space for Lipshitz-based approaches has left potential performance gains
on the table. In this work, we provide a more comprehensive evaluation to
better uncover the potential of Lipschitz-based certification methods. Using a
combination of novel techniques, design optimizations, and synthesis of prior
work, we are able to significantly improve the state-of-the-art \emph{verified
robust accuracy} (VRA) for deterministic certification on a variety of
benchmark datasets, and over a range of perturbation sizes. Of particular note,
we discover that the addition of large "Cholesky-orthogonalized residual dense"
layers to the end of existing state-of-the-art Lipschitz-controlled ResNet
architectures is especially effective for increasing network capacity and
performance. Combined with filtered generative data augmentation, our final
results further the state of the art deterministic VRA by up to 8.5 percentage
points. Code is available at \url{https://github.com/hukkai/liresnet}.
</p></li>
</ul>

<h3>Title: Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs. (arXiv:2310.02619v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02619">http://arxiv.org/abs/2310.02619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02619]] Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs(http://arxiv.org/abs/2310.02619)</code></li>
<li>Summary: <p>Generating realistic time series data is important for many engineering and
scientific applications. Existing work tackles this problem using generative
adversarial networks (GANs). However, GANs are often unstable during training,
and they can suffer from mode collapse. While variational autoencoders (VAEs)
are known to be more robust to these issues, they are (surprisingly) less often
considered for time series generation. In this work, we introduce Koopman VAE
(KVAE), a new generative framework that is based on a novel design for the
model prior, and that can be optimized for either regular and irregular
training data. Inspired by Koopman theory, we represent the latent conditional
prior dynamics using a linear map. Our approach enhances generative modeling
with two desired features: (i) incorporating domain knowledge can be achieved
by leverageing spectral tools that prescribe constraints on the eigenvalues of
the linear map; and (ii) studying the qualitative behavior and stablity of the
system can be performed using tools from dynamical systems theory. Our results
show that KVAE outperforms state-of-the-art GAN and VAE methods across several
challenging synthetic and real-world time series generation benchmarks. Whether
trained on regular or irregular data, KVAE generates time series that improve
both discriminative and predictive metrics. We also present visual evidence
suggesting that KVAE learns probability density functions that better
approximate empirical ground truth distributions.
</p></li>
</ul>

<h3>Title: Local Search GFlowNets. (arXiv:2310.02710v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02710">http://arxiv.org/abs/2310.02710</a></li>
<li>Code URL: https://github.com/dbsxodud-11/ls_gfn</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02710]] Local Search GFlowNets(http://arxiv.org/abs/2310.02710)</code></li>
<li>Summary: <p>Generative Flow Networks (GFlowNets) are amortized sampling methods that
learn a distribution over discrete objects proportional to their rewards.
GFlowNets exhibit a remarkable ability to generate diverse samples, yet
occasionally struggle to consistently produce samples with high rewards due to
over-exploration on wide sample space. This paper proposes to train GFlowNets
with local search which focuses on exploiting high rewarded sample space to
resolve this issue. Our main idea is to explore the local neighborhood via
destruction and reconstruction guided by backward and forward policies,
respectively. This allows biasing the samples toward high-reward solutions,
which is not possible for a typical GFlowNet solution generation scheme which
uses the forward policy to generate the solution from scratch. Extensive
experiments demonstrate a remarkable performance improvement in several
biochemical tasks. Source code is available:
\url{https://github.com/dbsxodud-11/ls_gfn}.
</p></li>
</ul>

<h3>Title: Expected flow networks in stochastic environments and two-player zero-sum games. (arXiv:2310.02779v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02779">http://arxiv.org/abs/2310.02779</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02779]] Expected flow networks in stochastic environments and two-player zero-sum games(http://arxiv.org/abs/2310.02779)</code></li>
<li>Summary: <p>Generative flow networks (GFlowNets) are sequential sampling models trained
to match a given distribution. GFlowNets have been successfully applied to
various structured object generation tasks, sampling a diverse set of
high-reward objects quickly. We propose expected flow networks (EFlowNets),
which extend GFlowNets to stochastic environments. We show that EFlowNets
outperform other GFlowNet formulations in stochastic tasks such as protein
design. We then extend the concept of EFlowNets to adversarial environments,
proposing adversarial flow networks (AFlowNets) for two-player zero-sum games.
We show that AFlowNets learn to find above 80% of optimal moves in Connect-4
via self-play and outperform AlphaZero in tournaments.
</p></li>
</ul>

<h3>Title: A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability. (arXiv:2310.02807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02807">http://arxiv.org/abs/2310.02807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02807]] A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability(http://arxiv.org/abs/2310.02807)</code></li>
<li>Summary: <p>In the past few years, there has been an explosive surge in the use of
machine learning (ML) techniques to address combinatorial optimization (CO)
problems, especially mixed-integer linear programs (MILPs). Despite the
achievements, the limited availability of real-world instances often leads to
sub-optimal decisions and biased solver assessments, which motivates a suite of
synthetic MILP instance generation techniques. However, existing methods either
rely heavily on expert-designed formulations or struggle to capture the rich
features of real-world instances. To tackle this problem, we propose G2MILP,
which to the best of our knowledge is the first deep generative framework for
MILP instances. Specifically, G2MILP represents MILP instances as bipartite
graphs, and applies a masked variational autoencoder to iteratively corrupt and
replace parts of the original graphs to generate new ones. The appealing
feature of G2MILP is that it can learn to generate novel and realistic MILP
instances without prior expert-designed formulations, while preserving the
structures and computational hardness of real-world datasets, simultaneously.
Thus the generated instances can facilitate downstream tasks for enhancing MILP
solvers under limited data availability. We design a suite of benchmarks to
evaluate the quality of the generated MILP instances. Experiments demonstrate
that our method can produce instances that closely resemble real-world datasets
in terms of both structures and computational hardness.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: A Prototype-Based Neural Network for Image Anomaly Detection and Localization. (arXiv:2310.02576v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02576">http://arxiv.org/abs/2310.02576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02576]] A Prototype-Based Neural Network for Image Anomaly Detection and Localization(http://arxiv.org/abs/2310.02576)</code></li>
<li>Summary: <p>Image anomaly detection and localization perform not only image-level anomaly
classification but also locate pixel-level anomaly regions. Recently, it has
received much research attention due to its wide application in various fields.
This paper proposes ProtoAD, a prototype-based neural network for image anomaly
detection and localization. First, the patch features of normal images are
extracted by a deep network pre-trained on nature images. Then, the prototypes
of the normal patch features are learned by non-parametric clustering. Finally,
we construct an image anomaly localization network (ProtoAD) by appending the
feature extraction network with $L2$ feature normalization, a $1\times1$
convolutional layer, a channel max-pooling, and a subtraction operation. We use
the prototypes as the kernels of the $1\times1$ convolutional layer; therefore,
our neural network does not need a training phase and can conduct anomaly
detection and localization in an end-to-end manner. Extensive experiments on
two challenging industrial anomaly detection datasets, MVTec AD and BTAD,
demonstrate that ProtoAD achieves competitive performance compared to the
state-of-the-art methods with a higher inference speed. The source code is
available at: https://github.com/98chao/ProtoAD.
</p></li>
</ul>

<h3>Title: Improving Vision Anomaly Detection with the Guidance of Language Modality. (arXiv:2310.02821v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02821">http://arxiv.org/abs/2310.02821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02821]] Improving Vision Anomaly Detection with the Guidance of Language Modality(http://arxiv.org/abs/2310.02821)</code></li>
<li>Summary: <p>Recent years have seen a surge of interest in anomaly detection for tackling
industrial defect detection, event detection, etc. However, existing
unsupervised anomaly detectors, particularly those for the vision modality,
face significant challenges due to redundant information and sparse latent
space. Conversely, the language modality performs well due to its relatively
single data. This paper tackles the aforementioned challenges for vision
modality from a multimodal point of view. Specifically, we propose Cross-modal
Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and
Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue
and sparse space issue, respectively. CMER masks parts of the raw image and
computes the matching score with the text. Then, CMER discards irrelevant
pixels to make the detector focus on critical contents. To learn a more compact
latent space for the vision anomaly detector, CMLE learns a correlation
structure matrix from the language modality, and then the latent space of
vision modality will be learned with the guidance of the matrix. Thereafter,
the vision latent space will get semantically similar images closer. Extensive
experiments demonstrate the effectiveness of the proposed methods.
Particularly, CMG outperforms the baseline that only uses images by 16.81%.
Ablation experiments further confirm the synergy among the proposed methods, as
each component depends on the other to achieve optimal performance.
</p></li>
</ul>

<h3>Title: Delving into CLIP latent space for Video Anomaly Recognition. (arXiv:2310.02835v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02835">http://arxiv.org/abs/2310.02835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02835]] Delving into CLIP latent space for Video Anomaly Recognition(http://arxiv.org/abs/2310.02835)</code></li>
<li>Summary: <p>We tackle the complex problem of detecting and recognising anomalies in
surveillance videos at the frame level, utilising only video-level supervision.
We introduce the novel method AnomalyCLIP, the first to combine Large Language
and Vision (LLV) models, such as CLIP, with multiple instance learning for
joint video anomaly detection and classification. Our approach specifically
involves manipulating the latent CLIP feature space to identify the normal
event subspace, which in turn allows us to effectively learn text-driven
directions for abnormal events. When anomalous frames are projected onto these
directions, they exhibit a large feature magnitude if they belong to a
particular class. We also introduce a computationally efficient Transformer
architecture to model short- and long-term temporal dependencies between
frames, ultimately producing the final anomaly score and class prediction
probabilities. We compare AnomalyCLIP against state-of-the-art methods
considering three major anomaly detection benchmarks, i.e. ShanghaiTech,
UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines
in recognising video anomalies.
</p></li>
</ul>

<h3>Title: ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph Convolution. (arXiv:2310.02269v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02269">http://arxiv.org/abs/2310.02269</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02269]] ARRQP: Anomaly Resilient Real-time QoS Prediction Framework with Graph Convolution(http://arxiv.org/abs/2310.02269)</code></li>
<li>Summary: <p>In the realm of modern service-oriented architecture, ensuring Quality of
Service (QoS) is of paramount importance. The ability to predict QoS values in
advance empowers users to make informed decisions. However, achieving accurate
QoS predictions in the presence of various issues and anomalies, including
outliers, data sparsity, grey-sheep instances, and cold-start scenarios,
remains a challenge. Current state-of-the-art methods often fall short when
addressing these issues simultaneously, resulting in performance degradation.
In this paper, we introduce a real-time QoS prediction framework (called ARRQP)
with a specific emphasis on improving resilience to anomalies in the data.
ARRQP utilizes the power of graph convolution techniques to capture intricate
relationships and dependencies among users and services, even when the data is
limited or sparse. ARRQP integrates both contextual information and
collaborative insights, enabling a comprehensive understanding of user-service
interactions. By utilizing robust loss functions, ARRQP effectively reduces the
impact of outliers during the model training. Additionally, we introduce a
sparsity-resilient grey-sheep detection method, which is subsequently treated
separately for QoS prediction. Furthermore, we address the cold-start problem
by emphasizing contextual features over collaborative features. Experimental
results on the benchmark WS-DREAM dataset demonstrate the framework's
effectiveness in achieving accurate and timely QoS predictions.
</p></li>
</ul>

<h3>Title: Expert enhanced dynamic time warping based anomaly detection. (arXiv:2310.02280v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02280">http://arxiv.org/abs/2310.02280</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02280]] Expert enhanced dynamic time warping based anomaly detection(http://arxiv.org/abs/2310.02280)</code></li>
<li>Summary: <p>Dynamic time warping (DTW) is a well-known algorithm for time series elastic
dissimilarity measure. Its ability to deal with non-linear time distortions
makes it helpful in variety of data mining tasks. Such a task is also anomaly
detection which attempts to reveal unexpected behaviour without false detection
alarms. In this paper, we propose a novel anomaly detection method named Expert
enhanced dynamic time warping anomaly detection (E-DTWA). It is based on DTW
with additional enhancements involving human-in-the-loop concept. The main
benefits of our approach comprise efficient detection, flexible retraining
based on strong consideration of the expert's detection feedback while
retaining low computational and space complexity.
</p></li>
</ul>

<h3>Title: Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02861">http://arxiv.org/abs/2310.02861</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02861]] Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection(http://arxiv.org/abs/2310.02861)</code></li>
<li>Summary: <p>Graph-level anomaly detection has gained significant attention as it finds
many applications in various domains, such as cancer diagnosis and enzyme
prediction. However, existing methods fail to capture the underlying properties
of graph anomalies, resulting in unexplainable framework design and
unsatisfying performance. In this paper, we take a step back and re-investigate
the spectral differences between anomalous and normal graphs. Our main
observation shows a significant disparity in the accumulated spectral energy
between these two classes. Moreover, we prove that the accumulated spectral
energy of the graph signal can be represented by its Rayleigh Quotient,
indicating that the Rayleigh Quotient is a driving factor behind the anomalous
properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph
Neural Network (RQGNN), the first spectral GNN for graph-level anomaly
detection, providing a new perspective on exploring the inherent spectral
features of anomalous graphs. Specifically, we introduce a novel framework that
consists of two components: the Rayleigh Quotient learning component (RQL) and
Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly captures the
Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the spectral space
of graphs. Extensive experiments on 10 real-world datasets show that RQGNN
outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in AUC,
demonstrating the effectiveness of our framework.
</p></li>
</ul>

<h3>Title: ELUQuant: Event-Level Uncertainty Quantification in Deep Inelastic Scattering. (arXiv:2310.02913v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02913">http://arxiv.org/abs/2310.02913</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02913]] ELUQuant: Event-Level Uncertainty Quantification in Deep Inelastic Scattering(http://arxiv.org/abs/2310.02913)</code></li>
<li>Summary: <p>We introduce a physics-informed Bayesian Neural Network (BNN) with flow
approximated posteriors using multiplicative normalizing flows (MNF) for
detailed uncertainty quantification (UQ) at the physics event-level. Our method
is capable of identifying both heteroskedastic aleatoric and epistemic
uncertainties, providing granular physical insights. Applied to Deep Inelastic
Scattering (DIS) events, our model effectively extracts the kinematic variables
$x$, $Q^2$, and $y$, matching the performance of recent deep learning
regression techniques but with the critical enhancement of event-level UQ. This
detailed description of the underlying uncertainty proves invaluable for
decision-making, especially in tasks like event filtering. It also allows for
the reduction of true inaccuracies without directly accessing the ground truth.
A thorough DIS simulation using the H1 detector at HERA indicates possible
applications for the future EIC. Additionally, this paves the way for related
tasks such as data quality monitoring and anomaly detection. Remarkably, our
approach effectively processes large samples at high rates.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.02954">http://arxiv.org/abs/2310.02954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.02954]] DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning(http://arxiv.org/abs/2310.02954)</code></li>
<li>Summary: <p>Recent advances in natural language processing, primarily propelled by Large
Language Models (LLMs), have showcased their remarkable capabilities grounded
in in-context learning. A promising avenue for guiding LLMs in intricate
reasoning tasks involves the utilization of intermediate reasoning steps within
the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies
in the effective selection of exemplars for facilitating in-context learning.
In this study, we introduce a framework that leverages Dual Queries and
Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars
for in-context learning. Dual Queries first query LLM to obtain LLM-generated
knowledge such as CoT, then query the retriever to obtain the final exemplars
via both question and the knowledge. Moreover, for the second query, LoRe
employs dimensionality reduction techniques to refine exemplar selection,
ensuring close alignment with the input question's knowledge. Through extensive
experiments, we demonstrate that DQ-LoRe significantly outperforms prior
state-of-the-art methods in the automatic selection of exemplars for GPT-4,
enhancing performance from 92.5\% to 94.2\%. Our comprehensive analysis further
reveals that DQ-LoRe consistently outperforms retrieval-based approaches in
terms of both performance and adaptability, especially in scenarios
characterized by distribution shifts. DQ-LoRe pushes the boundaries of
in-context learning and opens up new avenues for addressing complex reasoning
challenges. We will release the code soon.
</p></li>
</ul>

<h3>Title: Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions. (arXiv:2310.03016v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.03016">http://arxiv.org/abs/2310.03016</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.03016]] Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions(http://arxiv.org/abs/2310.03016)</code></li>
<li>Summary: <p>In order to understand the in-context learning phenomenon, recent works have
adopted a stylized experimental framework and demonstrated that Transformers
can learn gradient-based learning algorithms for various classes of real-valued
functions. However, the limitations of Transformers in implementing learning
algorithms, and their ability to learn other forms of algorithms are not well
understood. Additionally, the degree to which these capabilities are confined
to attention-based models is unclear. Furthermore, it remains to be seen
whether the insights derived from these stylized settings can be extrapolated
to pretrained Large Language Models (LLMs). In this work, we take a step
towards answering these questions by demonstrating the following: (a) On a
test-bed with a variety of Boolean function classes, we find that Transformers
can nearly match the optimal learning algorithm for 'simpler' tasks, while
their performance deteriorates on more 'complex' tasks. Additionally, we find
that certain attention-free models perform (almost) identically to Transformers
on a range of tasks. (b) When provided a teaching sequence, i.e. a set of
examples that uniquely identifies a function in a class, we show that
Transformers learn more sample-efficiently. Interestingly, our results show
that Transformers can learn to implement two distinct algorithms to solve a
single task, and can adaptively select the more sample-efficient algorithm
depending on the sequence of in-context examples. (c) Lastly, we show that
extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines
on prediction tasks that are guaranteed to not be in their training set.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
