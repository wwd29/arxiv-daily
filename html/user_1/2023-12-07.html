<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Diff-GO: Diffusion Goal-Oriented Communications to Achieve Ultra-High Spectrum Efficiency. (arXiv:2312.02984v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02984">http://arxiv.org/abs/2312.02984</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02984]] Diff-GO: Diffusion Goal-Oriented Communications to Achieve Ultra-High Spectrum Efficiency(http://arxiv.org/abs/2312.02984)</code></li>
<li>Summary: <p>The latest advances in artificial intelligence (AI) present many
unprecedented opportunities to achieve much improved bandwidth saving in
communications. Unlike conventional communication systems focusing on packet
transport, rich datasets and AI makes it possible to efficiently transfer only
the information most critical to the goals of message recipients. One of the
most exciting advances in generative AI known as diffusion model presents a
unique opportunity for designing ultra-fast communication systems well beyond
language-based messages. This work presents an ultra-efficient communication
design by utilizing generative AI-based on diffusion models as a specific
example of the general goal-oriented communication framework. To better control
the regenerated message at the receiver output, our diffusion system design
includes a local regeneration module with finite dimensional noise latent. The
critical significance of noise latent control and sharing residing on our
Diff-GO is the ability to introduce the concept of "local generative feedback"
(Local-GF), which enables the transmitter to monitor the quality and gauge the
quality or accuracy of the message recovery at the semantic system receiver. To
this end, we propose a new low-dimensional noise space for the training of
diffusion models, which significantly reduces the communication overhead and
achieves satisfactory message recovery performance. Our experimental results
demonstrate that the proposed noise space and the diffusion-based generative
model achieve ultra-high spectrum efficiency and accurate recovery of
transmitted image signals. By trading off computation for bandwidth efficiency
(C4BE), this new framework provides an important avenue to achieve exceptional
computation-bandwidth tradeoff.
</p></li>
</ul>

<h3>Title: DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance. (arXiv:2312.03018v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03018">http://arxiv.org/abs/2312.03018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03018]] DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance(http://arxiv.org/abs/2312.03018)</code></li>
<li>Summary: <p>Image-to-video generation, which aims to generate a video starting from a
given reference image, has drawn great attention. Existing methods try to
extend pre-trained text-guided image diffusion models to image-guided video
generation models. Nevertheless, these methods often result in either low
fidelity or flickering over time due to their limitation to shallow image
guidance and poor temporal consistency. To tackle these problems, we propose a
high-fidelity image-to-video generation method by devising a frame retention
branch on the basis of a pre-trained video diffusion model, named DreamVideo.
Instead of integrating the reference image into the diffusion process in a
semantic level, our DreamVideo perceives the reference image via convolution
layers and concatenate the features with the noisy latents as model input. By
this means, the details of the reference image can be preserved to the greatest
extent. In addition, by incorporating double-condition classifier-free
guidance, a single image can be directed to videos of different actions by
providing varying prompt texts. This has significant implications for
controllable video generation and holds broad application prospects. We conduct
comprehensive experiments on the public dataset, both quantitative and
qualitative results indicate that our method outperforms the state-of-the-art
method. Especially for fidelity, our model has powerful image retention ability
and result in high FVD in UCF101 compared to other image-to-video models. Also,
precise control can be achieved by giving different text prompts. Further
details and comprehensive results of our model will be presented in
https://anonymous0769.github.io/DreamVideo/.
</p></li>
</ul>

<h3>Title: Stable Diffusion Exposed: Gender Bias from Prompt to Image. (arXiv:2312.03027v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03027">http://arxiv.org/abs/2312.03027</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03027]] Stable Diffusion Exposed: Gender Bias from Prompt to Image(http://arxiv.org/abs/2312.03027)</code></li>
<li>Summary: <p>Recent studies have highlighted biases in generative models, shedding light
on their predisposition towards gender-based stereotypes and imbalances. This
paper contributes to this growing body of research by introducing an evaluation
protocol designed to automatically analyze the impact of gender indicators on
Stable Diffusion images. Leveraging insights from prior work, we explore how
gender indicators not only affect gender presentation but also the
representation of objects and layouts within the generated images. Our findings
include the existence of differences in the depiction of objects, such as
instruments tailored for specific genders, and shifts in overall layouts. We
also reveal that neutral prompts tend to produce images more aligned with
masculine prompts than their feminine counterparts, providing valuable insights
into the nuanced gender biases inherent in Stable Diffusion.
</p></li>
</ul>

<h3>Title: Customization Assistant for Text-to-image Generation. (arXiv:2312.03045v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03045">http://arxiv.org/abs/2312.03045</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03045]] Customization Assistant for Text-to-image Generation(http://arxiv.org/abs/2312.03045)</code></li>
<li>Summary: <p>Customizing pre-trained text-to-image generation model has attracted massive
research interest recently, due to its huge potential in real-world
applications. Although existing methods are able to generate creative content
for a novel concept contained in single user-input image, their capability are
still far from perfection. Specifically, most existing methods require
fine-tuning the generative model on testing images. Some existing methods do
not require fine-tuning, while their performance are unsatisfactory.
Furthermore, the interaction between users and models are still limited to
directive and descriptive prompts such as instructions and captions. In this
work, we build a customization assistant based on pre-trained large language
model and diffusion model, which can not only perform customized generation in
a tuning-free manner, but also enable more user-friendly interactions: users
can chat with the assistant and input either ambiguous text or clear
instruction. Specifically, we propose a new framework consists of a new model
design and a novel training strategy. The resulting assistant can perform
customized generation in 2-5 seconds without any test time fine-tuning.
Extensive experiments are conducted, competitive results have been obtained
across different domains, illustrating the effectiveness of the proposed
method.
</p></li>
</ul>

<h3>Title: MagicStick: Controllable Video Editing via Control Handle Transformations. (arXiv:2312.03047v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03047">http://arxiv.org/abs/2312.03047</a></li>
<li>Code URL: https://github.com/mayuelala/magicstick</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03047]] MagicStick: Controllable Video Editing via Control Handle Transformations(http://arxiv.org/abs/2312.03047)</code></li>
<li>Summary: <p>Text-based video editing has recently attracted considerable interest in
changing the style or replacing the objects with a similar structure. Beyond
this, we demonstrate that properties such as shape, size, location, motion,
etc., can also be edited in videos. Our key insight is that the keyframe
transformations of the specific internal feature (e.g., edge maps of objects or
human pose), can easily propagate to other frames to provide generation
guidance. We thus propose MagicStick, a controllable video editing method that
edits the video properties by utilizing the transformation on the extracted
internal control signals. In detail, to keep the appearance, we inflate both
the pretrained image diffusion model and ControlNet to the temporal dimension
and train low-rank adaptions (LORA) layers to fit the specific scenes. Then, in
editing, we perform an inversion and editing framework. Differently, finetuned
ControlNet is introduced in both inversion and generation for attention
guidance with the proposed attention remix between the spatial attention maps
of inversion and editing. Yet succinct, our method is the first method to show
the ability of video property editing from the pre-trained text-to-image model.
We present experiments on numerous examples within our unified framework. We
also compare with shape-aware text-based editing and handcrafted motion video
generation, demonstrating our superior temporal consistency and editing
capability than previous works. The code and models will be made publicly
available.
</p></li>
</ul>

<h3>Title: DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control. (arXiv:2312.03048v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03048">http://arxiv.org/abs/2312.03048</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03048]] DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control(http://arxiv.org/abs/2312.03048)</code></li>
<li>Summary: <p>Large, pretrained latent diffusion models (LDMs) have demonstrated an
extraordinary ability to generate creative content, specialize to user data
through few-shot fine-tuning, and condition their output on other modalities,
such as semantic maps. However, are they usable as large-scale data generators,
e.g., to improve tasks in the perception stack, like semantic segmentation? We
investigate this question in the context of autonomous driving, and answer it
with a resounding "yes". We propose an efficient data generation pipeline
termed DGInStyle. First, we examine the problem of specializing a pretrained
LDM to semantically-controlled generation within a narrow domain. Second, we
design a Multi-resolution Latent Fusion technique to overcome the bias of LDMs
towards dominant objects. Third, we propose a Style Swap technique to endow the
rich generative prior with the learned semantic control. Using DGInStyle, we
generate a diverse dataset of street scenes, train a domain-agnostic semantic
segmentation model on it, and evaluate the model on multiple popular autonomous
driving datasets. Our approach consistently increases the performance of
several domain generalization methods, in some cases by +2.5 mIoU compared to
the previous state-of-the-art method without our generative augmentation
scheme. Source code and dataset are available at https://dginstyle.github.io .
</p></li>
</ul>

<h3>Title: DiffusionPCR: Diffusion Models for Robust Multi-Step Point Cloud Registration. (arXiv:2312.03053v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03053">http://arxiv.org/abs/2312.03053</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03053]] DiffusionPCR: Diffusion Models for Robust Multi-Step Point Cloud Registration(http://arxiv.org/abs/2312.03053)</code></li>
<li>Summary: <p>Point Cloud Registration (PCR) estimates the relative rigid transformation
between two point clouds. We propose formulating PCR as a denoising diffusion
probabilistic process, mapping noisy transformations to the ground truth.
However, using diffusion models for PCR has nontrivial challenges, such as
adapting a generative model to a discriminative task and leveraging the
estimated nonlinear transformation from the previous step. Instead of training
a diffusion model to directly map pure noise to ground truth, we map the
predictions of an off-the-shelf PCR model to ground truth. The predictions of
off-the-shelf models are often imperfect, especially in challenging cases where
the two points clouds have low overlap, and thus could be seen as noisy
versions of the real rigid transformation. In addition, we transform the
rotation matrix into a spherical linear space for interpolation between samples
in the forward process, and convert rigid transformations into auxiliary
information to implicitly exploit last-step estimations in the reverse process.
As a result, conditioned on time step, the denoising model adapts to the
increasing accuracy across steps and refines registrations. Our extensive
experiments showcase the effectiveness of our DiffusionPCR, yielding
state-of-the-art registration recall rates (95.3%/81.6%) on 3DMatch and
3DLoMatch. The code will be made public upon publication.
</p></li>
</ul>

<h3>Title: LooseControl: Lifting ControlNet for Generalized Depth Conditioning. (arXiv:2312.03079v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03079">http://arxiv.org/abs/2312.03079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03079]] LooseControl: Lifting ControlNet for Generalized Depth Conditioning(http://arxiv.org/abs/2312.03079)</code></li>
<li>Summary: <p>We present LooseControl to allow generalized depth conditioning for
diffusion-based image generation. ControlNet, the SOTA for depth-conditioned
image generation, produces remarkable results but relies on having access to
detailed depth maps for guidance. Creating such exact depth maps, in many
scenarios, is challenging. This paper introduces a generalized version of depth
conditioning that enables many new content-creation workflows. Specifically, we
allow (C1) scene boundary control for loosely specifying scenes with only
boundary conditions, and (C2) 3D box control for specifying layout locations of
the target objects rather than the exact shape and appearance of the objects.
Using LooseControl, along with text guidance, users can create complex
environments (e.g., rooms, street views, etc.) by specifying only scene
boundaries and locations of primary objects. Further, we provide two editing
mechanisms to refine the results: (E1) 3D box editing enables the user to
refine images by changing, adding, or removing boxes while freezing the style
of the image. This yields minimal changes apart from changes induced by the
edited boxes. (E2) Attribute editing proposes possible editing directions to
change one particular aspect of the scene, such as the overall object density
or a particular object. Extensive tests and comparisons with baselines
demonstrate the generality of our method. We believe that LooseControl can
become an important design tool for easily creating complex environments and be
extended to other forms of guidance channels. Code and more information are
available at https://shariqfarooq123.github.io/loose-control/ .
</p></li>
</ul>

<h3>Title: ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet. (arXiv:2312.03154v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03154">http://arxiv.org/abs/2312.03154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03154]] ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet(http://arxiv.org/abs/2312.03154)</code></li>
<li>Summary: <p>This paper introduces ViscoNet, a novel method that enhances text-to-image
human generation models with visual prompting. Unlike existing methods that
rely on lengthy text descriptions to control the image structure, ViscoNet
allows users to specify the visual appearance of the target object with a
reference image. ViscoNet disentangles the object's appearance from the image
background and injects it into a pre-trained latent diffusion model (LDM) model
via a ControlNet branch. This way, ViscoNet mitigates the style mode collapse
problem and enables precise and flexible visual control. We demonstrate the
effectiveness of ViscoNet on human image generation, where it can manipulate
visual attributes and artistic styles with text and image prompts. We also show
that ViscoNet can learn visual conditioning from small and specific object
domains while preserving the generative power of the LDM backbone.
</p></li>
</ul>

<h3>Title: Cache Me if You Can: Accelerating Diffusion Models through Block Caching. (arXiv:2312.03209v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03209">http://arxiv.org/abs/2312.03209</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03209]] Cache Me if You Can: Accelerating Diffusion Models through Block Caching(http://arxiv.org/abs/2312.03209)</code></li>
<li>Summary: <p>Diffusion models have recently revolutionized the field of image synthesis
due to their ability to generate photorealistic images. However, one of the
major drawbacks of diffusion models is that the image generation process is
costly. A large image-to-image network has to be applied many times to
iteratively refine an image from random noise. While many recent works propose
techniques to reduce the number of required steps, they generally treat the
underlying denoising network as a black box. In this work, we investigate the
behavior of the layers within the network and find that 1) the layers' output
changes smoothly over time, 2) the layers show distinct patterns of change, and
3) the change from step to step is often very small. We hypothesize that many
layer computations in the denoising network are redundant. Leveraging this, we
introduce block caching, in which we reuse outputs from layer blocks of
previous steps to speed up inference. Furthermore, we propose a technique to
automatically determine caching schedules based on each block's changes over
timesteps. In our experiments, we show through FID, human evaluation and
qualitative analysis that Block Caching allows to generate images with higher
visual quality at the same computational cost. We demonstrate this for
different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM).
</p></li>
</ul>

<h3>Title: DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction. (arXiv:2312.03298v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03298">http://arxiv.org/abs/2312.03298</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03298]] DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction(http://arxiv.org/abs/2312.03298)</code></li>
<li>Summary: <p>Point cloud streaming is increasingly getting popular, evolving into the norm
for interactive service delivery and the future Metaverse. However, the
substantial volume of data associated with point clouds presents numerous
challenges, particularly in terms of high bandwidth consumption and large
storage capacity. Despite various solutions proposed thus far, with a focus on
point cloud compression, upsampling, and completion, these
reconstruction-related methods continue to fall short in delivering high
fidelity point cloud output. As a solution, in DiffPMAE, we propose an
effective point cloud reconstruction architecture. Inspired by self-supervised
learning concepts, we combine Masked Auto-Encoding and Diffusion Model
mechanism to remotely reconstruct point cloud data. By the nature of this
reconstruction process, DiffPMAE can be extended to many related downstream
tasks including point cloud compression, upsampling and completion. Leveraging
ShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the
performance of DiffPMAE exceeding many state-of-the-art methods in-terms of
auto-encoding and downstream tasks considered.
</p></li>
</ul>

<h3>Title: F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis. (arXiv:2312.03459v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03459">http://arxiv.org/abs/2312.03459</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03459]] F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis(http://arxiv.org/abs/2312.03459)</code></li>
<li>Summary: <p>Recently Text-to-Video (T2V) synthesis has undergone a breakthrough by
training transformers or diffusion models on large-scale datasets.
Nevertheless, inferring such large models incurs huge costs.Previous inference
acceleration works either require costly retraining or are model-specific.To
address this issue, instead of retraining we explore the inference process of
two mainstream T2V models using transformers and diffusion models.The
exploration reveals the redundancy in temporal attention modules of both
models, which are commonly utilized to establish temporal relations among
frames.Consequently, we propose a training-free and generalized pruning
strategy called F3-Pruning to prune redundant temporal attention
weights.Specifically, when aggregate temporal attention values are ranked below
a certain ratio, corresponding weights will be pruned.Extensive experiments on
three datasets using a classic transformer-based model CogVideo and a typical
diffusion-based model Tune-A-Video verify the effectiveness of F3-Pruning in
inference acceleration, quality assurance and broad applicability.
</p></li>
</ul>

<h3>Title: Kandinsky 3.0 Technical Report. (arXiv:2312.03511v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03511">http://arxiv.org/abs/2312.03511</a></li>
<li>Code URL: https://github.com/ai-forever/movqgan</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03511]] Kandinsky 3(http://arxiv.org/abs/2312.03511)</code></li>
<li>Summary: <p>We present Kandinsky 3.0, a large-scale text-to-image generation model based
on latent diffusion, continuing the series of text-to-image Kandinsky models
and reflecting our progress to achieve higher quality and realism of image
generation. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0
leverages a two times larger U-Net backbone, a ten times larger text encoder
and removes diffusion mapping. We describe the architecture of the model, the
data collection procedure, the training technique, and the production system of
user interaction. We focus on the key components that, as we have identified as
a result of a large number of experiments, had the most significant impact on
improving the quality of our model compared to the others. By our side-by-side
comparisons, Kandinsky becomes better in text understanding and works better on
specific domains. Project page: https://ai-forever.github.io/Kandinsky-3
</p></li>
</ul>

<h3>Title: FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models. (arXiv:2312.03517v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03517">http://arxiv.org/abs/2312.03517</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03517]] FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models(http://arxiv.org/abs/2312.03517)</code></li>
<li>Summary: <p>The substantial computational costs of diffusion models, particularly due to
the repeated denoising steps crucial for high-quality image generation, present
a major obstacle to their widespread adoption. While several studies have
attempted to address this issue by reducing the number of score function
evaluations using advanced ODE solvers without fine-tuning, the decreased
number of denoising iterations misses the opportunity to update fine details,
resulting in noticeable quality degradation. In our work, we introduce an
advanced acceleration technique that leverages the temporal redundancy inherent
in diffusion models. Reusing feature maps with high temporal similarity opens
up a new opportunity to save computation without sacrificing output quality. To
realize the practical benefits of this intuition, we conduct an extensive
analysis and propose a novel method, FRDiff. FRDiff is designed to harness the
advantages of both reduced NFE and feature reuse, achieving a Pareto frontier
that balances fidelity and latency trade-offs in various generative tasks.
</p></li>
</ul>

<h3>Title: FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation. (arXiv:2312.03540v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03540">http://arxiv.org/abs/2312.03540</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03540]] FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation(http://arxiv.org/abs/2312.03540)</code></li>
<li>Summary: <p>Current state-of-the-art image generation models such as Latent Diffusion
Models (LDMs) have demonstrated the capacity to produce visually striking
food-related images. However, these generated images often exhibit an artistic
or surreal quality that diverges from the authenticity of real-world food
representations. This inadequacy renders them impractical for applications
requiring realistic food imagery, such as training models for image-based
dietary assessment. To address these limitations, we introduce FoodFusion, a
Latent Diffusion model engineered specifically for the faithful synthesis of
realistic food images from textual descriptions. The development of the
FoodFusion model involves harnessing an extensive array of open-source food
datasets, resulting in over 300,000 curated image-caption pairs. Additionally,
we propose and employ two distinct data cleaning methodologies to ensure that
the resulting image-text pairs maintain both realism and accuracy. The
FoodFusion model, thus trained, demonstrates a remarkable ability to generate
food images that exhibit a significant improvement in terms of both realism and
diversity over the publicly available image generation models. We openly share
the dataset and fine-tuned models to support advancements in this critical
field of food image synthesis at https://bit.ly/genai4good.
</p></li>
</ul>

<h3>Title: Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention. (arXiv:2312.03556v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03556">http://arxiv.org/abs/2312.03556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03556]] Personalized Face Inpainting with Diffusion Models by Parallel Visual Attention(http://arxiv.org/abs/2312.03556)</code></li>
<li>Summary: <p>Face inpainting is important in various applications, such as photo
restoration, image editing, and virtual reality. Despite the significant
advances in face generative models, ensuring that a person's unique facial
identity is maintained during the inpainting process is still an elusive goal.
Current state-of-the-art techniques, exemplified by MyStyle, necessitate
resource-intensive fine-tuning and a substantial number of images for each new
identity. Furthermore, existing methods often fall short in accommodating
user-specified semantic attributes, such as beard or expression. To improve
inpainting results, and reduce the computational complexity during inference,
this paper proposes the use of Parallel Visual Attention (PVA) in conjunction
with diffusion models. Specifically, we insert parallel attention matrices to
each cross-attention module in the denoising network, which attends to features
extracted from reference images by an identity encoder. We train the added
attention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for
identity-preserving face inpainting. Experiments demonstrate that PVA attains
unparalleled identity resemblance in both face inpainting and face inpainting
with language guidance tasks, in comparison to various benchmarks, including
MyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA
ensures good identity preservation while offering effective
language-controllability. Additionally, in contrast to Custom Diffusion, PVA
requires just 40 fine-tuning steps for each new identity, which translates to a
significant speed increase of over 20 times.
</p></li>
</ul>

<h3>Title: Context Diffusion: In-Context Aware Image Generation. (arXiv:2312.03584v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03584">http://arxiv.org/abs/2312.03584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03584]] Context Diffusion: In-Context Aware Image Generation(http://arxiv.org/abs/2312.03584)</code></li>
<li>Summary: <p>We propose Context Diffusion, a diffusion-based framework that enables image
generation models to learn from visual examples presented in context. Recent
work tackles such in-context learning for image generation, where a query image
is provided alongside context examples and text prompts. However, the quality
and fidelity of the generated images deteriorate when the prompt is not
present, demonstrating that these models are unable to truly learn from the
visual context. To address this, we propose a novel framework that separates
the encoding of the visual context and preserving the structure of the query
images. This results in the ability to learn from the visual context and text
prompts, but also from either one of them. Furthermore, we enable our model to
handle few-shot settings, to effectively address diverse in-context learning
scenarios. Our experiments and user study demonstrate that Context Diffusion
excels in both in-domain and out-of-domain tasks, resulting in an overall
enhancement in image quality and fidelity compared to counterpart models.
</p></li>
</ul>

<h3>Title: DiffusionSat: A Generative Foundation Model for Satellite Imagery. (arXiv:2312.03606v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03606">http://arxiv.org/abs/2312.03606</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03606]] DiffusionSat: A Generative Foundation Model for Satellite Imagery(http://arxiv.org/abs/2312.03606)</code></li>
<li>Summary: <p>Diffusion models have achieved state-of-the-art results on many modalities
including images, speech, and video. However, existing models are not tailored
to support remote sensing data, which is widely used in important applications
including environmental monitoring and crop-yield prediction. Satellite images
are significantly different from natural images -- they can be multi-spectral,
irregularly sampled across time -- and existing diffusion models trained on
images from the Web do not support them. Furthermore, remote sensing data is
inherently spatio-temporal, requiring conditional generation tasks not
supported by traditional methods based on captions or images. In this paper, we
present DiffusionSat, to date the largest generative foundation model trained
on a collection of publicly available large, high-resolution remote sensing
datasets. As text-based captions are sparsely available for satellite images,
we incorporate the associated metadata such as geolocation as conditioning
information. Our method produces realistic samples and can be used to solve
multiple generative tasks including temporal generation, superresolution given
multi-spectral inputs and in-painting. Our method outperforms previous
state-of-the-art methods for satellite image generation and is the first
large-scale $\textit{generative}$ foundation model for satellite imagery.
</p></li>
</ul>

<h3>Title: DreamComposer: Controllable 3D Object Generation via Multi-View Conditions. (arXiv:2312.03611v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03611">http://arxiv.org/abs/2312.03611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03611]] DreamComposer: Controllable 3D Object Generation via Multi-View Conditions(http://arxiv.org/abs/2312.03611)</code></li>
<li>Summary: <p>Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</p></li>
</ul>

<h3>Title: TokenCompose: Grounding Diffusion with Token-level Supervision. (arXiv:2312.03626v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03626">http://arxiv.org/abs/2312.03626</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03626]] TokenCompose: Grounding Diffusion with Token-level Supervision(http://arxiv.org/abs/2312.03626)</code></li>
<li>Summary: <p>We present TokenCompose, a Latent Diffusion Model for text-to-image
generation that achieves enhanced consistency between user-specified text
prompts and model-generated images. Despite its tremendous success, the
standard denoising process in the Latent Diffusion Model takes text prompts as
conditions only, absent explicit constraint for the consistency between the
text prompts and the image contents, leading to unsatisfactory results for
composing multiple object categories. TokenCompose aims to improve
multi-category instance composition by introducing the token-wise consistency
terms between the image content and object segmentation maps in the finetuning
stage. TokenCompose can be applied directly to the existing training pipeline
of text-conditioned diffusion models without extra human labeling information.
By finetuning Stable Diffusion, the model exhibits significant improvements in
multi-category instance composition and enhanced photorealism for its generated
images.
</p></li>
</ul>

<h3>Title: WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on. (arXiv:2312.03667v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03667">http://arxiv.org/abs/2312.03667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03667]] WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on(http://arxiv.org/abs/2312.03667)</code></li>
<li>Summary: <p>Image-based Virtual Try-On (VITON) aims to transfer an in-shop garment image
onto a target person. While existing methods focus on warping the garment to
fit the body pose, they often overlook the synthesis quality around the
garment-skin boundary and realistic effects like wrinkles and shadows on the
warped garments. These limitations greatly reduce the realism of the generated
results and hinder the practical application of VITON techniques. Leveraging
the notable success of diffusion-based models in cross-modal image synthesis,
some recent diffusion-based methods have ventured to tackle this issue.
However, they tend to either consume a significant amount of training resources
or struggle to achieve realistic try-on effects and retain garment details. For
efficient and high-fidelity VITON, we propose WarpDiffusion, which bridges the
warping-based and diffusion-based paradigms via a novel informative and local
garment feature attention mechanism. Specifically, WarpDiffusion incorporates
local texture attention to reduce resource consumption and uses a novel
auto-mask module that effectively retains only the critical areas of the warped
garment while disregarding unrealistic or erroneous portions. Notably,
WarpDiffusion can be integrated as a plug-and-play component into existing
VITON methodologies, elevating their synthesis quality. Extensive experiments
on high-resolution VITON benchmarks and an in-the-wild test set demonstrate the
superiority of WarpDiffusion, surpassing state-of-the-art methods both
qualitatively and quantitatively.
</p></li>
</ul>

<h3>Title: Self-conditioned Image Generation via Generating Representations. (arXiv:2312.03701v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03701">http://arxiv.org/abs/2312.03701</a></li>
<li>Code URL: https://github.com/LTH14/rcg</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03701]] Self-conditioned Image Generation via Generating Representations(http://arxiv.org/abs/2312.03701)</code></li>
<li>Summary: <p>This paper presents $\textbf{R}$epresentation-$\textbf{C}$onditioned image
$\textbf{G}$eneration (RCG), a simple yet effective image generation framework
which sets a new benchmark in class-unconditional image generation. RCG does
not condition on any human annotations. Instead, it conditions on a
self-supervised representation distribution which is mapped from the image
distribution using a pre-trained encoder. During generation, RCG samples from
such representation distribution using a representation diffusion model (RDM),
and employs a pixel generator to craft image pixels conditioned on the sampled
representation. Such a design provides substantial guidance during the
generative process, resulting in high-quality image generation. Tested on
ImageNet 256$\times$256, RCG achieves a Frechet Inception Distance (FID) of
3.31 and an Inception Score (IS) of 253.4. These results not only significantly
improve the state-of-the-art of class-unconditional image generation but also
rival the current leading methods in class-conditional image generation,
bridging the long-standing performance gap between these two tasks. Code is
available at https://github.com/LTH14/rcg.
</p></li>
</ul>

<h3>Title: Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning. (arXiv:2312.03397v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03397">http://arxiv.org/abs/2312.03397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03397]] Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning(http://arxiv.org/abs/2312.03397)</code></li>
<li>Summary: <p>We present Generalized Contrastive Divergence (GCD), a novel objective
function for training an energy-based model (EBM) and a sampler simultaneously.
GCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm
for training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution
with a trainable sampler, such as a diffusion model. In GCD, the joint training
of EBM and a diffusion model is formulated as a minimax problem, which reaches
an equilibrium when both models converge to the data distribution. The minimax
learning with GCD bears interesting equivalence to inverse reinforcement
learning, where the energy corresponds to a negative reward, the diffusion
model is a policy, and the real data is expert demonstrations. We present
preliminary yet promising results showing that joint training is beneficial for
both EBM and a diffusion model. GCD enables EBM training without MCMC while
improving the sample quality of a diffusion model.
</p></li>
</ul>

<h3>Title: Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion. (arXiv:2312.03475v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03475">http://arxiv.org/abs/2312.03475</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03475]] Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion(http://arxiv.org/abs/2312.03475)</code></li>
<li>Summary: <p>Recently, artificial intelligence for drug discovery has raised increasing
interest in both machine learning and chemistry domains. The fundamental
building block for drug discovery is molecule geometry and thus, the molecule's
geometrical representation is the main bottleneck to better utilize machine
learning techniques for drug discovery. In this work, we propose a pretraining
method for molecule joint auto-encoding (MoleculeJAE). MoleculeJAE can learn
both the 2D bond (topology) and 3D conformation (geometry) information, and a
diffusion process model is applied to mimic the augmented trajectories of such
two modalities, based on which, MoleculeJAE will learn the inherent chemical
structure in a self-supervised manner. Thus, the pretrained geometrical
representation in MoleculeJAE is expected to benefit downstream
geometry-related tasks. Empirically, MoleculeJAE proves its effectiveness by
reaching state-of-the-art performance on 15 out of 20 tasks by comparing it
with 12 competitive baselines.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: PointJEM: Self-supervised Point Cloud Understanding for Reducing Feature Redundancy via Joint Entropy Maximization. (arXiv:2312.03339v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03339">http://arxiv.org/abs/2312.03339</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03339]] PointJEM: Self-supervised Point Cloud Understanding for Reducing Feature Redundancy via Joint Entropy Maximization(http://arxiv.org/abs/2312.03339)</code></li>
<li>Summary: <p>Most deep learning-based point cloud processing methods are supervised and
require large scale of labeled data. However, manual labeling of point cloud
data is laborious and time-consuming. Self-supervised representation learning
can address the aforementioned issue by learning robust and generalized
representations from unlabeled datasets. Nevertheless, the embedded features
obtained by representation learning usually contain redundant information, and
most current methods reduce feature redundancy by linear correlation
constraints. In this paper, we propose PointJEM, a self-supervised
representation learning method applied to the point cloud field. PointJEM
comprises an embedding scheme and a loss function based on joint entropy. The
embedding scheme divides the embedding vector into different parts, each part
can learn a distinctive feature. To reduce redundant information in the
features, PointJEM maximizes the joint entropy between the different parts,
thereby rendering the learned feature variables pairwise independent. To
validate the effectiveness of our method, we conducted experiments on multiple
datasets. The results demonstrate that our method can significantly reduce
feature redundancy beyond linear correlation. Furthermore, PointJEM achieves
competitive performance in downstream tasks such as classification and
segmentation.
</p></li>
</ul>

<h3>Title: PointMoment:Mixed-Moment-based Self-Supervised Representation Learning for 3D Point Clouds. (arXiv:2312.03350v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03350">http://arxiv.org/abs/2312.03350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03350]] PointMoment:Mixed-Moment-based Self-Supervised Representation Learning for 3D Point Clouds(http://arxiv.org/abs/2312.03350)</code></li>
<li>Summary: <p>Large and rich data is a prerequisite for effective training of deep neural
networks. However, the irregularity of point cloud data makes manual annotation
time-consuming and laborious. Self-supervised representation learning, which
leverages the intrinsic structure of large-scale unlabelled data to learn
meaningful feature representations, has attracted increasing attention in the
field of point cloud research. However, self-supervised representation learning
often suffers from model collapse, resulting in reduced information and
diversity of the learned representation, and consequently degrading the
performance of downstream tasks. To address this problem, we propose
PointMoment, a novel framework for point cloud self-supervised representation
learning that utilizes a high-order mixed moment loss function rather than the
conventional contrastive loss function. Moreover, our framework does not
require any special techniques such as asymmetric network architectures,
gradient stopping, etc. Specifically, we calculate the high-order mixed moment
of the feature variables and force them to decompose into products of their
individual moment, thereby making multiple variables more independent and
minimizing the feature redundancy. We also incorporate a contrastive learning
approach to maximize the feature invariance under different data augmentations
of the same point cloud. Experimental results show that our approach
outperforms previous unsupervised learning methods on the downstream task of 3D
point cloud classification and segmentation.
</p></li>
</ul>

<h3>Title: Intrinsic Harmonization for Illumination-Aware Compositing. (arXiv:2312.03698v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03698">http://arxiv.org/abs/2312.03698</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03698]] Intrinsic Harmonization for Illumination-Aware Compositing(http://arxiv.org/abs/2312.03698)</code></li>
<li>Summary: <p>Despite significant advancements in network-based image harmonization
techniques, there still exists a domain disparity between typical training
pairs and real-world composites encountered during inference. Most existing
methods are trained to reverse global edits made on segmented image regions,
which fail to accurately capture the lighting inconsistencies between the
foreground and background found in composited images. In this work, we
introduce a self-supervised illumination harmonization approach formulated in
the intrinsic image domain. First, we estimate a simple global lighting model
from mid-level vision representations to generate a rough shading for the
foreground region. A network then refines this inferred shading to generate a
harmonious re-shading that aligns with the background scene. In order to match
the color appearance of the foreground and background, we utilize ideas from
prior harmonization approaches to perform parameterized image edits in the
albedo domain. To validate the effectiveness of our approach, we present
results from challenging real-world composites and conduct a user study to
objectively measure the enhanced realism achieved compared to state-of-the-art
harmonization methods.
</p></li>
</ul>

<h3>Title: Bootstrap Your Own Variance. (arXiv:2312.03213v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03213">http://arxiv.org/abs/2312.03213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03213]] Bootstrap Your Own Variance(http://arxiv.org/abs/2312.03213)</code></li>
<li>Summary: <p>Understanding model uncertainty is important for many applications. We
propose Bootstrap Your Own Variance (BYOV), combining Bootstrap Your Own Latent
(BYOL), a negative-free Self-Supervised Learning (SSL) algorithm, with Bayes by
Backprop (BBB), a Bayesian method for estimating model posteriors. We find that
the learned predictive std of BYOV vs. a supervised BBB model is well captured
by a Gaussian distribution, providing preliminary evidence that the learned
parameter posterior is useful for label free uncertainty estimation. BYOV
improves upon the deterministic BYOL baseline (+2.83% test ECE, +1.03% test
Brier) and presents better calibration and reliability when tested with various
augmentations (eg: +2.4% test ECE, +1.2% test Brier for Salt &amp; Pepper noise).
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey. (arXiv:2312.03014v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03014">http://arxiv.org/abs/2312.03014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03014]] Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey(http://arxiv.org/abs/2312.03014)</code></li>
<li>Summary: <p>As artificial intelligence (AI) continues to rapidly evolve, the realm of
Earth and atmospheric sciences is increasingly adopting data-driven models,
powered by progressive developments in deep learning (DL). Specifically, DL
techniques are extensively utilized to decode the chaotic and nonlinear aspects
of Earth systems, and to address climate challenges via understanding weather
and climate data. Cutting-edge performance on specific tasks within narrower
spatio-temporal scales has been achieved recently through DL. The rise of large
models, specifically large language models (LLMs), has enabled fine-tuning
processes that yield remarkable outcomes across various downstream tasks,
thereby propelling the advancement of general AI. However, we are still
navigating the initial stages of crafting general AI for weather and climate.
In this survey, we offer an exhaustive, timely overview of state-of-the-art AI
methodologies specifically engineered for weather and climate data, with a
special focus on time series and text data. Our primary coverage encompasses
four critical aspects: types of weather and climate data, principal model
architectures, model scopes and applications, and datasets for weather and
climate. Furthermore, in relation to the creation and application of foundation
models for weather and climate data understanding, we delve into the field's
prevailing challenges, offer crucial insights, and propose detailed avenues for
future research. This comprehensive approach equips practitioners with the
requisite knowledge to make substantial progress in this domain. Our survey
encapsulates the most recent breakthroughs in research on large, data-driven
models for weather and climate data understanding, emphasizing robust
foundations, current advancements, practical applications, crucial resources,
and prospective research opportunities.
</p></li>
</ul>

<h3>Title: Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields. (arXiv:2312.03203v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03203">http://arxiv.org/abs/2312.03203</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03203]] Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields(http://arxiv.org/abs/2312.03203)</code></li>
<li>Summary: <p>3D scene representations have gained immense popularity in recent years.
Methods that use Neural Radiance fields are versatile for traditional tasks
such as novel view synthesis. In recent times, some work has emerged that aims
to extend the functionality of NeRF beyond view synthesis, for semantically
aware tasks such as editing and segmentation using 3D feature field
distillation from 2D foundation models. However, these methods have two major
limitations: (a) they are limited by the rendering speed of NeRF pipelines, and
(b) implicitly represented feature fields suffer from continuity artifacts
reducing feature quality. Recently, 3D Gaussian Splatting has shown
state-of-the-art performance on real-time radiance field rendering. In this
work, we go one step further: in addition to radiance field rendering, we
enable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D
foundation model distillation. This translation is not straightforward: naively
incorporating feature fields in the 3DGS framework leads to warp-level
divergence. We propose architectural and training changes to efficiently avert
this problem. Our proposed method is general, and our experiments showcase
novel view semantic segmentation, language-guided editing and segment anything
through learning feature fields from state-of-the-art 2D foundation models such
as SAM and CLIP-LSeg. Across experiments, our distillation method is able to
provide comparable or better results, while being significantly faster to both
train and render. Additionally, to the best of our knowledge, we are the first
method to enable point and bounding-box prompting for radiance field
manipulation, by leveraging the SAM model. Project website at:
https://feature-3dgs.github.io/
</p></li>
</ul>

<h3>Title: Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future. (arXiv:2312.03408v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03408">http://arxiv.org/abs/2312.03408</a></li>
<li>Code URL: https://github.com/opendrivelab/driveagi</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03408]] Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future(http://arxiv.org/abs/2312.03408)</code></li>
<li>Summary: <p>With the continuous maturation and application of autonomous driving
technology, a systematic examination of open-source autonomous driving datasets
becomes instrumental in fostering the robust evolution of the industry
ecosystem. Current autonomous driving datasets can broadly be categorized into
two generations. The first-generation autonomous driving datasets are
characterized by relatively simpler sensor modalities, smaller data scale, and
is limited to perception-level tasks. KITTI, introduced in 2012, serves as a
prominent representative of this initial wave. In contrast, the
second-generation datasets exhibit heightened complexity in sensor modalities,
greater data scale and diversity, and an expansion of tasks from perception to
encompass prediction and control. Leading examples of the second generation
include nuScenes and Waymo, introduced around 2019. This comprehensive review,
conducted in collaboration with esteemed colleagues from both academia and
industry, systematically assesses over seventy open-source autonomous driving
datasets from domestic and international sources. It offers insights into
various aspects, such as the principles underlying the creation of high-quality
datasets, the pivotal role of data engine systems, and the utilization of
generative foundation models to facilitate scalable data generation.
Furthermore, this review undertakes an exhaustive analysis and discourse
regarding the characteristics and data scales that future third-generation
autonomous driving datasets should possess. It also delves into the scientific
and technical challenges that warrant resolution. These endeavors are pivotal
in advancing autonomous innovation and fostering technological enhancement in
critical domains. For further details, please refer to
https://github.com/OpenDriveLab/DriveAGI.
</p></li>
</ul>

<h3>Title: Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation. (arXiv:2312.03502v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03502">http://arxiv.org/abs/2312.03502</a></li>
<li>Code URL: https://github.com/zhang-haojie/wesam</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03502]] Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation(http://arxiv.org/abs/2312.03502)</code></li>
<li>Summary: <p>The success of large language models has inspired the computer vision
community to explore image segmentation foundation model that is able to
zero/few-shot generalize through prompt engineering. Segment-Anything(SAM),
among others, is the state-of-the-art image segmentation foundation model
demonstrating strong zero/few-shot generalization. Despite the success, recent
studies reveal the weakness of SAM under strong distribution shift. In
particular, SAM performs awkwardly on corrupted natural images, camouflaged
images, medical images, etc. Motivated by the observations, we aim to develop a
self-training based strategy to adapt SAM to target distribution. Given the
unique challenges of large source dataset, high computation cost and incorrect
pseudo label, we propose a weakly supervised self-training architecture with
anchor regularization and low-rank finetuning to improve the robustness and
computation efficiency of adaptation. We validate the effectiveness on 5 types
of downstream segmentation tasks including natural clean/corrupted images,
medical images, camouflaged images and robotic images. Our proposed method is
task-agnostic in nature and outperforms pre-trained SAM and state-of-the-art
domain adaptation methods on almost all downstream tasks with the same testing
prompt inputs.
</p></li>
</ul>

<h3>Title: Low-shot Object Learning with Mutual Exclusivity Bias. (arXiv:2312.03533v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03533">http://arxiv.org/abs/2312.03533</a></li>
<li>Code URL: https://github.com/rehg-lab/lsme</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03533]] Low-shot Object Learning with Mutual Exclusivity Bias(http://arxiv.org/abs/2312.03533)</code></li>
<li>Summary: <p>This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias
(LSME), the first computational framing of mutual exclusivity bias, a
phenomenon commonly observed in infants during word learning. We provide a
novel dataset, comprehensive baselines, and a state-of-the-art method to enable
the ML community to tackle this challenging learning task. The goal of LSME is
to analyze an RGB image of a scene containing multiple objects and correctly
associate a previously-unknown object instance with a provided category label.
This association is then used to perform low-shot learning to test category
generalization. We provide a data generation pipeline for the LSME problem and
conduct a thorough analysis of the factors that contribute to its difficulty.
Additionally, we evaluate the performance of multiple baselines, including
state-of-the-art foundation models. Finally, we present a baseline approach
that outperforms state-of-the-art models in terms of low-shot accuracy.
</p></li>
</ul>

<h3>Title: Foundation Model Assisted Weakly Supervised Semantic Segmentation. (arXiv:2312.03585v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03585">http://arxiv.org/abs/2312.03585</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03585]] Foundation Model Assisted Weakly Supervised Semantic Segmentation(http://arxiv.org/abs/2312.03585)</code></li>
<li>Summary: <p>This work aims to leverage pre-trained foundation models, such as contrastive
language-image pre-training (CLIP) and segment anything model (SAM), to address
weakly supervised semantic segmentation (WSSS) using image-level labels. To
this end, we propose a coarse-to-fine framework based on CLIP and SAM for
generating high-quality segmentation seeds. Specifically, we construct an image
classification task and a seed segmentation task, which are jointly performed
by CLIP with frozen weights and two sets of learnable task-specific prompts. A
SAM-based seeding (SAMS) module is designed and applied to each task to produce
either coarse or fine seed maps. Moreover, we design a multi-label contrastive
loss supervised by image-level labels and a CAM activation loss supervised by
the generated coarse seed map. These losses are used to learn the prompts,
which are the only parts need to be learned in our framework. Once the prompts
are learned, we input each image along with the learned segmentation-specific
prompts into CLIP and the SAMS module to produce high-quality segmentation
seeds. These seeds serve as pseudo labels to train an off-the-shelf
segmentation network like other two-stage WSSS methods. Experiments show that
our method achieves the state-of-the-art performance on PASCAL VOC 2012 and
competitive results on MS COCO 2014.
</p></li>
</ul>

<h3>Title: Boosting Segment Anything Model Towards Open-Vocabulary Learning. (arXiv:2312.03628v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03628">http://arxiv.org/abs/2312.03628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03628]] Boosting Segment Anything Model Towards Open-Vocabulary Learning(http://arxiv.org/abs/2312.03628)</code></li>
<li>Summary: <p>The recent Segment Anything Model (SAM) has emerged as a new paradigmatic
vision foundation model, showcasing potent zero-shot generalization and
flexible prompting. Despite SAM finding applications and adaptations in various
domains, its primary limitation lies in the inability to grasp object
semantics. In this paper, we present Sambor to seamlessly integrate SAM with
the open-vocabulary object detector in an end-to-end framework. While retaining
all the remarkable capabilities inherent to SAM, we enhance it with the
capacity to detect arbitrary objects based on human inputs like category names
or reference expressions. To accomplish this, we introduce a novel SideFormer
module that extracts SAM features to facilitate zero-shot object localization
and inject comprehensive semantic information for open-vocabulary recognition.
In addition, we devise an open-set region proposal network (Open-set RPN),
enabling the detector to acquire the open-set proposals generated by SAM.
Sambor demonstrates superior zero-shot performance across benchmarks, including
COCO and LVIS, proving highly competitive against previous SoTA methods. We
aspire for this work to serve as a meaningful endeavor in endowing SAM to
recognize diverse object categories and advancing open-vocabulary learning with
the support of vision foundation models.
</p></li>
</ul>

<h3>Title: MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations. (arXiv:2312.03631v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03631">http://arxiv.org/abs/2312.03631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03631]] MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations(http://arxiv.org/abs/2312.03631)</code></li>
<li>Summary: <p>While recent years have seen rapid progress in image-conditioned text
generation, image captioning still suffers from the fundamental issue of
hallucinations, the generation of spurious details that cannot be inferred from
the given image. Dedicated methods for reducing hallucinations in image
captioning largely focus on closed-vocabulary object tokens, ignoring most
types of hallucinations that occur in practice. In this work, we propose MOCHa,
an approach that harnesses advancements in reinforcement learning (RL) to
address the sequence-level nature of hallucinations in an open-world setup. To
optimize for caption fidelity to the input image, we leverage ground-truth
reference captions as proxies to measure the logical consistency of generated
captions. However, optimizing for caption fidelity alone fails to preserve the
semantic adequacy of generations; therefore, we propose a multi-objective
reward function that jointly targets these qualities, without requiring any
strong supervision. We demonstrate that these goals can be simultaneously
optimized with our framework, enhancing performance for various captioning
models of different scales. Our qualitative and quantitative results
demonstrate MOCHa's superior performance across various established metrics. We
also demonstrate the benefit of our method in the open-vocabulary setting. To
this end, we contribute OpenCHAIR, a new benchmark for quantifying
open-vocabulary hallucinations in image captioning models, constructed using
generative foundation models. We will release our code, benchmark, and trained
models.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction. (arXiv:2312.03035v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03035">http://arxiv.org/abs/2312.03035</a></li>
<li>Code URL: https://github.com/cogtoolslab/visual_abstractions_benchmarking_public2023</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03035]] SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction(http://arxiv.org/abs/2312.03035)</code></li>
<li>Summary: <p>Sketching is a powerful tool for creating abstract images that are sparse but
meaningful. Sketch understanding poses fundamental challenges for
general-purpose vision algorithms because it requires robustness to the
sparsity of sketches relative to natural visual inputs and because it demands
tolerance for semantic ambiguity, as sketches can reliably evoke multiple
meanings. While current vision algorithms have achieved high performance on a
variety of visual tasks, it remains unclear to what extent they understand
sketches in a human-like way. Here we introduce SEVA, a new benchmark dataset
containing approximately 90K human-generated sketches of 128 object concepts
produced under different time constraints, and thus systematically varying in
sparsity. We evaluated a suite of state-of-the-art vision algorithms on their
ability to correctly identify the target concept depicted in these sketches and
to generate responses that are strongly aligned with human response patterns on
the same sketch recognition task. We found that vision algorithms that better
predicted human sketch recognition performance also better approximated human
uncertainty about sketch meaning, but there remains a sizable gap between model
and human response patterns. To explore the potential of models that emulate
human visual abstraction in generative tasks, we conducted further evaluations
of a recently developed sketch generation algorithm (Vinker et al., 2022)
capable of generating sketches that vary in sparsity. We hope that public
release of this dataset and evaluation protocol will catalyze progress towards
algorithms with enhanced capacities for human-like visual abstraction.
</p></li>
</ul>

<h3>Title: FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction. (arXiv:2312.03187v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03187">http://arxiv.org/abs/2312.03187</a></li>
<li>Code URL: https://github.com/shuangquanfeng/fergi</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03187]] FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction(http://arxiv.org/abs/2312.03187)</code></li>
<li>Summary: <p>Researchers have proposed to use data of human preference feedback to
fine-tune text-to-image generative models. However, the scalability of human
feedback collection has been limited by its reliance on manual annotation.
Therefore, we develop and test a method to automatically annotate user
preferences from their spontaneous facial expression reaction to the generated
images. We collect a dataset of Facial Expression Reaction to Generated Images
(FERGI) and show that the activations of multiple facial action units (AUs) are
highly correlated with user evaluations of the generated images. Specifically,
AU4 (brow lowerer) is most consistently reflective of negative evaluations of
the generated image. This can be useful in two ways. Firstly, we can
automatically annotate user preferences between image pairs with substantial
difference in AU4 responses to them with an accuracy significantly
outperforming state-of-the-art scoring models. Secondly, directly integrating
the AU4 responses with the scoring models improves their consistency with human
preferences. Additionally, the AU4 response best reflects the user's evaluation
of the image fidelity, making it complementary to the state-of-the-art scoring
models, which are generally better at reflecting image-text alignment. Finally,
this method of automatic annotation with facial expression analysis can be
potentially generalized to other generation tasks. The code is available at
https://github.com/ShuangquanFeng/FERGI, and the dataset is also available at
the same link for research purposes.
</p></li>
</ul>

<h3>Title: Data-driven Crop Growth Simulation on Time-varying Generated Images using Multi-conditional Generative Adversarial Networks. (arXiv:2312.03443v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03443">http://arxiv.org/abs/2312.03443</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03443]] Data-driven Crop Growth Simulation on Time-varying Generated Images using Multi-conditional Generative Adversarial Networks(http://arxiv.org/abs/2312.03443)</code></li>
<li>Summary: <p>Image-based crop growth modeling can substantially contribute to precision
agriculture by revealing spatial crop development over time, which allows an
early and location-specific estimation of relevant future plant traits, such as
leaf area or biomass. A prerequisite for realistic and sharp crop image
generation is the integration of multiple growth-influencing conditions in a
model, such as an image of an initial growth stage, the associated growth time,
and further information about the field treatment. We present a two-stage
framework consisting first of an image prediction model and second of a growth
estimation model, which both are independently trained. The image prediction
model is a conditional Wasserstein generative adversarial network (CWGAN). In
the generator of this model, conditional batch normalization (CBN) is used to
integrate different conditions along with the input image. This allows the
model to generate time-varying artificial images dependent on multiple
influencing factors of different kinds. These images are used by the second
part of the framework for plant phenotyping by deriving plant-specific traits
and comparing them with those of non-artificial (real) reference images. For
various crop datasets, the framework allows realistic, sharp image predictions
with a slight loss of quality from short-term to long-term predictions.
Simulations of varying growth-influencing conditions performed with the trained
framework provide valuable insights into how such factors relate to crop
appearances, which is particularly useful in complex, less explored crop
mixture systems. Further results show that adding process-based simulated
biomass as a condition increases the accuracy of the derived phenotypic traits
from the predicted images. This demonstrates the potential of our framework to
serve as an interface between an image- and process-based crop growth model.
</p></li>
</ul>

<h3>Title: MMM: Generative Masked Motion Model. (arXiv:2312.03596v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03596">http://arxiv.org/abs/2312.03596</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03596]] MMM: Generative Masked Motion Model(http://arxiv.org/abs/2312.03596)</code></li>
<li>Summary: <p>Recent advances in text-to-motion generation using diffusion and
autoregressive models have shown promising results. However, these models often
suffer from a trade-off between real-time performance, high fidelity, and
motion editability. To address this gap, we introduce MMM, a novel yet simple
motion generation paradigm based on Masked Motion Model. MMM consists of two
key components: (1) a motion tokenizer that transforms 3D human motion into a
sequence of discrete tokens in latent space, and (2) a conditional masked
motion transformer that learns to predict randomly masked motion tokens,
conditioned on the pre-computed text tokens. By attending to motion and text
tokens in all directions, MMM explicitly captures inherent dependency among
motion tokens and semantic mapping between motion and text tokens. During
inference, this allows parallel and iterative decoding of multiple motion
tokens that are highly consistent with fine-grained text descriptions,
therefore simultaneously achieving high-fidelity and high-speed motion
generation. In addition, MMM has innate motion editability. By simply placing
mask tokens in the place that needs editing, MMM automatically fills the gaps
while guaranteeing smooth transitions between editing and non-editing parts.
Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM
surpasses current leading methods in generating high-quality motion (evidenced
by superior FID scores of 0.08 and 0.429), while offering advanced editing
features such as body-part modification, motion in-betweening, and the
synthesis of long motion sequences. In addition, MMM is two orders of magnitude
faster on a single mid-range GPU than editable motion diffusion models. Our
project page is available at \url{https://exitudio.github.io/MMM-page}.
</p></li>
</ul>

<h3>Title: Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication. (arXiv:2312.03692v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03692">http://arxiv.org/abs/2312.03692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03692]] Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication(http://arxiv.org/abs/2312.03692)</code></li>
<li>Summary: <p>Diffusion-based models, such as the Stable Diffusion model, have
revolutionized text-to-image synthesis with their ability to produce
high-quality, high-resolution images. These advancements have prompted
significant progress in image generation and editing tasks. However, these
models also raise concerns due to their tendency to memorize and potentially
replicate exact training samples, posing privacy risks and enabling adversarial
attacks. Duplication in training datasets is recognized as a major factor
contributing to memorization, and various forms of memorization have been
studied so far. This paper focuses on two distinct and underexplored types of
duplication that lead to replication during inference in diffusion-based
models, particularly in the Stable Diffusion model. We delve into these
lesser-studied duplication phenomena and their implications through two case
studies, aiming to contribute to the safer and more responsible use of
generative models in various applications.
</p></li>
</ul>

<h3>Title: ZTCloudGuard: Zero Trust Context-Aware Access Management Framework to Avoid Misuse Cases in the Era of Generative AI and Cloud-based Health Information Ecosystem. (arXiv:2312.02993v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.02993">http://arxiv.org/abs/2312.02993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.02993]] ZTCloudGuard: Zero Trust Context-Aware Access Management Framework to Avoid Misuse Cases in the Era of Generative AI and Cloud-based Health Information Ecosystem(http://arxiv.org/abs/2312.02993)</code></li>
<li>Summary: <p>Managing access between large numbers of distributed medical devices has
become a crucial aspect of modern healthcare systems, enabling the
establishment of smart hospitals and telehealth infrastructure. However, as
telehealth technology continues to evolve and Internet of Things (IoT) devices
become more widely used, they are also becoming increasingly exposed to various
types of vulnerabilities and medical errors. In healthcare information systems,
about 90\% of vulnerabilities emerged from misuse cases and human errors. As a
result, there is a need for additional research and development of security
tools to prevent such attacks. This article proposes a zero-trust-based
context-aware framework for managing access to the main components of the cloud
ecosystem, including users, devices and output data. The main goal and benefit
of the proposed framework is to build a scoring system to prevent or alleviate
misuse cases while using distributed medical devices in cloud-based healthcare
information systems. The framework has two main scoring schemas to maintain the
chain of trust. First, it proposes a critical trust score based on cloud-native
micro-services of authentication, encryption, logging, and authorizations.
Second, creating a bond trust scoring to assess the real-time semantic and
syntactic analysis of attributes stored in a healthcare information system. The
analysis is based on a pre-trained machine learning model to generate the
semantic and syntactic scores. The framework also takes into account regulatory
compliance and user consent to create a scoring system. The advantage of this
method is that it is applicable to any language and adapts to all attributes as
it relies on a language model, not just a set of predefined and limited
attributes. The results show a high F1 score of 93.5%, which proves that it is
valid for detecting misuse cases.
</p></li>
</ul>

<h3>Title: Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models. (arXiv:2312.03419v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03419">http://arxiv.org/abs/2312.03419</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03419]] Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models(http://arxiv.org/abs/2312.03419)</code></li>
<li>Summary: <p>Backdoor attacks, representing an emerging threat to the integrity of deep
neural networks, have garnered significant attention due to their ability to
compromise deep learning systems clandestinely. While numerous backdoor attacks
occur within the digital realm, their practical implementation in real-world
prediction systems remains limited and vulnerable to disturbances in the
physical world. Consequently, this limitation has given rise to the development
of physical backdoor attacks, where trigger objects manifest as physical
entities within the real world. However, creating the requisite dataset to
train or evaluate a physical backdoor model is a daunting task, limiting the
backdoor researchers and practitioners from studying such physical attack
scenarios. This paper unleashes a recipe that empowers backdoor researchers to
effortlessly create a malicious, physical backdoor dataset based on advances in
generative modeling. Particularly, this recipe involves 3 automatic modules:
suggesting the suitable physical triggers, generating the poisoned candidate
samples (either by synthesizing new samples or editing existing clean samples),
and finally refining for the most plausible ones. As such, it effectively
mitigates the perceived complexity associated with creating a physical backdoor
dataset, transforming it from a daunting task into an attainable objective.
Extensive experiment results show that datasets created by our "recipe" enable
adversaries to achieve an impressive attack success rate on real physical world
data and exhibit similar properties compared to previous physical backdoor
attack studies. This paper offers researchers a valuable toolkit for studies of
physical backdoors, all within the confines of their laboratories.
</p></li>
</ul>

<h3>Title: MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment. (arXiv:2312.03644v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03644">http://arxiv.org/abs/2312.03644</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03644]] MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment(http://arxiv.org/abs/2312.03644)</code></li>
<li>Summary: <p>Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios
where online interaction is impractical or risky. While independent learning in
MARL offers flexibility and scalability, accurately assigning credit to
individual agents in offline settings poses challenges due to partial
observability and emergent behavior. Directly transferring the online credit
assignment method to offline settings results in suboptimal outcomes due to the
absence of real-time feedback and intricate agent interactions. Our approach,
MACCA, characterizing the generative process as a Dynamic Bayesian Network,
captures relationships between environmental variables, states, actions, and
rewards. Estimating this model on offline data, MACCA can learn each agent's
contribution by analyzing the causal relationship of their individual rewards,
ensuring accurate and interpretable credit assignment. Additionally, the
modularity of our approach allows it to seamlessly integrate with various
offline MARL methods. Theoretically, we proved that under the setting of the
offline dataset, the underlying causal structure and the function for
generating the individual rewards of agents are identifiable, which laid the
foundation for the correctness of our modeling. Experimentally, we tested MACCA
in two environments, including discrete and continuous action settings. The
results show that MACCA outperforms SOTA methods and improves performance upon
their backbones.
</p></li>
</ul>

<h3>Title: On the Role of Edge Dependency in Graph Generative Models. (arXiv:2312.03691v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03691">http://arxiv.org/abs/2312.03691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03691]] On the Role of Edge Dependency in Graph Generative Models(http://arxiv.org/abs/2312.03691)</code></li>
<li>Summary: <p>In this work, we introduce a novel evaluation framework for generative models
of graphs, emphasizing the importance of model-generated graph overlap
(Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. We
delineate a hierarchy of graph generative models categorized into three levels
of complexity: edge independent, node independent, and fully dependent models.
This hierarchy encapsulates a wide range of prevalent methods. We derive
theoretical bounds on the number of triangles and other short-length cycles
producible by each level of the hierarchy, contingent on the model overlap. We
provide instances demonstrating the asymptotic optimality of our bounds.
Furthermore, we introduce new generative models for each of the three
hierarchical levels, leveraging dense subgraph discovery (Gionis &amp; Tsourakakis,
2015). Our evaluation, conducted on real-world datasets, focuses on assessing
the output quality and overlap of our proposed models in comparison to other
popular models. Our results indicate that our simple, interpretable models
provide competitive baselines to popular generative models. Through this
investigation, we aim to propel the advancement of graph generative models by
offering a structured framework and robust evaluation metrics, thereby
facilitating the development of models capable of generating accurate and
edge-diverse graphs.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature Representations. (arXiv:2312.03005v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03005">http://arxiv.org/abs/2312.03005</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03005]] Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature Representations(http://arxiv.org/abs/2312.03005)</code></li>
<li>Summary: <p>Anomaly detection is a critical and challenging task that aims to identify
data points deviating from normal patterns and distributions within a dataset.
Various methods have been proposed using a one-class-one-model approach, but
these techniques often face practical problems such as memory inefficiency and
the requirement of sufficient data for training. In particular, few-shot
anomaly detection presents significant challenges in industrial applications,
where limited samples are available before mass production. In this paper, we
propose a few-shot anomaly detection method that integrates adversarial
training loss to obtain more robust and generalized feature representations. We
utilize the adversarial loss previously employed in domain adaptation to align
feature distributions between source and target domains, to enhance feature
robustness and generalization in few-shot anomaly detection tasks. We
hypothesize that adversarial loss is effective when applied to features that
should have similar characteristics, such as those from the same layer in a
Siamese network's parallel branches or input-output pairs of
reconstruction-based methods. Experimental results demonstrate that the
proposed method generally achieves better performance when utilizing the
adversarial loss.
</p></li>
</ul>

<h3>Title: Anomaly Detection for Scalable Task Grouping in Reinforcement Learning-based RAN Optimization. (arXiv:2312.03277v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03277">http://arxiv.org/abs/2312.03277</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03277]] Anomaly Detection for Scalable Task Grouping in Reinforcement Learning-based RAN Optimization(http://arxiv.org/abs/2312.03277)</code></li>
<li>Summary: <p>The use of learning-based methods for optimizing cellular radio access
networks (RAN) has received increasing attention in recent years. This
coincides with a rapid increase in the number of cell sites worldwide, driven
largely by dramatic growth in cellular network traffic. Training and
maintaining learned models that work well across a large number of cell sites
has thus become a pertinent problem. This paper proposes a scalable framework
for constructing a reinforcement learning policy bank that can perform RAN
optimization across a large number of cell sites with varying traffic patterns.
Central to our framework is a novel application of anomaly detection techniques
to assess the compatibility between sites (tasks) and the policy bank. This
allows our framework to intelligently identify when a policy can be reused for
a task, and when a new policy needs to be trained and added to the policy bank.
Our results show that our approach to compatibility assessment leads to an
efficient use of computational resources, by allowing us to construct a
performant policy bank without exhaustively training on all tasks, which makes
it applicable under real-world constraints.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning. (arXiv:2312.03703v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03703">http://arxiv.org/abs/2312.03703</a></li>
<li>Code URL: https://github.com/fanglaosi/skeleton-in-context</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03703]] Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning(http://arxiv.org/abs/2312.03703)</code></li>
<li>Summary: <p>In-context learning provides a new perspective for multi-task modeling for
vision and NLP. Under this setting, the model can perceive tasks from prompts
and accomplish them without any extra task-specific head predictions or model
fine-tuning. However, Skeleton sequence modeling via in-context learning
remains unexplored. Directly applying existing in-context models from other
areas onto skeleton sequences fails due to the inter-frame and cross-task pose
similarity that makes it outstandingly hard to perceive the task correctly from
a subtle context. To address this challenge, we propose Skeleton-in-Context
(SiC), an effective framework for in-context skeleton sequence modeling. Our
SiC is able to handle multiple skeleton-based tasks simultaneously after a
single training process and accomplish each task from context according to the
given prompt. It can further generalize to new, unseen tasks according to
customized prompts. To facilitate context perception, we additionally propose a
task-unified prompt, which adaptively learns tasks of different natures, such
as partial joint-level generation, sequence-level prediction, or 2D-to-3D
motion prediction. We conduct extensive experiments to evaluate the
effectiveness of our SiC on multiple tasks, including motion prediction, pose
estimation, joint completion, and future pose estimation. We also evaluate its
generalization capability on unseen tasks such as motion-in-between. These
experiments show that our model achieves state-of-the-art multi-task
performance and even outperforms single-task methods on certain tasks.
</p></li>
</ul>

<h3>Title: Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification. (arXiv:2312.03458v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03458">http://arxiv.org/abs/2312.03458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03458]] Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification(http://arxiv.org/abs/2312.03458)</code></li>
<li>Summary: <p>The proliferation of Large Language Models (LLMs) has spurred extensive
research into LLM-related Prompt investigations, such as Instruction Learning
(IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches
aim to improve LLMs' responses by enabling them to provide concise statements
or examples for deeper contemplation when addressing questions. However,
independent thinking by LLMs can introduce variability in their thought
processes, leading to potential inaccuracies. In response, our study seeks to
bridge the gap between LLM and human-like thinking processes, recognizing that
text comprehension begins with understanding individual words. To tackle this
challenge, we have expanded the CoT method to cater to a specific domain. Our
approach, known as "Think from Words" (TFW), initiates the comprehension
process at the word level and then extends it to encompass the entire text. We
also propose "TFW with Extra word-level information" (TFW Extra), augmenting
comprehension with additional word-level data. To assess our methods, we employ
text classification on six Japanese datasets comprising text-level and
word-level elements. Our findings not only validate the effectiveness of TFW
but also shed light on the impact of various word-level information types on
LLMs' text comprehension, offering insights into their potential to cause
misinterpretations and errors in the overall comprehension of the final text.
</p></li>
</ul>

<h3>Title: The mechanistic basis of data dependence and abrupt learning in an in-context classification task. (arXiv:2312.03002v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.03002">http://arxiv.org/abs/2312.03002</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.03002]] The mechanistic basis of data dependence and abrupt learning in an in-context classification task(http://arxiv.org/abs/2312.03002)</code></li>
<li>Summary: <p>Transformer models exhibit in-context learning: the ability to accurately
predict the response to a novel query based on illustrative examples in the
input sequence. In-context learning contrasts with traditional in-weights
learning of query-output relationships. What aspects of the training data
distribution and architecture favor in-context vs in-weights learning? Recent
work has shown that specific distributional properties inherent in language,
such as burstiness, large dictionaries and skewed rank-frequency distributions,
control the trade-off or simultaneous appearance of these two forms of
learning. We first show that these results are recapitulated in a minimal
attention-only network trained on a simplified dataset. In-context learning
(ICL) is driven by the abrupt emergence of an induction head, which
subsequently competes with in-weights learning. By identifying progress
measures that precede in-context learning and targeted experiments, we
construct a two-parameter model of an induction head which emulates the full
data distributional dependencies displayed by the attention-based network. A
phenomenological model of induction head formation traces its abrupt emergence
to the sequential learning of three nested logits enabled by an intrinsic
curriculum. We propose that the sharp transitions in attention-based networks
arise due to a specific chain of multi-layer operations necessary to achieve
ICL, which is implemented by nested nonlinearities sequentially learned during
training.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
