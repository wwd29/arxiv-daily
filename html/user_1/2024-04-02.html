<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-02</h1>
<h3>Title: BEACON: Bayesian Experimental design Acceleration with Conditional  Normalizing flows $-$ a case study in optimal monitor well placement for  CO$_2$ sequestration</h3>
<ul>
<li><strong>Authors: </strong>Rafael Orozco, Abhinav Gahlot, Felix J. Herrmann</a></li>
<li><strong>Subjects: </strong>cs.LG, math-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00075">https://arxiv.org/abs/2404.00075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00075">https://arxiv.org/pdf/2404.00075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00075]] BEACON: Bayesian Experimental design Acceleration with Conditional  Normalizing flows $-$ a case study in optimal monitor well placement for  CO$_2$ sequestration(https://arxiv.org/abs/2404.00075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>CO$_2$ sequestration is a crucial engineering solution for mitigating climate change. However, the uncertain nature of reservoir properties, necessitates rigorous monitoring of CO$_2$ plumes to prevent risks such as leakage, induced seismicity, or breaching licensed boundaries. To address this, project managers use borehole wells for direct CO$_2$ and pressure monitoring at specific locations. Given the high costs associated with drilling, it is crucial to strategically place a limited number of wells to ensure maximally effective monitoring within budgetary constraints. Our approach for selecting well locations integrates fluid-flow solvers for forecasting plume trajectories with generative neural networks for plume inference uncertainty. Our methodology is extensible to three-dimensional domains and is developed within a Bayesian framework for optimal experimental design, ensuring scalability and mathematical optimality. We use a realistic case study to verify these claims by demonstrating our method's application in a large scale domain and optimal performance as compared to baseline well placement.</li>
</ul>

<h3>Title: GDA: Generalized Diffusion for Robust Test-time Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yun-Yun Tsai, Fu-Chen Chen, Albert Y. C. Chen, Junfeng Yang, Che-Chun Su, Min Sun, Cheng-Hao Kuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00095">https://arxiv.org/abs/2404.00095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00095">https://arxiv.org/pdf/2404.00095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00095]] GDA: Generalized Diffusion for Robust Test-time Adaptation(https://arxiv.org/abs/2404.00095)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Machine learning models struggle with generalization when encountering out-of-distribution (OOD) samples with unexpected distribution shifts. For vision tasks, recent studies have shown that test-time adaptation employing diffusion models can achieve state-of-the-art accuracy improvements on OOD samples by generating new samples that align with the model's domain without the need to modify the model's weights. Unfortunately, those studies have primarily focused on pixel-level corruptions, thereby lacking the generalization to adapt to a broader range of OOD types. We introduce Generalized Diffusion Adaptation (GDA), a novel diffusion-based test-time adaptation method robust against diverse OOD types. Specifically, GDA iteratively guides the diffusion by applying a marginal entropy loss derived from the model, in conjunction with style and content preservation losses during the reverse sampling process. In other words, GDA considers the model's output behavior with the semantic information of the samples as a whole, which can reduce ambiguity in downstream tasks during the generation process. Evaluation across various popular model architectures and OOD benchmarks shows that GDA consistently outperforms prior work on diffusion-driven adaptation. Notably, it achieves the highest classification accuracy improvements, ranging from 4.4\% to 5.02\% on ImageNet-C and 2.5\% to 7.4\% on Rendition, Sketch, and Stylized benchmarks. This performance highlights GDA's generalization to a broader range of OOD benchmarks.</li>
</ul>

<h3>Title: Deepfake Sentry: Harnessing Ensemble Intelligence for Resilient  Detection and Generalisation</h3>
<ul>
<li><strong>Authors: </strong>Liviu-Daniel Ştefan (1), Dan-Cristian Stanciu (1), Mihai Dogariu (1), Mihai Gabriel Constantin (1), Andrei Cosmin Jitaru (1), Bogdan Ionescu (1) ((1) University "Politehnica" of Bucharest, Romania)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00114">https://arxiv.org/abs/2404.00114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00114">https://arxiv.org/pdf/2404.00114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00114]] Deepfake Sentry: Harnessing Ensemble Intelligence for Resilient  Detection and Generalisation(https://arxiv.org/abs/2404.00114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in Generative Adversarial Networks (GANs) have enabled photorealistic image generation with high quality. However, the malicious use of such generated media has raised concerns regarding visual misinformation. Although deepfake detection research has demonstrated high accuracy, it is vulnerable to advances in generation techniques and adversarial iterations on detection countermeasures. To address this, we propose a proactive and sustainable deepfake training augmentation solution that introduces artificial fingerprints into models. We achieve this by employing an ensemble learning approach that incorporates a pool of autoencoders that mimic the effect of the artefacts introduced by the deepfake generator models. Experiments on three datasets reveal that our proposed ensemble autoencoder-based data augmentation learning approach offers improvements in terms of generalisation, resistance against basic data perturbations such as noise, blurring, sharpness enhancement, and affine transforms, resilience to commonly used lossy compression algorithms such as JPEG, and enhanced resistance against adversarial attacks.</li>
</ul>

<h3>Title: Security Risks Concerns of Generative AI in the IoT</h3>
<ul>
<li><strong>Authors: </strong>Honghui Xu, Yingshu Li, Olusesi Balogun, Shaoen Wu, Yue Wang, Zhipeng Cai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00139">https://arxiv.org/abs/2404.00139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00139">https://arxiv.org/pdf/2404.00139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00139]] Security Risks Concerns of Generative AI in the IoT(https://arxiv.org/abs/2404.00139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In an era where the Internet of Things (IoT) intersects increasingly with generative Artificial Intelligence (AI), this article scrutinizes the emergent security risks inherent in this integration. We explore how generative AI drives innovation in IoT and we analyze the potential for data breaches when using generative AI and the misuse of generative AI technologies in IoT ecosystems. These risks not only threaten the privacy and efficiency of IoT systems but also pose broader implications for trust and safety in AI-driven environments. The discussion in this article extends to strategic approaches for mitigating these risks, including the development of robust security protocols, the multi-layered security approaches, and the adoption of AI technological solutions. Through a comprehensive analysis, this article aims to shed light on the critical balance between embracing AI advancements and ensuring stringent security in IoT, providing insights into the future direction of these intertwined technologies.</li>
</ul>

<h3>Title: Classifying Conspiratorial Narratives At Scale: False Alarms and  Erroneous Connections</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Diab, Rr. Nefriana, Yu-Ru Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00141">https://arxiv.org/abs/2404.00141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00141">https://arxiv.org/pdf/2404.00141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00141]] Classifying Conspiratorial Narratives At Scale: False Alarms and  Erroneous Connections(https://arxiv.org/abs/2404.00141)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Online discussions frequently involve conspiracy theories, which can contribute to the proliferation of belief in them. However, not all discussions surrounding conspiracy theories promote them, as some are intended to debunk them. Existing research has relied on simple proxies or focused on a constrained set of signals to identify conspiracy theories, which limits our understanding of conspiratorial discussions across different topics and online communities. This work establishes a general scheme for classifying discussions related to conspiracy theories based on authors' perspectives on the conspiracy belief, which can be expressed explicitly through narrative elements, such as the agent, action, or objective, or implicitly through references to known theories, such as chemtrails or the New World Order. We leverage human-labeled ground truth to train a BERT-based model for classifying online CTs, which we then compared to the Generative Pre-trained Transformer machine (GPT) for detecting online conspiratorial content. Despite GPT's known strengths in its expressiveness and contextual understanding, our study revealed significant flaws in its logical reasoning, while also demonstrating comparable strengths from our classifiers. We present the first large-scale classification study using posts from the most active conspiracy-related Reddit forums and find that only one-third of the posts are classified as positive. This research sheds light on the potential applications of large language models in tasks demanding nuanced contextual comprehension.</li>
</ul>

<h3>Title: The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Dominik Schlechtweg, Shafqat Mumtaz Virk, Nikolay Arefyev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00176">https://arxiv.org/abs/2404.00176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00176">https://arxiv.org/pdf/2404.00176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00176]] The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks(https://arxiv.org/abs/2404.00176)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducible and by standardization different components can be freely combined. The repository reflects the task's modularity by allowing model evaluation for WiC, WSI and LSCD. This allows for careful evaluation of increasingly complex model components providing new ways of model optimization.</li>
</ul>

<h3>Title: GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream  Neural Network Enhancement with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Jiawei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00189">https://arxiv.org/abs/2404.00189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00189">https://arxiv.org/pdf/2404.00189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00189]] GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream  Neural Network Enhancement with LLMs(https://arxiv.org/abs/2404.00189)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study introduces GPTA, a Large Language Model assistance training framework, that enhances the training of downstream task models via prefix prompt. By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training. GPTA utilizes a new synergistic training approach, optimizing the downstream models with parameter gradients and LLMs with the novel ``dialogue gradient''. The framework not only demonstrates significant improvements in model performance across six NLP benchmark datasets, but also reduces overfitting in low-resource scenarios effectively. The detailed analyses further validate that our pioneer framework provides a cost-efficient and adaptive method for downstream task model training with LLM support.</li>
</ul>

<h3>Title: Heterogeneous Contrastive Learning for Foundation Models and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00225">https://arxiv.org/abs/2404.00225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00225">https://arxiv.org/pdf/2404.00225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00225]] Heterogeneous Contrastive Learning for Foundation Models and Beyond(https://arxiv.org/abs/2404.00225)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how contrastive learning is applied to train and fine-tune the multi-view foundation models. Then, we move to contrastive learning methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with contrastive learning loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of contrastive learning.</li>
</ul>

<h3>Title: Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space</h3>
<ul>
<li><strong>Authors: </strong>Zheling Meng, Bo Peng, Jing Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00230">https://arxiv.org/abs/2404.00230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00230">https://arxiv.org/pdf/2404.00230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00230]] Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space(https://arxiv.org/abs/2404.00230)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Watermarking is a tool for actively identifying and attributing the images generated by latent diffusion models. Existing methods face the dilemma of watermark robustness and image quality. The reason for this dilemma is that watermark detection is performed in pixel space, implying an intrinsic link between image quality and watermark robustness. In this paper, we highlight that an effective solution to the problem is to both inject and detect watermarks in latent space, and propose Latent Watermark (LW) with a progressive training strategy. Experiments show that compared to the recently proposed methods such as StegaStamp, StableSignature, RoSteALS and TreeRing, LW not only surpasses them in terms of robustness but also offers superior image quality. When we inject 64-bit messages, LW can achieve an identification performance close to 100% and an attribution performance above 97% under 9 single-attack scenarios and one all-attack scenario. Our code will be available on GitHub.</li>
</ul>

<h3>Title: Grid Diffusion Models for Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Taegyeong Lee, Soyeong Kwon, Taehwan Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00234">https://arxiv.org/abs/2404.00234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00234">https://arxiv.org/pdf/2404.00234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00234]] Grid Diffusion Models for Text-to-Video Generation(https://arxiv.org/abs/2404.00234)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in the diffusion models have significantly improved text-to-image generation. However, generating videos from text is a more challenging task than generating images from text, due to the much larger dataset and higher computational cost required. Most existing video generation methods use either a 3D U-Net architecture that considers the temporal dimension or autoregressive generation. These methods require large datasets and are limited in terms of computational costs compared to text-to-image generation. To tackle these challenges, we propose a simple but effective novel grid diffusion for text-to-video generation without temporal dimension in architecture and a large text-video paired dataset. We can generate a high-quality video using a fixed amount of GPU memory regardless of the number of frames by representing the video as a grid image. Additionally, since our method reduces the dimensions of the video to the dimensions of the image, various image-based methods can be applied to videos, such as text-guided video manipulation from image manipulation. Our proposed method outperforms the existing methods in both quantitative and qualitative evaluations, demonstrating the suitability of our model for real-world video generation.</li>
</ul>

<h3>Title: Information Security and Privacy in the Digital World: Some Selected  Topics</h3>
<ul>
<li><strong>Authors: </strong>Jaydip Sen, Joceli Mayer, Subhasis Dasgupta, Subrata Nandi, Srinivasan Krishnaswamy, Pinaki Mitra, Mahendra Pratap Singh, Naga Prasanthi Kundeti, Chandra Sekhara Rao MVP, Sudha Sree Chekuri, Seshu Babu Pallapothu, Preethi Nanjundan, Jossy P. George, Abdelhadi El Allahi, Ilham Morino, Salma AIT Oussous, Siham Beloualid, Ahmed Tamtaoui, Abderrahim Bajit</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00235">https://arxiv.org/abs/2404.00235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00235">https://arxiv.org/pdf/2404.00235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00235]] Information Security and Privacy in the Digital World: Some Selected  Topics(https://arxiv.org/abs/2404.00235)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the era of generative artificial intelligence and the Internet of Things, while there is explosive growth in the volume of data and the associated need for processing, analysis, and storage, several new challenges are faced in identifying spurious and fake information and protecting the privacy of sensitive data. This has led to an increasing demand for more robust and resilient schemes for authentication, integrity protection, encryption, non-repudiation, and privacy-preservation of data. The chapters in this book present some of the state-of-the-art research works in the field of cryptography and security in computing and communications.</li>
</ul>

<h3>Title: Exploiting Self-Supervised Constraints in Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00260">https://arxiv.org/abs/2404.00260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00260">https://arxiv.org/pdf/2404.00260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00260]] Exploiting Self-Supervised Constraints in Image Super-Resolution(https://arxiv.org/abs/2404.00260)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent advances in self-supervised learning, predominantly studied in high-level visual tasks, have been explored in low-level image processing. This paper introduces a novel self-supervised constraint for single image super-resolution, termed SSC-SR. SSC-SR uniquely addresses the divergence in image complexity by employing a dual asymmetric paradigm and a target model updated via exponential moving average to enhance stability. The proposed SSC-SR framework works as a plug-and-play paradigm and can be easily applied to existing SR models. Empirical evaluations reveal that our SSC-SR framework delivers substantial enhancements on a variety of benchmark datasets, achieving an average increase of 0.1 dB over EDSR and 0.06 dB over SwinIR. In addition, extensive ablation studies corroborate the effectiveness of each constituent in our SSC-SR framework. Codes are available at https://github.com/Aitical/SSCSR.</li>
</ul>

<h3>Title: Image-to-Image Matching via Foundation Models: A New Perspective for  Open-Vocabulary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuan Wang, Rui Sun, Naisong Luo, Yuwen Pan, Tianzhu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00262">https://arxiv.org/abs/2404.00262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00262">https://arxiv.org/pdf/2404.00262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00262]] Image-to-Image Matching via Foundation Models: A New Perspective for  Open-Vocabulary Semantic Segmentation(https://arxiv.org/abs/2404.00262)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Open-vocabulary semantic segmentation (OVS) aims to segment images of arbitrary categories specified by class labels or captions. However, most previous best-performing methods, whether pixel grouping methods or region recognition methods, suffer from false matches between image features and category labels. We attribute this to the natural gap between the textual features and visual features. In this work, we rethink how to mitigate false matches from the perspective of image-to-image matching and propose a novel relation-aware intra-modal matching (RIM) framework for OVS based on visual foundation models. RIM achieves robust region classification by firstly constructing diverse image-modal reference features and then matching them with region features based on relation-aware ranking distribution. The proposed RIM enjoys several merits. First, the intra-modal reference features are better aligned, circumventing potential ambiguities that may arise in cross-modal matching. Second, the ranking-based matching process harnesses the structure information implicit in the inter-class relationships, making it more robust than comparing individually. Extensive experiments on three benchmarks demonstrate that RIM outperforms previous state-of-the-art methods by large margins, obtaining a lead of more than 10% in mIoU on PASCAL VOC benchmark.</li>
</ul>

<h3>Title: DiLM: Distilling Dataset into Language Model for Text-level Dataset  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Aru Maekawa, Satoshi Kosugi, Kotaro Funakoshi, Manabu Okumura</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00264">https://arxiv.org/abs/2404.00264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00264">https://arxiv.org/pdf/2404.00264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00264]] DiLM: Distilling Dataset into Language Model for Text-level Dataset  Distillation(https://arxiv.org/abs/2404.00264)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models. Our code will be available at https://github.com/arumaekawa/DiLM.</li>
</ul>

<h3>Title: IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D  Object Reconstruction from Single RGB-D Images</h3>
<ul>
<li><strong>Authors: </strong>Yushuang Wu, Luyue Shi, Junhao Cai, Weihao Yuan, Lingteng Qiu, Zilong Dong, Liefeng Bo, Shuguang Cui, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00269">https://arxiv.org/abs/2404.00269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00269">https://arxiv.org/pdf/2404.00269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00269]] IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D  Object Reconstruction from Single RGB-D Images(https://arxiv.org/abs/2404.00269)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generalizable 3D object reconstruction from single-view RGB-D images remains a challenging task, particularly with real-world data. Current state-of-the-art methods develop Transformer-based implicit field learning, necessitating an intensive learning paradigm that requires dense query-supervision uniformly sampled throughout the entire space. We propose a novel approach, IPoD, which harmonizes implicit field learning with point diffusion. This approach treats the query points for implicit field learning as a noisy point cloud for iterative denoising, allowing for their dynamic adaptation to the target object shape. Such adaptive query points harness diffusion learning's capability for coarse shape recovery and also enhances the implicit representation's ability to delineate finer details. Besides, an additional self-conditioning mechanism is designed to use implicit predictions as the guidance of diffusion learning, leading to a cooperative system. Experiments conducted on the CO3D-v2 dataset affirm the superiority of IPoD, achieving 7.8% improvement in F-score and 28.6% in Chamfer distance over existing methods. The generalizability of IPoD is also demonstrated on the MVImgNet dataset. Our project page is at https://yushuang-wu.github.io/IPoD.</li>
</ul>

<h3>Title: LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge  Retrieval-Augmented Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Pancheng Zhao, Peng Xu, Pengda Qin, Deng-Ping Fan, Zhicheng Zhang, Guoli Jia, Bowen Zhou, Jufeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00292">https://arxiv.org/abs/2404.00292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00292">https://arxiv.org/pdf/2404.00292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00292]] LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge  Retrieval-Augmented Diffusion(https://arxiv.org/abs/2404.00292)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Camouflaged vision perception is an important vision task with numerous practical applications. Due to the expensive collection and labeling costs, this community struggles with a major bottleneck that the species category of its datasets is limited to a small number of object species. However, the existing camouflaged generation methods require specifying the background manually, thus failing to extend the camouflaged sample diversity in a low-cost manner. In this paper, we propose a Latent Background Knowledge Retrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. To our knowledge, our contributions mainly include: (1) For the first time, we propose a camouflaged generation paradigm that does not need to receive any background inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmented method with interpretability for camouflaged generation, in which we propose an idea that knowledge retrieval and reasoning enhancement are separated explicitly, to alleviate the task-specific challenges. Moreover, our method is not restricted to specific foreground targets or backgrounds, offering a potential for extending camouflaged vision perception to more diverse domains. (3) Experimental results demonstrate that our method outperforms the existing approaches, generating more realistic camouflage images.</li>
</ul>

<h3>Title: MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview  and Text</h3>
<ul>
<li><strong>Authors: </strong>Takayuki Hara, Tatsuya Harada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00345">https://arxiv.org/abs/2404.00345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00345">https://arxiv.org/pdf/2404.00345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00345]] MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview  and Text(https://arxiv.org/abs/2404.00345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generation of 3D scenes from user-specified conditions offers a promising avenue for alleviating the production burden in 3D applications. Previous studies required significant effort to realize the desired scene, owing to limited control conditions. We propose a method for controlling and generating 3D scenes under multimodal conditions using partial images, layout information represented in the top view, and text prompts. Combining these conditions to generate a 3D scene involves the following significant difficulties: (1) the creation of large datasets, (2) reflection on the interaction of multimodal conditions, and (3) domain dependence of the layout conditions. We decompose the process of 3D scene generation into 2D image generation from the given conditions and 3D scene generation from 2D images. 2D image generation is achieved by fine-tuning a pretrained text-to-image model with a small artificial dataset of partial images and layouts, and 3D scene generation is achieved by layout-conditioned depth estimation and neural radiance fields (NeRF), thereby avoiding the creation of large datasets. The use of a common representation of spatial information using 360-degree images allows for the consideration of multimodal condition interactions and reduces the domain dependence of the layout control. The experimental results qualitatively and quantitatively demonstrated that the proposed method can generate 3D scenes in diverse domains, from indoor to outdoor, according to multimodal conditions.</li>
</ul>

<h3>Title: Reusable Architecture Growth for Continual Stereo Matching</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Zhang, Gaofeng Meng, Bin Fan, Kun Tian, Zhaoxiang Zhang, Shiming Xiang, Chunhong Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00360">https://arxiv.org/abs/2404.00360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00360">https://arxiv.org/pdf/2404.00360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00360]] Reusable Architecture Growth for Continual Stereo Matching(https://arxiv.org/abs/2404.00360)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The remarkable performance of recent stereo depth estimation models benefits from the successful use of convolutional neural networks to regress dense disparity. Akin to most tasks, this needs gathering training data that covers a number of heterogeneous scenes at deployment time. However, training samples are typically acquired continuously in practical applications, making the capability to learn new scenes continually even more crucial. For this purpose, we propose to perform continual stereo matching where a model is tasked to 1) continually learn new scenes, 2) overcome forgetting previously learned scenes, and 3) continuously predict disparities at inference. We achieve this goal by introducing a Reusable Architecture Growth (RAG) framework. RAG leverages task-specific neural unit search and architecture growth to learn new scenes continually in both supervised and self-supervised manners. It can maintain high reusability during growth by reusing previous units while obtaining good performance. Additionally, we present a Scene Router module to adaptively select the scene-specific architecture path at inference. Comprehensive experiments on numerous datasets show that our framework performs impressively in various weather, road, and city circumstances and surpasses the state-of-the-art methods in more challenging cross-dataset settings. Further experiments also demonstrate the adaptability of our method to unseen scenes, which can facilitate end-to-end stereo architecture learning and practical deployment.</li>
</ul>

<h3>Title: SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive  Canvas Layout</h3>
<ul>
<li><strong>Authors: </strong>Ayan Banerjee, Nityanand Mathur, Josep Lladós, Umapada Pal, Anjan Dutta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00412">https://arxiv.org/abs/2404.00412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00412">https://arxiv.org/pdf/2404.00412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00412]] SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive  Canvas Layout(https://arxiv.org/abs/2404.00412)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating VectorArt from text prompts is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained LLM for layout generation from text prompts, this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, this work explores the potential of primitive shapes in facilitating canvas completion in constrained environments. Through both qualitative and quantitative assessments, SVGCraft is demonstrated to surpass prior works in abstraction, recognizability, and detail, as evidenced by its performance metrics (CLIP-T: 0.4563, Cosine Similarity: 0.6342, Confusion: 0.66, Aesthetic: 6.7832). The code will be available at https://github.com/ayanban011/SVGCraft.</li>
</ul>

<h3>Title: Multiway Point Cloud Mosaicking with Diffusion and Global Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shengze Jin (Department of Computer Science, ETH Zurich, Switzerland), Iro Armeni (Department of Civil and Environmental Engineering, Stanford University), Marc Pollefeys (Department of Computer Science, ETH Zurich, Switzerland and Microsoft Mixed Reality & AI Lab, Zurich, Switzerland), Daniel Barath (Department of Computer Science, ETH Zurich, Switzerland)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00429">https://arxiv.org/abs/2404.00429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00429">https://arxiv.org/pdf/2404.00429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00429]] Multiway Point Cloud Mosaicking with Diffusion and Global Optimization(https://arxiv.org/abs/2404.00429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a novel framework for multiway point cloud mosaicking (named Wednesday), designed to co-align sets of partially overlapping point clouds -- typically obtained from 3D scanners or moving RGB-D cameras -- into a unified coordinate system. At the core of our approach is ODIN, a learned pairwise registration algorithm that iteratively identifies overlaps and refines attention scores, employing a diffusion-based process for denoising pairwise correlation matrices to enhance matching accuracy. Further steps include constructing a pose graph from all point clouds, performing rotation averaging, a novel robust algorithm for re-estimating translations optimally in terms of consensus maximization and translation optimization. Finally, the point cloud rotations and positions are optimized jointly by a diffusion-based approach. Tested on four diverse, large-scale datasets, our method achieves state-of-the-art pairwise and multiway registration results by a large margin on all benchmarks. Our code and models are available at https://github.com/jinsz/Multiway-Point-Cloud-Mosaicking-with-Diffusion-and-Global-Optimization.</li>
</ul>

<h3>Title: MetaIE: Distilling a Meta Model from LLM for All Kinds of Information  Extraction Tasks</h3>
<ul>
<li><strong>Authors: </strong>Letian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00457">https://arxiv.org/abs/2404.00457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00457">https://arxiv.org/pdf/2404.00457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00457]] MetaIE: Distilling a Meta Model from LLM for All Kinds of Information  Extraction Tasks(https://arxiv.org/abs/2404.00457)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract "important information", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to identify the typed spans of "important information". We evaluate the meta-model under the few-shot adaptation setting. Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer a better starting point for few-shot tuning on IE datasets and outperform other meta-models from (1) vanilla language model pre-training, (2) multi-IE-task pre-training with human annotations, and (3) single-IE-task symbolic distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE, such as the size of the distillation dataset, the meta-model architecture, and the size of the meta-model.</li>
</ul>

<h3>Title: Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to  Characterize Heterogeneity Across Alzheimer's Disease and Related Dementias</h3>
<ul>
<li><strong>Authors: </strong>Matthew West, Colin Magdamo, Lily Cheng, Yingnan He, Sudeshna Das</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00464">https://arxiv.org/abs/2404.00464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00464">https://arxiv.org/pdf/2404.00464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00464]] Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to  Characterize Heterogeneity Across Alzheimer's Disease and Related Dementias(https://arxiv.org/abs/2404.00464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease is a progressive, debilitating neurodegenerative disease that affects 50 million people globally. Despite this substantial health burden, available treatments for the disease are limited and its fundamental causes remain poorly understood. Previous work has suggested the existence of clinically-meaningful sub-types, which it is suggested may correspond to distinct etiologies, disease courses, and ultimately appropriate treatments. Here, we use unsupervised learning techniques on electronic health records (EHRs) from a cohort of memory disorder patients to characterise heterogeneity in this disease population. Pre-trained embeddings for medical codes as well as transformer-derived Clinical BERT embeddings of free text are used to encode patient EHRs. We identify the existence of sub-populations on the basis of comorbidities and shared textual features, and discuss their clinical significance.</li>
</ul>

<h3>Title: Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model  unless you have access to GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Aryo Pradipta Gema, Giwon Hong, Pasquale Minervini, Luke Daines, Beatrice Alex</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00484">https://arxiv.org/abs/2404.00484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00484">https://arxiv.org/pdf/2404.00484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00484]] Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model  unless you have access to GPT-4(https://arxiv.org/abs/2404.00484)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The NLI4CT task assesses Natural Language Inference systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various Large Language Models (LLMs) with multiple strategies, including Chain-of-Thought, In-Context Learning, and Parameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the consistency of LLMs by merging adapters that were fine-tuned separately using triplet and language modelling objectives. We found that merging the two PEFT adapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs. However, our novel methods did not produce more accurate results than GPT-4 in terms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks joint-first in the competition with 0.8328. Finally, our contamination analysis with GPT-4 indicates that there was no test data leakage.</li>
</ul>

<h3>Title: DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans</h3>
<ul>
<li><strong>Authors: </strong>Akash Sengupta, Thiemo Alldieck, Nikos Kolotouros, Enric Corona, Andrei Zanfir, Cristian Sminchisescu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00485">https://arxiv.org/abs/2404.00485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00485">https://arxiv.org/pdf/2404.00485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00485]] DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans(https://arxiv.org/abs/2404.00485)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present DiffHuman, a probabilistic method for photorealistic 3D human reconstruction from a single RGB image. Despite the ill-posed nature of this problem, most methods are deterministic and output a single solution, often resulting in a lack of geometric detail and blurriness in unseen or uncertain regions. In contrast, DiffHuman predicts a probability distribution over 3D reconstructions conditioned on an input 2D image, which allows us to sample multiple detailed 3D avatars that are consistent with the image. DiffHuman is implemented as a conditional diffusion model that denoises pixel-aligned 2D observations of an underlying 3D shape representation. During inference, we may sample 3D avatars by iteratively denoising 2D renders of the predicted 3D representation. Furthermore, we introduce a generator neural network that approximates rendering with considerably reduced runtime (55x speed up), resulting in a novel dual-branch diffusion framework. Our experiments show that DiffHuman can produce diverse and detailed reconstructions for the parts of the person that are unseen or uncertain in the input image, while remaining competitive with the state-of-the-art when reconstructing visible surfaces.</li>
</ul>

<h3>Title: Dialectical Alignment: Resolving the Tension of 3H and Security Threats  of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad Asif Ali, Lijie Hu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00486">https://arxiv.org/abs/2404.00486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00486">https://arxiv.org/pdf/2404.00486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00486]] Dialectical Alignment: Resolving the Tension of 3H and Security Threats  of LLMs(https://arxiv.org/abs/2404.00486)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts with different external evidence in context window (i.e., different ratios of poisoned factual contexts); (2) constructs the SFT dataset as well as the preference dataset based on the AI feedback and strategies above; (3) uses the above datasets for LLM alignment to defense poisoned context attack while preserving the effectiveness of in-context knowledge editing. Our experiments show that the dialectical alignment model improves poisoned data attack defense by 20 and does not require any additional prompt engineering or prior declaration of ``you may be attacked`` to the LLMs' context window.</li>
</ul>

<h3>Title: Denoising Monte Carlo Renders With Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Vaibhav Vavilala, Rahul Vasanth, David Forsyth</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00491">https://arxiv.org/abs/2404.00491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00491">https://arxiv.org/pdf/2404.00491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00491]] Denoising Monte Carlo Renders With Diffusion Models(https://arxiv.org/abs/2404.00491)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Physically-based renderings contain Monte-Carlo noise, with variance that increases as the number of rays per pixel decreases. This noise, while zero-mean for good modern renderers, can have heavy tails (most notably, for scenes containing specular or refractive objects). Learned methods for restoring low fidelity renders are highly developed, because suppressing render noise means one can save compute and use fast renders with few rays per pixel. We demonstrate that a diffusion model can denoise low fidelity renders successfully. Furthermore, our method can be conditioned on a variety of natural render information, and this conditioning helps performance. Quantitative experiments show that our method is competitive with SOTA across a range of sampling rates, but current metrics slightly favor competitor methods. Qualitative examination of the reconstructions suggests that the metrics themselves may not be reliable. The image prior applied by a diffusion method strongly favors reconstructions that are "like" real images -- so have straight shadow boundaries, curved specularities, no "fireflies" and the like -- and metrics do not account for this. We show numerous examples where methods preferred by current metrics produce qualitatively weaker reconstructions than ours.</li>
</ul>

<h3>Title: DailyMAE: Towards Pretraining Masked Autoencoders in One Day</h3>
<ul>
<li><strong>Authors: </strong>Jiantao Wu, Shentong Mo, Sara Atito, Zhenhua Feng, Josef Kittler, Muhammad Awais</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00509">https://arxiv.org/abs/2404.00509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00509">https://arxiv.org/pdf/2404.00509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00509]] DailyMAE: Towards Pretraining Masked Autoencoders in One Day(https://arxiv.org/abs/2404.00509)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recently, masked image modeling (MIM), an important self-supervised learning (SSL) method, has drawn attention for its effectiveness in learning data representation from unlabeled data. Numerous studies underscore the advantages of MIM, highlighting how models pretrained on extensive datasets can enhance the performance of downstream tasks. However, the high computational demands of pretraining pose significant challenges, particularly within academic environments, thereby impeding the SSL research progress. In this study, we propose efficient training recipes for MIM based SSL that focuses on mitigating data loading bottlenecks and employing progressive training techniques and other tricks to closely maintain pretraining performance. Our library enables the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs within just 18 hours, using a single machine equipped with 8 A100 GPUs. By achieving speed gains of up to 5.8 times, this work not only demonstrates the feasibility of conducting high-efficiency SSL training but also paves the way for broader accessibility and promotes advancement in SSL research particularly for prototyping and initial testing of SSL ideas. The code is available in https://github.com/erow/FastSSL.</li>
</ul>

<h3>Title: CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz  continuity constrAIned Normalization</h3>
<ul>
<li><strong>Authors: </strong>Yao Ni, Piotr Koniusz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00521">https://arxiv.org/abs/2404.00521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00521">https://arxiv.org/pdf/2404.00521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00521]] CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz  continuity constrAIned Normalization(https://arxiv.org/abs/2404.00521)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution few-shot image datasets.</li>
</ul>

<h3>Title: Creating synthetic energy meter data using conditional diffusion and  building metadata</h3>
<ul>
<li><strong>Authors: </strong>Chun Fu, Hussain Kazmi, Matias Quintana, Clayton Miller</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00525">https://arxiv.org/abs/2404.00525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00525">https://arxiv.org/pdf/2404.00525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00525]] Creating synthetic energy meter data using conditional diffusion and  building metadata(https://arxiv.org/abs/2404.00525)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Advances in machine learning and increased computational power have driven progress in energy-related research. However, limited access to private energy data from buildings hinders traditional regression models relying on historical data. While generative models offer a solution, previous studies have primarily focused on short-term generation periods (e.g., daily profiles) and a limited number of meters. Thus, the study proposes a conditional diffusion model for generating high-quality synthetic energy data using relevant metadata. Using a dataset comprising 1,828 power meters from various buildings and countries, this model is compared with traditional methods like Conditional Generative Adversarial Networks (CGAN) and Conditional Variational Auto-Encoders (CVAE). It explicitly handles long-term annual consumption profiles, harnessing metadata such as location, weather, building, and meter type to produce coherent synthetic data that closely resembles real-world energy consumption patterns. The results demonstrate the proposed diffusion model's superior performance, with a 36% reduction in Frechet Inception Distance (FID) score and a 13% decrease in Kullback-Leibler divergence (KL divergence) compared to the following best method. The proposed method successfully generates high-quality energy data through metadata, and its code will be open-sourced, establishing a foundation for a broader array of energy data generation models in the future.</li>
</ul>

<h3>Title: Generative weather for improved crop model simulations</h3>
<ul>
<li><strong>Authors: </strong>Yuji Saikai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00528">https://arxiv.org/abs/2404.00528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00528">https://arxiv.org/pdf/2404.00528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00528]] Generative weather for improved crop model simulations(https://arxiv.org/abs/2404.00528)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate and precise crop yield prediction is invaluable for decision making at both farm levels and regional levels. To make yield prediction, crop models are widely used for their capability to simulate hypothetical scenarios. While accuracy and precision of yield prediction critically depend on weather inputs to simulations, surprisingly little attention has been paid to preparing weather inputs. We propose a new method to construct generative models for long-term weather forecasts and ultimately improve crop yield prediction. We demonstrate use of the method in two representative scenarios -- single-year production of wheat, barley and canola and three-year production using rotations of these crops. Results show significant improvement from the conventional method, measured in terms of mean and standard deviation of prediction errors. Our method outperformed the conventional method in every one of 18 metrics for the first scenario and in 29 out of 36 metrics for the second scenario. For individual crop modellers to start applying the method to their problems, technical details are carefully explained, and all the code, trained PyTorch models, APSIM simulation files and result data are made available.</li>
</ul>

<h3>Title: Denoising Distillation Makes Event-Frame Transformers as Accurate Gaze  Trackers</h3>
<ul>
<li><strong>Authors: </strong>Jiading Li, Zhiyu Zhu, Jinhui Hou, Junhui Hou, Jinjian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00548">https://arxiv.org/abs/2404.00548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00548">https://arxiv.org/pdf/2404.00548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00548]] Denoising Distillation Makes Event-Frame Transformers as Accurate Gaze  Trackers(https://arxiv.org/abs/2404.00548)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper tackles the problem of passive gaze estimation using both event and frame data. Considering inherently different physiological structures, it's intractable to accurately estimate purely based on a given state. Thus, we reformulate the gaze estimation as the quantification of state transitions from the current state to several prior registered anchor states. Technically, we propose a two-stage learning-based gaze estimation framework to divide the whole gaze estimation process into a coarse-to-fine process of anchor state selection and final gaze location. Moreover, to improve generalization ability, we align a group of local experts with a student network, where a novel denoising distillation algorithm is introduced to utilize denoising diffusion technique to iteratively remove inherent noise of event data. Extensive experiments demonstrate the effectiveness of the proposed method, which greatly surpasses state-of-the-art methods by a large extent of 15$\%$. The code will be publicly available at https://github.com/jdjdli/Denoise_distill_EF_gazetracker.</li>
</ul>

<h3>Title: Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction</h3>
<ul>
<li><strong>Authors: </strong>Junuk Cha, Jihyeon Kim, Jae Shin Yoon, Seungryul Baek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00562">https://arxiv.org/abs/2404.00562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00562">https://arxiv.org/pdf/2404.00562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00562]] Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction(https://arxiv.org/abs/2404.00562)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text prompts. To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects' category, and thus, it is applicable to general objects. For motion generation, a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the object-hand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. Codes and data are available in: https://github.com/JunukCha/Text2HOI.</li>
</ul>

<h3>Title: ParaICL: Towards Robust Parallel In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Xingxuan Li, Xuan-Phi Nguyen, Shafiq Joty, Lidong Bing</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00570">https://arxiv.org/abs/2404.00570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00570">https://arxiv.org/pdf/2404.00570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00570]] ParaICL: Towards Robust Parallel In-Context Learning(https://arxiv.org/abs/2404.00570)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstration examples into different batches according to the semantic similarities of the questions in the demonstrations to the test question. It then computes normalized batch semantic scores for each batch. A weighted average semantic objective, constrained by adaptive plausibility, is applied to select the most appropriate tokens. Through extensive experiments, we validate the effectiveness of ParaICL and conduct ablation studies to underscore its design rationale. We further demonstrate that ParaICL can seamlessly integrate with existing methods.</li>
</ul>

<h3>Title: ADs: Active Data-sharing for Data Quality Assurance in Advanced  Manufacturing Systems</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhao, Yuxuan Li, Chenang Liu, Yinan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00572">https://arxiv.org/abs/2404.00572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00572">https://arxiv.org/pdf/2404.00572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00572]] ADs: Active Data-sharing for Data Quality Assurance in Advanced  Manufacturing Systems(https://arxiv.org/abs/2404.00572)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) methods are widely used in industrial applications, which usually require a large amount of training data. However, data collection needs extensive time costs and investments in the manufacturing system, and data scarcity commonly exists. Therefore, data-sharing is widely enabled among multiple machines with similar functionality to augment the dataset for building ML methods. However, distribution mismatch inevitably exists in their data due to different working conditions, while the ML methods are assumed to be built and tested on the dataset following the same distribution. Thus, an Active Data-sharing (ADs) framework is proposed to ensure the quality of the shared data among multiple machines. It is designed to simultaneously select the most informative data points benefiting the downstream tasks and mitigate the distribution mismatch among all selected data points. The proposed method is validated on anomaly detection on in-situ monitoring data from three additive manufacturing processes.</li>
</ul>

<h3>Title: LAESI: Leaf Area Estimation with Synthetic Imagery</h3>
<ul>
<li><strong>Authors: </strong>Jacek Kałużny, Yannik Schreckenberg, Karol Cyganik, Peter Annighöfer, Sören Pirk, Dominik L. Michels, Mikolaj Cieslak, Farhah Assaad-Gerbert, Bedrich Benes, Wojciech Pałubicki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00593">https://arxiv.org/abs/2404.00593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00593">https://arxiv.org/pdf/2404.00593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00593]] LAESI: Leaf Area Estimation with Synthetic Imagery(https://arxiv.org/abs/2404.00593)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and generative AI for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of generative AI in our procedural data generation pipeline and show how data filtering based on annotation consistency results in datasets which allow training the highest performing vision models.</li>
</ul>

<h3>Title: RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00610">https://arxiv.org/abs/2404.00610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00610">https://arxiv.org/pdf/2404.00610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00610]] RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation(https://arxiv.org/abs/2404.00610)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.</li>
</ul>

<h3>Title: Learning to Plan for Language Modeling from Unlabeled Data</h3>
<ul>
<li><strong>Authors: </strong>Nathan Cornille, Marie-Francine Moens, Florian Mai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00614">https://arxiv.org/abs/2404.00614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00614">https://arxiv.org/pdf/2404.00614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00614]] Learning to Plan for Language Modeling from Unlabeled Data(https://arxiv.org/abs/2404.00614)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.</li>
</ul>

<h3>Title: Against The Achilles' Heel: A Survey on Red Teaming for Generative  Models</h3>
<ul>
<li><strong>Authors: </strong>Lizhi Lin, Honglin Mu, Zenan Zhai, Minghan Wang, Yuxia Wang, Renxi Wang, Junjie Gao, Yixuan Zhang, Wanxiang Che, Timothy Baldwin, Xudong Han, Haonan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00629">https://arxiv.org/abs/2404.00629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00629">https://arxiv.org/pdf/2404.00629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00629]] Against The Achilles' Heel: A Survey on Red Teaming for Generative  Models(https://arxiv.org/abs/2404.00629)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new areas of research.</li>
</ul>

<h3>Title: IPT-V2: Efficient Image Processing Transformer using Hierarchical  Attentions</h3>
<ul>
<li><strong>Authors: </strong>Zhijun Tu, Kunpeng Du, Hanting Chen, Hailing Wang, Wei Li, Jie Hu, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00633">https://arxiv.org/abs/2404.00633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00633">https://arxiv.org/pdf/2404.00633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00633]] IPT-V2: Efficient Image Processing Transformer using Hierarchical  Attentions(https://arxiv.org/abs/2404.00633)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances have demonstrated the powerful capability of transformer architecture in image restoration. However, our analysis indicates that existing transformerbased methods can not establish both exact global and local dependencies simultaneously, which are much critical to restore the details and missing content of degraded images. To this end, we present an efficient image processing transformer architecture with hierarchical attentions, called IPTV2, adopting a focal context self-attention (FCSA) and a global grid self-attention (GGSA) to obtain adequate token interactions in local and global receptive fields. Specifically, FCSA applies the shifted window mechanism into the channel self-attention, helps capture the local context and mutual interaction across channels. And GGSA constructs long-range dependencies in the cross-window grid, aggregates global information in spatial dimension. Moreover, we introduce structural re-parameterization technique to feed-forward network to further improve the model capability. Extensive experiments demonstrate that our proposed IPT-V2 achieves state-of-the-art results on various image processing tasks, covering denoising, deblurring, deraining and obtains much better trade-off for performance and computational complexity than previous methods. Besides, we extend our method to image generation as latent diffusion backbone, and significantly outperforms DiTs.</li>
</ul>

<h3>Title: HypeBoy: Generative Self-Supervised Representation Learning on  Hypergraphs</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Kim, Shinhwan Kang, Fanchen Bu, Soo Yong Lee, Jaemin Yoo, Kijung Shin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00638">https://arxiv.org/abs/2404.00638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00638">https://arxiv.org/pdf/2404.00638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00638]] HypeBoy: Generative Self-Supervised Representation Learning on  Hypergraphs(https://arxiv.org/abs/2404.00638)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple nodes with hyperedges, and better capturing the topology is essential for effective representation learning. Recent advances in generative self-supervised learning (SSL) suggest that hypergraph neural networks learned from generative self supervision have the potential to effectively encode the complex hypergraph topology. Designing a generative SSL strategy for hypergraphs, however, is not straightforward. Questions remain with regard to its generative SSL task, connection to downstream tasks, and empirical properties of learned representations. In light of the promises and challenges, we propose a novel generative SSL strategy for hypergraphs. We first formulate a generative SSL task on hypergraphs, hyperedge filling, and highlight its theoretical connection to node classification. Based on the generative SSL task, we propose a hypergraph SSL method, HypeBoy. HypeBoy learns effective general-purpose hypergraph representations, outperforming 16 baseline methods across 11 benchmark datasets.</li>
</ul>

<h3>Title: Attire-Based Anomaly Detection in Restricted Areas Using YOLOv8 for  Enhanced CCTV Security</h3>
<ul>
<li><strong>Authors: </strong>Abdul Aziz A.B, Aindri Bajpai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00645">https://arxiv.org/abs/2404.00645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00645">https://arxiv.org/pdf/2404.00645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00645]] Attire-Based Anomaly Detection in Restricted Areas Using YOLOv8 for  Enhanced CCTV Security(https://arxiv.org/abs/2404.00645)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This research introduces an innovative security enhancement approach, employing advanced image analysis and soft computing. The focus is on an intelligent surveillance system that detects unauthorized individuals in restricted areas by analyzing attire. Traditional security measures face challenges in monitoring unauthorized access. Leveraging YOLOv8, an advanced object detection algorithm, our system identifies authorized personnel based on their attire in CCTV footage. The methodology involves training the YOLOv8 model on a comprehensive dataset of uniform patterns, ensuring precise recognition in specific regions. Soft computing techniques enhance adaptability to dynamic environments and varying lighting conditions. This research contributes to image analysis and soft computing, providing a sophisticated security solution. Emphasizing uniform-based anomaly detection, it establishes a foundation for robust security systems in restricted areas. The outcomes highlight the potential of YOLOv8-based surveillance in ensuring safety in sensitive locations.</li>
</ul>

<h3>Title: KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced  Transformer for 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Jihua Peng, Yanghong Zhou, P.Y. Mok</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00658">https://arxiv.org/abs/2404.00658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00658">https://arxiv.org/pdf/2404.00658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00658]] KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced  Transformer for 3D Human Pose Estimation(https://arxiv.org/abs/2404.00658)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a novel Kinematics and Trajectory Prior Knowledge-Enhanced Transformer (KTPFormer), which overcomes the weakness in existing transformer-based methods for 3D human pose estimation that the derivation of Q, K, V vectors in their self-attention mechanisms are all based on simple linear mapping. We propose two prior attention modules, namely Kinematics Prior Attention (KPA) and Trajectory Prior Attention (TPA) to take advantage of the known anatomical structure of the human body and motion trajectory information, to facilitate effective learning of global dependencies and features in the multi-head self-attention. KPA models kinematic relationships in the human body by constructing a topology of kinematics, while TPA builds a trajectory topology to learn the information of joint motion trajectory across frames. Yielding Q, K, V vectors with prior knowledge, the two modules enable KTPFormer to model both spatial and temporal correlations simultaneously. Extensive experiments on three benchmarks (Human3.6M, MPI-INF-3DHP and HumanEva) show that KTPFormer achieves superior performance in comparison to state-of-the-art methods. More importantly, our KPA and TPA modules have lightweight plug-and-play designs and can be integrated into various transformer-based networks (i.e., diffusion-based) to improve the performance with only a very small increase in the computational overhead. The code is available at: https://github.com/JihuaPeng/KTPFormer.</li>
</ul>

<h3>Title: DeeDSR: Towards Real-World Image Super-Resolution via Degradation-Aware  Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chunyang Bi, Xin Luo, Sheng Shen, Mengxi Zhang, Huanjing Yue, Jingyu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00661">https://arxiv.org/abs/2404.00661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00661">https://arxiv.org/pdf/2404.00661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00661]] DeeDSR: Towards Real-World Image Super-Resolution via Degradation-Aware  Stable Diffusion(https://arxiv.org/abs/2404.00661)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models, known for their powerful generative capabilities, play a crucial role in addressing real-world super-resolution challenges. However, these models often focus on improving local textures while neglecting the impacts of global degradation, which can significantly reduce semantic fidelity and lead to inaccurate reconstructions and suboptimal super-resolution performance. To address this issue, we introduce a novel two-stage, degradation-aware framework that enhances the diffusion model's ability to recognize content and degradation in low-resolution images. In the first stage, we employ unsupervised contrastive learning to obtain representations of image degradations. In the second stage, we integrate a degradation-aware module into a simplified ControlNet, enabling flexible adaptation to various degradations based on the learned representations. Furthermore, we decompose the degradation-aware features into global semantics and local details branches, which are then injected into the diffusion denoising module to modulate the target generation. Our method effectively recovers semantically precise and photorealistic details, particularly under significant degradation conditions, demonstrating state-of-the-art performance across various benchmarks. Codes will be released at https://github.com/bichunyang419/DeeDSR.</li>
</ul>

<h3>Title: Learning to Rank Patches for Unbiased Image Redundancy Reduction</h3>
<ul>
<li><strong>Authors: </strong>Yang Luo, Zhineng Chen, Peng Zhou, Zuxuan Wu, Xieping Gao, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00680">https://arxiv.org/abs/2404.00680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00680">https://arxiv.org/pdf/2404.00680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00680]] Learning to Rank Patches for Unbiased Image Redundancy Reduction(https://arxiv.org/abs/2404.00680)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Images suffer from heavy spatial redundancy because pixels in neighboring regions are spatially correlated. Existing approaches strive to overcome this limitation by reducing less meaningful image regions. However, current leading methods rely on supervisory signals. They may compel models to preserve content that aligns with labeled categories and discard content belonging to unlabeled categories. This categorical inductive bias makes these methods less effective in real-world scenarios. To address this issue, we propose a self-supervised framework for image redundancy reduction called Learning to Rank Patches (LTRP). We observe that image reconstruction of masked image modeling models is sensitive to the removal of visible patches when the masking ratio is high (e.g., 90\%). Building upon it, we implement LTRP via two steps: inferring the semantic density score of each patch by quantifying variation between reconstructions with and without this patch, and learning to rank the patches with the pseudo score. The entire process is self-supervised, thus getting out of the dilemma of categorical inductive bias. We design extensive experiments on different datasets and tasks. The results demonstrate that LTRP outperforms both supervised and other self-supervised methods due to the fair assessment of image content.</li>
</ul>

<h3>Title: CoUDA: Coherence Evaluation via Unified Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Dawei Zhu, Wenhao Wu, Yifan Song, Fangwei Zhu, Ziqiang Cao, Sujian Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00681">https://arxiv.org/abs/2404.00681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00681">https://arxiv.org/pdf/2404.00681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00681]] CoUDA: Coherence Evaluation via Unified Data Augmentation(https://arxiv.org/abs/2404.00681)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models. Due to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance. In this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively. Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse. Extensive experiments in coherence evaluation show that, with only 233M parameters, CoUDA achieves state-of-the-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.</li>
</ul>

<h3>Title: Privacy Re-identification Attacks on Tabular GANs</h3>
<ul>
<li><strong>Authors: </strong>Abdallah Alshantti, Adil Rasheed, Frank Westad</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00696">https://arxiv.org/abs/2404.00696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00696">https://arxiv.org/pdf/2404.00696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00696]] Privacy Re-identification Attacks on Tabular GANs(https://arxiv.org/abs/2404.00696)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models are subject to overfitting and thus may potentially leak sensitive information from the training data. In this work. we investigate the privacy risks that can potentially arise from the use of generative adversarial networks (GANs) for creating tabular synthetic datasets. For the purpose, we analyse the effects of re-identification attacks on synthetic data, i.e., attacks which aim at selecting samples that are predicted to correspond to memorised training samples based on their proximity to the nearest synthetic records. We thus consider multiple settings where different attackers might have different access levels or knowledge of the generative model and predictive, and assess which information is potentially most useful for launching more successful re-identification attacks. In doing so we also consider the situation for which re-identification attacks are formulated as reconstruction attacks, i.e., the situation where an attacker uses evolutionary multi-objective optimisation for perturbing synthetic samples closer to the training space. The results indicate that attackers can indeed pose major privacy risks by selecting synthetic samples that are likely representative of memorised training samples. In addition, we notice that privacy threats considerably increase when the attacker either has knowledge or has black-box access to the generative models. We also find that reconstruction attacks through multi-objective optimisation even increase the risk of identifying confidential samples.</li>
</ul>

<h3>Title: Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open  Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Mainak Singha, Ankit Jha, Shirsha Bose, Ashwin Nair, Moloud Abdar, Biplab Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00710">https://arxiv.org/abs/2404.00710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00710">https://arxiv.org/pdf/2404.00710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00710]] Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open  Domain Generalization(https://arxiv.org/abs/2404.00710)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We delve into Open Domain Generalization (ODG), marked by domain and category shifts between training's labeled source and testing's unlabeled target domains. Existing solutions to ODG face limitations due to constrained generalizations of traditional CNN backbones and errors in detecting target open samples in the absence of prior knowledge. Addressing these pitfalls, we introduce ODG-CLIP, harnessing the semantic prowess of the vision-language model, CLIP. Our framework brings forth three primary innovations: Firstly, distinct from prevailing paradigms, we conceptualize ODG as a multi-class classification challenge encompassing both known and novel categories. Central to our approach is modeling a unique prompt tailored for detecting unknown class samples, and to train this, we employ a readily accessible stable diffusion model, elegantly generating proxy images for the open class. Secondly, aiming for domain-tailored classification (prompt) weights while ensuring a balance of precision and simplicity, we devise a novel visual stylecentric prompt learning mechanism. Finally, we infuse images with class-discriminative knowledge derived from the prompt space to augment the fidelity of CLIP's visual embeddings. We introduce a novel objective to safeguard the continuity of this infused semantic intel across domains, especially for the shared classes. Through rigorous testing on diverse datasets, covering closed and open-set DG contexts, ODG-CLIP demonstrates clear supremacy, consistently outpacing peers with performance boosts between 8%-16%. Code will be available at https://github.com/mainaksingha01/ODG-CLIP.</li>
</ul>

<h3>Title: Absolute-Unified Multi-Class Anomaly Detection via Class-Agnostic  Distribution Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jia Guo, Shuai Lu, Weihang Zhang, Huiqi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00724">https://arxiv.org/abs/2404.00724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00724">https://arxiv.org/pdf/2404.00724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00724]] Absolute-Unified Multi-Class Anomaly Detection via Class-Agnostic  Distribution Alignment(https://arxiv.org/abs/2404.00724)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Conventional unsupervised anomaly detection (UAD) methods build separate models for each object category. Recent studies have proposed to train a unified model for multiple classes, namely model-unified UAD. However, such methods still implement the unified model separately on each class during inference with respective anomaly decision thresholds, which hinders their application when the image categories are entirely unavailable. In this work, we present a simple yet powerful method to address multi-class anomaly detection without any class information, namely \textit{absolute-unified} UAD. We target the crux of prior works in this challenging setting: different objects have mismatched anomaly score distributions. We propose Class-Agnostic Distribution Alignment (CADA) to align the mismatched score distribution of each implicit class without knowing class information, which enables unified anomaly detection for all classes and samples. The essence of CADA is to predict each class's score distribution of normal samples given any image, normal or anomalous, of this class. As a general component, CADA can activate the potential of nearly all UAD methods under absolute-unified setting. Our approach is extensively evaluated under the proposed setting on two popular UAD benchmark datasets, MVTec AD and VisA, where we exceed previous state-of-the-art by a large margin.</li>
</ul>

<h3>Title: Rethinking Interactive Image Segmentation with Low Latency, High  Quality, and Diverse Prompts</h3>
<ul>
<li><strong>Authors: </strong>Qin Liu, Jaemin Cho, Mohit Bansal, Marc Niethammer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00741">https://arxiv.org/abs/2404.00741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00741">https://arxiv.org/pdf/2404.00741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00741]] Rethinking Interactive Image Segmentation with Low Latency, High  Quality, and Diverse Prompts(https://arxiv.org/abs/2404.00741)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The goal of interactive image segmentation is to delineate specific regions within an image via visual or language prompts. Low-latency and high-quality interactive segmentation with diverse prompts remain challenging for existing specialist and generalist models. Specialist models, with their limited prompts and task-specific designs, experience high latency because the image must be recomputed every time the prompt is updated, due to the joint encoding of image and visual prompts. Generalist models, exemplified by the Segment Anything Model (SAM), have recently excelled in prompt diversity and efficiency, lifting image segmentation to the foundation model era. However, for high-quality segmentations, SAM still lags behind state-of-the-art specialist models despite SAM being trained with x100 more segmentation masks. In this work, we delve deep into the architectural differences between the two types of models. We observe that dense representation and fusion of visual prompts are the key design choices contributing to the high segmentation quality of specialist models. In light of this, we reintroduce this dense design into the generalist models, to facilitate the development of generalist models with high segmentation quality. To densely represent diverse visual prompts, we propose to use a dense map to capture five types: clicks, boxes, polygons, scribbles, and masks. Thus, we propose SegNext, a next-generation interactive segmentation approach offering low latency, high quality, and diverse prompt support. Our method outperforms current state-of-the-art methods on HQSeg-44K and DAVIS, both quantitatively and qualitatively.</li>
</ul>

<h3>Title: On the True Distribution Approximation of Minimum Bayes-Risk Decoding</h3>
<ul>
<li><strong>Authors: </strong>Atsumoto Ohashi, Ukyo Honda, Tetsuro Morimura, Yuu Jinnai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00752">https://arxiv.org/abs/2404.00752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00752">https://arxiv.org/pdf/2404.00752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00752]] On the True Distribution Approximation of Minimum Bayes-Risk Decoding(https://arxiv.org/abs/2404.00752)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation. MBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others. Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods. From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references. However, this approximation has not been the subject of in-depth study. In this study, we propose using anomaly detection to measure the degree of approximation. We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do. The results are the first to empirically support the link between the performance and the core assumption of MBR decoding.</li>
</ul>

<h3>Title: PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning</h3>
<ul>
<li><strong>Authors: </strong>Weihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan, Jure Leskovec, Matthias Fey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00776">https://arxiv.org/abs/2404.00776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00776">https://arxiv.org/pdf/2404.00776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00776]] PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning(https://arxiv.org/abs/2404.00776)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present PyTorch Frame, a PyTorch-based framework for deep learning over multi-modal tabular data. PyTorch Frame makes tabular deep learning easy by providing a PyTorch-based data structure to handle complex tabular data, introducing a model abstraction to enable modular implementation of tabular models, and allowing external foundation models to be incorporated to handle complex columns (e.g., LLMs for text columns). We demonstrate the usefulness of PyTorch Frame by implementing diverse tabular models in a modular way, successfully applying these models to complex multi-modal tabular data, and integrating our framework with PyTorch Geometric, a PyTorch library for Graph Neural Networks (GNNs), to perform end-to-end learning over relational databases.</li>
</ul>

<h3>Title: Disentangling Hippocampal Shape Variations: A Study of Neurological  Disorders Using Graph Variational Autoencoder with Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Jakaria Rabbi, Johannes Kiechle, Christian Beaulieu, Nilanjan Ray, Dana Cobzas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00785">https://arxiv.org/abs/2404.00785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00785">https://arxiv.org/pdf/2404.00785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00785]] Disentangling Hippocampal Shape Variations: A Study of Neurological  Disorders Using Graph Variational Autoencoder with Contrastive Learning(https://arxiv.org/abs/2404.00785)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in patients with Multiple Sclerosis (MS) using the hippocampus data. Our Graph VAE with Supervised Contrastive Learning shows the volume changes of the hippocampus of MS populations at different ages, and the result is consistent with the current neuroimaging literature. This research provides valuable insights into the relationship between neurological disorder and hippocampal shape changes in different age groups of MS populations using a Graph VAE with Supervised Contrastive loss.</li>
</ul>

<h3>Title: Towards Realistic Scene Generation with LiDAR Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haoxi Ran, Vitor Guizilini, Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00815">https://arxiv.org/abs/2404.00815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00815">https://arxiv.org/pdf/2404.00815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00815]] Towards Realistic Scene Generation with LiDAR Diffusion Models(https://arxiv.org/abs/2404.00815)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, geometry realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR generation in 64-beam scenario and state of the art on conditional LiDAR generation, while maintaining high efficiency compared to point-based DMs (up to 107$\times$ faster). Furthermore, by compressing LiDAR scenes into a latent space, we enable the controllability of DMs with various conditions such as semantic maps, camera views, and text prompts. Our code and pretrained weights are available at https://github.com/hancyran/LiDAR-Diffusion.</li>
</ul>

<h3>Title: Extracting Social Determinants of Health from Pediatric Patient Notes  Using Large Language Models: Novel Corpus and Methods</h3>
<ul>
<li><strong>Authors: </strong>Yujuan Fu, Giridhar Kaushik Ramachandran, Nicholas J Dobbins, Namu Park, Michael Leu, Abby R. Rosenberg, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00826">https://arxiv.org/abs/2404.00826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00826">https://arxiv.org/pdf/2404.00826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00826]] Extracting Social Determinants of Health from Pediatric Patient Notes  Using Large Language Models: Novel Corpus and Methods(https://arxiv.org/abs/2404.00826)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with an overall annotator agreement of 81.9 F1. Our proposed fine-tuning LLM-based extractors achieve high performance at 78.4 F1 for event arguments. In-context learning approaches with GPT-4 demonstrate promise for reliable SDoH extraction with limited annotated examples, with extraction performance at 82.3 F1 for event triggers.</li>
</ul>

<h3>Title: Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised  Video Anomaly Detection: A New Baseline</h3>
<ul>
<li><strong>Authors: </strong>Anas Al-lahham, Muhammad Zaigham Zaheer, Nurbek Tastan, Karthik Nandakumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00847">https://arxiv.org/abs/2404.00847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00847">https://arxiv.org/pdf/2404.00847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00847]] Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised  Video Anomaly Detection: A New Baseline(https://arxiv.org/abs/2404.00847)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Unsupervised (US) video anomaly detection (VAD) in surveillance applications is gaining more popularity recently due to its practical real-world applications. As surveillance videos are privacy sensitive and the availability of large-scale video data may enable better US-VAD systems, collaborative learning can be highly rewarding in this setting. However, due to the extremely challenging nature of the US-VAD task, where learning is carried out without any annotations, privacy-preserving collaborative learning of US-VAD systems has not been studied yet. In this paper, we propose a new baseline for anomaly detection capable of localizing anomalous events in complex surveillance videos in a fully unsupervised fashion without any labels on a privacy-preserving participant-based distributed training configuration. Additionally, we propose three new evaluation protocols to benchmark anomaly detection approaches on various scenarios of collaborations and data availability. Based on these protocols, we modify existing VAD datasets to extensively evaluate our approach as well as existing US SOTA methods on two large-scale datasets including UCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits, and codes are available here: https://github.com/AnasEmad11/CLAP</li>
</ul>

<h3>Title: Generating Content for HDR Deghosting from Frequency View</h3>
<ul>
<li><strong>Authors: </strong>Tao Hu, Qingsen Yan, Yuankai Qi, Yanning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00849">https://arxiv.org/abs/2404.00849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00849">https://arxiv.org/pdf/2404.00849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00849]] Generating Content for HDR Deghosting from Frequency View(https://arxiv.org/abs/2404.00849)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recovering ghost-free High Dynamic Range (HDR) images from multiple Low Dynamic Range (LDR) images becomes challenging when the LDR images exhibit saturation and significant motion. Recent Diffusion Models (DMs) have been introduced in HDR imaging field, demonstrating promising performance, particularly in achieving visually perceptible results compared to previous DNN-based methods. However, DMs require extensive iterations with large models to estimate entire images, resulting in inefficiency that hinders their practical application. To address this challenge, we propose the Low-Frequency aware Diffusion (LF-Diff) model for ghost-free HDR imaging. The key idea of LF-Diff is implementing the DMs in a highly compacted latent space and integrating it into a regression-based model to enhance the details of reconstructed images. Specifically, as low-frequency information is closely related to human visual perception we propose to utilize DMs to create compact low-frequency priors for the reconstruction process. In addition, to take full advantage of the above low-frequency priors, the Dynamic HDR Reconstruction Network (DHRNet) is carried out in a regression-based manner to obtain final HDR images. Extensive experiments conducted on synthetic and real-world benchmark datasets demonstrate that our LF-Diff performs favorably against several state-of-the-art methods and is 10$\times$ faster than previous DM-based methods.</li>
</ul>

<h3>Title: DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF</h3>
<ul>
<li><strong>Authors: </strong>Jie Long Lee, Chen Li, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00874">https://arxiv.org/abs/2404.00874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00874">https://arxiv.org/pdf/2404.00874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00874]] DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF(https://arxiv.org/abs/2404.00874)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present DiSR-NeRF, a diffusion-guided framework for view-consistent super-resolution (SR) NeRF. Unlike prior works, we circumvent the requirement for high-resolution (HR) reference images by leveraging existing powerful 2D super-resolution models. Nonetheless, independent SR 2D images are often inconsistent across different views. We thus propose Iterative 3D Synchronization (I3DS) to mitigate the inconsistency problem via the inherent multi-view consistency property of NeRF. Specifically, our I3DS alternates between upscaling low-resolution (LR) rendered images with diffusion models, and updating the underlying 3D representation with standard NeRF training. We further introduce Renoised Score Distillation (RSD), a novel score-distillation objective for 2D image resolution. Our RSD combines features from ancestral sampling and Score Distillation Sampling (SDS) to generate sharp images that are also LR-consistent. Qualitative and quantitative results on both synthetic and real-world datasets demonstrate that our DiSR-NeRF can achieve better results on NeRF super-resolution compared with existing works. Code and video results available at the project website.</li>
</ul>

<h3>Title: TryOn-Adapter: Efficient Fine-Grained Clothing Identity Adaptation for  High-Fidelity Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Jiazheng Xing, Chao Xu, Yijie Qian, Yang Liu, Guang Dai, Baigui Sun, Yong Liu, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00878">https://arxiv.org/abs/2404.00878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00878">https://arxiv.org/pdf/2404.00878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00878]] TryOn-Adapter: Efficient Fine-Grained Clothing Identity Adaptation for  High-Fidelity Virtual Try-On(https://arxiv.org/abs/2404.00878)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Virtual try-on focuses on adjusting the given clothes to fit a specific person seamlessly while avoiding any distortion of the patterns and textures of the garment. However, the clothing identity uncontrollability and training inefficiency of existing diffusion-based methods, which struggle to maintain the identity even with full parameter training, are significant limitations that hinder the widespread applications. In this work, we propose an effective and efficient framework, termed TryOn-Adapter. Specifically, we first decouple clothing identity into fine-grained factors: style for color and category information, texture for high-frequency details, and structure for smooth spatial adaptive transformation. Our approach utilizes a pre-trained exemplar-based diffusion model as the fundamental network, whose parameters are frozen except for the attention layers. We then customize three lightweight modules (Style Preserving, Texture Highlighting, and Structure Adapting) incorporated with fine-tuning techniques to enable precise and efficient identity control. Meanwhile, we introduce the training-free T-RePaint strategy to further enhance clothing identity preservation while maintaining the realistic try-on effect during the inference. Our experiments demonstrate that our approach achieves state-of-the-art performance on two widely-used benchmarks. Additionally, compared with recent full-tuning diffusion-based methods, we only use about half of their tunable parameters during training. The code will be made publicly available at https://github.com/jiazheng-xing/TryOn-Adapter.</li>
</ul>

<h3>Title: Model-Agnostic Human Preference Inversion in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jeeyung Kim, Ze Wang, Qiang Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00879">https://arxiv.org/abs/2404.00879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00879">https://arxiv.org/pdf/2404.00879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00879]] Model-Agnostic Human Preference Inversion in Diffusion Models(https://arxiv.org/abs/2404.00879)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Efficient text-to-image generation remains a challenging task due to the high computational costs associated with the multi-step sampling in diffusion models. Although distillation of pre-trained diffusion models has been successful in reducing sampling steps, low-step image generation often falls short in terms of quality. In this study, we propose a novel sampling design to achieve high-quality one-step image generation aligning with human preferences, particularly focusing on exploring the impact of the prior noise distribution. Our approach, Prompt Adaptive Human Preference Inversion (PAHI), optimizes the noise distributions for each prompt based on human preferences without the need for fine-tuning diffusion models. Our experiments showcase that the tailored noise distributions significantly improve image quality with only a marginal increase in computational cost. Our findings underscore the importance of noise optimization and pave the way for efficient and high-quality text-to-image synthesis.</li>
</ul>

<h3>Title: Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei He, Shichun Liu, Jun Zhao, Yiwen Ding, Yi Lu, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00884">https://arxiv.org/abs/2404.00884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00884">https://arxiv.org/pdf/2404.00884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00884]] Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large  Language Models(https://arxiv.org/abs/2404.00884)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights.</li>
</ul>

<h3>Title: Learning by Correction: Efficient Tuning Task for Zero-Shot Generative  Vision-Language Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Rongjie Li, Yu Wu, Xuming He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00909">https://arxiv.org/abs/2404.00909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00909">https://arxiv.org/pdf/2404.00909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00909]] Learning by Correction: Efficient Tuning Task for Zero-Shot Generative  Vision-Language Reasoning(https://arxiv.org/abs/2404.00909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative vision-language models (VLMs) have shown impressive performance in zero-shot vision-language tasks like image captioning and visual question answering. However, improving their zero-shot reasoning typically requires second-stage instruction tuning, which relies heavily on human-labeled or large language model-generated annotation, incurring high labeling costs. To tackle this challenge, we introduce Image-Conditioned Caption Correction (ICCC), a novel pre-training task designed to enhance VLMs' zero-shot performance without the need for labeled task-aware data. The ICCC task compels VLMs to rectify mismatches between visual and language concepts, thereby enhancing instruction following and text generation conditioned on visual inputs. Leveraging language structure and a lightweight dependency parser, we construct data samples of ICCC task from image-text datasets with low labeling and computation costs. Experimental results on BLIP-2 and InstructBLIP demonstrate significant improvements in zero-shot image-text generation-based VL tasks through ICCC instruction tuning.</li>
</ul>

<h3>Title: Towards Memorization-Free Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Daochang Liu, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00922">https://arxiv.org/abs/2404.00922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00922">https://arxiv.org/pdf/2404.00922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00922]] Towards Memorization-Free Diffusion Models(https://arxiv.org/abs/2404.00922)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Pretrained diffusion models and their outputs are widely accessible due to their exceptional capacity for synthesizing high-quality images and their open-source nature. The users, however, may face litigation risks owing to the models' tendency to memorize and regurgitate training data during inference. To address this, we introduce Anti-Memorization Guidance (AMG), a novel framework employing three targeted guidance strategies for the main causes of memorization: image and caption duplication, and highly specific user prompts. Consequently, AMG ensures memorization-free outputs while maintaining high image quality and text alignment, leveraging the synergy of its guidance methods, each indispensable in its own right. AMG also features an innovative automatic detection system for potential memorization during each step of inference process, allows selective application of guidance strategies, minimally interfering with the original sampling process to preserve output utility. We applied AMG to pretrained Denoising Diffusion Probabilistic Models (DDPM) and Stable Diffusion across various generation tasks. The results demonstrate that AMG is the first approach to successfully eradicates all instances of memorization with no or marginal impacts on image quality and text-alignment, as evidenced by FID and CLIP scores.</li>
</ul>

<h3>Title: GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields</h3>
<ul>
<li><strong>Authors: </strong>Yunsong Wang, Hanlin Chen, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00931">https://arxiv.org/abs/2404.00931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00931">https://arxiv.org/pdf/2404.00931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00931]] GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields(https://arxiv.org/abs/2404.00931)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent advancements in vision-language foundation models have significantly enhanced open-vocabulary 3D scene understanding. However, the generalizability of existing methods is constrained due to their framework designs and their reliance on 3D data. We address this limitation by introducing Generalizable Open-Vocabulary Neural Semantic Fields (GOV-NeSF), a novel approach offering a generalizable implicit representation of 3D scenes with open-vocabulary semantics. We aggregate the geometry-aware features using a cost volume, and propose a Multi-view Joint Fusion module to aggregate multi-view features through a cross-view attention mechanism, which effectively predicts view-specific blending weights for both colors and open-vocabulary features. Remarkably, our GOV-NeSF exhibits state-of-the-art performance in both 2D and 3D open-vocabulary semantic segmentation, eliminating the need for ground truth semantic labels or depth priors, and effectively generalize across scenes and datasets without fine-tuning.</li>
</ul>

<h3>Title: Diffusion-Driven Domain Adaptation for Generating 3D Molecules</h3>
<ul>
<li><strong>Authors: </strong>Haokai Hong, Wanyu Lin, Kay Chen Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00962">https://arxiv.org/abs/2404.00962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00962">https://arxiv.org/pdf/2404.00962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00962]] Diffusion-Driven Domain Adaptation for Generating 3D Molecules(https://arxiv.org/abs/2404.00962)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Can we train a molecule generator that can generate 3D molecules from a new domain, circumventing the need to collect data? This problem can be cast as the problem of domain adaptive molecule generation. This work presents a novel and principled diffusion-based approach, called GADM, that allows shifting a generative model to desired new domains without the need to collect even a single molecule. As the domain shift is typically caused by the structure variations of molecules, e.g., scaffold variations, we leverage a designated equivariant masked autoencoder (MAE) along with various masking strategies to capture the structural-grained representations of the in-domain varieties. In particular, with an asymmetric encoder-decoder module, the MAE can generalize to unseen structure variations from the target domains. These structure variations are encoded with an equivariant encoder and treated as domain supervisors to control denoising. We show that, with these encoded structural-grained domain supervisors, GADM can generate effective molecules within the desired new domains. We conduct extensive experiments across various domain adaptation tasks over benchmarking datasets. We show that our approach can improve up to 65.6% in terms of success rate defined based on molecular validity, uniqueness, and novelty compared to alternative baselines.</li>
</ul>

<h3>Title: PosterLlama: Bridging Design Ability of Langauge Model to Contents-Aware  Layout Generation</h3>
<ul>
<li><strong>Authors: </strong>Jaejung Seol, Seojun Kim, Jaejun Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.00995">https://arxiv.org/abs/2404.00995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.00995">https://arxiv.org/pdf/2404.00995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.00995]] PosterLlama: Bridging Design Ability of Langauge Model to Contents-Aware  Layout Generation(https://arxiv.org/abs/2404.00995)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual layout plays a critical role in graphic design fields such as advertising, posters, and web UI design. The recent trend towards content-aware layout generation through generative models has shown promise, yet it often overlooks the semantic intricacies of layout design by treating it as a simple numerical optimization. To bridge this gap, we introduce PosterLlama, a network designed for generating visually and textually coherent layouts by reformatting layout elements into HTML code and leveraging the rich design knowledge embedded within language models. Furthermore, we enhance the robustness of our model with a unique depth-based poster augmentation strategy. This ensures our generated layouts remain semantically rich but also visually appealing, even with limited data. Our extensive evaluations across several benchmarks demonstrate that PosterLlama outperforms existing methods in producing authentic and content-aware layouts. It supports an unparalleled range of conditions, including but not limited to unconditional layout generation, element conditional layout generation, layout completion, among others, serving as a highly versatile user manipulation tool.</li>
</ul>

<h3>Title: Harnessing Large Language Models for Training-free Video Anomaly  Detection</h3>
<ul>
<li><strong>Authors: </strong>Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, Elisa Ricci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01014">https://arxiv.org/abs/2404.01014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01014">https://arxiv.org/pdf/2404.01014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01014]] Harnessing Large Language Models for Training-free Video Anomaly  Detection(https://arxiv.org/abs/2404.01014)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Video anomaly detection (VAD) aims to temporally locate abnormal events in a video. Existing works mostly rely on training deep models to learn the distribution of normality with either video-level supervision, one-class supervision, or in an unsupervised setting. Training-based methods are prone to be domain-specific, thus being costly for practical deployment as any domain change will involve data collection and model training. In this paper, we radically depart from previous efforts and propose LAnguage-based VAD (LAVAD), a method tackling VAD in a novel, training-free paradigm, exploiting the capabilities of pre-trained large language models (LLMs) and existing vision-language models (VLMs). We leverage VLM-based captioning models to generate textual descriptions for each frame of any test video. With the textual scene description, we then devise a prompting mechanism to unlock the capability of LLMs in terms of temporal aggregation and anomaly score estimation, turning LLMs into an effective video anomaly detector. We further leverage modality-aligned VLMs and propose effective techniques based on cross-modal similarity for cleaning noisy captions and refining the LLM-based anomaly scores. We evaluate LAVAD on two large datasets featuring real-world surveillance scenarios (UCF-Crime and XD-Violence), showing that it outperforms both unsupervised and one-class methods without requiring any training or data collection.</li>
</ul>

<h3>Title: Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and  Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Yixin Wan, Arjun Subramonian, Anaelia Ovalle, Zongyu Lin, Ashima Suvarna, Christina Chance, Hritik Bansal, Rebecca Pattichis, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01030">https://arxiv.org/abs/2404.01030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01030">https://arxiv.org/pdf/2404.01030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01030]] Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and  Mitigation(https://arxiv.org/abs/2404.01030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The recent advancement of large and powerful models with Text-to-Image (T2I) generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables users to generate high-quality images from textual prompts. However, it has become increasingly evident that even simple prompts could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how these works define, evaluate, and mitigate different aspects of bias. We found that: (1) while gender and skintone biases are widely studied, geo-cultural bias remains under-explored; (2) most works on gender and skintone bias investigated occupational association, while other aspects are less frequently studied; (3) almost all gender bias works overlook non-binary identities in their studies; (4) evaluation datasets and metrics are scattered, with no unified framework for measuring biases; and (5) current mitigation methods fail to resolve biases comprehensively. Based on current limitations, we point out future research directions that contribute to human-centric definitions, evaluations, and mitigation of biases. We hope to highlight the importance of studying biases in T2I systems, as well as encourage future efforts to holistically understand and tackle biases, building fair and trustworthy T2I technologies for everyone.</li>
</ul>

<h3>Title: Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic  Propagation</h3>
<ul>
<li><strong>Authors: </strong>Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, Shengfeng He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01050">https://arxiv.org/abs/2404.01050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01050">https://arxiv.org/pdf/2404.01050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01050]] Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic  Propagation(https://arxiv.org/abs/2404.01050)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a single denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Comparative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at https://github.com/haofengl/DragNoise.</li>
</ul>

<h3>Title: Action Detection via an Image Diffusion Process</h3>
<ul>
<li><strong>Authors: </strong>Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01051">https://arxiv.org/abs/2404.01051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01051">https://arxiv.org/pdf/2404.01051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01051]] Action Detection via an Image Diffusion Process(https://arxiv.org/abs/2404.01051)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Action detection aims to localize the starting and ending points of action instances in untrimmed videos, and predict the classes of those instances. In this paper, we make the observation that the outputs of the action detection task can be formulated as images. Thus, from a novel perspective, we tackle action detection via a three-image generation process to generate starting point, ending point and action-class predictions as images via our proposed Action Detection Image Diffusion (ADI-Diff) framework. Furthermore, since our images differ from natural images and exhibit special properties, we further explore a Discrete Action-Detection Diffusion Process and a Row-Column Transformer design to better handle their processing. Our ADI-Diff framework achieves state-of-the-art results on two widely-used datasets.</li>
</ul>

<h3>Title: Efficient Prompting Methods for Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01077">https://arxiv.org/abs/2404.01077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01077">https://arxiv.org/pdf/2404.01077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01077]] Efficient Prompting Methods for Large Language Models: A Survey(https://arxiv.org/abs/2404.01077)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Prompting has become a mainstream paradigm for adapting large language models (LLMs) to specific natural language processing tasks. While this approach opens the door to in-context learning of LLMs, it brings the additional computational burden of model inference and human effort of manual-designed prompts, particularly when using lengthy and complex prompts to guide and control the behavior of LLMs. As a result, the LLM field has seen a remarkable surge in efficient prompting methods. In this paper, we present a comprehensive overview of these methods. At a high level, efficient prompting methods can broadly be categorized into two approaches: prompting with efficient computation and prompting with efficient design. The former involves various ways of compressing prompts, and the latter employs techniques for automatic prompt optimization. We present the basic concepts of prompting, review the advances for efficient prompting, and highlight future research directions.</li>
</ul>

<h3>Title: Stale Diffusion: Hyper-realistic 5D Movie Generation Using Old-school  Methods</h3>
<ul>
<li><strong>Authors: </strong>Joao F. Henriques, Dylan Campbell, Tengda Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01079">https://arxiv.org/abs/2404.01079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01079">https://arxiv.org/pdf/2404.01079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01079]] Stale Diffusion: Hyper-realistic 5D Movie Generation Using Old-school  Methods(https://arxiv.org/abs/2404.01079)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Two years ago, Stable Diffusion achieved super-human performance at generating images with super-human numbers of fingers. Following the steady decline of its technical novelty, we propose Stale Diffusion, a method that solidifies and ossifies Stable Diffusion in a maximum-entropy state. Stable Diffusion works analogously to a barn (the Stable) from which an infinite set of horses have escaped (the Diffusion). As the horses have long left the barn, our proposal may be seen as antiquated and irrelevant. Nevertheless, we vigorously defend our claim of novelty by identifying as early adopters of the Slow Science Movement, which will produce extremely important pearls of wisdom in the future. Our speed of contributions can also be seen as a quasi-static implementation of the recent call to pause AI experiments, which we wholeheartedly support. As a result of a careful archaeological expedition to 18-months-old Git commit histories, we found that naturally-accumulating errors have produced a novel entropy-maximising Stale Diffusion method, that can produce sleep-inducing hyper-realistic 5D video that is as good as one's imagination.</li>
</ul>

<h3>Title: Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Xu Yang, Changxing Ding, Zhibin Hong, Junhao Huang, Jin Tao, Xiangmin Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01089">https://arxiv.org/abs/2404.01089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01089">https://arxiv.org/pdf/2404.01089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01089]] Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On(https://arxiv.org/abs/2404.01089)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity. To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet. This enables the original self-attention layers contained in the diffusion model to achieve efficient and accurate texture transfer. Second, we propose a novel diffusion-based method that predicts a precise inpainting mask based on the person and reference garment images, further enhancing the reliability of the try-on results. In addition, we integrate mask prediction and image synthesis into a single compact model. The experimental results show that our approach can be applied to various try-on tasks, e.g., garment-to-person and person-to-person try-ons, and significantly outperforms state-of-the-art methods on popular VITON, VITON-HD databases.</li>
</ul>

<h3>Title: UFID: A Unified Framework for Input-level Backdoor Detection on  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01101">https://arxiv.org/abs/2404.01101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01101">https://arxiv.org/pdf/2404.01101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01101]] UFID: A Unified Framework for Input-level Backdoor Detection on  Diffusion Models(https://arxiv.org/abs/2404.01101)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper, we propose a Unified Framework for Input-level backdoor Detection (UFID) on the diffusion models, which is motivated by observations in the diffusion models and further validated with a theoretical causality analysis. Extensive experiments across different datasets on both conditional and unconditional diffusion models show that our method achieves a superb performance on detection effectiveness and run-time efficiency. The code is available at https://github.com/GuanZihan/official_UFID.</li>
</ul>

<h3>Title: Few shot point cloud reconstruction and denoising via learned Guassian  splats renderings and fine-tuned diffusion features</h3>
<ul>
<li><strong>Authors: </strong>Pietro Bonazzi, Marie-Julie Rakatosaona, Marco Cannici, Federico Tombari, Davide Scaramuzza</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01112">https://arxiv.org/abs/2404.01112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01112">https://arxiv.org/pdf/2404.01112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01112]] Few shot point cloud reconstruction and denoising via learned Guassian  splats renderings and fine-tuned diffusion features(https://arxiv.org/abs/2404.01112)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing deep learning methods for the reconstruction and denoising of point clouds rely on small datasets of 3D shapes. We circumvent the problem by leveraging deep learning methods trained on billions of images. We propose a method to reconstruct point clouds from few images and to denoise point clouds from their rendering by exploiting prior knowledge distilled from image-based deep learning models. To improve reconstruction in constraint settings, we regularize the training of a differentiable renderer with hybrid surface and appearance by introducing semantic consistency supervision. In addition, we propose a pipeline to finetune Stable Diffusion to denoise renderings of noisy point clouds and we demonstrate how these learned filters can be used to remove point cloud noise coming without 3D supervision. We compare our method with DSS and PointRadiance and achieved higher quality 3D reconstruction on the Sketchfab Testset and SCUT Dataset.</li>
</ul>

<h3>Title: Structured Information Matters: Incorporating Abstract Meaning  Representation into LLMs for Improved Open-Domain Dialogue Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01129">https://arxiv.org/abs/2404.01129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01129">https://arxiv.org/pdf/2404.01129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01129]] Structured Information Matters: Incorporating Abstract Meaning  Representation into LLMs for Improved Open-Domain Dialogue Evaluation(https://arxiv.org/abs/2404.01129)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs) with LLMs. The SLMs can explicitly incorporate Abstract Meaning Representation (AMR) graph information of the dialogue through a gating mechanism for enhanced semantic representation learning. The evaluation result of SLMs and AMR graph information are plugged into the prompt of LLM, for the enhanced in-context learning performance. Experimental results on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to a wide range of state-of-the-art baselines, especially in discriminating adversarial negative responses. Our code is available at https://github.com/Bernard-Yang/SIMAMR.</li>
</ul>

<h3>Title: Condition-Aware Neural Network for Controlled Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Han Cai, Muyang Li, Zhuoyang Zhang, Qinsheng Zhang, Ming-Yu Liu, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01143">https://arxiv.org/abs/2404.01143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01143">https://arxiv.org/pdf/2404.01143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01143]] Condition-Aware Neural Network for Controlled Image Generation(https://arxiv.org/abs/2404.01143)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present Condition-Aware Neural Network (CAN), a new method for adding control to image generative models. In parallel to prior conditional control methods, CAN controls the image generation process by dynamically manipulating the weight of the neural network. This is achieved by introducing a condition-aware weight generation module that generates conditional weight for convolution/linear layers based on the input condition. We test CAN on class-conditional image generation on ImageNet and text-to-image generation on COCO. CAN consistently delivers significant improvements for diffusion transformer models, including DiT and UViT. In particular, CAN combined with EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2 while requiring 52x fewer MACs per sampling step.</li>
</ul>

<h3>Title: Uncovering the Text Embedding in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hu Yu, Hao Luo, Fan Wang, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01154">https://arxiv.org/abs/2404.01154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01154">https://arxiv.org/pdf/2404.01154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01154]] Uncovering the Text Embedding in Text-to-Image Diffusion Models(https://arxiv.org/abs/2404.01154)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The correspondence between input text and the generated image exhibits opacity, wherein minor textual modifications can induce substantial deviations in the generated image. While, text embedding, as the pivotal intermediary between text and images, remains relatively underexplored. In this paper, we address this research gap by delving into the text embedding space, unleashing its capacity for controllable image editing and explicable semantic direction attributes within a learning-free framework. Specifically, we identify two critical insights regarding the importance of per-word embedding and their contextual correlations within text embedding, providing instructive principles for learning-free image editing. Additionally, we find that text embedding inherently possesses diverse semantic potentials, and further reveal this property through the lens of singular value decomposition (SVD). These uncovered properties offer practical utility for image editing and semantic discovery. More importantly, we expect the in-depth analyses and findings of the text embedding can enhance the understanding of text-to-image diffusion models.</li>
</ul>

<h3>Title: Video Interpolation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Siddhant Jain, Daniel Watson, Eric Tabellion, Aleksander Hołyński, Ben Poole, Janne Kontkanen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01203">https://arxiv.org/abs/2404.01203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01203">https://arxiv.org/pdf/2404.01203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01203]] Video Interpolation with Diffusion Models(https://arxiv.org/abs/2404.01203)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present VIDIM, a generative model for video interpolation, which creates short videos given a start and end frame. In order to achieve high fidelity and generate motions unseen in the input data, VIDIM uses cascaded diffusion models to first generate the target video at low resolution, and then generate the high-resolution video conditioned on the low-resolution generated video. We compare VIDIM to previous state-of-the-art methods on video interpolation, and demonstrate how such works fail in most settings where the underlying motion is complex, nonlinear, or ambiguous while VIDIM can easily handle such cases. We additionally demonstrate how classifier-free guidance on the start and end frame and conditioning the super-resolution model on the original high-resolution frames without additional parameters unlocks high-fidelity results. VIDIM is fast to sample from as it jointly denoises all the frames to be generated, requires less than a billion parameters per diffusion model to produce compelling results, and still enjoys scalability and improved quality at larger parameter counts.</li>
</ul>

<h3>Title: Incorporating Domain Differential Equations into Graph Convolutional  Networks to Lower Generalization Discrepancy</h3>
<ul>
<li><strong>Authors: </strong>Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S. Blum, Parv Venkitasubramaniam</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01217">https://arxiv.org/abs/2404.01217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01217">https://arxiv.org/pdf/2404.01217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01217]] Incorporating Domain Differential Equations into Graph Convolutional  Networks to Lower Generalization Discrepancy(https://arxiv.org/abs/2404.01217)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks called Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than the state-of-the-art deep learning methods.</li>
</ul>

<h3>Title: Feature Splatting: Language-Driven Physics-Based Scene Synthesis and  Editing</h3>
<ul>
<li><strong>Authors: </strong>Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01223">https://arxiv.org/abs/2404.01223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01223">https://arxiv.org/pdf/2404.01223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01223]] Feature Splatting: Language-Driven Physics-Based Scene Synthesis and  Editing(https://arxiv.org/abs/2404.01223)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunities in using feature-carrying 3D Gaussians as a unified format for appearance, geometry, material properties and semantics grounded on natural language. Project website: https://feature-splatting.github.io/</li>
</ul>

<h3>Title: Privacy Backdoors: Enhancing Membership Inference through Poisoning  Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein, Nicholas Carlini</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01231">https://arxiv.org/abs/2404.01231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01231">https://arxiv.org/pdf/2404.01231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01231]] Privacy Backdoors: Enhancing Membership Inference through Poisoning  Pre-trained Models(https://arxiv.org/abs/2404.01231)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a reevaluation of safety protocols in the use of open-source pre-trained models.</li>
</ul>

<h3>Title: StructLDM: Structured Latent Diffusion for 3D Human Generation</h3>
<ul>
<li><strong>Authors: </strong>Tao Hu, Fangzhou Hong, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01241">https://arxiv.org/abs/2404.01241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01241">https://arxiv.org/pdf/2404.01241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01241]] StructLDM: Structured Latent Diffusion for 3D Human Generation(https://arxiv.org/abs/2404.01241)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent 3D human generative models have achieved remarkable progress by learning 3D-aware GANs from 2D images. However, existing 3D human generative methods model humans in a compact 1D latent space, ignoring the articulated structure and semantics of human body topology. In this paper, we explore more expressive and higher-dimensional latent space for 3D human modeling and propose StructLDM, a diffusion-based unconditional 3D human generative model, which is learned from 2D images. StructLDM solves the challenges imposed due to the high-dimensional growth of latent space with three key designs: 1) A semantic structured latent space defined on the dense surface manifold of a statistical human body template. 2) A structured 3D-aware auto-decoder that factorizes the global latent space into several semantic body parts parameterized by a set of conditional structured local NeRFs anchored to the body template, which embeds the properties learned from the 2D training data and can be decoded to render view-consistent humans under different poses and clothing styles. 3) A structured latent diffusion model for generative human appearance sampling. Extensive experiments validate StructLDM's state-of-the-art generation performance and illustrate the expressiveness of the structured latent space over the well-adopted 1D latent space. Notably, StructLDM enables different levels of controllable 3D human generation and editing, including pose/view/shape control, and high-level tasks including compositional generations, part-aware clothing editing, 3D virtual try-on, etc. Our project page is at: https://taohuumd.github.io/projects/StructLDM/.</li>
</ul>

<h3>Title: A Unified and Interpretable Emotion Representation and Expression  Generation</h3>
<ul>
<li><strong>Authors: </strong>Reni Paskaleva, Mykyta Holubakha, Andela Ilic, Saman Motamed, Luc Van Gool, Danda Paudel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01243">https://arxiv.org/abs/2404.01243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01243">https://arxiv.org/pdf/2404.01243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01243]] A Unified and Interpretable Emotion Representation and Expression  Generation(https://arxiv.org/abs/2404.01243)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Canonical emotions, such as happy, sad, and fearful, are easy to understand and annotate. However, emotions are often compound, e.g. happily surprised, and can be mapped to the action units (AUs) used for expressing emotions, and trivially to the canonical ones. Intuitively, emotions are continuous as represented by the arousal-valence (AV) model. An interpretable unification of these four modalities - namely, Canonical, Compound, AUs, and AV - is highly desirable, for a better representation and understanding of emotions. However, such unification remains to be unknown in the current literature. In this work, we propose an interpretable and unified emotion model, referred as C2A2. We also develop a method that leverages labels of the non-unified models to annotate the novel unified one. Finally, we modify the text-conditional diffusion models to understand continuous numbers, which are then used to generate continuous expressions using our unified emotion model. Through quantitative and qualitative experiments, we show that our generated images are rich and capture subtle expressions. Our work allows a fine-grained generation of expressions in conjunction with other textual inputs and offers a new label space for emotions at the same time.</li>
</ul>

<h3>Title: An image speaks a thousand words, but can everyone listen? On  translating images for cultural relevance</h3>
<ul>
<li><strong>Authors: </strong>Simran Khanuja, Sathyanarayanan Ramamoorthy, Yueqi Song, Graham Neubig</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01247">https://arxiv.org/abs/2404.01247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01247">https://arxiv.org/pdf/2404.01247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01247]] An image speaks a thousand words, but can everyone listen? On  translating images for cultural relevance(https://arxiv.org/abs/2404.01247)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best pipelines can only translate 5% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: https://github.com/simran-khanuja/image-transcreation.</li>
</ul>

<h3>Title: Bridging Remote Sensors with Multisensor Geospatial Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Boran Han, Shuai Zhang, Xingjian Shi, Markus Reichstein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01260">https://arxiv.org/abs/2404.01260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01260">https://arxiv.org/pdf/2404.01260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01260]] Bridging Remote Sensors with Multisensor Geospatial Foundation Models(https://arxiv.org/abs/2404.01260)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.</li>
</ul>

<h3>Title: Decentralized Collaborative Learning Framework with External Privacy  Leakage Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tsuyoshi Idé, Dzung T. Phan, Rudy Raymond</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01270">https://arxiv.org/abs/2404.01270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01270">https://arxiv.org/pdf/2404.01270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01270]] Decentralized Collaborative Learning Framework with External Privacy  Leakage Analysis(https://arxiv.org/abs/2404.01270)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This paper presents two methodological advancements in decentralized multi-task learning under privacy constraints, aiming to pave the way for future developments in next-generation Blockchain platforms. First, we expand the existing framework for collaborative dictionary learning (CollabDict), which has previously been limited to Gaussian mixture models, by incorporating deep variational autoencoders (VAEs) into the framework, with a particular focus on anomaly detection. We demonstrate that the VAE-based anomaly score function shares the same mathematical structure as the non-deep model, and provide comprehensive qualitative comparison. Second, considering the widespread use of "pre-trained models," we provide a mathematical analysis on data privacy leakage when models trained with CollabDict are shared externally. We show that the CollabDict approach, when applied to Gaussian mixtures, adheres to a Renyi differential privacy criterion. Additionally, we propose a practical metric for monitoring internal privacy breaches during the learning process.</li>
</ul>

<h3>Title: LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action  Localization</h3>
<ul>
<li><strong>Authors: </strong>Akshita Gupta, Gaurav Mittal, Ahmed Magooda, Ye Yu, Graham W. Taylor, Mei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01282">https://arxiv.org/abs/2404.01282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01282">https://arxiv.org/pdf/2404.01282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01282]] LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action  Localization(https://arxiv.org/abs/2404.01282)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Temporal Action Localization (TAL) involves localizing and classifying action snippets in an untrimmed video. The emergence of large video foundation models has led RGB-only video backbones to outperform previous methods needing both RGB and optical flow modalities. Leveraging these large models is often limited to training only the TAL head due to the prohibitively large GPU memory required to adapt the video backbone for TAL. To overcome this limitation, we introduce LoSA, the first memory-and-parameter-efficient backbone adapter designed specifically for TAL to handle untrimmed videos. LoSA specializes for TAL by introducing Long-Short-range Adapters that adapt the intermediate layers of the video backbone over different temporal ranges. These adapters run parallel to the video backbone to significantly reduce memory footprint. LoSA also includes Long-Short-range Fusion that strategically combines the output of these adapters from the video backbone layers to enhance the video features provided to the TAL head. Experiments show that LoSA significantly outperforms all existing methods on standard TAL benchmarks, THUMOS-14 and ActivityNet-v1.3, by scaling end-to-end backbone adaptation to billion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them beyond head-only transfer learning.</li>
</ul>

<h3>Title: Large Motion Model for Unified Multi-Modal Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01284">https://arxiv.org/abs/2404.01284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01284">https://arxiv.org/pdf/2404.01284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01284]] Large Motion Model for Unified Multi-Modal Motion Generation(https://arxiv.org/abs/2404.01284)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human motion generation, a cornerstone technique in animation and video production, has widespread applications in various tasks like text-to-motion and music-to-dance. Previous works focus on developing specialist models tailored for each task without scalability. In this work, we present Large Motion Model (LMM), a motion-centric, multi-modal framework that unifies mainstream motion generation tasks into a generalist model. A unified motion model is appealing since it can leverage a wide range of motion data to achieve broad generalization beyond a single task. However, it is also challenging due to the heterogeneous nature of substantially different motion data and tasks. LMM tackles these challenges from three principled aspects: 1) Data: We consolidate datasets with different modalities, formats and tasks into a comprehensive yet unified motion generation dataset, MotionVerse, comprising 10 tasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2) Architecture: We design an articulated attention mechanism ArtAttention that incorporates body part-aware modeling into Diffusion Transformer backbone. 3) Pre-Training: We propose a novel pre-training strategy for LMM, which employs variable frame rates and masking forms, to better exploit knowledge from diverse training data. Extensive experiments demonstrate that our generalist LMM achieves competitive performance across various standard motion generation tasks over state-of-the-art specialist models. Notably, LMM exhibits strong generalization capabilities and emerging properties across many unseen tasks. Additionally, our ablation studies reveal valuable insights about training and scaling up large motion models for future research.</li>
</ul>

<h3>Title: Evaluating Text-to-Visual Generation with Image-to-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, Deva Ramanan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01291">https://arxiv.org/abs/2404.01291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01291">https://arxiv.org/pdf/2404.01291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01291]] Evaluating Text-to-Visual Generation with Image-to-Text Generation(https://arxiv.org/abs/2404.01291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benchmarks. We also compute VQAScore with an in-house model that follows best practices in the literature. For example, we use a bidirectional image-question encoder that allows image embeddings to depend on the question being asked (and vice versa). Our in-house model, CLIP-FlanT5, outperforms even the strongest baselines that make use of the proprietary GPT-4V. Interestingly, although we train with only images, VQAScore can also align text with video and 3D models. VQAScore allows researchers to benchmark text-to-visual generation using complex texts that capture the compositional structure of real-world prompts. We introduce GenAI-Bench, a more challenging benchmark with 1,600 compositional text prompts that require parsing scenes, objects, attributes, relationships, and high-order reasoning like comparison and logic. GenAI-Bench also offers over 15,000 human ratings for leading image and video generation models such as Stable Diffusion, DALL-E 3, and Gen2.</li>
</ul>

<h3>Title: Measuring Style Similarity in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, Tom Goldstein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01292">https://arxiv.org/abs/2404.01292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01292">https://arxiv.org/pdf/2404.01292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01292]] Measuring Style Similarity in Diffusion Models(https://arxiv.org/abs/2404.01292)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models are now widely used by graphic designers and artists. Prior works have shown that these models remember and often replicate content from their training data during generation. Hence as their proliferation increases, it has become important to perform a database search to determine whether the properties of the image are attributable to specific training data, every time before a generated image is used for professional purposes. Existing tools for this purpose focus on retrieving images of similar semantic content. Meanwhile, many artists are concerned with style replication in text-to-image models. We present a framework for understanding and extracting style descriptors from images. Our framework comprises a new dataset curated using the insight that style is a subjective property of an image that captures complex yet meaningful interactions of factors including but not limited to colors, textures, shapes, etc. We also propose a method to extract style descriptors that can be used to attribute style of a generated image to the images used in the training dataset of a text-to-image model. We showcase promising results in various style retrieval tasks. We also quantitatively and qualitatively analyze style attribution and matching in the Stable Diffusion model. Code and artifacts are available at https://github.com/learn2phoenix/CSD.</li>
</ul>

<h3>Title: CosmicMan: A Text-to-Image Foundation Model for Humans</h3>
<ul>
<li><strong>Authors: </strong>Shikai Li, Jianglin Fu, Kaiyuan Liu, Wentao Wang, Kwan-Yee Lin, Wayne Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01294">https://arxiv.org/abs/2404.01294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01294">https://arxiv.org/pdf/2404.01294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01294]] CosmicMan: A Text-to-Image Foundation Model for Humans(https://arxiv.org/abs/2404.01294)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>We present CosmicMan, a text-to-image foundation model specialized for generating high-fidelity human images. Unlike current general-purpose foundation models that are stuck in the dilemma of inferior quality and text-image misalignment for humans, CosmicMan enables generating photo-realistic human images with meticulous appearance, reasonable structure, and precise text-image alignment with detailed dense descriptions. At the heart of CosmicMan's success are the new reflections and perspectives on data and models: (1) We found that data quality and a scalable data production flow are essential for the final results from trained models. Hence, we propose a new data production paradigm, Annotate Anyone, which serves as a perpetual data flywheel to produce high-quality data with accurate yet cost-effective annotations over time. Based on this, we constructed a large-scale dataset, CosmicMan-HQ 1.0, with 6 Million high-quality real-world human images in a mean resolution of 1488x1255, and attached with precise text annotations deriving from 115 Million attributes in diverse granularities. (2) We argue that a text-to-image foundation model specialized for humans must be pragmatic -- easy to integrate into down-streaming tasks while effective in producing high-quality human images. Hence, we propose to model the relationship between dense text descriptions and image pixels in a decomposed manner, and present Decomposed-Attention-Refocusing (Daring) training framework. It seamlessly decomposes the cross-attention features in existing text-to-image diffusion model, and enforces attention refocusing without adding extra modules. Through Daring, we show that explicitly discretizing continuous text space into several basic groups that align with human body structure is the key to tackling the misalignment problem in a breeze.</li>
</ul>

<h3>Title: MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space</h3>
<ul>
<li><strong>Authors: </strong>Armand Comas-Massagué, Di Qiu, Menglei Chai, Marcel Bühler, Amit Raj, Ruiqi Gao, Qiangeng Xu, Mark Matthews, Paulo Gotardo, Octavia Camps, Sergio Orts-Escolano, Thabo Beeler</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01296">https://arxiv.org/abs/2404.01296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01296">https://arxiv.org/pdf/2404.01296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01296]] MagicMirror: Fast and High-Quality Avatar Generation with a Constrained  Search Space(https://arxiv.org/abs/2404.01296)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a novel framework for 3D human avatar generation and personalization, leveraging text prompts to enhance user engagement and customization. Central to our approach are key innovations aimed at overcoming the challenges in photo-realistic avatar synthesis. Firstly, we utilize a conditional Neural Radiance Fields (NeRF) model, trained on a large-scale unannotated multi-view dataset, to create a versatile initial solution space that accelerates and diversifies avatar generation. Secondly, we develop a geometric prior, leveraging the capabilities of Text-to-Image Diffusion Models, to ensure superior view invariance and enable direct optimization of avatar geometry. These foundational ideas are complemented by our optimization pipeline built on Variational Score Distillation (VSD), which mitigates texture loss and over-saturation issues. As supported by our extensive experiments, these strategies collectively enable the creation of custom avatars with unparalleled visual quality and better adherence to input text prompts. You can find more results and videos in our website: https://syntec-research.github.io/MagicMirror</li>
</ul>

<h3>Title: NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, Rares Ambrus</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.01300">https://arxiv.org/abs/2404.01300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.01300">https://arxiv.org/pdf/2404.01300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.01300]] NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation  Learning for Neural Radiance Fields(https://arxiv.org/abs/2404.01300)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit representation that canonicalizes scenes across domains by employing the camera trajectory for sampling. Our goal is made possible by masking random patches from NeRF's radiance and density grid and employing a standard 3D Swin Transformer to reconstruct the masked patches. In doing so, the model can learn the semantic and spatial structure of complete scenes. We pretrain this representation at scale on our proposed curated posed-RGB data, totaling over 1.6 million images. Once pretrained, the encoder is used for effective 3D transfer learning. Our novel self-supervised pretraining for NeRFs, NeRF-MAE, scales remarkably well and improves performance on various challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining, NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF scene understanding baselines on Front3D and ScanNet datasets with an absolute performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
