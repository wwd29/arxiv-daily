<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2023-12-27</h1>
<h2>diffusion</h2>
<h3>Title: Unraveling the Temporal Dynamics of the Unet in Diffusion Models. (arXiv:2312.14965v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14965">http://arxiv.org/abs/2312.14965</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14965]] Unraveling the Temporal Dynamics of the Unet in Diffusion Models(http://arxiv.org/abs/2312.14965)</code></li>
<li>Summary: <p>Diffusion models have garnered significant attention since they can
effectively learn complex multivariate Gaussian distributions, resulting in
diverse, high-quality outcomes. They introduce Gaussian noise into training
data and reconstruct the original data iteratively. Central to this iterative
process is a single Unet, adapting across time steps to facilitate generation.
Recent work revealed the presence of composition and denoising phases in this
generation process, raising questions about the Unets' varying roles. Our study
dives into the dynamic behavior of Unets within denoising diffusion
probabilistic models (DDPM), focusing on (de)convolutional blocks and skip
connections across time steps. We propose an analytical method to
systematically assess the impact of time steps and core Unet components on the
final output. This method eliminates components to study causal relations and
investigate their influence on output changes. The main purpose is to
understand the temporal dynamics and identify potential shortcuts during
inference. Our findings provide valuable insights into the various generation
phases during inference and shed light on the Unets' usage patterns across
these phases. Leveraging these insights, we identify redundancies in GLIDE (an
improved DDPM) and improve inference time by ~27% with minimal degradation in
output quality. Our ultimate goal is to guide more informed optimization
strategies for inference and influence new model designs.
</p></li>
</ul>

<h3>Title: Gaussian Harmony: Attaining Fairness in Diffusion-based Face Generation Models. (arXiv:2312.14976v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14976">http://arxiv.org/abs/2312.14976</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14976]] Gaussian Harmony: Attaining Fairness in Diffusion-based Face Generation Models(http://arxiv.org/abs/2312.14976)</code></li>
<li>Summary: <p>Diffusion models have achieved great progress in face generation. However,
these models amplify the bias in the generation process, leading to an
imbalance in distribution of sensitive attributes such as age, gender and race.
This paper proposes a novel solution to this problem by balancing the facial
attributes of the generated images. We mitigate the bias by localizing the
means of the facial attributes in the latent space of the diffusion model using
Gaussian mixture models (GMM). Our motivation for choosing GMMs over other
clustering frameworks comes from the flexible latent structure of diffusion
model. Since each sampling step in diffusion models follows a Gaussian
distribution, we show that fitting a GMM model helps us to localize the
subspace responsible for generating a specific attribute. Furthermore, our
method does not require retraining, we instead localize the subspace on-the-fly
and mitigate the bias for generating a fair dataset. We evaluate our approach
on multiple face attribute datasets to demonstrate the effectiveness of our
approach. Our results demonstrate that our approach leads to a more fair data
generation in terms of representational fairness while preserving the quality
of generated samples.
</p></li>
</ul>

<h3>Title: Emage: Non-Autoregressive Text-to-Image Generation. (arXiv:2312.14988v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14988">http://arxiv.org/abs/2312.14988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14988]] Emage: Non-Autoregressive Text-to-Image Generation(http://arxiv.org/abs/2312.14988)</code></li>
<li>Summary: <p>Autoregressive and diffusion models drive the recent breakthroughs on
text-to-image generation. Despite their huge success of generating
high-realistic images, a common shortcoming of these models is their high
inference latency - autoregressive models run more than a thousand times
successively to produce image tokens and diffusion models convert Gaussian
noise into images with many hundreds of denoising steps. In this work, we
explore non-autoregressive text-to-image models that efficiently generate
hundreds of image tokens in parallel. We develop many model variations with
different learning and inference strategies, initialized text encoders, etc.
Compared with autoregressive baselines that needs to run one thousand times,
our model only runs 16 times to generate images of competitive quality with an
order of magnitude lower inference latency. Our non-autoregressive model with
346M parameters generates an image of 256$\times$256 with about one second on
one V100 GPU.
</p></li>
</ul>

<h3>Title: Synthetic images aid the recognition of human-made art forgeries. (arXiv:2312.14998v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14998">http://arxiv.org/abs/2312.14998</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14998]] Synthetic images aid the recognition of human-made art forgeries(http://arxiv.org/abs/2312.14998)</code></li>
<li>Summary: <p>Previous research has shown that Artificial Intelligence is capable of
distinguishing between authentic paintings by a given artist and human-made
forgeries with remarkable accuracy, provided sufficient training. However, with
the limited amount of existing known forgeries, augmentation methods for
forgery detection are highly desirable. In this work, we examine the potential
of incorporating synthetic artworks into training datasets to enhance the
performance of forgery detection. Our investigation focuses on paintings by
Vincent van Gogh, for which we release the first dataset specialized for
forgery detection. To reinforce our results, we conduct the same analyses on
the artists Amedeo Modigliani and Raphael. We train a classifier to distinguish
original artworks from forgeries. For this, we use human-made forgeries and
imitations in the style of well-known artists and augment our training sets
with images in a similar style generated by Stable Diffusion and StyleGAN. We
find that the additional synthetic forgeries consistently improve the detection
of human-made forgeries. In addition, we find that, in line with previous
research, the inclusion of synthetic forgeries in the training also enables the
detection of AI-generated forgeries, especially if created using a similar
generator.
</p></li>
</ul>

<h3>Title: FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing. (arXiv:2312.15004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.15004">http://arxiv.org/abs/2312.15004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.15004]] FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing(http://arxiv.org/abs/2312.15004)</code></li>
<li>Summary: <p>Text-driven motion generation has achieved substantial progress with the
emergence of diffusion models. However, existing methods still struggle to
generate complex motion sequences that correspond to fine-grained descriptions,
depicting detailed and accurate spatio-temporal actions. This lack of fine
controllability limits the usage of motion generation to a larger audience. To
tackle these challenges, we present FineMoGen, a diffusion-based motion
generation and editing framework that can synthesize fine-grained motions, with
spatial-temporal composition to the user instructions. Specifically, FineMoGen
builds upon diffusion model with a novel transformer architecture dubbed
Spatio-Temporal Mixture Attention (SAMI). SAMI optimizes the generation of the
global attention template from two perspectives: 1) explicitly modeling the
constraints of spatio-temporal composition; and 2) utilizing sparsely-activated
mixture-of-experts to adaptively extract fine-grained features. To facilitate a
large-scale study on this new fine-grained motion generation task, we
contribute the HuMMan-MoGen dataset, which consists of 2,968 videos and 102,336
fine-grained spatio-temporal descriptions. Extensive experiments validate that
FineMoGen exhibits superior motion generation quality over state-of-the-art
methods. Notably, FineMoGen further enables zero-shot motion editing
capabilities with the aid of modern large language models (LLM), which
faithfully manipulates motion sequences with fine-grained instructions. Project
Page: https://mingyuan-zhang.github.io/projects/FineMoGen.html
</p></li>
</ul>

<h3>Title: Automatic Tooth Arrangement with Joint Features of Point and Mesh Representations via Diffusion Probabilistic Models. (arXiv:2312.15139v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.15139">http://arxiv.org/abs/2312.15139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.15139]] Automatic Tooth Arrangement with Joint Features of Point and Mesh Representations via Diffusion Probabilistic Models(http://arxiv.org/abs/2312.15139)</code></li>
<li>Summary: <p>Tooth arrangement is a crucial step in orthodontics treatment, in which
aligning teeth could improve overall well-being, enhance facial aesthetics, and
boost self-confidence. To improve the efficiency of tooth arrangement and
minimize errors associated with unreasonable designs by inexperienced
practitioners, some deep learning-based tooth arrangement methods have been
proposed. Currently, most existing approaches employ MLPs to model the
nonlinear relationship between tooth features and transformation matrices to
achieve tooth arrangement automatically. However, the limited datasets (which
to our knowledge, have not been made public) collected from clinical practice
constrain the applicability of existing methods, making them inadequate for
addressing diverse malocclusion issues. To address this challenge, we propose a
general tooth arrangement neural network based on the diffusion probabilistic
model. Conditioned on the features extracted from the dental model, the
diffusion probabilistic model can learn the distribution of teeth
transformation matrices from malocclusion to normal occlusion by gradually
denoising from a random variable, thus more adeptly managing real orthodontic
data. To take full advantage of effective features, we exploit both mesh and
point cloud representations by designing different encoding networks to extract
the tooth (local) and jaw (global) features, respectively. In addition to
traditional metrics ADD, PA-ADD, CSA, and ME_{rot}, we propose a new evaluation
metric based on dental arch curves to judge whether the generated teeth meet
the individual normal occlusion. Experimental results demonstrate that our
proposed method achieves state-of-the-art tooth alignment results and
satisfactory occlusal relationships between dental arches. We will publish the
code and dataset.
</p></li>
</ul>

<h3>Title: Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians. (arXiv:2312.14977v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14977">http://arxiv.org/abs/2312.14977</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14977]] Diffusion Models for Generative Artificial Intelligence: An Introduction for Applied Mathematicians(http://arxiv.org/abs/2312.14977)</code></li>
<li>Summary: <p>Generative artificial intelligence (AI) refers to algorithms that create
synthetic but realistic output. Diffusion models currently offer state of the
art performance in generative AI for images. They also form a key component in
more general tools, including text-to-image generators and large language
models. Diffusion models work by adding noise to the available training data
and then learning how to reverse the process. The reverse operation may then be
applied to new random data in order to produce new outputs. We provide a brief
introduction to diffusion models for applied mathematicians and statisticians.
Our key aims are (a) to present illustrative computational examples, (b) to
give a careful derivation of the underlying mathematical formulas involved, and
(c) to draw a connection with partial differential equation (PDE) diffusion
models. We provide code for the computational experiments. We hope that this
topic will be of interest to advanced undergraduate students and postgraduate
students. Portions of the material may also provide useful motivational
examples for those who teach courses in stochastic processes, inference,
machine learning, PDEs or scientific computing.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: AS-XAI: Self-supervised Automatic Semantic Interpretation for CNN. (arXiv:2312.14935v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.14935">http://arxiv.org/abs/2312.14935</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.14935]] AS-XAI: Self-supervised Automatic Semantic Interpretation for CNN(http://arxiv.org/abs/2312.14935)</code></li>
<li>Summary: <p>Explainable artificial intelligence (XAI) aims to develop transparent
explanatory approaches for "black-box" deep learning models. However,it remains
difficult for existing methods to achieve the trade-off of the three key
criteria in interpretability, namely, reliability, causality, and usability,
which hinder their practical applications. In this paper, we propose a
self-supervised automatic semantic interpretable explainable artificial
intelligence (AS-XAI) framework, which utilizes transparent orthogonal
embedding semantic extraction spaces and row-centered principal component
analysis (PCA) for global semantic interpretation of model decisions in the
absence of human interference, without additional computational costs. In
addition, the invariance of filter feature high-rank decomposition is used to
evaluate model sensitivity to different semantic concepts. Extensive
experiments demonstrate that robust and orthogonal semantic spaces can be
automatically extracted by AS-XAI, providing more effective global
interpretability for convolutional neural networks (CNNs) and generating
human-comprehensible explanations. The proposed approach offers broad
fine-grained extensible practical applications, including shared semantic
interpretation under out-of-distribution (OOD) categories, auxiliary
explanations for species that are challenging to distinguish, and
classification explanations from various perspectives.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases. (arXiv:2312.15011v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.15011">http://arxiv.org/abs/2312.15011</a></li>
<li>Code URL: <a href="https://github.com/qi-zhangyang/gemini-vs-gpt4v">https://github.com/qi-zhangyang/gemini-vs-gpt4v</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.15011]] Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases(http://arxiv.org/abs/2312.15011)</code></li>
<li>Summary: <p>The rapidly evolving sector of Multi-modal Large Language Models (MLLMs) is
at the forefront of integrating linguistic and visual processing in artificial
intelligence. This paper presents an in-depth comparative study of two
pioneering models: Google's Gemini and OpenAI's GPT-4V(ision). Our study
involves a multi-faceted evaluation of both models across key dimensions such
as Vision-Language Capability, Interaction with Humans, Temporal Understanding,
and assessments in both Intelligence and Emotional Quotients. The core of our
analysis delves into the distinct visual comprehension abilities of each model.
We conducted a series of structured experiments to evaluate their performance
in various industrial application scenarios, offering a comprehensive
perspective on their practical utility. We not only involve direct performance
comparisons but also include adjustments in prompts and scenarios to ensure a
balanced and fair analysis. Our findings illuminate the unique strengths and
niches of both models. GPT-4V distinguishes itself with its precision and
succinctness in responses, while Gemini excels in providing detailed, expansive
answers accompanied by relevant imagery and links. These understandings not
only shed light on the comparative merits of Gemini and GPT-4V but also
underscore the evolving landscape of multimodal foundation models, paving the
way for future advancements in this area. After the comparison, we attempted to
achieve better results by combining the two models. Finally, We would like to
express our profound gratitude to the teams behind GPT-4V and Gemini for their
pioneering contributions to the field. Our acknowledgments are also extended to
the comprehensive qualitative analysis presented in 'Dawn' by Yang et al. This
work, with its extensive collection of image samples, prompts, and
GPT-4V-related results, provided a foundational basis for our analysis.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images. (arXiv:2312.15037v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.15037">http://arxiv.org/abs/2312.15037</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.15037]] Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images(http://arxiv.org/abs/2312.15037)</code></li>
<li>Summary: <p>With the metaverse slowly becoming a reality and given the rapid pace of
developments toward the creation of digital humans, the need for a principled
style editing pipeline for human faces is bound to increase manifold. We cater
to this need by introducing the Latents2Semantics Autoencoder (L2SAE), a
Generative Autoencoder model that facilitates highly localized editing of style
attributes of several Regions of Interest (ROIs) in face images. The L2SAE
learns separate latent representations for encoded images' structure and style
information. Thus, allowing for structure-preserving style editing of the
chosen ROIs. The encoded structure representation is a multichannel 2D tensor
with reduced spatial dimensions, which captures both local and global structure
properties. The style representation is a 1D tensor that captures global style
attributes. In our framework, we slice the structure representation to build
strong and disentangled correspondences with different ROIs. Consequentially,
style editing of the chosen ROIs amounts to a simple combination of (a) the
ROI-mask generated from the sliced structure representation and (b) the decoded
image with global style changes, generated from the manipulated (using Gaussian
noise) global style and unchanged structure tensor. Style editing sans
additional human supervision is a significant win over SOTA style editing
pipelines because most existing works require additional human effort
(supervision) post-training for attributing semantic meaning to style edits. We
also do away with iterative-optimization-based inversion or determining
controllable latent directions post-training, which requires additional
computationally expensive operations. We provide qualitative and quantitative
results for the same over multiple applications, such as selective style
editing and swapping using test images sampled from several datasets.
</p></li>
</ul>

<h3>Title: EGAIN: Extended GAn INversion. (arXiv:2312.15116v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.15116">http://arxiv.org/abs/2312.15116</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.15116]] EGAIN: Extended GAn INversion(http://arxiv.org/abs/2312.15116)</code></li>
<li>Summary: <p>Generative Adversarial Networks (GANs) have witnessed significant advances in
recent years, generating increasingly higher quality images, which are
non-distinguishable from real ones. Recent GANs have proven to encode features
in a disentangled latent space, enabling precise control over various semantic
attributes of the generated facial images such as pose, illumination, or
gender. GAN inversion, which is projecting images into the latent space of a
GAN, opens the door for the manipulation of facial semantics of real face
images. This is useful for numerous applications such as evaluating the
performance of face recognition systems. In this work, EGAIN, an architecture
for constructing GAN inversion models, is presented. This architecture
explicitly addresses some of the shortcomings in previous GAN inversion models.
A specific model with the same name, egain, based on this architecture is also
proposed, demonstrating superior reconstruction quality over state-of-the-art
models, and illustrating the validity of the EGAIN architecture.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: C2FAR: Coarse-to-Fine Autoregressive Networks for Precise Probabilistic Forecasting. (arXiv:2312.15002v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.15002">http://arxiv.org/abs/2312.15002</a></li>
<li>Code URL: <a href="https://github.com/huaweicloud/c2far_forecasting">https://github.com/huaweicloud/c2far_forecasting</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.15002]] C2FAR: Coarse-to-Fine Autoregressive Networks for Precise Probabilistic Forecasting(http://arxiv.org/abs/2312.15002)</code></li>
<li>Summary: <p>We present coarse-to-fine autoregressive networks (C2FAR), a method for
modeling the probability distribution of univariate, numeric random variables.
C2FAR generates a hierarchical, coarse-to-fine discretization of a variable
autoregressively; progressively finer intervals of support are generated from a
sequence of binned distributions, where each distribution is conditioned on
previously-generated coarser intervals. Unlike prior (flat) binned
distributions, C2FAR can represent values with exponentially higher precision,
for only a linear increase in complexity. We use C2FAR for probabilistic
forecasting via a recurrent neural network, thus modeling time series
autoregressively in both space and time. C2FAR is the first method to
simultaneously handle discrete and continuous series of arbitrary scale and
distribution shape. This flexibility enables a variety of time series use
cases, including anomaly detection, interpolation, and compression. C2FAR
achieves improvements over the state-of-the-art on several benchmark
forecasting datasets.
</p></li>
</ul>

<h2>in-context</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
