<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-03</h1>
<h3>Title: Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset</h3>
<ul>
<li><strong>Authors: </strong>Leroy Z. Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01219">https://arxiv.org/abs/2510.01219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01219">https://arxiv.org/pdf/2510.01219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01219]] Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset(https://arxiv.org/abs/2510.01219)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We introduce a dataset of concept learning tasks that helps uncover implicit biases in large language models. Using in-context concept learning experiments, we found that language models may have a bias toward upward monotonicity in quantifiers; such bias is less apparent when the model is tested by direct prompting without concept learning components. This demonstrates that in-context concept learning can be an effective way to discover hidden biases in language models.</li>
</ul>

<h3>Title: Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Seyma Yaman Kayadibi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01242">https://arxiv.org/abs/2510.01242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01242">https://arxiv.org/pdf/2510.01242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01242]] Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI(https://arxiv.org/abs/2510.01242)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artificial intelligence is observed to age not through chronological time but through structural asymmetries in memory performance. In large language models, semantic cues such as the name of the day often remain stable across sessions, while episodic details like the sequential progression of experiment numbers tend to collapse when conversational context is reset. To capture this phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled, entropy-informed metric of memory aging derived from observable recall behavior. The score is formally proven to be well-defined, bounded, and monotonic under mild and model-agnostic assumptions, making it applicable across various tasks and domains. In its Redundancy-as-Masking formulation, the score interprets redundancy as overlapping information that reduces the penalized mass. However, in the present study, redundancy is not explicitly estimated; all reported values assume a redundancy-neutral setting (R = 0), yielding conservative upper bounds. The AAS framework was tested over a 25-day bilingual study involving ChatGPT-5, structured into stateless and persistent interaction phases. During persistent sessions, the model consistently recalled both semantic and episodic details, driving the AAS toward its theoretical minimum, indicative of structural youth. In contrast, when sessions were reset, the model preserved semantic consistency but failed to maintain episodic continuity, causing a sharp increase in the AAS and signaling structural memory aging. These findings support the utility of AAS as a theoretically grounded, task-independent diagnostic tool for evaluating memory degradation in artificial systems. The study builds on foundational concepts from von Neumann's work on automata, Shannon's theories of information and redundancy, and Turing's behavioral approach to intelligence.</li>
</ul>

<h3>Title: SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs</h3>
<ul>
<li><strong>Authors: </strong>Ruyue Liu, Rong Yin, Xiangzhen Bo, Xiaoshuai Hao, Yong Liu, Jinwen Zhong, Can Ma, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01248">https://arxiv.org/abs/2510.01248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01248">https://arxiv.org/pdf/2510.01248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01248]] SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs(https://arxiv.org/abs/2510.01248)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Large scale pretrained models have revolutionized Natural Language Processing (NLP) and Computer Vision (CV), showcasing remarkable cross domain generalization abilities. However, in graph learning, models are typically trained on individual graph datasets, limiting their capacity to transfer knowledge across different graphs and tasks. This approach also heavily relies on large volumes of annotated data, which presents a significant challenge in resource-constrained settings. Unlike NLP and CV, graph structured data presents unique challenges due to its inherent heterogeneity, including domain specific feature spaces and structural diversity across various applications. To address these challenges, we propose a novel structure aware self supervised learning method for Text Attributed Graphs (SSTAG). By leveraging text as a unified representation medium for graph learning, SSTAG bridges the gap between the semantic reasoning of Large Language Models (LLMs) and the structural modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces a dual knowledge distillation framework that co-distills both LLMs and GNNs into structure-aware multilayer perceptrons (MLPs), enhancing the scalability of large-scale TAGs. Additionally, we introduce an in-memory mechanism that stores typical graph representations, aligning them with memory anchors in an in-memory repository to integrate invariant knowledge, thereby improving the model's generalization ability. Extensive experiments demonstrate that SSTAG outperforms state-of-the-art models on cross-domain transfer learning tasks, achieves exceptional scalability, and reduces inference costs while maintaining competitive performance.</li>
</ul>

<h3>Title: Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Carlo Bono, Federico Belotti, Matteo Palmonari</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01251">https://arxiv.org/abs/2510.01251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01251">https://arxiv.org/pdf/2510.01251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01251]] Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data(https://arxiv.org/abs/2510.01251)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Linking textual values in tabular data to their corresponding entities in a Knowledge Base is a core task across a variety of data integration and enrichment applications. Although Large Language Models (LLMs) have shown State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in real-world scenarios requires not only accurate predictions but also reliable uncertainty estimates, which require resource-demanding multi-shot inference, posing serious limits to their actual applicability. As a more efficient alternative, we investigate a self-supervised approach for estimating uncertainty from single-shot LLM outputs using token-level features, reducing the need for multiple generations. Evaluation is performed on an EL task on tabular data across multiple LLMs, showing that the resulting uncertainty estimates are highly effective in detecting low-accuracy outputs. This is achieved at a fraction of the computational cost, ultimately supporting a cost-effective integration of uncertainty measures into LLM-based EL workflows. The method offers a practical way to incorporate uncertainty estimation into EL workflows with limited computational overhead.</li>
</ul>

<h3>Title: Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse</h3>
<ul>
<li><strong>Authors: </strong>Nathan Junzi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01258">https://arxiv.org/abs/2510.01258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01258">https://arxiv.org/pdf/2510.01258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01258]] Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse(https://arxiv.org/abs/2510.01258)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Amidst the rapid normalization of generative artificial intelligence (GAI), intelligent systems have come to dominate political discourse across information mediums. However, internalized political biases stemming from training data skews, human prejudice, and algorithmic flaws continue to plague the novel technology. This paper employs a zero-shot classification approach to evaluate algorithmic political partisanship through a methodical combination of ideological alignment, topicality, response sentiment, and objectivity. A total of 1800 model responses across six mainstream large language models (LLMs) were individually input into four distinct fine-tuned classification algorithms, each responsible for computing an aforementioned bias evaluation metric. Results show an amplified liberal-authoritarian alignment across all six LLMs evaluated, with notable instances of reasoning supersessions and canned refusals. The study subsequently highlights the psychological influences underpinning human-computer interactions and how intrinsic biases can permeate public discourse. The resulting distortion of the political landscape can ultimately manifest as conformity or polarization, depending on a region's pre-existing socio-political structures.</li>
</ul>

<h3>Title: TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shenxu Chang, Junchi Yu, Weixing Wang, Yongqiang Chen, Jialin Yu, Philip Torr, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01274">https://arxiv.org/abs/2510.01274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01274">https://arxiv.org/pdf/2510.01274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01274]] TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models(https://arxiv.org/abs/2510.01274)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (D-LLMs) have recently emerged as a promising alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination problem in D-LLMs remains underexplored, limiting their reliability in real-world applications. Existing hallucination detection methods are designed for AR-LLMs and rely on signals from single-step generation, making them ill-suited for D-LLMs where hallucination signals often emerge throughout the multi-step denoising process. To bridge this gap, we propose TraceDet, a novel framework that explicitly leverages the intermediate denoising steps of D-LLMs for hallucination detection. TraceDet models the denoising process as an action trace, with each action defined as the model's prediction over the cleaned response, conditioned on the previous intermediate output. By identifying the sub-trace that is maximally informative to the hallucinated responses, TraceDet leverages the key hallucination signals in the multi-step denoising process of D-LLMs for hallucination detection. Extensive experiments on various open source D-LLMs demonstrate that TraceDet consistently improves hallucination detection, achieving an average gain in AUROC of 15.2% compared to baselines.</li>
</ul>

<h3>Title: LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration</h3>
<ul>
<li><strong>Authors: </strong>Alessio Spagnoletti, Andrés Almansa, Marcelo Pereyra</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01339">https://arxiv.org/abs/2510.01339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01339">https://arxiv.org/pdf/2510.01339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01339]] LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration(https://arxiv.org/abs/2510.01339)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency.</li>
</ul>

<h3>Title: Self-Supervised Representation Learning as Mutual Information Maximization</h3>
<ul>
<li><strong>Authors: </strong>Akhlaqur Rahman Sabby, Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01345">https://arxiv.org/abs/2510.01345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01345">https://arxiv.org/pdf/2510.01345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01345]] Self-Supervised Representation Learning as Mutual Information Maximization(https://arxiv.org/abs/2510.01345)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised representation learning (SSRL) has demonstrated remarkable empirical success, yet its underlying principles remain insufficiently understood. While recent works attempt to unify SSRL methods by examining their information-theoretic objectives or summarizing their heuristics for preventing representation collapse, architectural elements like the predictor network, stop-gradient operation, and statistical regularizer are often viewed as empirically motivated additions. In this paper, we adopt a first-principles approach and investigate whether the learning objective of an SSRL algorithm dictates its possible optimization strategies and model design choices. In particular, by starting from a variational mutual information (MI) lower bound, we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint MI (JMI), each imposing distinct structural constraints and covering a set of existing SSRL algorithms. SDMI inherently requires alternating optimization, making stop-gradient operations theoretically essential. In contrast, JMI admits joint optimization through symmetric architectures without such components. Under the proposed formulation, predictor networks in SDMI and statistical regularizers in JMI emerge as tractable surrogates for the MI objective. We show that many existing SSRL methods are specific instances or approximations of these two paradigms. This paper provides a theoretical explanation behind the choices of different architectural components of existing SSRL methods, beyond heuristic conveniences.</li>
</ul>

<h3>Title: Image Generation Based on Image Style Extraction</h3>
<ul>
<li><strong>Authors: </strong>Shuochen Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01347">https://arxiv.org/abs/2510.01347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01347">https://arxiv.org/pdf/2510.01347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01347]] Image Generation Based on Image Style Extraction(https://arxiv.org/abs/2510.01347)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image generation based on text-to-image generation models is a task with practical application scenarios that fine-grained styles cannot be precisely described and controlled in natural language, while the guidance information of stylized reference images is difficult to be directly aligned with the textual conditions of traditional textual guidance generation. This study focuses on how to maximize the generative capability of the pretrained generative model, by obtaining fine-grained stylistic representations from a single given stylistic reference image, and injecting the stylistic representations into the generative body without changing the structural framework of the downstream generative model, so as to achieve fine-grained controlled stylized image generation. In this study, we propose a three-stage training style extraction-based image generation method, which uses a style encoder and a style projection layer to align the style representations with the textual representations to realize fine-grained textual cue-based style guide generation. In addition, this study constructs the Style30k-captions dataset, whose samples contain a triad of images, style labels, and text descriptions, to train the style encoder and style projection layer in this experiment.</li>
</ul>

<h3>Title: RheOFormer: A generative transformer model for simulation of complex fluids and flows</h3>
<ul>
<li><strong>Authors: </strong>Maedeh Saberi, Amir Barati Farimani, Safa Jamali</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01365">https://arxiv.org/abs/2510.01365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01365">https://arxiv.org/pdf/2510.01365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01365]] RheOFormer: A generative transformer model for simulation of complex fluids and flows(https://arxiv.org/abs/2510.01365)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The ability to model mechanics of soft materials under flowing conditions is key in designing and engineering processes and materials with targeted properties. This generally requires solution of internal stress tensor, related to the deformation tensor through nonlinear and history-dependent constitutive models. Traditional numerical methods for non-Newtonian fluid dynamics often suffer from prohibitive computational demands and poor scalability to new problem instances. Developments in data-driven methods have mitigated some limitations but still require retraining across varied physical conditions. In this work, we introduce Rheological Operator Transformer (RheOFormer), a generative operator learning method leveraging self-attention to efficiently learn different spatial interactions and features of complex fluid flows. We benchmark RheOFormer across a range of different viscometric and non-viscometric flows with different types of viscoelastic and elastoviscoplastic mechanics in complex domains against ground truth solutions. Our results demonstrate that RheOFormer can accurately learn both scalar and tensorial nonlinear mechanics of different complex fluids and predict the spatio-temporal evolution of their flows, even when trained on limited datasets. Its strong generalization capabilities and computational efficiency establish RheOFormer as a robust neural surrogate for accelerating predictive complex fluid simulations, advancing data-driven experimentation, and enabling real-time process optimization across a wide range of applications.</li>
</ul>

<h3>Title: SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs</h3>
<ul>
<li><strong>Authors: </strong>Abu Bucker Siddik, Diane Oyen, Alexander Most, Michal Kucer, Ayan Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01370">https://arxiv.org/abs/2510.01370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01370">https://arxiv.org/pdf/2510.01370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01370]] SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs(https://arxiv.org/abs/2510.01370)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce Small PDE U-Net Solver (SPUS), a compact and efficient foundation model (FM) designed as a unified neural operator for solving a wide range of partial differential equations (PDEs). Unlike existing state-of-the-art PDE FMs-primarily based on large complex transformer architectures with high computational and parameter overhead-SPUS leverages a lightweight residual U-Net-based architecture that has been largely underexplored as a foundation model architecture in this domain. To enable effective learning in this minimalist framework, we utilize a simple yet powerful auto-regressive pretraining strategy which closely replicates the behavior of numerical solvers to learn the underlying physics. SPUS is pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6 challenging unseen downstream PDEs spanning various physical systems. Experimental results demonstrate that SPUS using residual U-Net based architecture achieves state-of-the-art generalization on these downstream tasks while requiring significantly fewer parameters and minimal fine-tuning data, highlighting its potential as a highly parameter-efficient FM for solving diverse PDE systems.</li>
</ul>

<h3>Title: Selective Underfitting in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kiwhan Song, Jaeyeon Kim, Sitan Chen, Yilun Du, Sham Kakade, Vincent Sitzmann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01378">https://arxiv.org/abs/2510.01378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01378">https://arxiv.org/pdf/2510.01378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01378]] Selective Underfitting in Diffusion Models(https://arxiv.org/abs/2510.01378)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as the principal paradigm for generative modeling across various domains. During training, they learn the score function, which in turn is used to generate samples at inference. They raise a basic yet unsolved question: which score do they actually learn? In principle, a diffusion model that matches the empirical score in the entire data space would simply reproduce the training data, failing to generate novel samples. Recent work addresses this question by arguing that diffusion models underfit the empirical score due to training-time inductive biases. In this work, we refine this perspective, introducing the notion of selective underfitting: instead of underfitting the score everywhere, better diffusion models more accurately approximate the score in certain regions of input space, while underfitting it in others. We characterize these regions and design empirical interventions to validate our perspective. Our results establish that selective underfitting is essential for understanding diffusion models, yielding new, testable insights into their generalization and generative performance.</li>
</ul>

<h3>Title: Fine-Tuning Masked Diffusion for Provable Self-Correction</h3>
<ul>
<li><strong>Authors: </strong>Jaeyeon Kim, Seunggeun Kim, Taekyun Lee, David Z. Pan, Hyeji Kim, Sham Kakade, Sitan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01384">https://arxiv.org/abs/2510.01384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01384">https://arxiv.org/pdf/2510.01384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01384]] Fine-Tuning Masked Diffusion for Provable Self-Correction(https://arxiv.org/abs/2510.01384)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>A natural desideratum for generative models is self-correction--detecting and revising low-quality tokens at inference. While Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces, their capacity for self-correction remains poorly understood. Prior attempts to incorporate self-correction into MDMs either require overhauling MDM architectures/training or rely on imprecise proxies for token quality, limiting their applicability. Motivated by this, we introduce PRISM--Plug-in Remasking for Inference-time Self-correction of Masked Diffusions--a lightweight, model-agnostic approach that applies to any pretrained MDM. Theoretically, PRISM defines a self-correction loss that provably learns per-token quality scores, without RL or a verifier. These quality scores are computed in the same forward pass with MDM and used to detect low-quality tokens. Empirically, PRISM advances MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B).</li>
</ul>

<h3>Title: DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation</h3>
<ul>
<li><strong>Authors: </strong>Shubhankar Borse, Farzad Farhadzadeh, Munawar Hayat, Fatih Porikli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01399">https://arxiv.org/abs/2510.01399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01399">https://arxiv.org/pdf/2510.01399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01399]] DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation(https://arxiv.org/abs/2510.01399)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-image models excel at realism but collapse on multi-human prompts - duplicating faces, merging identities, and miscounting individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the first RL-based framework to directly optimize identity diversity in multi-human generation. DisCo fine-tunes flow-matching models via Group-Relative Policy Optimization (GRPO) with a compositional reward that (i) penalizes intra-image facial similarity, (ii) discourages cross-sample identity repetition, (iii) enforces accurate person counts, and (iv) preserves visual fidelity through human preference scores. A single-stage curriculum stabilizes training as complexity scales, requiring no extra annotations. On the DiverseHumans Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global Identity Spread - surpassing both open-source and proprietary methods (e.g., Gemini, GPT-Image) while maintaining competitive perceptual quality. Our results establish DisCo as a scalable, annotation-free solution that resolves the long-standing identity crisis in generative models and sets a new benchmark for compositional multi-human generation.</li>
</ul>

<h3>Title: Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression</h3>
<ul>
<li><strong>Authors: </strong>Yifei Zuo, Yutong Yin, Zhichen Zeng, Ang Li, Banghua Zhu, Zhaoran Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01450">https://arxiv.org/abs/2510.01450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01450">https://arxiv.org/pdf/2510.01450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01450]] Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression(https://arxiv.org/abs/2510.01450)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and $\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at this https URL.</li>
</ul>

<h3>Title: SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Brett Barkley, Preston Culbertson, David Fridovich-Keil</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01456">https://arxiv.org/abs/2510.01456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01456">https://arxiv.org/pdf/2510.01456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01456]] SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion(https://arxiv.org/abs/2510.01456)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) detection is essential for reliable deployment of machine learning systems in vision, robotics, reinforcement learning, and beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion (SCOPED), a fast and general-purpose OOD detection method for diffusion models that reduces the number of forward passes on the trained model by an order of magnitude compared to prior methods, outperforming most diffusion-based baselines and closely approaching the accuracy of the strongest ones. SCOPED is computed from a single diffusion model trained once on a diverse dataset, and combines the Jacobian trace and squared norm of the model's score function into a single test statistic. Rather than thresholding on a fixed value, we estimate the in-distribution density of SCOPED scores using kernel density estimation, enabling a flexible, unsupervised test that, in the simplest case, only requires a single forward pass and one Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator. On four vision benchmarks, SCOPED achieves competitive or state-of-the-art precision-recall scores despite its low computational cost. The same method generalizes to robotic control tasks with shared state and action spaces, identifying distribution shifts across reward functions and training regimes. These results position SCOPED as a practical foundation for fast and reliable OOD detection in real-world domains, including perceptual artifacts in vision, outlier detection in autoregressive models, exploration in reinforcement learning, and dataset curation for unsupervised training.</li>
</ul>

<h3>Title: AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Ou, Ning Bi, Jiazhen Pan, Jiancheng Yang, Boliang Yu, Usama Zidan, Regent Lee, Vicente Grau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01498">https://arxiv.org/abs/2510.01498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01498">https://arxiv.org/pdf/2510.01498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01498]] AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging(https://arxiv.org/abs/2510.01498)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at this https URL.</li>
</ul>

<h3>Title: Flock: A Knowledge Graph Foundation Model via Learning on Random Walks</h3>
<ul>
<li><strong>Authors: </strong>Jinwoo Kim, Xingyue Huang, Krzysztof Olejniczak, Kyungbin Min, Michael Bronstein, Seunghoon Hong, İsmail İlkan Ceylan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01510">https://arxiv.org/abs/2510.01510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01510">https://arxiv.org/pdf/2510.01510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01510]] Flock: A Knowledge Graph Foundation Model via Learning on Random Walks(https://arxiv.org/abs/2510.01510)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We study the problem of zero-shot link prediction on knowledge graphs (KGs), which requires models to generalize over novel entities and novel relations. Knowledge graph foundation models (KGFMs) address this task by enforcing equivariance over both nodes and relations, learning from structural properties of nodes and relations, which are then transferable to novel graphs with similar structural properties. However, the conventional notion of deterministic equivariance imposes inherent limits on the expressive power of KGFMs, preventing them from distinguishing structurally similar but semantically distinct relations. To overcome this limitation, we introduce probabilistic node-relation equivariance, which preserves equivariance in distribution while incorporating a principled randomization to break symmetries during inference. Building on this principle, we present Flock, a KGFM that iteratively samples random walks, encodes them into sequences via a recording protocol, embeds them with a sequence model, and aggregates representations of nodes and relations via learned pooling. Crucially, Flock respects probabilistic node-relation equivariance and is a universal approximator for isomorphism-invariant link-level functions over KGs. Empirically, Flock perfectly solves our new diagnostic dataset Petals where current KGFMs fail, and achieves state-of-the-art performances on entity- and relation prediction tasks on 54 KGs from diverse domains.</li>
</ul>

<h3>Title: CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Diptyaroop Maji, Kang Yang, Prashant Shenoy, Ramesh K Sitaraman, Mani Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01521">https://arxiv.org/abs/2510.01521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01521">https://arxiv.org/pdf/2510.01521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01521]] CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models(https://arxiv.org/abs/2510.01521)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Computational decarbonization aims to reduce carbon emissions in computing and societal systems such as data centers, transportation, and built environments. This requires accurate, fine-grained carbon intensity forecasts, yet existing tools have several key limitations: (i) they require grid-specific electricity mix data, restricting use where such information is unavailable; (ii) they depend on separate grid-specific models that make it challenging to provide global coverage; and (iii) they provide forecasts without uncertainty estimates, limiting reliability for downstream carbon-aware applications. In this paper, we present CarbonX, an open-source tool that leverages Time Series Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX utilizes the versatility of TSFMs to provide strong performance across multiple tasks, such as carbon intensity forecasting and imputation, and across diverse grids. Using only historical carbon intensity data and a single general model, our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE) of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX performance is comparable with the current state-of-the-art, with an average MAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing prediction intervals with 95% coverage. CarbonX can provide forecasts for up to 21 days with minimal accuracy degradation. Further, when fully fine-tuned, CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation task. Overall, these results demonstrate that CarbonX can be used easily on any grid with limited data and still deliver strong performance, making it a practical tool for global-scale decarbonization.</li>
</ul>

<h3>Title: Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lecheng Kong, Xiyuan Wang, Yixin Chen, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01527">https://arxiv.org/abs/2510.01527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01527">https://arxiv.org/pdf/2510.01527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01527]] Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs(https://arxiv.org/abs/2510.01527)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are emerging as versatile foundation models for computational chemistry, handling bidirectional tasks like reaction prediction and retrosynthesis. However, these models often lack round-trip consistency. For instance, a state-of-the-art chemical LLM may successfully caption a molecule, yet be unable to accurately reconstruct the original structure from its own generated text. This inconsistency suggests that models are learning unidirectional memorization rather than flexible mastery. Indeed, recent work has demonstrated a strong correlation between a model's round-trip consistency and its performance on the primary tasks. This strong correlation reframes consistency into a direct target for model improvement. We therefore introduce Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model to improve its consistency by using the success of a round-trip transformation as a reward signal. We further propose an iterative variant where forward and reverse mappings alternately train each other in a self-improvement loop, a process that is highly data-efficient and notably effective with the massive amount of unlabelled data common in chemistry. Experiments demonstrate that RTRL significantly \textbf{boosts performance and consistency} over strong baselines across supervised, self-supervised, and synthetic data regimes. This work shows that round-trip consistency is not just a desirable property but a trainable objective, offering a new path toward more robust and reliable foundation models.</li>
</ul>

<h3>Title: Towards Better Optimization For Listwise Preference in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiamu Bai, Xin Yu, Meilong Xu, Weitao Lu, Xin Pan, Kiwan Maeng, Daniel Kifer, Jian Wang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01540">https://arxiv.org/abs/2510.01540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01540">https://arxiv.org/pdf/2510.01540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01540]] Towards Better Optimization For Listwise Preference in Diffusion Models(https://arxiv.org/abs/2510.01540)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.</li>
</ul>

<h3>Title: Growing Visual Generative Capacity for Pre-Trained MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Hanyu Wang, Jiaming Han, Ziyan Yang, Qi Zhao, Shanchuan Lin, Xiangyu Yue, Abhinav Shrivastava, Zhenheng Yang, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01546">https://arxiv.org/abs/2510.01546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01546">https://arxiv.org/pdf/2510.01546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01546]] Growing Visual Generative Capacity for Pre-Trained MLLMs(https://arxiv.org/abs/2510.01546)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) extend the success of language models to visual understanding, and recent efforts have sought to build unified MLLMs that support both understanding and generation. However, constructing such models remains challenging: hybrid approaches combine continuous embeddings with diffusion or flow-based objectives, producing high-quality images but breaking the autoregressive paradigm, while pure autoregressive approaches unify text and image prediction over discrete visual tokens but often face trade-offs between semantic alignment and pixel-level fidelity. In this work, we present Bridge, a pure autoregressive unified MLLM that augments pre-trained visual understanding models with generative ability through a Mixture-of-Transformers architecture, enabling both image understanding and generation within a single next-token prediction framework. To further improve visual generation fidelity, we propose a semantic-to-pixel discrete representation that integrates compact semantic tokens with fine-grained pixel tokens, achieving strong language alignment and precise description of visual details with only a 7.9% increase in sequence length. Extensive experiments across diverse multimodal benchmarks demonstrate that Bridge achieves competitive or superior results in both understanding and generation benchmarks, while requiring less training data and reduced training time compared to prior unified MLLMs.</li>
</ul>

<h3>Title: MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Zhai, Utsav Singh, Anirudh Thatipelli, Souradip Chakraborty, Anit Kumar Sahu, Furong Huang, Amrit Singh Bedi, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01549">https://arxiv.org/abs/2510.01549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01549">https://arxiv.org/pdf/2510.01549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01549]] MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models(https://arxiv.org/abs/2510.01549)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at generating images conditioned on text prompts, but the resulting images often do not satisfy user-specific criteria measured by scalar rewards such as Aesthetic Scores. This alignment typically requires fine-tuning, which is computationally demanding. Recently, inference-time alignment via noise optimization has emerged as an efficient alternative, modifying initial input noise to steer the diffusion denoising process towards generating high-reward images. However, this approach suffers from reward hacking, where the model produces images that score highly, yet deviate significantly from the original prompt. We show that noise-space regularization is insufficient and that preventing reward hacking requires an explicit image-space constraint. To this end, we propose MIRA (MItigating Reward hAcking), a training-free, inference-time alignment method. MIRA introduces an image-space, score-based KL surrogate that regularizes the sampling trajectory with a frozen backbone, constraining the output distribution so reward can increase without off-distribution drift (reward hacking). We derive a tractable approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g., Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while preserving prompt adherence; mechanism plots show reward gains with near-zero drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO, mapping preference optimization to inference time with a frozen backbone, extending MIRA to non-differentiable rewards without fine-tuning.</li>
</ul>

<h3>Title: TetriServe: Efficient DiT Serving for Heterogeneous Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Runyu Lu, Shiqi He, Wenxuan Tan, Shenggui Li, Ruofan Wu, Jeff J. Ma, Ang Chen, Mosharaf Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01565">https://arxiv.org/abs/2510.01565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01565">https://arxiv.org/pdf/2510.01565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01565]] TetriServe: Efficient DiT Serving for Heterogeneous Image Generation(https://arxiv.org/abs/2510.01565)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT) models excel at generating highquality images through iterative denoising steps, but serving them under strict Service Level Objectives (SLOs) is challenging due to their high computational cost, particularly at large resolutions. Existing serving systems use fixed degree sequence parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment. In this paper, we propose step-level sequence parallelism to dynamically adjust the parallel degree of individual requests according to their deadlines. We present TetriServe, a DiT serving system that implements this strategy for highly efficient image generation. Specifically, TetriServe introduces a novel round-based scheduling mechanism that improves SLO attainment: (1) discretizing time into fixed rounds to make deadline-aware scheduling tractable, (2) adapting parallelism at the step level and minimize GPU hour consumption, and (3) jointly packing requests to minimize late completions. Extensive evaluation on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.</li>
</ul>

<h3>Title: Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziming Tang, Chengbin Hou, Tianyu Zhang, Bangxu Tian, Jinbao Wang, Hairong Lv</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01588">https://arxiv.org/abs/2510.01588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01588">https://arxiv.org/pdf/2510.01588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01588]] Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation(https://arxiv.org/abs/2510.01588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is one of the most common neurodegenerative disorder. PD telemonitoring emerges as a novel assessment modality enabling self-administered at-home tests of Unified Parkinson's Disease Rating Scale (UPDRS) scores, enhancing accessibility for PD patients. However, three types of noise would occur during measurements: (1) patient-induced measurement inaccuracies, (2) environmental noise, and (3) data packet loss during transmission, resulting in higher prediction errors. To address these challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First, the original speech features are grouped into ordered bins, based on the continuous values of a selected feature, to construct contrastive pairs. Second, the contrastive pairs are employed to train a multilayer perceptron encoder for generating noise-robust features. Finally, these features are concatenated with the original features as the augmented features, which are then fed into the UPDRS prediction models. Notably, we further introduces a novel evaluation approach with customizable noise injection module, and extensive experiments show that NoRo can successfully enhance the noise robustness of UPDRS prediction across various downstream prediction models under different noisy environments.</li>
</ul>

<h3>Title: Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness</h3>
<ul>
<li><strong>Authors: </strong>Youwei Bao, Shuhan Yang, Hyunsoo Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01598">https://arxiv.org/abs/2510.01598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01598">https://arxiv.org/pdf/2510.01598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01598]] Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness(https://arxiv.org/abs/2510.01598)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deterministic pseudo random number generators (PRNGs) used in generative artificial intelligence (GAI) models produce predictable patterns vulnerable to exploitation by attackers. Conventional defences against the vulnerabilities often come with significant energy and latency overhead. Here, we embed hardware-generated true random bits from spin-transfer torque magnetic tunnel junctions (STT-MTJs) to address the challenges. A highly parallel, FPGA-assisted prototype computing system delivers megabit-per-second true random numbers, passing NIST randomness tests after in-situ operations with minimal overhead. Integrating the hardware random bits into a generative adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to 18.6 times compared to the low-quality random number generators (RNG) baseline. With nanosecond switching speed, high energy efficiency, and established scalability, our STT-MTJ-based system holds the potential to scale beyond 106 parallel cells, achieving gigabit-per-second throughput suitable for large language model sampling. This advancement highlights spintronic RNGs as practical security components for next-generation GAI systems.</li>
</ul>

<h3>Title: NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Roman Jacome, Romario Gualdrón-Hurtado, Leon Suarez, Henry Arguello</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01608">https://arxiv.org/abs/2510.01608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01608">https://arxiv.org/pdf/2510.01608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01608]] NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems(https://arxiv.org/abs/2510.01608)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Imaging inverse problems aims to recover high-dimensional signals from undersampled, noisy measurements, a fundamentally ill-posed task with infinite solutions in the null-space of the sensing operator. To resolve this ambiguity, prior information is typically incorporated through handcrafted regularizers or learned models that constrain the solution space. However, these priors typically ignore the task-specific structure of that null-space. In this work, we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel class of regularization that, instead of enforcing structural constraints in the image domain, promotes solutions that lie in a low-dimensional projection of the sensing matrix's null-space with a neural network. Our approach has two key advantages: (1) Interpretability: by focusing on the structure of the null-space, we design sensing-matrix-specific priors that capture information orthogonal to the signal components that are fundamentally blind to the sensing process. (2) Flexibility: NPN is adaptable to various inverse problems, compatible with existing reconstruction frameworks, and complementary to conventional image-domain priors. We provide theoretical guarantees on convergence and reconstruction accuracy when used within plug-and-play methods. Empirical results across diverse sensing matrices demonstrate that NPN priors consistently enhance reconstruction fidelity in various imaging inverse problems, such as compressive sensing, deblurring, super-resolution, computed tomography, and magnetic resonance imaging, with plug-and-play methods, unrolling networks, deep image prior, and diffusion models.</li>
</ul>

<h3>Title: Posterior Collapse as a Phase Transition in Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Zhen Li, Fan Zhang, Zheng Zhang, Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01621">https://arxiv.org/abs/2510.01621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01621">https://arxiv.org/pdf/2510.01621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01621]] Posterior Collapse as a Phase Transition in Variational Autoencoders(https://arxiv.org/abs/2510.01621)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate the phenomenon of posterior collapse in variational autoencoders (VAEs) from the perspective of statistical physics, and reveal that it constitutes a phase transition governed jointly by data structure and model hyper-parameters. By analyzing the stability of the trivial solution associated with posterior collapse, we identify a critical hyper-parameter threshold. This critical boundary, separating meaningful latent inference from collapse, is characterized by a discontinuity in the KL divergence between the approximate posterior and the prior distribution. We validate this critical behavior on both synthetic and real-world datasets, confirming the existence of a phase transition. Our results demonstrate that posterior collapse is not merely an optimization failure, but rather an emerging phase transition arising from the interplay between data structure and variational constraints. This perspective offers new insights into the trainability and representational capacity of deep generative models.</li>
</ul>

<h3>Title: FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Liu, Zhengyan Zhou, Zihang Xu, Jiezhang Cao, Zheng Chen, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01641">https://arxiv.org/abs/2510.01641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01641">https://arxiv.org/pdf/2510.01641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01641]] FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring(https://arxiv.org/abs/2510.01641)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at this https URL.</li>
</ul>

<h3>Title: Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing</h3>
<ul>
<li><strong>Authors: </strong>Amin Jalali, Milad Soltany, Michael Greenspan, Ali Etemad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01658">https://arxiv.org/abs/2510.01658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01658">https://arxiv.org/pdf/2510.01658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01658]] Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing(https://arxiv.org/abs/2510.01658)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We propose TimeHUT, a novel method for learning time-series representations by hierarchical uniformity-tolerance balancing of contrastive representations. Our method uses two distinct losses to learn strong representations with the aim of striking an effective balance between uniformity and tolerance in the embedding space. First, TimeHUT uses a hierarchical setup to learn both instance-wise and temporal information from input time-series. Next, we integrate a temperature scheduler within the vanilla contrastive loss to balance the uniformity and tolerance characteristics of the embeddings. Additionally, a hierarchical angular margin loss enforces instance-wise and temporal contrast losses, creating geometric margins between positive and negative pairs of temporal sequences. This approach improves the coherence of positive pairs and their separation from the negatives, enhancing the capture of temporal dependencies within a time-series sample. We evaluate our approach on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and multivariate classification, as well as Yahoo and KPI datasets for anomaly detection. The results demonstrate that TimeHUT outperforms prior methods by considerable margins on classification, while obtaining competitive results for anomaly detection. Finally, detailed sensitivity and ablation studies are performed to evaluate different components and hyperparameters of our method.</li>
</ul>

<h3>Title: Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale</h3>
<ul>
<li><strong>Authors: </strong>Yongbo Chen, Yanhao Zhang, Shaifali Parashar, Liang Zhao, Shoudong Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01665">https://arxiv.org/abs/2510.01665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01665">https://arxiv.org/pdf/2510.01665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01665]] Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale(https://arxiv.org/abs/2510.01665)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Non-rigid structure-from-motion (NRSfM), a promising technique for addressing the mapping challenges in monocular visual deformable simultaneous localization and mapping (SLAM), has attracted growing attention. We introduce a novel method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing isometric deformations as a subset. Our approach performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework. Unlike existing methods that rely on strict assumptions, such as locally planar surfaces or locally linear deformations, and fail to recover the conformal scale, our method eliminates these constraints and accurately computes the local conformal scale. Additionally, our framework decouples constraints on depth and conformal scale, which are inseparable in other approaches, enabling more precise depth estimation. To address the sensitivity of the formulated problem, we employ a parallel separable iterative optimization strategy. Furthermore, a self-supervised learning framework, utilizing an encoder-decoder network, is incorporated to generate dense 3D point clouds with texture. Simulation and experimental results using both synthetic and real datasets demonstrate that our method surpasses existing approaches in terms of reconstruction accuracy and robustness. The code for the proposed method will be made publicly available on the project website: this https URL.</li>
</ul>

<h3>Title: UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jin Cao, Hongrui Wu, Ziyong Feng, Hujun Bao, Xiaowei Zhou, Sida Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01669">https://arxiv.org/abs/2510.01669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01669">https://arxiv.org/pdf/2510.01669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01669]] UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction(https://arxiv.org/abs/2510.01669)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene this http URL, these methods rely heavily on dense observations for robustly optimizing model this http URL address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization this http URL this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored this http URL with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image this http URL experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: this https URL</li>
</ul>

<h3>Title: Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations</h3>
<ul>
<li><strong>Authors: </strong>Yue Li, Linying Xue, Dongdong Lin, Qiushi Li, Hui Tian, Hongxia Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01699">https://arxiv.org/abs/2510.01699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01699">https://arxiv.org/pdf/2510.01699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01699]] Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations(https://arxiv.org/abs/2510.01699)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the flourishing prosperity of generative models, manipulated facial images have become increasingly accessible, raising concerns regarding privacy infringement and societal trust. In response, proactive defense strategies embed adversarial perturbations into facial images to counter deepfake manipulation. However, existing methods often face a tradeoff between imperceptibility and defense effectiveness-strong perturbations may disrupt forgeries but degrade visual fidelity. Recent studies have attempted to address this issue by introducing additional visual loss constraints, yet often overlook the underlying gradient conflicts among losses, ultimately weakening defense performance. To bridge the gap, we propose a gradient-projection-based adversarial proactive defense (GRASP) method that effectively counters facial deepfakes while minimizing perceptual degradation. GRASP is the first approach to successfully integrate both structural similarity loss and low-frequency loss to enhance perturbation imperceptibility. By analyzing gradient conflicts between defense effectiveness loss and visual quality losses, GRASP pioneers the design of the gradient-projection mechanism to mitigate these conflicts, enabling balanced optimization that preserves image fidelity without sacrificing defensive performance. Extensive experiments validate the efficacy of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense success rate against facial attribute manipulations, significantly outperforming existing approaches in visual quality.</li>
</ul>

<h3>Title: ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning</h3>
<ul>
<li><strong>Authors: </strong>Aidan Acquah, Shing Chan, Aiden Doherty</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01712">https://arxiv.org/abs/2510.01712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01712">https://arxiv.org/pdf/2510.01712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01712]] ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning(https://arxiv.org/abs/2510.01712)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The use of reliable and accurate human activity recognition (HAR) models on passively collected wrist-accelerometer data is essential in large-scale epidemiological studies that investigate the association between physical activity and health outcomes. While the use of self-supervised learning has generated considerable excitement in improving HAR, it remains unknown the extent to which these models, coupled with hidden Markov models (HMMs), would make a tangible improvement to classification performance, and the effect this may have on the predicted daily activity intensity compositions. Using 151 CAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised, 18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM) smoothing to classify labels of activity intensity. The performance of this model, evaluated using 5-fold stratified group cross-validation, was then compared to a baseline random forest (RF) + HMM, established in existing literature. Differences in performance and classification outputs were compared with different subgroups of age and sex within the Capture-24 population. The ActiNet model was able to distinguish labels of activity intensity with a mean macro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the performance of the RF + HMM, trained and validated on the same dataset, with mean scores of 0.77 and 0.81, respectively. These findings were consistent across subgroups of age and sex. These findings encourage the use of ActiNet for the extraction of activity intensity labels from wrist-accelerometer data in future epidemiological studies.</li>
</ul>

<h3>Title: Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Mandal, Yashaswini Murthy, R. Srikant</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01721">https://arxiv.org/abs/2510.01721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01721">https://arxiv.org/pdf/2510.01721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01721]] Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation(https://arxiv.org/abs/2510.01721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Distributionally robust reinforcement learning (DRRL) focuses on designing policies that achieve good performance under model uncertainties. In particular, we are interested in maximizing the worst-case long-term discounted reward, where the data for RL comes from a nominal model while the deployed environment can deviate from the nominal model within a prescribed uncertainty set. Existing convergence guarantees for robust temporal-difference (TD) learning for policy evaluation are limited to tabular MDPs or are dependent on restrictive discount-factor assumptions when function approximation is used. We present the first robust TD learning with linear function approximation, where robustness is measured with respect to the total-variation distance and Wasserstein-l distance uncertainty set. Additionally, our algorithm is both model-free and does not require generative access to the MDP. Our algorithm combines a two-time-scale stochastic-approximation update with an outer-loop target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample complexity to obtain an $\epsilon$-accurate value estimate. Our results close a key gap between the empirical success of robust RL algorithms and the non-asymptotic guarantees enjoyed by their non-robust counterparts. The key ideas in the paper also extend in a relatively straightforward fashion to robust Q-learning with function approximation.</li>
</ul>

<h3>Title: Rethinking the shape convention of an MLP</h3>
<ul>
<li><strong>Authors: </strong>Meng-Hsi Chen, Yu-Ang Lee, Feng-Ting Liao, Da-shan Shiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01796">https://arxiv.org/abs/2510.01796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01796">https://arxiv.org/pdf/2510.01796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01796]] Rethinking the shape convention of an MLP(https://arxiv.org/abs/2510.01796)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.</li>
</ul>

<h3>Title: Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Olivier Goudet, Quentin Suire, Adrien Goëffon, Frédéric Saubion, Sylvain Lamprier</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01824">https://arxiv.org/abs/2510.01824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01824">https://arxiv.org/pdf/2510.01824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01824]] Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning(https://arxiv.org/abs/2510.01824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce an order-invariant reinforcement learning framework for black-box combinatorial optimization. Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently. In contrast, we parameterize a multivariate autoregressive generative model trained without a fixed variable ordering. By sampling random generation orders during training - a form of information-preserving dropout - the model is encouraged to be invariant to variable order, promoting search-space diversity and shaping the model to focus on the most relevant variable dependencies, improving sample efficiency. We adapt Generalized Reinforcement Policy Optimization (GRPO) to this setting, providing stable policy-gradient updates from scale-invariant advantages. Across a wide range of benchmark algorithms and problem instances of varying sizes, our method frequently achieves the best performance and consistently avoids catastrophic failures.</li>
</ul>

<h3>Title: Leveraging Prior Knowledge of Diffusion Model for Person Search</h3>
<ul>
<li><strong>Authors: </strong>Giyeol Kim, Sooyoung Yang, Jihyong Oh, Myungjoo Kang, Chanho Eom</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01841">https://arxiv.org/abs/2510.01841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01841">https://arxiv.org/pdf/2510.01841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01841]] Leveraging Prior Knowledge of Diffusion Model for Person Search(https://arxiv.org/abs/2510.01841)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.</li>
</ul>

<h3>Title: Learning Representations Through Contrastive Neural Model Checking</h3>
<ul>
<li><strong>Authors: </strong>Vladimir Krsmanovic, Matthias Cosler, Mohamed Ghanem, Bernd Finkbeiner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01853">https://arxiv.org/abs/2510.01853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01853">https://arxiv.org/pdf/2510.01853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01853]] Learning Representations Through Contrastive Neural Model Checking(https://arxiv.org/abs/2510.01853)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Model checking is a key technique for verifying safety-critical systems against formal specifications, where recent applications of deep learning have shown promise. However, while ubiquitous for vision and language domains, representation learning remains underexplored in formal verification. We introduce Contrastive Neural Model Checking (CNML), a novel method that leverages the model checking task as a guiding signal for learning aligned representations. CNML jointly embeds logical specifications and systems into a shared latent space through a self-supervised contrastive objective. On industry-inspired retrieval tasks, CNML considerably outperforms both algorithmic and neural baselines in cross-modal and intra-modal this http URL further show that the learned representations effectively transfer to downstream tasks and generalize to more complex formulas. These findings demonstrate that model checking can serve as an objective for learning representations for formal languages.</li>
</ul>

<h3>Title: Compositional meta-learning through probabilistic task inference</h3>
<ul>
<li><strong>Authors: </strong>Jacob J. W. Bakermans, Pablo Tano, Reidar Riveland, Charles Findling, Alexandre Pouget</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01858">https://arxiv.org/abs/2510.01858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01858">https://arxiv.org/pdf/2510.01858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01858]] Compositional meta-learning through probabilistic task inference(https://arxiv.org/abs/2510.01858)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To solve a new task from minimal experience, it is essential to effectively reuse knowledge from previous tasks, a problem known as meta-learning. Compositional solutions, where common elements of computation are flexibly recombined into new configurations, are particularly well-suited for meta-learning. Here, we propose a compositional meta-learning model that explicitly represents tasks as structured combinations of reusable computations. We achieve this by learning a generative model that captures the underlying components and their statistics shared across a family of tasks. This approach transforms learning a new task into a probabilistic inference problem, which allows for finding solutions without parameter updates through highly constrained hypothesis testing. Our model successfully recovers ground truth components and statistics in rule learning and motor learning tasks. We then demonstrate its ability to quickly infer new solutions from just single examples. Together, our framework joins the expressivity of neural networks with the data-efficiency of probabilistic inference to achieve rapid compositional meta-learning.</li>
</ul>

<h3>Title: Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data</h3>
<ul>
<li><strong>Authors: </strong>Thomas Gravier, Thomas Boyer, Auguste Genovesio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01894">https://arxiv.org/abs/2510.01894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01894">https://arxiv.org/pdf/2510.01894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01894]] Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data(https://arxiv.org/abs/2510.01894)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Many natural dynamic processes -- such as in vivo cellular differentiation or disease progression -- can only be observed through the lens of static sample snapshots. While challenging, reconstructing their temporal evolution to decipher underlying dynamic properties is of major interest to scientific research. Existing approaches enable data transport along a temporal axis but are poorly scalable in high dimension and require restrictive assumptions to be met. To address these issues, we propose \textit{\textbf{Multi-Marginal temporal Schrödinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video generation from unpaired data}, extending the theoretical guarantees and empirical efficiency of Diffusion Schrödinger Bridge Matching (arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting algorithm to multiple marginals in a novel factorized fashion. Experiments show that MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real world datasets such as transcriptomic trajectory inference in 100 dimensions, and for the first time recovers couplings and dynamics in very high dimensional image settings. Our work establishes multi-marginal Schrödinger bridges as a practical and principled approach for recovering hidden dynamics from static data.</li>
</ul>

<h3>Title: Multimodal Foundation Models for Early Disease Detection</h3>
<ul>
<li><strong>Authors: </strong>Md Talha Mohsin, Ismail Abdulrashid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01899">https://arxiv.org/abs/2510.01899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01899">https://arxiv.org/pdf/2510.01899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01899]] Multimodal Foundation Models for Early Disease Detection(https://arxiv.org/abs/2510.01899)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Healthcare generates diverse streams of data, including electronic health records (EHR), medical imaging, genetics, and ongoing monitoring from wearable devices. Traditional diagnostic models frequently analyze these sources in isolation, which constrains their capacity to identify cross-modal correlations essential for early disease diagnosis. Our research presents a multimodal foundation model that consolidates diverse patient data through an attention-based transformer framework. At first, dedicated encoders put each modality into a shared latent space. Then, they combine them using multi-head attention and residual normalization. The architecture is made for pretraining on many tasks, which makes it easy to adapt to new diseases and datasets with little extra work. We provide an experimental strategy that uses benchmark datasets in oncology, cardiology, and neurology, with the goal of testing early detection tasks. The framework includes data governance and model management tools in addition to technological performance to improve transparency, reliability, and clinical interpretability. The suggested method works toward a single foundation model for precision diagnostics, which could improve the accuracy of predictions and help doctors make decisions.</li>
</ul>

<h3>Title: Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yi Ai, Yuanhao Cai, Yulun Zhang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01912">https://arxiv.org/abs/2510.01912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01912">https://arxiv.org/pdf/2510.01912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01912]] Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction(https://arxiv.org/abs/2510.01912)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) provides rich spatial-spectral information but remains costly to acquire due to hardware limitations and the difficulty of reconstructing three-dimensional data from compressed measurements. Although compressive sensing systems such as CASSI improve efficiency, accurate reconstruction is still challenged by severe degradation and loss of fine spectral details. We propose the Flow-Matching-guided Unfolding network (FMU), which, to our knowledge, is the first to integrate flow matching into HSI reconstruction by embedding its generative prior within a deep unfolding framework. To further strengthen the learned dynamics, we introduce a mean velocity loss that enforces global consistency of the flow, leading to a more robust and accurate reconstruction. This hybrid design leverages the interpretability of optimization-based methods and the generative capacity of flow matching. Extensive experiments on both simulated and real datasets show that FMU significantly outperforms existing approaches in reconstruction quality. Code and models will be available at this https URL.</li>
</ul>

<h3>Title: Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors</h3>
<ul>
<li><strong>Authors: </strong>Guangyao Zhai, Yue Zhou, Xinyan Deng, Lars Heckler, Nassir Navab, Benjamin Busam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01934">https://arxiv.org/abs/2510.01934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01934">https://arxiv.org/pdf/2510.01934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01934]] Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors(https://arxiv.org/abs/2510.01934)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Few-shot anomaly detection streamlines and simplifies industrial safety inspection. However, limited samples make accurate differentiation between normal and abnormal features challenging, and even more so under category-agnostic conditions. Large-scale pre-training of foundation visual encoders has advanced many fields, as the enormous quantity of data helps to learn the general distribution of normal images. We observe that the anomaly amount in an image directly correlates with the difference in the learnt embeddings and utilize this to design a few-shot anomaly detector termed FoundAD. This is done by learning a nonlinear projection operator onto the natural image manifold. The simple operator acts as an effective tool for anomaly detection to characterize and identify out-of-distribution regions in an image. Extensive experiments show that our approach supports multi-class detection and achieves competitive performance while using substantially fewer parameters than prior methods. Backed up by evaluations with multiple foundation encoders, including fresh DINOv3, we believe this idea broadens the perspective on foundation features and advances the field of few-shot anomaly detection.</li>
</ul>

<h3>Title: ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs</h3>
<ul>
<li><strong>Authors: </strong>Aadarsh Anantha Ramakrishnan, Shubham Agarwal, Selvanayagam S, Kunwar Singh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01967">https://arxiv.org/abs/2510.01967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01967">https://arxiv.org/pdf/2510.01967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01967]] ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs(https://arxiv.org/abs/2510.01967)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.</li>
</ul>

<h3>Title: Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuanyuan Yao, Yuhan Shi, Lu Chen, Ziquan Fang, Yunjun Gao, Leong Hou U, Yushuai Li, Tianyi Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01970">https://arxiv.org/abs/2510.01970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01970">https://arxiv.org/pdf/2510.01970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01970]] Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection(https://arxiv.org/abs/2510.01970)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Multivariate time series (MTS) anomaly detection identifies abnormal patterns where each timestamp contains multiple variables. Existing MTS anomaly detection methods fall into three categories: reconstruction-based, prediction-based, and classifier-based methods. However, these methods face two key challenges: (1) Unsupervised learning methods, such as reconstruction-based and prediction-based methods, rely on error thresholds, which can lead to inaccuracies; (2) Semi-supervised methods mainly model normal data and often underuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised learning methods, such as classifier-based approaches, often fail to capture local relationships, incur high computational costs, and are constrained by the scarcity of labeled data. To address these limitations, we propose Moon, a supervised modality conversion-based multivariate time series anomaly detection framework. Moon enhances the efficiency and accuracy of anomaly detection while providing detailed anomaly analysis reports. First, Moon introduces a novel multivariate Markov Transition Field (MV-MTF) technique to convert numeric time series data into image representations, capturing relationships across variables and timestamps. Since numeric data retains unique patterns that cannot be fully captured by image conversion alone, Moon employs a Multimodal-CNN to integrate numeric and image data through a feature fusion model with parameter sharing, enhancing training efficiency. Finally, a SHAP-based anomaly explainer identifies key variables contributing to anomalies, improving interpretability. Extensive experiments on six real-world MTS datasets demonstrate that Moon outperforms six state-of-the-art methods by up to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation performance.</li>
</ul>

<h3>Title: $\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Yujie Zhou, Pengyang Ling, Jiazi Bu, Yibin Wang, Yuhang Zang, Jiaqi Wang, Li Niu, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01982">https://arxiv.org/abs/2510.01982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01982">https://arxiv.org/pdf/2510.01982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01982]] $\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models(https://arxiv.org/abs/2510.01982)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.</li>
</ul>

<h3>Title: PepCompass: Navigating peptide embedding spaces using Riemannian Geometry</h3>
<ul>
<li><strong>Authors: </strong>Marcin Możejko (1), Adam Bielecki (1), Jurand Prądzyński (1), Marcin Traskowski (1), Antoni Janowski (1), Karol Jurasz (1), Michał Kucharczyk (1), Hyun-Su Lee (2), Marcelo Der Torossian Torres (2), Cesar de la Fuente-Nunez (2), Paulina Szymczak (3), Michał Kmicikiewicz (3), Ewa Szczurek (1 and 3) ((1) University of Warsaw, (2) University of Pennsylvania, (3) Hemholtz Center Munich)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.01988">https://arxiv.org/abs/2510.01988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.01988">https://arxiv.org/pdf/2510.01988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.01988]] PepCompass: Navigating peptide embedding spaces using Riemannian Geometry(https://arxiv.org/abs/2510.01988)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Antimicrobial peptide discovery is challenged by the astronomical size of peptide space and the relative scarcity of active peptides. Generative models provide continuous latent "maps" of peptide space, but conventionally ignore decoder-induced geometry and rely on flat Euclidean metrics, rendering exploration and optimization distorted and inefficient. Prior manifold-based remedies assume fixed intrinsic dimensionality, which critically fails in practice for peptide data. Here, we introduce PepCompass, a geometry-aware framework for peptide exploration and optimization. At its core, we define a Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family of decoder-induced manifolds that captures local geometry while ensuring computational stability. We propose two local exploration methods: Second-Order Riemannian Brownian Efficient Sampling, which provides a convergent second-order approximation to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space, which reinterprets tangent directions as discrete amino-acid substitutions. Combining these yields Local Enumeration Bayesian Optimization (LE-BO), an efficient algorithm for local activity optimization. Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which interpolates between prototype embeddings along property-enriched geodesics, biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro validation confirms the effectiveness of PepCompass: PoGS yields four novel seeds, and subsequent optimization with LE-BO discovers 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains. These results demonstrate that geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design.</li>
</ul>

<h3>Title: Normality Calibration in Semi-supervised Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Guolei Zeng, Hezhe Qiao, Guoguo Ai, Jinsong Guo, Guansong Pang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02014">https://arxiv.org/abs/2510.02014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02014">https://arxiv.org/pdf/2510.02014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02014]] Normality Calibration in Semi-supervised Graph Anomaly Detection(https://arxiv.org/abs/2510.02014)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Graph anomaly detection (GAD) has attracted growing interest for its crucial ability to uncover irregular patterns in broad applications. Semi-supervised GAD, which assumes a subset of annotated normal nodes available during training, is among the most widely explored application settings. However, the normality learned by existing semi-supervised GAD methods is limited to the labeled normal nodes, often inclining to overfitting the given patterns. These can lead to high detection errors, such as high false positives. To overcome this limitation, we propose GraphNC , a graph normality calibration framework that leverages both labeled and unlabeled data to calibrate the normality from a teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly score and node representation spaces. GraphNC includes two main components, anomaly score distribution alignment (ScoreDA) and perturbation-based normality regularization (NormReg). ScoreDA optimizes the anomaly scores of our model by aligning them with the score distribution yielded by the teacher model. Due to accurate scores in most of the normal nodes and part of the anomaly nodes in the teacher model, the score alignment effectively pulls the anomaly scores of the normal and abnormal classes toward the two ends, resulting in more separable anomaly scores. Nevertheless, there are inaccurate scores from the teacher model. To mitigate the misleading by these scores, NormReg is designed to regularize the graph normality in the representation space, making the representations of normal nodes more compact by minimizing a perturbation-guided consistency loss solely on the labeled nodes.</li>
</ul>

<h3>Title: FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Aida Tayebi, Ali Khodabandeh Yalabadi, Mehdi Yazdani-Jahromi, Ozlem Ozmen Garibay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02017">https://arxiv.org/abs/2510.02017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02017">https://arxiv.org/pdf/2510.02017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02017]] FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data(https://arxiv.org/abs/2510.02017)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>As AI systems become more embedded in everyday life, the development of fair and unbiased models becomes more critical. Considering the social impact of AI systems is not merely a technical challenge but a moral imperative. As evidenced in numerous research studies, learning fair and robust representations has proven to be a powerful approach to effectively debiasing algorithms and improving fairness while maintaining essential information for prediction tasks. Representation learning frameworks, particularly those that utilize self-supervised and contrastive learning, have demonstrated superior robustness and generalizability across various domains. Despite the growing interest in applying these approaches to tabular data, the issue of fairness in these learned representations remains underexplored. In this study, we introduce a contrastive learning framework specifically designed to address bias and learn fair representations in tabular datasets. By strategically selecting positive pair samples and employing supervised and self-supervised contrastive learning, we significantly reduce bias compared to existing state-of-the-art contrastive learning models for tabular data. Our results demonstrate the efficacy of our approach in mitigating bias with minimum trade-off in accuracy and leveraging the learned fair representations in various downstream tasks.</li>
</ul>

<h3>Title: Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers</h3>
<ul>
<li><strong>Authors: </strong>Sahil Bhandary Karnoor, Romit Roy Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02043">https://arxiv.org/abs/2510.02043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02043">https://arxiv.org/pdf/2510.02043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02043]] Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers(https://arxiv.org/abs/2510.02043)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.</li>
</ul>

<h3>Title: Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyi Li, Jingtao Ding, Yong Li, Shihua Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02081">https://arxiv.org/abs/2510.02081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02081">https://arxiv.org/pdf/2510.02081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02081]] Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions(https://arxiv.org/abs/2510.02081)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow Matching (FM) algorithm achieves remarkable results in generative tasks especially in robotic manipulation. Building upon the foundations of diffusion models, the simulation-free paradigm of FM enables simple and efficient training, but inherently introduces a train-inference gap. Specifically, we cannot assess the model's output during the training phase. In contrast, other generative models including Variational Autoencoder (VAE), Normalizing Flow and Generative Adversarial Networks (GANs) directly optimize on the reconstruction loss. Such a gap is particularly evident in scenarios that demand high precision, such as robotic manipulation. Moreover, we show that FM's over-pursuit of straight predefined paths may introduce some serious problems such as stiffness into the system. These motivate us to fine-tune FM via Maximum Likelihood Estimation of reconstructions - an approach made feasible by FM's underlying smooth ODE formulation, in contrast to the stochastic differential equations (SDEs) used in diffusion models. This paper first theoretically analyzes the relation between training loss and inference error in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood Estimation of reconstructions, which includes both straightforward fine-tuning and residual-based fine-tuning approaches. Furthermore, through specifically designed architectures, the residual-based fine-tuning can incorporate the contraction property into the model, which is crucial for the model's robustness and interpretability. Experimental results in image generation and robotic manipulation verify that our method reliably improves the inference performance of FM.</li>
</ul>

<h3>Title: KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Kuiye Ding, Fanda Fan, Zheya Wang, Hongxiao Li, Yifan Wang, Lei Wang, Chunjie Luo, Jianfeng Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02084">https://arxiv.org/abs/2510.02084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02084">https://arxiv.org/pdf/2510.02084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02084]] KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting(https://arxiv.org/abs/2510.02084)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>In the World Wide Web, reliable time series forecasts provide the forward-looking signals that drive resource planning, cache placement, and anomaly response, enabling platforms to operate efficiently as user behavior and content distributions evolve. Compared with other domains, time series forecasting for Web applications requires much faster responsiveness to support real-time decision making. We present KAIROS, a non-autoregressive time series forecasting framework that directly models segment-level multi-peak distributions. Unlike autoregressive approaches, KAIROS avoids error accumulation and achieves just-in-time inference, while improving over existing non-autoregressive models that collapse to over-smoothed predictions. Trained on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization on six widely used benchmarks, delivering forecasting performance comparable to state-of-the-art foundation models with similar scale, at a fraction of their inference cost. Beyond empirical results, KAIROS highlights the importance of non-autoregressive design as a scalable paradigm for foundation models in time series.</li>
</ul>

<h3>Title: VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Arman Behnam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02086">https://arxiv.org/abs/2510.02086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02086">https://arxiv.org/pdf/2510.02086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02086]] VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation(https://arxiv.org/abs/2510.02086)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate detection and segmentation of brain tumors from magnetic resonance imaging (MRI) are essential for diagnosis, treatment planning, and clinical monitoring. While convolutional architectures such as U-Net have long been the backbone of medical image segmentation, their limited capacity to capture long-range dependencies constrains performance on complex tumor structures. Recent advances in diffusion models have demonstrated strong potential for generating high-fidelity medical images and refining segmentation boundaries. In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation framework, a transformer-driven diffusion framework for brain tumor detection and segmentation. By embedding a vision transformer at the core of the diffusion process, the model leverages global contextual reasoning together with iterative denoising to enhance both volumetric accuracy and boundary precision. The transformer backbone enables more effective modeling of spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details. This hybrid design provides a pathway toward improved robustness and scalability in neuro-oncology, moving beyond conventional U-Net baselines. Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance, underscoring the potential of transformer-guided diffusion models to advance the state of the art in tumor segmentation.</li>
</ul>

<h3>Title: FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ding-Ruei Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02114">https://arxiv.org/abs/2510.02114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02114">https://arxiv.org/pdf/2510.02114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02114]] FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation(https://arxiv.org/abs/2510.02114)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Federeated Learning (FL) offers a privacy-preserving solution for Semantic Segmentation (SS) tasks to adapt to new domains, but faces significant challenges from these domain shifts, particularly when client data is unlabeled. However, most existing FL methods unrealistically assume access to labeled data on remote clients or fail to leverage the power of modern Vision Foundation Models (VFMs). Here, we propose a novel and challenging task, FFREEDG, in which a model is pretrained on a server's labeled source dataset and subsequently trained across clients using only their unlabeled data, without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a framework that leverages the knowledge of a VFM by integrating vision and language modalities. Our approach employs a Vision-Language decoder guided by CLIP-based text embeddings to improve semantic disambiguation and uses a weak-to-strong consistency learning strategy for robust local training on pseudo-labels. Our experiments on synthetic-to-real and clear-to-adverse-weather benchmarks demonstrate that our framework effectively tackles this new task, achieving competitive performance against established domain generalization and adaptation methods and setting a strong baseline for future research.</li>
</ul>

<h3>Title: Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study</h3>
<ul>
<li><strong>Authors: </strong>Lena Podina, Christina Humer, Alexandre Duval, Victor Schmidt, Ali Ramlaoui, Shahana Chatterjee, Yoshua Bengio, Alex Hernandez-Garcia, David Rolnick, Félix Therrien</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02142">https://arxiv.org/abs/2510.02142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02142">https://arxiv.org/pdf/2510.02142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02142]] Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study(https://arxiv.org/abs/2510.02142)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Efficient and inexpensive energy storage is essential for accelerating the adoption of renewable energy and ensuring a stable supply, despite fluctuations in sources such as wind and solar. Electrocatalysts play a key role in hydrogen energy storage (HES), allowing the energy to be stored as hydrogen. However, the development of affordable and high-performance catalysts for this process remains a significant challenge. We introduce Catalyst GFlowNet, a generative model that leverages machine learning-based predictors of formation and adsorption energy to design crystal surfaces that act as efficient catalysts. We demonstrate the performance of the model through a proof-of-concept application to the hydrogen evolution reaction, a key reaction in HES, for which we successfully identified platinum as the most efficient known catalyst. In future work, we aim to extend this approach to the oxygen evolution reaction, where current optimal catalysts are expensive metal oxides, and open the search space to discover new materials. This generative modeling framework offers a promising pathway for accelerating the search for novel and efficient catalysts.</li>
</ul>

<h3>Title: Policy Gradient Guidance Enables Test Time Control</h3>
<ul>
<li><strong>Authors: </strong>Jianing Qi, Hao Tang, Zhigang Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02148">https://arxiv.org/abs/2510.02148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02148">https://arxiv.org/pdf/2510.02148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02148]] Policy Gradient Guidance Enables Test Time Control(https://arxiv.org/abs/2510.02148)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Policy Gradient Guidance (PGG), a simple extension of classifier-free guidance from diffusion models to classical policy gradient methods. PGG augments the policy gradient with an unconditional branch and interpolates conditional and unconditional branches, yielding a test-time control knob that modulates behavior without retraining. We provide a theoretical derivation showing that the additional normalization term vanishes under advantage estimation, leading to a clean guided policy gradient update. Empirically, we evaluate PGG on discrete and continuous control benchmarks. We find that conditioning dropout-central to diffusion guidance-offers gains in simple discrete tasks and low sample regimes, but dropout destabilizes continuous control. Training with modestly larger guidance ($\gamma>1$) consistently improves stability, sample efficiency, and controllability. Our results show that guidance, previously confined to diffusion policies, can be adapted to standard on-policy methods, opening new directions for controllable online reinforcement learning.</li>
</ul>

<h3>Title: Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting</h3>
<ul>
<li><strong>Authors: </strong>Shu Zou, Xinyu Tian, Lukas Wesemann, Fabian Waschkowski, Zhaoyuan Yang, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02155">https://arxiv.org/abs/2510.02155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02155">https://arxiv.org/pdf/2510.02155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02155]] Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting(https://arxiv.org/abs/2510.02155)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Prompting has emerged as a practical way to adapt frozen vision-language models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are often overly abstract, overlooking the fine-grained human-object interactions or action semantics that define complex anomalies in surveillance videos. We propose ASK-Hint, a structured prompting framework that leverages action-centric knowledge to elicit more accurate and interpretable reasoning from frozen VLMs. Our approach organizes prompts into semantically coherent groups (e.g. violence, property crimes, public safety) and formulates fine-grained guiding questions that align model predictions with discriminative visual cues. Extensive experiments on UCF-Crime and XD-Violence show that ASK-Hint consistently improves AUC over prior baselines, achieving state-of-the-art performance compared to both fine-tuned and training-free methods. Beyond accuracy, our framework provides interpretable reasoning traces towards anomaly and demonstrates strong generalization across datasets and VLM backbones. These results highlight the critical role of prompt granularity and establish ASK-Hint as a new training-free and generalizable solution for explainable video anomaly detection.</li>
</ul>

<h3>Title: GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Weijia Dou, Xu Zhang, Yi Bin, Jian Liu, Bo Peng, Guoqing Wang, Yang Yang, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02186">https://arxiv.org/abs/2510.02186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02186">https://arxiv.org/pdf/2510.02186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02186]] GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation(https://arxiv.org/abs/2510.02186)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to 3D semantic segmentation expose a persistent trade-off. Directly projecting 2D features into 3D yields noisy and fragmented predictions, whereas enforcing geometric coherence necessitates costly training pipelines and large-scale annotated 3D data. We argue that this limitation stems from the dominant segmentation-and-matching paradigm, which fails to reconcile 2D semantics with 3D geometric structure. The geometric cues are not eliminated during the 2D-to-3D transfer but remain latent within the noisy and view-aggregated features. To exploit this property, we propose GeoPurify that applies a small Student Affinity Network to purify 2D VLM-generated 3D point features using geometric priors distilled from a 3D self-supervised teacher model. During inference, we devise a Geometry-Guided Pooling module to further denoise the point cloud and ensure the semantic and structural consistency. Benefiting from latent geometric information and the learned affinity network, GeoPurify effectively mitigates the trade-off and achieves superior data efficiency. Extensive experiments on major 3D benchmarks demonstrate that GeoPurify achieves or surpasses state-of-the-art performance while utilizing only about 1.5% of the training data. Our codes and checkpoints are available at [this https URL](this https URL).</li>
</ul>

<h3>Title: DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02212">https://arxiv.org/abs/2510.02212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02212">https://arxiv.org/pdf/2510.02212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02212]] DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning(https://arxiv.org/abs/2510.02212)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers/controllers of dLLMs policy. Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks.</li>
</ul>

<h3>Title: Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Zeqi Ye, Minshuo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02216">https://arxiv.org/abs/2510.02216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02216">https://arxiv.org/pdf/2510.02216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02216]] Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification(https://arxiv.org/abs/2510.02216)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Imputation methods play a critical role in enhancing the quality of practical time-series data, which often suffer from pervasive missing values. Recently, diffusion-based generative imputation methods have demonstrated remarkable success compared to autoregressive and conventional statistical approaches. Despite their empirical success, the theoretical understanding of how well diffusion-based models capture complex spatial and temporal dependencies between the missing values and observed ones remains limited. Our work addresses this gap by investigating the statistical efficiency of conditional diffusion transformers for imputation and quantifying the uncertainty in missing values. Specifically, we derive statistical sample complexity bounds based on a novel approximation theory for conditional score functions using transformers, and, through this, construct tight confidence regions for missing values. Our findings also reveal that the efficiency and accuracy of imputation are significantly influenced by the missing patterns. Furthermore, we validate these theoretical insights through simulation and propose a mixed-masking training strategy to enhance the imputation performance.</li>
</ul>

<h3>Title: Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ethan Baron, Boris Oreshkin, Ruijun Ma, Hanyu Zhang, Kari Torkkola, Michael W. Mahoney, Andrew Gordon Wilson, Tatiana Konstantinova</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02224">https://arxiv.org/abs/2510.02224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02224">https://arxiv.org/pdf/2510.02224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02224]] Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models(https://arxiv.org/abs/2510.02224)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Many time series applications require access to multi-step forecast trajectories in the form of sample paths. Recently, time series foundation models have leveraged multi-step lookahead predictions to improve the quality and efficiency of multi-step forecasts. However, these models only predict independent marginal distributions for each time step, rather than a full joint predictive distribution. To generate forecast sample paths with realistic correlation structures, one typically resorts to autoregressive sampling, which can be extremely expensive. In this paper, we present a copula-based approach to efficiently generate accurate, correlated sample paths from existing multi-step time series foundation models in one forward pass. Our copula-based approach generates correlated sample paths orders of magnitude faster than autoregressive sampling, and it yields improved sample path quality by mitigating the snowballing error phenomenon.</li>
</ul>

<h3>Title: TempoControl: Temporal Attention Guidance for Text-to-Video Models</h3>
<ul>
<li><strong>Authors: </strong>Shira Schiber, Ofir Lindenbaum, Idan Schwartz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02226">https://arxiv.org/abs/2510.02226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02226">https://arxiv.org/pdf/2510.02226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02226]] TempoControl: Temporal Attention Guidance for Text-to-Video Models(https://arxiv.org/abs/2510.02226)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.</li>
</ul>

<h3>Title: PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks</h3>
<ul>
<li><strong>Authors: </strong>Ricardo Misael Ayala Molina, Hyame Assem Alameddine, Makan Pourzandi, Chadi Assi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02236">https://arxiv.org/abs/2510.02236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02236">https://arxiv.org/pdf/2510.02236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02236]] PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks(https://arxiv.org/abs/2510.02236)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Network Slices (NSs) are virtual networks operating over a shared physical infrastructure, each designed to meet specific application requirements while maintaining consistent Quality of Service (QoS). In Fifth Generation (5G) networks, User Equipment (UE) can connect to and seamlessly switch between multiple NSs to access diverse services. However, this flexibility, known as Inter-Slice Switching (ISS), introduces a potential vulnerability that can be exploited to launch Distributed Slice Mobility (DSM) attacks, a form of Distributed Denial of Service (DDoS) attack. To secure 5G networks and their NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and incorporates a combination of Long Short-Term Memory Autoencoders and K-Means clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership Project (3GPP) key performance indicators and performance measurement counters as features for its machine learning models to detect DSM attack variants while maintaining robustness in the presence of contaminated training data. When evaluated on data collected from our 5G testbed based on the open-source free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator; PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training datasets with 10% to 40% attack contamination, consistently outperforming its counterpart Inter-Slice Defender and other PUL based solutions combining One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.</li>
</ul>

<h3>Title: DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02253">https://arxiv.org/abs/2510.02253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02253">https://arxiv.org/pdf/2510.02253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02253]] DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing(https://arxiv.org/abs/2510.02253)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.</li>
</ul>

<h3>Title: NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes</h3>
<ul>
<li><strong>Authors: </strong>Shiyi Zhang, Dong Liang, Yihang Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02266">https://arxiv.org/abs/2510.02266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02266">https://arxiv.org/pdf/2510.02266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02266]] NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes(https://arxiv.org/abs/2510.02266)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods.</li>
</ul>

<h3>Title: Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps</h3>
<ul>
<li><strong>Authors: </strong>Kyoungjun Park, Yifan Yang, Changhan Ge, Lili Qiu, Shiqi Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02274">https://arxiv.org/abs/2510.02274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02274">https://arxiv.org/pdf/2510.02274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02274]] Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps(https://arxiv.org/abs/2510.02274)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Modeling radio frequency (RF) signal propagation is essential for understanding the environment, as RF signals offer valuable insights beyond the capabilities of RGB cameras, which are limited by the visible-light spectrum, lens coverage, and occlusions. It is also useful for supporting wireless diagnosis, deployment, and optimization. However, accurately predicting RF signals in complex environments remains a challenge due to interactions with obstacles such as absorption and reflection. We introduce Diffusion^2, a diffusion-based approach that uses 3D point clouds to model the propagation of RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves. To effectively capture RF-related features from 3D data, we present the RF-3D Encoder, which encapsulates the complexities of 3D geometry along with signal-specific details. These features undergo multi-scale embedding to simulate the actual RF signal dissemination process. Our evaluation, based on synthetic and real-world measurements, demonstrates that Diffusion^2 accurately estimates the behavior of RF signals in various frequency bands and environmental conditions, with an error margin of just 1.9 dB and 27x faster than existing methods, marking a significant advancement in the field. Refer to this https URL for more information.</li>
</ul>

<h3>Title: Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02283">https://arxiv.org/abs/2510.02283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02283">https://arxiv.org/pdf/2510.02283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02283]] Self-Forcing++: Towards Minute-Scale High-Quality Video Generation(https://arxiv.org/abs/2510.02283)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at this https URL</li>
</ul>

<h3>Title: Learning to Generate Object Interactions with Physics-Guided Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02284">https://arxiv.org/abs/2510.02284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02284">https://arxiv.org/pdf/2510.02284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02284]] Learning to Generate Object Interactions with Physics-Guided Video Diffusion(https://arxiv.org/abs/2510.02284)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.</li>
</ul>

<h3>Title: MultiModal Action Conditioned Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yichen Li, Antonio Torralba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02287">https://arxiv.org/abs/2510.02287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02287">https://arxiv.org/pdf/2510.02287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02287]] MultiModal Action Conditioned Video Generation(https://arxiv.org/abs/2510.02287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.</li>
</ul>

<h3>Title: Test-Time Anchoring for Discrete Diffusion Posterior Sampling</h3>
<ul>
<li><strong>Authors: </strong>Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02291">https://arxiv.org/abs/2510.02291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02291">https://arxiv.org/pdf/2510.02291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02291]] Test-Time Anchoring for Discrete Diffusion Posterior Sampling(https://arxiv.org/abs/2510.02291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations -- quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding. Our approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks. We further demonstrate the benefits of our approach in training-free stylization and text-guided editing.</li>
</ul>

<h3>Title: F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data</h3>
<ul>
<li><strong>Authors: </strong>Ziyin Zhang, Zihan Liao, Hang Yu, Peng Di, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02294">https://arxiv.org/abs/2510.02294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02294">https://arxiv.org/pdf/2510.02294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02294]] F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data(https://arxiv.org/abs/2510.02294)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works.</li>
</ul>

<h3>Title: Continual Personalization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yu-Chien Liao, Jr-Jen Chen, Chi-Pin Huang, Ci-Siang Lin, Meng-Lin Wu, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02296">https://arxiv.org/abs/2510.02296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02296">https://arxiv.org/pdf/2510.02296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02296]] Continual Personalization for Diffusion Models(https://arxiv.org/abs/2510.02296)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization.</li>
</ul>

<h3>Title: Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Runqian Wang, Yilun Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02300">https://arxiv.org/abs/2510.02300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02300">https://arxiv.org/pdf/2510.02300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02300]] Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models(https://arxiv.org/abs/2510.02300)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.</li>
</ul>

<h3>Title: Knowledge Distillation Detection for Open-weights Models</h3>
<ul>
<li><strong>Authors: </strong>Qin Shi, Amber Yijia Zheng, Qifan Song, Raymond A. Yeh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02302">https://arxiv.org/abs/2510.02302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02302">https://arxiv.org/pdf/2510.02302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02302]] Knowledge Distillation Detection for Open-weights Models(https://arxiv.org/abs/2510.02302)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose the task of knowledge distillation detection, which aims to determine whether a student model has been distilled from a given teacher, under a practical setting where only the student's weights and the teacher's API are available. This problem is motivated by growing concerns about model provenance and unauthorized replication through distillation. To address this task, we introduce a model-agnostic framework that combines data-free input synthesis and statistical score computation for detecting distillation. Our approach is applicable to both classification and generative models. Experiments on diverse architectures for image classification and text-to-image generation show that our method improves detection accuracy over the strongest baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation. The code is available at this https URL.</li>
</ul>

<h3>Title: Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive</h3>
<ul>
<li><strong>Authors: </strong>Tyler Farghly, Peter Potaptchik, Samuel Howard, George Deligiannidis, Jakiw Pidstrigach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02305">https://arxiv.org/abs/2510.02305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02305">https://arxiv.org/pdf/2510.02305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02305]] Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive(https://arxiv.org/abs/2510.02305)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved state-of-the-art performance, demonstrating remarkable generalisation capabilities across diverse domains. However, the mechanisms underpinning these strong capabilities remain only partially understood. A leading conjecture, based on the manifold hypothesis, attributes this success to their ability to adapt to low-dimensional geometric structure within the data. This work provides evidence for this conjecture, focusing on how such phenomena could result from the formulation of the learning problem through score matching. We inspect the role of implicit regularisation by investigating the effect of smoothing minimisers of the empirical score matching objective. Our theoretical and empirical results confirm that smoothing the score function -- or equivalently, smoothing in the log-density domain -- produces smoothing tangential to the data manifold. In addition, we show that the manifold along which the diffusion model generalises can be controlled by choosing an appropriate smoothing.</li>
</ul>

<h3>Title: NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruozhen He, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02307">https://arxiv.org/abs/2510.02307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02307">https://arxiv.org/pdf/2510.02307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02307]] NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation(https://arxiv.org/abs/2510.02307)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.</li>
</ul>

<h3>Title: Inferring Dynamic Physical Properties from Video Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Guanqi Zhan, Xianzheng Ma, Weidi Xie, Andrew Zisserman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02311">https://arxiv.org/abs/2510.02311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02311">https://arxiv.org/pdf/2510.02311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02311]] Inferring Dynamic Physical Properties from Video Foundation Models(https://arxiv.org/abs/2510.02311)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model, generative</a></li>
<li><strong>Abstract: </strong>We study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dynamic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, consisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mechanism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strategies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting.</li>
</ul>

<h3>Title: Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity</h3>
<ul>
<li><strong>Authors: </strong>Eric Tillmann Bill, Enis Simsar, Thomas Hofmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.02315">https://arxiv.org/abs/2510.02315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.02315">https://arxiv.org/pdf/2510.02315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.02315]] Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity(https://arxiv.org/abs/2510.02315)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
