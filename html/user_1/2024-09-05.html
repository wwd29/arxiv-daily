<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-05</h1>
<h3>Title: TrajWeaver: Trajectory Recovery with State Propagation Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jinming Wang, Hai Wang, Hongkai Wen, Geyong Min, Man Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02124">https://arxiv.org/abs/2409.02124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02124">https://arxiv.org/pdf/2409.02124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02124]] TrajWeaver: Trajectory Recovery with State Propagation Diffusion Model(https://arxiv.org/abs/2409.02124)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the proliferation of location-aware devices, large amount of trajectories have been generated when agents such as people, vehicles and goods flow around the urban environment. These raw trajectories, typically collected from various sources such as GPS in cars, personal mobile devices, and public transport, are often sparse and fragmented due to limited sampling rates, infrastructure coverage and data loss. In this context, trajectory recovery aims to reconstruct such sparse raw trajectories into their dense and continuous counterparts, so that fine-grained movement of agents across space and time can be captured faithfully. Existing trajectory recovery approaches typically rely on the prior knowledge of travel mode or motion patterns, and often fail in densely populated urban areas where accurate maps are absent. In this paper, we present a new recovery framework called TrajWeaver based on probabilistic diffusion models, which is able to recover dense and refined trajectories from the sparse raw ones, conditioned on various auxiliary features such as Areas of Interest along the way, user identity and waybill information. The core of TrajWeaver is a novel State Propagation Diffusion Model (SPDM), which introduces a new state propagation mechanism on top of the standard diffusion models, so that knowledge computed in earlier diffusion steps can be reused later, improving the recovery performance while reducing the number of steps needed. Extensive experiments show that the proposed TrajWeaver can recover from raw trajectories of various lengths, sparsity levels and heterogeneous travel modes, and outperform the state-of-the-art baselines significantly in recovery accuracy. Our code is available at: https://anonymous.4open.science/r/TrajWeaver/</li>
</ul>

<h3>Title: A Financial Time Series Denoiser Based on Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Zhuohan Wang, Carmine Ventre</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-fin.CP, q-fin.TR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02138">https://arxiv.org/abs/2409.02138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02138">https://arxiv.org/pdf/2409.02138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02138]] A Financial Time Series Denoiser Based on Diffusion Model(https://arxiv.org/abs/2409.02138)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Financial time series often exhibit low signal-to-noise ratio, posing significant challenges for accurate data interpretation and prediction and ultimately decision making. Generative models have gained attention as powerful tools for simulating and predicting intricate data patterns, with the diffusion model emerging as a particularly effective method. This paper introduces a novel approach utilizing the diffusion model as a denoiser for financial time series in order to improve data predictability and trading performance. By leveraging the forward and reverse processes of the conditional diffusion model to add and remove noise progressively, we reconstruct original data from noisy inputs. Our extensive experiments demonstrate that diffusion model-based denoised time series significantly enhance the performance on downstream future return classification tasks. Moreover, trading signals derived from the denoised data yield more profitable trades with fewer transactions, thereby minimizing transaction costs and increasing overall trading efficiency. Finally, we show that by using classifiers trained on denoised time series, we can recognize the noising state of the market and obtain excess return.</li>
</ul>

<h3>Title: The Role of Transformer Models in Advancing Blockchain Technology: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Tianxu Liu, Yanbin Wang, Jianguo Sun, Ye Tian, Yanyu Huang, Tao Xue, Peiyue Li, Yiwei Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02139">https://arxiv.org/abs/2409.02139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02139">https://arxiv.org/pdf/2409.02139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02139]] The Role of Transformer Models in Advancing Blockchain Technology: A Systematic Review(https://arxiv.org/abs/2409.02139)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>As blockchain technology rapidly evolves, the demand for enhanced efficiency, security, and scalability grows.Transformer models, as powerful deep learning architectures,have shown unprecedented potential in addressing various blockchain challenges. However, a systematic review of Transformer applications in blockchain is lacking. This paper aims to fill this research gap by surveying over 200 relevant papers, comprehensively reviewing practical cases and research progress of Transformers in blockchain applications. Our survey covers key areas including anomaly detection, smart contract security analysis, cryptocurrency prediction and trend analysis, and code summary generation. To clearly articulate the advancements of Transformers across various blockchain domains, we adopt a domain-oriented classification system, organizing and introducing representative methods based on major challenges in current blockchain research. For each research domain,we first introduce its background and objectives, then review previous representative methods and analyze their limitations,and finally introduce the advancements brought by Transformer models. Furthermore, we explore the challenges of utilizing Transformer, such as data privacy, model complexity, and real-time processing requirements. Finally, this article proposes future research directions, emphasizing the importance of exploring the Transformer architecture in depth to adapt it to specific blockchain applications, and discusses its potential role in promoting the development of blockchain technology. This review aims to provide new perspectives and a research foundation for the integrated development of blockchain technology and machine learning, supporting further innovation and application expansion of blockchain technology.</li>
</ul>

<h3>Title: Self-Supervised Learning for Identifying Defects in Sewer Footage</h3>
<ul>
<li><strong>Authors: </strong>Daniel Otero, Rafael Mateus</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02140">https://arxiv.org/abs/2409.02140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02140">https://arxiv.org/pdf/2409.02140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02140]] Self-Supervised Learning for Identifying Defects in Sewer Footage(https://arxiv.org/abs/2409.02140)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Sewerage infrastructure is among the most expensive modern investments requiring time-intensive manual inspections by qualified personnel. Our study addresses the need for automated solutions without relying on large amounts of labeled data. We propose a novel application of Self-Supervised Learning (SSL) for sewer inspection that offers a scalable and cost-effective solution for defect detection. We achieve competitive results with a model that is at least 5 times smaller than other approaches found in the literature and obtain competitive performance with 10\% of the available data when training with a larger architecture. Our findings highlight the potential of SSL to revolutionize sewer maintenance in resource-limited settings.</li>
</ul>

<h3>Title: How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?</h3>
<ul>
<li><strong>Authors: </strong>Saeid Asgari Taghanaki, Joseph Lambourne, Alana Mongkhounsavath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02253">https://arxiv.org/abs/2409.02253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02253">https://arxiv.org/pdf/2409.02253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02253]] How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?(https://arxiv.org/abs/2409.02253)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Large foundation models have revolutionized the field, yet challenges remain in optimizing multi-modal models for specialized visual tasks. We propose a novel, generalizable methodology to identify preferred image distributions for black-box Vision-Language Models (VLMs) by measuring output consistency across varied input prompts. Applying this to different rendering types of 3D objects, we demonstrate its efficacy across various domains requiring precise interpretation of complex structures, with a focus on Computer-Aided Design (CAD) as an exemplar field. We further refine VLM outputs using in-context learning with human feedback, significantly enhancing explanation quality. To address the lack of benchmarks in specialized domains, we introduce CAD-VQA, a new dataset for evaluating VLMs on CAD-related visual question answering tasks. Our evaluation of state-of-the-art VLMs on CAD-VQA establishes baseline performance levels, providing a framework for advancing VLM capabilities in complex visual reasoning tasks across various fields requiring expert-level visual interpretation. We release the dataset and evaluation codes at \url{this https URL}.</li>
</ul>

<h3>Title: TimeDiT: General-purpose Diffusion Transformers for Time Series Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Defu Cao, Wen Ye, Yizhou Zhang, Yan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02322">https://arxiv.org/abs/2409.02322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02322">https://arxiv.org/pdf/2409.02322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02322]] TimeDiT: General-purpose Diffusion Transformers for Time Series Foundation Model(https://arxiv.org/abs/2409.02322)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative, anomaly</a></li>
<li><strong>Abstract: </strong>With recent advances in building foundation models for texts and video data, there is a surge of interest in foundation models for time series. A family of models have been developed, utilizing a temporal auto-regressive generative Transformer architecture, whose effectiveness has been proven in Large Language Models. While the empirical results are promising, almost all existing time series foundation models have only been tested on well-curated ``benchmark'' datasets very similar to texts. However, real-world time series exhibit unique challenges, such as variable channel sizes across domains, missing values, and varying signal sampling intervals due to the multi-resolution nature of real-world data. Additionally, the uni-directional nature of temporally auto-regressive decoding limits the incorporation of domain knowledge, such as physical laws expressed as partial differential equations (PDEs). To address these challenges, we introduce the Time Diffusion Transformer (TimeDiT), a general foundation model for time series that employs a denoising diffusion paradigm instead of temporal auto-regressive generation. TimeDiT leverages the Transformer architecture to capture temporal dependencies and employs diffusion processes to generate high-quality candidate samples without imposing stringent assumptions on the target distribution via novel masking schemes and a channel alignment strategy. Furthermore, we propose a finetuning-free model editing strategy that allows the seamless integration of external knowledge during the sampling process without updating any model parameters. Extensive experiments conducted on a varity of tasks such as forecasting, imputation, and anomaly detection, demonstrate the effectiveness of TimeDiT.</li>
</ul>

<h3>Title: Robust Federated Finetuning of Foundation Models via Alternating Minimization of LoRA</h3>
<ul>
<li><strong>Authors: </strong>Shuangyi Chen, Yue Ju, Hardik Dalal, Zhongwen Zhu, Ashish Khisti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02346">https://arxiv.org/abs/2409.02346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02346">https://arxiv.org/pdf/2409.02346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02346]] Robust Federated Finetuning of Foundation Models via Alternating Minimization of LoRA(https://arxiv.org/abs/2409.02346)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Parameter-Efficient Fine-Tuning (PEFT) has risen as an innovative training strategy that updates only a select few model parameters, significantly lowering both computational and memory demands. PEFT also helps to decrease data transfer in federated learning settings, where communication depends on the size of updates. In this work, we explore the constraints of previous studies that integrate a well-known PEFT method named LoRA with federated fine-tuning, then introduce RoLoRA, a robust federated fine-tuning framework that utilizes an alternating minimization approach for LoRA, providing greater robustness against decreasing fine-tuning parameters and increasing data heterogeneity. Our results indicate that RoLoRA not only presents the communication benefits but also substantially enhances the robustness and effectiveness in multiple federated fine-tuning scenarios.</li>
</ul>

<h3>Title: Unfolding Videos Dynamics via Taylor Expansion</h3>
<ul>
<li><strong>Authors: </strong>Siyi Chen, Minkyu Choi, Zesen Zhao, Kuan Han, Qing Qu, Zhongming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02371">https://arxiv.org/abs/2409.02371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02371">https://arxiv.org/pdf/2409.02371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02371]] Unfolding Videos Dynamics via Taylor Expansion(https://arxiv.org/abs/2409.02371)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Taking inspiration from physical motion, we present a new self-supervised dynamics learning strategy for videos: Video Time-Differentiation for Instance Discrimination (ViDiDi). ViDiDi is a simple and data-efficient strategy, readily applicable to existing self-supervised video representation learning frameworks based on instance discrimination. At its core, ViDiDi observes different aspects of a video through various orders of temporal derivatives of its frame sequence. These derivatives, along with the original frames, support the Taylor series expansion of the underlying continuous dynamics at discrete times, where higher-order derivatives emphasize higher-order motion features. ViDiDi learns a single neural network that encodes a video and its temporal derivatives into consistent embeddings following a balanced alternating learning algorithm. By learning consistent representations for original frames and derivatives, the encoder is steered to emphasize motion features over static backgrounds and uncover the hidden dynamics in original frames. Hence, video representations are better separated by dynamic features. We integrate ViDiDi into existing instance discrimination frameworks (VICReg, BYOL, and SimCLR) for pretraining on UCF101 or Kinetics and test on standard benchmarks including video retrieval, action recognition, and action detection. The performances are enhanced by a significant margin without the need for large models or extensive datasets.</li>
</ul>

<h3>Title: Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Siyi Chen, Huijie Zhang, Minzhe Guo, Yifu Lu, Peng Wang, Qing Qu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02374">https://arxiv.org/abs/2409.02374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02374">https://arxiv.org/pdf/2409.02374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02374]] Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing(https://arxiv.org/abs/2409.02374)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recently, diffusion models have emerged as a powerful class of generative models. Despite their success, there is still limited understanding of their semantic spaces. This makes it challenging to achieve precise and disentangled image generation without additional training, especially in an unsupervised way. In this work, we improve the understanding of their semantic spaces from intriguing observations: among a certain range of noise levels, (1) the learned posterior mean predictor (PMP) in the diffusion model is locally linear, and (2) the singular vectors of its Jacobian lie in low-dimensional semantic subspaces. We provide a solid theoretical basis to justify the linearity and low-rankness in the PMP. These insights allow us to propose an unsupervised, single-step, training-free LOw-rank COntrollable image editing (LOCO Edit) method for precise local editing in diffusion models. LOCO Edit identified editing directions with nice properties: homogeneity, transferability, composability, and linearity. These properties of LOCO Edit benefit greatly from the low-dimensional semantic subspace. Our method can further be extended to unsupervised or text-supervised editing in various text-to-image diffusion models (T-LOCO Edit). Finally, extensive empirical experiments demonstrate the effectiveness and efficiency of LOCO Edit. The codes will be released at this https URL.</li>
</ul>

<h3>Title: GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Huasong Han, Kaixuan Zhou, Xiaoxiao Long, Yusen Wang, Chunxia Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02382">https://arxiv.org/abs/2409.02382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02382">https://arxiv.org/pdf/2409.02382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02382]] GGS: Generalizable Gaussian Splatting for Lane Switching in Autonomous Driving(https://arxiv.org/abs/2409.02382)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose GGS, a Generalizable Gaussian Splatting method for Autonomous Driving which can achieve realistic rendering under large viewpoint changes. Previous generalizable 3D gaussian splatting methods are limited to rendering novel views that are very close to the original pair of images, which cannot handle large differences in viewpoint. Especially in autonomous driving scenarios, images are typically collected from a single lane. The limited training perspective makes rendering images of a different lane very challenging. To further improve the rendering capability of GGS under large viewpoint changes, we introduces a novel virtual lane generation module into GSS method to enables high-quality lane switching even without a multi-lane dataset. Besides, we design a diffusion loss to supervise the generation of virtual lane image to further address the problem of lack of data in the virtual lanes. Finally, we also propose a depth refinement module to optimize depth estimation in the GSS model. Extensive validation of our method, compared to existing approaches, demonstrates state-of-the-art performance.</li>
</ul>

<h3>Title: Determination of language families using deep learning</h3>
<ul>
<li><strong>Authors: </strong>Peter B. Lerner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02393">https://arxiv.org/abs/2409.02393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02393">https://arxiv.org/pdf/2409.02393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02393]] Determination of language families using deep learning(https://arxiv.org/abs/2409.02393)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We use a c-GAN (convolutional generative adversarial) neural network to analyze transliterated text fragments of extant, dead comprehensible, and one dead non-deciphered (Cypro-Minoan) language to establish linguistic affinities. The paper is agnostic with respect to translation and/or deciphering. However, there is hope that the proposed approach can be useful for decipherment with more sophisticated neural network techniques.</li>
</ul>

<h3>Title: Learning Privacy-Preserving Student Networks via Discriminative-Generative Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shiming Ge, Bochao Liu, Pengju Wang, Yong Li, Dan Zeng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02404">https://arxiv.org/abs/2409.02404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02404">https://arxiv.org/pdf/2409.02404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02404]] Learning Privacy-Preserving Student Networks via Discriminative-Generative Distillation(https://arxiv.org/abs/2409.02404)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While deep models have proved successful in learning rich knowledge from massive well-annotated data, they may pose a privacy leakage risk in practical deployment. It is necessary to find an effective trade-off between high utility and strong privacy. In this work, we propose a discriminative-generative distillation approach to learn privacy-preserving deep models. Our key idea is taking models as bridge to distill knowledge from private data and then transfer it to learn a student network via two streams. First, discriminative stream trains a baseline classifier on private data and an ensemble of teachers on multiple disjoint private subsets, respectively. Then, generative stream takes the classifier as a fixed discriminator and trains a generator in a data-free manner. After that, the generator is used to generate massive synthetic data which are further applied to train a variational autoencoder (VAE). Among these synthetic data, a few of them are fed into the teacher ensemble to query labels via differentially private aggregation, while most of them are embedded to the trained VAE for reconstructing synthetic data. Finally, a semi-supervised student learning is performed to simultaneously handle two tasks: knowledge transfer from the teachers with distillation on few privately labeled synthetic data, and knowledge enhancement with tangent-normal adversarial regularization on many triples of reconstructed synthetic data. In this way, our approach can control query cost over private data and mitigate accuracy degradation in a unified manner, leading to a privacy-preserving student model. Extensive experiments and analysis clearly show the effectiveness of the proposed approach.</li>
</ul>

<h3>Title: Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering</h3>
<ul>
<li><strong>Authors: </strong>Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, Qing Qu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02426">https://arxiv.org/abs/2409.02426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02426">https://arxiv.org/pdf/2409.02426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02426]] Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering(https://arxiv.org/abs/2409.02426)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.</li>
</ul>

<h3>Title: Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Aishwarya Agarwal, Srikrishna Karanam, Balaji Vasan Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02429">https://arxiv.org/abs/2409.02429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02429">https://arxiv.org/pdf/2409.02429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02429]] Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis(https://arxiv.org/abs/2409.02429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We consider the problem of independently, in a disentangled fashion, controlling the outputs of text-to-image diffusion models with color and style attributes of a user-supplied reference image. We present the first training-free, test-time-only method to disentangle and condition text-to-image models on color and style attributes from reference image. To realize this, we propose two key innovations. Our first contribution is to transform the latent codes at inference time using feature transformations that make the covariance matrix of current generation follow that of the reference image, helping meaningfully transfer color. Next, we observe that there exists a natural disentanglement between color and style in the LAB image space, which we exploit to transform the self-attention feature maps of the image being generated with respect to those of the reference computed from its L channel. Both these operations happen purely at test time and can be done independently or merged. This results in a flexible method where color and style information can come from the same reference image or two different sources, and a new generation can seamlessly fuse them in either scenario.</li>
</ul>

<h3>Title: Reliable Deep Diffusion Tensor Estimation: Rethinking the Power of Data-Driven Optimization Routine</h3>
<ul>
<li><strong>Authors: </strong>Jialong Li, Zhicheng Zhang, Yunwei Chen, Qiqi Lu, Ye Wu, Xiaoming Liu, QianJin Feng, Yanqiu Feng, Xinyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02492">https://arxiv.org/abs/2409.02492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02492">https://arxiv.org/pdf/2409.02492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02492]] Reliable Deep Diffusion Tensor Estimation: Rethinking the Power of Data-Driven Optimization Routine(https://arxiv.org/abs/2409.02492)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion tensor imaging (DTI) holds significant importance in clinical diagnosis and neuroscience research. However, conventional model-based fitting methods often suffer from sensitivity to noise, leading to decreased accuracy in estimating DTI parameters. While traditional data-driven deep learning methods have shown potential in terms of accuracy and efficiency, their limited generalization to out-of-training-distribution data impedes their broader application due to the diverse scan protocols used across centers, scanners, and studies. This work aims to tackle these challenges and promote the use of DTI by introducing a data-driven optimization-based method termed DoDTI. DoDTI combines the weighted linear least squares fitting algorithm and regularization by denoising technique. The former fits DW images from diverse acquisition settings into diffusion tensor field, while the latter applies a deep learning-based denoiser to regularize the diffusion tensor field instead of the DW images, which is free from the limitation of fixed-channel assignment of the network. The optimization object is solved using the alternating direction method of multipliers and then unrolled to construct a deep neural network, leveraging a data-driven strategy to learn network parameters. Extensive validation experiments are conducted utilizing both internally simulated datasets and externally obtained in-vivo datasets. The results, encompassing both qualitative and quantitative analyses, showcase that the proposed method attains state-of-the-art performance in DTI parameter estimation. Notably, it demonstrates superior generalization, accuracy, and efficiency, rendering it highly reliable for widespread application in the field.</li>
</ul>

<h3>Title: Continual Diffuser (CoD): Mastering Continual Offline Reinforcement Learning with Experience Rehearsal</h3>
<ul>
<li><strong>Authors: </strong>Jifeng Hu, Li Shen, Sili Huang, Zhejian Yang, Hechang Chen, Lichao Sun, Yi Chang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02512">https://arxiv.org/abs/2409.02512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02512">https://arxiv.org/pdf/2409.02512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02512]] Continual Diffuser (CoD): Mastering Continual Offline Reinforcement Learning with Experience Rehearsal(https://arxiv.org/abs/2409.02512)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Artificial neural networks, especially recent diffusion-based models, have shown remarkable superiority in gaming, control, and QA systems, where the training tasks' datasets are usually static. However, in real-world applications, such as robotic control of reinforcement learning (RL), the tasks are changing, and new tasks arise in a sequential order. This situation poses the new challenge of plasticity-stability trade-off for training an agent who can adapt to task changes and retain acquired knowledge. In view of this, we propose a rehearsal-based continual diffusion model, called Continual Diffuser (CoD), to endow the diffuser with the capabilities of quick adaptation (plasticity) and lasting retention (stability). Specifically, we first construct an offline benchmark that contains 90 tasks from multiple domains. Then, we train the CoD on each task with sequential modeling and conditional generation for making decisions. Next, we preserve a small portion of previous datasets as the rehearsal buffer and replay it to retain the acquired knowledge. Extensive experiments on a series of tasks show CoD can achieve a promising plasticity-stability trade-off and outperform existing diffusion-based methods and other representative baselines on most tasks.</li>
</ul>

<h3>Title: Sample what you cant compress</h3>
<ul>
<li><strong>Authors: </strong>Vighnesh Birodkar, Gabriel Barcik, James Lyon, Sergey Ioffe, David Minnen, Joshua V. Dillon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02529">https://arxiv.org/abs/2409.02529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02529">https://arxiv.org/pdf/2409.02529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02529]] Sample what you cant compress(https://arxiv.org/abs/2409.02529)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>For learned image representations, basic autoencoders often produce blurry results. Reconstruction quality can be improved by incorporating additional penalties such as adversarial (GAN) and perceptual losses. Arguably, these approaches lack a principled interpretation. Concurrently, in generative settings diffusion has demonstrated a remarkable ability to create crisp, high quality results and has solid theoretical underpinnings (from variational inference to direct study as the Fisher Divergence). Our work combines autoencoder representation learning with diffusion and is, to our knowledge, the first to demonstrate the efficacy of jointly learning a continuous encoder and decoder under a diffusion-based loss. We demonstrate that this approach yields better reconstruction quality as compared to GAN-based autoencoders while being easier to tune. We also show that the resulting representation is easier to model with a latent diffusion model as compared to the representation obtained from a state-of-the-art GAN-based loss. Since our decoder is stochastic, it can generate details not encoded in the otherwise deterministic latent representation; we therefore name our approach "Sample what you can't compress", or SWYCC for short.</li>
</ul>

<h3>Title: Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02530">https://arxiv.org/abs/2409.02530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02530">https://arxiv.org/pdf/2409.02530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02530]] Understanding eGFR Trajectories and Kidney Function Decline via Large Multimodal Models(https://arxiv.org/abs/2409.02530)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.</li>
</ul>

<h3>Title: StyleTokenizer: Defining Image Style by a Single Instance for Controlling Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Wen Li, Muyuan Fang, Cheng Zou, Biao Gong, Ruobing Zheng, Meng Wang, Jingdong Chen, Ming Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02543">https://arxiv.org/abs/2409.02543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02543">https://arxiv.org/pdf/2409.02543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02543]] StyleTokenizer: Defining Image Style by a Single Instance for Controlling Diffusion Models(https://arxiv.org/abs/2409.02543)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the burst of innovative methods for controlling the diffusion process, effectively controlling image styles in text-to-image generation remains a challenging task. Many adapter-based methods impose image representation conditions on the denoising process to accomplish image control. However these conditions are not aligned with the word embedding space, leading to interference between image and text control conditions and the potential loss of semantic information from the text prompt. Addressing this issue involves two key challenges. Firstly, how to inject the style representation without compromising the effectiveness of text representation in control. Secondly, how to obtain the accurate style representation from a single reference image. To tackle these challenges, we introduce StyleTokenizer, a zero-shot style control image generation method that aligns style representation with text representation using a style tokenizer. This alignment effectively minimizes the impact on the effectiveness of text prompts. Furthermore, we collect a well-labeled style dataset named Style30k to train a style feature extractor capable of accurately representing style while excluding other content information. Experimental results demonstrate that our method fully grasps the style characteristics of the reference image, generating appealing images that are consistent with both the target image style and text prompt. The code and dataset are available at this https URL.</li>
</ul>

<h3>Title: UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching</h3>
<ul>
<li><strong>Authors: </strong>Soomin Kim, Hyesong Choi, Jihye Ahn, Dongbo Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02545">https://arxiv.org/abs/2409.02545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02545">https://arxiv.org/pdf/2409.02545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02545]] UniTT-Stereo: Unified Training of Transformer for Enhanced Stereo Matching(https://arxiv.org/abs/2409.02545)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Unlike other vision tasks where Transformer-based approaches are becoming increasingly common, stereo depth estimation is still dominated by convolution-based approaches. This is mainly due to the limited availability of real-world ground truth for stereo matching, which is a limiting factor in improving the performance of Transformer-based stereo approaches. In this paper, we propose UniTT-Stereo, a method to maximize the potential of Transformer-based stereo architectures by unifying self-supervised learning used for pre-training with stereo matching framework based on supervised learning. To be specific, we explore the effectiveness of reconstructing features of masked portions in an input image and at the same time predicting corresponding points in another image from the perspective of locality inductive bias, which is crucial in training models with limited training data. Moreover, to address these challenging tasks of reconstruction-and-prediction, we present a new strategy to vary a masking ratio when training the stereo model with stereo-tailored losses. State-of-the-art performance of UniTT-Stereo is validated on various benchmarks such as ETH3D, KITTI 2012, and KITTI 2015 datasets. Lastly, to investigate the advantages of the proposed approach, we provide a frequency analysis of feature maps and the analysis of locality inductive bias based on attention maps.</li>
</ul>

<h3>Title: Advancing Cyber Incident Timeline Analysis Through Rule Based AI and Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fatma Yasmine Loumachi, Mohamed Chahine Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02572">https://arxiv.org/abs/2409.02572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02572">https://arxiv.org/pdf/2409.02572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02572]] Advancing Cyber Incident Timeline Analysis Through Rule Based AI and Large Language Models(https://arxiv.org/abs/2409.02572)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital Forensics (DF), focusing primarily on examining and analysing temporal digital artefacts such as timestamps, derived from event logs, file metadata, and other related data to correlate events resulting from cyber incidents and reconstruct their chronological timeline. Traditional tools often struggle to efficiently process the vast volume and variety of data acquired during DF investigations and Incident Response (IR) processes. This paper presents a novel framework, GenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to advance and automate the TA process. Our approach consists of two main stages (1) We use R-BAI to identify and select anomalous digital artefacts based on predefined rules. (2) The selected artefacts are then converted into embeddings for processing by an LLM with the help of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently leverages its capabilities to perform automated TA on the artefacts and predict potential incident scenarios. To validate our framework, we evaluate GenDFIR performance, efficiency, and reliability using various metrics across synthetic cyber incident simulation scenarios. This paper presents a proof of concept, where the findings demonstrate the significant potential of integrating R-BAI and LLMs for TA. This novel approach highlights the power of Generative AI (GenAI), specifically LLMs, and opens new avenues for advanced threat detection and incident reconstruction, representing a significant step forward in the field.</li>
</ul>

<h3>Title: Solving Video Inverse Problems Using Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Taesung Kwon, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02574">https://arxiv.org/abs/2409.02574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02574">https://arxiv.org/pdf/2409.02574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02574]] Solving Video Inverse Problems Using Image Diffusion Models(https://arxiv.org/abs/2409.02574)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, diffusion model-based inverse problem solvers (DIS) have emerged as state-of-the-art approaches for addressing inverse problems, including image super-resolution, deblurring, inpainting, etc. However, their application to video inverse problems arising from spatio-temporal degradation remains largely unexplored due to the challenges in training video diffusion models. To address this issue, here we introduce an innovative video inverse solver that leverages only image diffusion models. Specifically, by drawing inspiration from the success of the recent decomposed diffusion sampler (DDS), our method treats the time dimension of a video as the batch dimension of image diffusion models and solves spatio-temporal optimization problems within denoised spatio-temporal batches derived from each image diffusion model. Moreover, we introduce a batch-consistent diffusion sampling strategy that encourages consistency across batches by synchronizing the stochastic noise components in image diffusion models. Our approach synergistically combines batch-consistent sampling with simultaneous optimization of denoised spatio-temporal batches at each reverse diffusion step, resulting in a novel and efficient diffusion sampling strategy for video inverse problems. Experimental results demonstrate that our method effectively addresses various spatio-temporal degradations in video inverse problems, achieving state-of-the-art reconstructions. Project page: this https URL</li>
</ul>

<h3>Title: An Analysis of Linear Complexity Attention Substitutes with BEST-RQ</h3>
<ul>
<li><strong>Authors: </strong>Ryan Whetten, Titouan Parcollet, Adel Moumen, Marco Dinarelli, Yannick Estève</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02596">https://arxiv.org/abs/2409.02596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02596">https://arxiv.org/pdf/2409.02596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02596]] An Analysis of Linear Complexity Attention Substitutes with BEST-RQ(https://arxiv.org/abs/2409.02596)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-Supervised Learning (SSL) has proven to be effective in various domains, including speech processing. However, SSL is computationally and memory expensive. This is in part due the quadratic complexity of multi-head self-attention (MHSA). Alternatives for MHSA have been proposed and used in the speech domain, but have yet to be investigated properly in an SSL setting. In this work, we study the effects of replacing MHSA with recent state-of-the-art alternatives that have linear complexity, namely, HyperMixing, Fastformer, SummaryMixing, and Mamba. We evaluate these methods by looking at the speed, the amount of VRAM consumed, and the performance on the SSL MP3S benchmark. Results show that these linear alternatives maintain competitive performance compared to MHSA while, on average, decreasing VRAM consumption by around 20% to 60% and increasing speed from 7% to 65% for input sequences ranging from 20 to 80 seconds.</li>
</ul>

<h3>Title: Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency</h3>
<ul>
<li><strong>Authors: </strong>Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, Yanbo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02634">https://arxiv.org/abs/2409.02634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02634">https://arxiv.org/pdf/2409.02634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02634]] Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency(https://arxiv.org/abs/2409.02634)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.</li>
</ul>

<h3>Title: MADiff: Motion-Aware Mamba Diffusion Models for Hand Trajectory Prediction on Egocentric Videos</h3>
<ul>
<li><strong>Authors: </strong>Junyi Ma, Xieyuanli Chen, Wentao Bao, Jingyi Xu, Hesheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02638">https://arxiv.org/abs/2409.02638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02638">https://arxiv.org/pdf/2409.02638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02638]] MADiff: Motion-Aware Mamba Diffusion Models for Hand Trajectory Prediction on Egocentric Videos(https://arxiv.org/abs/2409.02638)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Understanding human intentions and actions through egocentric videos is important on the path to embodied artificial intelligence. As a branch of egocentric vision techniques, hand trajectory prediction plays a vital role in comprehending human motion patterns, benefiting downstream tasks in extended reality and robot manipulation. However, capturing high-level human intentions consistent with reasonable temporal causality is challenging when only egocentric videos are available. This difficulty is exacerbated under camera egomotion interference and the absence of affordance labels to explicitly guide the optimization of hand waypoint distribution. In this work, we propose a novel hand trajectory prediction method dubbed MADiff, which forecasts future hand waypoints with diffusion models. The devised denoising operation in the latent space is achieved by our proposed motion-aware Mamba, where the camera wearer's egomotion is integrated to achieve motion-driven selective scan (MDSS). To discern the relationship between hands and scenarios without explicit affordance supervision, we leverage a foundation model that fuses visual and language features to capture high-level semantics from video clips. Comprehensive experiments conducted on five public datasets with the existing and our proposed new evaluation metrics demonstrate that MADiff predicts comparably reasonable hand trajectories compared to the state-of-the-art baselines, and achieves real-time performance. We will release our code and pretrained models of MADiff at the project page: this https URL.</li>
</ul>

<h3>Title: Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects</h3>
<ul>
<li><strong>Authors: </strong>Kyungmin Jo, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02653">https://arxiv.org/abs/2409.02653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02653">https://arxiv.org/pdf/2409.02653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02653]] Skip-and-Play: Depth-Driven Pose-Preserved Image Generation for Any Objects(https://arxiv.org/abs/2409.02653)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The emergence of diffusion models has enabled the generation of diverse high-quality images solely from text, prompting subsequent efforts to enhance the controllability of these models. Despite the improvement in controllability, pose control remains limited to specific objects (e.g., humans) or poses (e.g., frontal view) due to the fact that pose is generally controlled via camera parameters (e.g., rotation angle) or keypoints (e.g., eyes, nose). Specifically, camera parameters-conditional pose control models generate unrealistic images depending on the object, owing to the small size of 3D datasets for training. Also, keypoint-based approaches encounter challenges in acquiring reliable keypoints for various objects (e.g., church) or poses (e.g., back view). To address these limitations, we propose depth-based pose control, as depth maps are easily obtainable from a single depth estimation model regardless of objects and poses, unlike camera parameters and keypoints. However, depth-based pose control confronts issues of shape dependency, as depth maps influence not only the pose but also the shape of the generated images. To tackle this issue, we propose Skip-and-Play (SnP), designed via analysis of the impact of three components of depth-conditional ControlNet on the pose and the shape of the generated images. To be specific, based on the analysis, we selectively skip parts of the components to mitigate shape dependency on the depth map while preserving the pose. Through various experiments, we demonstrate the superiority of SnP over baselines and showcase the ability of SnP to generate images of diverse objects and poses. Remarkably, SnP exhibits the ability to generate images even when the objects in the condition (e.g., a horse) and the prompt (e.g., a hedgehog) differ from each other.</li>
</ul>

<h3>Title: PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation</h3>
<ul>
<li><strong>Authors: </strong>Jun Ling, Yiwen Wang, Han Xue, Rong Xie, Li Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02657">https://arxiv.org/abs/2409.02657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02657">https://arxiv.org/pdf/2409.02657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02657]] PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation(https://arxiv.org/abs/2409.02657)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose \textbf{PoseTalk}, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4\% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions. Project: this https URL.</li>
</ul>

<h3>Title: Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Kaiqing Lin, Yuzhen Lin, Weixiang Li, Taiping Yao, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02664">https://arxiv.org/abs/2409.02664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02664">https://arxiv.org/pdf/2409.02664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02664]] Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection(https://arxiv.org/abs/2409.02664)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The proliferation of deepfake faces poses huge potential negative impacts on our daily lives. Despite substantial advancements in deepfake detection over these years, the generalizability of existing methods against forgeries from unseen datasets or created by emerging generative models remains constrained. In this paper, inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach that repurposes a well-trained VLM for general deepfake detection. Motivated by the model reprogramming paradigm that manipulates the model prediction via data perturbations, our method can reprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its input without tuning the inner parameters. Furthermore, we insert a pseudo-word guided by facial identity into the text prompt. Extensive experiments on several popular benchmarks demonstrate that (1) the cross-dataset and cross-manipulation performances of deepfake detection can be significantly and consistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to WildDeepfake) using a pre-trained CLIP model with our proposed reprogramming method; (2) our superior performances are at less cost of trainable parameters, making it a promising approach for real-world applications.</li>
</ul>

<h3>Title: Independence Constrained Disentangled Representation Learning from Epistemological Perspective</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Wang, Lina Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02672">https://arxiv.org/abs/2409.02672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02672">https://arxiv.org/pdf/2409.02672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02672]] Independence Constrained Disentangled Representation Learning from Epistemological Perspective(https://arxiv.org/abs/2409.02672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Disentangled Representation Learning aims to improve the explainability of deep learning methods by training a data encoder that identifies semantically meaningful latent variables in the data generation process. Nevertheless, there is no consensus regarding a universally accepted definition for the objective of disentangled representation learning. In particular, there is a considerable amount of discourse regarding whether should the latent variables be mutually independent or not. In this paper, we first investigate these arguments on the interrelationships between latent variables by establishing a conceptual bridge between Epistemology and Disentangled Representation Learning. Then, inspired by these interdisciplinary concepts, we introduce a two-level latent space framework to provide a general solution to the prior arguments on this issue. Finally, we propose a novel method for disentangled representation learning by employing an integration of mutual information constraint and independence constraint within the Generative Adversarial Network (GAN) framework. Experimental results demonstrate that our proposed method consistently outperforms baseline approaches in both quantitative and qualitative evaluations. The method exhibits strong performance across multiple commonly used metrics and demonstrates a great capability in disentangling various semantic factors, leading to an improved quality of controllable generation, which consequently benefits the explainability of the algorithm.</li>
</ul>

<h3>Title: Rethinking HTG Evaluation: Bridging Generation and Recognition</h3>
<ul>
<li><strong>Authors: </strong>Konstantina Nikolaidou, George Retsinas, Giorgos Sfikas, Marcus Liwicki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02683">https://arxiv.org/abs/2409.02683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02683">https://arxiv.org/pdf/2409.02683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02683]] Rethinking HTG Evaluation: Bridging Generation and Recognition(https://arxiv.org/abs/2409.02683)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The evaluation of generative models for natural image tasks has been extensively studied. Similar protocols and metrics are used in cases with unique particularities, such as Handwriting Generation, even if they might not be completely appropriate. In this work, we introduce three measures tailored for HTG evaluation, $ \text{HTG}_{\text{HTR}} $, $ \text{HTG}_{\text{style}} $, and $ \text{HTG}_{\text{OOV}} $, and argue that they are more expedient to evaluate the quality of generated handwritten images. The metrics rely on the recognition error/accuracy of Handwriting Text Recognition and Writer Identification models and emphasize writing style, textual content, and diversity as the main aspects that adhere to the content of handwritten images. We conduct comprehensive experiments on the IAM handwriting database, showcasing that widely used metrics such as FID fail to properly quantify the diversity and the practical utility of generated handwriting samples. Our findings show that our metrics are richer in information and underscore the necessity of standardized evaluation protocols in HTG. The proposed metrics provide a more robust and informative protocol for assessing HTG quality, contributing to improved performance in HTR. Code for the evaluation protocol is available at: this https URL.</li>
</ul>

<h3>Title: MOOSS: Mask-Enhanced Temporal Contrastive Learning for Smooth State Evolution in Visual Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Sun, M. Ugur Akcal, Wei Zhang, Girish Chowdhary</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02714">https://arxiv.org/abs/2409.02714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02714">https://arxiv.org/pdf/2409.02714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02714]] MOOSS: Mask-Enhanced Temporal Contrastive Learning for Smooth State Evolution in Visual Reinforcement Learning(https://arxiv.org/abs/2409.02714)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In visual Reinforcement Learning (RL), learning from pixel-based observations poses significant challenges on sample efficiency, primarily due to the complexity of extracting informative state representations from high-dimensional data. Previous methods such as contrastive-based approaches have made strides in improving sample efficiency but fall short in modeling the nuanced evolution of states. To address this, we introduce MOOSS, a novel framework that leverages a temporal contrastive objective with the help of graph-based spatial-temporal masking to explicitly model state evolution in visual RL. Specifically, we propose a self-supervised dual-component strategy that integrates (1) a graph construction of pixel-based observations for spatial-temporal masking, coupled with (2) a multi-level contrastive learning mechanism that enriches state representations by emphasizing temporal continuity and change of states. MOOSS advances the understanding of state dynamics by disrupting and learning from spatial-temporal correlations, which facilitates policy learning. Our comprehensive evaluation on multiple continuous and discrete control benchmarks shows that MOOSS outperforms previous state-of-the-art visual RL methods in terms of sample efficiency, demonstrating the effectiveness of our method. Our code is released at this https URL.</li>
</ul>

<h3>Title: Pooling And Attention: What Are Effective Designs For LLm-Based Embedding Models?</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Tang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02727">https://arxiv.org/abs/2409.02727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02727">https://arxiv.org/pdf/2409.02727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02727]] Pooling And Attention: What Are Effective Designs For LLm-Based Embedding Models?(https://arxiv.org/abs/2409.02727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.</li>
</ul>

<h3>Title: Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhibin Liu, Haoye Dong, Aviral Chharia, Hefeng Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02851">https://arxiv.org/abs/2409.02851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02851">https://arxiv.org/pdf/2409.02851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02851]] Human-VDM: Learning Single-Image 3D Human Gaussian Splatting from Video Diffusion Models(https://arxiv.org/abs/2409.02851)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating lifelike 3D humans from a single RGB image remains a challenging task in computer vision, as it requires accurate modeling of geometry, high-quality texture, and plausible unseen parts. Existing methods typically use multi-view diffusion models for 3D generation, but they often face inconsistent view issues, which hinder high-quality 3D human generation. To address this, we propose Human-VDM, a novel method for generating 3D human from a single RGB image using Video Diffusion Models. Human-VDM provides temporally consistent views for 3D human generation using Gaussian Splatting. It consists of three modules: a view-consistent human video diffusion module, a video augmentation module, and a Gaussian Splatting module. First, a single image is fed into a human video diffusion module to generate a coherent human video. Next, the video augmentation module applies super-resolution and video interpolation to enhance the textures and geometric smoothness of the generated video. Finally, the 3D Human Gaussian Splatting module learns lifelike humans under the guidance of these high-resolution and view-consistent images. Experiments demonstrate that Human-VDM achieves high-quality 3D human from a single image, outperforming state-of-the-art methods in both generation quality and quantity. Project page: this https URL</li>
</ul>

<h3>Title: The Impact of Balancing Real and Synthetic Data on Accuracy and Fairness in Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Andrea Atzori, Pietro Cosseddu, Gianni Fenu, Mirko Marras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02867">https://arxiv.org/abs/2409.02867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02867">https://arxiv.org/pdf/2409.02867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02867]] The Impact of Balancing Real and Synthetic Data on Accuracy and Fairness in Face Recognition(https://arxiv.org/abs/2409.02867)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Over the recent years, the advancements in deep face recognition have fueled an increasing demand for large and diverse datasets. Nevertheless, the authentic data acquired to create those datasets is typically sourced from the web, which, in many cases, can lead to significant privacy issues due to the lack of explicit user consent. Furthermore, obtaining a demographically balanced, large dataset is even more difficult because of the natural imbalance in the distribution of images from different demographic groups. In this paper, we investigate the impact of demographically balanced authentic and synthetic data, both individually and in combination, on the accuracy and fairness of face recognition models. Initially, several generative methods were used to balance the demographic representations of the corresponding synthetic datasets. Then a state-of-the-art face encoder was trained and evaluated using (combinations of) synthetic and authentic images. Our findings emphasized two main points: (i) the increased effectiveness of training data generated by diffusion-based models in enhancing accuracy, whether used alone or combined with subsets of authentic data, and (ii) the minimal impact of incorporating balanced data from pre-trained generative methods on fairness (in nearly all tested scenarios using combined datasets, fairness scores remained either unchanged or worsened, even when compared to unbalanced authentic datasets). Source code and data are available at \url{this https URL} for reproducibility.</li>
</ul>

<h3>Title: Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02908">https://arxiv.org/abs/2409.02908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02908">https://arxiv.org/pdf/2409.02908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02908]] Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling(https://arxiv.org/abs/2409.02908)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in simplifying the masked diffusion framework further leads to alignment with continuous-space diffusion models and more principled training and sampling recipes. In this paper, however, we reveal that both training and sampling of MDMs are theoretically free from the time variable, arguably the key signature of diffusion models, and are instead equivalent to masked models. The connection on the sampling aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we show that the FHS is theoretically equivalent to MDMs' original generation process while significantly alleviating the time-consuming categorical sampling and achieving a 20$\times$ speedup. In addition, our investigation challenges previous claims that MDMs can surpass ARMs in generative perplexity. We identify, for the first time, an underlying numerical issue, even with the 32-bit floating-point precision, which results in inaccurate categorical sampling. We show that the numerical issue lowers the effective temperature both theoretically and empirically, leading to unfair assessments of MDMs' generation results in the previous literature.</li>
</ul>

<h3>Title: HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Liu, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, Wenhan Luo, Qingfeng Liu, Yike Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.02919">https://arxiv.org/abs/2409.02919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.02919">https://arxiv.org/pdf/2409.02919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.02919]] HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts(https://arxiv.org/abs/2409.02919)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The potential for higher-resolution image generation using pretrained diffusion models is immense, yet these models often struggle with issues of object repetition and structural artifacts especially when scaling to 4K resolution and higher. We figure out that the problem is caused by that, a single prompt for the generation of multiple scales provides insufficient efficacy. In response, we propose HiPrompt, a new tuning-free solution that tackles the above problems by introducing hierarchical prompts. The hierarchical prompts offer both global and local guidance. Specifically, the global guidance comes from the user input that describes the overall content, while the local guidance utilizes patch-wise descriptions from MLLMs to elaborately guide the regional structure and texture generation. Furthermore, during the inverse denoising process, the generated noise is decomposed into low- and high-frequency spatial components. These components are conditioned on multiple prompt levels, including detailed patch-wise descriptions and broader image-level prompts, facilitating prompt-guided denoising under hierarchical semantic guidance. It further allows the generation to focus more on local spatial regions and ensures the generated images maintain coherent local and global semantics, structures, and textures with high definition. Extensive experiments demonstrate that HiPrompt outperforms state-of-the-art works in higher-resolution image generation, significantly reducing object repetition and enhancing structural quality.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
