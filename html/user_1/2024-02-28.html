<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-28</h1>
<h3>Title: Generative Models are Self-Watermarked: Declaring Model Authentication  through Re-Generation</h3>
<ul>
<li><strong>Authors: </strong>Aditya Desu, Xuanli He, Qiongkai Xu, Wei Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16889">https://arxiv.org/abs/2402.16889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16889">https://arxiv.org/pdf/2402.16889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16889]] Generative Models are Self-Watermarked: Declaring Model Authentication  through Re-Generation(https://arxiv.org/abs/2402.16889)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As machine- and AI-generated content proliferates, protecting the intellectual property of generative models has become imperative, yet verifying data ownership poses formidable challenges, particularly in cases of unauthorized reuse of generated data. The challenge of verifying data ownership is further amplified by using Machine Learning as a Service (MLaaS), which often functions as a black-box system. Our work is dedicated to detecting data reuse from even an individual sample. Traditionally, watermarking has been leveraged to detect AI-generated content. However, unlike watermarking techniques that embed additional information as triggers into models or generated content, potentially compromising output quality, our approach identifies latent fingerprints inherently present within the outputs through re-generation. We propose an explainable verification procedure that attributes data ownership through re-generation, and further amplifies these fingerprints in the generative models through iterative data re-generation. This methodology is theoretically grounded and demonstrates viability and robustness using recent advanced text and image generative models. Our methodology is significant as it goes beyond protecting the intellectual property of APIs and addresses important issues such as the spread of misinformation and academic misconduct. It provides a useful tool to ensure the integrity of sources and authorship, expanding its application in different scenarios where authenticity and ownership verification are essential.</li>
</ul>

<h3>Title: DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM  Jailbreakers</h3>
<ul>
<li><strong>Authors: </strong>Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16914">https://arxiv.org/abs/2402.16914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16914">https://arxiv.org/pdf/2402.16914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16914]] DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM  Jailbreakers(https://arxiv.org/abs/2402.16914)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but harmless reassembling demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts' synonyms that maintain the original intent while jailbreaking LLMs. An extensive empirical study across multiple open-source and closed-source LLMs demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA prompt-only attackers. Notably, the success rate of 78.0\% on GPT-4 with merely 15 queries surpassed previous art by 33.1\%.</li>
</ul>

<h3>Title: More Than Routing: Joint GPS and Route Modeling for Refine Trajectory  Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Ma, Zheyan Tu, Xinhai Chen, Yan Zhang, Deguo Xia, Guyue Zhou, Yilun Chen, Yu Zheng, Jiangtao Gong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16915">https://arxiv.org/abs/2402.16915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16915">https://arxiv.org/pdf/2402.16915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16915]] More Than Routing: Joint GPS and Route Modeling for Refine Trajectory  Representation Learning(https://arxiv.org/abs/2402.16915)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Trajectory representation learning plays a pivotal role in supporting various downstream tasks. Traditional methods in order to filter the noise in GPS trajectories tend to focus on routing-based methods used to simplify the trajectories. However, this approach ignores the motion details contained in the GPS data, limiting the representation capability of trajectory representation learning. To fill this gap, we propose a novel representation learning framework that Joint GPS and Route Modelling based on self-supervised technology, namely JGRM. We consider GPS trajectory and route as the two modes of a single movement observation and fuse information through inter-modal information interaction. Specifically, we develop two encoders, each tailored to capture representations of route and GPS trajectories respectively. The representations from the two modalities are fed into a shared transformer for inter-modal information interaction. Eventually, we design three self-supervised tasks to train the model. We validate the effectiveness of the proposed method on two real datasets based on extensive experiments. The experimental results demonstrate that JGRM outperforms existing methods in both road segment representation and trajectory representation tasks. Our source code is available at Anonymous Github.</li>
</ul>

<h3>Title: GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Petrov, Pradyumn Goyal, Vikas Thamizharasan, Vladimir G. Kim, Matheus Gadelha, Melinos Averkiou, Siddhartha Chaudhuri, Evangelos Kalogerakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.16994">https://arxiv.org/abs/2402.16994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.16994">https://arxiv.org/pdf/2402.16994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.16994]] GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis(https://arxiv.org/abs/2402.16994)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the state-of-the-art, also involving challenging scenarios of reconstructing and synthesizing structurally complex, high-genus shape surfaces from Thingi10K and ShapeNet.</li>
</ul>

<h3>Title: DiffuCOMET: Contextual Commonsense Knowledge Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Silin Gao, Mete Ismayilzada, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17011">https://arxiv.org/abs/2402.17011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17011">https://arxiv.org/pdf/2402.17011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17011]] DiffuCOMET: Contextual Commonsense Knowledge Diffusion(https://arxiv.org/abs/2402.17011)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inferring contextually-relevant and diverse commonsense to understand narratives remains challenging for knowledge models. In this work, we develop a series of knowledge models, DiffuCOMET, that leverage diffusion to learn to reconstruct the implicit semantic connections between narrative contexts and relevant commonsense knowledge. Across multiple diffusion steps, our method progressively refines a representation of commonsense facts that is anchored to a narrative, producing contextually-relevant and diverse commonsense inferences for an input context. To evaluate DiffuCOMET, we introduce new metrics for commonsense inference that more closely measure knowledge diversity and contextual relevance. Our results on two different benchmarks, ComFact and WebNLG+, show that knowledge generated by DiffuCOMET achieves a better trade-off between commonsense diversity, contextual relevance and alignment to known gold references, compared to baseline knowledge models.</li>
</ul>

<h3>Title: Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review</h3>
<ul>
<li><strong>Authors: </strong>Richard Kimanzi, Peter Kimanga, Dedan Cherori, Patrick K. Gikunda</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17020">https://arxiv.org/abs/2402.17020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17020">https://arxiv.org/pdf/2402.17020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17020]] Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review(https://arxiv.org/abs/2402.17020)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The increase in network attacks has necessitated the development of robust and efficient intrusion detection systems (IDS) capable of identifying malicious activities in real-time. In the last five years, deep learning algorithms have emerged as powerful tools in this domain, offering enhanced detection capabilities compared to traditional methods. This review paper studies recent advancements in the application of deep learning techniques, including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Deep Belief Networks (DBN), Deep Neural Networks (DNN), Long Short-Term Memory (LSTM), autoencoders (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing Networks (SNN) and hybrid models, within network intrusion detection systems. we delve into the unique architectures, training models, and classification methodologies tailored for network traffic analysis and anomaly detection. Furthermore, we analyze the strengths and limitations of each deep learning approach in terms of detection accuracy, computational efficiency, scalability, and adaptability to evolving threats. Additionally, this paper highlights prominent datasets and benchmarking frameworks commonly utilized for evaluating the performance of deep learning-based IDS. This review will provide researchers and industry practitioners with valuable insights into the state-of-the-art deep learning algorithms for enhancing the security framework of network environments through intrusion detection.</li>
</ul>

<h3>Title: Taming the Tail in Class-Conditional GANs: Knowledge Sharing via  Unconditional Training at Lower Resolutions</h3>
<ul>
<li><strong>Authors: </strong>Saeed Khorram, Mingqi Jiang, Mohamad Shahbazi, Mohamad H. Danesh, Li Fuxin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17065">https://arxiv.org/abs/2402.17065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17065">https://arxiv.org/pdf/2402.17065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17065]] Taming the Tail in Class-Conditional GANs: Knowledge Sharing via  Unconditional Training at Lower Resolutions(https://arxiv.org/abs/2402.17065)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on several long-tail benchmarks and GAN architectures demonstrate a significant improvement over existing methods in both the diversity and fidelity of the generated images. The code is available at https://github.com/khorrams/utlo.</li>
</ul>

<h3>Title: Structural Teacher-Student Normality Learning for Multi-Class Anomaly  Detection and Localization</h3>
<ul>
<li><strong>Authors: </strong>Hanqiu Deng, Xingyu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17091">https://arxiv.org/abs/2402.17091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17091">https://arxiv.org/pdf/2402.17091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17091]] Structural Teacher-Student Normality Learning for Multi-Class Anomaly  Detection and Localization(https://arxiv.org/abs/2402.17091)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Visual anomaly detection is a challenging open-set task aimed at identifying unknown anomalous patterns while modeling normal data. The knowledge distillation paradigm has shown remarkable performance in one-class anomaly detection by leveraging teacher-student network feature comparisons. However, extending this paradigm to multi-class anomaly detection introduces novel scalability challenges. In this study, we address the significant performance degradation observed in previous teacher-student models when applied to multi-class anomaly detection, which we identify as resulting from cross-class interference. To tackle this issue, we introduce a novel approach known as Structural Teacher-Student Normality Learning (SNL): (1) We propose spatial-channel distillation and intra-&inter-affinity distillation techniques to measure structural distance between the teacher and student networks. (2) We introduce a central residual aggregation module (CRAM) to encapsulate the normal representation space of the student network. We evaluate our proposed approach on two anomaly detection datasets, MVTecAD and VisA. Our method surpasses the state-of-the-art distillation-based algorithms by a significant margin of 3.9% and 1.5% on MVTecAD and 1.2% and 2.5% on VisA in the multi-class anomaly detection and localization tasks, respectively. Furthermore, our algorithm outperforms the current state-of-the-art unified models on both MVTecAD and VisA.</li>
</ul>

<h3>Title: T-HITL Effectively Addresses Problematic Associations in Image  Generation and Maintains Overall Visual Quality</h3>
<ul>
<li><strong>Authors: </strong>Susan Epstein, Li Chen, Alessandro Vecchiato, Ankit Jain</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17101">https://arxiv.org/abs/2402.17101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17101">https://arxiv.org/pdf/2402.17101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17101]] T-HITL Effectively Addresses Problematic Associations in Image  Generation and Maintains Overall Visual Quality(https://arxiv.org/abs/2402.17101)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI image models may inadvertently generate problematic representations of people. Past research has noted that millions of users engage daily across the world with these models and that the models, including through problematic representations of people, have the potential to compound and accelerate real-world discrimination and other harms (Bianchi et al, 2023). In this paper, we focus on addressing the generation of problematic associations between demographic groups and semantic concepts that may reflect and reinforce negative narratives embedded in social data. Building on sociological literature (Blumer, 1958) and mapping representations to model behaviors, we have developed a taxonomy to study problematic associations in image generation models. We explore the effectiveness of fine tuning at the model level as a method to address these associations, identifying a potential reduction in visual quality as a limitation of traditional fine tuning. We also propose a new methodology with twice-human-in-the-loop (T-HITL) that promises improvements in both reducing problematic associations and also maintaining visual quality. We demonstrate the effectiveness of T-HITL by providing evidence of three problematic associations addressed by T-HITL at the model level. Our contributions to scholarship are two-fold. By defining problematic associations in the context of machine learning models and generative AI, we introduce a conceptual and technical taxonomy for addressing some of these associations. Finally, we provide a method, T-HITL, that addresses these associations and simultaneously maintains visual quality of image model generations. This mitigation need not be a tradeoff, but rather an enhancement.</li>
</ul>

<h3>Title: Transparent Image Layer Diffusion using Latent Transparency</h3>
<ul>
<li><strong>Authors: </strong>Lvmin Zhang, Maneesh Agrawala</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17113">https://arxiv.org/abs/2402.17113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17113">https://arxiv.org/pdf/2402.17113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17113]] Transparent Image Layer Diffusion using Latent Transparency(https://arxiv.org/abs/2402.17113)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present LayerDiffusion, an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a "latent transparency" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.</li>
</ul>

<h3>Title: SAM-DiffSR: Structure-Modulated Diffusion Model for Image  Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17133">https://arxiv.org/abs/2402.17133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17133">https://arxiv.org/pdf/2402.17133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17133]] SAM-DiffSR: Structure-Modulated Diffusion Model for Image  Super-Resolution(https://arxiv.org/abs/2402.17133)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based super-resolution (SR) models have recently garnered significant attention due to their potent restoration capabilities. But conventional diffusion models perform noise sampling from a single distribution, constraining their ability to handle real-world scenes and complex textures across semantic regions. With the success of segment anything model (SAM), generating sufficiently fine-grained region masks can enhance the detail recovery of diffusion-based SR model. However, directly integrating SAM into SR models will result in much higher computational cost. In this paper, we propose the SAM-DiffSR model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, showcasing superior performance in suppressing artifacts, and surpassing existing diffusion-based methods by 0.74 dB at the maximum in terms of PSNR on DIV2K dataset. The code and dataset are available at https://github.com/lose4578/SAM-DiffSR.</li>
</ul>

<h3>Title: Video as the New Language for Real-World Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, Dale Schuurmans</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17139">https://arxiv.org/abs/2402.17139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17139">https://arxiv.org/pdf/2402.17139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17139]] Video as the New Language for Real-World Decision Making(https://arxiv.org/abs/2402.17139)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, in-context</a></li>
<li><strong>Abstract: </strong>Both text and video data are abundant on the internet and support large-scale self-supervised learning through next token or frame prediction. However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment. Yet video data captures important information about the physical world that is difficult to express in language. To address this gap, we discuss an under-appreciated opportunity to extend video generation to solve tasks in the real world. We observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks. Moreover, we demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as in-context learning, planning and reinforcement learning. We identify major impact opportunities in domains such as robotics, self-driving, and science, supported by recent work that demonstrates how such advanced capabilities in video generation are plausibly within reach. Lastly, we identify key challenges in video generation that mitigate progress. Addressing these challenges will enable video generation models to demonstrate unique value alongside language models in a wider array of AI applications.</li>
</ul>

<h3>Title: Actions Speak Louder than Words: Trillion-Parameter Sequential  Transducers for Generative Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, Yu Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17152">https://arxiv.org/abs/2402.17152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17152">https://arxiv.org/pdf/2402.17152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17152]] Actions Speak Louder than Words: Trillion-Parameter Sequential  Transducers for Generative Recommendations(https://arxiv.org/abs/2402.17152)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large-scale recommendation systems are characterized by their reliance on high cardinality, heterogeneous features and the need to handle tens of billions of user actions on a daily basis. Despite being trained on huge volume of data with thousands of features, most Deep Learning Recommendation Models (DLRMs) in industry fail to scale with compute. Inspired by success achieved by Transformers in language and vision domains, we revisit fundamental design choices in recommendation systems. We reformulate recommendation problems as sequential transduction tasks within a generative modeling framework (``Generative Recommenders''), and propose a new architecture, HSTU, designed for high cardinality, non-stationary streaming recommendation data. HSTU outperforms baselines over synthetic and public datasets by up to 65.8\% in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on 8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion parameters, improve metrics in online A/B tests by 12.4\% and have been deployed on multiple surfaces of a large internet platform with billions of users. More importantly, the model quality of Generative Recommenders empirically scales as a power-law of training compute across three orders of magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for future model developments, and further paves the way for the first foundational models in recommendations.</li>
</ul>

<h3>Title: TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence  Generation</h3>
<ul>
<li><strong>Authors: </strong>Lin Zongying, Li Hao, Lv Liuzhenghao, Lin Bin, Zhang Junwu, Chen Calvin Yu-Chian, Yuan Li, Tian Yonghong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17156">https://arxiv.org/abs/2402.17156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17156">https://arxiv.org/pdf/2402.17156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17156]] TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence  Generation(https://arxiv.org/abs/2402.17156)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Designing protein sequences with specific biological functions and structural stability is crucial in biology and chemistry. Generative models already demonstrated their capabilities for reliable protein design. However, previous models are limited to the unconditional generation of protein sequences and lack the controllable generation ability that is vital to biological tasks. In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space. Specifically, taxonomic control information is inserted into each layer of the transformer block to achieve fine-grained control. The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins. Extensive experiments demonstrate that TaxDiff can consistently achieve better performance on multiple protein sequence generation benchmarks in both taxonomic-guided controllable generation and unconditional generation. Remarkably, the sequences generated by TaxDiff even surpass those produced by direct-structure-generation models in terms of confidence based on predicted structures and require only a quarter of the time of models based on the diffusion model. The code for generating proteins and training new versions of TaxDiff is available at:https://github.com/Linzy19/TaxDiff.</li>
</ul>

<h3>Title: Generative Learning for Forecasting the Dynamics of Complex Systems</h3>
<ul>
<li><strong>Authors: </strong>Han Gao, Sebastian Kaltenbach, Petros Koumoutsakos</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph, physics.flu-dyn, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17157">https://arxiv.org/abs/2402.17157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17157">https://arxiv.org/pdf/2402.17157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17157]] Generative Learning for Forecasting the Dynamics of Complex Systems(https://arxiv.org/abs/2402.17157)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce generative models for accelerating simulations of complex systems through learning and evolving their effective dynamics. In the proposed Generative Learning of Effective Dynamics (G-LED), instances of high dimensional data are down sampled to a lower dimensional manifold that is evolved through an auto-regressive attention mechanism. In turn, Bayesian diffusion models, that map this low-dimensional manifold onto its corresponding high-dimensional space, capture the statistics of the system dynamics. We demonstrate the capabilities and drawbacks of G-LED in simulations of several benchmark systems, including the Kuramoto-Sivashinsky (KS) equation, two-dimensional high Reynolds number flow over a backward-facing step, and simulations of three-dimensional turbulent channel flow. The results demonstrate that generative learning offers new frontiers for the accurate forecasting of the statistical properties of complex systems at a reduced computational cost.</li>
</ul>

<h3>Title: NocPlace: Nocturnal Visual Place Recognition Using Generative and  Inherited Knowledge Transfer</h3>
<ul>
<li><strong>Authors: </strong>Bingxi Liu, Yiqun Wang, Huaqi Tao, Tingjun Huang, Fulin Tang, Yihong Wu, Jinqiang Cui, Hong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17159">https://arxiv.org/abs/2402.17159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17159">https://arxiv.org/pdf/2402.17159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17159]] NocPlace: Nocturnal Visual Place Recognition Using Generative and  Inherited Knowledge Transfer(https://arxiv.org/abs/2402.17159)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual Place Recognition (VPR) is crucial in computer vision, aiming to retrieve database images similar to a query image from an extensive collection of known images. However, like many vision-related tasks, learning-based VPR often experiences a decline in performance during nighttime due to the scarcity of nighttime images. Specifically, VPR needs to address the cross-domain problem of night-to-day rather than just the issue of a single nighttime domain. In response to these issues, we present NocPlace, which leverages a generated large-scale, multi-view, nighttime VPR dataset to embed resilience against dazzling lights and extreme darkness in the learned global descriptor. Firstly, we establish a day-night urban scene dataset called NightCities, capturing diverse nighttime scenarios and lighting variations across 60 cities globally. Following this, an unpaired image-to-image translation network is trained on this dataset. Using this trained translation network, we process an existing VPR dataset, thereby obtaining its nighttime version. The NocPlace is then fine-tuned using night-style images, the original labels, and descriptors inherited from the Daytime VPR model. Comprehensive experiments on various nighttime VPR test sets reveal that NocPlace considerably surpasses previous state-of-the-art methods.</li>
</ul>

<h3>Title: Deep Umbra: A Generative Approach for Sunlight Access Computation in  Urban Spaces</h3>
<ul>
<li><strong>Authors: </strong>Kazi Shahrukh Omar, Gustavo Moreira, Daniel Hodczak, Maryam Hosseini, Nicola Colaninno, Marcos Lage, Fabio Miranda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17169">https://arxiv.org/abs/2402.17169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17169">https://arxiv.org/pdf/2402.17169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17169]] Deep Umbra: A Generative Approach for Sunlight Access Computation in  Urban Spaces(https://arxiv.org/abs/2402.17169)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sunlight and shadow play critical roles in how urban spaces are utilized, thrive, and grow. While access to sunlight is essential to the success of urban environments, shadows can provide shaded places to stay during the hot seasons, mitigate heat island effect, and increase pedestrian comfort levels. Properly quantifying sunlight access and shadows in large urban environments is key in tackling some of the important challenges facing cities today. In this paper, we propose Deep Umbra, a novel computational framework that enables the quantification of sunlight access and shadows at a global scale. Our framework is based on a conditional generative adversarial network that considers the physical form of cities to compute high-resolution spatial information of accumulated sunlight access for the different seasons of the year. We use data from seven different cities to train our model, and show, through an extensive set of experiments, its low overall RMSE (below 0.1) as well as its extensibility to cities that were not part of the training set. Additionally, we contribute a set of case studies and a comprehensive dataset with sunlight access information for more than 100 cities across six continents of the world. Deep Umbra is available at https://urbantk.org/shadows.</li>
</ul>

<h3>Title: DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Shen, Yici Yan, Zhizhen Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17176">https://arxiv.org/abs/2402.17176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17176">https://arxiv.org/pdf/2402.17176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17176]] DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection(https://arxiv.org/abs/2402.17176)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Model-X knockoff, among various feature selection methods, received much attention recently due to its guarantee on false discovery rate (FDR) control. Subsequent to its introduction in parametric design, knockoff is advanced to handle arbitrary data distributions using deep learning-based generative modeling. However, we observed that current implementations of the deep Model-X knockoff framework exhibit limitations. Notably, the "swap property" that knockoffs necessitate frequently encounter challenges on sample level, leading to a diminished selection power. To overcome, we develop "Deep Dependency Regularized Knockoff (DeepDRK)", a distribution-free deep learning method that strikes a balance between FDR and power. In DeepDRK, a generative model grounded in a transformer architecture is introduced to better achieve the "swap property". Novel efficient regularization techniques are also proposed to reach higher power. Our model outperforms other benchmarks in synthetic, semi-synthetic, and real-world data, especially when sample size is small and data distribution is complex.</li>
</ul>

<h3>Title: Sora: A Review on Background, Technology, Limitations, and Opportunities  of Large Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, Lichao Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17177">https://arxiv.org/abs/2402.17177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17177">https://arxiv.org/pdf/2402.17177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17177]] Sora: A Review on Background, Technology, Limitations, and Opportunities  of Large Vision Models(https://arxiv.org/abs/2402.17177)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation.</li>
</ul>

<h3>Title: Dual-Space Optimization: Improved Molecule Sequence Design by Latent  Prompt Transformer</h3>
<ul>
<li><strong>Authors: </strong>Deqian Kong, Yuhao Huang, Jianwen Xie, Edouardo Honig, Ming Xu, Shuanghong Xue, Pei Lin, Sanping Zhou, Sheng Zhong, Nanning Zheng, Ying Nian Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17179">https://arxiv.org/abs/2402.17179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17179">https://arxiv.org/pdf/2402.17179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17179]] Dual-Space Optimization: Improved Molecule Sequence Design by Latent  Prompt Transformer(https://arxiv.org/abs/2402.17179)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem. In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem. DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values. Our generative model takes the form of a Latent Prompt Transformer (LPT) where the latent vector serves as the prompt of a causal transformer. Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance benchmarks across single-objective, multi-objective and constrained molecule design tasks.</li>
</ul>

<h3>Title: Advancing Generative Model Evaluation: A Novel Algorithm for Realistic  Image Synthesis and Comparison in OCR System</h3>
<ul>
<li><strong>Authors: </strong>Majid Memari, Khaled R. Ahmed, Shahram Rahimi, Noorbakhsh Amiri Golilarz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17204">https://arxiv.org/abs/2402.17204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17204">https://arxiv.org/pdf/2402.17204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17204]] Advancing Generative Model Evaluation: A Novel Algorithm for Realistic  Image Synthesis and Comparison in OCR System(https://arxiv.org/abs/2402.17204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This research addresses a critical challenge in the field of generative models, particularly in the generation and evaluation of synthetic images. Given the inherent complexity of generative models and the absence of a standardized procedure for their comparison, our study introduces a pioneering algorithm to objectively assess the realism of synthetic images. This approach significantly enhances the evaluation methodology by refining the Fr\'echet Inception Distance (FID) score, allowing for a more precise and subjective assessment of image quality. Our algorithm is particularly tailored to address the challenges in generating and evaluating realistic images of Arabic handwritten digits, a task that has traditionally been near-impossible due to the subjective nature of realism in image generation. By providing a systematic and objective framework, our method not only enables the comparison of different generative models but also paves the way for improvements in their design and output. This breakthrough in evaluation and comparison is crucial for advancing the field of OCR, especially for scripts that present unique complexities, and sets a new standard in the generation and assessment of high-quality synthetic images.</li>
</ul>

<h3>Title: Measuring Vision-Language STEM Skills of Neural Models</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17205">https://arxiv.org/abs/2402.17205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17205">https://arxiv.org/pdf/2402.17205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17205]] Measuring Vision-Language STEM Skills of Neural Models(https://arxiv.org/abs/2402.17205)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well below (averaging 54.7%) the performance of elementary students, not to mention near expert-level performance. To understand and increase the performance on our dataset, we teach the models on a training split of our dataset. Even though we observe improved performance, the model performance remains relatively low compared to average elementary students. To solve STEM problems, we will need novel algorithmic innovations from the community.</li>
</ul>

<h3>Title: VCD: Knowledge Base Guided Visual Commonsense Discovery in Images</h3>
<ul>
<li><strong>Authors: </strong>Xiangqing Shen, Yurun Song, Siwei Wu, Rui Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17213">https://arxiv.org/abs/2402.17213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17213">https://arxiv.org/pdf/2402.17213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17213]] VCD: Knowledge Base Guided Visual Commonsense Discovery in Images(https://arxiv.org/abs/2402.17213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual commonsense contains knowledge about object properties, relationships, and behaviors in visual data. Discovering visual commonsense can provide a more comprehensive and richer understanding of images, and enhance the reasoning and decision-making capabilities of computer vision systems. However, the visual commonsense defined in existing visual commonsense discovery studies is coarse-grained and incomplete. In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of visual commonsense. Based on this, we introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image. We accordingly construct a dataset (VCDD) from Visual Genome and ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs. We furthermore propose a generative model (VCDM) that integrates a vision-language model with instruction tuning to tackle VCD. Automatic and human evaluations demonstrate VCDM's proficiency in VCD, particularly outperforming GPT-4V in implicit commonsense discovery. The value of VCD is further demonstrated by its application to two downstream tasks, including visual commonsense evaluation and visual question answering. The data and code will be made available on GitHub.</li>
</ul>

<h3>Title: CharacterGen: Efficient 3D Character Generation from Single Images with  Multi-View Pose Canonicalization</h3>
<ul>
<li><strong>Authors: </strong>Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17214">https://arxiv.org/abs/2402.17214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17214">https://arxiv.org/pdf/2402.17214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17214]] CharacterGen: Efficient 3D Character Generation from Single Images with  Multi-View Pose Canonicalization(https://arxiv.org/abs/2402.17214)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model. This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.</li>
</ul>

<h3>Title: Feature Re-Embedding: Towards Foundation Model-Level Performance in  Computational Pathology</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Tang, Fengtao Zhou, Sheng Huang, Xiang Zhu, Yi Zhang, Bo Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17228">https://arxiv.org/abs/2402.17228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17228">https://arxiv.org/pdf/2402.17228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17228]] Feature Re-Embedding: Towards Foundation Model-Level Performance in  Computational Pathology(https://arxiv.org/abs/2402.17228)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multiple instance learning (MIL) is the most widely used framework in computational pathology, encompassing sub-typing, diagnosis, prognosis, and more. However, the existing MIL paradigm typically requires an offline instance feature extractor, such as a pre-trained ResNet or a foundation model. This approach lacks the capability for feature fine-tuning within the specific downstream tasks, limiting its adaptability and performance. To address this issue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding the instance features online, which captures fine-grained local features and establishes connections across different regions. Unlike existing works that focus on pre-training powerful feature extractor or designing sophisticated instance aggregator, R$^2$T is tailored to re-embed instance features online. It serves as a portable module that can seamlessly integrate into mainstream MIL models. Extensive experimental results on common computational pathology tasks validate that: 1) feature re-embedding improves the performance of MIL models based on ResNet-50 features to the level of foundation model features, and further enhances the performance of foundation model features; 2) the R$^2$T can introduce more significant performance improvements to various MIL models; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest methods by a large margin. The code is available at:~\href{https://github.com/DearCaat/RRT-MIL}{https://github.com/DearCaat/RRT-MIL}.</li>
</ul>

<h3>Title: Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in  Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail Doshi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17245">https://arxiv.org/abs/2402.17245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17245">https://arxiv.org/pdf/2402.17245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17245]] Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in  Text-to-Image Generation(https://arxiv.org/abs/2402.17245)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions and aspect ratios, outperforming both widely-used open-source models like SDXL and Playground v2, and closed-source commercial systems such as DALLE 3 and Midjourney v5.2. Our model is open-source, and we hope the development of Playground v2.5 provides valuable guidelines for researchers aiming to elevate the aesthetic quality of diffusion-based image generation models.</li>
</ul>

<h3>Title: Explicit Interaction for Fusion-Based Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Xu, Junyi Ma, Qi Wu, Zijie Zhou, Yue Wang, Xieyuanli Chen, Ling Pei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17264">https://arxiv.org/abs/2402.17264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17264">https://arxiv.org/pdf/2402.17264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17264]] Explicit Interaction for Fusion-Based Place Recognition(https://arxiv.org/abs/2402.17264)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Fusion-based place recognition is an emerging technique jointly utilizing multi-modal perception data, to recognize previously visited places in GPS-denied scenarios for robots and autonomous vehicles. Recent fusion-based place recognition methods combine multi-modal features in implicit manners. While achieving remarkable results, they do not explicitly consider what the individual modality affords in the fusion system. Therefore, the benefit of multi-modal feature fusion may not be fully explored. In this paper, we propose a novel fusion-based network, dubbed EINet, to achieve explicit interaction of the two modalities. EINet uses LiDAR ranges to supervise more robust vision features for long time spans, and simultaneously uses camera RGB data to improve the discrimination of LiDAR point clouds. In addition, we develop a new benchmark for the place recognition task based on the nuScenes dataset. To establish this benchmark for future research with comprehensive comparisons, we introduce both supervised and self-supervised training schemes alongside evaluation protocols. We conduct extensive experiments on the proposed benchmark, and the experimental results show that our EINet exhibits better recognition performance as well as solid generalization ability compared to the state-of-the-art fusion-based place recognition approaches. Our open-source code and benchmark are released at: https://github.com/BIT-XJY/EINet.</li>
</ul>

<h3>Title: One-Shot Structure-Aware Stylized Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Hansam Cho, Jonghyun Lee, Seunggyu Chang, Yonghyun Jeong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17275">https://arxiv.org/abs/2402.17275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17275">https://arxiv.org/pdf/2402.17275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17275]] One-Shot Structure-Aware Stylized Image Synthesis(https://arxiv.org/abs/2402.17275)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with out-of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models.</li>
</ul>

<h3>Title: Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder  Super-resolution Network</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17285">https://arxiv.org/abs/2402.17285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17285">https://arxiv.org/pdf/2402.17285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17285]] Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder  Super-resolution Network(https://arxiv.org/abs/2402.17285)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically.</li>
</ul>

<h3>Title: An Interpretable Evaluation of Entropy-based Novelty of Generative  Models</h3>
<ul>
<li><strong>Authors: </strong>Jingwei Zhang, Cheuk Ting Li, Farzan Farnia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17287">https://arxiv.org/abs/2402.17287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17287">https://arxiv.org/pdf/2402.17287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17287]] An Interpretable Evaluation of Entropy-based Novelty of Generative  Models(https://arxiv.org/abs/2402.17287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models. While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community. In this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: Given the samples of a generative model $\mathcal{G}$ and a reference dataset $\mathcal{S}$, how can we discover and count the modes expressed by $\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty of distribution $P_\mathcal{G}$ with respect to distribution $P_\mathcal{S}$. We analytically interpret the behavior of the KEN score under mixture distributions with sub-Gaussian components. Next, we develop a method based on Cholesky decomposition to compute the KEN score from observed samples. We support the KEN-based quantification of novelty by presenting several numerical results on synthetic and real image distributions. Our numerical results indicate the success of the proposed approach in detecting the novel modes and the comparison of state-of-the-art generative models.</li>
</ul>

<h3>Title: DivAvatar: Diverse 3D Avatar Generation with a Single Prompt</h3>
<ul>
<li><strong>Authors: </strong>Weijing Tao, Biwen Lei, Kunhao Liu, Shijian Lu, Miaomiao Cui, Xuansong Xie, Chunyan Miao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17292">https://arxiv.org/abs/2402.17292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17292">https://arxiv.org/pdf/2402.17292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17292]] DivAvatar: Diverse 3D Avatar Generation with a Single Prompt(https://arxiv.org/abs/2402.17292)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-Avatar generation has recently made significant strides due to advancements in diffusion models. However, most existing work remains constrained by limited diversity, producing avatars with subtle differences in appearance for a given text prompt. We design DivAvatar, a novel framework that generates diverse avatars, empowering 3D creatives with a multitude of distinct and richly varied 3D avatars from a single text prompt. Different from most existing work that exploits scene-specific 3D representations such as NeRF, DivAvatar finetunes a 3D generative model (i.e., EVA3D), allowing diverse avatar generation from simply noise sampling in inference time. DivAvatar has two key designs that help achieve generation diversity and visual quality. The first is a noise sampling technique during training phase which is critical in generating diverse appearances. The second is a semantic-aware zoom mechanism and a novel depth loss, the former producing appearances of high textual fidelity by separate fine-tuning of specific body parts and the latter improving geometry quality greatly by smoothing the generated mesh in the features space. Extensive experiments show that DivAvatar is highly versatile in generating avatars of diverse appearances.</li>
</ul>

<h3>Title: SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific  Summarization</h3>
<ul>
<li><strong>Authors: </strong>Huy Quoc To, Hung-Nghiep Tran, Andr'e Greiner-Petter, Felix Beierle, Akiko Aizawa</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17311">https://arxiv.org/abs/2402.17311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17311">https://arxiv.org/pdf/2402.17311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17311]] SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific  Summarization(https://arxiv.org/abs/2402.17311)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Summarization for scientific text has shown significant benefits both for the research community and human society. Given the fact that the nature of scientific text is distinctive and the input of the multi-document summarization task is substantially long, the task requires sufficient embedding generation and text truncation without losing important information. To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid framework for multi-document scientific summarization (MDSS). We leverage the Sentence-Transformer version of Scientific Paper Embeddings using Citation-Informed Transformers (SPECTER) to encode and represent textual sentences, allowing for efficient extractive summarization using k-means clustering. We employ the T5 family of models to generate abstractive summaries using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on the Multi-XScience dataset. Through extensive experiments and evaluation, we showcase the benefits of our model by using less complicated models to achieve remarkable results, thereby highlighting its potential in advancing the field of multi-document summarization for scientific text.</li>
</ul>

<h3>Title: Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via  Selective Entropy Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yaofo Chen, Shuaicheng Niu, Shoukai Xu, Hengjie Song, Yaowei Wang, Mingkui Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17316">https://arxiv.org/abs/2402.17316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17316">https://arxiv.org/pdf/2402.17316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17316]] Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via  Selective Entropy Distillation(https://arxiv.org/abs/2402.17316)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.</li>
</ul>

<h3>Title: SDDGR: Stable Diffusion-based Deep Generative Replay for Class  Incremental Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17323">https://arxiv.org/abs/2402.17323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17323">https://arxiv.org/pdf/2402.17323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17323]] SDDGR: Stable Diffusion-based Deep Generative Replay for Class  Incremental Object Detection(https://arxiv.org/abs/2402.17323)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In the field of class incremental learning (CIL), genera- tive replay has become increasingly prominent as a method to mitigate the catastrophic forgetting, alongside the con- tinuous improvements in generative models. However, its application in class incremental object detection (CIOD) has been significantly limited, primarily due to the com- plexities of scenes involving multiple labels. In this paper, we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text- to-diffusion networks to generate realistic and diverse syn- thetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally, we adopt an L2 knowledge distilla- tion technique to improve the retention of prior knowledge in synthetic images. Furthermore, our approach includes pseudo-labeling for old objects within new task images, pre- venting misclassification as background elements. Exten- sive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms, achieving a new state-of-the-art in various CIOD scenarios. The source code will be made available to the public.</li>
</ul>

<h3>Title: Data-Efficient Learning via Clustering-Based Sensitivity Sampling:  Foundation Models and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, David Woodruff, Michael Wunder</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17327">https://arxiv.org/abs/2402.17327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17327">https://arxiv.org/pdf/2402.17327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17327]] Data-Efficient Learning via Clustering-Based Sensitivity Sampling:  Foundation Models and Beyond(https://arxiv.org/abs/2402.17327)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We study the data selection problem, whose aim is to select a small representative subset of data that can be used to efficiently train a machine learning model. We present a new data selection approach based on $k$-means clustering and sensitivity sampling. Assuming access to an embedding representation of the data with respect to which the model loss is H\"older continuous, our approach provably allows selecting a set of ``typical'' $k + 1/\varepsilon^2$ elements whose average loss corresponds to the average loss of the whole dataset, up to a multiplicative $(1\pm\varepsilon)$ factor and an additive $\varepsilon \lambda \Phi_k$, where $\Phi_k$ represents the $k$-means cost for the input embeddings and $\lambda$ is the H\"older constant. We furthermore demonstrate the performance and scalability of our approach on fine-tuning foundation models and show that it outperforms state-of-the-art methods. We also show how it can be applied on linear regression, leading to a new sampling strategy that surprisingly matches the performances of leverage score sampling, while being conceptually simpler and more scalable.</li>
</ul>

<h3>Title: LocalGCL: Local-aware Contrastive Learning for Graphs</h3>
<ul>
<li><strong>Authors: </strong>Haojun Jiang, Jiawei Sun, Jie Li, Chentao Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17345">https://arxiv.org/abs/2402.17345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17345">https://arxiv.org/pdf/2402.17345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17345]] LocalGCL: Local-aware Contrastive Learning for Graphs(https://arxiv.org/abs/2402.17345)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph representation learning (GRL) makes considerable progress recently, which encodes graphs with topological structures into low-dimensional embeddings. Meanwhile, the time-consuming and costly process of annotating graph labels manually prompts the growth of self-supervised learning (SSL) techniques. As a dominant approach of SSL, Contrastive learning (CL) learns discriminative representations by differentiating between positive and negative samples. However, when applied to graph data, it overemphasizes global patterns while neglecting local structures. To tackle the above issue, we propose \underline{Local}-aware \underline{G}raph \underline{C}ontrastive \underline{L}earning (\textbf{\methnametrim}), a self-supervised learning framework that supplementarily captures local graph information with masking-based modeling compared with vanilla contrastive learning. Extensive experiments validate the superiority of \methname against state-of-the-art methods, demonstrating its promise as a comprehensive graph representation learner.</li>
</ul>

<h3>Title: RECOST: External Knowledge Guided Data-efficient Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Qi Zhang, Yiming Zhang, Haobo Wang, Junbo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17355">https://arxiv.org/abs/2402.17355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17355">https://arxiv.org/pdf/2402.17355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17355]] RECOST: External Knowledge Guided Data-efficient Instruction Tuning(https://arxiv.org/abs/2402.17355)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In the current landscape of large language models (LLMs), the process of instruction tuning serves as an essential step. Considering the high computing power overhead, data-efficient instruction tuning was proposed to reduce the training data size in this process, aiming at selecting high-quality instructional data. Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by LLMs with an in-context-based relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as \textbf{RECOST}, which integrates external-knowledge-base re-ranking and diversity-consistent sampling into a single pipeline. Through extensive experiments on several synthetic datasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our method and achieve even better results with only \textbf{1\%} of the full dataset.</li>
</ul>

<h3>Title: Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud  Matching</h3>
<ul>
<li><strong>Authors: </strong>Matteo Bastico, Etienne Decencière, Laurent Corté, Yannick Tillier, David Ryckelynck</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17372">https://arxiv.org/abs/2402.17372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17372">https://arxiv.org/pdf/2402.17372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17372]] Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud  Matching(https://arxiv.org/abs/2402.17372)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Point cloud matching, a crucial technique in computer vision, medical and robotics fields, is primarily concerned with finding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights on the paired geometries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account fine local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian, that allows to easily generate aligned eigenspaces for multiple rigidly-registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We initially evaluate the performance of the proposed technique in a point-wise manner, specifically focusing on the task of object anomaly localization using the MVTec 3D-AD dataset. Additionally, we define a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks. The code to reproduce our experiments is publicly available at https://github.com/matteo-bastico/CoupledLaplacian and in the Supplementary Code.</li>
</ul>

<h3>Title: Accelerating Diffusion Sampling with Optimized Time Steps</h3>
<ul>
<li><strong>Authors: </strong>Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, Zhenguo Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17376">https://arxiv.org/abs/2402.17376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17376">https://arxiv.org/pdf/2402.17376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17376]] Accelerating Diffusion Sampling with Optimized Time Steps(https://arxiv.org/abs/2402.17376)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than $15$ seconds. Our extensive experiments on both unconditional and conditional sampling using pixel- and latent-space DPMs demonstrate that, when combined with the state-of-the-art sampling method UniPC, our optimized time steps significantly improve image generation performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet, compared to using uniform time steps.</li>
</ul>

<h3>Title: LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Shentong Mo, Yansen Wang, Xufang Luo, Dongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17406">https://arxiv.org/abs/2402.17406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17406">https://arxiv.org/pdf/2402.17406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17406]] LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning(https://arxiv.org/abs/2402.17406)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks. Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding. This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories. To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks. Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance.</li>
</ul>

<h3>Title: DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized  Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, Pin-Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17412">https://arxiv.org/abs/2402.17412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17412">https://arxiv.org/pdf/2402.17412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17412]] DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized  Diffusion Model(https://arxiv.org/abs/2402.17412)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints, we introduce \textbf{\textit{DiffuseKronA}}, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by 35\% and 99.947\% compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis. Crucially, \textit{DiffuseKronA} mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning. Furthermore, a more controllable decomposition makes \textit{DiffuseKronA} more interpretable and even can achieve up to a 50\% reduction with results comparable to LoRA-Dreambooth. Evaluated against diverse and complex input images and text prompts, \textit{DiffuseKronA} consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, all the while upholding exceptional parameter efficiency, thus presenting a substantial advancement in the field of T2I generative modeling. Our project page, consisting of links to the code, and pre-trained checkpoints, is available at \href{https://diffusekrona.github.io/}{https://diffusekrona.github.io/}.</li>
</ul>

<h3>Title: Reinforced In-Context Black-Box Optimization</h3>
<ul>
<li><strong>Authors: </strong>Lei Song, Chenxiao Gao, Ke Xue, Chenyang Wu, Dong Li, Jianye Hao, Zongzhang Zhang, Chao Qian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17423">https://arxiv.org/abs/2402.17423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17423">https://arxiv.org/pdf/2402.17423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17423]] Reinforced In-Context Black-Box Optimization(https://arxiv.org/abs/2402.17423)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumulative regret of the histories. The integration of regret-to-go tokens enables RIBBO to automatically generate sequences of query points that satisfy the user-desired regret, which is verified by its universally good empirical performance on diverse problems, including BBOB functions, hyper-parameter optimization and robot control problems.</li>
</ul>

<h3>Title: Enhancing EEG-to-Text Decoding through Transferable Representations from  Pre-trained Contrastive EEG-Text Masked Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min Zhang, Zhiguo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17433">https://arxiv.org/abs/2402.17433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17433">https://arxiv.org/pdf/2402.17433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17433]] Enhancing EEG-to-Text Decoding through Transferable Representations from  Pre-trained Contrastive EEG-Text Masked Autoencoder(https://arxiv.org/abs/2402.17433)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. These results indicate significant advancements in the field and underscores the proposed framework's potential to enable more powerful and widespread BCI applications.</li>
</ul>

<h3>Title: Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing</h3>
<ul>
<li><strong>Authors: </strong>Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17464">https://arxiv.org/abs/2402.17464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17464">https://arxiv.org/pdf/2402.17464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17464]] Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing(https://arxiv.org/abs/2402.17464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative 3D part assembly involves understanding part relationships and predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work often focus on the geometry of individual parts, neglecting part-whole hierarchies of objects. Leveraging two key observations: 1) super-part poses provide strong hints about part poses, and 2) predicting super-part poses is easier due to fewer superparts, we propose a part-whole-hierarchy message passing network for efficient 3D part assembly. We first introduce super-parts by grouping geometrically similar parts without any semantic labels. Then we employ a part-whole hierarchical encoder, wherein a super-part encoder predicts latent super-part poses based on input parts. Subsequently, we transform the point cloud using the latent poses, feeding it to the part encoder for aggregating super-part information and reasoning about part relationships to predict all part poses. In training, only ground-truth part poses are required. During inference, the predicted latent poses of super-parts enhance interpretability. Experimental results on the PartNet dataset show that our method achieves state-of-the-art performance in part and connectivity accuracy and enables an interpretable hierarchical part assembly.</li>
</ul>

<h3>Title: EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with  Audio2Video Diffusion Model under Weak Conditions</h3>
<ul>
<li><strong>Authors: </strong>Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17485">https://arxiv.org/abs/2402.17485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17485">https://arxiv.org/pdf/2402.17485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17485]] EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with  Audio2Video Diffusion Model under Weak Conditions(https://arxiv.org/abs/2402.17485)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we tackle the challenge of enhancing the realism and expressiveness in talking head video generation by focusing on the dynamic and nuanced relationship between audio cues and facial movements. We identify the limitations of traditional techniques that often fail to capture the full spectrum of human expressions and the uniqueness of individual facial styles. To address these issues, we propose EMO, a novel framework that utilizes a direct audio-to-video synthesis approach, bypassing the need for intermediate 3D models or facial landmarks. Our method ensures seamless frame transitions and consistent identity preservation throughout the video, resulting in highly expressive and lifelike animations. Experimental results demonsrate that EMO is able to produce not only convincing speaking videos but also singing videos in various styles, significantly outperforming existing state-of-the-art methodologies in terms of expressiveness and realism.</li>
</ul>

<h3>Title: Prescribing Large Language Models for Perioperative Care: What's The  Right Dose for Pre-trained Models?</h3>
<ul>
<li><strong>Authors: </strong>Bing Xue, Charles Alba, Joanna Abraham, Thomas Kannampallil, Chenyang Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17493">https://arxiv.org/abs/2402.17493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17493">https://arxiv.org/pdf/2402.17493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17493]] Prescribing Large Language Models for Perioperative Care: What's The  Right Dose for Pre-trained Models?(https://arxiv.org/abs/2402.17493)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC. Adapting models further improved performance: (1) self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care.</li>
</ul>

<h3>Title: Intensive Care as One Big Sequence Modeling Problem</h3>
<ul>
<li><strong>Authors: </strong>Vadim Liventsev, Tobias Fritz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17501">https://arxiv.org/abs/2402.17501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17501">https://arxiv.org/pdf/2402.17501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17501]] Intensive Care as One Big Sequence Modeling Problem(https://arxiv.org/abs/2402.17501)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a uniform event stream format, train a baseline model and explore its capabilities.</li>
</ul>

<h3>Title: QUCE: The Minimisation and Quantification of Path-Based Uncertainty for  Generative Counterfactual Explanations</h3>
<ul>
<li><strong>Authors: </strong>Jamie Duell, Hsuan Fu, Monika Seisenberger, Xiuyi Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17516">https://arxiv.org/abs/2402.17516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17516">https://arxiv.org/pdf/2402.17516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17516]] QUCE: The Minimisation and Quantification of Path-Based Uncertainty for  Generative Counterfactual Explanations(https://arxiv.org/abs/2402.17516)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting explanations but also generates more certain counterfactual examples. We showcase the performance of the QUCE method by comparing it with competing methods for both path-based explanations and generative counterfactual examples. The code repository for the QUCE method is available at: https://github.com/jamie-duell/QUCE.</li>
</ul>

<h3>Title: Label-Noise Robust Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Byeonghu Na, Yeongmin Kim, HeeSun Bae, Jung Hyun Lee, Se Jung Kwon, Wanmo Kang, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17517">https://arxiv.org/abs/2402.17517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17517">https://arxiv.org/pdf/2402.17517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17517]] Label-Noise Robust Diffusion Models(https://arxiv.org/abs/2402.17517)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to condition mismatch and quality degradation of generated data. This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional diffusion models with noisy labels, which is the first study in the line of diffusion models. The TDSM objective contains a weighted sum of score networks, incorporating instance-wise and time-dependent label transition probabilities. We introduce a transition-aware weight estimator, which leverages a time-dependent noisy-label classifier distinctively customized to the diffusion process. Through experiments across various datasets and noisy label settings, TDSM improves the quality of generated samples aligned with given conditions. Furthermore, our method improves generation performance even on prevalent benchmark datasets, which implies the potential noisy labels and their risk of generative model learning. Finally, we show the improved performance of TDSM on top of conventional noisy label corrections, which empirically proving its contribution as a part of label-noise robust generative models. Our code is available at: https://github.com/byeonghu-na/tdsm.</li>
</ul>

<h3>Title: Diffusion Model-Based Image Editing: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17525">https://arxiv.org/abs/2402.17525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17525">https://arxiv.org/pdf/2402.17525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17525]] Diffusion Model-Based Image Editing: A Survey(https://arxiv.org/abs/2402.17525)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.</li>
</ul>

<h3>Title: Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised  Semantic Segmentation with Its Class Label</h3>
<ul>
<li><strong>Authors: </strong>Xinliang Zhang, Lei Zhu, Hangzhou He, Lujia Jin, Yanye Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17555">https://arxiv.org/abs/2402.17555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17555">https://arxiv.org/pdf/2402.17555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17555]] Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised  Semantic Segmentation with Its Class Label(https://arxiv.org/abs/2402.17555)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.</li>
</ul>

<h3>Title: Structure-Guided Adversarial Training of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17563">https://arxiv.org/abs/2402.17563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17563">https://arxiv.org/pdf/2402.17563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17563]] Structure-Guided Adversarial Training of Diffusion Models(https://arxiv.org/abs/2402.17563)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their training primarily emphasizes instance-level optimization, overlooking valuable structural information within each mini-batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffusion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers (DiT) and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image generation at resolutions of 256x256 and 512x512, respectively.</li>
</ul>

<h3>Title: A Large-scale Evaluation of Pretraining Paradigms for the Detection of  Defects in Electroluminescence Solar Cell Images</h3>
<ul>
<li><strong>Authors: </strong>David Torpey, Lawrence Pratt, Richard Klein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17611">https://arxiv.org/abs/2402.17611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17611">https://arxiv.org/pdf/2402.17611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17611]] A Large-scale Evaluation of Pretraining Paradigms for the Detection of  Defects in Electroluminescence Solar Cell Images(https://arxiv.org/abs/2402.17611)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Pretraining has been shown to improve performance in many domains, including semantic segmentation, especially in domains with limited labelled data. In this work, we perform a large-scale evaluation and benchmarking of various pretraining methods for Solar Cell Defect Detection (SCDD) in electroluminescence images, a field with limited labelled datasets. We cover supervised training with semantic segmentation, semi-supervised learning, and two self-supervised techniques. We also experiment with both in-distribution and out-of-distribution (OOD) pretraining and observe how this affects downstream performance. The results suggest that supervised training on a large OOD dataset (COCO), self-supervised pretraining on a large OOD dataset (ImageNet), and semi-supervised pretraining (CCT) all yield statistically equivalent performance for mean Intersection over Union (mIoU). We achieve a new state-of-the-art for SCDD and demonstrate that certain pretraining schemes result in superior performance on underrepresented classes. Additionally, we provide a large-scale unlabelled EL image dataset of $22000$ images, and a $642$-image labelled semantic segmentation EL dataset, for further research in developing self- and semi-supervised training techniques in this domain.</li>
</ul>

<h3>Title: Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image  Modeling</h3>
<ul>
<li><strong>Authors: </strong>David S. W. Williams, Matthew Gadd, Paul Newman, Daniele De Martini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17622">https://arxiv.org/abs/2402.17622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17622">https://arxiv.org/pdf/2402.17622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17622]] Masked Gamma-SSL: Learning Uncertainty Estimation via Masked Image  Modeling(https://arxiv.org/abs/2402.17622)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This work proposes a semantic segmentation network that produces high-quality uncertainty estimates in a single forward pass. We exploit general representations from foundation models and unlabelled datasets through a Masked Image Modeling (MIM) approach, which is robust to augmentation hyper-parameters and simpler than previous techniques. For neural networks used in safety-critical applications, bias in the training data can lead to errors; therefore it is crucial to understand a network's limitations at run time and act accordingly. To this end, we test our proposed method on a number of test domains including the SAX Segmentation benchmark, which includes labelled test data from dense urban, rural and off-road driving domains. The proposed method consistently outperforms uncertainty estimation and Out-of-Distribution (OoD) techniques on this difficult benchmark.</li>
</ul>

<h3>Title: Securing Reliability: A Brief Overview on Enhancing In-Context Learning  for Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen, Xiaoxing Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17671">https://arxiv.org/abs/2402.17671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17671">https://arxiv.org/pdf/2402.17671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17671]] Securing Reliability: A Brief Overview on Enhancing In-Context Learning  for Foundation Models(https://arxiv.org/abs/2402.17671)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.</li>
</ul>

<h3>Title: Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion  Latent Aligners</h3>
<ul>
<li><strong>Authors: </strong>Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17723">https://arxiv.org/abs/2402.17723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17723">https://arxiv.org/pdf/2402.17723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17723]] Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion  Latent Aligners(https://arxiv.org/abs/2402.17723)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/</li>
</ul>

<h3>Title: Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using  Contrastive Learning and Geometric Unfolding</h3>
<ul>
<li><strong>Authors: </strong>Alexander Oberstrass, Jordan DeKraker, Nicola Palomero-Gallagher, Sascha E. A. Muenzing, Alan C. Evans, Markus Axer, Katrin Amunts, Timo Dickscheid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.17744">https://arxiv.org/abs/2402.17744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.17744">https://arxiv.org/pdf/2402.17744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.17744]] Analyzing Regional Organization of the Human Hippocampus in 3D-PLI Using  Contrastive Learning and Geometric Unfolding(https://arxiv.org/abs/2402.17744)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Understanding the cortical organization of the human brain requires interpretable descriptors for distinct structural and functional imaging data. 3D polarized light imaging (3D-PLI) is an imaging modality for visualizing fiber architecture in postmortem brains with high resolution that also captures the presence of cell bodies, for example, to identify hippocampal subfields. The rich texture in 3D-PLI images, however, makes this modality particularly difficult to analyze and best practices for characterizing architectonic patterns still need to be established. In this work, we demonstrate a novel method to analyze the regional organization of the human hippocampus in 3D-PLI by combining recent advances in unfolding methods with deep texture features obtained using a self-supervised contrastive learning approach. We identify clusters in the representations that correspond well with classical descriptions of hippocampal subfields, lending validity to the developed methodology.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
