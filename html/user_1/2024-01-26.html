<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-26</h1>
<h3>Title: Can I trust my fake data -- A comprehensive quality assessment framework  for synthetic tabular data in healthcare</h3>
<ul>
<li><strong>Authors: </strong>Vibeke Binz Vallevik, Aleksandar Babic, Serena Elizabeth Marshall, Severin Elvatun, Helga Brøgger, Sharmini Alagaratnam, Bjørn Edwin, Narasimha Raghavan Veeraragavan, Anne Kjersti Befring, Jan Franz Nygård</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13716">https://arxiv.org/abs/2401.13716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13716">https://arxiv.org/pdf/2401.13716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13716]] Can I trust my fake data -- A comprehensive quality assessment framework  for synthetic tabular data in healthcare(https://arxiv.org/abs/2401.13716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Ensuring safe adoption of AI tools in healthcare hinges on access to sufficient data for training, testing and validation. In response to privacy concerns and regulatory requirements, using synthetic data has been suggested. Synthetic data is created by training a generator on real data to produce a dataset with similar statistical properties. Competing metrics with differing taxonomies for quality evaluation have been suggested, resulting in a complex landscape. Optimising quality entails balancing considerations that make the data fit for use, yet relevant dimensions are left out of existing frameworks. We performed a comprehensive literature review on the use of quality evaluation metrics on SD within the scope of tabular healthcare data and SD made using deep generative methods. Based on this and the collective team experiences, we developed a conceptual framework for quality assurance. The applicability was benchmarked against a practical case from the Dutch National Cancer Registry. We present a conceptual framework for quality assurance of SD for AI applications in healthcare that aligns diverging taxonomies, expands on common quality dimensions to include the dimensions of Fairness and Carbon footprint, and proposes stages necessary to support real-life applications. Building trust in synthetic data by increasing transparency and reducing the safety risk will accelerate the development and uptake of trustworthy AI tools for the benefit of patients. Despite the growing emphasis on algorithmic fairness and carbon footprint, these metrics were scarce in the literature review. The overwhelming focus was on statistical similarity using distance metrics while sequential logic detection was scarce. A consensus-backed framework that includes all relevant quality dimensions can provide assurance for safe and responsible real-life applications of SD.</li>
</ul>

<h3>Title: Inference Attacks Against Face Recognition Model without Classification  Layers</h3>
<ul>
<li><strong>Authors: </strong>Yuanqing Huang, Huilong Chen, Yinggui Wang, Lei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13719">https://arxiv.org/abs/2401.13719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13719">https://arxiv.org/pdf/2401.13719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13719]] Inference Attacks Against Face Recognition Model without Classification  Layers(https://arxiv.org/abs/2401.13719)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face recognition (FR) has been applied to nearly every aspect of daily life, but it is always accompanied by the underlying risk of leaking private information. At present, almost all attack models against FR rely heavily on the presence of a classification layer. However, in practice, the FR model can obtain complex features of the input via the model backbone, and then compare it with the target for inference, which does not explicitly involve the outputs of the classification layer adopting logit or other losses. In this work, we advocate a novel inference attack composed of two stages for practical FR models without a classification layer. The first stage is the membership inference attack. Specifically, We analyze the distances between the intermediate features and batch normalization (BN) parameters. The results indicate that this distance is a critical metric for membership inference. We thus design a simple but effective attack model that can determine whether a face image is from the training dataset or not. The second stage is the model inversion attack, where sensitive private data is reconstructed using a pre-trained generative adversarial network (GAN) guided by the attack model in the first stage. To the best of our knowledge, the proposed attack model is the very first in the literature developed for FR models without a classification layer. We illustrate the application of the proposed attack model in the establishment of privacy-preserving FR techniques.</li>
</ul>

<h3>Title: Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent  Diffusion Models for Virtual Try-All</h3>
<ul>
<li><strong>Authors: </strong>Mehmet Saygin Seyfioglu, Karim Bouyarmane, Suren Kumar, Amir Tavanaei, Ismail B. Tutar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13795">https://arxiv.org/abs/2401.13795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13795">https://arxiv.org/pdf/2401.13795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13795]] Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent  Diffusion Models for Virtual Try-All(https://arxiv.org/abs/2401.13795)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As online shopping is growing, the ability for buyers to virtually visualize products in their settings-a phenomenon we define as "Virtual Try-All"-has become crucial. Recent diffusion models inherently contain a world model, rendering them suitable for this task within an inpainting context. However, traditional image-conditioned diffusion models often fail to capture the fine-grained details of products. In contrast, personalization-driven models such as DreamPaint are good at preserving the item's details but they are not optimized for real-time applications. We present "Diffuse to Choose," a novel diffusion-based image-conditioned inpainting model that efficiently balances fast inference with the retention of high-fidelity details in a given reference item while ensuring accurate semantic manipulations in the given scene content. Our approach is based on incorporating fine-grained features from the reference image directly into the latent feature maps of the main diffusion model, alongside with a perceptual loss to further preserve the reference item's details. We conduct extensive testing on both in-house and publicly available datasets, and show that Diffuse to Choose is superior to existing zero-shot diffusion inpainting methods as well as few-shot diffusion personalization algorithms like DreamPaint.</li>
</ul>

<h3>Title: Automated Root Causing of Cloud Incidents using In-Context Learning with  GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Xuchao Zhang, Supriyo Ghosh, Chetan Bansal, Rujia Wang, Minghua Ma, Yu Kang, Saravan Rajmohan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13810">https://arxiv.org/abs/2401.13810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13810">https://arxiv.org/pdf/2401.13810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13810]] Automated Root Causing of Cloud Incidents using In-Context Learning with  GPT-4(https://arxiv.org/abs/2401.13810)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model's immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000 production incidents, comparing several large language models using multiple metrics. The results reveal that our in-context learning approach outperforms the previous fine-tuned large language models such as GPT-3 by an average of 24.8\% across all metrics, with an impressive 49.7\% improvement over the zero-shot model. Moreover, human evaluation involving actual incident owners demonstrates its superiority over the fine-tuned model, achieving a 43.5\% improvement in correctness and an 8.7\% enhancement in readability. The impressive results demonstrate the viability of utilizing a vanilla GPT model for the RCA task, thereby avoiding the high computational and maintenance costs associated with a fine-tuned model.</li>
</ul>

<h3>Title: Inverse Molecular Design with Multi-Conditional Diffusion Guidance</h3>
<ul>
<li><strong>Authors: </strong>Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13858">https://arxiv.org/abs/2401.13858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13858">https://arxiv.org/pdf/2401.13858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13858]] Inverse Molecular Design with Multi-Conditional Diffusion Guidance(https://arxiv.org/abs/2401.13858)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution learning to condition control for molecular properties. An inverse polymer design task for gas separation with feedback from domain experts further demonstrates its practical utility.</li>
</ul>

<h3>Title: Appearance Debiased Gaze Estimation via Stochastic Subject-Wise  Adversarial Learning</h3>
<ul>
<li><strong>Authors: </strong>Suneung Kim, Woo-Jeoung Nam, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13865">https://arxiv.org/abs/2401.13865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13865">https://arxiv.org/pdf/2401.13865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13865]] Appearance Debiased Gaze Estimation via Stochastic Subject-Wise  Adversarial Learning(https://arxiv.org/abs/2401.13865)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, appearance-based gaze estimation has been attracting attention in computer vision, and remarkable improvements have been achieved using various deep learning techniques. Despite such progress, most methods aim to infer gaze vectors from images directly, which causes overfitting to person-specific appearance factors. In this paper, we address these challenges and propose a novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE), which trains a network to generalize the appearance of subjects. We design a Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face identity classifier and a proposed adversarial loss. The proposed loss generalizes face appearance factors so that the identity classifier inferences a uniform probability distribution. In addition, the Fgen-Net is trained by a learning mechanism that optimizes the network by reselecting a subset of subjects at every training step to avoid overfitting. Our experimental results verify the robustness of the method in that it yields state-of-the-art performance, achieving 3.89 and 4.42 on the MPIIGaze and EyeDiap datasets, respectively. Furthermore, we demonstrate the positive generalization effect by conducting further experiments using face images involving different styles generated from the generative model.</li>
</ul>

<h3>Title: Edge Conditional Node Update Graph Neural Network for Multi-variate Time  Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Hayoung Jo, Seong-Whan Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13872">https://arxiv.org/abs/2401.13872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13872">https://arxiv.org/pdf/2401.13872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13872]] Edge Conditional Node Update Graph Neural Network for Multi-variate Time  Series Anomaly Detection(https://arxiv.org/abs/2401.13872)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>With the rapid advancement in cyber-physical systems, the increasing number of sensors has significantly complicated manual monitoring of system states. Consequently, graph-based time-series anomaly detection methods have gained attention due to their ability to explicitly represent relationships between sensors. However, these methods often apply a uniform source node representation across all connected target nodes, even when updating different target node representations. Moreover, the graph attention mechanism, commonly used to infer unknown graph structures, could constrain the diversity of source node representations. In this paper, we introduce the Edge Conditional Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge conditional node update module, dynamically transforms source node representations based on connected edges to represent target nodes aptly. We validate performance on three real-world datasets: SWaT, WADI, and PSM. Our model demonstrates 5.4%, 12.4%, and 6.0% higher performance, respectively, compared to best F1 baseline models.</li>
</ul>

<h3>Title: A Survey of Deep Learning and Foundation Models for Time Series  Forecasting</h3>
<ul>
<li><strong>Authors: </strong>John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13912">https://arxiv.org/abs/2401.13912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13912">https://arxiv.org/pdf/2401.13912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13912]] A Survey of Deep Learning and Foundation Models for Time Series  Forecasting(https://arxiv.org/abs/2401.13912)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided.</li>
</ul>

<h3>Title: Self-supervised Video Object Segmentation with Distillation Learning of  Deformable Attention</h3>
<ul>
<li><strong>Authors: </strong>Quang-Trung Truong, Duc Thanh Nguyen, Binh-Son Hua, Sai-Kit Yeung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13937">https://arxiv.org/abs/2401.13937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13937">https://arxiv.org/pdf/2401.13937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13937]] Self-supervised Video Object Segmentation with Distillation Learning of  Deformable Attention(https://arxiv.org/abs/2401.13937)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Video object segmentation is a fundamental research problem in computer vision. Recent techniques have often applied attention mechanism to object representation learning from video sequences. However, due to temporal changes in the video data, attention maps may not well align with the objects of interest across video frames, causing accumulated errors in long-term video processing. In addition, existing techniques have utilised complex architectures, requiring highly computational complexity and hence limiting the ability to integrate video object segmentation into low-powered devices. To address these issues, we propose a new method for self-supervised video object segmentation based on distillation learning of deformable attention. Specifically, we devise a lightweight architecture for video object segmentation that is effectively adapted to temporal changes. This is enabled by deformable attention mechanism, where the keys and values capturing the memory of a video sequence in the attention module have flexible locations updated across frames. The learnt object representations are thus adaptive to both the spatial and temporal dimensions. We train the proposed architecture in a self-supervised fashion through a new knowledge distillation paradigm where deformable attention maps are integrated into the distillation loss. We qualitatively and quantitatively evaluate our method and compare it with existing methods on benchmark datasets including DAVIS 2016/2017 and YouTube-VOS 2018/2019. Experimental results verify the superiority of our method via its achieved state-of-the-art performance and optimal memory usage.</li>
</ul>

<h3>Title: StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Yalong Bai, Mohan Zhou, Qing Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13942">https://arxiv.org/abs/2401.13942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13942">https://arxiv.org/pdf/2401.13942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13942]] StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion  Models(https://arxiv.org/abs/2401.13942)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The ability to fine-tune generative models for text-to-image generation tasks is crucial, particularly facing the complexity involved in accurately interpreting and visualizing textual inputs. While LoRA is efficient for language model adaptation, it often falls short in text-to-image tasks due to the intricate demands of image generation, such as accommodating a broad spectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a specialized fine-tuning approach tailored for text-to-image models. StyleInject comprises multiple parallel low-rank parameter matrices, maintaining the diversity of visual features. It dynamically adapts to varying styles by adjusting the variance of visual features based on the characteristics of the input signal. This approach significantly minimizes the impact on the original model's text-image alignment capabilities while adeptly adapting to various styles in transfer learning. StyleInject proves particularly effective in learning from and enhancing a range of advanced, community-fine-tuned generative models. Our comprehensive experiments, including both small-sample and large-scale data fine-tuning as well as base model distillation, show that StyleInject surpasses traditional LoRA in both text-image semantic consistency and human preference evaluation, all while ensuring greater parameter efficiency.</li>
</ul>

<h3>Title: BootPIG: Bootstrapping Zero-shot Personalized Image Generation  Capabilities in Pretrained Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Senthil Purushwalkam, Akash Gokul, Shafiq Joty, Nikhil Naik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13974">https://arxiv.org/abs/2401.13974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13974">https://arxiv.org/pdf/2401.13974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13974]] BootPIG: Bootstrapping Zero-shot Personalized Image Generation  Capabilities in Pretrained Diffusion Models(https://arxiv.org/abs/2401.13974)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images. The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and image segmentation models. In contrast to existing methods that require several days of pretraining, the BootPIG architecture can be trained in approximately 1 hour. Experiments on the DreamBooth dataset demonstrate that BootPIG outperforms existing zero-shot methods while being comparable with test-time finetuning approaches. Through a user study, we validate the preference for BootPIG generations over existing methods both in maintaining fidelity to the reference object's appearance and aligning with textual prompts.</li>
</ul>

<h3>Title: Learning to Manipulate Artistic Images</h3>
<ul>
<li><strong>Authors: </strong>Wei Guo, Yuqi Zhang, De Ma, Qian Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13976">https://arxiv.org/abs/2401.13976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13976">https://arxiv.org/pdf/2401.13976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13976]] Learning to Manipulate Artistic Images(https://arxiv.org/abs/2401.13976)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent advancement in computer vision has significantly lowered the barriers to artistic creation. Exemplar-based image translation methods have attracted much attention due to flexibility and controllability. However, these methods hold assumptions regarding semantics or require semantic information as the input, while accurate semantics is not easy to obtain in artistic images. Besides, these methods suffer from cross-domain artifacts due to training data prior and generate imprecise structure due to feature compression in the spatial domain. In this paper, we propose an arbitrary Style Image Manipulation Network (SIM-Net), which leverages semantic-free information as guidance and a region transportation strategy in a self-supervised manner for image generation. Our method balances computational efficiency and high resolution to a certain extent. Moreover, our method facilitates zero-shot style image manipulation. Both qualitative and quantitative experiments demonstrate the superiority of our method over state-of-the-art methods.Code is available at https://github.com/SnailForce/SIM-Net.</li>
</ul>

<h3>Title: Diffusion-based Data Augmentation for Object Counting Problems</h3>
<ul>
<li><strong>Authors: </strong>Zhen Wang, Yuelei Li, Jia Wan, Nuno Vasconcelos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.13992">https://arxiv.org/abs/2401.13992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.13992">https://arxiv.org/pdf/2401.13992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.13992]] Diffusion-based Data Augmentation for Object Counting Problems(https://arxiv.org/abs/2401.13992)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Crowd counting is an important problem in computer vision due to its wide range of applications in image understanding. Currently, this problem is typically addressed using deep learning approaches, such as Convolutional Neural Networks (CNNs) and Transformers. However, deep networks are data-driven and are prone to overfitting, especially when the available labeled crowd dataset is limited. To overcome this limitation, we have designed a pipeline that utilizes a diffusion model to generate extensive training data. We are the first to generate images conditioned on a location dot map (a binary dot map that specifies the location of human heads) with a diffusion model. We are also the first to use these diverse synthetic data to augment the crowd counting models. Our proposed smoothed density map input for ControlNet significantly improves ControlNet's performance in generating crowds in the correct locations. Also, Our proposed counting loss for the diffusion model effectively minimizes the discrepancies between the location dot map and the crowd images generated. Additionally, our innovative guidance sampling further directs the diffusion process toward regions where the generated crowd images align most accurately with the location dot map. Collectively, we have enhanced ControlNet's ability to generate specified objects from a location dot map, which can be used for data augmentation in various counting problems. Moreover, our framework is versatile and can be easily adapted to all kinds of counting problems. Extensive experiments demonstrate that our framework improves the counting performance on the ShanghaiTech, NWPU-Crowd, UCF-QNRF, and TRANCOS datasets, showcasing its effectiveness.</li>
</ul>

<h3>Title: ConstraintChecker: A Plugin for Large Language Models to Reason on  Commonsense Knowledge Bases</h3>
<ul>
<li><strong>Authors: </strong>Quyet V. Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14003">https://arxiv.org/abs/2401.14003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14003">https://arxiv.org/pdf/2401.14003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14003]] ConstraintChecker: A Plugin for Large Language Models to Reason on  Commonsense Knowledge Bases(https://arxiv.org/abs/2401.14003)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge. Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning. One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (Bengio et al., 2021). To this end, we proposed **ConstraintChecker**, a plugin over prompting techniques to provide and check explicit constraints. When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints. The acquired constraint-checking result is then aggregated with the output of the main prompting technique to produce the final output. Experimental results on CSKB Reasoning benchmarks demonstrate the effectiveness of our method by bringing consistent improvements over all prompting methods. Codes and data are available at \url{https://github.com/HKUST-KnowComp/ConstraintChecker}.</li>
</ul>

<h3>Title: Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation  for Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed, Ofir Arviv, Matan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera, Leshem Choshen, Michal Shmueli-Scheuer, Yoav Katz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14019">https://arxiv.org/abs/2401.14019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14019">https://arxiv.org/pdf/2401.14019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14019]] Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation  for Generative AI(https://arxiv.org/abs/2401.14019)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations. The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution. Addressing this need, we present Unitxt, an innovative library for customizable textual data preparation and evaluation tailored to generative language models. Unitxt natively integrates with common libraries like HuggingFace and LM-eval-harness and deconstructs processing flows into modular components, enabling easy customization and sharing between practitioners. These components encompass model-specific formats, task prompts, and many other comprehensive dataset processing definitions. The Unitxt-Catalog centralizes these components, fostering collaboration and exploration in modern textual data workflows. Beyond being a tool, Unitxt is a community-driven platform, empowering users to build, share, and advance their pipelines collaboratively. Join the Unitxt community at https://github.com/IBM/unitxt!</li>
</ul>

<h3>Title: (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Francesco Periti, Haim Dubossarsky, Nina Tahmasebi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14040">https://arxiv.org/abs/2401.14040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14040">https://arxiv.org/pdf/2401.14040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14040]] (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection(https://arxiv.org/abs/2401.14040)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.</li>
</ul>

<h3>Title: CreativeSynth: Creative Blending and Synthesis of Visual Arts based on  Multimodal Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Changsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14066">https://arxiv.org/abs/2401.14066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14066">https://arxiv.org/pdf/2401.14066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14066]] CreativeSynth: Creative Blending and Synthesis of Visual Arts based on  Multimodal Diffusion(https://arxiv.org/abs/2401.14066)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-image generative models have made impressive strides, showcasing their ability to synthesize a vast array of high-quality images. However, adapting these models for artistic image editing presents two significant challenges. Firstly, users struggle to craft textual prompts that meticulously detail visual elements of the input image. Secondly, prevalent models, when effecting modifications in specific zones, frequently disrupt the overall artistic style, complicating the attainment of cohesive and aesthetically unified artworks. To surmount these obstacles, we build the innovative unified framework CreativeSynth, which is based on a diffusion model with the ability to coordinate multimodal inputs and multitask in the field of artistic image generation. By integrating multimodal features with customized attention mechanisms, CreativeSynth facilitates the importation of real-world semantic content into the domain of art through inversion and real-time style transfer. This allows for the precise manipulation of image style and content while maintaining the integrity of the original model parameters. Rigorous qualitative and quantitative evaluations underscore that CreativeSynth excels in enhancing artistic images' fidelity and preserves their innate aesthetic essence. By bridging the gap between generative models and artistic finesse, CreativeSynth becomes a custom digital palette.</li>
</ul>

<h3>Title: Ta'keed: The First Generative Fact-Checking System for Arabic Claims</h3>
<ul>
<li><strong>Authors: </strong>Saud Althabiti, Mohammad Ammar Alsalka, Eric Atwell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14067">https://arxiv.org/abs/2401.14067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14067">https://arxiv.org/pdf/2401.14067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14067]] Ta'keed: The First Generative Fact-Checking System for Arabic Claims(https://arxiv.org/abs/2401.14067)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces Ta'keed, an explainable Arabic automatic fact-checking system. While existing research often focuses on classifying claims as "True" or "False," there is a limited exploration of generating explanations for claim credibility, particularly in Arabic. Ta'keed addresses this gap by assessing claim truthfulness based on retrieved snippets, utilizing two main components: information retrieval and LLM-based claim verification. We compiled the ArFactEx, a testing gold-labelled dataset with manually justified references, to evaluate the system. The initial model achieved a promising F1 score of 0.72 in the classification task. Meanwhile, the system's generated explanations are compared with gold-standard explanations syntactically and semantically. The study recommends evaluating using semantic similarities, resulting in an average cosine similarity score of 0.76. Additionally, we explored the impact of varying snippet quantities on claim classification accuracy, revealing a potential correlation, with the model using the top seven hits outperforming others with an F1 score of 0.77.</li>
</ul>

<h3>Title: CompactifAI: Extreme Compression of Large Language Models using  Quantum-Inspired Tensor Networks</h3>
<ul>
<li><strong>Authors: </strong>Andrei Tomut, Saeed S. Jahromi, Sukhbinder Singh, Faysal Ishtiaq, Cesar Muñoz, Prabdeep Singh Bajaj, Ali Elborady, Gianni del Bimbo, Mehrazin Alizadeh, David Montero, Pablo Martin-Ramiro, Muhammad Ibrahim, Oussama Tahiri Alaoui, John Malcolm, Samuel Mugel, Roman Orus</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14109">https://arxiv.org/abs/2401.14109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14109">https://arxiv.org/pdf/2401.14109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14109]] CompactifAI: Extreme Compression of Large Language Models using  Quantum-Inspired Tensor Networks(https://arxiv.org/abs/2401.14109)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined and interpretable model compression. Our method is versatile and can be implemented with - or on top of - other compression techniques. As a benchmark, we demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model to only $30\%$ of its original size while recovering over $90\%$ of the original accuracy after a brief distributed retraining.</li>
</ul>

<h3>Title: Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph  Conditioning in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Rameshwar Mishra, A V Subramanyam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14111">https://arxiv.org/abs/2401.14111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14111">https://arxiv.org/pdf/2401.14111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14111]] Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph  Conditioning in Diffusion Models(https://arxiv.org/abs/2401.14111)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset.</li>
</ul>

<h3>Title: Expression-aware video inpainting for HMD removal in XR applications</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Ghorbani Lohesara, Karen Egiazarian, Sebastian Knorr</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14136">https://arxiv.org/abs/2401.14136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14136">https://arxiv.org/pdf/2401.14136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14136]] Expression-aware video inpainting for HMD removal in XR applications(https://arxiv.org/abs/2401.14136)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Head-mounted displays (HMDs) serve as indispensable devices for observing extended reality (XR) environments and virtual content. However, HMDs present an obstacle to external recording techniques as they block the upper face of the user. This limitation significantly affects social XR applications, specifically teleconferencing, where facial features and eye gaze information play a vital role in creating an immersive user experience. In this study, we propose a new network for expression-aware video inpainting for HMD removal (EVI-HRnet) based on generative adversarial networks (GANs). Our model effectively fills in missing information with regard to facial landmarks and a single occlusion-free reference image of the user. The framework and its components ensure the preservation of the user's identity across frames using the reference frame. To further improve the level of realism of the inpainted output, we introduce a novel facial expression recognition (FER) loss function for emotion preservation. Our results demonstrate the remarkable capability of the proposed framework to remove HMDs from facial videos while maintaining the subject's facial expression and identity. Moreover, the outputs exhibit temporal consistency along the inpainted frames. This lightweight framework presents a practical approach for HMD occlusion removal, with the potential to enhance various collaborative XR applications without the need for additional hardware.</li>
</ul>

<h3>Title: Alleviating Structural Distribution Shift in Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, Yongdong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14155">https://arxiv.org/abs/2401.14155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14155">https://arxiv.org/pdf/2401.14155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14155]] Alleviating Structural Distribution Shift in Graph Anomaly Detection(https://arxiv.org/abs/2401.14155)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes -- abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes. Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper. The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization. This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophily. We tease out the anomaly features on which we constrain to mitigate the effect of heterophilous neighbors and make them invariant. We term our proposed framework as Graph Decomposition Network (GDN). Extensive experiments are conducted on two benchmark datasets, and the proposed framework achieves a remarkable performance boost in GAD, especially in an SDS environment where anomalies have largely different structural distribution across training and testing environments. Codes are open-sourced in https://github.com/blacksingular/wsdm_GDN.</li>
</ul>

<h3>Title: Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</h3>
<ul>
<li><strong>Authors: </strong>Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14159">https://arxiv.org/abs/2401.14159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14159">https://arxiv.org/pdf/2401.14159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14159]] Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks(https://arxiv.org/abs/2401.14159)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.</li>
</ul>

<h3>Title: AR-GAN: Generative Adversarial Network-Based Defense Method Against  Adversarial Attacks on the Traffic Sign Classification System of Autonomous  Vehicles</h3>
<ul>
<li><strong>Authors: </strong>M Sabbir Salek, Abdullah Al Mamun, Mashrur Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14232">https://arxiv.org/abs/2401.14232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14232">https://arxiv.org/pdf/2401.14232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14232]] AR-GAN: Generative Adversarial Network-Based Defense Method Against  Adversarial Attacks on the Traffic Sign Classification System of Autonomous  Vehicles(https://arxiv.org/abs/2401.14232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study developed a generative adversarial network (GAN)-based defense method for traffic sign classification in an autonomous vehicle (AV), referred to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i) assuming zero knowledge of adversarial attack models and samples and (ii) providing consistently high traffic sign classification performance under various adversarial attack types. The AR-GAN classification system consists of a generator that denoises an image by reconstruction, and a classifier that classifies the reconstructed image. The authors have tested the AR-GAN under no-attack and under various adversarial attacks, such as Fast Gradient Sign Method (FGSM), DeepFool, Carlini and Wagner (C&W), and Projected Gradient Descent (PGD). The authors considered two forms of these attacks, i.e., (i) black-box attacks (assuming the attackers possess no prior knowledge of the classifier), and (ii) white-box attacks (assuming the attackers possess full knowledge of the classifier). The classification performance of the AR-GAN was compared with several benchmark adversarial defense methods. The results showed that both the AR-GAN and the benchmark defense methods are resilient against black-box attacks and could achieve similar classification performance to that of the unperturbed images. However, for all the white-box attacks considered in this study, the AR-GAN method outperformed the benchmark defense methods. In addition, the AR-GAN was able to maintain its high classification performance under varied white-box adversarial perturbation magnitudes, whereas the performance of the other defense methods dropped abruptly at increased perturbation magnitudes.</li>
</ul>

<h3>Title: Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14257">https://arxiv.org/abs/2401.14257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14257">https://arxiv.org/pdf/2401.14257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14257]] Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation(https://arxiv.org/abs/2401.14257)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment.</li>
</ul>

<h3>Title: UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation  and Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Timo Kapsalis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14379">https://arxiv.org/abs/2401.14379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14379">https://arxiv.org/pdf/2401.14379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14379]] UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation  and Diffusion Models(https://arxiv.org/abs/2401.14379)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Intersection over Union (IoU) and CLIP scores across iterative evaluations for various categories of urban landscape features. Preliminary testing included utilising UrbanGenAI as an educational tool enhancing the learning experience in design pedagogy, and as a participatory instrument facilitating community-driven urban planning. Early results suggested that UrbanGenAI not only advances the technical frontiers of urban landscape reconstruction but also provides significant pedagogical and participatory planning benefits. The ongoing development of UrbanGenAI aims to further validate its effectiveness across broader contexts and integrate additional features such as real-time feedback mechanisms and 3D modelling capabilities. Keywords: generative AI; panoptic image segmentation; diffusion models; urban landscape design; design pedagogy; co-design</li>
</ul>

<h3>Title: Manifold GCN: Diffusion-based Convolutional Neural Network for  Manifold-valued Graphs</h3>
<ul>
<li><strong>Authors: </strong>Martin Hanik, Gabriele Steidl, Christoph von Tycowicz</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14381">https://arxiv.org/abs/2401.14381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14381">https://arxiv.org/pdf/2401.14381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14381]] Manifold GCN: Diffusion-based Convolutional Neural Network for  Manifold-valued Graphs(https://arxiv.org/abs/2401.14381)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose two graph neural network layers for graphs with features in a Riemannian manifold. First, based on a manifold-valued graph diffusion equation, we construct a diffusion layer that can be applied to an arbitrary number of nodes and graph connectivity patterns. Second, we model a tangent multilayer perceptron by transferring ideas from the vector neuron framework to our general setting. Both layers are equivariant with respect to node permutations and isometries of the feature manifold. These properties have been shown to lead to a beneficial inductive bias in many deep learning tasks. Numerical examples on synthetic data as well as on triangle meshes of the right hippocampus to classify Alzheimer's disease demonstrate the very good performance of our layers.</li>
</ul>

<h3>Title: pix2gestalt: Amodal Segmentation by Synthesizing Wholes</h3>
<ul>
<li><strong>Authors: </strong>Ege Ozguroglu, Ruoshi Liu, Dídac Surís, Dian Chen, Achal Dave, Pavel Tokmakov, Carl Vondrick</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14398">https://arxiv.org/abs/2401.14398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14398">https://arxiv.org/pdf/2401.14398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14398]] pix2gestalt: Amodal Segmentation by Synthesizing Wholes(https://arxiv.org/abs/2401.14398)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce pix2gestalt, a framework for zero-shot amodal segmentation, which learns to estimate the shape and appearance of whole objects that are only partially visible behind occlusions. By capitalizing on large-scale diffusion models and transferring their representations to this task, we learn a conditional diffusion model for reconstructing whole objects in challenging zero-shot cases, including examples that break natural and physical priors, such as art. As training data, we use a synthetically curated dataset containing occluded objects paired with their whole counterparts. Experiments show that our approach outperforms supervised baselines on established benchmarks. Our model can furthermore be used to significantly improve the performance of existing object recognition and 3D reconstruction methods in the presence of occlusions.</li>
</ul>

<h3>Title: Deconstructing Denoising Diffusion Models for Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.14404">https://arxiv.org/abs/2401.14404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.14404">https://arxiv.org/pdf/2401.14404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.14404]] Deconstructing Denoising Diffusion Models for Self-Supervised Learning(https://arxiv.org/abs/2401.14404)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
