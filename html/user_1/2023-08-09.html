<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Synthetic Augmentation with Large-scale Unconditional Pre-training. (arXiv:2308.04020v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04020">http://arxiv.org/abs/2308.04020</a></li>
<li>Code URL: https://github.com/karenyyy/histodiffaug</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04020]] Synthetic Augmentation with Large-scale Unconditional Pre-training(http://arxiv.org/abs/2308.04020)</code></li>
<li>Summary: <p>Deep learning based medical image recognition systems often require a
substantial amount of training data with expert annotations, which can be
expensive and time-consuming to obtain. Recently, synthetic augmentation
techniques have been proposed to mitigate the issue by generating realistic
images conditioned on class labels. However, the effectiveness of these methods
heavily depends on the representation capability of the trained generative
model, which cannot be guaranteed without sufficient labeled training data. To
further reduce the dependency on annotated data, we propose a synthetic
augmentation method called HistoDiffusion, which can be pre-trained on
large-scale unlabeled datasets and later applied to a small-scale labeled
dataset for augmented training. In particular, we train a latent diffusion
model (LDM) on diverse unlabeled datasets to learn common features and generate
realistic images without conditional inputs. Then, we fine-tune the model with
classifier guidance in latent space on an unseen labeled dataset so that the
model can synthesize images of specific categories. Additionally, we adopt a
selective mechanism to only add synthetic samples with high confidence of
matching to target labels. We evaluate our proposed method by pre-training on
three histopathology datasets and testing on a histopathology dataset of
colorectal cancer (CRC) excluded from the pre-training datasets. With
HistoDiffusion augmentation, the classification accuracy of a backbone
classifier is remarkably improved by 6.4% using a small set of the original
labels. Our code is available at https://github.com/karenyyy/HistoDiffAug.
</p></li>
</ul>

<h3>Title: MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2308.04249v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04249">http://arxiv.org/abs/2308.04249</a></li>
<li>Code URL: https://github.com/reedonepeck/minddiffuser</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04249]] MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion(http://arxiv.org/abs/2308.04249)</code></li>
<li>Summary: <p>Reconstructing visual stimuli from brain recordings has been a meaningful and
challenging task. Especially, the achievement of precise and controllable image
reconstruction bears great significance in propelling the progress and
utilization of brain-computer interfaces. Despite the advancements in complex
image reconstruction techniques, the challenge persists in achieving a cohesive
alignment of both semantic (concepts and objects) and structure (position,
orientation, and size) with the image stimuli. To address the aforementioned
issue, we propose a two-stage image reconstruction model called MindDiffuser.
In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings
decoded from fMRI are put into Stable Diffusion, which yields a preliminary
image that contains semantic information. In Stage 2, we utilize the CLIP
visual feature decoded from fMRI as supervisory information, and continually
adjust the two feature vectors decoded in Stage 1 through backpropagation to
align the structural information. The results of both qualitative and
quantitative analyses demonstrate that our model has surpassed the current
state-of-the-art models on Natural Scenes Dataset (NSD). The subsequent
experimental findings corroborate the neurobiological plausibility of the
model, as evidenced by the interpretability of the multimodal feature employed,
which align with the corresponding brain responses.
</p></li>
</ul>

<h3>Title: Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On. (arXiv:2308.04288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04288">http://arxiv.org/abs/2308.04288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04288]] Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On(http://arxiv.org/abs/2308.04288)</code></li>
<li>Summary: <p>Fabricating and designing 3D garments has become extremely demanding with the
increasing need for synthesizing realistic dressed persons for a variety of
applications, e.g. 3D virtual try-on, digitalization of 2D clothes into 3D
apparel, and cloth animation. It thus necessitates a simple and straightforward
pipeline to obtain high-quality texture from simple input, such as 2D reference
images. Since traditional warping-based texture generation methods require a
significant number of control points to be manually selected for each type of
garment, which can be a time-consuming and tedious process. We propose a novel
method, called Cloth2Tex, which eliminates the human burden in this process.
Cloth2Tex is a self-supervised method that generates texture maps with
reasonable layout and structural consistency. Another key feature of Cloth2Tex
is that it can be used to support high-fidelity texture inpainting. This is
done by combining Cloth2Tex with a prevailing latent diffusion model. We
evaluate our approach both qualitatively and quantitatively and demonstrate
that Cloth2Tex can generate high-quality texture maps and achieve the best
visual effects in comparison to other methods. Project page:
tomguluson92.github.io/projects/cloth2tex/
</p></li>
</ul>

<h3>Title: DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images. (arXiv:2308.04417v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04417">http://arxiv.org/abs/2308.04417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04417]] DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal from Optical Satellite Images(http://arxiv.org/abs/2308.04417)</code></li>
<li>Summary: <p>Optical satellite images are a critical data source; however, cloud cover
often compromises their quality, hindering image applications and analysis.
Consequently, effectively removing clouds from optical satellite images has
emerged as a prominent research direction. While recent advancements in cloud
removal primarily rely on generative adversarial networks, which may yield
suboptimal image quality, diffusion models have demonstrated remarkable success
in diverse image-generation tasks, showcasing their potential in addressing
this challenge. This paper presents a novel framework called DiffCR, which
leverages conditional guided diffusion with deep convolutional networks for
high-performance cloud removal for optical satellite imagery. Specifically, we
introduce a decoupled encoder for conditional image feature extraction,
providing a robust color representation to ensure the close similarity of
appearance information between the conditional input and the synthesized
output. Moreover, we propose a novel and efficient time and condition fusion
block within the cloud removal model to accurately simulate the correspondence
between the appearance in the conditional image and the target image at a low
computational cost. Extensive experimental evaluations on two commonly used
benchmark datasets demonstrate that DiffCR consistently achieves
state-of-the-art performance on all metrics, with parameter and computational
complexities amounting to only 5.1% and 5.4%, respectively, of those previous
best methods. The source code, pre-trained models, and all the experimental
results will be publicly available at https://github.com/XavierJiezou/DiffCR
upon the paper's acceptance of this work.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning. (arXiv:2308.03975v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03975">http://arxiv.org/abs/2308.03975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03975]] Prompted Contrast with Masked Motion Modeling: Towards Versatile 3D Action Representation Learning(http://arxiv.org/abs/2308.03975)</code></li>
<li>Summary: <p>Self-supervised learning has proved effective for skeleton-based human action
understanding, which is an important yet challenging topic. Previous works
mainly rely on contrastive learning or masked motion modeling paradigm to model
the skeleton relations. However, the sequence-level and joint-level
representation learning cannot be effectively and simultaneously handled by
these methods. As a result, the learned representations fail to generalize to
different downstream tasks. Moreover, combining these two paradigms in a naive
manner leaves the synergy between them untapped and can lead to interference in
training. To address these problems, we propose Prompted Contrast with Masked
Motion Modeling, PCM$^{\rm 3}$, for versatile 3D action representation
learning. Our method integrates the contrastive learning and masked prediction
tasks in a mutually beneficial manner, which substantially boosts the
generalization capacity for various downstream tasks. Specifically, masked
prediction provides novel training views for contrastive learning, which in
turn guides the masked prediction training with high-level semantic
information. Moreover, we propose a dual-prompted multi-task pretraining
strategy, which further improves model representations by reducing the
interference caused by learning the two different pretext tasks. Extensive
experiments on five downstream tasks under three large-scale datasets are
conducted, demonstrating the superior generalization capacity of PCM$^{\rm 3}$
compared to the state-of-the-art works. Our project is publicly available at:
https://jhang2020.github.io/Projects/PCM3/PCM3.html .
</p></li>
</ul>

<h3>Title: BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning. (arXiv:2308.04263v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04263">http://arxiv.org/abs/2308.04263</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04263]] BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning(http://arxiv.org/abs/2308.04263)</code></li>
<li>Summary: <p>This paper introduces BarlowRL, a data-efficient reinforcement learning agent
that combines the Barlow Twins self-supervised learning framework with DER
(Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its
contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids
dimensional collapse by enforcing information spread to the whole space. This
helps RL algorithms to utilize uniformly spread state representation that
eventually results in a remarkable performance. The integration of Barlow Twins
with DER enhances data efficiency and achieves superior performance in the RL
tasks. BarlowRL demonstrates the potential of incorporating self-supervised
learning techniques to improve RL algorithms.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions. (arXiv:2308.04152v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04152">http://arxiv.org/abs/2308.04152</a></li>
<li>Code URL: https://github.com/dcdmllm/cheetah</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04152]] Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions(http://arxiv.org/abs/2308.04152)</code></li>
<li>Summary: <p>Multimodal Large Language Models (MLLMs) have recently sparked significant
interest, which demonstrates emergent capabilities to serve as a
general-purpose model for various vision-language tasks. However, existing
methods mainly focus on limited types of instructions with a single image as
visual context, which hinders the widespread availability of MLLMs. In this
paper, we introduce the I4 benchmark to comprehensively evaluate the
instruction following ability on complicated interleaved vision-language
instructions, which involve intricate image-text sequential context, covering a
diverse range of scenarios (e.g., visually-rich webpages/textbooks, lecture
slides, embodied dialogue). Systematic evaluation on our I4 benchmark reveals a
common defect of existing methods: the Visual Prompt Generator (VPG) trained on
image-captioning alignment objective tends to attend to common foreground
information for captioning but struggles to extract specific information
required by particular tasks. To address this issue, we propose a generic and
lightweight controllable knowledge re-injection module, which utilizes the
sophisticated reasoning ability of LLMs to control the VPG to conditionally
extract instruction-specific visual information and re-inject it into the LLM.
Further, we introduce an annotation-free cross-attention guided counterfactual
image training strategy to methodically learn the proposed module by
collaborating a cascade of foundation models. Enhanced by the proposed module
and training strategy, we present Cheetah, a MLLM that can effectively handle a
wide variety of interleaved vision-language instructions and achieves
state-of-the-art zero-shot performance across all tasks of I4, without
high-quality multimodal instruction tuning data. Moreover, Cheetah also
exhibits competitive performance compared with state-of-the-art instruction
tuned models on concurrent MME benchmark.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings. (arXiv:2308.04052v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04052">http://arxiv.org/abs/2308.04052</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04052]] The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings(http://arxiv.org/abs/2308.04052)</code></li>
<li>Summary: <p>The five-dollar model is a lightweight text-to-image generative architecture
that generates low dimensional images from an encoded text prompt. This model
can successfully generate accurate and aesthetically pleasing content in low
dimensional domains, with limited amounts of training data. Despite the small
size of both the model and datasets, the generated images are still able to
maintain the encoded semantic meaning of the textual prompt. We apply this
model to three small datasets: pixel art video game maps, video game sprite
images, and down-scaled emoji images and apply novel augmentation strategies to
improve the performance of our model on these limited datasets. We evaluate our
models performance using cosine similarity score between text-image pairs
generated by the CLIP VIT-B/32 model.
</p></li>
</ul>

<h3>Title: From Unimodal to Multimodal: improving the sEMG-Based Pattern Recognition via deep generative models. (arXiv:2308.04091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04091">http://arxiv.org/abs/2308.04091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04091]] From Unimodal to Multimodal: improving the sEMG-Based Pattern Recognition via deep generative models(http://arxiv.org/abs/2308.04091)</code></li>
<li>Summary: <p>Multimodal hand gesture recognition (HGR) systems can achieve higher
recognition accuracy. However, acquiring multimodal gesture recognition data
typically requires users to wear additional sensors, thereby increasing
hardware costs. This paper proposes a novel generative approach to improve
Surface Electromyography (sEMG)-based HGR accuracy via virtual Inertial
Measurement Unit (IMU) signals. Specifically, we trained a deep generative
model based on the intrinsic correlation between forearm sEMG signals and
forearm IMU signals to generate virtual forearm IMU signals from the input
forearm sEMG signals at first. Subsequently, the sEMG signals and virtual IMU
signals were fed into a multimodal Convolutional Neural Network (CNN) model for
gesture recognition. To evaluate the performance of the proposed approach, we
conducted experiments on 6 databases, including 5 publicly available databases
and our collected database comprising 28 subjects performing 38 gestures,
containing both sEMG and IMU data. The results show that our proposed approach
outperforms the sEMG-based unimodal HGR method (with increases of
2.15%-13.10%). It demonstrates that incorporating virtual IMU signals,
generated by deep generative models, can significantly enhance the accuracy of
sEMG-based HGR. The proposed approach represents a successful attempt to
transition from unimodal HGR to multimodal HGR without additional sensor
hardware.
</p></li>
</ul>

<h3>Title: Vision-Based Autonomous Navigation for Unmanned Surface Vessel in Extreme Marine Conditions. (arXiv:2308.04283v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04283">http://arxiv.org/abs/2308.04283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04283]] Vision-Based Autonomous Navigation for Unmanned Surface Vessel in Extreme Marine Conditions(http://arxiv.org/abs/2308.04283)</code></li>
<li>Summary: <p>Visual perception is an important component for autonomous navigation of
unmanned surface vessels (USV), particularly for the tasks related to
autonomous inspection and tracking. These tasks involve vision-based navigation
techniques to identify the target for navigation. Reduced visibility under
extreme weather conditions in marine environments makes it difficult for
vision-based approaches to work properly. To overcome these issues, this paper
presents an autonomous vision-based navigation framework for tracking target
objects in extreme marine conditions. The proposed framework consists of an
integrated perception pipeline that uses a generative adversarial network (GAN)
to remove noise and highlight the object features before passing them to the
object detector (i.e., YOLOv5). The detected visual features are then used by
the USV to track the target. The proposed framework has been thoroughly tested
in simulation under extremely reduced visibility due to sandstorms and fog. The
results are compared with state-of-the-art de-hazing methods across the
benchmarked MBZIRC simulation dataset, on which the proposed scheme has
outperformed the existing methods across various metrics.
</p></li>
</ul>

<h3>Title: Domain Adaptive Person Search via GAN-based Scene Synthesis for Cross-scene Videos. (arXiv:2308.04322v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04322">http://arxiv.org/abs/2308.04322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04322]] Domain Adaptive Person Search via GAN-based Scene Synthesis for Cross-scene Videos(http://arxiv.org/abs/2308.04322)</code></li>
<li>Summary: <p>Person search has recently been a challenging task in the computer vision
domain, which aims to search specific pedestrians from real
cameras.Nevertheless, most surveillance videos comprise only a handful of
images of each pedestrian, which often feature identical backgrounds and
clothing. Hence, it is difficult to learn more discriminative features for
person search in real scenes. To tackle this challenge, we draw on Generative
Adversarial Networks (GAN) to synthesize data from surveillance videos. GAN has
thrived in computer vision problems because it produces high-quality images
efficiently. We merely alter the popular Fast R-CNN model, which is capable of
processing videos and yielding accurate detection outcomes. In order to
appropriately relieve the pressure brought by the two-stage model, we design an
Assisted-Identity Query Module (AIDQ) to provide positive images for the behind
part. Besides, the proposed novel GAN-based Scene Synthesis model that can
synthesize high-quality cross-id person images for person search tasks. In
order to facilitate the feature learning of the GAN-based Scene Synthesis
model, we adopt an online learning strategy that collaboratively learns the
synthesized images and original images. Extensive experiments on two widely
used person search benchmarks, CUHK-SYSU and PRW, have shown that our method
has achieved great performance, and the extensive ablation study further
justifies our GAN-synthetic data can effectively increase the variability of
the datasets and be more realistic.
</p></li>
</ul>

<h3>Title: A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces. (arXiv:2308.04426v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04426">http://arxiv.org/abs/2308.04426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04426]] A Deep-Learning Method Using Auto-encoder and Generative Adversarial Network for Anomaly Detection on Ancient Stone Stele Surfaces(http://arxiv.org/abs/2308.04426)</code></li>
<li>Summary: <p>Accurate detection of natural deterioration and man-made damage on the
surfaces of ancient stele in the first instance is essential for their
preventive conservation. Existing methods for cultural heritage preservation
are not able to achieve this goal perfectly due to the difficulty of balancing
accuracy, efficiency, timeliness, and cost. This paper presents a deep-learning
method to automatically detect above mentioned emergencies on ancient stone
stele in real time, employing autoencoder (AE) and generative adversarial
network (GAN). The proposed method overcomes the limitations of existing
methods by requiring no extensive anomaly samples while enabling comprehensive
detection of unpredictable anomalies. the method includes stages of monitoring,
data acquisition, pre-processing, model structuring, and post-processing.
Taking the Longmen Grottoes' stone steles as a case study, an unsupervised
learning model based on AE and GAN architectures is proposed and validated with
a reconstruction accuracy of 99.74\%. The method's evaluation revealed the
proficient detection of seven artificially designed anomalies and demonstrated
precision and reliability without false alarms. This research provides novel
ideas and possibilities for the application of deep learning in the field of
cultural heritage.
</p></li>
</ul>

<h3>Title: SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool. (arXiv:2308.03983v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03983">http://arxiv.org/abs/2308.03983</a></li>
<li>Code URL: https://github.com/rcgai/simplyretrieve</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03983]] SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool(http://arxiv.org/abs/2308.03983)</code></li>
<li>Summary: <p>Large Language Model (LLM) based Generative AI systems have seen significant
progress in recent years. Integrating a knowledge retrieval architecture allows
for seamless integration of private data into publicly available Generative AI
systems using pre-trained LLM without requiring additional model fine-tuning.
Moreover, Retrieval-Centric Generation (RCG) approach, a promising future
research direction that explicitly separates roles of LLMs and retrievers in
context interpretation and knowledge memorization, potentially leads to more
efficient implementation. SimplyRetrieve is an open-source tool with the goal
of providing a localized, lightweight, and user-friendly interface to these
sophisticated advancements to the machine learning community. SimplyRetrieve
features a GUI and API based RCG platform, assisted by a Private Knowledge Base
Constructor and a Retrieval Tuning Module. By leveraging these capabilities,
users can explore the potential of RCG for improving generative AI performance
while maintaining privacy standards. The tool is available at
https://github.com/RCGAI/SimplyRetrieve with an MIT license.
</p></li>
</ul>

<h3>Title: Goodness-of-Fit of Attributed Probabilistic Graph Generative Models. (arXiv:2308.03773v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03773">http://arxiv.org/abs/2308.03773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03773]] Goodness-of-Fit of Attributed Probabilistic Graph Generative Models(http://arxiv.org/abs/2308.03773)</code></li>
<li>Summary: <p>Probabilistic generative models of graphs are important tools that enable
representation and sampling. Many recent works have created probabilistic
models of graphs that are capable of representing not only entity interactions
but also their attributes. However, given a generative model of random
attributed graph(s), the general conditions that establish goodness of fit are
not clear a-priori. In this paper, we define goodness of fit in terms of the
mean square contingency coefficient for random binary networks. For this
statistic, we outline a procedure for assessing the quality of the structure of
a learned attributed graph by ensuring that the discrepancy of the mean square
contingency coefficient (constant, or random) is minimal with high probability.
We apply these criteria to verify the representation capability of a
probabilistic generative model for various popular types of graph models.
</p></li>
</ul>

<h3>Title: PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning. (arXiv:2308.03953v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03953">http://arxiv.org/abs/2308.03953</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03953]] PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning(http://arxiv.org/abs/2308.03953)</code></li>
<li>Summary: <p>Deep learning has emerged as an effective solution for addressing the
challenges of short-term voltage stability assessment (STVSA) in power systems.
However, existing deep learning-based STVSA approaches face limitations in
adapting to topological changes, sample labeling, and handling small datasets.
To overcome these challenges, this paper proposes a novel phasor measurement
unit (PMU) measurements-based STVSA method by using deep transfer learning. The
method leverages the real-time dynamic information captured by PMUs to create
an initial dataset. It employs temporal ensembling for sample labeling and
utilizes least squares generative adversarial networks (LSGAN) for data
augmentation, enabling effective deep learning on small-scale datasets.
Additionally, the method enhances adaptability to topological changes by
exploring connections between different faults. Experimental results on the
IEEE 39-bus test system demonstrate that the proposed method improves model
evaluation accuracy by approximately 20% through transfer learning, exhibiting
strong adaptability to topological changes. Leveraging the self-attention
mechanism of the Transformer model, this approach offers significant advantages
over shallow learning methods and other deep learning-based approaches.
</p></li>
</ul>

<h3>Title: Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models. (arXiv:2308.03960v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.03960">http://arxiv.org/abs/2308.03960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.03960]] Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models(http://arxiv.org/abs/2308.03960)</code></li>
<li>Summary: <p>Preliminary trajectory design is a global search problem that seeks multiple
qualitatively different solutions to a trajectory optimization problem. Due to
its high dimensionality and non-convexity, and the frequent adjustment of
problem parameters, the global search becomes computationally demanding. In
this paper, we exploit the clustering structure in the solutions and propose an
amortized global search (AmorGS) framework. We use deep generative models to
predict trajectory solutions that share similar structures with previously
solved problems, which accelerates the global search for unseen parameter
values. Our method is evaluated using De Jong's 5th function and a low-thrust
circular restricted three-body problem.
</p></li>
</ul>

<h2>anomaly</h2>
<h2>in-context</h2>
<h3>Title: Large Language Model Prompt Chaining for Long Legal Document Classification. (arXiv:2308.04138v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04138">http://arxiv.org/abs/2308.04138</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04138]] Large Language Model Prompt Chaining for Long Legal Document Classification(http://arxiv.org/abs/2308.04138)</code></li>
<li>Summary: <p>Prompting is used to guide or steer a language model in generating an
appropriate response that is consistent with the desired outcome. Chaining is a
strategy used to decompose complex tasks into smaller, manageable components.
In this study, we utilize prompt chaining for extensive legal document
classification tasks, which present difficulties due to their intricate
domain-specific language and considerable length. Our approach begins with the
creation of a concise summary of the original document, followed by a semantic
search for related exemplar texts and their corresponding annotations from a
training corpus. Finally, we prompt for a label - based on the task - to
assign, by leveraging the in-context learning from the few-shot prompt. We
demonstrate that through prompt chaining, we can not only enhance the
performance over zero-shot, but also surpass the micro-F1 score achieved by
larger models, such as ChatGPT zero-shot, using smaller models.
</p></li>
</ul>

<h3>Title: In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning. (arXiv:2308.04275v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04275">http://arxiv.org/abs/2308.04275</a></li>
<li>Code URL: https://github.com/xhan77/in-context-alignment</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04275]] In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning(http://arxiv.org/abs/2308.04275)</code></li>
<li>Summary: <p>In this note, we explore inference-time alignment through in-context
learning. We consider a vanilla pretrained language model Llama-2 before any
fine-tuning and retrieve an average of 9 demonstration alignment examples when
the model is prompted to follow chat-style instructions. Compared to direct
prompting, the in-context alignment without changing model weights leads to a
7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making
the vanilla language model comparable to strong baselines with alignment
fine-tuning.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
