<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-31</h1>
<h3>Title: IPGO: Indirect Prompt Gradient Optimization on Text-to-Image Generative Models with High Data Efficiency</h3>
<ul>
<li><strong>Authors: </strong>Jianping Ye, Michel Wedel, Kunpeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21812">https://arxiv.org/abs/2503.21812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21812">https://arxiv.org/pdf/2503.21812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21812]] IPGO: Indirect Prompt Gradient Optimization on Text-to-Image Generative Models with High Data Efficiency(https://arxiv.org/abs/2503.21812)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-Image Diffusion models excel at generating images from text prompts but often lack optimal alignment with content semantics, aesthetics, and human preferences. To address these issues, in this study we introduce a novel framework, Indirect Prompt Gradient Optimization (IPGO), for prompt-level fine-tuning. IPGO enhances prompt embeddings by injecting continuously differentiable tokens at the beginning and end of the prompt embeddings, while exploiting low-rank benefits and flexibility from rotations. It allows for gradient-based optimization of injected tokens while enforcing value, orthonormality, and conformity constraints, facilitating continuous updates and empowering computational efficiency. To evaluate the performance of IPGO, we conduct prompt-wise and prompt-batch training with three reward models targeting image aesthetics, image-text alignment, and human preferences under three datasets of different complexity. The results show that IPGO consistently matches or outperforms cutting-edge benchmarks, including stable diffusion v1.5 with raw prompts, training-based approaches (DRaFT and DDPO), and training-free methods (DPO-Diffusion, Promptist, and ChatGPT-4o). Furthermore, we demonstrate IPGO's effectiveness in enhancing image generation quality while requiring minimal training data and limited computational resources.</li>
</ul>

<h3>Title: Low-Rank Adaptation of Pre-Trained Stable Diffusion for Rigid-Body Target ISAR Imaging</h3>
<ul>
<li><strong>Authors: </strong>Boan Zhang, Hang Dong, Jiongge Zhang, Long Tian, Rongrong Wang, Zhenhua Wu, Xiyang Liu, Hongwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21823">https://arxiv.org/abs/2503.21823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21823">https://arxiv.org/pdf/2503.21823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21823]] Low-Rank Adaptation of Pre-Trained Stable Diffusion for Rigid-Body Target ISAR Imaging(https://arxiv.org/abs/2503.21823)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Traditional range-instantaneous Doppler (RID) methods for rigid-body target imaging often suffer from low resolution due to the limitations of time-frequency analysis (TFA). To address this challenge, our primary focus is on obtaining high resolution time-frequency representations (TFRs) from their low resolution counterparts. Recognizing that the curve features of TFRs are a specific type of texture feature, we argue that pre trained generative models such as Stable Diffusion (SD) are well suited for enhancing TFRs, thanks to their powerful capability in capturing texture representations. Building on this insight, we propose a novel inverse synthetic aperture radar (ISAR) imaging method for rigid-body targets, leveraging the low-rank adaptation (LoRA) of a pre-trained SD model. Our approach adopts the basic structure and pre-trained parameters of SD Turbo while incorporating additional linear operations for LoRA and adversarial training to achieve super-resolution and noise suppression. Then we integrate LoRA-SD into the RID-based ISAR imaging, enabling sharply focused and denoised imaging with super-resolution capabilities. We evaluate our method using both simulated and real radar data. The experimental results demonstrate the superiority of our approach in frequency es timation and ISAR imaging compared to traditional methods. Notably, the generalization capability is verified by training on simulated radar data and testing on measured radar data.</li>
</ul>

<h3>Title: Shape Generation via Weight Space Learning</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Plattner, Arturs Berzins, Johannes Brandstetter</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21830">https://arxiv.org/abs/2503.21830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21830">https://arxiv.org/pdf/2503.21830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21830]] Shape Generation via Weight Space Learning(https://arxiv.org/abs/2503.21830)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Foundation models for 3D shape generation have recently shown a remarkable capacity to encode rich geometric priors across both global and local dimensions. However, leveraging these priors for downstream tasks can be challenging as real-world data are often scarce or noisy, and traditional fine-tuning can lead to catastrophic forgetting. In this work, we treat the weight space of a large 3D shape-generative model as a data modality that can be explored directly. We hypothesize that submanifolds within this high-dimensional weight space can modulate topological properties or fine-grained part features separately, demonstrating early-stage evidence via two experiments. First, we observe a sharp phase transition in global connectivity when interpolating in conditioning space, suggesting that small changes in weight space can drastically alter topology. Second, we show that low-dimensional reparameterizations yield controlled local geometry changes even with very limited data. These results highlight the potential of weight space learning to unlock new approaches for 3D shape generation and specialized fine-tuning.</li>
</ul>

<h3>Title: Refining Time Series Anomaly Detectors using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alan Yang, Yulin Chen, Sean Lee, Venus Montes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21833">https://arxiv.org/abs/2503.21833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21833">https://arxiv.org/pdf/2503.21833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21833]] Refining Time Series Anomaly Detectors using Large Language Models(https://arxiv.org/abs/2503.21833)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) is of widespread interest across many industries, including finance, healthcare, and manufacturing. Despite the development of numerous automatic methods for detecting anomalies, human oversight remains necessary to review and act upon detected anomalies, as well as verify their accuracy. We study the use of multimodal large language models (LLMs) to partially automate this process. We find that LLMs can effectively identify false alarms by integrating visual inspection of time series plots with text descriptions of the data-generating process. By leveraging the capabilities of LLMs, we aim to reduce the reliance on human effort required to maintain a TSAD system</li>
</ul>

<h3>Title: iMedImage Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Ran Wei, ZhiXiong Lan, Qing Yan, Ning Song, Ming Lv, LongQing Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21836">https://arxiv.org/abs/2503.21836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21836">https://arxiv.org/pdf/2503.21836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21836]] iMedImage Technical Report(https://arxiv.org/abs/2503.21836)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Background: Chromosome karyotype analysis is crucial for diagnosing hereditary diseases, yet detecting structural abnormalities remains challenging. While AI has shown promise in medical imaging, its effectiveness varies across modalities. Leveraging advances in Foundation Models that integrate multimodal medical imaging for robust feature extraction and accurate diagnosis, we developed iMedImage, an end-to-end model for general medical image recognition, demonstrating strong performance across multiple imaging tasks, including chromosome abnormality detection. Materials and Methods: We constructed a comprehensive medical image dataset encompassing multiple modalities from common medical domains, including chromosome, cell, pathology, ultrasound, X-ray, CT, and MRI images. Based on this dataset, we developed the iMedImage model, which incorporates the following key features: (1) a unified representation method for diverse modality inputs and medical imaging tasks; (2) multi-level (case-level, image-level, patch-level) image recognition capabilities enhanced by Chain of Thought (CoT) embedding and Mixture of Experts (MoE) strategies. Results: The test set comprised data from 12 institutions across six regions in China, covering three mainstream scanning devices, and included naturally distributed, unscreened abnormal cases. On this diverse dataset, the model achieved a fully automated chromosome analysis workflow, including segmentation, karyotyping, and abnormality detection, reaching a sensitivity of 92.75% and a specificity of 91.5%. Conclusion: We propose iMedImage, an end-to-end foundation model for medical image analysis, demonstrating its superior performance across various medical imaging tasks. iMedImage provides clinicians with a precise imaging analysis tool and contributes to improving diagnostic accuracy and disease screening.</li>
</ul>

<h3>Title: HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery</h3>
<ul>
<li><strong>Authors: </strong>Jingtao Li, Yingyi Liu, Xinyu Wang, Yunning Peng, Chen Sun, Shaoyu Wang, Zhendong Sun, Tian Ke, Xiao Jiang, Tangwei Lu, Anran Zhao, Yanfei Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21841">https://arxiv.org/abs/2503.21841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21841">https://arxiv.org/pdf/2503.21841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21841]] HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery(https://arxiv.org/abs/2503.21841)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Advanced interpretation of hyperspectral remote sensing images benefits many precise Earth observation tasks. Recently, visual foundation models have promoted the remote sensing interpretation but concentrating on RGB and multispectral images. Due to the varied hyperspectral channels,existing foundation models would face image-by-image tuning situation, imposing great pressure on hardware and time resources. In this paper, we propose a tuning-free hyperspectral foundation model called HyperFree, by adapting the existing visual prompt engineering. To process varied channel numbers, we design a learned weight dictionary covering full-spectrum from $0.4 \sim 2.5 \, \mu\text{m}$, supporting to build the embedding layer dynamically. To make the prompt design more tractable, HyperFree can generate multiple semantic-aware masks for one prompt by treating feature distance as semantic-similarity. After pre-training HyperFree on constructed large-scale high-resolution hyperspectral images, HyperFree (1 prompt) has shown comparable results with specialized models (5 shots) on 5 tasks and 11 this http URL and dataset are accessible at this https URL.</li>
</ul>

<h3>Title: StarFlow: Generating Structured Workflow Outputs From Sketch Images</h3>
<ul>
<li><strong>Authors: </strong>Patrice Bechard, Chao Wang, Amirhossein Abaskohi, Juan Rodriguez, Christopher Pal, David Vazquez, Spandana Gella, Sai Rajeswar, Perouz Taslakian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21889">https://arxiv.org/abs/2503.21889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21889">https://arxiv.org/pdf/2503.21889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21889]] StarFlow: Generating Structured Workflow Outputs From Sketch Images(https://arxiv.org/abs/2503.21889)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Workflows are a fundamental component of automation in enterprise platforms, enabling the orchestration of tasks, data processing, and system integrations. Despite being widely used, building workflows can be complex, often requiring manual configuration through low-code platforms or visual programming tools. To simplify this process, we explore the use of generative foundation models, particularly vision-language models (VLMs), to automatically generate structured workflows from visual inputs. Translating hand-drawn sketches or computer-generated diagrams into executable workflows is challenging due to the ambiguity of free-form drawings, variations in diagram styles, and the difficulty of inferring execution logic from visual elements. To address this, we introduce StarFlow, a framework for generating structured workflow outputs from sketches using vision-language models. We curate a diverse dataset of workflow diagrams -- including synthetic, manually annotated, and real-world samples -- to enable robust training and evaluation. We finetune and benchmark multiple vision-language models, conducting a series of ablation studies to analyze the strengths and limitations of our approach. Our results show that finetuning significantly enhances structured workflow generation, outperforming large vision-language models on this task.</li>
</ul>

<h3>Title: AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Yang, Chen Gao, Jing Liu, Peng Wu, Guansong Pang, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21904">https://arxiv.org/abs/2503.21904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21904">https://arxiv.org/pdf/2503.21904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21904]] AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis(https://arxiv.org/abs/2503.21904)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The rapid advancements in large language models (LLMs) have spurred growing interest in LLM-based video anomaly detection (VAD). However, existing approaches predominantly focus on video-level anomaly question answering or offline detection, ignoring the real-time nature essential for practical VAD applications. To bridge this gap and facilitate the practical deployment of LLM-based VAD, we introduce AssistPDA, the first online video anomaly surveillance assistant that unifies video anomaly prediction, detection, and analysis (VAPDA) within a single framework. AssistPDA enables real-time inference on streaming videos while supporting interactive user engagement. Notably, we introduce a novel event-level anomaly prediction task, enabling proactive anomaly forecasting before anomalies fully unfold. To enhance the ability to model intricate spatiotemporal relationships in anomaly events, we propose a Spatio-Temporal Relation Distillation (STRD) module. STRD transfers the long-term spatiotemporal modeling capabilities of vision-language models (VLMs) from offline settings to real-time scenarios. Thus it equips AssistPDA with a robust understanding of complex temporal dependencies and long-sequence memory. Additionally, we construct VAPDA-127K, the first large-scale benchmark designed for VLM-based online VAPDA. Extensive experiments demonstrate that AssistPDA outperforms existing offline VLM-based approaches, setting a new state-of-the-art for real-time VAPDA. Our dataset and code will be open-sourced to facilitate further research in the community.</li>
</ul>

<h3>Title: KernelFusion: Assumption-Free Blind Super-Resolution via Patch Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Oliver Heinimann, Assaf Shocher, Tal Zimbalist, Michal Irani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21907">https://arxiv.org/abs/2503.21907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21907">https://arxiv.org/pdf/2503.21907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21907]] KernelFusion: Assumption-Free Blind Super-Resolution via Patch Diffusion(https://arxiv.org/abs/2503.21907)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Traditional super-resolution (SR) methods assume an ``ideal'' downscaling SR-kernel (e.g., bicubic downscaling) between the high-resolution (HR) image and the low-resolution (LR) image. Such methods fail once the LR images are generated differently. Current blind-SR methods aim to remove this assumption, but are still fundamentally restricted to rather simplistic downscaling SR-kernels (e.g., anisotropic Gaussian kernels), and fail on more complex (out of distribution) downscaling degradations. However, using the correct SR-kernel is often more important than using a sophisticated SR algorithm. In ``KernelFusion'' we introduce a zero-shot diffusion-based method that makes no assumptions about the kernel. Our method recovers the unique image-specific SR-kernel directly from the LR input image, while simultaneously recovering its corresponding HR image. KernelFusion exploits the principle that the correct SR-kernel is the one that maximizes patch similarity across different scales of the LR image. We first train an image-specific patch-based diffusion model on the single LR input image, capturing its unique internal patch statistics. We then reconstruct a larger HR image with the same learned patch distribution, while simultaneously recovering the correct downscaling SR-kernel that maintains this cross-scale relation between the HR and LR images. Empirical results show that KernelFusion vastly outperforms all SR baselines on complex downscaling degradations, where existing SotA Blind-SR methods fail miserably. By breaking free from predefined kernel assumptions, KernelFusion pushes Blind-SR into a new assumption-free paradigm, handling downscaling kernels previously thought impossible.</li>
</ul>

<h3>Title: Multimodal Data Integration for Sustainable Indoor Gardening: Tracking Anyplant with Time Series Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Seyed Hamidreza Nabaei, Zeyang Zheng, Dong Chen, Arsalan Heydarian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21932">https://arxiv.org/abs/2503.21932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21932">https://arxiv.org/pdf/2503.21932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21932]] Multimodal Data Integration for Sustainable Indoor Gardening: Tracking Anyplant with Time Series Foundation Model(https://arxiv.org/abs/2503.21932)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Indoor gardening within sustainable buildings offers a transformative solution to urban food security and environmental sustainability. By 2030, urban farming, including Controlled Environment Agriculture (CEA) and vertical farming, is expected to grow at a compound annual growth rate (CAGR) of 13.2% from 2024 to 2030, according to market reports. This growth is fueled by advancements in Internet of Things (IoT) technologies, sustainable innovations such as smart growing systems, and the rising interest in green interior design. This paper presents a novel framework that integrates computer vision, machine learning (ML), and environmental sensing for the automated monitoring of plant health and growth. Unlike previous approaches, this framework combines RGB imagery, plant phenotyping data, and environmental factors such as temperature and humidity, to predict plant water stress in a controlled growth environment. The system utilizes high-resolution cameras to extract phenotypic features, such as RGB, plant area, height, and width while employing the Lag-Llama time series model to analyze and predict water stress. Experimental results demonstrate that integrating RGB, size ratios, and environmental data significantly enhances predictive accuracy, with the Fine-tuned model achieving the lowest errors (MSE = 0.420777, MAE = 0.595428) and reduced uncertainty. These findings highlight the potential of multimodal data and intelligent systems to automate plant care, optimize resource consumption, and align indoor gardening with sustainable building management practices, paving the way for resilient, green urban spaces.</li>
</ul>

<h3>Title: Parametric Shadow Control for Portrait Generationin Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haoming Cai, Tsung-Wei Huang, Shiv Gehlot, Brandon Y. Feng, Sachin Shah, Guan-Ming Su, Christopher Metzler</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21943">https://arxiv.org/abs/2503.21943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21943">https://arxiv.org/pdf/2503.21943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21943]] Parametric Shadow Control for Portrait Generationin Text-to-Image Diffusion Models(https://arxiv.org/abs/2503.21943)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models excel at generating diverse portraits, but lack intuitive shadow control. Existing editing approaches, as post-processing, struggle to offer effective manipulation across diverse styles. Additionally, these methods either rely on expensive real-world light-stage data collection or require extensive computational resources for training. To address these limitations, we introduce Shadow Director, a method that extracts and manipulates hidden shadow attributes within well-trained diffusion models. Our approach uses a small estimation network that requires only a few thousand synthetic images and hours of training-no costly real-world light-stage data needed. Shadow Director enables parametric and intuitive control over shadow shape, placement, and intensity during portrait generation while preserving artistic integrity and identity across diverse styles. Despite training only on synthetic data built on real-world identities, it generalizes effectively to generated portraits with diverse styles, making it a more accessible and resource-friendly solution.</li>
</ul>

<h3>Title: Improving Equivariant Networks with Probabilistic Symmetry Breaking</h3>
<ul>
<li><strong>Authors: </strong>Hannah Lawrence, Vasco Portilheiro, Yan Zhang, SÃ©kou-Oumar Kaba</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21985">https://arxiv.org/abs/2503.21985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21985">https://arxiv.org/pdf/2503.21985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21985]] Improving Equivariant Networks with Probabilistic Symmetry Breaking(https://arxiv.org/abs/2503.21985)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Equivariance encodes known symmetries into neural networks, often enhancing generalization. However, equivariant networks cannot break symmetries: the output of an equivariant network must, by definition, have at least the same self-symmetries as the input. This poses an important problem, both (1) for prediction tasks on domains where self-symmetries are common, and (2) for generative models, which must break symmetries in order to reconstruct from highly symmetric latent spaces. This fundamental limitation can be addressed by considering equivariant conditional distributions, instead of equivariant functions. We present novel theoretical results that establish necessary and sufficient conditions for representing such distributions. Concretely, this representation provides a practical framework for breaking symmetries in any equivariant network via randomized canonicalization. Our method, SymPE (Symmetry-breaking Positional Encodings), admits a simple interpretation in terms of positional encodings. This approach expands the representational power of equivariant networks while retaining the inductive bias of symmetry, which we justify through generalization bounds. Experimental results demonstrate that SymPE significantly improves performance of group-equivariant and graph neural networks across diffusion models for graphs, graph autoencoders, and lattice spin system modeling.</li>
</ul>

<h3>Title: BOOTPLACE: Bootstrapped Object Placement with Detection Transformers</h3>
<ul>
<li><strong>Authors: </strong>Hang Zhou, Xinxin Zuo, Rui Ma, Li Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.21991">https://arxiv.org/abs/2503.21991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.21991">https://arxiv.org/pdf/2503.21991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.21991]] BOOTPLACE: Bootstrapped Object Placement with Detection Transformers(https://arxiv.org/abs/2503.21991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we tackle the copy-paste image-to-image composition problem with a focus on object placement learning. Prior methods have leveraged generative models to reduce the reliance for dense supervision. However, this often limits their capacity to model complex data distributions. Alternatively, transformer networks with a sparse contrastive loss have been explored, but their over-relaxed regularization often leads to imprecise object placement. We introduce BOOTPLACE, a novel paradigm that formulates object placement as a placement-by-detection problem. Our approach begins by identifying suitable regions of interest for object placement. This is achieved by training a specialized detection transformer on object-subtracted backgrounds, enhanced with multi-object supervisions. It then semantically associates each target compositing object with detected regions based on their complementary characteristics. Through a boostrapped training approach applied to randomly object-subtracted images, our model enforces meaningful placements through extensive paired data augmentation. Experimental results on established benchmarks demonstrate BOOTPLACE's superior performance in object repositioning, markedly surpassing state-of-the-art baselines on Cityscapes and OPA datasets with notable improvements in IOU scores. Additional ablation studies further showcase the compositionality and generalizability of our approach, supported by user study evaluations.</li>
</ul>

<h3>Title: Monte Carlo Sampling for Analyzing In-Context Examples</h3>
<ul>
<li><strong>Authors: </strong>Stephanie Schoch, Yangfeng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22002">https://arxiv.org/abs/2503.22002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22002">https://arxiv.org/pdf/2503.22002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22002]] Monte Carlo Sampling for Analyzing In-Context Examples(https://arxiv.org/abs/2503.22002)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Prior works have shown that in-context learning is brittle to presentation factors such as the order, number, and choice of selected examples. However, ablation-based guidance on selecting the number of examples may ignore the interplay between different presentation factors. In this work we develop a Monte Carlo sampling-based method to study the impact of number of examples while explicitly accounting for effects from order and selected examples. We find that previous guidance on how many in-context examples to select does not always generalize across different sets of selected examples and orderings, and whether one-shot settings outperform zero-shot settings is highly dependent on the selected example. Additionally, inspired by data valuation, we apply our sampling method to in-context example selection to select examples that perform well across different orderings. We find a negative result, that while performance is robust to ordering and number of examples, there is an unexpected performance degradation compared to random sampling.</li>
</ul>

<h3>Title: AGILE: A Diffusion-Based Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification</h3>
<ul>
<li><strong>Authors: </strong>Earl Ranario, Lars Lundqvist, Heesup Yun, Brian N. Bailey, J. Mason Earles</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22019">https://arxiv.org/abs/2503.22019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22019">https://arxiv.org/pdf/2503.22019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22019]] AGILE: A Diffusion-Based Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification(https://arxiv.org/abs/2503.22019)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Semantically consistent cross-domain image translation facilitates the generation of training data by transferring labels across different domains, making it particularly useful for plant trait identification in agriculture. However, existing generative models struggle to maintain object-level accuracy when translating images between domains, especially when domain gaps are significant. In this work, we introduce AGILE (Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification), a diffusion-based framework that leverages optimized text embeddings and attention guidance to semantically constrain image translation. AGILE utilizes pretrained diffusion models and publicly available agricultural datasets to improve the fidelity of translated images while preserving critical object semantics. Our approach optimizes text embeddings to strengthen the correspondence between source and target images and guides attention maps during the denoising process to control object placement. We evaluate AGILE on cross-domain plant datasets and demonstrate its effectiveness in generating semantically accurate translated images. Quantitative experiments show that AGILE enhances object detection performance in the target domain while maintaining realism and consistency. Compared to prior image translation methods, AGILE achieves superior semantic alignment, particularly in challenging cases where objects vary significantly or domain gaps are substantial.</li>
</ul>

<h3>Title: The Risks of Using Large Language Models for Text Annotation in Social Science Research</h3>
<ul>
<li><strong>Authors: </strong>Hao Lin, Yongjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22040">https://arxiv.org/abs/2503.22040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22040">https://arxiv.org/pdf/2503.22040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22040]] The Risks of Using Large Language Models for Text Annotation in Social Science Research(https://arxiv.org/abs/2503.22040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (GenAI) or large language models (LLMs) have the potential to revolutionize computational social science, particularly in automated textual analysis. In this paper, we conduct a systematic evaluation of the promises and risks of using LLMs for diverse coding tasks, with social movement studies serving as a case example. We propose a framework for social scientists to incorporate LLMs into text annotation, either as the primary coding decision-maker or as a coding assistant. This framework provides tools for researchers to develop the optimal prompt, and to examine and report the validity and reliability of LLMs as a methodological tool. Additionally, we discuss the associated epistemic risks related to validity, reliability, replicability, and transparency. We conclude with several practical guidelines for using LLMs in text annotation tasks, and how we can better communicate the epistemic risks in research.</li>
</ul>

<h3>Title: Arch-LLM: Taming LLMs for Neural Architecture Generation via Unsupervised Discrete Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Deshani Geethika Poddenige, Sachith Seneviratne, Damith Senanayake, Mahesan Niranjan, PN Suganthan, Saman Halgamuge</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22063">https://arxiv.org/abs/2503.22063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22063">https://arxiv.org/pdf/2503.22063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22063]] Arch-LLM: Taming LLMs for Neural Architecture Generation via Unsupervised Discrete Representation Learning(https://arxiv.org/abs/2503.22063)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Unsupervised representation learning has been widely explored across various modalities, including neural architectures, where it plays a key role in downstream applications like Neural Architecture Search (NAS). These methods typically learn an unsupervised representation space before generating/ sampling architectures for the downstream search. A common approach involves the use of Variational Autoencoders (VAEs) to map discrete architectures onto a continuous representation space, however, sampling from these spaces often leads to a high percentage of invalid or duplicate neural architectures. This could be due to the unnatural mapping of inherently discrete architectural space onto a continuous space, which emphasizes the need for a robust discrete representation of these architectures. To address this, we introduce a Vector Quantized Variational Autoencoder (VQ-VAE) to learn a discrete latent space more naturally aligned with the discrete neural architectures. In contrast to VAEs, VQ-VAEs (i) map each architecture into a discrete code sequence and (ii) allow the prior to be learned by any generative model rather than assuming a normal distribution. We then represent these architecture latent codes as numerical sequences and train a text-to-text model leveraging a Large Language Model to learn and generate sequences representing architectures. We experiment our method with Inception/ ResNet-like cell-based search spaces, namely NAS-Bench-101 and NAS-Bench-201. Compared to VAE-based methods, our approach improves the generation of valid and unique architectures by over 80% on NASBench-101 and over 8% on NASBench-201. Finally, we demonstrate the applicability of our method in NAS employing a sequence-modeling-based NAS algorithm.</li>
</ul>

<h3>Title: A Survey on Remote Sensing Foundation Models: From Vision to Multimodality</h3>
<ul>
<li><strong>Authors: </strong>Ziyue Huang, Hongxi Yan, Qiqi Zhan, Shuai Yang, Mingming Zhang, Chenkai Zhang, YiMing Lei, Zeming Liu, Qingjie Liu, Yunhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22081">https://arxiv.org/abs/2503.22081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22081">https://arxiv.org/pdf/2503.22081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22081]] A Survey on Remote Sensing Foundation Models: From Vision to Multimodality(https://arxiv.org/abs/2503.22081)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of remote sensing foundation models, particularly vision and multimodal models, has significantly enhanced the capabilities of intelligent geospatial data interpretation. These models combine various data modalities, such as optical, radar, and LiDAR imagery, with textual and geographic information, enabling more comprehensive analysis and understanding of remote sensing data. The integration of multiple modalities allows for improved performance in tasks like object detection, land cover classification, and change detection, which are often challenged by the complex and heterogeneous nature of remote sensing data. However, despite these advancements, several challenges remain. The diversity in data types, the need for large-scale annotated datasets, and the complexity of multimodal fusion techniques pose significant obstacles to the effective deployment of these models. Moreover, the computational demands of training and fine-tuning multimodal models require significant resources, further complicating their practical application in remote sensing image interpretation tasks. This paper provides a comprehensive review of the state-of-the-art in vision and multimodal foundation models for remote sensing, focusing on their architecture, training methods, datasets and application scenarios. We discuss the key challenges these models face, such as data alignment, cross-modal transfer learning, and scalability, while also identifying emerging research directions aimed at overcoming these limitations. Our goal is to provide a clear understanding of the current landscape of remote sensing foundation models and inspire future research that can push the boundaries of what these models can achieve in real-world applications. The list of resources collected by the paper can be found in the this https URL.</li>
</ul>

<h3>Title: Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations</h3>
<ul>
<li><strong>Authors: </strong>Tharun Anand, Siva Sankar, Pravin Nair</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22121">https://arxiv.org/abs/2503.22121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22121">https://arxiv.org/pdf/2503.22121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22121]] Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations(https://arxiv.org/abs/2503.22121)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With rapid advancements in generative modeling, deepfake techniques are increasingly narrowing the gap between real and synthetic videos, raising serious privacy and security concerns. Beyond traditional face swapping and reenactment, an emerging trend in recent state-of-the-art deepfake generation methods involves localized edits such as subtle manipulations of specific facial features like raising eyebrows, altering eye shapes, or modifying mouth expressions. These fine-grained manipulations pose a significant challenge for existing detection models, which struggle to capture such localized variations. To the best of our knowledge, this work presents the first detection approach explicitly designed to generalize to localized edits in deepfake videos by leveraging spatiotemporal representations guided by facial action units. Our method leverages a cross-attention-based fusion of representations learned from pretext tasks like random masking and action unit detection, to create an embedding that effectively encodes subtle, localized changes. Comprehensive evaluations across multiple deepfake generation methods demonstrate that our approach, despite being trained solely on the traditional FF+ dataset, sets a new benchmark in detecting recent deepfake-generated videos with fine-grained local edits, achieving a $20\%$ improvement in accuracy over current state-of-the-art detection methods. Additionally, our method delivers competitive performance on standard datasets, highlighting its robustness and generalization across diverse types of local and global forgeries.</li>
</ul>

<h3>Title: Tokenization of Gaze Data</h3>
<ul>
<li><strong>Authors: </strong>Tim Rolff, Jurik Karimian, Niklas Hypki, Susanne Schmidt, Markus Lappe, Frank Steinicke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22145">https://arxiv.org/abs/2503.22145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22145">https://arxiv.org/pdf/2503.22145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22145]] Tokenization of Gaze Data(https://arxiv.org/abs/2503.22145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A considerable part of the performance of today's large language models (LLM's) and multimodal large language models (MLLM's) depends on their tokenization strategies. While tokenizers are extensively researched for textual and visual input, there is no research on tokenization strategies for gaze data due to its nature. However, a corresponding tokenization strategy would allow using the vision capabilities of pre-trained MLLM's for gaze data, for example, through fine-tuning. In this paper, we aim to close this research gap by analyzing five different tokenizers for gaze data on three different datasets for the forecasting and generation of gaze data through LLMs (cf.~\cref{fig:teaser}). We evaluate the tokenizers regarding their reconstruction and compression abilities. Further, we train an LLM for each tokenization strategy, measuring its generative and predictive performance. Overall, we found that a quantile tokenizer outperforms all others in predicting the gaze positions and k-means is best when predicting gaze velocities.</li>
</ul>

<h3>Title: Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Woojung Han, Yeonkyung Lee, Chanyoung Kim, Kwanghyun Park, Seong Jae Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22168">https://arxiv.org/abs/2503.22168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22168">https://arxiv.org/pdf/2503.22168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22168]] Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis(https://arxiv.org/abs/2503.22168)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image (T2I) models have recently excelled in high-quality image generation, particularly in a training-free manner, enabling cost-effective adaptability and generalization across diverse tasks. However, while the existing methods have been continuously focusing on several challenges, such as "missing objects" and "mismatched attributes," another critical issue of "mislocated objects" remains where generated spatial positions fail to align with text prompts. Surprisingly, ensuring such seemingly basic functionality remains challenging in popular T2I models due to the inherent difficulty of imposing explicit spatial guidance via text forms. To address this, we propose STORM (Spatial Transport Optimization by Repositioning Attention Map), a novel training-free approach for spatially coherent T2I synthesis. STORM employs Spatial Transport Optimization (STO), rooted in optimal transport theory, to dynamically adjust object attention maps for precise spatial adherence, supported by a Spatial Transport (ST) Cost function that enhances spatial understanding. Our analysis shows that integrating spatial awareness is most effective in the early denoising stages, while later phases refine details. Extensive experiments demonstrate that STORM surpasses existing methods, effectively mitigating mislocated objects while improving missing and mismatched attributes, setting a new benchmark for spatial alignment in T2I synthesis.</li>
</ul>

<h3>Title: An Empirical Study of Validating Synthetic Data for Text-Based Person Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Min Cao, ZiYin Zeng, YuXin Lu, Mang Ye, Dong Yi, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22171">https://arxiv.org/abs/2503.22171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22171">https://arxiv.org/pdf/2503.22171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22171]] An Empirical Study of Validating Synthetic Data for Text-Based Person Retrieval(https://arxiv.org/abs/2503.22171)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data plays a pivotal role in Text-Based Person Retrieval (TBPR) research. Mainstream research paradigm necessitates real-world person images with manual textual annotations for training models, posing privacy-sensitive and labor-intensive issues. Several pioneering efforts explore synthetic data for TBPR but still rely on real data, keeping the aforementioned issues and also resulting in diversity-deficient issue in synthetic datasets, thus impacting TBPR performance. Moreover, these works tend to explore synthetic data for TBPR through limited perspectives, leading to exploration-restricted issue. In this paper, we conduct an empirical study to explore the potential of synthetic data for TBPR, highlighting three key aspects. (1) We propose an inter-class image generation pipeline, in which an automatic prompt construction strategy is introduced to guide generative Artificial Intelligence (AI) models in generating various inter-class images without reliance on original data. (2) We develop an intra-class image augmentation pipeline, in which the generative AI models are applied to further edit the images for obtaining various intra-class images. (3) Building upon the proposed pipelines and an automatic text generation pipeline, we explore the effectiveness of synthetic data in diverse scenarios through extensive experiments. Additionally, we experimentally investigate various noise-robust learning strategies to mitigate the inherent noise in synthetic data. We will release the code, along with the synthetic large-scale dataset generated by our pipelines, which are expected to advance practical TBPR research.</li>
</ul>

<h3>Title: High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Dailan He, Xiahong Wang, Shulun Wang, Guanglu Song, Bingqi Ma, Hao Shao, Yu Liu, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22179">https://arxiv.org/abs/2503.22179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22179">https://arxiv.org/pdf/2503.22179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22179]] High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning(https://arxiv.org/abs/2503.22179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Face swapping aims to seamlessly transfer a source facial identity onto a target while preserving target attributes such as pose and expression. Diffusion models, known for their superior generative capabilities, have recently shown promise in advancing face-swapping quality. This paper addresses two key challenges in diffusion-based face swapping: the prioritized preservation of identity over target attributes and the inherent conflict between identity and attribute conditioning. To tackle these issues, we introduce an identity-constrained attribute-tuning framework for face swapping that first ensures identity preservation and then fine-tunes for attribute alignment, achieved through a decoupled condition injection. We further enhance fidelity by incorporating identity and adversarial losses in a post-training refinement stage. Our proposed identity-constrained diffusion-based face-swapping model outperforms existing methods in both qualitative and quantitative evaluations, demonstrating superior identity similarity and attribute consistency, achieving a new state-of-the-art performance in high-fidelity face swapping.</li>
</ul>

<h3>Title: ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunhong Min, Daehyeon Choi, Kyeongmin Yeo, Jihyun Lee, Minhyuk Sung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22194">https://arxiv.org/abs/2503.22194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22194">https://arxiv.org/pdf/2503.22194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22194]] ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation(https://arxiv.org/abs/2503.22194)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.</li>
</ul>

<h3>Title: Extremely Simple Out-of-distribution Detection for Audio-visual Generalized Zero-shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Xun Zhang, Jiale Du, Xinbo Gao, Jungong Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22197">https://arxiv.org/abs/2503.22197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22197">https://arxiv.org/pdf/2503.22197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22197]] Extremely Simple Out-of-distribution Detection for Audio-visual Generalized Zero-shot Learning(https://arxiv.org/abs/2503.22197)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Zero-shot Learning(ZSL) attains knowledge transfer from seen classes to unseen classes by exploring auxiliary category information, which is a promising yet difficult research topic. In this field, Audio-Visual Generalized Zero-Shot Learning~(AV-GZSL) has aroused researchers' great interest in which intricate relations within triple modalities~(audio, video, and natural language) render this task quite challenging but highly research-worthy. However, both existing embedding-based and generative-based AV-GZSL methods tend to suffer from domain shift problem a lot and we propose an extremely simple Out-of-distribution~(OOD) detection based AV-GZSL method~(EZ-AVOOD) to further mitigate bias problem by differentiating seen and unseen samples at the initial beginning. EZ-AVOOD accomplishes effective seen-unseen separation by exploiting the intrinsic discriminative information held in class-specific logits and class-agnostic feature subspace without training an extra OOD detector network. Followed by seen-unseen binary classification, we employ two expert models to classify seen samples and unseen samples separately. Compared to existing state-of-the-art methods, our model achieves superior ZSL and GZSL performances on three audio-visual datasets and becomes the new SOTA, which comprehensively demonstrates the effectiveness of the proposed EZ-AVOOD.</li>
</ul>

<h3>Title: Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces</h3>
<ul>
<li><strong>Authors: </strong>Wonhyeok Choi, Kyumin Hwang, Minwoo Choi, Kiljoon Han, Wonjoon Choi, Mingyu Shin, Sunghoon Im</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22209">https://arxiv.org/abs/2503.22209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22209">https://arxiv.org/pdf/2503.22209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22209]] Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces(https://arxiv.org/abs/2503.22209)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised monocular depth estimation (SSMDE) has gained attention in the field of deep learning as it estimates depth without requiring ground truth depth maps. This approach typically uses a photometric consistency loss between a synthesized image, generated from the estimated depth, and the original image, thereby reducing the need for extensive dataset acquisition. However, the conventional photometric consistency loss relies on the Lambertian assumption, which often leads to significant errors when dealing with reflective surfaces that deviate from this model. To address this limitation, we propose a novel framework that incorporates intrinsic image decomposition into SSMDE. Our method synergistically trains for both monocular depth estimation and intrinsic image decomposition. The accurate depth estimation facilitates multi-image consistency for intrinsic image decomposition by aligning different view coordinate systems, while the decomposition process identifies reflective areas and excludes corrupted gradients from the depth training process. Furthermore, our framework introduces a pseudo-depth generation and knowledge distillation technique to further enhance the performance of the student model across both reflective and non-reflective surfaces. Comprehensive evaluations on multiple datasets show that our approach significantly outperforms existing SSMDE baselines in depth prediction, especially on reflective surfaces.</li>
</ul>

<h3>Title: Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance</h3>
<ul>
<li><strong>Authors: </strong>Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22225">https://arxiv.org/abs/2503.22225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22225">https://arxiv.org/pdf/2503.22225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22225]] Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance(https://arxiv.org/abs/2503.22225)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Pre-trained conditional diffusion models have demonstrated remarkable potential in image editing. However, they often face challenges with temporal consistency, particularly in the talking head domain, where continuous changes in facial expressions intensify the level of difficulty. These issues stem from the independent editing of individual images and the inherent loss of temporal continuity during the editing process. In this paper, we introduce Follow Your Motion (FYM), a generic framework for maintaining temporal consistency in portrait editing. Specifically, given portrait images rendered by a pre-trained 3D Gaussian Splatting model, we first develop a diffusion model that intuitively and inherently learns motion trajectory changes at different scales and pixel coordinates, from the first frame to each subsequent frame. This approach ensures that temporally inconsistent edited avatars inherit the motion information from the rendered avatars. Secondly, to maintain fine-grained expression temporal consistency in talking head editing, we propose a dynamic re-weighted attention mechanism. This mechanism assigns higher weight coefficients to landmark points in space and dynamically updates these weights based on landmark loss, achieving more consistent and refined facial expressions. Extensive experiments demonstrate that our method outperforms existing approaches in terms of temporal consistency and can be used to optimize and compensate for temporally inconsistent outputs in a range of applications, such as text-driven editing, relighting, and various other applications.</li>
</ul>

<h3>Title: Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, Lin Yan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22230">https://arxiv.org/abs/2503.22230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22230">https://arxiv.org/pdf/2503.22230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22230]] Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback(https://arxiv.org/abs/2503.22230)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.</li>
</ul>

<h3>Title: SCHNet: SAM Marries CLIP for Human Parsing</h3>
<ul>
<li><strong>Authors: </strong>Kunliang Liu, Jianming Wang, Rize Jin, Wonjun Hwang, Tae-Sun Chung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22237">https://arxiv.org/abs/2503.22237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22237">https://arxiv.org/pdf/2503.22237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22237]] SCHNet: SAM Marries CLIP for Human Parsing(https://arxiv.org/abs/2503.22237)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision Foundation Model (VFM) such as the Segment Anything Model (SAM) and Contrastive Language-Image Pre-training Model (CLIP) has shown promising performance for segmentation and detection tasks. However, although SAM excels in fine-grained segmentation, it faces major challenges when applying it to semantic-aware segmentation. While CLIP exhibits a strong semantic understanding capability via aligning the global features of language and vision, it has deficiencies in fine-grained segmentation tasks. Human parsing requires to segment human bodies into constituent parts and involves both accurate fine-grained segmentation and high semantic understanding of each part. Based on traits of SAM and CLIP, we formulate high efficient modules to effectively integrate features of them to benefit human parsing. We propose a Semantic-Refinement Module to integrate semantic features of CLIP with SAM features to benefit parsing. Moreover, we formulate a high efficient Fine-tuning Module to adjust the pretrained SAM for human parsing that needs high semantic information and simultaneously demands spatial details, which significantly reduces the training time compared with full-time training and achieves notable performance. Extensive experiments demonstrate the effectiveness of our method on LIP, PPP, and CIHP databases.</li>
</ul>

<h3>Title: Efficient Building Roof Type Classification: A Domain-Specific Self-Supervised Approach</h3>
<ul>
<li><strong>Authors: </strong>Guneet Mutreja, Ksenia Bittner</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22251">https://arxiv.org/abs/2503.22251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22251">https://arxiv.org/pdf/2503.22251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22251]] Efficient Building Roof Type Classification: A Domain-Specific Self-Supervised Approach(https://arxiv.org/abs/2503.22251)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate classification of building roof types from aerial imagery is crucial for various remote sensing applications, including urban planning, disaster management, and infrastructure monitoring. However, this task is often hindered by the limited availability of labeled data for supervised learning approaches. To address this challenge, this paper investigates the effectiveness of self supervised learning with EfficientNet architectures, known for their computational efficiency, for building roof type classification. We propose a novel framework that incorporates a Convolutional Block Attention Module (CBAM) to enhance the feature extraction capabilities of EfficientNet. Furthermore, we explore the benefits of pretraining on a domain-specific dataset, the Aerial Image Dataset (AID), compared to ImageNet pretraining. Our experimental results demonstrate the superiority of our approach. Employing Simple Framework for Contrastive Learning of Visual Representations (SimCLR) with EfficientNet-B3 and CBAM achieves a 95.5% accuracy on our validation set, matching the performance of state-of-the-art transformer-based models while utilizing significantly fewer parameters. We also provide a comprehensive evaluation on two challenging test sets, demonstrating the generalization capability of our method. Notably, our findings highlight the effectiveness of domain-specific pretraining, consistently leading to higher accuracy compared to models pretrained on the generic ImageNet dataset. Our work establishes EfficientNet based self-supervised learning as a computationally efficient and highly effective approach for building roof type classification, particularly beneficial in scenarios with limited labeled data.</li>
</ul>

<h3>Title: Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion</h3>
<ul>
<li><strong>Authors: </strong>Songsong Yu, Yuxin Chen, Zhongang Qi, Zeke Xie, Yifan Wang, Lijun Wang, Ying Shan, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22262">https://arxiv.org/abs/2503.22262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22262">https://arxiv.org/pdf/2503.22262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22262]] Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion(https://arxiv.org/abs/2503.22262)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the rapid proliferation of 3D devices and the shortage of 3D content, stereo conversion is attracting increasing attention. Recent works introduce pretrained Diffusion Models (DMs) into this task. However, due to the scarcity of large-scale training data and comprehensive benchmarks, the optimal methodologies for employing DMs in stereo conversion and the accurate evaluation of stereo effects remain largely unexplored. In this work, we introduce the Mono2Stereo dataset, providing high-quality training data and benchmark to support in-depth exploration of stereo conversion. With this dataset, we conduct an empirical study that yields two primary findings. 1) The differences between the left and right views are subtle, yet existing metrics consider overall pixels, failing to concentrate on regions critical to stereo effects. 2) Mainstream methods adopt either one-stage left-to-right generation or warp-and-inpaint pipeline, facing challenges of degraded stereo effect and image distortion respectively. Based on these findings, we introduce a new evaluation metric, Stereo Intersection-over-Union, which prioritizes disparity and achieves a high correlation with human judgments on stereo effect. Moreover, we propose a strong baseline model, harmonizing the stereo effect and image quality simultaneously, and notably surpassing current mainstream methods. Our code and data will be open-sourced to promote further research in stereo conversion. Our models are available at this http URL.</li>
</ul>

<h3>Title: A Dataset for Semantic Segmentation in the Presence of Unknowns</h3>
<ul>
<li><strong>Authors: </strong>Zakaria Laskar, Tomas Vojir, Matej Grcic, Iaroslav Melekhov, Shankar Gangisettye, Juho Kannala, Jiri Matas, Giorgos Tolias, C.V. Jawahar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22309">https://arxiv.org/abs/2503.22309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22309">https://arxiv.org/pdf/2503.22309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22309]] A Dataset for Semantic Segmentation in the Presence of Unknowns(https://arxiv.org/abs/2503.22309)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Before deployment in the real-world deep neural networks require thorough evaluation of how they handle both knowns, inputs represented in the training data, and unknowns (anomalies). This is especially important for scene understanding tasks with safety critical applications, such as in autonomous driving. Existing datasets allow evaluation of only knowns or unknowns - but not both, which is required to establish "in the wild" suitability of deep neural network models. To bridge this gap, we propose a novel anomaly segmentation dataset, ISSU, that features a diverse set of anomaly inputs from cluttered real-world environments. The dataset is twice larger than existing anomaly segmentation datasets, and provides a training, validation and test set for controlled in-domain evaluation. The test set consists of a static and temporal part, with the latter comprised of videos. The dataset provides annotations for both closed-set (knowns) and anomalies, enabling closed-set and open-set evaluation. The dataset covers diverse conditions, such as domain and cross-sensor shift, illumination variation and allows ablation of anomaly detection methods with respect to these variations. Evaluation results of current state-of-the-art methods confirm the need for improvements especially in domain-generalization, small and large object segmentation.</li>
</ul>

<h3>Title: VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow</h3>
<ul>
<li><strong>Authors: </strong>Yancong Lin, Shiming Wang, Liangliang Nan, Julian Kooij, Holger Caesar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22328">https://arxiv.org/abs/2503.22328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22328">https://arxiv.org/pdf/2503.22328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22328]] VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow(https://arxiv.org/abs/2503.22328)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Scene flow estimation aims to recover per-point motion from two adjacent LiDAR scans. However, in real-world applications such as autonomous driving, points rarely move independently of others, especially for nearby points belonging to the same object, which often share the same motion. Incorporating this locally rigid motion constraint has been a key challenge in self-supervised scene flow estimation, which is often addressed by post-processing or appending extra regularization. While these approaches are able to improve the rigidity of predicted flows, they lack an architectural inductive bias for local rigidity within the model structure, leading to suboptimal learning efficiency and inferior performance. In contrast, we enforce local rigidity with a lightweight add-on module in neural network design, enabling end-to-end learning. We design a discretized voting space that accommodates all possible translations and then identify the one shared by nearby points by differentiable voting. Additionally, to ensure computational efficiency, we operate on pillars rather than points and learn representative features for voting per pillar. We plug the Voting Module into popular model designs and evaluate its benefit on Argoverse 2 and Waymo datasets. We outperform baseline works with only marginal compute overhead. Code is available at this https URL.</li>
</ul>

<h3>Title: Imperceptible but Forgeable: Practical Invisible Watermark Forgery via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ziping Dong, Chao Shuai, Zhongjie Ba, Peng Cheng, Zhan Qin, Qinglong Wang, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22330">https://arxiv.org/abs/2503.22330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22330">https://arxiv.org/pdf/2503.22330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22330]] Imperceptible but Forgeable: Practical Invisible Watermark Forgery via Diffusion Models(https://arxiv.org/abs/2503.22330)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Invisible watermarking is critical for content provenance and accountability in Generative AI. Although commercial companies have increasingly committed to using watermarks, the robustness of existing watermarking schemes against forgery attacks is understudied. This paper proposes DiffForge, the first watermark forgery framework capable of forging imperceptible watermarks under a no-box setting. We estimate the watermark distribution using an unconditional diffusion model and introduce shallow inversion to inject the watermark into a non-watermarked image seamlessly. This approach facilitates watermark injection while preserving image quality by adaptively selecting the depth of inversion steps, leveraging our key insight that watermarks degrade with added noise during the early diffusion phases. Comprehensive evaluations show that DiffForge deceives open-source watermark detectors with a 96.38% success rate and misleads a commercial watermark system with over 97% success rate, achieving high confidence.1 This work reveals fundamental security limitations in current watermarking paradigms.</li>
</ul>

<h3>Title: SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection</h3>
<ul>
<li><strong>Authors: </strong>Shrikant Malviya, Pablo Arnau-GonzÃ¡lez, Miguel Arevalillo-HerrÃ¡ez, Stamos Katsigiannis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22338">https://arxiv.org/abs/2503.22338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22338">https://arxiv.org/pdf/2503.22338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22338]] SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection(https://arxiv.org/abs/2503.22338)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has introduced new challenges in distinguishing human-written text from AI-generated content. In this work, we explored a pipelined approach for AI-generated text detection that includes a feature extraction step (i.e. prompt-based rewriting features inspired by RAIDAR and content-based features derived from the NELA toolkit) followed by a classification module. Comprehensive experiments were conducted on the Defactify4.0 dataset, evaluating two tasks: binary classification to differentiate human-written and AI-generated text, and multi-class classification to identify the specific generative model used to generate the input text. Our findings reveal that NELA features significantly outperform RAIDAR features in both tasks, demonstrating their ability to capture nuanced linguistic, stylistic, and content-based differences. Combining RAIDAR and NELA features provided minimal improvement, highlighting the redundancy introduced by less discriminative features. Among the classifiers tested, XGBoost emerged as the most effective, leveraging the rich feature sets to achieve high accuracy and generalisation.</li>
</ul>

<h3>Title: Semantix: An Energy Guided Sampler for Semantic Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Huiang He, Minghui Hu, Chuanxia Zheng, Chaoyue Wang, Tat-Jen Cham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22344">https://arxiv.org/abs/2503.22344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22344">https://arxiv.org/pdf/2503.22344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22344]] Semantix: An Energy Guided Sampler for Semantic Style Transfer(https://arxiv.org/abs/2503.22344)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in style and appearance transfer are impressive, but most methods isolate global style and local appearance transfer, neglecting semantic correspondence. Additionally, image and video tasks are typically handled in isolation, with little focus on integrating them for video transfer. To address these limitations, we introduce a novel task, Semantic Style Transfer, which involves transferring style and appearance features from a reference image to a target visual content based on semantic correspondence. We subsequently propose a training-free method, Semantix an energy-guided sampler designed for Semantic Style Transfer that simultaneously guides both style and appearance transfer based on semantic understanding capacity of pre-trained diffusion models. Additionally, as a sampler, Semantix be seamlessly applied to both image and video models, enabling semantic style transfer to be generic across various visual media. Specifically, once inverting both reference and context images or videos to noise space by SDEs, Semantix utilizes a meticulously crafted energy function to guide the sampling process, including three key components: Style Feature Guidance, Spatial Feature Guidance and Semantic Distance as a regularisation term. Experimental results demonstrate that Semantix not only effectively accomplishes the task of semantic style transfer across images and videos, but also surpasses existing state-of-the-art solutions in both fields. The project website is available at this https URL</li>
</ul>

<h3>Title: GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Li-Heng Chen, Zi-Xin Zou, Chang Liu, Tianjiao Jing, Yan-Pei Cao, Shi-Sheng Huang, Hongbo Fu, Hua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22349">https://arxiv.org/abs/2503.22349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22349">https://arxiv.org/pdf/2503.22349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22349]] GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion(https://arxiv.org/abs/2503.22349)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate surface reconstruction from unposed images is crucial for efficient 3D object or scene creation. However, it remains challenging, particularly for the joint camera pose estimation. Previous approaches have achieved impressive pose-free surface reconstruction results in dense-view settings, but could easily fail for sparse-view scenarios without sufficient visual overlap. In this paper, we propose a new technique for pose-free surface reconstruction, which follows triplane-based signed distance field (SDF) learning but regularizes the learning by explicit points sampled from ray-based diffusion of camera pose estimation. Our key contribution is a novel Geometric Consistent Ray Diffusion model (GCRayDiffusion), where we represent camera poses as neural bundle rays and regress the distribution of noisy rays via a diffusion model. More importantly, we further condition the denoising process of RGRayDiffusion using the triplane-based SDF of the entire scene, which provides effective 3D consistent regularization to achieve multi-view consistent camera pose estimation. Finally, we incorporate RGRayDiffusion into the triplane-based SDF learning by introducing on-surface geometric regularization from the sampling points of the neural bundle rays, which leads to highly accurate pose-free surface reconstruction results even for sparse-view inputs. Extensive evaluations on public datasets show that our GCRayDiffusion achieves more accurate camera pose estimation than previous approaches, with geometrically more consistent surface reconstruction results, especially given sparse-view inputs.</li>
</ul>

<h3>Title: Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization</h3>
<ul>
<li><strong>Authors: </strong>BarÄ±Å Batuhan Topal, Umut Ãzyurt, Zafer DoÄan Budak, Ramazan Gokberk Cinbis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22352">https://arxiv.org/abs/2503.22352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22352">https://arxiv.org/pdf/2503.22352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22352]] Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization(https://arxiv.org/abs/2503.22352)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image generative models, particularly latent diffusion models (LDMs), have demonstrated remarkable capabilities in synthesizing high-quality images from textual prompts. However, achieving identity personalization-ensuring that a model consistently generates subject-specific outputs from limited reference images-remains a fundamental challenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA), a novel framework that leverages meta-learning to encode domain-specific priors into LoRA-based identity personalization. Our method introduces a structured three-layer LoRA architecture that separates identity-agnostic knowledge from identity-specific adaptation. In the first stage, the LoRA Meta-Down layers are meta-trained across multiple subjects, learning a shared manifold that captures general identity-related features. In the second stage, only the LoRA-Mid and LoRA-Up layers are optimized to specialize on a given subject, significantly reducing adaptation time while improving identity fidelity. To evaluate our approach, we introduce Meta-PHD, a new benchmark dataset for identity personalization, and compare Meta-LoRA against state-of-the-art methods. Our results demonstrate that Meta-LoRA achieves superior identity retention, computational efficiency, and adaptability across diverse identity conditions. The code, model weights, and dataset will be released publicly upon acceptance.</li>
</ul>

<h3>Title: EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Hadrien Reynaud, Alberto Gomez, Paul Leeson, Qingjie Meng, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22357">https://arxiv.org/abs/2503.22357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22357">https://arxiv.org/pdf/2503.22357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22357]] EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation(https://arxiv.org/abs/2503.22357)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Advances in deep learning have significantly enhanced medical image analysis, yet the availability of large-scale medical datasets remains constrained by patient privacy concerns. We present EchoFlow, a novel framework designed to generate high-quality, privacy-preserving synthetic echocardiogram images and videos. EchoFlow comprises four key components: an adversarial variational autoencoder for defining an efficient latent representation of cardiac ultrasound images, a latent image flow matching model for generating accurate latent echocardiogram images, a latent re-identification model to ensure privacy by filtering images anatomically, and a latent video flow matching model for animating latent images into realistic echocardiogram videos conditioned on ejection fraction. We rigorously evaluate our synthetic datasets on the clinically relevant task of ejection fraction regression and demonstrate, for the first time, that downstream models trained exclusively on EchoFlow-generated synthetic datasets achieve performance parity with models trained on real datasets. We release our models and synthetic datasets, enabling broader, privacy-compliant research in medical ultrasound imaging at this https URL.</li>
</ul>

<h3>Title: GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain</h3>
<ul>
<li><strong>Authors: </strong>Vida Adeli, Soroush Mehraban, Majid Mirmehdi, Alan Whone, Benjamin Filtjens, Amirhossein Dadashzadeh, Alfonso Fasano, Andrea Iaboni Babak Taati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22397">https://arxiv.org/abs/2503.22397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22397">https://arxiv.org/pdf/2503.22397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22397]] GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain(https://arxiv.org/abs/2503.22397)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Gait analysis is crucial for the diagnosis and monitoring of movement disorders like Parkinson's Disease. While computer vision models have shown potential for objectively evaluating parkinsonian gait, their effectiveness is limited by scarce clinical datasets and the challenge of collecting large and well-labelled data, impacting model accuracy and risk of bias. To address these gaps, we propose GAITGen, a novel framework that generates realistic gait sequences conditioned on specified pathology severity levels. GAITGen employs a Conditional Residual Vector Quantized Variational Autoencoder to learn disentangled representations of motion dynamics and pathology-specific factors, coupled with Mask and Residual Transformers for conditioned sequence generation. GAITGen generates realistic, diverse gait sequences across severity levels, enriching datasets and enabling large-scale model training in parkinsonian gait analysis. Experiments on our new PD-GaM (real) dataset demonstrate that GAITGen outperforms adapted state-of-the-art models in both reconstruction fidelity and generation quality, accurately capturing critical pathology-specific gait features. A clinical user study confirms the realism and clinical relevance of our generated sequences. Moreover, incorporating GAITGen-generated data into downstream tasks improves parkinsonian gait severity estimation, highlighting its potential for advancing clinical gait analysis.</li>
</ul>

<h3>Title: Generative Reliability-Based Design Optimization Using In-Context Learning Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhonglin Jiang, Qian Tang, Zequn Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22401">https://arxiv.org/abs/2503.22401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22401">https://arxiv.org/pdf/2503.22401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22401]] Generative Reliability-Based Design Optimization Using In-Context Learning Capabilities of Large Language Models(https://arxiv.org/abs/2503.22401)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable in-context learning capabilities, enabling flexible utilization of limited historical information to play pivotal roles in reasoning, problem-solving, and complex pattern recognition tasks. Inspired by the successful applications of LLMs in multiple domains, this paper proposes a generative design method by leveraging the in-context learning capabilities of LLMs with the iterative search mechanisms of metaheuristic algorithms for solving reliability-based design optimization problems. In detail, reliability analysis is performed by engaging the LLMs and Kriging surrogate modeling to overcome the computational burden. By dynamically providing critical information of design points to the LLMs with prompt engineering, the method enables rapid generation of high-quality design alternatives that satisfy reliability constraints while achieving performance optimization. With the Deepseek-V3 model, three case studies are used to demonstrated the performance of the proposed approach. Experimental results indicate that the proposed LLM-RBDO method successfully identifies feasible solutions that meet reliability constraints while achieving a comparable convergence rate compared to traditional genetic algorithms.</li>
</ul>

<h3>Title: Robustness quantification and how it allows for reliable classification, even in the presence of distribution shift and for small training sets</h3>
<ul>
<li><strong>Authors: </strong>AdriÃ¡n Detavernier, Jasper De Bock</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22418">https://arxiv.org/abs/2503.22418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22418">https://arxiv.org/pdf/2503.22418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22418]] Robustness quantification and how it allows for reliable classification, even in the presence of distribution shift and for small training sets(https://arxiv.org/abs/2503.22418)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Based on existing ideas in the field of imprecise probabilities, we present a new approach for assessing the reliability of the individual predictions of a generative probabilistic classifier. We call this approach robustness quantification, compare it to uncertainty quantification, and demonstrate that it continues to work well even for classifiers that are learned from small training sets that are sampled from a shifted distribution.</li>
</ul>

<h3>Title: Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Max Hennick, Stijn De Baerdemacker</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22478">https://arxiv.org/abs/2503.22478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22478">https://arxiv.org/pdf/2503.22478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22478]] Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent(https://arxiv.org/abs/2503.22478)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We show that the behavior of stochastic gradient descent is related to Bayesian statistics by showing that SGD is effectively diffusion on a fractal landscape, where the fractal dimension can be accounted for in a purely Bayesian way. By doing this we show that SGD can be regarded as a modified Bayesian sampler which accounts for accessibility constraints induced by the fractal structure of the loss landscape. We verify our results experimentally by examining the diffusion of weights during training. These results offer insight into the factors which determine the learning process, and seemingly answer the question of how SGD and purely Bayesian sampling are related.</li>
</ul>

<h3>Title: Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets</h3>
<ul>
<li><strong>Authors: </strong>Martin KiÅ¡Å¡, Michal HradiÅ¡</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22513">https://arxiv.org/abs/2503.22513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22513">https://arxiv.org/pdf/2503.22513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22513]] Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets(https://arxiv.org/abs/2503.22513)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has emerged as a powerful approach for leveraging large-scale unlabeled data to improve model performance in various domains. In this paper, we explore masked self-supervised pre-training for text recognition transformers. Specifically, we propose two modifications to the pre-training phase: progressively increasing the masking probability, and modifying the loss function to incorporate both masked and non-masked patches. We conduct extensive experiments using a dataset of 50M unlabeled text lines for pre-training and four differently sized annotated datasets for fine-tuning. Furthermore, we compare our pre-trained models against those trained with transfer learning, demonstrating the effectiveness of the self-supervised pre-training. In particular, pre-training consistently improves the character error rate of models, in some cases up to 30 % relatively. It is also on par with transfer learning but without relying on extra annotated text lines.</li>
</ul>

<h3>Title: Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery</h3>
<ul>
<li><strong>Authors: </strong>Samira Alkaee Taleghan, Morteza Karimzadeh, Andrew P. Barrett, Walter N. Meier, Farnoush Banaei-Kashani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22516">https://arxiv.org/abs/2503.22516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22516">https://arxiv.org/pdf/2503.22516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22516]] Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery(https://arxiv.org/abs/2503.22516)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of sea ice types is essential for mapping and operational forecasting of sea ice conditions for safe navigation and resource extraction in ice-covered waters, as well as for understanding polar climate processes. While deep learning methods have shown promise in automating sea ice segmentation, they often rely on extensive labeled datasets which require expert knowledge and are time-consuming to create. Recently, foundation models (FMs) have shown excellent results for segmenting remote sensing images by utilizing pre-training on large datasets using self-supervised techniques. However, their effectiveness for sea ice segmentation remains unexplored, especially given sea ice's complex structures, seasonal changes, and unique spectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagery characteristics including banding and scalloping noise, and varying ice backscatter characteristics, which are often missing in standard remote sensing pre-training datasets. In particular, SAR images over polar regions are acquired using different modes than used to capture the images at lower latitudes by the same sensors that form training datasets for FMs. This study evaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1 SAR imagery, focusing on their seasonal and spatial generalization. Among the selected models, Prithvi-600M outperforms the baseline models, while CROMA achieves a very similar performance in F1-score. Our contributions include offering a systematic methodology for selecting FMs for sea ice data analysis, a comprehensive benchmarking study on performances of FMs for sea ice segmentation with tailored performance metrics, and insights into existing gaps and future directions for improving domain-specific models in polar applications using SAR data.</li>
</ul>

<h3>Title: Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities</h3>
<ul>
<li><strong>Authors: </strong>Raman Dutt, Harleen Hanspal, Guoxuan Xia, Petru-Daniel Tudosiu, Alexander Black, Yongxin Yang, Steven McDonagh, Sarah Parisot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22517">https://arxiv.org/abs/2503.22517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22517">https://arxiv.org/pdf/2503.22517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22517]] Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities(https://arxiv.org/abs/2503.22517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we undertake the challenge of augmenting the existing generative capabilities of pre-trained text-only large language models (LLMs) with multi-modal generation capability while satisfying two core constraints: C1 preserving the preservation of original language generative capabilities with negligible performance degradation, and C2 adhering to a small parameter budget to learn the new modality, ensuring scalability and efficiency. In contrast to current approaches that add dedicated modules, thereby significantly increasing the parameter count, we propose a method that leverages the underutilized capacity inherent in deep models. Specifically, we exploit the parameter redundancy within Mixture-of-Experts (MoEs) as a source of additional capacity for learning a new modality, enabling better parameter efficiency (C1). Moreover, we preserve the original language generation capabilities by applying low-rank adaptation exclusively to the tokens of the new modality (C2). Furthermore, we introduce a novel parameter initialization scheme based on the Gromov-Wasserstein distance to improve convergence and training stability. Through an extensive analysis of the routing mechanism, we uncover the emergence of modality-specific pathways and decreased redundancy within the experts that can efficiently unlock multi-modal generative capabilities. Overall, our method can be seamlessly applied to a wide range of contemporary LLMs, providing a new pathway for transitioning from uni-modal to multi-modal architectures.</li>
</ul>

<h3>Title: LIM: Large Interpolator Model for Dynamic Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Remy Sabathier, Niloy J. Mitra, David Novotny</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22537">https://arxiv.org/abs/2503.22537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22537">https://arxiv.org/pdf/2503.22537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22537]] LIM: Large Interpolator Model for Dynamic Reconstruction(https://arxiv.org/abs/2503.22537)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reconstructing dynamic assets from video data is central to many in computer vision and graphics tasks. Existing 4D reconstruction approaches are limited by category-specific models or slow optimization-based methods. Inspired by the recent Large Reconstruction Model (LRM), we present the Large Interpolation Model (LIM), a transformer-based feed-forward solution, guided by a novel causal consistency loss, for interpolating implicit 3D representations across time. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces a deformed shape at any continuous time $t\in[t_0,t_1]$, delivering high-quality interpolated frames in seconds. Furthermore, LIM allows explicit mesh tracking across time, producing a consistently uv-textured mesh sequence ready for integration into existing production pipelines. We also use LIM, in conjunction with a diffusion-based multiview generator, to produce dynamic 4D reconstructions from monocular videos. We evaluate LIM on various dynamic datasets, benchmarking against image-space interpolation methods (e.g., FiLM) and direct triplane linear interpolation, and demonstrate clear advantages. In summary, LIM is the first feed-forward model capable of high-speed tracked 4D asset reconstruction across diverse categories.</li>
</ul>

<h3>Title: Comparing Methods for Bias Mitigation in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Barbara Hoffmann, Ruben Mayer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22569">https://arxiv.org/abs/2503.22569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22569">https://arxiv.org/pdf/2503.22569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22569]] Comparing Methods for Bias Mitigation in Graph Neural Networks(https://arxiv.org/abs/2503.22569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper examines the critical role of Graph Neural Networks (GNNs) in data preparation for generative artificial intelligence (GenAI) systems, with a particular focus on addressing and mitigating biases. We present a comparative analysis of three distinct methods for bias mitigation: data sparsification, feature modification, and synthetic data augmentation. Through experimental analysis using the german credit dataset, we evaluate these approaches using multiple fairness metrics, including statistical parity, equality of opportunity, and false positive rates. Our research demonstrates that while all methods improve fairness metrics compared to the original dataset, stratified sampling and synthetic data augmentation using GraphSAGE prove particularly effective in balancing demographic representation while maintaining model performance. The results provide practical insights for developing more equitable AI systems while maintaining model performance.</li>
</ul>

<h3>Title: Generative Latent Neural PDE Solver using Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zijie Li, Anthony Zhou, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22600">https://arxiv.org/abs/2503.22600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22600">https://arxiv.org/pdf/2503.22600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22600]] Generative Latent Neural PDE Solver using Flow Matching(https://arxiv.org/abs/2503.22600)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive next-step prediction models have become the de-facto standard for building data-driven neural solvers to forecast time-dependent partial differential equations (PDEs). Denoise training that is closely related to diffusion probabilistic model has been shown to enhance the temporal stability of neural solvers, while its stochastic inference mechanism enables ensemble predictions and uncertainty quantification. In principle, such training involves sampling a series of discretized diffusion timesteps during both training and inference, inevitably increasing computational overhead. In addition, most diffusion models apply isotropic Gaussian noise on structured, uniform grids, limiting their adaptability to irregular domains. We propose a latent diffusion model for PDE simulation that embeds the PDE state in a lower-dimensional latent space, which significantly reduces computational costs. Our framework uses an autoencoder to map different types of meshes onto a unified structured latent grid, capturing complex geometries. By analyzing common diffusion paths, we propose to use a coarsely sampled noise schedule from flow matching for both training and testing. Numerical experiments show that the proposed model outperforms several deterministic baselines in both accuracy and long-term stability, highlighting the potential of diffusion-based approaches for robust data-driven PDE learning.</li>
</ul>

<h3>Title: Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines</h3>
<ul>
<li><strong>Authors: </strong>Jayaprakashreddy Cheenepalli, John D. Hastings, Khandaker Mamun Ahmed, Chad Fenner</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22612">https://arxiv.org/abs/2503.22612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22612">https://arxiv.org/pdf/2503.22612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22612]] Advancing DevSecOps in SMEs: Challenges and Best Practices for Secure CI/CD Pipelines(https://arxiv.org/abs/2503.22612)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This study evaluates the adoption of DevSecOps among small and medium-sized enterprises (SMEs), identifying key challenges, best practices, and future trends. Through a mixed methods approach backed by the Technology Acceptance Model (TAM) and Diffusion of Innovations (DOI) theory, we analyzed survey data from 405 SME professionals, revealing that while 68% have implemented DevSecOps, adoption is hindered by technical complexity (41%), resource constraints (35%), and cultural resistance (38%). Despite strong leadership prioritization of security (73%), automation gaps persist, with only 12% of organizations conducting security scans per commit. Our findings highlight a growing integration of security tools, particularly API security (63%) and software composition analysis (62%), although container security adoption remains low (34%). Looking ahead, SMEs anticipate artificial intelligence and machine learning to significantly influence DevSecOps, underscoring the need for proactive adoption of AI-driven security enhancements. Based on our findings, this research proposes strategic best practices to enhance CI/CD pipeline security including automation, leadership-driven security culture, and cross-team collaboration.</li>
</ul>

<h3>Title: Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jangho Park, Taesung Kwon, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22622">https://arxiv.org/abs/2503.22622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22622">https://arxiv.org/pdf/2503.22622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22622]] Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model(https://arxiv.org/abs/2503.22622)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, multi-view or 4D video generation has emerged as a significant research topic. Nonetheless, recent approaches to 4D generation still struggle with fundamental limitations, as they primarily rely on harnessing multiple video diffusion models with additional training or compute-intensive training of a full 4D diffusion model with limited real-world 4D data and large computational costs. To address these challenges, here we propose the first training-free 4D video generation method that leverages the off-the-shelf video diffusion models to generate multi-view videos from a single input video. Our approach consists of two key steps: (1) By designating the edge frames in the spatio-temporal sampling grid as key frames, we first synthesize them using a video diffusion model, leveraging a depth-based warping technique for guidance. This approach ensures structural consistency across the generated frames, preserving spatial and temporal coherence. (2) We then interpolate the remaining frames using a video diffusion model, constructing a fully populated and temporally coherent sampling grid while preserving spatial and temporal consistency. Through this approach, we extend a single video into a multi-view video along novel camera trajectories while maintaining spatio-temporal consistency. Our method is training-free and fully utilizes an off-the-shelf video diffusion model, offering a practical and effective solution for multi-view video generation.</li>
</ul>

<h3>Title: DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness</h3>
<ul>
<li><strong>Authors: </strong>Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.22677">https://arxiv.org/abs/2503.22677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.22677">https://arxiv.org/pdf/2503.22677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.22677]] DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness(https://arxiv.org/abs/2503.22677)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
