<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-12</h1>
<h3>Title: Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Xiran Fan, Zhimeng Jiang, Chin-Chia Michael Yeh, Yuzhong Chen, Yingtong Dou, Menghai Pan, Yan Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05271">https://arxiv.org/abs/2601.05271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05271">https://arxiv.org/pdf/2601.05271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05271]] Enhancing Foundation Models in Transaction Understanding with LLM-based Sentence Embeddings(https://arxiv.org/abs/2601.05271)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The ubiquity of payment networks generates vast transactional data encoding rich consumer and merchant behavioral patterns. Recent foundation models for transaction analysis process tabular data sequentially but rely on index-based representations for categorical merchant fields, causing substantial semantic information loss by converting rich textual data into discrete tokens. While Large Language Models (LLMs) can address this limitation through superior semantic understanding, their computational overhead challenges real-time financial deployment. We introduce a hybrid framework that uses LLM-generated embeddings as semantic initializations for lightweight transaction models, balancing interpretability with operational efficiency. Our approach employs multi-source data fusion to enrich merchant categorical fields and a one-word constraint principle for consistent embedding generation across LLM architectures. We systematically address data quality through noise filtering and context-aware enrichment. Experiments on large-scale transaction datasets demonstrate significant performance improvements across multiple transaction understanding tasks.</li>
</ul>

<h3>Title: A Survey of Agentic AI and Cybersecurity: Challenges, Opportunities and Use-case Prototypes</h3>
<ul>
<li><strong>Authors: </strong>Sahaya Jestus Lazer, Kshitiz Aryal, Maanak Gupta, Elisa Bertino</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05293">https://arxiv.org/abs/2601.05293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05293">https://arxiv.org/pdf/2601.05293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05293]] A Survey of Agentic AI and Cybersecurity: Challenges, Opportunities and Use-case Prototypes(https://arxiv.org/abs/2601.05293)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Agentic AI marks an important transition from single-step generative models to systems capable of reasoning, planning, acting, and adapting over long-lasting tasks. By integrating memory, tool use, and iterative decision cycles, these systems enable continuous, autonomous workflows in real-world environments. This survey examines the implications of agentic AI for cybersecurity. On the defensive side, agentic capabilities enable continuous monitoring, autonomous incident response, adaptive threat hunting, and fraud detection at scale. Conversely, the same properties amplify adversarial power by accelerating reconnaissance, exploitation, coordination, and social-engineering attacks. These dual-use dynamics expose fundamental gaps in existing governance, assurance, and accountability mechanisms, which were largely designed for non-autonomous and short-lived AI systems. To address these challenges, we survey emerging threat models, security frameworks, and evaluation pipelines tailored to agentic systems, and analyze systemic risks including agent collusion, cascading failures, oversight evasion, and memory poisoning. Finally, we present three representative use-case implementations that illustrate how agentic AI behaves in practical cybersecurity workflows, and how design choices shape reliability, safety, and operational effectiveness.</li>
</ul>

<h3>Title: TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Susmit Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05300">https://arxiv.org/abs/2601.05300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05300">https://arxiv.org/pdf/2601.05300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05300]] TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning(https://arxiv.org/abs/2601.05300)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at this https URL and TIMEBench is available at this https URL</li>
</ul>

<h3>Title: Multi-turn Jailbreaking Attack in Multi-Modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Badhan Chandra Das, Md Tasnim Jawad, Joaquin Molto, M. Hadi Amini, Yanzhao Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05339">https://arxiv.org/abs/2601.05339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05339">https://arxiv.org/pdf/2601.05339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05339]] Multi-turn Jailbreaking Attack in Multi-Modal Large Language Models(https://arxiv.org/abs/2601.05339)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, the security vulnerabilities of Multi-modal Large Language Models (MLLMs) have become a serious concern in the Generative Artificial Intelligence (GenAI) research. These highly intelligent models, capable of performing multi-modal tasks with high accuracy, are also severely susceptible to carefully launched security attacks, such as jailbreaking attacks, which can manipulate model behavior and bypass safety constraints. This paper introduces MJAD-MLLMs, a holistic framework that systematically analyzes the proposed Multi-turn Jailbreaking Attacks and multi-LLM-based defense techniques for MLLMs. In this paper, we make three original contributions. First, we introduce a novel multi-turn jailbreaking attack to exploit the vulnerabilities of the MLLMs under multi-turn prompting. Second, we propose a novel fragment-optimized and multi-LLM defense mechanism, called FragGuard, to effectively mitigate jailbreaking attacks in the MLLMs. Third, we evaluate the efficacy of the proposed attacks and defenses through extensive experiments on several state-of-the-art (SOTA) open-source and closed-source MLLMs and benchmark datasets, and compare their performance with the existing techniques.</li>
</ul>

<h3>Title: Coding the Visual World: From Image to Simulation Using Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sagi Eppel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05344">https://arxiv.org/abs/2601.05344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05344">https://arxiv.org/pdf/2601.05344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05344]] Coding the Visual World: From Image to Simulation Using Vision Language Models(https://arxiv.org/abs/2601.05344)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.</li>
</ul>

<h3>Title: Multi-task Cross-modal Learning for Chest X-ray Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zhaohui Liang, Sivaramakrishnan Rajaraman, Niccolo Marini, Zhiyun Xue, Sameer Antani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05399">https://arxiv.org/abs/2601.05399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05399">https://arxiv.org/pdf/2601.05399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05399]] Multi-task Cross-modal Learning for Chest X-ray Image Retrieval(https://arxiv.org/abs/2601.05399)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model's enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.</li>
</ul>

<h3>Title: Efficient Inference for Noisy LLM-as-a-Judge Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yiqun T Chen, Sizhu Lu, Sijia Li, Moran Guo, Shengyi Li</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05420">https://arxiv.org/abs/2601.05420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05420">https://arxiv.org/pdf/2601.05420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05420]] Efficient Inference for Noisy LLM-as-a-Judge Evaluation(https://arxiv.org/abs/2601.05420)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at this https URL.</li>
</ul>

<h3>Title: TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Hongyang Xie, Hongyang He, Victor Sanchez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05446">https://arxiv.org/abs/2601.05446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05446">https://arxiv.org/pdf/2601.05446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05446]] TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection(https://arxiv.org/abs/2601.05446)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.</li>
</ul>

<h3>Title: MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jiefu Ou, Sapana Chaudhary, Kaj Bostrom, Nathaniel Weir, Shuai Zhang, Huzefa Rangwala, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05475">https://arxiv.org/abs/2601.05475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05475">https://arxiv.org/pdf/2601.05475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05475]] MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization(https://arxiv.org/abs/2601.05475)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.</li>
</ul>

<h3>Title: Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection</h3>
<ul>
<li><strong>Authors: </strong>Feihu Jin, Ying Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05501">https://arxiv.org/abs/2601.05501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05501">https://arxiv.org/pdf/2601.05501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05501]] Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection(https://arxiv.org/abs/2601.05501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of "beneficial stochasticity" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.</li>
</ul>

<h3>Title: DeMa: Dual-Path Delay-Aware Mamba for Efficient Multivariate Time Series Analysis</h3>
<ul>
<li><strong>Authors: </strong>Rui An, Haohao Qu, Wenqi Fan, Xuequn Shang, Qing Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05527">https://arxiv.org/abs/2601.05527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05527">https://arxiv.org/pdf/2601.05527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05527]] DeMa: Dual-Path Delay-Aware Mamba for Efficient Multivariate Time Series Analysis(https://arxiv.org/abs/2601.05527)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Accurate and efficient multivariate time series (MTS) analysis is increasingly critical for a wide range of intelligent applications. Within this realm, Transformers have emerged as the predominant architecture due to their strong ability to capture pairwise dependencies. However, Transformer-based models suffer from quadratic computational complexity and high memory overhead, limiting their scalability and practical deployment in long-term and large-scale MTS modeling. Recently, Mamba has emerged as a promising linear-time alternative with high expressiveness. Nevertheless, directly applying vanilla Mamba to MTS remains suboptimal due to three key limitations: (i) the lack of explicit cross-variate modeling, (ii) difficulty in disentangling the entangled intra-series temporal dynamics and inter-series interactions, and (iii) insufficient modeling of latent time-lag interaction effects. These issues constrain its effectiveness across diverse MTS tasks. To address these challenges, we propose DeMa, a dual-path delay-aware Mamba backbone. DeMa preserves Mamba's linear-complexity advantage while substantially improving its suitability for MTS settings. Specifically, DeMa introduces three key innovations: (i) it decomposes the MTS into intra-series temporal dynamics and inter-series interactions; (ii) it develops a temporal path with a Mamba-SSD module to capture long-range dynamics within each individual series, enabling series-independent, parallel computation; and (iii) it designs a variate path with a Mamba-DALA module that integrates delay-aware linear attention to model cross-variate dependencies. Extensive experiments on five representative tasks, long- and short-term forecasting, data imputation, anomaly detection, and series classification, demonstrate that DeMa achieves state-of-the-art performance while delivering remarkable computational efficiency.</li>
</ul>

<h3>Title: One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Bin-Bin Gao, Chengjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05552">https://arxiv.org/abs/2601.05552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05552">https://arxiv.org/pdf/2601.05552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05552]] One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection(https://arxiv.org/abs/2601.05552)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at this https URL.</li>
</ul>

<h3>Title: Orient Anything V2: Unifying Orientation and Rotation Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zehan Wang, Ziang Zhang, Jiayang Xu, Jialei Wang, Tianyu Pang, Chao Du, HengShuang Zhao, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05573">https://arxiv.org/abs/2601.05573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05573">https://arxiv.org/pdf/2601.05573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05573]] Orient Anything V2: Unifying Orientation and Rotation Understanding(https://arxiv.org/abs/2601.05573)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.</li>
</ul>

<h3>Title: Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Hanyi Wang, Jun Lan, Yaoyu Kang, Huijia Zhu, Weiqiang Wang, Zhuosheng Zhang, Shilin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05580">https://arxiv.org/abs/2601.05580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05580">https://arxiv.org/pdf/2601.05580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05580]] Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection(https://arxiv.org/abs/2601.05580)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.</li>
</ul>

<h3>Title: Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow</h3>
<ul>
<li><strong>Authors: </strong>Xue Feng, Li Wang, Deanna Needell, Rongjie Lai</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05583">https://arxiv.org/abs/2601.05583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05583">https://arxiv.org/pdf/2601.05583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05583]] Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow(https://arxiv.org/abs/2601.05583)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.</li>
</ul>

<h3>Title: LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chengen Xie, Bin Sun, Tianyu Li, Junjie Wu, Zhihui Hao, XianPeng Lang, Hongyang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05611">https://arxiv.org/abs/2601.05611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05611">https://arxiv.org/pdf/2601.05611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05611]] LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction(https://arxiv.org/abs/2601.05611)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>End-to-end autonomous driving models trained on largescale datasets perform well in common scenarios but struggle with rare, long-tail situations due to limited scenario diversity. Recent Vision-Language-Action (VLA) models leverage broad knowledge from pre-trained visionlanguage models to address this limitation, yet face critical challenges: (1) numerical imprecision in trajectory prediction due to discrete tokenization, (2) heavy reliance on language annotations that introduce linguistic bias and annotation burden, and (3) computational inefficiency from multi-step chain-of-thought reasoning hinders real-time deployment. We propose LatentVLA, a novel framework that employs self-supervised latent action prediction to train VLA models without language annotations, eliminating linguistic bias while learning rich driving representations from unlabeled trajectory data. Through knowledge distillation, LatentVLA transfers the generalization capabilities of VLA models to efficient vision-based networks, achieving both robust performance and real-time efficiency. LatentVLA establishes a new state-of-the-art on the NAVSIM benchmark with a PDMS score of 92.4 and demonstrates strong zeroshot generalization on the nuScenes benchmark.</li>
</ul>

<h3>Title: Transformer Is Inherently a Causal Learner</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Wang, Stephen Wang, Biwei Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05647">https://arxiv.org/abs/2601.05647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05647">https://arxiv.org/pdf/2601.05647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05647]] Transformer Is Inherently a Causal Learner(https://arxiv.org/abs/2601.05647)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.</li>
</ul>

<h3>Title: AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces</h3>
<ul>
<li><strong>Authors: </strong>Yeonsang Shin, Insoo Kim, Bongkeun Kim, Keonwoo Bae, Bohyung Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05680">https://arxiv.org/abs/2601.05680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05680">https://arxiv.org/pdf/2601.05680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05680]] AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces(https://arxiv.org/abs/2601.05680)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.</li>
</ul>

<h3>Title: Multimodal In-context Learning for ASR of Low-resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Zhaolin Li, Jan Niehues</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05707">https://arxiv.org/abs/2601.05707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05707">https://arxiv.org/pdf/2601.05707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05707]] Multimodal In-context Learning for ASR of Low-resource Languages(https://arxiv.org/abs/2601.05707)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Automatic speech recognition (ASR) still covers only a small fraction of the world's languages, mainly due to supervised data scarcity. In-context learning (ICL) with large language models (LLMs) addresses this problem, but prior work largely focuses on high-resource languages covered during training and text-only settings. This paper investigates whether speech LLMs can learn unseen languages with multimodal ICL (MICL), and how this learning can be used to improve ASR. We conduct experiments with two speech LLMs, Phi-4 and Qwen3-Omni, on three diverse endangered languages. Firstly, we find that MICL is effective for unseen languages, leveraging both speech and text modalities. We further show that cross-lingual transfer learning improves MICL efficiency on target languages without training on them. Moreover, we analyze attention patterns to interpret MICL mechanisms, and we observe layer-dependent preferences between audio and text context, with an overall bias towards text. Finally, we show that prompt-based ASR with speech LLMs performs poorly on unseen languages, motivating a simple ASR system that combines a stronger acoustic model with a speech LLM via MICL-based selection of acoustic hypotheses. Results show that MICL consistently improves ASR performance, and that cross-lingual transfer learning matches or outperforms corpus-trained language models without using target-language data. Our code is publicly available.</li>
</ul>

<h3>Title: Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging</h3>
<ul>
<li><strong>Authors: </strong>Thomas Fabian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05713">https://arxiv.org/abs/2601.05713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05713">https://arxiv.org/pdf/2601.05713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05713]] Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging(https://arxiv.org/abs/2601.05713)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.</li>
</ul>

<h3>Title: Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation</h3>
<ul>
<li><strong>Authors: </strong>Jin Wang, Jianxiang Lu, Comi Chen, Guangzheng Xu, Haoyu Yang, Peng Chen, Na Zhang, Yifan Xu, Longhuang Wu, Shuai Shao, Qinglin Lu, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05722">https://arxiv.org/abs/2601.05722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05722">https://arxiv.org/pdf/2601.05722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05722]] Rotate Your Character: Revisiting Video Diffusion Models for High-Quality 3D Character Generation(https://arxiv.org/abs/2601.05722)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating high-quality 3D characters from single images remains a significant challenge in digital content creation, particularly due to complex body poses and self-occlusion. In this paper, we present RCM (Rotate your Character Model), an advanced image-to-video diffusion framework tailored for high-quality novel view synthesis (NVS) and 3D character generation. Compared to existing diffusion-based approaches, RCM offers several key advantages: (1) transferring characters with any complex poses into a canonical pose, enabling consistent novel view synthesis across the entire viewing orbit, (2) high-resolution orbital video generation at 1024x1024 resolution, (3) controllable observation positions given different initial camera poses, and (4) multi-view conditioning supporting up to 4 input images, accommodating diverse user scenarios. Extensive experiments demonstrate that RCM outperforms state-of-the-art methods in both novel view synthesis and 3D generation quality.</li>
</ul>

<h3>Title: FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time</h3>
<ul>
<li><strong>Authors: </strong>Christopher Thirgood, Oscar Mendez, Erin Ling, Jon Storey, Simon Hadfield</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05738">https://arxiv.org/abs/2601.05738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05738">https://arxiv.org/pdf/2601.05738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05738]] FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time(https://arxiv.org/abs/2601.05738)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present a real-time tracking SLAM system that unifies efficient camera tracking with photorealistic feature-enriched mapping using 3D Gaussian Splatting (3DGS). Our main contribution is integrating dense feature rasterization into the novel-view synthesis, aligned with a visual foundation model. This yields strong semantics, going beyond basic RGB-D input, aiding both tracking and mapping accuracy. Unlike previous semantic SLAM approaches (which embed pre-defined class labels) FeatureSLAM enables entirely new downstream tasks via free-viewpoint, open-set segmentation. Across standard benchmarks, our method achieves real-time tracking, on par with state-of-the-art systems while improving tracking stability and map fidelity without prohibitive compute. Quantitatively, we obtain 9\% lower pose error and 8\% higher mapping accuracy compared to recent fixed-set SLAM baselines. Our results confirm that real-time feature-embedded SLAM, is not only valuable for enabling new downstream applications. It also improves the performance of the underlying tracking and mapping subsystems, providing semantic and language masking results that are on-par with offline 3DGS models, alongside state-of-the-art tracking, depth and RGB rendering.</li>
</ul>

<h3>Title: Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms</h3>
<ul>
<li><strong>Authors: </strong>Turkan Simge Ispak, Salih Tileylioglu, Erdem Akagunduz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05759">https://arxiv.org/abs/2601.05759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05759">https://arxiv.org/pdf/2601.05759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05759]] Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms(https://arxiv.org/abs/2601.05759)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce "overgeneralization", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.</li>
</ul>

<h3>Title: SceneFoundry: Generating Interactive Infinite 3D Worlds</h3>
<ul>
<li><strong>Authors: </strong>ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, YuanFu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05810">https://arxiv.org/abs/2601.05810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05810">https://arxiv.org/pdf/2601.05810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05810]] SceneFoundry: Generating Interactive Infinite 3D Worlds(https://arxiv.org/abs/2601.05810)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.</li>
</ul>

<h3>Title: Boosting Latent Diffusion Models via Disentangled Representation Alignment</h3>
<ul>
<li><strong>Authors: </strong>John Page, Xuesong Niu, Kai Wu, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05823">https://arxiv.org/abs/2601.05823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05823">https://arxiv.org/pdf/2601.05823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05823]] Boosting Latent Diffusion Models via Disentangled Representation Alignment(https://arxiv.org/abs/2601.05823)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.</li>
</ul>

<h3>Title: GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras</h3>
<ul>
<li><strong>Authors: </strong>Weimin Liu, Wenjun Wang, Joshua H. Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05839">https://arxiv.org/abs/2601.05839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05839">https://arxiv.org/pdf/2601.05839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05839]] GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras(https://arxiv.org/abs/2601.05839)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.</li>
</ul>

<h3>Title: Kidney Cancer Detection Using 3D-Based Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jen Dusseljee, Sarah de Boer, Alessa Hering</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05852">https://arxiv.org/abs/2601.05852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05852">https://arxiv.org/pdf/2601.05852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05852]] Kidney Cancer Detection Using 3D-Based Latent Diffusion Models(https://arxiv.org/abs/2601.05852)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, anomaly</a></li>
<li><strong>Abstract: </strong>In this work, we present a novel latent diffusion-based pipeline for 3D kidney anomaly detection on contrast-enhanced abdominal CT. The method combines Denoising Diffusion Probabilistic Models (DDPMs), Denoising Diffusion Implicit Models (DDIMs), and Vector-Quantized Generative Adversarial Networks (VQ-GANs). Unlike prior slice-wise approaches, our method operates directly on an image volume and leverages weak supervision with only case-level pseudo-labels. We benchmark our approach against state-of-the-art supervised segmentation and detection models. This study demonstrates the feasibility and promise of 3D latent diffusion for weakly supervised anomaly detection. While the current results do not yet match supervised baselines, they reveal key directions for improving reconstruction fidelity and lesion localization. Our findings provide an important step toward annotation-efficient, generative modeling of complex abdominal anatomy.</li>
</ul>

<h3>Title: LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yinghan Xu, John Dingliana</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05853">https://arxiv.org/abs/2601.05853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05853">https://arxiv.org/pdf/2601.05853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05853]] LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting(https://arxiv.org/abs/2601.05853)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at this https URL</li>
</ul>

<h3>Title: Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense</h3>
<ul>
<li><strong>Authors: </strong>Vctor Mayoral-Vilches, Mara Sanz-Gmez, Francesco Balassone, Stefan Rass, Lidia Salas-Espejo, Benjamin Jablonski, Luis Javier Navarrete-Lozano, Maite del Mundo de Torres, Cristbal R. J. Veas Chavez</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05887">https://arxiv.org/abs/2601.05887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05887">https://arxiv.org/pdf/2601.05887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05887]] Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense(https://arxiv.org/abs/2601.05887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI-driven penetration testing now executes thousands of actions per hour but still lacks the strategic intuition humans apply in competitive security. To build cybersecurity superintelligence --Cybersecurity AI exceeding best human capability-such strategic intuition must be embedded into agentic reasoning processes. We present Generative Cut-the-Rope (G-CTR), a game-theoretic guidance layer that extracts attack graphs from agent's context, computes Nash equilibria with effort-aware scoring, and feeds a concise digest back into the LLM loop \emph{guiding} the agent's actions. Across five real-world exercises, G-CTR matches 70--90% of expert graph structure while running 60--245x faster and over 140x cheaper than manual analysis. In a 44-run cyber-range, adding the digest lifts success from 20.0% to 42.9%, cuts cost-per-success by 2.7x, and reduces behavioral variance by 5.2x. In Attack-and-Defense exercises, a shared digest produces the Purple agent, winning roughly 2:1 over the LLM-only baseline and 3.7:1 over independently guided teams. This closed-loop guidance is what produces the breakthrough: it reduces ambiguity, collapses the LLM's search space, suppresses hallucinations, and keeps the model anchored to the most relevant parts of the problem, yielding large gains in success rate, consistency, and reliability.</li>
</ul>

<h3>Title: Pantagruel: Unified Self-Supervised Encoders for French Text and Speech</h3>
<ul>
<li><strong>Authors: </strong>Phuong-Hang Le, Valentin Pelloin, Arnault Chatelain, Maryem Bouziane, Mohammed Ghennai, Qianwen Guan, Kirill Milintsevich, Salima Mdhaffar, Aidan Mannion, Nils Defauw, Shuyue Gu, Alexandre Audibert, Marco Dinarelli, Yannick Estve, Lorraine Goeuriot, Steffen Lalande, Nicolas Herv, Maximin Coavoux, Franois Portet, tienne Ollion, Marie Candito, Maxime Peyrard, Solange Rossato, Benjamin Lecouteux, Aurlie Nardy, Gilles Srasset, Vincent Segonne, Solne Evain, Diandra Fabre, Didier Schwab</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05911">https://arxiv.org/abs/2601.05911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05911">https://arxiv.org/pdf/2601.05911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05911]] Pantagruel: Unified Self-Supervised Encoders for French Text and Speech(https://arxiv.org/abs/2601.05911)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead of predicting modality-tailored targets such as textual tokens or speech units, Pantagruel learns contextualized target representations in the feature space, allowing modality-specific encoders to capture linguistic and acoustic regularities more effectively. Separate models are pre-trained on large-scale French corpora, including Wikipedia, OSCAR and CroissantLLM for text, together with MultilingualLibriSpeech, LeBenchmark, and INA-100k for speech. INA-100k is a newly introduced 100,000-hour corpus of French audio derived from the archives of the Institut National de l'Audiovisuel (INA), the national repository of French radio and television broadcasts, providing highly diverse audio data. We evaluate Pantagruel across a broad range of downstream tasks spanning both modalities, including those from the standard French benchmarks such as FLUE or LeBenchmark. Across these tasks, Pantagruel models show competitive or superior performance compared to strong French baselines such as CamemBERT, FlauBERT, and LeBenchmark2.0, while maintaining a shared architecture that can seamlessly handle either speech or text inputs. These results confirm the effectiveness of feature-space self-supervised objectives for French representation learning and highlight Pantagruel as a robust foundation for multimodal speech-text understanding.</li>
</ul>

<h3>Title: VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction</h3>
<ul>
<li><strong>Authors: </strong>Longbin Ji, Xiaoxiong Liu, Junyuan Shang, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05966">https://arxiv.org/abs/2601.05966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05966">https://arxiv.org/pdf/2601.05966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05966]] VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction(https://arxiv.org/abs/2601.05966)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.</li>
</ul>

<h3>Title: Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks</h3>
<ul>
<li><strong>Authors: </strong>Sahibzada Saadoon Hammad, Joaqun Huerta Guijarro, Francisco Ramos, Michael Gould Carlson, Sergio Trilles Oliver</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05984">https://arxiv.org/abs/2601.05984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05984">https://arxiv.org/pdf/2601.05984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05984]] Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks(https://arxiv.org/abs/2601.05984)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar operational and environmental characteristics. This work presents an anomaly detection framework based on the CoI paradigm by grouping sensors into communities using a fused similarity matrix that incorporates temporal correlations via Spearman coefficients, spatial proximity using Gaussian distance decay, and elevation similarities. For each community, representative stations based on the best silhouette are selected and three autoencoder architectures (BiLSTM, LSTM, and MLP) are trained using Bayesian hyperparameter optimization with expanding window cross-validation and tested on stations from the same cluster and the best representative stations of other clusters. The models are trained on normal temperature patterns of the data and anomalies are detected through reconstruction error analysis. Experimental results show a robust within-community performance across the evaluated configurations, while variations across communities are observed. Overall, the results support the applicability of community-based model sharing in reducing computational overhead and to analyse model generalisability across IoT sensor networks.</li>
</ul>

<h3>Title: CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks</h3>
<ul>
<li><strong>Authors: </strong>Isaiah J. King, Bernardo Trindade, Benjamin Bowman, H. Howie Huang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.05988">https://arxiv.org/abs/2601.05988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.05988">https://arxiv.org/pdf/2601.05988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.05988]] CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks(https://arxiv.org/abs/2601.05988)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Representing networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection. Existing works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks. However, random walk-based approaches are unable to incorporate rich edge data, while the GNN-based approaches require large amounts of memory to train. In this work, we propose extending the original insight from random walk-based skip-grams--that random walks through a graph are analogous to sentences in a corpus--to the more modern transformer-based foundation models. Using language models that take advantage of GPU optimizations, we can quickly train a graph foundation model to predict missing tokens in random walks through a network of computers. The graph foundation model is then finetuned for link prediction and used as a network anomaly detector. This new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods. This system, which we call CyberGFM, achieved state-of-the-art results on three widely used network anomaly detection datasets, delivering a up to 2$\times$ improvement in average precision. We found that CyberGFM outperforms all prior works in unsupervised link prediction for network anomaly detection, using the same number of parameters, and with equal or better efficiency than the previous best approaches.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
