<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01937">http://arxiv.org/abs/2308.01937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01937]] Training Data Protection with Compositional Diffusion Models(http://arxiv.org/abs/2308.01937)</code></li>
<li>Summary: <p>We introduce Compartmentalized Diffusion Models (CDM), a method to train
different diffusion models (or prompts) on distinct data sources and
arbitrarily compose them at inference time. The individual models can be
trained in isolation, at different times, and on different distributions and
domains and can be later composed to achieve performance comparable to a
paragon model trained on all data simultaneously. Furthermore, each model only
contains information about the subset of the data it was exposed to during
training, enabling several forms of training data protection. In particular,
CDMs are the first method to enable both selective forgetting and continual
learning for large-scale diffusion models, as well as allowing serving
customized models based on the user's access rights. CDMs also allow
determining the importance of a subset of the data in generating particular
samples.
</p></li>
</ul>

<h3>Title: A Multidimensional Analysis of Social Biases in Vision Transformers. (arXiv:2308.01948v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01948">http://arxiv.org/abs/2308.01948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01948]] A Multidimensional Analysis of Social Biases in Vision Transformers(http://arxiv.org/abs/2308.01948)</code></li>
<li>Summary: <p>The embedding spaces of image models have been shown to encode a range of
social biases such as racism and sexism. Here, we investigate specific factors
that contribute to the emergence of these biases in Vision Transformers (ViT).
Therefore, we measure the impact of training data, model architecture, and
training objectives on social biases in the learned representations of ViTs.
Our findings indicate that counterfactual augmentation training using
diffusion-based image editing can mitigate biases, but does not eliminate them.
Moreover, we find that larger models are less biased than smaller models, and
that models trained using discriminative objectives are less biased than those
trained using generative objectives. In addition, we observe inconsistencies in
the learned social biases. To our surprise, ViTs can exhibit opposite biases
when trained on the same data set using different self-supervised objectives.
Our findings give insights into the factors that contribute to the emergence of
social biases and suggests that we could achieve substantial fairness
improvements based on model design choices.
</p></li>
</ul>

<h3>Title: SDDM: Score-Decomposed Diffusion Models on Manifolds for Unpaired Image-to-Image Translation. (arXiv:2308.02154v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02154">http://arxiv.org/abs/2308.02154</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02154]] SDDM: Score-Decomposed Diffusion Models on Manifolds for Unpaired Image-to-Image Translation(http://arxiv.org/abs/2308.02154)</code></li>
<li>Summary: <p>Recent score-based diffusion models (SBDMs) show promising results in
unpaired image-to-image translation (I2I). However, existing methods, either
energy-based or statistically-based, provide no explicit form of the interfered
intermediate generative distributions. This work presents a new
score-decomposed diffusion model (SDDM) on manifolds to explicitly optimize the
tangled distributions during image generation. SDDM derives manifolds to make
the distributions of adjacent time steps separable and decompose the score
function or energy guidance into an image ``denoising" part and a content
``refinement" part. To refine the image in the same noise level, we equalize
the refinement parts of the score function and energy guidance, which permits
multi-objective optimization on the manifold. We also leverage the block
adaptive instance normalization module to construct manifolds with lower
dimensions but still concentrated with the perturbed reference image. SDDM
outperforms existing SBDM-based methods with much fewer diffusion steps on
several I2I benchmarks.
</p></li>
</ul>

<h3>Title: Painterly Image Harmonization using Diffusion Model. (arXiv:2308.02228v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02228">http://arxiv.org/abs/2308.02228</a></li>
<li>Code URL: https://github.com/bcmi/phdiffusion-painterly-image-harmonization</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02228]] Painterly Image Harmonization using Diffusion Model(http://arxiv.org/abs/2308.02228)</code></li>
<li>Summary: <p>Painterly image harmonization aims to insert photographic objects into
paintings and obtain artistically coherent composite images. Previous methods
for this task mainly rely on inference optimization or generative adversarial
network, but they are either very time-consuming or struggling at fine control
of the foreground objects (e.g., texture and content details). To address these
issues, we propose a novel Painterly Harmonization stable Diffusion model
(PHDiffusion), which includes a lightweight adaptive encoder and a Dual Encoder
Fusion (DEF) module. Specifically, the adaptive encoder and the DEF module
first stylize foreground features within each encoder. Then, the stylized
foreground features from both encoders are combined to guide the harmonization
process. During training, besides the noise loss in diffusion model, we
additionally employ content loss and two style losses, i.e., AdaIN style loss
and contrastive style loss, aiming to balance the trade-off between style
migration and content preservation. Compared with the state-of-the-art models
from related fields, our PHDiffusion can stylize the foreground more
sufficiently and simultaneously retain finer content. Our code and model are
available at https://github.com/bcmi/PHDiffusion-Painterly-Image-Harmonization.
</p></li>
</ul>

<h3>Title: Diffusion-Augmented Depth Prediction with Sparse Annotations. (arXiv:2308.02283v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02283">http://arxiv.org/abs/2308.02283</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02283]] Diffusion-Augmented Depth Prediction with Sparse Annotations(http://arxiv.org/abs/2308.02283)</code></li>
<li>Summary: <p>Depth estimation aims to predict dense depth maps. In autonomous driving
scenes, sparsity of annotations makes the task challenging. Supervised models
produce concave objects due to insufficient structural information. They
overfit to valid pixels and fail to restore spatial structures. Self-supervised
methods are proposed for the problem. Their robustness is limited by pose
estimation, leading to erroneous results in natural scenes. In this paper, we
propose a supervised framework termed Diffusion-Augmented Depth Prediction
(DADP). We leverage the structural characteristics of diffusion model to
enforce depth structures of depth models in a plug-and-play manner. An
object-guided integrality loss is also proposed to further enhance regional
structure integrality by fetching objective information. We evaluate DADP on
three driving benchmarks and achieve significant improvements in depth
structures and robustness. Our work provides a new perspective on depth
estimation with sparse annotations in autonomous driving scenes.
</p></li>
</ul>

<h3>Title: Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling. (arXiv:2308.02157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02157">http://arxiv.org/abs/2308.02157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02157]] Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling(http://arxiv.org/abs/2308.02157)</code></li>
<li>Summary: <p>Efficient differential equation solvers have significantly reduced the
sampling time of diffusion models (DMs) while retaining high sampling quality.
Among these solvers, exponential integrators (EI) have gained prominence by
demonstrating state-of-the-art performance. However, existing high-order
EI-based sampling algorithms rely on degenerate EI solvers, resulting in
inferior error bounds and reduced accuracy in contrast to the theoretically
anticipated results under optimal settings. This situation makes the sampling
quality extremely vulnerable to seemingly innocuous design choices such as
timestep schedules. For example, an inefficient timestep scheduler might
necessitate twice the number of steps to achieve a quality comparable to that
obtained through carefully optimized timesteps. To address this issue, we
reevaluate the design of high-order differential solvers for DMs. Through a
thorough order analysis, we reveal that the degeneration of existing high-order
EI solvers can be attributed to the absence of essential order conditions. By
reformulating the differential equations in DMs and capitalizing on the theory
of exponential integrators, we propose refined EI solvers that fulfill all the
order conditions, which we designate as Refined Exponential Solver (RES).
Utilizing these improved solvers, RES exhibits more favorable error bounds
theoretically and achieves superior sampling efficiency and stability in
practical applications. For instance, a simple switch from the single-step
DPM-Solver++ to our order-satisfied RES solver when Number of Function
Evaluations (NFE) $=9$, results in a reduction of numerical defects by $25.2\%$
and FID improvement of $25.4\%$ (16.77 vs 12.51) on a pre-trained ImageNet
diffusion model.
</p></li>
</ul>

<h3>Title: Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling. (arXiv:2308.02165v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02165">http://arxiv.org/abs/2308.02165</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02165]] Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling(http://arxiv.org/abs/2308.02165)</code></li>
<li>Summary: <p>The crystal diffusion variational autoencoder (CDVAE) is a machine learning
model that leverages score matching to generate realistic crystal structures
that preserve crystal symmetry. In this study, we leverage novel diffusion
probabilistic (DP) models to denoise atomic coordinates rather than adopting
the standard score matching approach in CDVAE. Our proposed DP-CDVAE model can
reconstruct and generate crystal structures whose qualities are statistically
comparable to those of the original CDVAE. Furthermore, notably, when comparing
the carbon structures generated by the DP-CDVAE model with relaxed structures
obtained from density functional theory calculations, we find that the DP-CDVAE
generated structures are remarkably closer to their respective ground states.
The energy differences between these structures and the true ground states are,
on average, 68.1 meV/atom lower than those generated by the original CDVAE.
This significant improvement in the energy accuracy highlights the
effectiveness of the DP-CDVAE model in generating crystal structures that
better represent their ground-state configurations.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Semi Supervised Meta Learning for Spatiotemporal Learning. (arXiv:2308.01916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01916">http://arxiv.org/abs/2308.01916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01916]] Semi Supervised Meta Learning for Spatiotemporal Learning(http://arxiv.org/abs/2308.01916)</code></li>
<li>Summary: <p>We approached the goal of applying meta-learning to self-supervised masked
autoencoders for spatiotemporal learning in three steps. Broadly, we seek to
understand the impact of applying meta-learning to existing state-of-the-art
representation learning architectures. Thus, we test spatiotemporal learning
through: a meta-learning architecture only, a representation learning
architecture only, and an architecture applying representation learning
alongside a meta learning architecture. We utilize the Memory Augmented Neural
Network (MANN) architecture to apply meta-learning to our framework.
Specifically, we first experiment with applying a pre-trained MAE and
fine-tuning on our small-scale spatiotemporal dataset for video reconstruction
tasks. Next, we experiment with training an MAE encoder and applying a
classification head for action classification tasks. Finally, we experiment
with applying a pre-trained MAE and fine-tune with MANN backbone for action
classification tasks.
</p></li>
</ul>

<h3>Title: Robust Self-Supervised Extrinsic Self-Calibration. (arXiv:2308.02153v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02153">http://arxiv.org/abs/2308.02153</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02153]] Robust Self-Supervised Extrinsic Self-Calibration(http://arxiv.org/abs/2308.02153)</code></li>
<li>Summary: <p>Autonomous vehicles and robots need to operate over a wide variety of
scenarios in order to complete tasks efficiently and safely. Multi-camera
self-supervised monocular depth estimation from videos is a promising way to
reason about the environment, as it generates metrically scaled geometric
predictions from visual data without requiring additional sensors. However,
most works assume well-calibrated extrinsics to fully leverage this
multi-camera setup, even though accurate and efficient calibration is still a
challenging problem. In this work, we introduce a novel method for extrinsic
calibration that builds upon the principles of self-supervised monocular depth
and ego-motion learning. Our proposed curriculum learning strategy uses
monocular depth and pose estimators with velocity supervision to estimate
extrinsics, and then jointly learns extrinsic calibration along with depth and
pose for a set of overlapping cameras rigidly attached to a moving vehicle.
Experiments on a benchmark multi-camera dataset (DDAD) demonstrate that our
method enables self-calibration in various scenes robustly and efficiently
compared to a traditional vision-based pose estimation pipeline. Furthermore,
we demonstrate the benefits of extrinsics self-calibration as a way to improve
depth prediction via joint optimization.
</p></li>
</ul>

<h3>Title: ES-MVSNet: Efficient Framework for End-to-end Self-supervised Multi-View Stereo. (arXiv:2308.02191v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02191">http://arxiv.org/abs/2308.02191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02191]] ES-MVSNet: Efficient Framework for End-to-end Self-supervised Multi-View Stereo(http://arxiv.org/abs/2308.02191)</code></li>
<li>Summary: <p>Compared to the multi-stage self-supervised multi-view stereo (MVS) method,
the end-to-end (E2E) approach has received more attention due to its concise
and efficient training pipeline. Recent E2E self-supervised MVS approaches have
integrated third-party models (such as optical flow models, semantic
segmentation models, NeRF models, etc.) to provide additional consistency
constraints, which grows GPU memory consumption and complicates the model's
structure and training pipeline. In this work, we propose an efficient
framework for end-to-end self-supervised MVS, dubbed ES-MVSNet. To alleviate
the high memory consumption of current E2E self-supervised MVS frameworks, we
present a memory-efficient architecture that reduces memory usage by 43%
without compromising model performance. Furthermore, with the novel design of
asymmetric view selection policy and region-aware depth consistency, we achieve
state-of-the-art performance among E2E self-supervised MVS methods, without
relying on third-party models for additional consistency signals. Extensive
experiments on DTU and Tanks&amp;Temples benchmarks demonstrate that the proposed
ES-MVSNet approach achieves state-of-the-art performance among E2E
self-supervised MVS methods and competitive performance to many supervised and
multi-stage self-supervised methods.
</p></li>
</ul>

<h3>Title: Class Incremental Learning with Self-Supervised Pre-Training and Prototype Learning. (arXiv:2308.02346v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02346">http://arxiv.org/abs/2308.02346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02346]] Class Incremental Learning with Self-Supervised Pre-Training and Prototype Learning(http://arxiv.org/abs/2308.02346)</code></li>
<li>Summary: <p>Deep Neural Network (DNN) has achieved great success on datasets of closed
class set. However, new classes, like new categories of social media topics,
are continuously added to the real world, making it necessary to incrementally
learn. This is hard for DNN because it tends to focus on fitting to new classes
while ignoring old classes, a phenomenon known as catastrophic forgetting.
State-of-the-art methods rely on knowledge distillation and data replay
techniques but still have limitations. In this work, we analyze the causes of
catastrophic forgetting in class incremental learning, which owes to three
factors: representation drift, representation confusion, and classifier
distortion. Based on this view, we propose a two-stage learning framework with
a fixed encoder and an incrementally updated prototype classifier. The encoder
is trained with self-supervised learning to generate a feature space with high
intrinsic dimensionality, thus improving its transferability and generality.
The classifier incrementally learns new prototypes while retaining the
prototypes of previously learned data, which is crucial in preserving the
decision boundary.Our method does not rely on preserved samples of old classes,
is thus a non-exemplar based CIL method. Experiments on public datasets show
that our method can significantly outperform state-of-the-art exemplar-based
methods when they reserved 5 examplers per class, under the incremental setting
of 10 phases, by 18.24% on CIFAR-100 and 9.37% on ImageNet100.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02463">http://arxiv.org/abs/2308.02463</a></li>
<li>Code URL: https://github.com/chaoyi-wu/radfm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02463]] Towards Generalist Foundation Model for Radiology(http://arxiv.org/abs/2308.02463)</code></li>
<li>Summary: <p>In this study, we aim to initiate the development of Radiology Foundation
Model, termed as RadFM.We consider the construction of foundational models from
the perspectives of data, model design, and evaluation thoroughly. Our
contribution can be concluded as follows: (i), we construct a large-scale
Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.
To the best of our knowledge, this is the first multi-modal dataset containing
3D medical scans. (ii), We propose an architecture that enables visually
conditioned generative pre-training, allowing for the integration of text input
interleaved with 2D or 3D medical scans to generate response for diverse
radiologic tasks. The model was initially pre-trained on MedMD and subsequently
domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD,
containing 3M radiologic visual-language pairs. (iii), we propose a new
evaluation benchmark that comprises five tasks, aiming to comprehensively
assess the capability of foundation models in handling practical clinical
problems. Our experimental results confirm that RadFM significantly outperforms
existing multi-modal foundation models. The codes, data, and model checkpoint
will all be made publicly available to promote further research and development
in the field.
</p></li>
</ul>

<h3>Title: Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text. (arXiv:2308.02357v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02357">http://arxiv.org/abs/2308.02357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02357]] Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text(http://arxiv.org/abs/2308.02357)</code></li>
<li>Summary: <p>The recent advances in large language models (LLM) and foundation models with
emergent capabilities have been shown to improve the performance of many NLP
tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs
can be used for KG construction or completion while existing KGs can be used
for different tasks such as making LLM outputs explainable or fact-checking in
Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to
evaluate the capabilities of language models to generate KGs from natural
language text guided by an ontology. Given an input ontology and a set of
sentences, the task is to extract facts from the text while complying with the
given ontology (concepts, relations, domain/range constraints) and being
faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen
with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19
ontologies and 4,860 sentences. We define seven evaluation metrics to measure
fact extraction performance, ontology conformance, and hallucinations by LLMs.
Furthermore, we provide results for two baseline models, Vicuna-13B and
Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline
results show that there is room for improvement using both Semantic Web and
Natural Language Processing techniques.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: On the Biometric Capacity of Generative Face Models. (arXiv:2308.02065v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02065">http://arxiv.org/abs/2308.02065</a></li>
<li>Code URL: https://github.com/human-analysis/capacity-generative-face-models</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02065]] On the Biometric Capacity of Generative Face Models(http://arxiv.org/abs/2308.02065)</code></li>
<li>Summary: <p>There has been tremendous progress in generating realistic faces with high
fidelity over the past few years. Despite this progress, a crucial question
remains unanswered: "Given a generative face model, how many unique identities
can it generate?" In other words, what is the biometric capacity of the
generative face model? A scientific basis for answering this question will
benefit evaluating and comparing different generative face models and establish
an upper bound on their scalability. This paper proposes a statistical approach
to estimate the biometric capacity of generated face images in a hyperspherical
feature space. We employ our approach on multiple generative models, including
unconditional generators like StyleGAN, Latent Diffusion Model, and "Generated
Photos," as well as DCFace, a class-conditional generator. We also estimate
capacity w.r.t. demographic attributes such as gender and age. Our capacity
estimates indicate that (a) under ArcFace representation at a false acceptance
rate (FAR) of 0.1%, StyleGAN3 and DCFace have a capacity upper bound of
$1.43\times10^6$ and $1.190\times10^4$, respectively; (b) the capacity reduces
drastically as we lower the desired FAR with an estimate of $1.796\times10^4$
and $562$ at FAR of 1% and 10%, respectively, for StyleGAN3; (c) there is no
discernible disparity in the capacity w.r.t gender; and (d) for some generative
models, there is an appreciable disparity in the capacity w.r.t age. Code is
available at https://github.com/human-analysis/capacity-generative-face-models.
</p></li>
</ul>

<h3>Title: You talk what you read: Understanding News Comment Behavior by Dispositional and Situational Attribution. (arXiv:2308.02168v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02168">http://arxiv.org/abs/2308.02168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02168]] You talk what you read: Understanding News Comment Behavior by Dispositional and Situational Attribution(http://arxiv.org/abs/2308.02168)</code></li>
<li>Summary: <p>Many news comment mining studies are based on the assumption that comment is
explicitly linked to the corresponding news. In this paper, we observed that
users' comments are also heavily influenced by their individual characteristics
embodied by the interaction history. Therefore, we position to understand news
comment behavior by considering both the dispositional factors from news
interaction history, and the situational factors from corresponding news. A
three-part encoder-decoder framework is proposed to model the generative
process of news comment. The resultant dispositional and situational
attribution contributes to understanding user focus and opinions, which are
validated in applications of reader-aware news summarization and news
aspect-opinion forecasting.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: UGainS: Uncertainty Guided Anomaly Instance Segmentation. (arXiv:2308.02046v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02046">http://arxiv.org/abs/2308.02046</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02046]] UGainS: Uncertainty Guided Anomaly Instance Segmentation(http://arxiv.org/abs/2308.02046)</code></li>
<li>Summary: <p>A single unexpected object on the road can cause an accident or may lead to
injuries. To prevent this, we need a reliable mechanism for finding anomalous
objects on the road. This task, called anomaly segmentation, can be a stepping
stone to safe and reliable autonomous driving. Current approaches tackle
anomaly segmentation by assigning an anomaly score to each pixel and by
grouping anomalous regions using simple heuristics. However, pixel grouping is
a limiting factor when it comes to evaluating the segmentation performance of
individual anomalous objects. To address the issue of grouping multiple anomaly
instances into one, we propose an approach that produces accurate anomaly
instance masks. Our approach centers on an out-of-distribution segmentation
model for identifying uncertain regions and a strong generalist segmentation
model for anomaly instances segmentation. We investigate ways to use uncertain
regions to guide such a segmentation model to perform segmentation of anomalous
instances. By incorporating strong object priors from a generalist model we
additionally improve the per-pixel anomaly segmentation performance. Our
approach outperforms current pixel-level anomaly segmentation methods,
achieving an AP of 80.08% and 88.98% on the Fishyscapes Lost and Found and the
RoadAnomaly validation sets respectively. Project page:
https://vision.rwth-aachen.de/ugains
</p></li>
</ul>

<h3>Title: Synthetic outlier generation for anomaly detection in autonomous driving. (arXiv:2308.02184v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.02184">http://arxiv.org/abs/2308.02184</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.02184]] Synthetic outlier generation for anomaly detection in autonomous driving(http://arxiv.org/abs/2308.02184)</code></li>
<li>Summary: <p>Anomaly detection, or outlier detection, is a crucial task in various domains
to identify instances that significantly deviate from established patterns or
the majority of data. In the context of autonomous driving, the identification
of anomalies is particularly important to prevent safety-critical incidents, as
deep learning models often exhibit overconfidence in anomalous or outlier
samples. In this study, we explore different strategies for training an image
semantic segmentation model with an anomaly detection module. By introducing
modifications to the training stage of the state-of-the-art DenseHybrid model,
we achieve significant performance improvements in anomaly detection. Moreover,
we propose a simplified detector that achieves comparable results to our
modified DenseHybrid approach, while also surpassing the performance of the
original DenseHybrid model. These findings demonstrate the efficacy of our
proposed strategies for enhancing anomaly detection in the context of
autonomous driving.
</p></li>
</ul>

<h3>Title: Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model. (arXiv:2308.01947v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01947">http://arxiv.org/abs/2308.01947</a></li>
<li>Code URL: https://github.com/whb605/gladst</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01947]] Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model(http://arxiv.org/abs/2308.01947)</code></li>
<li>Summary: <p>Different from the current node-level anomaly detection task, the goal of
graph-level anomaly detection is to find abnormal graphs that significantly
differ from others in a graph set. Due to the scarcity of research on the work
of graph-level anomaly detection, the detailed description of graph-level
anomaly is insufficient. Furthermore, existing works focus on capturing
anomalous graph information to learn better graph representations, but they
ignore the importance of an effective anomaly score function for evaluating
abnormal graphs. Thus, in this work, we first define anomalous graph
information including node and graph property anomalies in a graph set and
adopt node-level and graph-level information differences to identify them,
respectively. Then, we introduce a discriminative graph-level anomaly detection
framework with dual-students-teacher model, where the teacher model with a
heuristic loss are trained to make graph representations more divergent. Then,
two competing student models trained by normal and abnormal graphs respectively
fit graph representations of the teacher model in terms of node-level and
graph-level representation perspectives. Finally, we combine representation
errors between two student models to discriminatively distinguish anomalous
graphs. Extensive experiment analysis demonstrates that our method is effective
for the graph-level anomaly detection task on graph datasets in the real world.
</p></li>
</ul>

<h2>in-context</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
