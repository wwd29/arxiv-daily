<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-25</h1>
<h3>Title: Deep Generative Domain Adaptation with Temporal Relation Knowledge for  Cross-User Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xiaozhou Ye, Kevin I-Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14682">https://arxiv.org/abs/2403.14682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14682">https://arxiv.org/pdf/2403.14682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14682]] Deep Generative Domain Adaptation with Temporal Relation Knowledge for  Cross-User Activity Recognition(https://arxiv.org/abs/2403.14682)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In human activity recognition (HAR), the assumption that training and testing data are independent and identically distributed (i.i.d.) often fails, particularly in cross-user scenarios where data distributions vary significantly. This discrepancy highlights the limitations of conventional domain adaptation methods in HAR, which typically overlook the inherent temporal relations in time-series data. To bridge this gap, our study introduces a Conditional Variational Autoencoder with Universal Sequence Mapping (CVAE-USM) approach, which addresses the unique challenges of time-series domain adaptation in HAR by relaxing the i.i.d. assumption and leveraging temporal relations to align data distributions effectively across different users. This method combines the strengths of Variational Autoencoder (VAE) and Universal Sequence Mapping (USM) to capture and utilize common temporal patterns between users for improved activity recognition. Our results, evaluated on two public HAR datasets (OPPT and PAMAP2), demonstrate that CVAE-USM outperforms existing state-of-the-art methods, offering a more accurate and generalizable solution for cross-user activity recognition.</li>
</ul>

<h3>Title: Cyclical Log Annealing as a Learning Rate Scheduler</h3>
<ul>
<li><strong>Authors: </strong>Philip Naveen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14685">https://arxiv.org/abs/2403.14685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14685">https://arxiv.org/pdf/2403.14685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14685]] Cyclical Log Annealing as a Learning Rate Scheduler(https://arxiv.org/abs/2403.14685)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A learning rate scheduler is a predefined set of instructions for varying search stepsizes during model training processes. This paper introduces a new logarithmic method using harsh restarting of step sizes through stochastic gradient descent. Cyclical log annealing implements the restart pattern more aggressively to maybe allow the usage of more greedy algorithms on the online convex optimization framework. The algorithm was tested on the CIFAR-10 image datasets, and seemed to perform analogously with cosine annealing on large transformer-enhanced residual neural networks. Future experiments would involve testing the scheduler in generative adversarial networks and finding the best parameters for the scheduler with more experiments.</li>
</ul>

<h3>Title: Foundation Models for Time Series Analysis: A Tutorial and Survey</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14735">https://arxiv.org/abs/2403.14735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14735">https://arxiv.org/pdf/2403.14735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14735]] Foundation Models for Time Series Analysis: A Tutorial and Survey(https://arxiv.org/abs/2403.14735)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advancements in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored specifically for time series analysis. In this survey, we aim to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either the application or the pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a model-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future research exploration.</li>
</ul>

<h3>Title: A task of anomaly detection for a smart satellite Internet of things  system</h3>
<ul>
<li><strong>Authors: </strong>Zilong Shao</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14738">https://arxiv.org/abs/2403.14738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14738">https://arxiv.org/pdf/2403.14738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14738]] A task of anomaly detection for a smart satellite Internet of things  system(https://arxiv.org/abs/2403.14738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>When the equipment is working, real-time collection of environmental sensor data for anomaly detection is one of the key links to prevent industrial process accidents and network attacks and ensure system security. However, under the environment with specific real-time requirements, the anomaly detection for environmental sensors still faces the following difficulties: (1) The complex nonlinear correlation characteristics between environmental sensor data variables lack effective expression methods, and the distribution between the data is difficult to be captured. (2) it is difficult to ensure the real-time monitoring requirements by using complex machine learning models, and the equipment cost is too high. (3) Too little sample data leads to less labeled data in supervised learning. This paper proposes an unsupervised deep learning anomaly detection system. Based on the generative adversarial network and self-attention mechanism, considering the different feature information contained in the local subsequences, it automatically learns the complex linear and nonlinear dependencies between environmental sensor variables, and uses the anomaly score calculation method combining reconstruction error and discrimination error. It can monitor the abnormal points of real sensor data with high real-time performance and can run on the intelligent satellite Internet of things system, which is suitable for the real working environment. Anomaly detection outperforms baseline methods in most cases and has good interpretability, which can be used to prevent industrial accidents and cyber-attacks for monitoring environmental sensors.</li>
</ul>

<h3>Title: VURF: A General-purpose Reasoning and Self-refinement Framework for  Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, Fahad Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14743">https://arxiv.org/abs/2403.14743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14743">https://arxiv.org/pdf/2403.14743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14743]] VURF: A General-purpose Reasoning and Self-refinement Framework for  Video Understanding(https://arxiv.org/abs/2403.14743)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated the effectiveness of Large Language Models (LLMs) as reasoning modules that can deconstruct complex tasks into more manageable sub-tasks, particularly when applied to visual reasoning tasks for images. In contrast, this paper introduces a Video Understanding and Reasoning Framework (VURF) based on the reasoning power of LLMs. Ours is a novel approach to extend the utility of LLMs in the context of video tasks, leveraging their capacity to generalize from minimal input and output demonstrations within a contextual framework. By presenting LLMs with pairs of instructions and their corresponding high-level programs, we harness their contextual learning capabilities to generate executable visual programs for video understanding. To enhance program's accuracy and robustness, we implement two important strategies. Firstly, we employ a feedback-generation approach, powered by GPT-3.5, to rectify errors in programs utilizing unsupported functions. Secondly, taking motivation from recent works on self refinement of LLM outputs, we introduce an iterative procedure for improving the quality of the in-context examples by aligning the initial outputs to the outputs that would have been generated had the LLM not been bound by the structure of the in-context examples. Our results on several video-specific tasks, including visual QA, video anticipation, pose estimation and multi-video QA illustrate the efficacy of these enhancements in improving the performance of visual programming approaches for video tasks. Our Codes and data will be publicly released.</li>
</ul>

<h3>Title: StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation  from Text</h3>
<ul>
<li><strong>Authors: </strong>Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14773">https://arxiv.org/abs/2403.14773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14773">https://arxiv.org/pdf/2403.14773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14773]] StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation  from Text(https://arxiv.org/abs/2403.14773)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene, and (iii) a randomized blending approach that enables to apply a video enhancer autoregressively for infinitely long videos without inconsistencies between chunks. Experiments show that StreamingT2V generates high motion amount. In contrast, all competing image-to-video methods are prone to video stagnation when applied naively in an autoregressive manner. Thus, we propose with StreamingT2V a high-quality seamless text-to-long video generator that outperforms competitors with consistency and motion. Our code will be available at: https://github.com/Picsart-AI-Research/StreamingT2V</li>
</ul>

<h3>Title: Few-Shot Adversarial Prompt Learning on Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Zhou, Xiaobo Xia, Zhiwei Lin, Bo Han, Tongliang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14774">https://arxiv.org/abs/2403.14774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14774">https://arxiv.org/pdf/2403.14774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14774]] Few-Shot Adversarial Prompt Learning on Vision-Language Models(https://arxiv.org/abs/2403.14774)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only 1% training data.</li>
</ul>

<h3>Title: Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image  Attacking</h3>
<ul>
<li><strong>Authors: </strong>Qianyu Guo, Jiaming Fu, Yawen Lu, Dongming Gan</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14778">https://arxiv.org/abs/2403.14778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14778">https://arxiv.org/pdf/2403.14778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14778]] Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image  Attacking(https://arxiv.org/abs/2403.14778)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In Virtual Reality (VR), adversarial attack remains a significant security threat. Most deep learning-based methods for physical and digital adversarial attacks focus on enhancing attack performance by crafting adversarial examples that contain large printable distortions that are easy for human observers to identify. However, attackers rarely impose limitations on the naturalness and comfort of the appearance of the generated attack image, resulting in a noticeable and unnatural attack. To address this challenge, we propose a framework to incorporate style transfer to craft adversarial inputs of natural styles that exhibit minimal detectability and maximum natural appearance, while maintaining superior attack capabilities.</li>
</ul>

<h3>Title: Champ: Controllable and Consistent Human Image Animation with 3D  Parametric Guidance</h3>
<ul>
<li><strong>Authors: </strong>Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, Siyu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14781">https://arxiv.org/abs/2403.14781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14781">https://arxiv.org/pdf/2403.14781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14781]] Champ: Controllable and Consistent Human Image Animation with 3D  Parametric Guidance(https://arxiv.org/abs/2403.14781)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this study, we introduce a methodology for human image animation by leveraging a 3D human parametric model within a latent diffusion framework to enhance shape alignment and motion guidance in curernt human generative techniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear) model as the 3D human parametric model to establish a unified representation of body shape and pose. This facilitates the accurate capture of intricate human geometry and motion characteristics from source videos. Specifically, we incorporate rendered depth images, normal maps, and semantic maps obtained from SMPL sequences, alongside skeleton-based motion guidance, to enrich the conditions to the latent diffusion model with comprehensive 3D shape and detailed pose attributes. A multi-layer motion fusion module, integrating self-attention mechanisms, is employed to fuse the shape and motion latent representations in the spatial domain. By representing the 3D human parametric model as the motion guidance, we can perform parametric shape alignment of the human body between the reference image and the source video motion. Experimental evaluations conducted on benchmark datasets demonstrate the methodology's superior ability to generate high-quality human animations that accurately capture both pose and shape variations. Furthermore, our approach also exhibits superior generalization capabilities on the proposed wild dataset. Project page: https://fudan-generative-vision.github.io/champ.</li>
</ul>

<h3>Title: Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot  Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Bowen Jiang, Zhijun Zhuang, Shreyas S. Shivakumar, Dan Roth, Camillo J. Taylor</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14783">https://arxiv.org/abs/2403.14783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14783">https://arxiv.org/pdf/2403.14783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14783]] Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot  Visual Question Answering(https://arxiv.org/abs/2403.14783)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi-Agent VQA, to overcome the limitations of foundation models in object detection and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system's performance without fine-tuning it on specific VQA datasets, making it more practical and robust in the open world. We present preliminary experimental results under zero-shot scenarios and highlight some failure cases, offering new directions for future research.</li>
</ul>

<h3>Title: Latent Diffusion Models for Attribute-Preserving Image Anonymization</h3>
<ul>
<li><strong>Authors: </strong>Luca Piano, Pietro Basci, Fabrizio Lamberti, Lia Morra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14790">https://arxiv.org/abs/2403.14790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14790">https://arxiv.org/pdf/2403.14790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14790]] Latent Diffusion Models for Attribute-Preserving Image Anonymization(https://arxiv.org/abs/2403.14790)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative techniques for image anonymization have great potential to generate datasets that protect the privacy of those depicted in the images, while achieving high data fidelity and utility. Existing methods have focused extensively on preserving facial attributes, but failed to embrace a more comprehensive perspective that considers the scene and background into the anonymization process. This paper presents, to the best of our knowledge, the first approach to image anonymization based on Latent Diffusion Models (LDMs). Every element of a scene is maintained to convey the same meaning, yet manipulated in a way that makes re-identification difficult. We propose two LDMs for this purpose: CAMOUFLaGE-Base exploits a combination of pre-trained ControlNets, and a new controlling mechanism designed to increase the distance between the real and anonymized images. CAMOFULaGE-Light is based on the Adapter technique, coupled with an encoding designed to efficiently represent the attributes of different persons in a scene. The former solution achieves superior performance on most metrics and benchmarks, while the latter cuts the inference time in half at the cost of fine-tuning a lightweight module. We show through extensive experimental comparison that the proposed method is competitive with the state-of-the-art concerning identity obfuscation whilst better preserving the original content of the image and tackling unresolved challenges that current solutions fail to address.</li>
</ul>

<h3>Title: Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Alberto Baldrati, Davide Morelli, Marcella Cornia, Marco Bertini, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14828">https://arxiv.org/abs/2403.14828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14828">https://arxiv.org/pdf/2403.14828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14828]] Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing(https://arxiv.org/abs/2403.14828)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Fashion illustration is a crucial medium for designers to convey their creative vision and transform design concepts into tangible representations that showcase the interplay between clothing and the human body. In the context of fashion design, computer vision techniques have the potential to enhance and streamline the design process. Departing from prior research primarily focused on virtual try-on, this paper tackles the task of multimodal-conditioned fashion image editing. Our approach aims to generate human-centric fashion images guided by multimodal prompts, including text, human body poses, garment sketches, and fabric textures. To address this problem, we propose extending latent diffusion models to incorporate these multiple modalities and modifying the structure of the denoising network, taking multimodal prompts as input. To condition the proposed architecture on fabric textures, we employ textual inversion techniques and let diverse cross-attention layers of the denoising network attend to textual and texture information, thus incorporating different granularity conditioning details. Given the lack of datasets for the task, we extend two existing fashion datasets, Dress Code and VITON-HD, with multimodal annotations. Experimental evaluations demonstrate the effectiveness of our proposed approach in terms of realism and coherence concerning the provided multimodal inputs.</li>
</ul>

<h3>Title: Osmosis: RGBD Diffusion Prior for Underwater Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Opher Bar Nathan, Deborah Levy, Tali Treibitz, Dan Rosenbaum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14837">https://arxiv.org/abs/2403.14837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14837">https://arxiv.org/pdf/2403.14837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14837]] Osmosis: RGBD Diffusion Prior for Underwater Image Restoration(https://arxiv.org/abs/2403.14837)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Underwater image restoration is a challenging task because of strong water effects that increase dramatically with distance. This is worsened by lack of ground truth data of clean scenes without water. Diffusion priors have emerged as strong image restoration priors. However, they are often trained with a dataset of the desired restored output, which is not available in our case. To overcome this critical issue, we show how to leverage in-air images to train diffusion priors for underwater restoration. We also observe that only color data is insufficient, and augment the prior with a depth channel. We train an unconditional diffusion model prior on the joint space of color and depth, using standard RGBD datasets of natural outdoor scenes in air. Using this prior together with a novel guidance method based on the underwater image formation model, we generate posterior samples of clean images, removing the water effects. Even though our prior did not see any underwater images during training, our method outperforms state-of-the-art baselines for image restoration on very challenging scenes. Data, models and code are published in the project page.</li>
</ul>

<h3>Title: VidLA: Video-Language Alignment at Scale</h3>
<ul>
<li><strong>Authors: </strong>Mamshad Nayeem Rizve, Fan Fei, Jayakrishnan Unnikrishnan, Son Tran, Benjamin Z. Yao, Belinda Zeng, Mubarak Shah, Trishul Chilimbi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14870">https://arxiv.org/abs/2403.14870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14870">https://arxiv.org/pdf/2403.14870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14870]] VidLA: Video-Language Alignment at Scale(https://arxiv.org/abs/2403.14870)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In this paper, we propose VidLA, an approach for video-language alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained image-text foundation models. To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained image-text foundation models, thereby boosting the final performance. Second, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training data. To overcome it, we leverage recent LLMs to curate the largest video-language dataset to date with better visual grounding. Furthermore, unlike existing video-text datasets which only contain short clips, our dataset is enriched with video clips of varying durations to aid our temporally hierarchical data tokens in extracting better representations at varying temporal scales. Overall, empirical results show that our proposed approach surpasses state-of-the-art methods on multiple retrieval benchmarks, especially on longer videos, and performs competitively on classification benchmarks.</li>
</ul>

<h3>Title: Stance Reasoner: Zero-Shot Stance Detection on Social Media with  Explicit Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Maksym Taranukhin, Vered Shwartz, Evangelos Milios</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14895">https://arxiv.org/abs/2403.14895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14895">https://arxiv.org/pdf/2403.14895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14895]] Stance Reasoner: Zero-Shot Stance Detection on Social Media with  Explicit Reasoning(https://arxiv.org/abs/2403.14895)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Social media platforms are rich sources of opinionated content. Stance detection allows the automatic extraction of users' opinions on various topics from such content. We focus on zero-shot stance detection, where the model's success relies on (a) having knowledge about the target topic; and (b) learning general reasoning strategies that can be employed for new topics. We present Stance Reasoner, an approach to zero-shot stance detection on social media that leverages explicit reasoning over background knowledge to guide the model's inference about the document's stance on a target. Specifically, our method uses a pre-trained language model as a source of world knowledge, with the chain-of-thought in-context learning approach to generate intermediate reasoning steps. Stance Reasoner outperforms the current state-of-the-art models on 3 Twitter datasets, including fully supervised models. It can better generalize across targets, while at the same time providing explicit and interpretable explanations for its predictions.</li>
</ul>

<h3>Title: Geometric Generative Models based on Morphological Equivariant PDEs and  GANs</h3>
<ul>
<li><strong>Authors: </strong>El Hadji S. Diop, Thierno Fall, Alioune Mbengue, Mohamed Daoudi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14897">https://arxiv.org/abs/2403.14897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14897">https://arxiv.org/pdf/2403.14897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14897]] Geometric Generative Models based on Morphological Equivariant PDEs and  GANs(https://arxiv.org/abs/2403.14897)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Content and image generation consist in creating or generating data from noisy information by extracting specific features such as texture, edges, and other thin image structures. We are interested here in generative models, and two main problems are addressed. Firstly, the improvements of specific feature extraction while accounting at multiscale levels intrinsic geometric features; and secondly, the equivariance of the network to reduce its complexity and provide a geometric interpretability. To proceed, we propose a geometric generative model based on an equivariant partial differential equation (PDE) for group convolution neural networks (G-CNNs), so called PDE-G-CNNs, built on morphology operators and generative adversarial networks (GANs). Equivariant morphological PDE layers are composed of multiscale dilations and erosions formulated in Riemannian manifolds, while group symmetries are defined on a Lie group. We take advantage of the Lie group structure to properly integrate the equivariance in layers, and are able to use the Riemannian metric to solve the multiscale morphological operations. Each point of the Lie group is associated with a unique point in the manifold, which helps us derive a metric on the Riemannian manifold from a tensor field invariant under the Lie group so that the induced metric has the same symmetries. The proposed geometric morphological GAN (GM-GAN) is obtained by using the proposed morphological equivariant convolutions in PDE-G-CNNs to bring nonlinearity in classical CNNs. GM-GAN is evaluated on MNIST data and compared with GANs. Preliminary results show that GM-GAN model outperforms classical GAN.</li>
</ul>

<h3>Title: On Zero-Shot Counterspeech Generation by LLMs</h3>
<ul>
<li><strong>Authors: </strong>Punyajoy Saha, Aalok Agrawal, Abhik Jana, Chris Biemann, Animesh Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14938">https://arxiv.org/abs/2403.14938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14938">https://arxiv.org/pdf/2403.14938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14938]] On Zero-Shot Counterspeech Generation by LLMs(https://arxiv.org/abs/2403.14938)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the emergence of numerous Large Language Models (LLM), the usage of such models in various Natural Language Processing (NLP) applications is increasing extensively. Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings. In this work, we present a comprehensive analysis of the performances of four LLMs namely GPT-2, DialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech generation, which is the first of its kind. For GPT-2 and DialoGPT, we further investigate the deviation in performance with respect to the sizes (small, medium, large) of the models. On the other hand, we propose three different prompting strategies for generating different types of counterspeech and analyse the impact of such strategies on the performance of the models. Our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size. Considering type of model, GPT-2 and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT. ChatGPT are much better at generating counter speech than other models across all metrics. In terms of prompting, we find that our proposed strategies help in improving counter speech generation across all the models.</li>
</ul>

<h3>Title: STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians</h3>
<ul>
<li><strong>Authors: </strong>Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14939">https://arxiv.org/abs/2403.14939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14939">https://arxiv.org/pdf/2403.14939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14939]] STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians(https://arxiv.org/abs/2403.14939)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in pre-trained diffusion models and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained diffusion models with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view diffusion model to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video diffusion model. To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the self-attention computation. With the almost consistent multi-view sequences, we then apply the score distillation sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or fine-tuning of diffusion networks, offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video.</li>
</ul>

<h3>Title: CLIP-VQDiffusion : Langauge Free Training of Text To Image generation  using CLIP and vector quantized diffusion model</h3>
<ul>
<li><strong>Authors: </strong>Seungdae Han, Joohee Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14944">https://arxiv.org/abs/2403.14944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14944">https://arxiv.org/pdf/2403.14944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14944]] CLIP-VQDiffusion : Langauge Free Training of Text To Image generation  using CLIP and vector quantized diffusion model(https://arxiv.org/abs/2403.14944)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>There has been a significant progress in text conditional image generation models. Recent advancements in this field depend not only on improvements in model structures, but also vast quantities of text-image paired datasets. However, creating these kinds of datasets is very costly and requires a substantial amount of labor. Famous face datasets don't have corresponding text captions, making it difficult to develop text conditional image generation models on these datasets. Some research has focused on developing text to image generation models using only images without text captions. Here, we propose CLIP-VQDiffusion, which leverage the pretrained CLIP model to provide multimodal text-image representations and strong image generation capabilities. On the FFHQ dataset, our model outperformed previous state-of-the-art methods by 4.4% in clipscore and generated very realistic images even when the text was both in and out of distribution. The pretrained models and codes will soon be available at https://github.com/INFINIQ-AI1/CLIPVQDiffusion</li>
</ul>

<h3>Title: DreamFlow: High-Quality Text-to-3D Generation by Approximating  Probability Flow</h3>
<ul>
<li><strong>Authors: </strong>Kyungmin Lee, Kihyuk Sohn, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14966">https://arxiv.org/abs/2403.14966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14966">https://arxiv.org/pdf/2403.14966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14966]] DreamFlow: High-Quality Text-to-3D Generation by Approximating  Probability Flow(https://arxiv.org/abs/2403.14966)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-3D generation has been achieved through the utilization of score distillation methods: they make use of the pre-trained text-to-image (T2I) diffusion models by distilling via the diffusion model training objective. However, such an approach inevitably results in the use of random timesteps at each update, which increases the variance of the gradient and ultimately prolongs the optimization process. In this paper, we propose to enhance the text-to-3D optimization by leveraging the T2I diffusion prior in the generative sampling process with a predetermined timestep schedule. To this end, we interpret text-to3D optimization as a multi-view image-to-image translation problem, and propose a solution by approximating the probability flow. By leveraging the proposed novel optimization algorithm, we design DreamFlow, a practical three-stage coarseto-fine text-to-3D optimization framework that enables fast generation of highquality and high-resolution (i.e., 1024x1024) 3D contents. For example, we demonstrate that DreamFlow is 5 times faster than the existing state-of-the-art text-to-3D method, while producing more photorealistic 3D contents. Visit our project page (https://kyungmnlee.github.io/dreamflow.github.io/) for visualizations.</li>
</ul>

<h3>Title: Trajectory Regularization Enhances Self-Supervised Geometric  Representation</h3>
<ul>
<li><strong>Authors: </strong>Jiayun Wang, Stella X. Yu, Yubei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14973">https://arxiv.org/abs/2403.14973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14973">https://arxiv.org/pdf/2403.14973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14973]] Trajectory Regularization Enhances Self-Supervised Geometric  Representation(https://arxiv.org/abs/2403.14973)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has proven effective in learning high-quality representations for various downstream tasks, with a primary focus on semantic tasks. However, its application in geometric tasks remains underexplored, partially due to the absence of a standardized evaluation method for geometric representations. To address this gap, we introduce a new pose-estimation benchmark for assessing SSL geometric representations, which demands training without semantic or pose labels and achieving proficiency in both semantic and geometric downstream tasks. On this benchmark, we study enhancing SSL geometric representations without sacrificing semantic classification accuracy. We find that leveraging mid-layer representations improves pose-estimation performance by 10-20%. Further, we introduce an unsupervised trajectory-regularization loss, which improves performance by an additional 4% and improves generalization ability on out-of-distribution data. We hope the proposed benchmark and methods offer new insights and improvements in self-supervised geometric representation learning.</li>
</ul>

<h3>Title: Generative Active Learning for Image Synthesis Personalization</h3>
<ul>
<li><strong>Authors: </strong>Xulu Zhang, Wengyu Zhang, Xiao-Yong Wei, Jinlin Wu, Zhaoxiang Zhang, Zhen Lei, Qing Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.14987">https://arxiv.org/abs/2403.14987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.14987">https://arxiv.org/pdf/2403.14987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.14987]] Generative Active Learning for Image Synthesis Personalization(https://arxiv.org/abs/2403.14987)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a pilot study that explores the application of active learning, traditionally studied in the context of discriminative models, to generative models. We specifically focus on image synthesis personalization tasks. The primary challenge in conducting active learning on generative models lies in the open-ended nature of querying, which differs from the closed form of querying in discriminative models that typically target a single concept. We introduce the concept of anchor directions to transform the querying process into a semi-open problem. We propose a direction-based uncertainty sampling strategy to enable generative active learning and tackle the exploitation-exploration dilemma. Extensive experiments are conducted to validate the effectiveness of our approach, demonstrating that an open-source model can achieve superior performance compared to closed-source models developed by large companies, such as Google's StyleDrop. The source code is available at https://github.com/zhangxulu1996/GAL4Personalization.</li>
</ul>

<h3>Title: Toward Tiny and High-quality Facial Makeup with Data Amplify Learning</h3>
<ul>
<li><strong>Authors: </strong>Qiaoqiao Jin, Xuanhong Chen, Meiguang Jin, Ying Cheng, Rui Shi, Yucheng Zheng, Yupeng Zhu, Bingbing Ni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15033">https://arxiv.org/abs/2403.15033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15033">https://arxiv.org/pdf/2403.15033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15033]] Toward Tiny and High-quality Facial Makeup with Data Amplify Learning(https://arxiv.org/abs/2403.15033)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Contemporary makeup approaches primarily hinge on unpaired learning paradigms, yet they grapple with the challenges of inaccurate supervision (e.g., face misalignment) and sophisticated facial prompts (including face parsing, and landmark detection). These challenges prohibit low-cost deployment of facial makeup models, especially on mobile devices. To solve above problems, we propose a brand-new learning paradigm, termed "Data Amplify Learning (DAL)," alongside a compact makeup model named "TinyBeauty." The core idea of DAL lies in employing a Diffusion-based Data Amplifier (DDA) to "amplify" limited images for the model training, thereby enabling accurate pixel-to-pixel supervision with merely a handful of annotations. Two pivotal innovations in DDA facilitate the above training approach: (1) A Residual Diffusion Model (RDM) is designed to generate high-fidelity detail and circumvent the detail vanishing problem in the vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is proposed to achieve precise makeup control and combination while retaining face identity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to achieve a state-of-the-art performance without intricate face prompts. Meanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on the iPhone 13. Extensive experiments show that DAL can produce highly competitive makeup models using only 5 image pairs.</li>
</ul>

<h3>Title: ESG Classification by Implicit Rule Learning via GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Hyo Jeong Yun, Chanyoung Kim, Moonjeong Hahm, Kyuri Kim, Guijin Son</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15040">https://arxiv.org/abs/2403.15040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15040">https://arxiv.org/pdf/2403.15040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15040]] ESG Classification by Implicit Rule Learning via GPT-4(https://arxiv.org/abs/2403.15040)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Environmental, social, and governance (ESG) factors are widely adopted as higher investment return indicators. Accordingly, ongoing efforts are being made to automate ESG evaluation with language models to extract signals from massive web text easily. However, recent approaches suffer from a lack of training data, as rating agencies keep their evaluation metrics confidential. This paper investigates whether state-of-the-art language models like GPT-4 can be guided to align with unknown ESG evaluation criteria through strategies such as prompting, chain-of-thought reasoning, and dynamic in-context learning. We demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task ML-ESG-3 Impact Type track for Korean without updating the model on the provided training data. We also explore how adjusting prompts impacts the ability of language models to address financial tasks leveraging smaller models with openly available weights. We observe longer general pre-training to correlate with enhanced performance in financial downstream tasks. Our findings showcase the potential of language models to navigate complex, subjective evaluation guidelines despite lacking explicit training examples, revealing opportunities for training-free solutions for financial downstream tasks.</li>
</ul>

<h3>Title: Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning</h3>
<ul>
<li><strong>Authors: </strong>Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15048">https://arxiv.org/abs/2403.15048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15048">https://arxiv.org/pdf/2403.15048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15048]] Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning(https://arxiv.org/abs/2403.15048)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.</li>
</ul>

<h3>Title: MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition  Integration</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Wei, Qingkun Su, Long Qin, Weizhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15059">https://arxiv.org/abs/2403.15059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15059">https://arxiv.org/pdf/2403.15059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15059]] MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition  Integration(https://arxiv.org/abs/2403.15059)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in tuning-free personalized image generation based on diffusion models are impressive. However, to improve subject fidelity, existing methods either retrain the diffusion model or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency. Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism. In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds. Specifically, to simultaneously enhance text consistency and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings. CLS embeddings are used on the one hand to augment the text embeddings, and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddings, both of which are efficiently integrated into the diffusion model through the well-designed multimodal cross-attention mechanism. Additionally, MM-Diff introduces cross-attention map constraints during the training phase, ensuring flexible multi-subject image sampling during inference without any predefined inputs (e.g., layout). Extensive experiments demonstrate the superior performance of MM-Diff over other leading methods.</li>
</ul>

<h3>Title: Recent Trends in 3D Reconstruction of General Non-Rigid Scenes</h3>
<ul>
<li><strong>Authors: </strong>Raza Yunus, Jan Eric Lenssen, Michael Niemeyer, Yiyi Liao, Christian Rupprecht, Christian Theobalt, Gerard Pons-Moll, Jia-Bin Huang, Vladislav Golyanik, Eddy Ilg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15064">https://arxiv.org/abs/2403.15064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15064">https://arxiv.org/pdf/2403.15064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15064]] Recent Trends in 3D Reconstruction of General Non-Rigid Scenes(https://arxiv.org/abs/2403.15064)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reconstructing models of the real world, including 3D geometry, appearance, and motion of real scenes, is essential for computer graphics and computer vision. It enables the synthesizing of photorealistic novel views, useful for the movie industry and AR/VR applications. It also facilitates the content creation necessary in computer games and AR/VR by avoiding laborious manual design processes. Further, such models are fundamental for intelligent computing systems that need to interpret real-world scenes and actions to act and interact safely with the human world. Notably, the world surrounding us is dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a severely underconstrained and challenging problem. This state-of-the-art report (STAR) offers the reader a comprehensive summary of state-of-the-art techniques with monocular and multi-view inputs such as data from RGB and RGB-D sensors, among others, conveying an understanding of different approaches, their potential applications, and promising further research directions. The report covers 3D reconstruction of general non-rigid scenes and further addresses the techniques for scene decomposition, editing and controlling, and generalizable and generative modeling. More specifically, we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the state-of-the-art techniques by reviewing recent approaches that use traditional and machine-learning-based neural representations, including a discussion on the newly enabled applications. The STAR is concluded with a discussion of the remaining limitations and open challenges.</li>
</ul>

<h3>Title: SYNCS: Synthetic Data and Contrastive Self-Supervised Training for  Central Sulcus Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Vladyslav Zalevskyi, Kristoffer Hougaard Madsen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15121">https://arxiv.org/abs/2403.15121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15121">https://arxiv.org/pdf/2403.15121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15121]] SYNCS: Synthetic Data and Contrastive Self-Supervised Training for  Central Sulcus Segmentation(https://arxiv.org/abs/2403.15121)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders with profound societal impact. Identifying risk markers early is crucial for understanding disease progression and enabling preventive measures. The Danish High Risk and Resilience Study (VIA) focuses on understanding early disease processes, particularly in children with familial high risk (FHR). Understanding structural brain changes associated with these diseases during early stages is essential for effective interventions. The central sulcus (CS) is a prominent brain landmark related to brain regions involved in motor and sensory processing. Analyzing CS morphology can provide valuable insights into neurodevelopmental abnormalities in the FHR group. However, segmenting the central sulcus (CS) presents challenges due to its variability, especially in adolescents. This study introduces two novel approaches to improve CS segmentation: synthetic data generation to model CS variability and self-supervised pre-training with multi-task learning to adapt models to new cohorts. These methods aim to enhance segmentation performance across diverse populations, eliminating the need for extensive preprocessing.</li>
</ul>

<h3>Title: Transfer CLIP for Generalizable Image Denoising</h3>
<ul>
<li><strong>Authors: </strong>Jun Cheng, Dong Liang, Shan Tan</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15132">https://arxiv.org/abs/2403.15132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15132">https://arxiv.org/pdf/2403.15132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15132]] Transfer CLIP for Generalizable Image Denoising(https://arxiv.org/abs/2403.15132)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Image denoising is a fundamental task in computer vision. While prevailing deep learning-based supervised and self-supervised methods have excelled in eliminating in-distribution noise, their susceptibility to out-of-distribution (OOD) noise remains a significant challenge. The recent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation. Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored. This paper uncovers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising. Leveraging these properties, we devise an asymmetrical encoder-decoder denoising network, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising. The progressive feature augmentation strategy is further proposed to mitigate feature overfitting and improve the robustness of the learnable decoder. Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method.</li>
</ul>

<h3>Title: Deep Generative Model based Rate-Distortion for Image Downscaling  Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuanbang Liang, Bhavesh Garg, Paul L Rosin, Yipeng Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15139">https://arxiv.org/abs/2403.15139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15139">https://arxiv.org/pdf/2403.15139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15139]] Deep Generative Model based Rate-Distortion for Image Downscaling  Assessment(https://arxiv.org/abs/2403.15139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose Image Downscaling Assessment by Rate-Distortion (IDA-RD), a novel measure to quantitatively evaluate image downscaling algorithms. In contrast to image-based methods that measure the quality of downscaled images, ours is process-based that draws ideas from rate-distortion theory to measure the distortion incurred during downscaling. Our main idea is that downscaling and super-resolution (SR) can be viewed as the encoding and decoding processes in the rate-distortion model, respectively, and that a downscaling algorithm that preserves more details in the resulting low-resolution (LR) images should lead to less distorted high-resolution (HR) images in SR. In other words, the distortion should increase as the downscaling algorithm deteriorates. However, it is non-trivial to measure this distortion as it requires the SR algorithm to be blind and stochastic. Our key insight is that such requirements can be met by recent SR algorithms based on deep generative models that can find all matching HR images for a given LR image on their learned image manifolds. Extensive experimental results show the effectiveness of our IDA-RD measure.</li>
</ul>

<h3>Title: A Multimodal Approach for Cross-Domain Image Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Lucas Iijima, Tania Stathaki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15152">https://arxiv.org/abs/2403.15152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15152">https://arxiv.org/pdf/2403.15152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15152]] A Multimodal Approach for Cross-Domain Image Retrieval(https://arxiv.org/abs/2403.15152)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image generators are gaining vast amount of popularity and have rapidly changed how digital content is created. With the latest AI technology, millions of high quality images are being generated by the public, which are constantly motivating the research community to push the limits of generative models to create more complex and realistic images. This paper focuses on Cross-Domain Image Retrieval (CDIR) which can be used as an additional tool to inspect collections of generated images by determining the level of similarity between images in a dataset. An ideal retrieval system would be able to generalize to unseen complex images from multiple domains (e.g., photos, drawings and paintings). To address this goal, we propose a novel caption-matching approach that leverages multimodal language-vision architectures pre-trained on large datasets. The method is tested on DomainNet and Office-Home datasets and consistently achieves state-of-the-art performance over the latest approaches in the literature for cross-domain image retrieval. In order to verify the effectiveness with AI-generated images, the method was also put to test with a database composed by samples collected from Midjourney, which is a widely used generative platform for content creation.</li>
</ul>

<h3>Title: Exploring the Task-agnostic Trait of Self-supervised Learning in the  Context of Detecting Mental Disorders</h3>
<ul>
<li><strong>Authors: </strong>Rohan Kumar Gupta, Rohit Sinha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15170">https://arxiv.org/abs/2403.15170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15170">https://arxiv.org/pdf/2403.15170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15170]] Exploring the Task-agnostic Trait of Self-supervised Learning in the  Context of Detecting Mental Disorders(https://arxiv.org/abs/2403.15170)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has been investigated to generate task-agnostic representations across various domains. However, such investigation has not been conducted for detecting multiple mental disorders. The rationale behind the existence of a task-agnostic representation lies in the overlapping symptoms among multiple mental disorders. Consequently, the behavioural data collected for mental health assessment may carry a mixed bag of attributes related to multiple disorders. Motivated by that, in this study, we explore a task-agnostic representation derived through SSL in the context of detecting major depressive disorder (MDD) and post-traumatic stress disorder (PTSD) using audio and video data collected during interactive sessions. This study employs SSL models trained by predicting multiple fixed targets or masked frames. We propose a list of fixed targets to make the generated representation more efficient for detecting MDD and PTSD. Furthermore, we modify the hyper-parameters of the SSL encoder predicting fixed targets to generate global representations that capture varying temporal contexts. Both these innovations are noted to yield improved detection performances for considered mental disorders and exhibit task-agnostic traits. In the context of the SSL model predicting masked frames, the generated global representations are also noted to exhibit task-agnostic traits.</li>
</ul>

<h3>Title: Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment  Anything Model for Crowd-Sourcing Medical Image Annotations</h3>
<ul>
<li><strong>Authors: </strong>Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina Chatterjee, Paul H. Yi, Vishwa S. Parekh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15218">https://arxiv.org/abs/2403.15218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15218">https://arxiv.org/pdf/2403.15218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15218]] Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment  Anything Model for Crowd-Sourcing Medical Image Annotations(https://arxiv.org/abs/2403.15218)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in "narrowly" focused deep learning (DL) models with limited translational utility. Recently, foundation models like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional zero-shot generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing "sparse" annotations from non-experts to generate "dense" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth annotations, nnU-Net models trained on SAM-generated annotations perform significantly worse than nnU-Net models trained on ground-truth annotations ($p<0.001$, all).</li>
</ul>

<h3>Title: Shadow Generation for Composite Image Using Diffusion model</h3>
<ul>
<li><strong>Authors: </strong>Qingyang Liu, Junqi You, Jianting Wang, Xinhao Tao, Bo Zhang, Li Niu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15234">https://arxiv.org/abs/2403.15234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15234">https://arxiv.org/pdf/2403.15234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15234]] Shadow Generation for Composite Image Using Diffusion model(https://arxiv.org/abs/2403.15234)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>In the realm of image composition, generating realistic shadow for the inserted foreground remains a formidable challenge. Previous works have developed image-to-image translation models which are trained on paired training data. However, they are struggling to generate shadows with accurate shapes and intensities, hindered by data scarcity and inherent task complexity. In this paper, we resort to foundation model with rich prior knowledge of natural shadow images. Specifically, we first adapt ControlNet to our task and then propose intensity modulation modules to improve the shadow intensity. Moreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel data acquisition pipeline. Experimental results on both DESOBA and DESOBAv2 datasets as well as real composite images demonstrate the superior capability of our model for shadow generation task. The dataset, code, and model are released at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.</li>
</ul>

<h3>Title: Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Sudhir Sornapudi (1), Rajhans Singh (1) ((1) Corteva Agriscience, Indianapolis, USA)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15248">https://arxiv.org/abs/2403.15248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15248">https://arxiv.org/pdf/2403.15248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15248]] Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks(https://arxiv.org/abs/2403.15248)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Computer vision in agriculture is game-changing with its ability to transform farming into a data-driven, precise, and sustainable industry. Deep learning has empowered agriculture vision to analyze vast, complex visual data, but heavily rely on the availability of large annotated datasets. This remains a bottleneck as manual labeling is error-prone, time-consuming, and expensive. The lack of efficient labeling approaches inspired us to consider self-supervised learning as a paradigm shift, learning meaningful feature representations from raw agricultural image data. In this work, we explore how self-supervised representation learning unlocks the potential applicability to diverse agriculture vision tasks by eliminating the need for large-scale annotated datasets. We propose a lightweight framework utilizing SimCLR, a contrastive learning approach, to pre-train a ResNet-50 backbone on a large, unannotated dataset of real-world agriculture field images. Our experimental analysis and results indicate that the model learns robust features applicable to a broad range of downstream agriculture tasks discussed in the paper. Additionally, the reduced reliance on annotated data makes our approach more cost-effective and accessible, paving the way for broader adoption of computer vision in agriculture.</li>
</ul>

<h3>Title: Spectral Motion Alignment for Video Motion Transfer using Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15249">https://arxiv.org/abs/2403.15249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15249">https://arxiv.org/pdf/2403.15249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15249]] Spectral Motion Alignment for Video Motion Transfer using Diffusion  Models(https://arxiv.org/abs/2403.15249)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while maintaining computational efficiency and compatibility across various video customization frameworks.</li>
</ul>

<h3>Title: Parametric PDE Control with Deep Reinforcement Learning and  Differentiable L0-Sparse Polynomial Policies</h3>
<ul>
<li><strong>Authors: </strong>Nicol Botteghi, Urban Fasel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15267">https://arxiv.org/abs/2403.15267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15267">https://arxiv.org/pdf/2403.15267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15267]] Parametric PDE Control with Deep Reinforcement Learning and  Differentiable L0-Sparse Polynomial Policies(https://arxiv.org/abs/2403.15267)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Optimal control of parametric partial differential equations (PDEs) is crucial in many applications in engineering and science. In recent years, the progress in scientific machine learning has opened up new frontiers for the control of parametric PDEs. In particular, deep reinforcement learning (DRL) has the potential to solve high-dimensional and complex control problems in a large variety of applications. Most DRL methods rely on deep neural network (DNN) control policies. However, for many dynamical systems, DNN-based control policies tend to be over-parametrized, which means they need large amounts of training data, show limited robustness, and lack interpretability. In this work, we leverage dictionary learning and differentiable L$_0$ regularization to learn sparse, robust, and interpretable control policies for parametric PDEs. Our sparse policy architecture is agnostic to the DRL method and can be used in different policy-gradient and actor-critic DRL algorithms without changing their policy-optimization procedure. We test our approach on the challenging tasks of controlling parametric Kuramoto-Sivashinsky and convection-diffusion-reaction PDEs. We show that our method (1) outperforms baseline DNN-based DRL policies, (2) allows for the derivation of interpretable equations of the learned optimal control laws, and (3) generalizes to unseen parameters of the PDE without retraining the policies.</li>
</ul>

<h3>Title: Controlled Training Data Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15309">https://arxiv.org/abs/2403.15309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15309">https://arxiv.org/pdf/2403.15309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15309]] Controlled Training Data Generation with Diffusion Models(https://arxiv.org/abs/2403.15309)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this work, we present a method to control a text-to-image generative model to produce training data specifically "useful" for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, datasets and architectures, with different types of distribution shifts (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches.</li>
</ul>

<h3>Title: Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for  Weakly Semi-supervised 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, Jiaming Liu, Shanghang Zhang, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15317">https://arxiv.org/abs/2403.15317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15317">https://arxiv.org/pdf/2403.15317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15317]] Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for  Weakly Semi-supervised 3D Object Detection(https://arxiv.org/abs/2403.15317)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for object localization.In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget.Different from Point-DETR which encodes 3D positional information solely through a point encoder, we propose an explicit positional query initialization strategy to enhance the positional prior. Considering the low quality of pseudo labels at distant regions produced by the teacher model, we enhance the detector's perception by incorporating dense imagery data through a novel Cross-Modal Deformable RoI Fusion (D-RoI).Moreover, an innovative point-guided self-supervised learning technique is proposed to allow for fully exploiting point priors, even in student models.Extensive experiments on representative nuScenes dataset demonstrate our Point-DETR3D obtains significant improvements compared to previous works. Notably, with only 5% of labeled data, Point-DETR3D achieves over 90% performance of its fully supervised counterpart.</li>
</ul>

<h3>Title: Multi-Review Fusion-in-Context</h3>
<ul>
<li><strong>Authors: </strong>Aviv Slobodkin, Ori Shapira, Ran Levy, Ido Dagan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15351">https://arxiv.org/abs/2403.15351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15351">https://arxiv.org/pdf/2403.15351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15351]] Multi-Review Fusion-in-Context(https://arxiv.org/abs/2403.15351)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness. Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize \textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information. Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment. Several baseline models exhibit promising outcomes and provide insightful analyses. This study lays the groundwork for further exploration of modular text generation in the multi-document setting, offering potential improvements in the quality and reliability of generated content. \footnote{Our benchmark, FuseReviews, including the dataset, evaluation framework and designated leaderboard, can be found at \url{https://fusereviews.github.io/}.}</li>
</ul>

<h3>Title: Neural Plasticity-Inspired Foundation Model for Observing the Earth  Crossing Modalities</h3>
<ul>
<li><strong>Authors: </strong>Zhitong Xiong, Yi Wang, Fahong Zhang, Adam J. Stewart, Jolle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15356">https://arxiv.org/abs/2403.15356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15356">https://arxiv.org/pdf/2403.15356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15356]] Neural Plasticity-Inspired Foundation Model for Observing the Earth  Crossing Modalities(https://arxiv.org/abs/2403.15356)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The development of foundation models has revolutionized our ability to interpret the Earth's surface using satellite observational data. Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyperspectral, each with its own unique characteristics. This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources. Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively. This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile Transformer jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining. DOFA's innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of multimodal Earth observation data.</li>
</ul>

<h3>Title: CoLLEGe: Concept Embedding Generation for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ryan Teehan, Brenden Lake, Mengye Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15362">https://arxiv.org/abs/2403.15362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15362">https://arxiv.org/pdf/2403.15362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15362]] CoLLEGe: Concept Embedding Generation for Large Language Models(https://arxiv.org/abs/2403.15362)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal reasoning, and demonstrate that our method succeeds in each setting without task-specific training.</li>
</ul>

<h3>Title: Can large language models explore in-context?</h3>
<ul>
<li><strong>Authors: </strong>Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15371">https://arxiv.org/abs/2403.15371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15371">https://arxiv.org/pdf/2403.15371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15371]] Can large language models explore in-context?(https://arxiv.org/abs/2403.15371)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history. Although these findings can be interpreted positively, they suggest that external summarization -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from LLM agents. We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings.</li>
</ul>

<h3>Title: InternVideo2: Scaling Video Foundation Models for Multimodal Video  Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15377">https://arxiv.org/abs/2403.15377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15377">https://arxiv.org/pdf/2403.15377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15377]] InternVideo2: Scaling Video Foundation Models for Multimodal Video  Understanding(https://arxiv.org/abs/2403.15377)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce InternVideo2, a new video foundation model (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue. Our approach employs a progressive training paradigm that unifies the different self- or weakly-supervised learning frameworks of masked video token reconstruction, cross-modal contrastive learning, and next token prediction. Different training stages would guide our model to capture different levels of structure and semantic information through different pretext tasks. At the data level, we prioritize the spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. We scale both data and model size for our InternVideo2. Through extensive experiments, we validate our designs and demonstrate the state-of-the-art performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related captioning, dialogue, and long video understanding benchmarks, highlighting its ability to reason and comprehend long temporal contexts. Code and models are available at https://github.com/OpenGVLab/InternVideo2/.</li>
</ul>

<h3>Title: LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15385">https://arxiv.org/abs/2403.15385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15385">https://arxiv.org/pdf/2403.15385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15385]] LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis(https://arxiv.org/abs/2403.15385)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per prompt. Amortized methods like ATT3D optimize multiple prompts simultaneously to improve efficiency, enabling fast text-to-3D synthesis. However, they cannot capture high-frequency geometry and texture details and struggle to scale to large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger prompt set. Key to our method is 1) building a scalable architecture and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training prompts. LATTE3D amortizes both neural field and textured surface generation to produce highly detailed textured meshes in a single forward pass. LATTE3D generates 3D objects in 400ms, and can be further enhanced with fast test-time optimization.</li>
</ul>

<h3>Title: DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from  Partially Annotated Data</h3>
<ul>
<li><strong>Authors: </strong>Hanrong Ye, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.15389">https://arxiv.org/abs/2403.15389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.15389">https://arxiv.org/pdf/2403.15389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.15389]] DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from  Partially Annotated Data(https://arxiv.org/abs/2403.15389)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, there has been an increased interest in the practical problem of learning multiple dense scene understanding tasks from partially annotated data, where each training sample is only labeled for a subset of the tasks. The missing of task labels in training leads to low-quality and noisy predictions, as can be observed from state-of-the-art methods. To tackle this issue, we reformulate the partially-labeled multi-task dense prediction as a pixel-level denoising problem, and propose a novel multi-task denoising diffusion framework coined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to model a potential noisy distribution in the task prediction or feature maps and generate rectified outputs for different tasks. To exploit multi-task consistency in denoising, we further introduce a Multi-Task Conditioning strategy, which can implicitly utilize the complementary nature of the tasks to help learn the unlabeled tasks, leading to an improvement in the denoising performance of the different tasks. Extensive quantitative and qualitative experiments demonstrate that the proposed multi-task denoising diffusion model can significantly improve multi-task prediction maps, and outperform the state-of-the-art methods on three challenging multi-task benchmarks, under two different partial-labeling evaluation settings. The code is available at https://prismformore.github.io/diffusionmtl/.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
