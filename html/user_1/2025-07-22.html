<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-07-22</h1>
<h3>Title: Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI</h3>
<ul>
<li><strong>Authors: </strong>Julien Pourcel, Cédric Colas, Pierre-Yves Oudeyer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14172">https://arxiv.org/abs/2507.14172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14172">https://arxiv.org/pdf/2507.14172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14172]] Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI(https://arxiv.org/abs/2507.14172)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many program synthesis tasks prove too challenging for even state-of-the-art language models to solve in single attempts. Search-based evolutionary methods offer a promising alternative by exploring solution spaces iteratively, but their effectiveness remain limited by the fixed capabilities of the underlying generative model. We propose SOAR, a method that learns program synthesis by integrating language models into a self-improving evolutionary loop. SOAR alternates between (1) an evolutionary search that uses an LLM to sample and refine candidate solutions, and (2) a hindsight learning phase that converts search attempts into valid problem-solution pairs used to fine-tune the LLM's sampling and refinement capabilities\, -- \,enabling increasingly effective search in subsequent iterations. On the challenging ARC-AGI benchmark, SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our code is open-sourced at: this https URL</li>
</ul>

<h3>Title: LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, Xin Dong, Jan Kautz, Pavlo Molchanov, Yingyan (Celine)Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14204">https://arxiv.org/abs/2507.14204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14204">https://arxiv.org/pdf/2507.14204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14204]] LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models(https://arxiv.org/abs/2507.14204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have spurred interest in numerous applications requiring robust long-range capabilities, essential for processing extensive input contexts and continuously generating extended outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in LLMs escalates, creating a significant efficiency bottleneck. In this paper, we propose a new KV cache optimization paradigm called LaCache, a training-free method for efficient and accurate generative inference of LLMs. LaCache enables LLMs to simultaneously address both of the critical challenges in long-range modeling: robust long-range capabilities and continuous generation without running out-of-memory (OOM). Specifically, LaCache integrates two key innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only sequentially (left-to-right within each layer) but also across layers (from shallow to deep), providing an extended span for capturing long-range dependencies under a fixed storage budget, thereby boosting long-range capabilities; and (2) an iterative compaction mechanism that progressively compresses older caches, freeing up space for new tokens within a fixed cache size. This token distance-based dynamic compression enables more effective continuous generation under constrained cache budgets. Experiments across various tasks, benchmarks, and LLM models consistently validate LaCache's effectiveness in enhancing LLMs' long-range capabilities. Our code is available at this https URL.</li>
</ul>

<h3>Title: A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions</h3>
<ul>
<li><strong>Authors: </strong>Hengjie Yu, Kenneth A. Dawson, Haiyun Yang, Shuya Liu, Yan Yan, Yaochu Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI, cs.CE, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14245">https://arxiv.org/abs/2507.14245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14245">https://arxiv.org/pdf/2507.14245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14245]] A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions(https://arxiv.org/abs/2507.14245)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Unlocking the potential of nanomaterials in medicine and environmental science hinges on understanding their interactions with proteins, a complex decision space where AI is poised to make a transformative impact. However, progress has been hindered by limited datasets and the restricted generalizability of existing models. Here, we propose NanoPro-3M, the largest nanomaterial-protein interaction dataset to date, comprising over 3.2 million samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer, a foundational model that predicts nanomaterial-protein affinities through multimodal representation learning, demonstrating strong generalization, handling missing features, and unseen nanomaterials or proteins. We show that multimodal modeling significantly outperforms single-modality approaches and identifies key determinants of corona formation. Furthermore, we demonstrate its applicability to a range of downstream tasks through zero-shot inference and fine-tuning. Together, this work establishes a solid foundation for high-performance and generalized prediction of nanomaterial-protein interaction endpoints, reducing experimental reliance and accelerating various in vitro applications.</li>
</ul>

<h3>Title: Linearized Diffusion Map</h3>
<ul>
<li><strong>Authors: </strong>Julio Candanedo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14257">https://arxiv.org/abs/2507.14257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14257">https://arxiv.org/pdf/2507.14257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14257]] Linearized Diffusion Map(https://arxiv.org/abs/2507.14257)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce the Linearized Diffusion Map (LDM), a novel linear dimensionality reduction method constructed via a linear approximation of the diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based nonlinear methods with the computational simplicity, efficiency, and interpretability inherent in linear embeddings such as PCA and classical MDS. Through comprehensive experiments on synthetic datasets (Swiss roll and hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that LDM captures distinct geometric features of datasets compared to PCA, offering complementary advantages. Specifically, LDM embeddings outperform PCA in datasets exhibiting explicit manifold structures, particularly in high-dimensional regimes, whereas PCA remains preferable in scenarios dominated by variance or noise. Furthermore, the complete positivity of LDM's kernel matrix allows direct applicability of Non-negative Matrix Factorization (NMF), suggesting opportunities for interpretable latent-structure discovery. Our analysis positions LDM as a valuable new linear dimensionality reduction technique with promising theoretical and practical extensions.</li>
</ul>

<h3>Title: What Makes You CLIC: Detection of Croatian Clickbait Headlines</h3>
<ul>
<li><strong>Authors: </strong>Marija Anđedelić, Dominik Šipek, Laura Majer, Jan Šnajder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14314">https://arxiv.org/abs/2507.14314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14314">https://arxiv.org/pdf/2507.14314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14314]] What Makes You CLIC: Detection of Croatian Clickbait Headlines(https://arxiv.org/abs/2507.14314)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Online news outlets operate predominantly on an advertising-based revenue model, compelling journalists to create headlines that are often scandalous, intriguing, and provocative -- commonly referred to as clickbait. Automatic detection of clickbait headlines is essential for preserving information quality and reader trust in digital media and requires both contextual understanding and world knowledge. For this task, particularly in less-resourced languages, it remains unclear whether fine-tuned methods or in-context learning (ICL) yield better results. In this paper, we compile CLIC, a novel dataset for clickbait detection of Croatian news headlines spanning a 20-year period and encompassing mainstream and fringe outlets. We fine-tune the BERTić model on this task and compare its performance to LLM-based ICL methods with prompts both in Croatian and English. Finally, we analyze the linguistic properties of clickbait. We find that nearly half of the analyzed headlines contain clickbait, and that finetuned models deliver better results than general LLMs.</li>
</ul>

<h3>Title: Rethinking Individual Fairness in Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Aryana Hou, Li Lin, Justin Li, Shu Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14326">https://arxiv.org/abs/2507.14326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14326">https://arxiv.org/pdf/2507.14326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14326]] Rethinking Individual Fairness in Deepfake Detection(https://arxiv.org/abs/2507.14326)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI models have substantially improved the realism of synthetic media, yet their misuse through sophisticated DeepFakes poses significant risks. Despite recent advances in deepfake detection, fairness remains inadequately addressed, enabling deepfake markers to exploit biases against specific populations. While previous studies have emphasized group-level fairness, individual fairness (i.e., ensuring similar predictions for similar individuals) remains largely unexplored. In this work, we identify for the first time that the original principle of individual fairness fundamentally fails in the context of deepfake detection, revealing a critical gap previously unexplored in the literature. To mitigate it, we propose the first generalizable framework that can be integrated into existing deepfake detectors to enhance individual fairness and generalization. Extensive experiments conducted on leading deepfake datasets demonstrate that our approach significantly improves individual fairness while maintaining robust detection performance, outperforming state-of-the-art methods. The code is available at this https URL.</li>
</ul>

<h3>Title: Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers</h3>
<ul>
<li><strong>Authors: </strong>Harsh Nilesh Pathak, Randy Paffenroth</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14353">https://arxiv.org/abs/2507.14353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14353">https://arxiv.org/pdf/2507.14353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14353]] Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers(https://arxiv.org/abs/2507.14353)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parameter efficient fine tuning (PEFT) is a versatile and extensible approach for adapting a Large Language Model (LLM) for newer tasks. One of the most prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on adjusting the attention weight matrices within individual decoder blocks of a Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo Connection a novel method that adapts the representation at the decoder-block level rather than modifying individual weight matrices. Not only does Solo Connection outperform LoRA on E2E natural language generation benchmarks, but it also reduces the number of trainable parameters by 59% relative to LoRA and by more than 99% compared to full fine-tuning of GPT2, an early version of Large Language Models (LLMs). Solo Connection is also motivated by homotopy theory: we introduce a trainable linear transformation that gradually interpolates between a zero vector and the task-specific representation, enabling smooth and stable adaptation over time. While skip connections in the original 12 layer GPT2 are typically confined to individual decoder blocks, subsequent GPT2 variants scale up to 48 layers, and even larger language models can include 128 or more decoder blocks. These expanded architectures underscore the need to revisit how skip connections are employed during fine-tuning. This paper focuses on long skip connections that link outputs of different decoder blocks, potentially enhancing the model's ability to adapt to new tasks while leveraging pre-trained knowledge.</li>
</ul>

<h3>Title: Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Weiming Ren, Raghav Goyal, Zhiming Hu, Tristan Ty Aumentado-Armstrong, Iqbal Mohomed, Alex Levinshtein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14367">https://arxiv.org/abs/2507.14367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14367">https://arxiv.org/pdf/2507.14367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14367]] Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution(https://arxiv.org/abs/2507.14367)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative super-resolution (GSR) currently sets the state-of-the-art in terms of perceptual image quality, overcoming the "regression-to-the-mean" blur of prior non-generative models. However, from a human perspective, such models do not fully conform to the optimal balance between quality and fidelity. Instead, a different class of artifacts, in which generated details fail to perceptually match the low resolution image (LRI) or ground-truth image (GTI), is a critical but under studied issue in GSR, limiting its practical deployments. In this work, we focus on measuring, analyzing, and mitigating these artifacts (i.e., "hallucinations"). We observe that hallucinations are not well-characterized with existing image metrics or quality models, as they are orthogonal to both exact fidelity and no-reference quality. Instead, we take advantage of a multimodal large language model (MLLM) by constructing a prompt that assesses hallucinatory visual elements and generates a "Hallucination Score" (HS). We find that our HS is closely aligned with human evaluations, and also provides complementary insights to prior image metrics used for super-resolution (SR) models. In addition, we find certain deep feature distances have strong correlations with HS. We therefore propose to align the GSR models by using such features as differentiable reward functions to mitigate hallucinations.</li>
</ul>

<h3>Title: Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures</h3>
<ul>
<li><strong>Authors: </strong>Arun Vignesh Malarkkan, Dongjie Wang, Haoyue Bai, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14387">https://arxiv.org/abs/2507.14387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14387">https://arxiv.org/pdf/2507.14387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14387]] Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures(https://arxiv.org/abs/2507.14387)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The escalating threat of cyberattacks on real-time critical infrastructures poses serious risks to public safety, demanding detection methods that effectively capture complex system interdependencies and adapt to evolving attack patterns. Traditional real-time anomaly detection techniques often suffer from excessive false positives due to their statistical sensitivity to high data variance and class imbalance. To address these limitations, recent research has explored modeling causal relationships among system components. However, prior work mainly focuses on offline causal graph-based approaches that require static historical data and fail to generalize to real-time settings. These methods are fundamentally constrained by: (1) their inability to adapt to dynamic shifts in data distribution without retraining, and (2) the risk of catastrophic forgetting when lacking timely supervision in live systems. To overcome these challenges, we propose INCADET, a novel framework for incremental causal graph learning tailored to real-time cyberattack detection. INCADET dynamically captures evolving system behavior by incrementally updating causal graphs across streaming time windows. The framework comprises three modules: 1) Early Symptom Detection: Detects transitions in system status using divergence in edge-weight distributions across sequential causal graphs. 2) Incremental Causal Graph Learning: Leverages experience replay and edge reinforcement to continually refine causal structures while preserving prior knowledge. 3) Causal Graph Classification: Employs Graph Convolutional Networks (GCNs) to classify system status using the learned causal graphs. Extensive experiments on real-world critical infrastructure datasets demonstrate that INCADET achieves superior accuracy, robustness, and adaptability compared to both static causal and deep temporal baselines in evolving attack scenarios.</li>
</ul>

<h3>Title: ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions</h3>
<ul>
<li><strong>Authors: </strong>Yule Li, Yifeng Lu, Zhen Wang, Zhewei Wei, Yaliang Li, Bolin Ding</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14484">https://arxiv.org/abs/2507.14484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14484">https://arxiv.org/pdf/2507.14484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14484]] ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions(https://arxiv.org/abs/2507.14484)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, graph neural networks (GNN) have achieved unprecedented successes in node classification tasks. Although GNNs inherently encode specific inductive biases (e.g., acting as low-pass or high-pass filters), most existing methods implicitly assume conditional independence among node labels in their optimization objectives. While this assumption is suitable for traditional classification tasks such as image recognition, it contradicts the intuitive observation that node labels in graphs remain correlated, even after conditioning on the graph structure. To make structured predictions for node labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for Structured node Classification. ReDiSC estimates the joint distribution of node labels using a reparameterized masked diffusion model, which is learned through the variational expectation-maximization (EM) framework. Our theoretical analysis shows the efficiency advantage of ReDiSC in the E-step compared to DPM-SNC, a state-of-the-art model that relies on a manifold-constrained diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's M-step objective to popular GNN and label propagation hybrid approaches. Extensive experiments demonstrate that ReDiSC achieves superior or highly competitive performance compared to state-of-the-art GNN, label propagation, and diffusion-based baselines across both homophilic and heterophilic graphs of varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on which previous structured diffusion methods fail due to computational constraints, highlighting its significant practical advantage in structured node classification tasks.</li>
</ul>

<h3>Title: Efficient Whole Slide Pathology VQA via Token Compression</h3>
<ul>
<li><strong>Authors: </strong>Weimin Lyu, Qingqiao Hu, Kehan Qi, Zhan Shi, Wentao Huang, Saumya Gupta, Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14497">https://arxiv.org/abs/2507.14497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14497">https://arxiv.org/pdf/2507.14497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14497]] Efficient Whole Slide Pathology VQA via Token Compression(https://arxiv.org/abs/2507.14497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000 pixels, posing significant challenges for multimodal large language model (MLLM) due to long context length and high computational demands. Previous methods typically focus on patch-level analysis or slide-level classification using CLIP-based models with multi-instance learning, but they lack the generative capabilities needed for visual question answering (VQA). More recent MLLM-based approaches address VQA by feeding thousands of patch tokens directly into the language model, which leads to excessive resource consumption. To address these limitations, we propose Token Compression Pathology LLaVA (TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token compression. TCP-LLaVA introduces a set of trainable compression tokens that aggregate visual and textual information through a modality compression module, inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are forwarded to the LLM for answer generation, significantly reducing input length and computational cost. Experiments on ten TCGA tumor subtypes show that TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing training resource consumption by a substantial margin.</li>
</ul>

<h3>Title: Generative Distribution Distillation</h3>
<ul>
<li><strong>Authors: </strong>Jiequan Cui, Beier Zhu, Qingshan Xu, Xiaogang Xu, Pengguang Chen, Xiaojuan Qi, Bei Yu, Hanwang Zhang, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14503">https://arxiv.org/abs/2507.14503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14503">https://arxiv.org/pdf/2507.14503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14503]] Generative Distribution Distillation(https://arxiv.org/abs/2507.14503)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we formulate the knowledge distillation (KD) as a conditional generative problem and propose the \textit{Generative Distribution Distillation (GenDD)} framework. A naive \textit{GenDD} baseline encounters two major challenges: the curse of high-dimensional optimization and the lack of semantic supervision from labels. To address these issues, we introduce a \textit{Split Tokenization} strategy, achieving stable and effective unsupervised KD. Additionally, we develop the \textit{Distribution Contraction} technique to integrate label supervision into the reconstruction objective. Our theoretical proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction} serves as a gradient-level surrogate for multi-task learning, realizing efficient supervised training without explicit classification loss on multi-step sampling image representations. To evaluate the effectiveness of our method, we conduct experiments on balanced, imbalanced, and unlabeled data. Experimental results show that \textit{GenDD} performs competitively in the unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%} on ImageNet validation set. With label supervision, our ResNet-50 achieves \textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training, establishing a new state-of-the-art.</li>
</ul>

<h3>Title: SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Jeyoung Lee, Hochul Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14516">https://arxiv.org/abs/2507.14516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14516">https://arxiv.org/pdf/2507.14516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14516]] SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning(https://arxiv.org/abs/2507.14516)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric function for time series self-supervised representation learning. Most Self-Supervised Learning (SSL) methods for signals commonly adopt distance-based objectives such as mean squared error (MSE), which are sensitive to amplitude, invariant to waveform polarity, and unbounded in scale. These properties hinder semantic alignment and reduce interpretability. SDSC addresses this by quantifying structural agreement between temporal signals based on the intersection of signed amplitudes, derived from the Dice Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware metric, it can be used as a loss by subtracting from 1 and applying a differentiable approximation of the Heaviside function for gradient-based optimization. A hybrid loss formulation is also proposed to combine SDSC with MSE, improving stability and preserving amplitude where necessary. Experiments on forecasting and classification benchmarks demonstrate that SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios. The results suggest that structural fidelity in signal representations enhances the semantic representation quality, supporting the consideration of structure-aware metrics as viable alternatives to conventional distance-based methods.</li>
</ul>

<h3>Title: Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025</h3>
<ul>
<li><strong>Authors: </strong>Sujata Gaihre, Amir Thapa Magar, Prasuna Pokharel, Laxmi Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14544">https://arxiv.org/abs/2507.14544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14544">https://arxiv.org/pdf/2507.14544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14544]] Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025(https://arxiv.org/abs/2507.14544)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA 2025 Challenge, which targets visual question answering (VQA) for gastrointestinal endoscopy. We adopt the Florence model-a large-scale multimodal foundation model-as the backbone of our VQA pipeline, pairing a powerful vision encoder with a text encoder to interpret endoscopic images and produce clinically relevant answers. To improve generalization, we apply domain-specific augmentations that preserve medical features while increasing training diversity. Experiments on the KASVIR dataset show that fine-tuning Florence yields accurate responses on the official challenge metrics. Our results highlight the potential of large multimodal models in medical VQA and provide a strong baseline for future work on explainability, robustness, and clinical integration. The code is publicly available at: this https URL</li>
</ul>

<h3>Title: Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance</h3>
<ul>
<li><strong>Authors: </strong>Xiaoran Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14553">https://arxiv.org/abs/2507.14553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14553">https://arxiv.org/pdf/2507.14553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14553]] Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance(https://arxiv.org/abs/2507.14553)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Clutter in photos is a distraction preventing photographers from conveying the intended emotions or stories to the audience. Photography amateurs frequently include clutter in their photos due to unconscious negligence or the lack of experience in creating a decluttered, aesthetically appealing scene for shooting. We are thus motivated to develop a camera guidance system that provides solutions and guidance for clutter identification and removal. We estimate and visualize the contribution of objects to the overall aesthetics and content of a photo, based on which users can interactively identify clutter. Suggestions on getting rid of clutter, as well as a tool that removes cluttered objects computationally, are provided to guide users to deal with different kinds of clutter and improve their photographic work. Two technical novelties underpin interactions in our system: a clutter distinguishment algorithm with aesthetics evaluations for objects and an iterative image inpainting algorithm based on generative adversarial nets that reconstructs missing regions of removed objects for high-resolution images. User studies demonstrate that our system provides flexible interfaces and accurate algorithms that allow users to better identify distractions and take higher quality images within less time.</li>
</ul>

<h3>Title: LEAD: Exploring Logit Space Evolution for Model Selection</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Hu, Xiaotong Li, Shixiang Tang, Jun Liu, Yichun Hu, Ling-Yu Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14559">https://arxiv.org/abs/2507.14559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14559">https://arxiv.org/pdf/2507.14559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14559]] LEAD: Exploring Logit Space Evolution for Model Selection(https://arxiv.org/abs/2507.14559)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The remarkable success of pretrain-then-finetune paradigm has led to a proliferation of available pre-trained models for vision tasks. This surge presents a significant challenge in efficiently choosing the most suitable pre-trained models for downstream tasks. The critical aspect of this challenge lies in effectively predicting the model transferability by considering the underlying fine-tuning dynamics. Existing methods often model fine-tuning dynamics in feature space with linear transformations, which do not precisely align with the fine-tuning objective and fail to grasp the essential nonlinearity from optimization. To this end, we present LEAD, a finetuning-aligned approach based on the network output of logits. LEAD proposes a theoretical framework to model the optimization process and derives an ordinary differential equation (ODE) to depict the nonlinear evolution toward the final logit state. Additionally, we design a class-aware decomposition method to consider the varying evolution dynamics across classes and further ensure practical applicability. Integrating the closely aligned optimization objective and nonlinear modeling capabilities derived from the differential equation, our method offers a concise solution to effectively bridge the optimization gap in a single step, bypassing the lengthy fine-tuning process. The comprehensive experiments on 24 supervised and self-supervised pre-trained models across 10 downstream datasets demonstrate impressive performances and showcase its broad adaptability even in low-data scenarios.</li>
</ul>

<h3>Title: Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation</h3>
<ul>
<li><strong>Authors: </strong>Andrea Moschetto, Lemuel Puglisi, Alec Sargood, Pierluigi Dell'Acqua, Francesco Guarnera, Sebastiano Battiato, Daniele Ravì</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14575">https://arxiv.org/abs/2507.14575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14575">https://arxiv.org/pdf/2507.14575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14575]] Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation(https://arxiv.org/abs/2507.14575)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at this https URL.</li>
</ul>

<h3>Title: XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification</h3>
<ul>
<li><strong>Authors: </strong>Sachin Yadav, Dominik Schlechtweg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14578">https://arxiv.org/abs/2507.14578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14578">https://arxiv.org/pdf/2507.14578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14578]] XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification(https://arxiv.org/abs/2507.14578)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We propose XL-DURel, a finetuned, multilingual Sentence Transformer model optimized for ordinal Word-in-Context classification. We test several loss functions for regression and ranking tasks managing to outperform previous models on ordinal and binary data with a ranking objective based on angular distance in complex space. We further show that binary WiC can be treated as a special case of ordinal WiC and that optimizing models for the general ordinal task improves performance on the more specific binary task. This paves the way for a unified treatment of WiC modeling across different task formulations.</li>
</ul>

<h3>Title: Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification</h3>
<ul>
<li><strong>Authors: </strong>Łukasz Radliński, Mateusz Guściora, Jan Kocoń</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14590">https://arxiv.org/abs/2507.14590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14590">https://arxiv.org/pdf/2507.14590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14590]] Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification(https://arxiv.org/abs/2507.14590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Numerous domain-specific machine learning tasks struggle with data scarcity and class imbalance. This paper systematically explores data augmentation methods for NLP, particularly through large language models like GPT. The purpose of this paper is to examine and evaluate whether traditional methods such as paraphrasing and backtranslation can leverage a new generation of models to achieve comparable performance to purely generative methods. Methods aimed at solving the problem of data scarcity and utilizing ChatGPT were chosen, as well as an exemplary dataset. We conducted a series of experiments comparing four different approaches to data augmentation in multiple experimental setups. We then evaluated the results both in terms of the quality of generated data and its impact on classification performance. The key findings indicate that backtranslation and paraphrasing can yield comparable or even better results than zero and a few-shot generation of examples.</li>
</ul>

<h3>Title: A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification</h3>
<ul>
<li><strong>Authors: </strong>Haochen Liu, Jia Bi, Xiaomin Wang, Xin Yang, Ling Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14592">https://arxiv.org/abs/2507.14592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14592">https://arxiv.org/pdf/2507.14592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14592]] A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification(https://arxiv.org/abs/2507.14592)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance, logistics, agriculture, disaster management, and military operations. Accurate detection and classification of UAV flight states, such as hovering, cruising, ascending, or transitioning, which are essential for safe and effective operations. However, conventional time series classification (TSC) methods often lack robustness and generalization for dynamic UAV environments, while state of the art(SOTA) models like Transformers and LSTM based architectures typically require large datasets and entail high computational costs, especially with high-dimensional data streams. This paper proposes a novel framework that integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET) to address these challenges in UAV flight state classification. The Transformer encoder captures long-range temporal dependencies and complex telemetry dynamics, while the GAN module augments limited datasets with realistic synthetic samples. MIL is incorporated to focus attention on the most discriminative input segments, reducing noise and computational overhead. Experimental results show that the proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and 98.6% on the DroneRF dataset that outperforming other SOTA approaches. The framework also demonstrates strong computational efficiency and robust generalization across diverse UAV platforms and flight states, highlighting its potential for real-time deployment in resource constrained environments.</li>
</ul>

<h3>Title: VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Juntao Tan, Anran Li, Quanchao Liu, Peng Ran, Lan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14625">https://arxiv.org/abs/2507.14625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14625">https://arxiv.org/pdf/2507.14625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14625]] VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning(https://arxiv.org/abs/2507.14625)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Vertical federated learning (VFL) enables multiple parties with disjoint features to collaboratively train models without sharing raw data. While privacy vulnerabilities of VFL are extensively-studied, its security threats-particularly targeted label attacks-remain underexplored. In such attacks, a passive party perturbs inputs at inference to force misclassification into adversary-chosen labels. Existing methods rely on unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore anomaly detectors deployed in real-world systems. To bridge this gap, we introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly designed to evade detector-enhanced VFL inference. During the preparation stage, the attacker selects a minimal set of high-expressiveness samples (via maximum mean discrepancy), submits them through VFL protocol to collect predicted labels, and uses these pseudo-labels to train estimated detector and surrogate model on local features. In attack stage, these models guide gradient-based perturbations of remaining samples, crafting adversarial instances that induce targeted misclassifications and evade detection. We implement VTarbel and evaluate it against four model architectures, seven multimodal datasets, and two anomaly detectors. Across all settings, VTarbel outperforms four state-of-the-art baselines, evades detection, and retains effective against three representative privacy-preserving defenses. These results reveal critical security blind spots in current VFL deployments and underscore urgent need for robust, attack-aware defenses.</li>
</ul>

<h3>Title: BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM</h3>
<ul>
<li><strong>Authors: </strong>Haiquan Wen, Tianxiao Li, Zhenglin Huang, Yiwei He, Guangliang Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14632">https://arxiv.org/abs/2507.14632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14632">https://arxiv.org/pdf/2507.14632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14632]] BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM(https://arxiv.org/abs/2507.14632)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative AI have dramatically improved image and video synthesis capabilities, significantly increasing the risk of misinformation through sophisticated fake content. In response, detection methods have evolved from traditional approaches to multimodal large language models (MLLMs), offering enhanced transparency and interpretability in identifying synthetic media. However, current detection systems remain fundamentally limited by their single-modality design. These approaches analyze images or videos separately, making them ineffective against synthetic content that combines multiple media formats. To address these challenges, we introduce \textbf{BusterX++}, a novel framework designed specifically for cross-modal detection and explanation of synthetic media. Our approach incorporates an advanced reinforcement learning (RL) post-training strategy that eliminates cold-start. Through Multi-stage Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and substantial performance improvements. To enable comprehensive evaluation, we also present \textbf{GenBuster++}, a cross-modal benchmark leveraging state-of-the-art image and video generation techniques. This benchmark comprises 4,000 images and video clips, meticulously curated by human experts using a novel filtering methodology to ensure high quality, diversity, and real-world applicability. Extensive experiments demonstrate the effectiveness and generalizability of our approach.</li>
</ul>

<h3>Title: Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yiming Xu, Zhen Peng, Bin Shi, Xu Hua, Bo Dong, Song Wang, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14677">https://arxiv.org/abs/2507.14677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14677">https://arxiv.org/pdf/2507.14677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14677]] Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective(https://arxiv.org/abs/2507.14677)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The superiority of graph contrastive learning (GCL) has prompted its application to anomaly detection tasks for more powerful risk warning systems. Unfortunately, existing GCL-based models tend to excessively prioritize overall detection performance while neglecting robustness to structural imbalance, which can be problematic for many real-world networks following power-law degree distributions. Particularly, GCL-based methods may fail to capture tail anomalies (abnormal nodes with low degrees). This raises concerns about the security and robustness of current anomaly detection algorithms and therefore hinders their applicability in a variety of realistic high-risk scenarios. To the best of our knowledge, research on the robustness of graph anomaly detection to structural imbalance has received little scrutiny. To address the above issues, this paper presents a novel GCL-based framework named AD-GCL. It devises the neighbor pruning strategy to filter noisy edges for head nodes and facilitate the detection of genuine tail nodes by aligning from head nodes to forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to enlarge the receptive field of tail nodes through anomaly-guided neighbor completion. We further introduce intra- and inter-view consistency loss of the original and augmentation graph for enhanced representation. The performance evaluation of the whole, head, and tail nodes on multiple datasets validates the comprehensive superiority of the proposed AD-GCL in detecting both head anomalies and tail anomalies.</li>
</ul>

<h3>Title: GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks</h3>
<ul>
<li><strong>Authors: </strong>Zixin Xu, Zhijie Wang, Zhiyuan Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14679">https://arxiv.org/abs/2507.14679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14679">https://arxiv.org/pdf/2507.14679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14679]] GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks(https://arxiv.org/abs/2507.14679)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The exponential growth of spam text on the Internet necessitates robust detection mechanisms to mitigate risks such as information leakage and social instability. This work addresses two principal challenges: adversarial strategies employed by spammers and the scarcity of labeled data. We propose a novel spam-text detection framework GCC-Spam, which integrates three core innovations. First, a character similarity network captures orthographic and phonetic features to counter character-obfuscation attacks and furthermore produces sentence embeddings for downstream classification. Second, contrastive learning enhances discriminability by optimizing the latent-space distance between spam and normal texts. Third, a Generative Adversarial Network (GAN) generates realistic pseudo-spam samples to alleviate data scarcity while improving model robustness and classification accuracy. Extensive experiments on real-world datasets demonstrate that our model outperforms baseline approaches, achieving higher detection rates with significantly fewer labeled examples.</li>
</ul>

<h3>Title: From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition</h3>
<ul>
<li><strong>Authors: </strong>Chen Cai, Tianyi Liu, Jianjun Gao, Wenyang Liu, Kejun Wu, Ruoyu Wang, Yi Wang, Soo Chin Liew</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14686">https://arxiv.org/abs/2507.14686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14686">https://arxiv.org/pdf/2507.14686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14686]] From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition(https://arxiv.org/abs/2507.14686)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot abilities but struggle with complex Grounded Situation Recognition (GSR) and are resource-intensive for edge device deployment. Meanwhile, conventional GSR models often lack generalization ability, falling short in recognizing unseen and rare situations. In this paper, we exploit transferring knowledge from a teacher MLLM to a small GSR model to enhance its generalization and zero-shot abilities, thereby introducing the task of Open-vocabulary Grounded Situation Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt Distillation (MIPD), a novel framework that distills enriched multimodal knowledge from the foundation model, enabling the student Ov-GSR model to recognize unseen situations and be better aware of rare situations. Specifically, the MIPD framework first leverages the LLM-based Judgmental Rationales Generator (JRG) to construct positive and negative glimpse and gaze rationales enriched with contextual semantic information. The proposed scene-aware and instance-perception prompts are then introduced to align rationales with visual information from the MLLM teacher via the Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively capturing holistic and perceptual multimodal knowledge. Finally, the aligned multimodal knowledge is distilled into the student Ov-GSR model, providing a stronger foundation for generalization that enhances situation understanding, bridges the gap between seen and unseen scenarios, and mitigates prediction bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving superior performance on seen, rare, and unseen situations, and further demonstrate improved unseen detection on the HICO-DET dataset.</li>
</ul>

<h3>Title: Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling</h3>
<ul>
<li><strong>Authors: </strong>Claudio Giusti, Luca Guarnera, Mirko Casu, Sebastiano Battiato</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14706">https://arxiv.org/abs/2507.14706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14706">https://arxiv.org/pdf/2507.14706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14706]] Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling(https://arxiv.org/abs/2507.14706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Detecting fraudulent credit card transactions remains a significant challenge, due to the extreme class imbalance in real-world data and the often subtle patterns that separate fraud from legitimate activity. Existing research commonly attempts to address this by generating synthetic samples for the minority class using approaches such as GANs, VAEs, or hybrid generative models. However, these techniques, particularly when applied only to minority-class data, tend to result in overconfident classifiers and poor latent cluster separation, ultimately limiting real-world detection performance. In this study, we propose the Causal Prototype Attention Classifier (CPAC), an interpretable architecture that promotes class-aware clustering and improved latent space structure through prototype-based attention mechanisms and we will couple it with the encoder in a VAE-GAN allowing it to offer a better cluster separation moving beyond post-hoc sample augmentation. We compared CPAC-augmented models to traditional oversamplers, such as SMOTE, as well as to state-of-the-art generative models, both with and without CPAC-based latent classifiers. Our results show that classifier-guided latent shaping with CPAC delivers superior performance, achieving an F1-score of 93.14\% percent and recall of 90.18\%, along with improved latent cluster separation. Further ablation studies and visualizations provide deeper insight into the benefits and limitations of classifier-driven representation learning for fraud detection. The codebase for this work will be available at final submission.</li>
</ul>

<h3>Title: Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems</h3>
<ul>
<li><strong>Authors: </strong>Rachid Karami, Rajeev Patwari, Hyoukjun Kwon, Ashish Sirasao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14715">https://arxiv.org/abs/2507.14715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14715">https://arxiv.org/pdf/2507.14715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14715]] Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems(https://arxiv.org/abs/2507.14715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The integration of generative AI models, particularly large language models (LLMs), into real-time multi-model AI applications such as video conferencing and gaming is giving rise to a new class of workloads: real-time generative AI (RTGen). These workloads combine the compute intensity and dynamic execution patterns of generative models with the stringent latency and concurrency constraints of real-time inference. To meet the diverse demands of RTGen workloads, modern edge platforms increasingly adopt heterogeneous system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite the potential of heterogeneous SoC, the scheduling space complexity and performance implications of RTGen workloads on such platforms remain underexplored. In this work, we perform a comprehensive characterization of RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct realistic multi-model scenarios inspired by industry use cases and profile model performance across all available backends. Using this data, we evaluate five scheduling policies and their impact on both real-time metrics (e.g., deadline violation rate) and LLM performance (e.g., time-to-first-token and tokens-per-second). Our results show that scheduling decisions significantly affect workload performance (e.g., leading to a 41.7% difference in deadline violation rates on average), and highlight the need for scheduling strategies that are aware of workload dynamics and hardware heterogeneity. Our findings underscore the importance of workload-aware, dynamic heterogeneous scheduling in enabling high-performance, on-device RTGen applications.</li>
</ul>

<h3>Title: Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Patrik Reizinger, Bálint Mucsányi, Siyuan Guo, Benjamin Eysenbach, Bernhard Schölkopf, Wieland Brendel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14748">https://arxiv.org/abs/2507.14748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14748">https://arxiv.org/pdf/2507.14748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14748]] Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning(https://arxiv.org/abs/2507.14748)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised feature learning and pretraining methods in reinforcement learning (RL) often rely on information-theoretic principles, termed mutual information skill learning (MISL). These methods aim to learn a representation of the environment while also incentivizing exploration thereof. However, the role of the representation and mutual information parametrization in MISL is not yet well understood theoretically. Our work investigates MISL through the lens of identifiable representation learning by focusing on the Contrastive Successor Features (CSF) method. We prove that CSF can provably recover the environment's ground-truth features up to a linear transformation due to the inner product parametrization of the features and skill diversity in a discriminative sense. This first identifiability guarantee for representation learning in RL also helps explain the implications of different mutual information objectives and the downsides of entropy regularizers. We empirically validate our claims in MuJoCo and DeepMind Control and show how CSF provably recovers the ground-truth features both from states and pixels.</li>
</ul>

<h3>Title: GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Luyi Ma, Wanjia Zhang, Kai Zhao, Abhishek Kulkarni, Lalitesh Morishetti, Anjana Ganesh, Ashish Ranjan, Aashika Padmanabhan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sumit Dutta, Kamiya Motwani, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14758">https://arxiv.org/abs/2507.14758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14758">https://arxiv.org/pdf/2507.14758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14758]] GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization(https://arxiv.org/abs/2507.14758)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have recently demonstrated strong potential in multi-behavior recommendation systems, leveraging the expressive power of transformers and tokenization to generate personalized item sequences. However, their adoption is hindered by (1) the lack of explicit information for token reasoning, (2) high computational costs due to quadratic attention complexity and dense sequence representations after tokenization, and (3) limited multi-scale modeling over user history. In this work, we propose GRACE (Generative Recommendation via journey-aware sparse Attention on Chain-of-thought tokEnization), a novel generative framework for multi-behavior sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT) tokenization method that encodes user-item interactions with explicit attributes from product knowledge graphs (e.g., category, brand, price) over semantic tokenization, enabling interpretable and behavior-aligned generation. To address the inefficiency of standard attention, we design a Journey-Aware Sparse Attention (JSA) mechanism, which selectively attends to compressed, intra-, inter-, and current-context segments in the tokenized sequence. Experiments on two real-world datasets show that GRACE significantly outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and +106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces attention computation by up to 48% with long sequences.</li>
</ul>

<h3>Title: Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards</h3>
<ul>
<li><strong>Authors: </strong>Derek Li, Jiaming Zhou, Amirreza Kazemi, Qianyi Sun, Abbas Ghaddar, Mohammad Ali Alomrani, Liheng Ma, Yu Luo, Dong Li, Feng Wen, Jianye Hao, Mark Coates, Yingxue Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14783">https://arxiv.org/abs/2507.14783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14783">https://arxiv.org/pdf/2507.14783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14783]] Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards(https://arxiv.org/abs/2507.14783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The advancement of general-purpose artificial intelligence relies on large language models (LLMs) that excel across a wide range of tasks, from structured reasoning to creative generation. However, post-training methods like Supervised Fine-Tuning (SFT) often struggle with generalization, favoring memorization over transferable learning. In this work, we introduce Omni-Think, a unified reinforcement learning (RL) framework that enhances LLM performance across diverse tasks by combining rule-based verifiable rewards with generative preference signals via LLM-as-a-Judge evaluations. Our approach enables consistent optimization across task types and scales RL-based training to subjective domains. We further investigate training strategies, demonstrating that a curriculum-based progression that orders tasks from structured to open-ended improves performance and reduces forgetting. Experimental results across four domains reveal that curriculum learning improves performance by 5.2\% over joint training and 9.1\% over model merging. These results highlight the importance of task-aware sampling and hybrid supervision in scaling RL-based post-training for general-purpose LLMs.</li>
</ul>

<h3>Title: Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs</h3>
<ul>
<li><strong>Authors: </strong>Erfan Pirmorad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14785">https://arxiv.org/abs/2507.14785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14785">https://arxiv.org/pdf/2507.14785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14785]] Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs(https://arxiv.org/abs/2507.14785)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The complexity and interconnectivity of entities involved in money laundering demand investigative reasoning over graph-structured data. This paper explores the use of large language models (LLMs) as reasoning engines over localized subgraphs extracted from a financial knowledge graph. We propose a lightweight pipeline that retrieves k-hop neighborhoods around entities of interest, serializes them into structured text, and prompts an LLM via few-shot in-context learning to assess suspiciousness and generate justifications. Using synthetic anti-money laundering (AML) scenarios that reflect common laundering behaviors, we show that LLMs can emulate analyst-style logic, highlight red flags, and provide coherent explanations. While this study is exploratory, it illustrates the potential of LLM-based graph reasoning in AML and lays groundwork for explainable, language-driven financial crime analytics.</li>
</ul>

<h3>Title: Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Beier Zhu, Ruoyu Wang, Tong Zhao, Hanwang Zhang, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14797">https://arxiv.org/abs/2507.14797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14797">https://arxiv.org/pdf/2507.14797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14797]] Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models(https://arxiv.org/abs/2507.14797)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face image quality degradation under a low-latency budget. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as \ours), a novel ODE solver that mitigates truncation errors by incorporating multiple parallel gradient evaluations in each ODE step. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling. Our method optimizes a small set of learnable parameters in a distillation fashion, ensuring minimal training overhead. In addition, our method can serve as a plugin to improve existing ODE samplers. Extensive experiments on various image synthesis benchmarks demonstrate the effectiveness of our \ours~in achieving high-quality and low-latency sampling. For example, at the same latency level of 5 NFE, EPD achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26 on LSUN Bedroom, surpassing existing learning-based solvers by a significant margin. Codes are available in this https URL.</li>
</ul>

<h3>Title: SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaji Zhang, Ruichao Sun, Hailiang Zhao, Jiaju Wu, Peng Chen, Hao Li, Xinkui Zhao, Kingsum Chow, Gang Xiong, Lin Ye, Shuiguang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14811">https://arxiv.org/abs/2507.14811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14811">https://arxiv.org/pdf/2507.14811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14811]] SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models(https://arxiv.org/abs/2507.14811)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated exceptional generative capabilities but are computationally intensive, posing significant challenges for deployment in resource-constrained or latency-sensitive environments. Quantization offers an effective means to reduce model size and computational cost, with post-training quantization (PTQ) being particularly appealing due to its compatibility with pre-trained models without requiring retraining or training data. However, existing PTQ methods for diffusion models often rely on architecture-specific heuristics that limit their generalizability and hinder integration with industrial deployment pipelines. To address these limitations, we propose SegQuant, a unified quantization framework that adaptively combines complementary techniques to enhance cross-model versatility. SegQuant consists of a segment-aware, graph-based quantization strategy (SegLinear) that captures structural semantics and spatial heterogeneity, along with a dual-scale quantization scheme (DualScale) that preserves polarity-asymmetric activations, which is crucial for maintaining visual fidelity in generated outputs. SegQuant is broadly applicable beyond Transformer-based diffusion models, achieving strong performance while ensuring seamless compatibility with mainstream deployment tools.</li>
</ul>

<h3>Title: Benchmarking Foundation Models with Multimodal Public Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Kunyu Yu, Rui Yang, Jingchi Liao, Siqi Li, Huitao Li, Irene Li, Yifan Peng, Rishikesan Kamaleswaran, Nan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14824">https://arxiv.org/abs/2507.14824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14824">https://arxiv.org/pdf/2507.14824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14824]] Benchmarking Foundation Models with Multimodal Public Electronic Health Records(https://arxiv.org/abs/2507.14824)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have emerged as a powerful approach for processing electronic health records (EHRs), offering flexibility to handle diverse medical data modalities. In this study, we present a comprehensive benchmark that evaluates the performance, fairness, and interpretability of foundation models, both as unimodal encoders and as multimodal learners, using the publicly available MIMIC-IV database. To support consistent and reproducible evaluation, we developed a standardized data processing pipeline that harmonizes heterogeneous clinical records into an analysis-ready format. We systematically compared eight foundation models, encompassing both unimodal and multimodal models, as well as domain-specific and general-purpose variants. Our findings demonstrate that incorporating multiple data modalities leads to consistent improvements in predictive performance without introducing additional bias. Through this benchmark, we aim to support the development of effective and trustworthy multimodal artificial intelligence (AI) systems for real-world clinical applications. Our code is available at this https URL.</li>
</ul>

<h3>Title: Paired Image Generation with Diffusion-Guided Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Zhang, Wenju Cui, Yuzhu Cao, Tao Tan, Jie Liu, Yunsong Peng, Jian Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14833">https://arxiv.org/abs/2507.14833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14833">https://arxiv.org/pdf/2507.14833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14833]] Paired Image Generation with Diffusion-Guided Diffusion Models(https://arxiv.org/abs/2507.14833)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The segmentation of mass lesions in digital breast tomosynthesis (DBT) images is very significant for the early screening of breast cancer. However, the high-density breast tissue often leads to high concealment of the mass lesions, which makes manual annotation difficult and time-consuming. As a result, there is a lack of annotated data for model training. Diffusion models are commonly used for data augmentation, but the existing methods face two challenges. First, due to the high concealment of lesions, it is difficult for the model to learn the features of the lesion area. This leads to the low generation quality of the lesion areas, thus limiting the quality of the generated images. Second, existing methods can only generate images and cannot generate corresponding annotations, which restricts the usability of the generated images in supervised training. In this work, we propose a paired image generation method. The method does not require external conditions and can achieve the generation of paired images by training an extra diffusion guider for the conditional diffusion model. During the experimental phase, we generated paired DBT slices and mass lesion masks. Then, we incorporated them into the supervised training process of the mass lesion segmentation task. The experimental results show that our method can improve the generation quality without external conditions. Moreover, it contributes to alleviating the shortage of annotated data, thus enhancing the performance of downstream tasks.</li>
</ul>

<h3>Title: Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Rizhao Fan, Zhigen Li, Heping Li, Ning An</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14845">https://arxiv.org/abs/2507.14845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14845">https://arxiv.org/pdf/2507.14845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14845]] Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image(https://arxiv.org/abs/2507.14845)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Depth completion is an important vision task, and many efforts have been made to enhance the quality of depth maps from sparse depth measurements. Despite significant advances, training these models to recover dense depth from sparse measurements remains a challenging problem. Supervised learning methods rely on dense depth labels to predict unobserved regions, while self-supervised approaches require image sequences to enforce geometric constraints and photometric consistency between frames. However, acquiring dense annotations is costly, and multi-frame dependencies limit the applicability of self-supervised methods in static or single-frame scenarios. To address these challenges, we propose a novel self-supervised depth completion paradigm that requires only sparse depth measurements and their corresponding image for training. Unlike existing methods, our approach eliminates the need for dense depth labels or additional images captured from neighboring viewpoints. By leveraging the characteristics of depth distribution, we design novel loss functions that effectively propagate depth information from observed points to unobserved regions. Additionally, we incorporate segmentation maps generated by vision foundation models to further enhance depth estimation. Extensive experiments demonstrate the effectiveness of our proposed method.</li>
</ul>

<h3>Title: Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yifei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14849">https://arxiv.org/abs/2507.14849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14849">https://arxiv.org/pdf/2507.14849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14849]] Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding(https://arxiv.org/abs/2507.14849)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Reasoning distillation has emerged as an effective approach to enhance the reasoning capabilities of smaller language models. However, the impact of large-scale reasoning distillation on other critical abilities, particularly in-context retrieval and reasoning, remains unexplored. This gap in understanding is particularly significant given the increasing importance of Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and utilization of contextual information are paramount for generating reliable responses. Motivated by the need to understand how the extended long-CoT process influences long-context comprehension, we conduct a comprehensive investigation using a series of open-source models distilled from Deepseek-R1, renowned for its exceptional reasoning capabilities. Our study focuses on evaluating these models' performance in extracting and integrating relevant information from extended contexts through multi-document question and answering tasks. Through rigorous experimentation, we demonstrate that distilled reasoning patterns significantly improve long-context understanding. Our analysis reveals that distillation fosters greater long-context awareness by promoting more detailed and explicit reasoning processes during context analysis and information parsing. This advancement effectively mitigates the persistent "lost in the middle" issue that has hindered long-context models.</li>
</ul>

<h3>Title: Grounding Degradations in Natural Language for All-In-One Video Restoration</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Kamran Janjua, Amirhosein Ghasemabadi, Kunlin Zhang, Mohammad Salameh, Chao Gao, Di Niu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14851">https://arxiv.org/abs/2507.14851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14851">https://arxiv.org/pdf/2507.14851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14851]] Grounding Degradations in Natural Language for All-In-One Video Restoration(https://arxiv.org/abs/2507.14851)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In this work, we propose an all-in-one video restoration framework that grounds degradation-aware semantic context of video frames in natural language via foundation models, offering interpretable and flexible guidance. Unlike prior art, our method assumes no degradation knowledge in train or test time and learns an approximation to the grounded knowledge such that the foundation model can be safely disentangled during inference adding no extra cost. Further, we call for standardization of benchmarks in all-in-one video restoration, and propose two benchmarks in multi-degradation setting, three-task (3D) and four-task (4D), and two time-varying composite degradation benchmarks; one of the latter being our proposed dataset with varying snow intensity, simulating how weather degradations affect videos naturally. We compare our method with prior works and report state-of-the-art performance on all benchmarks.</li>
</ul>

<h3>Title: Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zhaoqiang Xia, Hexiang Huang, Haoyu Chen, Xiaoyi Feng, Guoying Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14867">https://arxiv.org/abs/2507.14867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14867">https://arxiv.org/pdf/2507.14867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14867]] Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition(https://arxiv.org/abs/2507.14867)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Micro-gestures are unconsciously performed body gestures that can convey the emotion states of humans and start to attract more research attention in the fields of human behavior understanding and affective computing as an emerging topic. However, the modeling of human emotion based on micro-gestures has not been explored sufficiently. In this work, we propose to recognize the emotion states based on the micro-gestures by reconstructing the behavior patterns with a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the framework, hypergraph Transformer based encoder and decoder are separately designed by stacking the hypergraph-enhanced self-attention and multiscale temporal convolution modules. Especially, to better capture the subtle motion of micro-gestures, we construct a decoder with additional upsampling operations for a reconstruction task in a self-supervised learning manner. We further propose a hypergraph-enhanced self-attention module where the hyperedges between skeleton joints are gradually updated to present the relationships of body joints for modeling the subtle local motion. Lastly, for exploiting the relationship between the emotion states and local motion of micro-gestures, an emotion recognition head from the output of encoder is designed with a shallow architecture and learned in a supervised way. The end-to-end framework is jointly trained in a one-stage way by comprehensively utilizing self-reconstruction and supervision information. The proposed method is evaluated on two publicly available datasets, namely iMiGUE and SMG, and achieves the best performance under multiple metrics, which is superior to the existing methods.</li>
</ul>

<h3>Title: Region-aware Depth Scale Adaptation with Sparse Measurements</h3>
<ul>
<li><strong>Authors: </strong>Rizhao Fan, Tianfang Ma, Zhigen Li, Ning An, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14879">https://arxiv.org/abs/2507.14879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14879">https://arxiv.org/pdf/2507.14879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14879]] Region-aware Depth Scale Adaptation with Sparse Measurements(https://arxiv.org/abs/2507.14879)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In recent years, the emergence of foundation models for depth prediction has led to remarkable progress, particularly in zero-shot monocular depth estimation. These models generate impressive depth predictions; however, their outputs are often in relative scale rather than metric scale. This limitation poses challenges for direct deployment in real-world applications. To address this, several scale adaptation methods have been proposed to enable foundation models to produce metric depth. However, these methods are typically costly, as they require additional training on new domains and datasets. Moreover, fine-tuning these models often compromises their original generalization capabilities, limiting their adaptability across diverse scenes. In this paper, we introduce a non-learning-based approach that leverages sparse depth measurements to adapt the relative-scale predictions of foundation models into metric-scale depth. Our method requires neither retraining nor fine-tuning, thereby preserving the strong generalization ability of the original foundation models while enabling them to produce metric depth. Experimental results demonstrate the effectiveness of our approach, high-lighting its potential to bridge the gap between relative and metric depth without incurring additional computational costs or sacrificing generalization ability.</li>
</ul>

<h3>Title: Open-set Cross Modal Generalization via Multimodal Unified Representation</h3>
<ul>
<li><strong>Authors: </strong>Hai Huang, Yan Xia, Shulei Wang, Hanting Wang, Minghui Fang, Shengpeng Ji, Sashuai Zhou, Tao Jin, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.14935">https://arxiv.org/abs/2507.14935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.14935">https://arxiv.org/pdf/2507.14935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.14935]] Open-set Cross Modal Generalization via Multimodal Unified Representation(https://arxiv.org/abs/2507.14935)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper extends Cross Modal Generalization (CMG) to open-set environments by proposing the more challenging Open-set Cross Modal Generalization (OSCMG) task. This task evaluates multimodal unified representations in open-set conditions, addressing the limitations of prior closed-set cross-modal evaluations. OSCMG requires not only cross-modal knowledge transfer but also robust generalization to unseen classes within new modalities, a scenario frequently encountered in real-world applications. Existing multimodal unified representation work lacks consideration for open-set environments. To tackle this, we propose MICU, comprising two key components: Fine-Coarse Masked multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI enhances multimodal alignment by applying contrastive learning at both holistic semantic and temporal levels, incorporating masking to enhance generalization. CUJP enhances feature diversity and model uncertainty by integrating modality-agnostic feature selection with self-supervised learning, thereby strengthening the model's ability to handle unknown categories in open-set tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the effectiveness of our approach. The code is available at this https URL.</li>
</ul>

<h3>Title: OmniVTON: Training-Free Universal Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Zhaotong Yang, Yuhui Li, Shengfeng He, Xinzhe Li, Yangyang Xu, Junyu Dong, Yong Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15037">https://arxiv.org/abs/2507.15037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15037">https://arxiv.org/pdf/2507.15037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15037]] OmniVTON: Training-Free Universal Virtual Try-On(https://arxiv.org/abs/2507.15037)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image-based Virtual Try-On (VTON) techniques rely on either supervised in-shop approaches, which ensure high fidelity but struggle with cross-domain generalization, or unsupervised in-the-wild methods, which improve adaptability but remain constrained by data biases and limited universality. A unified, training-free solution that works across both scenarios remains an open challenge. We propose OmniVTON, the first training-free universal VTON framework that decouples garment and pose conditioning to achieve both texture fidelity and pose consistency across diverse settings. To preserve garment details, we introduce a garment prior generation mechanism that aligns clothing with the body, followed by continuous boundary stitching technique to achieve fine-grained texture retention. For precise pose alignment, we utilize DDIM inversion to capture structural cues while suppressing texture interference, ensuring accurate body alignment independent of the original image textures. By disentangling garment and pose constraints, OmniVTON eliminates the bias inherent in diffusion models when handling multiple conditions simultaneously. Experimental results demonstrate that OmniVTON achieves superior performance across diverse datasets, garment types, and application scenarios. Notably, it is the first framework capable of multi-human VTON, enabling realistic garment transfer across multiple individuals in a single scene. Code is available at this https URL</li>
</ul>

<h3>Title: StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation</h3>
<ul>
<li><strong>Authors: </strong>Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15064">https://arxiv.org/abs/2507.15064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15064">https://arxiv.org/pdf/2507.15064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15064]] StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation(https://arxiv.org/abs/2507.15064)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current diffusion models for human image animation often struggle to maintain identity (ID) consistency, especially when the reference image and driving video differ significantly in body size or position. We introduce StableAnimator++, the first ID-preserving video diffusion framework with learnable pose alignment, capable of generating high-quality videos conditioned on a reference image and a pose sequence without any post-processing. Building upon a video diffusion model, StableAnimator++ contains carefully designed modules for both training and inference, striving for identity consistency. In particular, StableAnimator++ first uses learnable layers to predict the similarity transformation matrices between the reference image and the driven poses via injecting guidance from Singular Value Decomposition (SVD). These matrices align the driven poses with the reference image, mitigating misalignment to a great extent. StableAnimator++ then computes image and face embeddings using off-the-shelf encoders, refining the face embeddings via a global content-aware Face Encoder. To further maintain ID, we introduce a distribution-aware ID Adapter that counteracts interference caused by temporal layers while preserving ID via distribution alignment. During the inference stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization integrated into the denoising process, guiding the diffusion trajectory for enhanced facial fidelity. Experiments on benchmarks show the effectiveness of StableAnimator++ both qualitatively and quantitatively.</li>
</ul>

<h3>Title: Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Yang, Zichuan Liu, Lei Song, Kai Ying, Zhiguang Wang, Tom Bamford, Svitlana Vyetrenko, Jiang Bian, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15066">https://arxiv.org/abs/2507.15066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15066">https://arxiv.org/pdf/2507.15066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15066]] Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback(https://arxiv.org/abs/2507.15066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection is critical across various domains, yet current approaches often limit analysis to mere binary anomaly classification without detailed categorization or further explanatory reasoning. To address these limitations, we propose a novel task, Time-series Reasoning for Anomaly (Time-RA) that transforms classical time series anomaly detection from a discriminative into a generative, reasoning-intensive task leveraging Large Language Models (LLMs). Also, we introduce the first real-world multimodal benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning, comprising approximately 40,000 samples across 10 real-world domains. Each sample includes numeric time series data, contextual text information, and visual representations, each annotated with fine-grained categories (14 types for univariate anomalies and 6 for multivariate anomalies) and structured explanatory reasoning. We develop a sophisticated annotation framework utilizing ensemble-generated labels refined through GPT-4-driven feedback, ensuring accuracy and interpretability. Extensive benchmarking of LLMs and multimodal LLMs demonstrates the capabilities and limitations of current models, highlighting the critical role of supervised fine-tuning. Our dataset and task pave the way for significant advancements in interpretable time series anomaly detection and reasoning.</li>
</ul>

<h3>Title: Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR</h3>
<ul>
<li><strong>Authors: </strong>Peirong Zhang, Haowei Xu, Jiaxin Zhang, Guitao Xu, Xuhan Zheng, Zhenhua Yang, Junle Liu, Yuyi Zhang, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15085">https://arxiv.org/abs/2507.15085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15085">https://arxiv.org/pdf/2507.15085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15085]] Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR(https://arxiv.org/abs/2507.15085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text image is a unique and crucial information medium that integrates visual aesthetics and linguistic semantics in modern e-society. Due to their subtlety and complexity, the generation of text images represents a challenging and evolving frontier in the image generation field. The recent surge of specialized image generators (\emph{e.g.}, Flux-series) and unified generative models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a natural question: can they master the intricacies of text image generation and editing? Motivated by this, we assess current state-of-the-art generative models' capabilities in terms of text image generation and editing. We incorporate various typical optical character recognition (OCR) tasks into our evaluation and broaden the concept of text-based generation tasks into OCR generative tasks. We select 33 representative tasks and categorize them into five categories: document, handwritten text, scene text, artistic text, and complex \& layout-rich text. For comprehensive evaluation, we examine six models across both closed-source and open-source domains, using tailored, high-quality image inputs and prompts. Through this evaluation, we draw crucial observations and identify the weaknesses of current generative models for OCR tasks. We argue that photorealistic text image generation and editing should be internalized as foundational skills into general-domain generative models, rather than being delegated to specialized solutions, and we hope this empirical analysis can provide valuable insights for the community to achieve this goal. This evaluation is online and will be continuously updated at our GitHub repository.</li>
</ul>

<h3>Title: AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Qiufeng Li, Shu Hong, Jian Gao, Xuan Zhang, Tian Lan, Weidong Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15104">https://arxiv.org/abs/2507.15104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15104">https://arxiv.org/pdf/2507.15104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15104]] AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI(https://arxiv.org/abs/2507.15104)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize analog design automation through data-driven approaches. In particular, researchers are increasingly fascinated by harnessing the power of generative AI to automate the discovery of novel analog circuit topologies. Unlocking the full potential of generative AI in these data-driven discoveries requires access to large and diverse this http URL, there is a significant barrier in the analog domain--Analog circuit design is inherently proprietary, involving not only confidential circuit structures but also the underlying commercial semiconductor processes. As a result, current generative AI research is largely confined to individual researchers who construct small, narrowly focused private datasets. This fragmentation severely limits collaborative innovation and impedes progress across the research community. To address these challenges, we propose AnalogFed. AnalogFed enables collaborative topology discovery across decentralized clients (e.g., individual researchers or institutions) without requiring the sharing of raw private data. To make this vision practical, we introduce a suite of techniques tailored to the unique challenges of applying FedL in analog design--from generative model development and data heterogeneity handling to privacy-preserving strategies that ensure both flexibility and security for circuit designers and semiconductor manufacturers. Extensive experiments across varying client counts and dataset sizes demonstrate that AnalogFed achieves performance comparable to centralized baselines--while maintaining strict data privacy. Specifically, the generative AI model within AnalogFed achieves state-of-the-art efficiency and scalability in the design of analog circuit topologies.</li>
</ul>

<h3>Title: Better Models and Algorithms for Learning Ising Models from Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Jason Gaitonde, Ankur Moitra, Elchanan Mossel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15173">https://arxiv.org/abs/2507.15173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15173">https://arxiv.org/pdf/2507.15173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15173]] Better Models and Algorithms for Learning Ising Models from Dynamics(https://arxiv.org/abs/2507.15173)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the problem of learning the structure and parameters of the Ising model, a fundamental model of high-dimensional data, when observing the evolution of an associated Markov chain. A recent line of work has studied the natural problem of learning when observing an evolution of the well-known Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018, Gaitonde, Mossel STOC 2024], which provides an arguably more realistic generative model than the classical i.i.d. setting. However, this prior work crucially assumes that all site update attempts are observed, \emph{even when this attempt does not change the configuration}: this strong observation model is seemingly essential for these approaches. While perhaps possible in restrictive contexts, this precludes applicability to most realistic settings where we can observe \emph{only} the stochastic evolution itself, a minimal and natural assumption for any process we might hope to learn from. However, designing algorithms that succeed in this more realistic setting has remained an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018, Gaitonde, Moitra, Mossel, STOC 2025]. In this work, we give the first algorithms that efficiently learn the Ising model in this much more natural observation model that only observes when the configuration changes. For Ising models with maximum degree $d$, our algorithm recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time, which qualitatively matches the state-of-the-art even in the i.i.d. setting in a much weaker observation model. Our analysis holds more generally for a broader class of reversible, single-site Markov chains that also includes the popular Metropolis chain by leveraging more robust properties of reversible Markov chains.</li>
</ul>

<h3>Title: MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yusuke Yoshiyasu, Leyuan Sun, Ryusuke Sagawa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15212">https://arxiv.org/abs/2507.15212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15212">https://arxiv.org/pdf/2507.15212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15212]] MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction(https://arxiv.org/abs/2507.15212)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce MeshMamba, a neural network model for learning 3D articulated mesh models by employing the recently proposed Mamba State Space Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large number of input tokens, enabling the generation and reconstruction of body mesh models with more than 10,000 vertices, capturing clothing and hand geometries. The key to effectively learning MeshMamba is the serialization technique of mesh vertices into orderings that are easily processed by Mamba. This is achieved by sorting the vertices based on body part annotations or the 3D vertex locations of a template mesh, such that the ordering respects the structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D, a denoising diffusion model for generating 3D articulated meshes and 2) Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape and pose from a single image. Experimental results showed that MambaDiff3D can generate dense 3D human meshes in clothes, with grasping hands, etc., and outperforms previous approaches in the 3D human shape generation task. Additionally, Mamba-HMR extends the capabilities of previous non-parametric human mesh recovery approaches, which were limited to handling body-only poses using around 500 vertex tokens, to the whole-body setting with face and hands, while achieving competitive performance in (near) real-time.</li>
</ul>

<h3>Title: Improving Joint Embedding Predictive Architecture with Diffusion Noise</h3>
<ul>
<li><strong>Authors: </strong>Yuping Qiu, Rui Zhu, Ying-cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15216">https://arxiv.org/abs/2507.15216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15216">https://arxiv.org/pdf/2507.15216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15216]] Improving Joint Embedding Predictive Architecture with Diffusion Noise(https://arxiv.org/abs/2507.15216)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has become an incredibly successful method for feature learning, widely applied to many downstream tasks. It has proven especially effective for discriminative tasks, surpassing the trending generative models. However, generative models perform better in image generation and detail enhancement. Thus, it is natural for us to find a connection between SSL and generative models to further enhance the representation capacity of SSL. As generative models can create new samples by approximating the data distribution, such modeling should also lead to a semantic understanding of the raw visual data, which is necessary for recognition tasks. This enlightens us to combine the core principle of the diffusion model: diffusion noise, with SSL to learn a competitive recognition model. Specifically, diffusion noise can be viewed as a particular state of mask that reveals a close relationship between masked image modeling (MIM) and diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to incorporate diffusion noise into MIM by the position embedding of masked tokens. The multi-level noise schedule is a series of feature augmentations to further enhance the robustness of our model. We perform a comprehensive study to confirm its effectiveness in the classification of downstream tasks. Codes will be released soon in public.</li>
</ul>

<h3>Title: Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel</h3>
<ul>
<li><strong>Authors: </strong>Siqi Chen, Guoqing Zhang, Jiahao Lai, Bingzhi Shen, Sihong Zhang, Caixia Dong, Xuejin Chen, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15223">https://arxiv.org/abs/2507.15223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15223">https://arxiv.org/pdf/2507.15223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15223]] Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel(https://arxiv.org/abs/2507.15223)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advancements in 3D vision have increased the impact of blood vessel modeling on medical applications. However, accurately representing the complex geometry and topology of blood vessels remains a challenge due to their intricate branching patterns, curvatures, and irregular shapes. In this study, we propose a hierarchical part-based frame work for 3D vessel generation that separates the global binary tree-like topology from local geometric details. Our approach proceeds in three stages: (1) key graph generation to model the overall hierarchical struc ture, (2) vessel segment generation conditioned on geometric properties, and (3) hierarchical vessel assembly by integrating the local segments according to the global key graph. We validate our framework on real world datasets, demonstrating superior performance over existing methods in modeling complex vascular networks. This work marks the first successful application of a part-based generative approach for 3D vessel modeling, setting a new benchmark for vascular data generation. The code is available at: this https URL.</li>
</ul>

<h3>Title: Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Krishna Kanth Nakka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15227">https://arxiv.org/abs/2507.15227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15227">https://arxiv.org/pdf/2507.15227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15227]] Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders(https://arxiv.org/abs/2507.15227)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Interpretability is critical in high-stakes domains such as medical imaging, where understanding model decisions is essential for clinical adoption. In this work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast imaging by analyzing {Mammo-CLIP}, a vision--language foundation model pretrained on large-scale mammogram image--report pairs. We train a patch-level \texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features associated with clinically relevant breast concepts such as \textit{mass} and \textit{suspicious calcification}. Our findings reveal that top activated class level latent neurons in the SAE latent space often tend to align with ground truth regions, and also uncover several confounding factors influencing the model's decision-making process. Additionally, we analyze which latent neurons the model relies on during downstream finetuning for improving the breast concept prediction. This study highlights the promise of interpretable SAE latent representations in providing deeper insight into the internal workings of foundation models at every layer for breast imaging.</li>
</ul>

<h3>Title: Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation</h3>
<ul>
<li><strong>Authors: </strong>Naeem Paeedeh, Mahardhika Pratama, Wolfgang Mayer, Jimmy Cao, Ryszard Kowlczyk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15243">https://arxiv.org/abs/2507.15243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15243">https://arxiv.org/pdf/2507.15243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15243]] Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation(https://arxiv.org/abs/2507.15243)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model pre-trained with DINO combined with a prototypical classifier outperforms the latest SOTA methods. A crucial limitation that needs to be overcome is that updating too many parameters of the transformers leads to overfitting due to the scarcity of labeled samples. To address this challenge, we propose a new concept, Coalescent Projection (CP), as an effective successor to soft prompts. Additionally, we propose a novel pseudo-class generation method combined with Self-Supervised Transformations (SSTs) that relies solely on the base domain to prepare the network for encountering unseen samples from different domains. The proposed method exhibits its effectiveness in comprehensive experiments on the extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published at this https URL.</li>
</ul>

<h3>Title: FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yanbing Zhang, Zhe Wang, Qin Zhou, Mengping Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15249">https://arxiv.org/abs/2507.15249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15249">https://arxiv.org/pdf/2507.15249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15249]] FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers(https://arxiv.org/abs/2507.15249)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In light of recent breakthroughs in text-to-image (T2I) generation, particularly with diffusion transformers (DiT), subject-driven technologies are increasingly being employed for high-fidelity customized production that preserves subject identity from reference inputs, enabling thrilling design workflows and engaging entertainment. Existing alternatives typically require either per-subject optimization via trainable text embeddings or training specialized encoders for subject feature extraction on large-scale datasets. Such dependencies on training procedures fundamentally constrain their practical applications. More importantly, current methodologies fail to fully leverage the inherent zero-shot potential of modern diffusion transformers (e.g., the Flux series) for authentic subject-driven synthesis. To bridge this gap, we propose FreeCus, a genuinely training-free framework that activates DiT's capabilities through three key innovations: 1) We introduce a pivotal attention sharing mechanism that captures the subject's layout integrity while preserving crucial editing flexibility. 2) Through a straightforward analysis of DiT's dynamic shifting, we propose an upgraded variant that significantly improves fine-grained feature extraction. 3) We further integrate advanced Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic representations. Extensive experiments reflect that our method successfully unlocks DiT's zero-shot ability for consistent subject synthesis across diverse contexts, achieving state-of-the-art or comparable results compared to approaches that require additional training. Notably, our framework demonstrates seamless compatibility with existing inpainting pipelines and control modules, facilitating more compelling experiences. Our code is available at: this https URL.</li>
</ul>

<h3>Title: CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Han, Haotian Ye, Puheng Li, Minkai Xu, James Zou, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15260">https://arxiv.org/abs/2507.15260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15260">https://arxiv.org/pdf/2507.15260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15260]] CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers(https://arxiv.org/abs/2507.15260)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative models have become dominant generators of high-fidelity images and videos but remain limited by their computationally expensive inference procedures. Existing acceleration techniques either require extensive model retraining or compromise significantly on sample quality. This paper explores a general, training-free, and model-agnostic acceleration strategy via multi-core parallelism. Our framework views multi-core diffusion sampling as an ODE solver pipeline, where slower yet accurate solvers progressively rectify faster solvers through a theoretically justified inter-core communication mechanism. This motivates our multi-core training-free diffusion sampling accelerator, CHORDS, which is compatible with various diffusion samplers, model architectures, and modalities. Through extensive experiments, CHORDS significantly accelerates sampling across diverse large-scale image and video diffusion models, yielding up to 2.1x speedup with four cores, improving by 50% over baselines, and 2.9x speedup with eight cores, all without quality degradation. This advancement enables CHORDS to establish a solid foundation for real-time, high-fidelity diffusion generation.</li>
</ul>

<h3>Title: Conditional Video Generation for High-Efficiency Video Compression</h3>
<ul>
<li><strong>Authors: </strong>Fangqiu Yi, Jingyu Xu, Jiawei Shao, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15269">https://arxiv.org/abs/2507.15269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15269">https://arxiv.org/pdf/2507.15269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15269]] Conditional Video Generation for High-Efficiency Video Compression(https://arxiv.org/abs/2507.15269)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Perceptual studies demonstrate that conditional diffusion models excel at reconstructing video content aligned with human visual perception. Building on this insight, we propose a video compression framework that leverages conditional diffusion models for perceptually optimized reconstruction. Specifically, we reframe video compression as a conditional generation task, where a generative model synthesizes video from sparse, yet informative signals. Our approach introduces three key modules: (1) Multi-granular conditioning that captures both static scene structure and dynamic spatio-temporal cues; (2) Compact representations designed for efficient transmission without sacrificing semantic richness; (3) Multi-condition training with modality dropout and role-aware embeddings, which prevent over-reliance on any single modality and enhance robustness. Extensive experiments show that our method significantly outperforms both traditional and neural codecs on perceptual quality metrics such as Fréchet Video Distance (FVD) and LPIPS, especially under high compression ratios.</li>
</ul>

<h3>Title: In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems</h3>
<ul>
<li><strong>Authors: </strong>Lazaro Janier Gonzalez-Soler, Maciej Salwowski, Christoph Busch</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15285">https://arxiv.org/abs/2507.15285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15285">https://arxiv.org/pdf/2507.15285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15285]] In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems(https://arxiv.org/abs/2507.15285)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advances in biometric systems have significantly improved the detection and prevention of fraudulent activities. However, as detection methods improve, attack techniques become increasingly sophisticated. Attacks on face recognition systems can be broadly divided into physical and digital approaches. Traditionally, deep learning models have been the primary defence against such attacks. While these models perform exceptionally well in scenarios for which they have been trained, they often struggle to adapt to different types of attacks or varying environmental conditions. These subsystems require substantial amounts of training data to achieve reliable performance, yet biometric data collection faces significant challenges, including privacy concerns and the logistical difficulties of capturing diverse attack scenarios under controlled conditions. This work investigates the application of Vision Language Models (VLM) and proposes an in-context learning framework for detecting physical presentation attacks and digital morphing attacks in biometric systems. Focusing on open-source models, the first systematic framework for the quantitative evaluation of VLMs in security-critical scenarios through in-context learning techniques is established. The experimental evaluation conducted on freely available databases demonstrates that the proposed subsystem achieves competitive performance for physical and digital attack detection, outperforming some of the traditional CNNs without resource-intensive training. The experimental results validate the proposed framework as a promising tool for improving generalisation in attack detection.</li>
</ul>

<h3>Title: Universal crystal material property prediction via multi-view geometric fusion in graph transformers</h3>
<ul>
<li><strong>Authors: </strong>Liang Zhang, Kong Chen, Yuen Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15303">https://arxiv.org/abs/2507.15303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15303">https://arxiv.org/pdf/2507.15303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15303]] Universal crystal material property prediction via multi-view geometric fusion in graph transformers(https://arxiv.org/abs/2507.15303)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurately and comprehensively representing crystal structures is critical for advancing machine learning in large-scale crystal materials simulations, however, effectively capturing and leveraging the intricate geometric and topological characteristics of crystal structures remains a core, long-standing challenge for most existing methods in crystal property prediction. Here, we propose MGT, a multi-view graph transformer framework that synergistically fuses SE3 invariant and SO3 equivariant graph representations, which respectively captures rotation-translation invariance and rotation equivariance in crystal geometries. To strategically incorporate these complementary geometric representations, we employ a lightweight mixture of experts router in MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on the specific target task. Compared with previous state-of-the-art models, MGT reduces the mean absolute error by up to 21% on crystal property prediction tasks through multi-task self-supervised pretraining. Ablation experiments and interpretable investigations confirm the effectiveness of each technique implemented in our framework. Additionally, in transfer learning scenarios including crystal catalyst adsorption energy and hybrid perovskite bandgap prediction, MGT achieves performance improvements of up to 58% over existing baselines, demonstrating domain-agnostic scalability across diverse application domains. As evidenced by the above series of studies, we believe that MGT can serve as useful model for crystal material property prediction, providing a valuable tool for the discovery of novel materials.</li>
</ul>

<h3>Title: BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Li, Haotong Lin, Jiashi Feng, Peter Wonka, Bingyi Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15321">https://arxiv.org/abs/2507.15321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15321">https://arxiv.org/pdf/2507.15321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15321]] BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?(https://arxiv.org/abs/2507.15321)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Depth estimation is a fundamental task in computer vision with diverse applications. Recent advancements in deep learning have led to powerful depth foundation models (DFMs), yet their evaluation remains challenging due to inconsistencies in existing protocols. Traditional benchmarks rely on alignment-based metrics that introduce biases, favor certain depth representations, and complicate fair comparisons. In this work, we propose BenchDepth, a new benchmark that evaluates DFMs through five carefully selected downstream proxy tasks: depth completion, stereo matching, monocular feed-forward 3D scene reconstruction, SLAM, and vision-language spatial understanding. Unlike conventional evaluation protocols, our approach assesses DFMs based on their practical utility in real-world applications, bypassing problematic alignment procedures. We benchmark eight state-of-the-art DFMs and provide an in-depth analysis of key findings and observations. We hope our work sparks further discussion in the community on best practices for depth model evaluation and paves the way for future research and advancements in depth estimation.</li>
</ul>

<h3>Title: ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Aqeel, Federico Leonardi, Francesco Setti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15335">https://arxiv.org/abs/2507.15335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15335">https://arxiv.org/pdf/2507.15335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15335]] ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis(https://arxiv.org/abs/2507.15335)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Industrial defect detection systems face critical limitations when confined to one-class anomaly detection paradigms, which assume uniform outlier distributions and struggle with data scarcity in realworld manufacturing environments. We present ExDD (Explicit Dual Distribution), a novel framework that transcends these limitations by explicitly modeling dual feature distributions. Our approach leverages parallel memory banks that capture the distinct statistical properties of both normality and anomalous patterns, addressing the fundamental flaw of uniform outlier assumptions. To overcome data scarcity, we employ latent diffusion models with domain-specific textual conditioning, generating in-distribution synthetic defects that preserve industrial context. Our neighborhood-aware ratio scoring mechanism elegantly fuses complementary distance metrics, amplifying signals in regions exhibiting both deviation from normality and similarity to known defect patterns. Experimental validation on KSDD2 demonstrates superior performance (94.2% I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.</li>
</ul>

<h3>Title: RoadFusion: Latent Diffusion Model for Pavement Defect Detection</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Aqeel, Kidus Dagnaw Bellete, Francesco Setti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15346">https://arxiv.org/abs/2507.15346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15346">https://arxiv.org/pdf/2507.15346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15346]] RoadFusion: Latent Diffusion Model for Pavement Defect Detection(https://arxiv.org/abs/2507.15346)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Pavement defect detection faces critical challenges including limited annotated data, domain shift between training and deployment environments, and high variability in defect appearances across different road conditions. We propose RoadFusion, a framework that addresses these limitations through synthetic anomaly generation with dual-path feature adaptation. A latent diffusion model synthesizes diverse, realistic defects using text prompts and spatial masks, enabling effective training under data scarcity. Two separate feature adaptors specialize representations for normal and anomalous inputs, improving robustness to domain shift and defect variability. A lightweight discriminator learns to distinguish fine-grained defect patterns at the patch level. Evaluated on six benchmark datasets, RoadFusion achieves consistently strong performance across both classification and localization tasks, setting new state-of-the-art in multiple metrics relevant to real-world road inspection.</li>
</ul>

<h3>Title: Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding</h3>
<ul>
<li><strong>Authors: </strong>Elisa Sanchez-Bayona, Rodrigo Agerri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15357">https://arxiv.org/abs/2507.15357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15357">https://arxiv.org/pdf/2507.15357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15357]] Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding(https://arxiv.org/abs/2507.15357)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive evaluation of the capabilities of Large Language Models (LLMs) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. Although metaphor processing has gained significant attention in Natural Language Processing (NLP), previous research has been limited to single-dataset evaluations and specific task settings, often using artificially constructed data through lexical replacement. We address these limitations by conducting extensive experiments using diverse publicly available datasets with inference and metaphor annotations, focusing on Natural Language Inference (NLI) and Question Answering (QA) tasks. The results indicate that LLMs' performance is more influenced by features like lexical overlap and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of LLMs to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of LLMs in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks. Data and code are publicly available.</li>
</ul>

<h3>Title: To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Julia Machnio, Mads Nielsen, Mostafa Mehdipour Ghazi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15381">https://arxiv.org/abs/2507.15381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15381">https://arxiv.org/pdf/2507.15381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15381]] To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models(https://arxiv.org/abs/2507.15381)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Active learning (AL) seeks to reduce annotation costs by selecting the most informative samples for labeling, making it particularly valuable in resource-constrained settings. However, traditional evaluation methods, which focus solely on final accuracy, fail to capture the full dynamics of the learning process. To address this gap, we propose PALM (Performance Analysis of Active Learning Models), a unified and interpretable mathematical model that characterizes AL trajectories through four key parameters: achievable accuracy, coverage efficiency, early-stage performance, and scalability. PALM provides a predictive description of AL behavior from partial observations, enabling the estimation of future performance and facilitating principled comparisons across different strategies. We validate PALM through extensive experiments on CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and self-supervised embeddings. Our results demonstrate that PALM generalizes effectively across datasets, budgets, and strategies, accurately predicting full learning curves from limited labeled data. Importantly, PALM reveals crucial insights into learning efficiency, data space coverage, and the scalability of AL methods. By enabling the selection of cost-effective strategies and predicting performance under tight budget constraints, PALM lays the basis for more systematic, reproducible, and data-efficient evaluation of AL in both research and real-world applications. The code is available at: this https URL.</li>
</ul>

<h3>Title: An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Owen Douglas, Aku Kammonen, Anamika Pandey, Raúl Tempone</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15442">https://arxiv.org/abs/2507.15442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15442">https://arxiv.org/pdf/2507.15442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15442]] An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations(https://arxiv.org/abs/2507.15442)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work proposes a training algorithm based on adaptive random Fourier features (ARFF) with Metropolis sampling and resampling \cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and diffusion components of stochastic differential equations from snapshot data. Specifically, this study considers Itô diffusion processes and a likelihood-based loss function derived from the Euler-Maruyama integration introduced in \cite{Dietrich2023} and \cite{dridi2021learningstochasticdynamicalsystems}. This work evaluates the proposed method against benchmark problems presented in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin dynamics, a stochastic susceptible-infected-recovered model, and a stochastic wave equation. Across all cases, the ARFF-based approach matches or surpasses the performance of conventional Adam-based optimization in both loss minimization and convergence speed. These results highlight the potential of ARFF as a compelling alternative for data-driven modeling of stochastic dynamics.</li>
</ul>

<h3>Title: An aerial color image anomaly dataset for search missions in complex forested terrain</h3>
<ul>
<li><strong>Authors: </strong>Rakesh John Amala Arokia Nathan, Matthias Gessner, Nurullah Özkan, Marius Bock, Mohamed Youssef, Maximilian Mews, Björn Piltz, Ralf Berger, Oliver Bimber</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15492">https://arxiv.org/abs/2507.15492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15492">https://arxiv.org/pdf/2507.15492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15492]] An aerial color image anomaly dataset for search missions in complex forested terrain(https://arxiv.org/abs/2507.15492)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>After a family murder in rural Germany, authorities failed to locate the suspect in a vast forest despite a massive search. To aid the search, a research aircraft captured high-resolution aerial imagery. Due to dense vegetation obscuring small clues, automated analysis was ineffective, prompting a crowd-search initiative. This effort produced a unique dataset of labeled, hard-to-detect anomalies under occluded, real-world conditions. It can serve as a benchmark for improving anomaly detection approaches in complex forest environments, supporting manhunts and rescue operations. Initial benchmark tests showed existing methods performed poorly, highlighting the need for context-aware approaches. The dataset is openly accessible for offline processing. An additional interactive web interface supports online viewing and dynamic growth by allowing users to annotate and submit new findings.</li>
</ul>

<h3>Title: Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Syed Ahmed Mahmood, Ali Shah Ali, Umer Ahmed, Fawad Javed Fateh, M. Zeeshan Zia, Quoc-Huy Tran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15540">https://arxiv.org/abs/2507.15540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15540">https://arxiv.org/pdf/2507.15540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15540]] Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport(https://arxiv.org/abs/2507.15540)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We study the problem of self-supervised procedure learning, which discovers key steps and establishes their order from a set of unlabeled procedural videos. Previous procedure learning methods typically learn frame-to-frame correspondences between videos before determining key steps and their order. However, their performance often suffers from order variations, background/redundant frames, and repeated actions. To overcome these challenges, we propose a self-supervised procedure learning framework, which utilizes a fused Gromov-Wasserstein optimal transport formulation with a structural prior for computing frame-to-frame mapping between videos. However, optimizing exclusively for the above temporal alignment term may lead to degenerate solutions, where all frames are mapped to a small cluster in the embedding space and hence every video is associated with only one key step. To address that limitation, we further integrate a contrastive regularization term, which maps different frames to different points in the embedding space, avoiding the collapse to trivial solutions. Finally, we conduct extensive experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e., ProceL and CrossTask) benchmarks to demonstrate superior performance by our approach against previous methods, including OPEL which relies on a traditional Kantorovich optimal transport formulation with an optimality prior.</li>
</ul>

<h3>Title: Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Poggi, Shashank Agnihotri, Margret Keuper</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15576">https://arxiv.org/abs/2507.15576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15576">https://arxiv.org/pdf/2507.15576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15576]] Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging(https://arxiv.org/abs/2507.15576)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Terahertz (THz) imaging enables non-invasive analysis for applications such as security screening and material classification, but effective image classification remains challenging due to limited annotations, low resolution, and visual ambiguity. We introduce In-Context Learning (ICL) with Vision-Language Models (VLMs) as a flexible, interpretable alternative that requires no fine-tuning. Using a modality-aligned prompting framework, we adapt two open-weight VLMs to the THz domain and evaluate them under zero-shot and one-shot settings. Our results show that ICL improves classification and interpretability in low-data regimes. This is the first application of ICL-enhanced VLMs to THz imaging, offering a promising direction for resource-constrained scientific domains. Code: \href{this https URL}{GitHub repository}.</li>
</ul>

<h3>Title: We Need to Rethink Benchmarking in Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Philipp Röchner, Simon Klüttermann, Franz Rothlauf, Daniel Schlör</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15584">https://arxiv.org/abs/2507.15584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15584">https://arxiv.org/pdf/2507.15584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15584]] We Need to Rethink Benchmarking in Anomaly Detection(https://arxiv.org/abs/2507.15584)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Despite the continuous proposal of new anomaly detection algorithms and extensive benchmarking efforts, progress seems to stagnate, with only minor performance differences between established baselines and new algorithms. In this position paper, we argue that this stagnation is due to limitations in how we evaluate anomaly detection algorithms. Current benchmarking does not, for example, sufficiently reflect the diversity of anomalies in applications ranging from predictive maintenance to scientific discovery. Consequently, we need to rethink benchmarking in anomaly detection. In our opinion, anomaly detection should be studied using scenarios that capture the relevant characteristics of different applications. We identify three key areas for improvement: First, we need to identify anomaly detection scenarios based on a common taxonomy. Second, anomaly detection pipelines should be analyzed end-to-end and by component. Third, evaluating anomaly detection algorithms should be meaningful regarding the scenario's objectives.</li>
</ul>

<h3>Title: SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Salah Eddine Bekhouche, Gaby Maroun, Fadi Dornaika, Abdenour Hadid</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15595">https://arxiv.org/abs/2507.15595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15595">https://arxiv.org/pdf/2507.15595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15595]] SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging(https://arxiv.org/abs/2507.15595)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is crucial for many healthcare tasks, including disease diagnosis and treatment planning. One key area is the segmentation of skin lesions, which is vital for diagnosing skin cancer and monitoring patients. In this context, this paper introduces SegDT, a new segmentation model based on diffusion transformer (DiT). SegDT is designed to work on low-cost hardware and incorporates Rectified Flow, which improves the generation quality at reduced inference steps and maintains the flexibility of standard diffusion models. Our method is evaluated on three benchmarking datasets and compared against several existing works, achieving state-of-the-art results while maintaining fast inference speeds. This makes the proposed model appealing for real-world medical applications. This work advances the performance and capabilities of deep learning models in medical image analysis, enabling faster, more accurate diagnostic tools for healthcare professionals. The code is made publicly available at \href{this https URL}{GitHub}.</li>
</ul>

<h3>Title: CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ru Jia, Xiaozhuang Ma, Jianji Wang, Nanning Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15606">https://arxiv.org/abs/2507.15606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15606">https://arxiv.org/pdf/2507.15606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15606]] CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation(https://arxiv.org/abs/2507.15606)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While the proposal of the Tri-plane representation has advanced the development of the 3D-aware image generative models, problems rooted in its inherent structure, such as multi-face artifacts caused by sharing the same features in symmetric regions, limit its ability to generate 360$^\circ$ view images. In this paper, we propose CylinderPlane, a novel implicit representation based on Cylindrical Coordinate System, to eliminate the feature ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different from the inevitable feature entanglement in Cartesian coordinate-based Tri-plane representation, the cylindrical coordinate system explicitly separates features at different angles, allowing our cylindrical representation possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis. We further introduce the nested cylinder representation that composites multiple cylinders at different scales, thereby enabling the model more adaptable to complex geometry and varying resolutions. The combination of cylinders with different resolutions can effectively capture more critical locations and multi-scale features, greatly facilitates fine detail learning and robustness to different resolutions. Moreover, our representation is agnostic to implicit rendering methods and can be easily integrated into any neural rendering pipeline. Extensive experiments on both synthetic dataset and unstructured in-the-wild images demonstrate that our proposed representation achieves superior performance over previous methods.</li>
</ul>

<h3>Title: Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems</h3>
<ul>
<li><strong>Authors: </strong>Andrii Balashov, Olena Ponomarova, Xiaohua Zhai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15613">https://arxiv.org/abs/2507.15613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15613">https://arxiv.org/pdf/2507.15613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15613]] Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems(https://arxiv.org/abs/2507.15613)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) deployed in enterprise settings (e.g., as Microsoft 365 Copilot) face novel security challenges. One critical threat is prompt inference attacks: adversaries chain together seemingly benign prompts to gradually extract confidential data. In this paper, we present a comprehensive study of multi-stage prompt inference attacks in an enterprise LLM context. We simulate realistic attack scenarios where an attacker uses mild-mannered queries and indirect prompt injections to exploit an LLM integrated with private corporate data. We develop a formal threat model for these multi-turn inference attacks and analyze them using probability theory, optimization frameworks, and information-theoretic leakage bounds. The attacks are shown to reliably exfiltrate sensitive information from the LLM's context (e.g., internal SharePoint documents or emails), even when standard safety measures are in place. We propose and evaluate defenses to counter such attacks, including statistical anomaly detection, fine-grained access control, prompt sanitization techniques, and architectural modifications to LLM deployment. Each defense is supported by mathematical analysis or experimental simulation. For example, we derive bounds on information leakage under differential privacy-based training and demonstrate an anomaly detection method that flags multi-turn attacks with high AUC. We also introduce an approach called "spotlighting" that uses input transformations to isolate untrusted prompt content, reducing attack success by an order of magnitude. Finally, we provide a formal proof of concept and empirical validation for a combined defense-in-depth strategy. Our work highlights that securing LLMs in enterprise settings requires moving beyond single-turn prompt filtering toward a holistic, multi-stage perspective on both attacks and defenses.</li>
</ul>

<h3>Title: Towards Explainable Anomaly Detection in Shared Mobility Systems</h3>
<ul>
<li><strong>Authors: </strong>Elnur Isgandarov, Matteo Cederle, Federico Chiariotti, Gian Antonio Susto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15643">https://arxiv.org/abs/2507.15643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15643">https://arxiv.org/pdf/2507.15643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15643]] Towards Explainable Anomaly Detection in Shared Mobility Systems(https://arxiv.org/abs/2507.15643)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Shared mobility systems, such as bike-sharing networks, play a crucial role in urban transportation. Identifying anomalies in these systems is essential for optimizing operations, improving service reliability, and enhancing user experience. This paper presents an interpretable anomaly detection framework that integrates multi-source data, including bike-sharing trip records, weather conditions, and public transit availability. The Isolation Forest algorithm is employed for unsupervised anomaly detection, along with the Depth-based Isolation Forest Feature Importance (DIFFI) algorithm providing interpretability. Results show that station-level analysis offers a robust understanding of anomalies, highlighting the influence of external factors such as adverse weather and limited transit availability. Our findings contribute to improving decision-making in shared mobility operations.</li>
</ul>

<h3>Title: DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Hung Nguyen, Runfa Li, An Le, Truong Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15690">https://arxiv.org/abs/2507.15690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15690">https://arxiv.org/pdf/2507.15690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15690]] DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting(https://arxiv.org/abs/2507.15690)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in reconstructing high-quality novel views, as it often overfits to the widely-varying high-frequency (HF) details of the sparse training views. While frequency regularization can be a promising approach, its typical reliance on Fourier transforms causes difficult parameter tuning and biases towards detrimental HF learning. We propose DWTGS, a framework that rethinks frequency regularization by leveraging wavelet-space losses that provide additional spatial supervision. Specifically, we supervise only the low-frequency (LF) LL subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband in a self-supervised manner. Experiments across benchmarks show that DWTGS consistently outperforms Fourier-based counterparts, as this LF-centric strategy improves generalization and reduces HF hallucinations.</li>
</ul>

<h3>Title: Explainable Anomaly Detection for Electric Vehicles Charging Stations</h3>
<ul>
<li><strong>Authors: </strong>Matteo Cederle, Andrea Mazzucco, Andrea Demartini, Eugenio Mazza, Eugenia Suriani, Federico Vitti, Gian Antonio Susto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15718">https://arxiv.org/abs/2507.15718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15718">https://arxiv.org/pdf/2507.15718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15718]] Explainable Anomaly Detection for Electric Vehicles Charging Stations(https://arxiv.org/abs/2507.15718)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Electric vehicles (EV) charging stations are one of the critical infrastructures needed to support the transition to renewable-energy-based mobility, but ensuring their reliability and efficiency requires effective anomaly detection to identify irregularities in charging behavior. However, in such a productive scenario, it is also crucial to determine the underlying cause behind the detected anomalies. To achieve this goal, this study investigates unsupervised anomaly detection techniques for EV charging infrastructure, integrating eXplainable Artificial Intelligence techniques to enhance interpretability and uncover root causes of anomalies. Using real-world sensors and charging session data, this work applies Isolation Forest to detect anomalies and employs the Depth-based Isolation Forest Feature Importance (DIFFI) method to identify the most important features contributing to such anomalies. The efficacy of the proposed approach is evaluated in a real industrial case.</li>
</ul>

<h3>Title: A Practical Investigation of Spatially-Controlled Image Generation with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Guoxuan Xia, Harleen Hanspal, Petru-Daniel Tudosiu, Shifeng Zhang, Sarah Parisot</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15724">https://arxiv.org/abs/2507.15724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15724">https://arxiv.org/pdf/2507.15724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15724]] A Practical Investigation of Spatially-Controlled Image Generation with Transformers(https://arxiv.org/abs/2507.15724)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Enabling image generation models to be spatially controlled is an important area of research, empowering users to better generate images according to their own fine-grained specifications via e.g. edge maps, poses. Although this task has seen impressive improvements in recent times, a focus on rapidly producing stronger models has come at the cost of detailed and fair scientific comparison. Differing training data, model architectures and generation paradigms make it difficult to disentangle the factors contributing to performance. Meanwhile, the motivations and nuances of certain approaches become lost in the literature. In this work, we aim to provide clear takeaways across generation paradigms for practitioners wishing to develop transformer-based systems for spatially-controlled generation, clarifying the literature and addressing knowledge gaps. We perform controlled experiments on ImageNet across diffusion-based/flow-based and autoregressive (AR) models. First, we establish control token prefilling as a simple, general and performant baseline approach for transformers. We then investigate previously underexplored sampling time enhancements, showing that extending classifier-free guidance to control, as well as softmax truncation, have a strong impact on control-generation consistency. Finally, we re-clarify the motivation of adapter-based approaches, demonstrating that they mitigate "forgetting" and maintain generation quality when trained on limited downstream data, but underperform full training in terms of generation-control consistency. Code will be released upon publication.</li>
</ul>

<h3>Title: TokensGen: Harnessing Condensed Tokens for Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenqi Ouyang, Zeqi Xiao, Danni Yang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15728">https://arxiv.org/abs/2507.15728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15728">https://arxiv.org/pdf/2507.15728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15728]] TokensGen: Harnessing Condensed Tokens for Long Video Generation(https://arxiv.org/abs/2507.15728)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at this https URL .</li>
</ul>

<h3>Title: Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ghassen Baklouti, Julio Silva-Rodríguez, Jose Dolz, Houda Bahig, Ismail Ben Ayed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15793">https://arxiv.org/abs/2507.15793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15793">https://arxiv.org/pdf/2507.15793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15793]] Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation(https://arxiv.org/abs/2507.15793)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is increasingly attracting interest in medical imaging due to its effectiveness and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA) is a notable approach based on the assumption that the adaptation inherently occurs in a low-dimensional subspace. While it has shown good performance, its implementation requires a fixed and unalterable rank, which might be challenging to select given the unique complexities and requirements of each medical imaging downstream task. Inspired by advancements in natural image processing, we introduce a novel approach for medical image segmentation that dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank representation of the trainable weight matrices as a singular value decomposition, we introduce an l_1 sparsity regularizer to the loss function, and tackle it with a proximal optimizer. The regularizer could be viewed as a penalty on the decomposition rank. Hence, its minimization enables to find task-adapted ranks automatically. Our method is evaluated in a realistic few-shot fine-tuning setting, where we compare it first to the standard LoRA and then to several other PEFT methods across two distinguishable tasks: base organs and novel organs. Our extensive experiments demonstrate the significant performance improvements driven by our method, highlighting its efficiency and robustness against suboptimal rank initialization. Our code is publicly available: this https URL</li>
</ul>

<h3>Title: ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Danhui Chen, Ziquan Liu, Chuxi Yang, Dan Wang, Yan Yan, Yi Xu, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15803">https://arxiv.org/abs/2507.15803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15803">https://arxiv.org/pdf/2507.15803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15803]] ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction(https://arxiv.org/abs/2507.15803)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain's labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in.</li>
</ul>

<h3>Title: True Multimodal In-Context Learning Needs Attention to the Visual Context</h3>
<ul>
<li><strong>Authors: </strong>Shuo Chen, Jianzhe Liu, Zhen Han, Yan Xia, Daniel Cremers, Philip Torr, Volker Tresp, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15807">https://arxiv.org/abs/2507.15807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15807">https://arxiv.org/pdf/2507.15807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15807]] True Multimodal In-Context Learning Needs Attention to the Visual Context(https://arxiv.org/abs/2507.15807)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. More importantly, this limitation is often concealed by the improved performance on tasks that do not require understanding the visual context. As a result, how to effectively enhance MICL ability and reliably evaluate the MICL performance remains underexplored. To address these issues, we first introduce Dynamic Attention Reallocation (DARA), an efficient fine-tuning strategy that encourages models to attend to the visual context by rebalancing attention across visual and textual tokens. In addition, we present TrueMICL, an MICL-dedicated dataset with both support and test sets that explicitly requires the integration of multimodal information-particularly visual content-for correct task completion. Extensive experiments demonstrate the effectiveness of our holistic solution, showcasing substantial improvements in the true multimodal in-context learning capabilities. Code and datasets are available at this https URL .</li>
</ul>

<h3>Title: Diffusion models for multivariate subsurface generation and efficient probabilistic inversion</h3>
<ul>
<li><strong>Authors: </strong>Roberto Miele, Niklas Linde</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, physics.geo-ph, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15809">https://arxiv.org/abs/2507.15809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15809">https://arxiv.org/pdf/2507.15809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15809]] Diffusion models for multivariate subsurface generation and efficient probabilistic inversion(https://arxiv.org/abs/2507.15809)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models offer stable training and state-of-the-art performance for deep generative modeling tasks. Here, we consider their use in the context of multivariate subsurface modeling and probabilistic inversion. We first demonstrate that diffusion models enhance multivariate modeling capabilities compared to variational autoencoders and generative adversarial networks. In diffusion modeling, the generative process involves a comparatively large number of time steps with update rules that can be modified to account for conditioning data. We propose different corrections to the popular Diffusion Posterior Sampling approach by Chung et al. (2023). In particular, we introduce a likelihood approximation accounting for the noise-contamination that is inherent in diffusion modeling. We assess performance in a multivariate geological scenario involving facies and correlated acoustic impedance. Conditional modeling is demonstrated using both local hard data (well logs) and nonlinear geophysics (fullstack seismic data). Our tests show significantly improved statistical robustness, enhanced sampling of the posterior probability density function and reduced computational costs, compared to the original approach. The method can be used with both hard and indirect conditioning data, individually or simultaneously. As the inversion is included within the diffusion process, it is faster than other methods requiring an outer-loop around the generative model, such as Markov chain Monte Carlo.</li>
</ul>

<h3>Title: Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Enes Sanli, Baris Sarper Tezcan, Aykut Erdem, Erkut Erdem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15824">https://arxiv.org/abs/2507.15824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15824">https://arxiv.org/pdf/2507.15824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15824]] Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models(https://arxiv.org/abs/2507.15824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-video (T2V) generation has enabled the synthesis of visually compelling and temporally coherent videos from natural language. However, these models often fall short in basic physical commonsense, producing outputs that violate intuitive expectations around causality, object behavior, and tool use. Addressing this gap, we present PhysVidBench, a benchmark designed to evaluate the physical reasoning capabilities of T2V systems. The benchmark includes 383 carefully curated prompts, emphasizing tool use, material properties, and procedural interactions, and domains where physical plausibility is crucial. For each prompt, we generate videos using diverse state-of-the-art models and adopt a three-stage evaluation pipeline: (1) formulate grounded physics questions from the prompt, (2) caption the generated video with a vision-language model, and (3) task a language model to answer several physics-involved questions using only the caption. This indirect strategy circumvents common hallucination issues in direct video-based evaluation. By highlighting affordances and tool-mediated actions, areas overlooked in current T2V evaluations, PhysVidBench provides a structured, interpretable framework for assessing physical commonsense in generative video models.</li>
</ul>

<h3>Title: Latent Denoising Makes Good Visual Tokenizers</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15856">https://arxiv.org/abs/2507.15856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15856">https://arxiv.org/pdf/2507.15856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15856]] Latent Denoising Makes Good Visual Tokenizers(https://arxiv.org/abs/2507.15856)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design.</li>
</ul>

<h3>Title: Diffusion Beats Autoregressive in Data-Constrained Settings</h3>
<ul>
<li><strong>Authors: </strong>Mihir Prabhudesai, Menging Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2507.15857">https://arxiv.org/abs/2507.15857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2507.15857">https://arxiv.org/pdf/2507.15857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2507.15857]] Diffusion Beats Autoregressive in Data-Constrained Settings(https://arxiv.org/abs/2507.15857)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
