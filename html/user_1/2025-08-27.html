<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-27</h1>
<h3>Title: Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges</h3>
<ul>
<li><strong>Authors: </strong>Edgar Rangel, Fabio Martinez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18296">https://arxiv.org/abs/2508.18296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18296">https://arxiv.org/pdf/2508.18296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18296]] Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges(https://arxiv.org/abs/2508.18296)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Stroke is the second leading cause of death and the third leading cause of disability worldwide. Clinical guidelines establish diffusion resonance imaging (DWI, ADC) as the standard for localizing, characterizing, and measuring infarct volume, enabling treatment support and prognosis. Nonetheless, such lesion analysis is highly variable due to different patient demographics, scanner vendors, and expert annotations. Computational support approaches have been key to helping with the localization and segmentation of lesions. However, these strategies are dedicated solutions that learn patterns from only one institution, lacking the variability to generalize geometrical lesions shape models. Even worse, many clinical centers lack sufficient labeled samples to adjust these dedicated solutions. This work developed a collaborative framework for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge from deep center-independent representations. From 14 emulated healthcare centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm 0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm 0.26$ over all centers, outperforming both the centralized and other federated rules. Interestingly, the model demonstrated strong generalization properties, showing uniform performance across different lesion categories and reliable performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD of $4.44 \pm 8.74$ without any additional training).</li>
</ul>

<h3>Title: Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Ziaeetabar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18421">https://arxiv.org/abs/2508.18421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18421">https://arxiv.org/pdf/2508.18421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18421]] Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?(https://arxiv.org/abs/2508.18421)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision foundation models (FMs) have become the predominant architecture in computer vision, providing highly transferable representations learned from large-scale, multimodal corpora. Nonetheless, they exhibit persistent limitations on tasks that require explicit reasoning over entities, roles, and spatio-temporal relations. Such relational competence is indispensable for fine-grained human activity recognition, egocentric video understanding, and multimodal medical image analysis, where spatial, temporal, and semantic dependencies are decisive for performance. We advance the position that next-generation FMs should incorporate explicit relational interfaces, instantiated as dynamic relational graphs (graphs whose topology and edge semantics are inferred from the input and task context). We illustrate this position with cross-domain evidence from recent systems in human manipulation action recognition and brain tumor segmentation, showing that augmenting FMs with lightweight, context-adaptive graph-reasoning modules improves fine-grained semantic fidelity, out of distribution robustness, interpretability, and computational efficiency relative to FM only baselines. Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks.</li>
</ul>

<h3>Title: A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Anders Mølmen Høst, Pierre Lison, Leon Moonen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18439">https://arxiv.org/abs/2508.18439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18439">https://arxiv.org/pdf/2508.18439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18439]] A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs(https://arxiv.org/abs/2508.18439)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Vulnerability databases, such as the National Vulnerability Database (NVD), offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but often lack information on their real-world impact, such as the tactics, techniques, and procedures (TTPs) that adversaries may use to exploit the vulnerability. However, manually linking CVEs to their corresponding TTPs is a challenging and time-consuming task, and the high volume of new vulnerabilities published annually makes automated support desirable. This paper introduces TRIAGE, a two-pronged automated approach that uses Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK knowledge base. We first prompt an LLM with instructions based on MITRE's CVE Mapping Methodology to predict an initial list of techniques. This list is then combined with the results from a second LLM-based module that uses in-context learning to map a CVE to relevant techniques. This hybrid approach strategically combines rule-based reasoning with data-driven inference. Our evaluation reveals that in-context learning outperforms the individual mapping methods, and the hybrid approach improves recall of exploitation techniques. We also find that GPT-4o-mini performs better than Llama3.3-70B on this task. Overall, our results show that LLMs can be used to automatically predict the impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping CVEs to ATT&CK more efficient. Keywords: vulnerability impact, CVE, ATT&CK techniques, large language models, automated mapping.</li>
</ul>

<h3>Title: Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling</h3>
<ul>
<li><strong>Authors: </strong>Md. Rashid Shahriar Khan, Md. Abrar Hasan, Mohammod Tareq Aziz Justice</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18463">https://arxiv.org/abs/2508.18463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18463">https://arxiv.org/pdf/2508.18463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18463]] Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling(https://arxiv.org/abs/2508.18463)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in surveillance footage is inherently challenging due to their unpredictable and context-dependent nature. This work introduces a novel context-aware zero-shot anomaly detection framework that identifies abnormal events without exposure to anomaly examples during training. The proposed hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal dynamics and semantic context. TimeSformer serves as the vision backbone to extract rich spatial-temporal features, while DPC forecasts future representations to identify temporal deviations. Furthermore, a CLIP-based semantic stream enables concept-level anomaly detection through context-specific text prompts. These components are jointly trained using InfoNCE and CPC losses, aligning visual inputs with their temporal and semantic representations. A context-gating mechanism further enhances decision-making by modulating predictions with scene-aware cues or global video features. By integrating predictive modeling with vision-language understanding, the system can generalize to previously unseen behaviors in complex environments. This framework bridges the gap between temporal reasoning and semantic context in zero-shot anomaly detection for surveillance. The code for this research has been made available at this https URL.</li>
</ul>

<h3>Title: DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Bahareh Golchin, Banafsheh Rekabdar, Kunpeng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18474">https://arxiv.org/abs/2508.18474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18474">https://arxiv.org/pdf/2508.18474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18474]] DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection(https://arxiv.org/abs/2508.18474)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection in time series data is important for applications in finance, healthcare, sensor networks, and industrial monitoring. Traditional methods usually struggle with limited labeled data, high false-positive rates, and difficulty generalizing to novel anomaly types. To overcome these challenges, we propose a reinforcement learning-based framework that integrates dynamic reward shaping, Variational Autoencoder (VAE), and active learning, called DRTA. Our method uses an adaptive reward mechanism that balances exploration and exploitation by dynamically scaling the effect of VAE-based reconstruction error and classification rewards. This approach enables the agent to detect anomalies effectively in low-label systems while maintaining high precision and recall. Our experimental results on the Yahoo A1 and Yahoo A2 benchmark datasets demonstrate that the proposed method consistently outperforms state-of-the-art unsupervised and semi-supervised approaches. These findings show that our framework is a scalable and efficient solution for real-world anomaly detection tasks.</li>
</ul>

<h3>Title: DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance</h3>
<ul>
<li><strong>Authors: </strong>Ajinkya Khoche, Qingwen Zhang, Yixi Cai, Sina Sharif Mansouri, Patric Jensfelt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18506">https://arxiv.org/abs/2508.18506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18506">https://arxiv.org/pdf/2508.18506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18506]] DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance(https://arxiv.org/abs/2508.18506)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate 3D scene flow estimation is critical for autonomous systems to navigate dynamic environments safely, but creating the necessary large-scale, manually annotated datasets remains a significant bottleneck for developing robust perception models. Current self-supervised methods struggle to match the performance of fully supervised approaches, especially in challenging long-range and adverse weather scenarios, while supervised methods are not scalable due to their reliance on expensive human labeling. We introduce DoGFlow, a novel self-supervised framework that recovers full 3D object motions for LiDAR scene flow estimation without requiring any manual ground truth annotations. This paper presents our cross-modal label transfer approach, where DoGFlow computes motion pseudo-labels in real-time directly from 4D radar Doppler measurements and transfers them to the LiDAR domain using dynamic-aware association and ambiguity-resolved propagation. On the challenging MAN TruckScenes dataset, DoGFlow substantially outperforms existing self-supervised methods and improves label efficiency by enabling LiDAR backbones to achieve over 90% of fully supervised performance with only 10% of the ground truth data. For more details, please visit this https URL</li>
</ul>

<h3>Title: COMET-poly: Machine Translation Metric Grounded in Other Candidates</h3>
<ul>
<li><strong>Authors: </strong>Maike Züfle, Vilém Zouhar, Tu Anh Dinh, Felipe Maia Polo, Jan Niehues, Mrinmaya Sachan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18549">https://arxiv.org/abs/2508.18549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18549">https://arxiv.org/pdf/2508.18549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18549]] COMET-poly: Machine Translation Metric Grounded in Other Candidates(https://arxiv.org/abs/2508.18549)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Automated metrics for machine translation attempt to replicate human judgment. Unlike humans, who often assess a translation in the context of multiple alternatives, these metrics typically consider only the source sentence and a single translation. This discrepancy in the evaluation setup may negatively impact the performance of automated metrics. We propose two automated metrics that incorporate additional information beyond the single translation. COMET-polycand uses alternative translations of the same source sentence to compare and contrast with the translation at hand, thereby providing a more informed assessment of its quality. COMET-polyic, inspired by retrieval-based in-context learning, takes in translations of similar source texts along with their human-labeled quality scores to guide the evaluation. We find that including a single additional translation in COMET-polycand improves the segment-level metric performance (0.079 to 0.118 Kendall's tau-b correlation), with further gains when more translations are added. Incorporating retrieved examples in COMET-polyic yields similar improvements (0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.</li>
</ul>

<h3>Title: Enhancing Chemical Explainability Through Counterfactual Masking</h3>
<ul>
<li><strong>Authors: </strong>Łukasz Janisiów, Marek Kochańczyk, Bartosz Zieliński, Tomasz Danel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18561">https://arxiv.org/abs/2508.18561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18561">https://arxiv.org/pdf/2508.18561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18561]] Enhancing Chemical Explainability Through Counterfactual Masking(https://arxiv.org/abs/2508.18561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Molecular property prediction is a crucial task that guides the design of new compounds, including drugs and materials. While explainable artificial intelligence methods aim to scrutinize model predictions by identifying influential molecular substructures, many existing approaches rely on masking strategies that remove either atoms or atom-level features to assess importance via fidelity metrics. These methods, however, often fail to adhere to the underlying molecular distribution and thus yield unintuitive explanations. In this work, we propose counterfactual masking, a novel framework that replaces masked substructures with chemically reasonable fragments sampled from generative models trained to complete molecular graphs. Rather than evaluating masked predictions against implausible zeroed-out baselines, we assess them relative to counterfactual molecules drawn from the data distribution. Our method offers two key benefits: (1) molecular realism underpinning robust and distribution-consistent explanations, and (2) meaningful counterfactuals that directly indicate how structural modifications may affect predicted properties. We demonstrate that counterfactual masking is well-suited for benchmarking model explainers and yields more actionable insights across multiple datasets and property prediction tasks. Our approach bridges the gap between explainability and molecular design, offering a principled and generative path toward explainable machine learning in chemistry.</li>
</ul>

<h3>Title: ROSE: Remove Objects with Side Effects in Videos</h3>
<ul>
<li><strong>Authors: </strong>Chenxuan Miao, Yutong Feng, Jianshu Zeng, Zixiang Gao, Hantang Liu, Yunfeng Yan, Donglian Qi, Xi Chen, Bin Wang, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18633">https://arxiv.org/abs/2508.18633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18633">https://arxiv.org/pdf/2508.18633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18633]] ROSE: Remove Objects with Side Effects in Videos(https://arxiv.org/abs/2508.18633)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is this https URL.</li>
</ul>

<h3>Title: Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance</h3>
<ul>
<li><strong>Authors: </strong>Ifrah Tariq, Ernest Fraenkel</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18638">https://arxiv.org/abs/2508.18638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18638">https://arxiv.org/pdf/2508.18638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18638]] Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance(https://arxiv.org/abs/2508.18638)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet patient responses remain highly variable, and the biological mechanisms underlying resistance are poorly understood. While machine learning models hold promise for predicting responses to ICIs, most existing methods lack interpretability and do not effectively leverage the biological structure inherent to multi-omics data. Here, we introduce the Biologically Disentangled Variational Autoencoder (BDVAE), a deep generative model that integrates transcriptomic and genomic data through modality- and pathway-specific encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a modular encoder architecture combined with variational inference to learn biologically meaningful latent features associated with immune, genomic, and metabolic processes. Applied to a pan-cancer cohort of 366 patients across four cancer types treated with ICIs, BDVAE accurately predicts treatment response (AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance mechanisms, including immune suppression, metabolic shifts, and neuronal signaling. Importantly, BDVAE reveals that resistance spans a continuous biological spectrum rather than strictly binary states, reflecting gradations of tumor dysfunction. Several latent features correlate with survival outcomes and known clinical subtypes, demonstrating BDVAE's capability to generate interpretable, clinically relevant insights. These findings underscore the value of biologically structured machine learning in elucidating complex resistance patterns and guiding precision immunotherapy strategies.</li>
</ul>

<h3>Title: Thinking Before You Speak: A Proactive Test-time Scaling Approach</h3>
<ul>
<li><strong>Authors: </strong>Cong Li, Wenchang Chai, Hejun Wu, Yan Pan, Pengxu Wei, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18648">https://arxiv.org/abs/2508.18648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18648">https://arxiv.org/pdf/2508.18648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18648]] Thinking Before You Speak: A Proactive Test-time Scaling Approach(https://arxiv.org/abs/2508.18648)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often exhibit deficiencies with complex reasoning tasks, such as maths, which we attribute to the discrepancy between human reasoning patterns and those presented in the LLMs' training data. When dealing with complex problems, humans tend to think carefully before expressing solutions. However, they often do not articulate their inner thoughts, including their intentions and chosen methodologies. Consequently, critical insights essential for bridging reasoning steps may be absent in training data collected from human sources. To bridge this gap, we proposes inserting \emph{insight}s between consecutive reasoning steps, which review the status and initiate the next reasoning steps. Unlike prior prompting strategies that rely on a single or a workflow of static prompts to facilitate reasoning, \emph{insight}s are \emph{proactively} generated to guide reasoning processes. We implement our idea as a reasoning framework, named \emph{Thinking Before You Speak} (TBYS), and design a pipeline for automatically collecting and filtering in-context examples for the generation of \emph{insight}s, which alleviates human labeling efforts and fine-tuning overheads. Experiments on challenging mathematical datasets verify the effectiveness of TBYS. Project website: this https URL</li>
</ul>

<h3>Title: FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge</h3>
<ul>
<li><strong>Authors: </strong>Gang Hu, Yinglei Teng, Pengfei Wu, Nan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18663">https://arxiv.org/abs/2508.18663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18663">https://arxiv.org/pdf/2508.18663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18663]] FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge(https://arxiv.org/abs/2508.18663)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>As FMs drive progress toward Artificial General Intelligence (AGI), fine-tuning them under privacy and resource constraints has become increasingly critical particularly when highquality training data resides on distributed edge devices. Federated Learning (FL) offers a compelling solution through Federated Fine-Tuning (FFT), which enables collaborative model adaptation without sharing raw data. Recent approaches incorporate Parameter-Efficient Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce computational overhead. However, LoRA-based FFT faces two major limitations in heterogeneous FL environments: structural incompatibility across clients with varying LoRA configurations and limited adaptability to non-IID data distributions, which hinders convergence and generalization. To address these challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight gating network to selectively activate a personalized subset of experts, enabling fine-grained adaptation to local resource budgets while preserving aggregation compatibility. To further combat the expert load imbalance caused by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary loss that dynamically regularizes the routing distribution to ensure expert diversity and balanced utilization. Extensive experiments spanning both IID and non-IID conditions demonstrate that FFT MoE consistently outperforms state of the art FFT baselines in generalization performance and training efficiency.</li>
</ul>

<h3>Title: Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Yi Liu, Hongji Zhang, Yiwen Wang, Dimitris Tsaras, Lei Chen, Mingxuan Yuan, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18730">https://arxiv.org/abs/2508.18730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18730">https://arxiv.org/pdf/2508.18730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18730]] Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning(https://arxiv.org/abs/2508.18730)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Estimating the quality of register transfer level (RTL) designs is crucial in the electronic design automation (EDA) workflow, as it enables instant feedback on key metrics like area and delay without the need for time-consuming logic synthesis. While recent approaches have leveraged large language models (LLMs) to derive embeddings from RTL code and achieved promising results, they overlook the structural semantics essential for accurate quality estimation. In contrast, the control data flow graph (CDFG) view exposes the design's structural characteristics more explicitly, offering richer cues for representation learning. In this work, we introduce a novel structure-aware graph self-supervised learning framework, StructRTL, for improved RTL design quality estimation. By learning structure-informed representations from CDFGs, our method significantly outperforms prior art on various quality estimation tasks. To further boost performance, we incorporate a knowledge distillation strategy that transfers low-level insights from post-mapping netlists into the CDFG predictor. Experiments show that our approach establishes new state-of-the-art results, demonstrating the effectiveness of combining structural learning with cross-stage supervision.</li>
</ul>

<h3>Title: Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings</h3>
<ul>
<li><strong>Authors: </strong>Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18733">https://arxiv.org/abs/2508.18733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18733">https://arxiv.org/pdf/2508.18733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18733]] Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings(https://arxiv.org/abs/2508.18733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods</h3>
<ul>
<li><strong>Authors: </strong>Qinqian Lei, Bo Wang, Robby T. Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18753">https://arxiv.org/abs/2508.18753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18753">https://arxiv.org/pdf/2508.18753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18753]] Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods(https://arxiv.org/abs/2508.18753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prior human-object interaction (HOI) detection methods have integrated early vision-language models (VLMs) such as CLIP, but only as supporting components within their frameworks. In contrast, recent advances in large, generative VLMs suggest that these models may already possess strong ability to understand images involving HOI. This naturally raises an important question: can general-purpose standalone VLMs effectively solve HOI detection, and how do they compare with specialized HOI methods? Answering this requires a benchmark that can accommodate both paradigms. However, existing HOI benchmarks such as HICO-DET were developed before the emergence of modern VLMs, and their evaluation protocols require exact matches to annotated HOI classes. This is poorly aligned with the generative nature of VLMs, which often yield multiple valid interpretations in ambiguous cases. For example, a static image may capture a person mid-motion with a frisbee, which can plausibly be interpreted as either "throwing" or "catching". When only "catching" is annotated, the other, though equally plausible for the image, is marked incorrect when exact matching is used. As a result, correct predictions might be penalized, affecting both VLMs and HOI-specific methods. To avoid penalizing valid predictions, we introduce a new benchmark that reformulates HOI detection as a multiple-answer multiple-choice task, where each question includes only ground-truth positive options and a curated set of negatives that are constructed to reduce ambiguity (e.g., when "catching" is annotated, "throwing" is not selected as a negative to avoid penalizing valid predictions). The proposed evaluation protocol is the first of its kind for both VLMs and HOI methods, enabling direct comparison and offering new insight into the current state of progress in HOI understanding.</li>
</ul>

<h3>Title: UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Zihao Huang, Yu Bao, Qiyang Min, Siyan Chen, Ran Guo, Hongzhi Huang, Defa Zhu, Yutao Zeng, Banggu Wu, Xun Zhou, Siyuan Qiao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18756">https://arxiv.org/abs/2508.18756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18756">https://arxiv.org/pdf/2508.18756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18756]] UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning(https://arxiv.org/abs/2508.18756)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every transformer block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total sparse parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient sparse computation.</li>
</ul>

<h3>Title: EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding</h3>
<ul>
<li><strong>Authors: </strong>Luqing Luo, Wenjin Gui, Yunfei Liu, Ziyue Zhang, Yunxi Zhang, Fengxiang Wang, Zonghao Guo, Zizhi Ma, Xinzhu Liu, Hanxiang He, Jinhai Li, Xin Qiu, Wupeng Xie, Yangang Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18785">https://arxiv.org/abs/2508.18785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18785">https://arxiv.org/pdf/2508.18785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18785]] EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding(https://arxiv.org/abs/2508.18785)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Deep understanding of electromagnetic signals is fundamental to dynamic spectrum management, intelligent transportation, autonomous driving and unmanned vehicle perception. The field faces challenges because electromagnetic signals differ greatly from text and images, showing high heterogeneity, strong background noise and complex joint time frequency structure, which prevents existing general models from direct use. Electromagnetic communication and sensing tasks are diverse, current methods lack cross task generalization and transfer efficiency, and the scarcity of large high quality datasets blocks the creation of a truly general multitask learning framework. To overcome these issue, we introduce EMind, an electromagnetic signals foundation model that bridges large scale pretraining and the unique nature of this modality. We build the first unified and largest standardized electromagnetic signal dataset covering multiple signal types and tasks. By exploiting the physical properties of electromagnetic signals, we devise a length adaptive multi-signal packing method and a hardware-aware training strategy that enable efficient use and representation learning from heterogeneous multi-source signals. Experiments show that EMind achieves strong performance and broad generalization across many downstream tasks, moving decisively from task specific models to a unified framework for electromagnetic intelligence. The code is available at: this https URL.</li>
</ul>

<h3>Title: A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Tao, Yizhe Zhang, Qiang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18790">https://arxiv.org/abs/2508.18790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18790">https://arxiv.org/pdf/2508.18790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18790]] A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework(https://arxiv.org/abs/2508.18790)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The development of artificial intelligence models for macular edema (ME) analy-sis always relies on expert-annotated pixel-level image datasets which are expen-sive to collect prospectively. While anomaly-detection-based weakly-supervised methods have shown promise in edema area (EA) segmentation task, their per-formance still lags behind fully-supervised approaches. In this paper, we leverage the strong correlation between EA and retinal layers in spectral-domain optical coherence tomography (SD-OCT) images, along with the update characteristics of weakly-supervised learning, to enhance an off-the-shelf adversarial framework for EA segmentation with a novel layer-structure-guided post-processing step and a test-time-adaptation (TTA) strategy. By incorporating additional retinal lay-er information, our framework reframes the dense EA prediction task as one of confirming intersection points between the EA contour and retinal layers, result-ing in predictions that better align with the shape prior of EA. Besides, the TTA framework further helps address discrepancies in the manifestations and presen-tations of EA between training and test sets. Extensive experiments on two pub-licly available datasets demonstrate that these two proposed ingredients can im-prove the accuracy and robustness of EA segmentation, bridging the gap between weakly-supervised and fully-supervised models.</li>
</ul>

<h3>Title: LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Shubham Gupta, Shraban Kumar Chatterjee, Suman Kundu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18819">https://arxiv.org/abs/2508.18819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18819">https://arxiv.org/pdf/2508.18819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18819]] LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection(https://arxiv.org/abs/2508.18819)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The proliferation of misinformation in the digital age has led to significant societal challenges. Existing approaches often struggle with capturing long-range dependencies, complex semantic relations, and the social dynamics influencing news dissemination. Furthermore, these methods require extensive labelled datasets, making their deployment resource-intensive. In this study, we propose a novel self-supervised misinformation detection framework that integrates both complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. We introduce an LLM-based graph contrastive loss (LGCL) that utilizes negative anchor points generated by a Large Language Model (LLM) to enhance feature separability in a zero-shot manner. To incorporate social context, we employ a multi view graph masked autoencoder, which learns news propagation features from social context graph. By combining these semantic and propagation-based features, our approach effectively differentiates between fake and real news in a self-supervised manner. Extensive experiments demonstrate that our self-supervised framework achieves superior performance compared to other state-of-the-art methodologies, even with limited labelled datasets while improving generalizability.</li>
</ul>

<h3>Title: Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory</h3>
<ul>
<li><strong>Authors: </strong>Takayuki Ishikawa, Carmelo Bonannella, Bas J. W. Lerink, Marc Rußwurm</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18829">https://arxiv.org/abs/2508.18829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18829">https://arxiv.org/pdf/2508.18829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18829]] Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory(https://arxiv.org/abs/2508.18829)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>National Forest Inventory (NFI)s serve as the primary source of forest information, providing crucial tree species distribution data. However, maintaining these inventories requires labor-intensive on-site campaigns. Remote sensing approaches, particularly when combined with machine learning, offer opportunities to update NFIs more frequently and at larger scales. While the use of Satellite Image Time Series has proven effective for distinguishing tree species through seasonal canopy reflectance patterns, current approaches rely primarily on Random Forest classifiers with hand-designed features and phenology-based metrics. Using deep features from an available pre-trained remote sensing foundation models offers a complementary strategy. These pre-trained models leverage unannotated global data and are meant to used for general-purpose applications and can then be efficiently fine-tuned with smaller labeled datasets for specific classification tasks. This work systematically investigates how deep features improve tree species classification accuracy in the Netherlands with few annotated data. Data-wise, we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites data and SRTM data using Google Earth Engine. Our results demonstrate that fine-tuning a publicly available remote sensing time series foundation model outperforms the current state-of-the-art in NFI classification in the Netherlands by a large margin of up to 10% across all datasets. This demonstrates that classic hand-defined harmonic features are too simple for this task and highlights the potential of using deep AI features for data-limited application like NFI classification. By leveraging openly available satellite data and pre-trained models, this approach significantly improves classification accuracy compared to traditional methods and can effectively complement existing forest inventory processes.</li>
</ul>

<h3>Title: Energy-Based Flow Matching for Generating 3D Molecular Structure</h3>
<ul>
<li><strong>Authors: </strong>Wenyin Zhou, Christopher Iliffe Sprague, Vsevolod Viliuga, Matteo Tadiello, Arne Elofsson, Hossein Azizpour</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18949">https://arxiv.org/abs/2508.18949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18949">https://arxiv.org/pdf/2508.18949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18949]] Energy-Based Flow Matching for Generating 3D Molecular Structure(https://arxiv.org/abs/2508.18949)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Molecular structure generation is a fundamental problem that involves determining the 3D positions of molecules' constituents. It has crucial biological applications, such as molecular docking, protein folding, and molecular design. Recent advances in generative modeling, such as diffusion models and flow matching, have made great progress on these tasks by modeling molecular conformations as a distribution. In this work, we focus on flow matching and adopt an energy-based perspective to improve training and inference of structure generation models. Our view results in a mapping function, represented by a deep network, that is directly learned to \textit{iteratively} map random configurations, i.e. samples from the source distribution, to target structures, i.e. points in the data manifold. This yields a conceptually simple and empirically effective flow matching setup that is theoretically justified and has interesting connections to fundamental properties such as idempotency and stability, as well as the empirically useful techniques such as structure refinement in AlphaFold. Experiments on protein docking as well as protein backbone generation consistently demonstrate the method's effectiveness, where it outperforms recent baselines of task-associated flow matching and diffusion models, using a similar computational budget.</li>
</ul>

<h3>Title: Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers</h3>
<ul>
<li><strong>Authors: </strong>Claudio Affolter, Sidi Wu, Yizi Chen, Lorenz Hurni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18959">https://arxiv.org/abs/2508.18959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18959">https://arxiv.org/pdf/2508.18959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18959]] Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers(https://arxiv.org/abs/2508.18959)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Traditional map-making relies heavily on Geographic Information Systems (GIS), requiring domain expertise and being time-consuming, especially for repetitive tasks. Recent advances in generative AI (GenAI), particularly image diffusion models, offer new opportunities for automating and democratizing the map-making process. However, these models struggle with accurate map creation due to limited control over spatial composition and semantic layout. To address this, we integrate vector data to guide map generation in different styles, specified by the textual prompts. Our model is the first to generate accurate maps in controlled styles, and we have integrated it into a web application to improve its usability and accessibility. We conducted a user study with professional cartographers to assess the fidelity of generated maps, the usability of the web application, and the implications of ever-emerging GenAI in map-making. The findings have suggested the potential of our developed application and, more generally, the GenAI models in helping both non-expert users and professionals in creating maps more efficiently. We have also outlined further technical improvements and emphasized the new role of cartographers to advance the paradigm of AI-assisted map-making.</li>
</ul>

<h3>Title: Can we make NeRF-based visual localization privacy-preserving?</h3>
<ul>
<li><strong>Authors: </strong>Maxime Pietrantoni, Martin Humenberger, Torsten Sattler, Gabriela Csurka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.18971">https://arxiv.org/abs/2508.18971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.18971">https://arxiv.org/pdf/2508.18971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.18971]] Can we make NeRF-based visual localization privacy-preserving?(https://arxiv.org/abs/2508.18971)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Visual localization (VL) is the task of estimating the camera pose in a known scene. VL methods, a.o., can be distinguished based on how they represent the scene, e.g., explicitly through a (sparse) point cloud or a collection of images or implicitly through the weights of a neural network. Recently, NeRF-based methods have become popular for VL. While NeRFs offer high-quality novel view synthesis, they inadvertently encode fine scene details, raising privacy concerns when deployed in cloud-based localization services as sensitive information could be recovered. In this paper, we tackle this challenge on two ends. We first propose a new protocol to assess privacy-preservation of NeRF-based representations. We show that NeRFs trained with photometric losses store fine-grained details in their geometry representations, making them vulnerable to privacy attacks, even if the head that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving Neural Segmentation Field), a NeRF variant trained with segmentation supervision instead of RGB images. These segmentation labels are learned in a self-supervised manner, ensuring they are coarse enough to obscure identifiable scene details while remaining discriminativeness in 3D. The segmentation space of ppNeSF can be used for accurate visual localization, yielding state-of-the-art results.</li>
</ul>

<h3>Title: STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems</h3>
<ul>
<li><strong>Authors: </strong>Gary Simethy, Daniel Ortiz-Arroyo, Petar Durdevic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19011">https://arxiv.org/abs/2508.19011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19011">https://arxiv.org/pdf/2508.19011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19011]] STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems(https://arxiv.org/abs/2508.19011)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Most deep learning methods for imputing missing values treat the task as completing patterns within a fixed time window. This assumption often fails in industrial systems, where dynamics are driven by control actions, are highly non-stationary, and can experience long, uninterrupted gaps. We propose STDiff, which reframes imputation as learning how the system evolves from one state to the next. STDiff uses a conditional denoising diffusion model with a causal bias aligned to control theory, generating missing values step-by-step based on the most recent known state and relevant control or environmental inputs. On a public wastewater treatment dataset with simulated missing blocks, STDiff consistently achieves the lowest errors, with its advantage increasing for longer gaps. On a raw industrial dataset with substantial real gaps, it produces trajectories that remain dynamically plausible, in contrast to window-based models that tend to flatten or over-smooth. These results support dynamics-aware, explicitly conditioned imputation as a robust approach for industrial time series, and we discuss computational trade-offs and extensions to broader domains.</li>
</ul>

<h3>Title: Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Sidahmed Benabderrahmane, Talal Rahwan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19019">https://arxiv.org/abs/2508.19019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19019">https://arxiv.org/pdf/2508.19019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19019]] Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence(https://arxiv.org/abs/2508.19019)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Advanced Persistent Threats (APTs) pose a severe challenge to cyber defense due to their stealthy behavior and the extreme class imbalance inherent in detection datasets. To address these issues, we propose a novel active learning-based anomaly detection framework that leverages similarity search to iteratively refine the decision space. Built upon an Attention-Based Autoencoder, our approach uses feature-space similarity to identify normal-like and anomaly-like instances, thereby enhancing model robustness with minimal oracle supervision. Crucially, we perform a formal evaluation of various similarity measures to understand their influence on sample selection and anomaly ranking effectiveness. Through experiments on diverse datasets, including DARPA Transparent Computing APT traces, we demonstrate that the choice of similarity metric significantly impacts model convergence, anomaly detection accuracy, and label efficiency. Our results offer actionable insights for selecting similarity functions in active learning pipelines tailored for threat intelligence and cyber defense.</li>
</ul>

<h3>Title: When recalling in-context, Transformers are not SSMs</h3>
<ul>
<li><strong>Authors: </strong>Destiny Okpekpe, Antonio Orvieto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19029">https://arxiv.org/abs/2508.19029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19029">https://arxiv.org/pdf/2508.19029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19029]] When recalling in-context, Transformers are not SSMs(https://arxiv.org/abs/2508.19029)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Despite the advantageous subquadratic complexity of modern recurrent deep learning models -- such as state-space models (SSMs) -- recent studies have highlighted their potential shortcomings compared to transformers on reasoning and memorization tasks. In this paper, we dive deeper into one of such benchmarks: associative recall (AR), which has been shown to correlate well with language modeling performance, and inspect in detail the effects of scaling and optimization issues in recently proposed token mixing strategies. We first demonstrate that, unlike standard transformers, the choice of learning rate plays a critical role in the performance of modern recurrent models: an issue that can severely affect reported performance in previous works and suggests further research is needed to stabilize training. Next, we show that recurrent and attention-based models exhibit contrasting benefits when scaling in width as opposed to depth, with attention being notably unable to solve AR when limited to a single layer. We then further inspect 1-layer transformers, revealing that despite their poor performance, their training dynamics surprisingly resemble the formation of induction heads, a phenomenon previously observed only in their 2-layer counterparts. Finally, through architectural ablations, we study how components affects Transformer and Mamba's performance and optimization stability.</li>
</ul>

<h3>Title: GReAT: leveraging geometric artery data to improve wall shear stress assessment</h3>
<ul>
<li><strong>Authors: </strong>Julian Suk, Jolanda J. Wentzel, Patryk Rygiel, Joost Daemen, Daniel Rueckert, Jelmer M. Wolterink</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19030">https://arxiv.org/abs/2508.19030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19030">https://arxiv.org/pdf/2508.19030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19030]] GReAT: leveraging geometric artery data to improve wall shear stress assessment(https://arxiv.org/abs/2508.19030)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Leveraging big data for patient care is promising in many medical fields such as cardiovascular health. For example, hemodynamic biomarkers like wall shear stress could be assessed from patient-specific medical images via machine learning algorithms, bypassing the need for time-intensive computational fluid simulation. However, it is extremely challenging to amass large-enough datasets to effectively train such models. We could address this data scarcity by means of self-supervised pre-training and foundations models given large datasets of geometric artery models. In the context of coronary arteries, leveraging learned representations to improve hemodynamic biomarker assessment has not yet been well studied. In this work, we address this gap by investigating whether a large dataset (8449 shapes) consisting of geometric models of 3D blood vessels can benefit wall shear stress assessment in coronary artery models from a small-scale clinical trial (49 patients). We create a self-supervised target for the 3D blood vessels by computing the heat kernel signature, a quantity obtained via Laplacian eigenvectors, which captures the very essence of the shapes. We show how geometric representations learned from this datasets can boost segmentation of coronary arteries into regions of low, mid and high (time-averaged) wall shear stress even when trained on limited data.</li>
</ul>

<h3>Title: No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes</h3>
<ul>
<li><strong>Authors: </strong>Blaž Rolih, Matic Fučka, Danijel Skočaj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19060">https://arxiv.org/abs/2508.19060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19060">https://arxiv.org/pdf/2508.19060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19060]] No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes(https://arxiv.org/abs/2508.19060)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: this https URL</li>
</ul>

<h3>Title: "Where does it hurt?" - Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Tom Röhr, Soumyadeep Roy, Fares Al Mohamad, Jens-Michalis Papaioannou, Wolfgang Nejdl, Felix Gers, Alexander Löser</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19077">https://arxiv.org/abs/2508.19077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19077">https://arxiv.org/pdf/2508.19077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19077]] "Where does it hurt?" - Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues(https://arxiv.org/abs/2508.19077)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In a doctor-patient dialogue, the primary objective of physicians is to diagnose patients and propose a treatment plan. Medical doctors guide these conversations through targeted questioning to efficiently gather the information required to provide the best possible outcomes for patients. To the best of our knowledge, this is the first work that studies physician intent trajectories in doctor-patient dialogues. We use the `Ambient Clinical Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with medical professionals to develop a fine-grained taxonomy of physician intents based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We then conduct a large-scale annotation effort to label over 5000 doctor-patient turns with the help of a large number of medical experts recruited using Prolific, a popular crowd-sourcing platform. This large labeled dataset is an important resource contribution that we use for benchmarking the state-of-the-art generative and encoder models for medical intent classification tasks. Our findings show that our models understand the general structure of medical dialogues with high accuracy, but often fail to identify transitions between SOAP categories. We also report for the first time common trajectories in medical dialogue structures that provide valuable insights for designing `differential diagnosis' systems. Finally, we extensively study the impact of intent filtering for medical dialogue summarization and observe a significant boost in performance. We make the codes and data, including annotation guidelines, publicly available at this https URL.</li>
</ul>

<h3>Title: It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yue Li, Zhixue Zhao, Carolina Scarton</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19089">https://arxiv.org/abs/2508.19089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19089">https://arxiv.org/pdf/2508.19089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19089]] It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs(https://arxiv.org/abs/2508.19089)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Extremely low-resource languages, especially those written in rare scripts, as shown in Figure 1, remain largely unsupported by large language models (LLMs). This is due in part to compounding factors such as the lack of training data. This paper delivers the first comprehensive analysis of whether LLMs can acquire such languages purely via in-context learning (ICL), with or without auxiliary alignment signals, and how these methods compare to parameter-efficient fine-tuning (PEFT). We systematically evaluate 20 under-represented languages across three state-of-the-art multilingual LLMs. Our findings highlight the limitation of PEFT when both language and its script are extremely under-represented by the LLM. In contrast, zero-shot ICL with language alignment is impressively effective on extremely low-resource languages, while few-shot ICL or PEFT is more beneficial for languages relatively better represented by LLMs. For LLM practitioners working on extremely low-resource languages, we summarise guidelines grounded by our results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning a multilingual model on languages of unseen scripts.</li>
</ul>

<h3>Title: Composition and Alignment of Diffusion Models using Constrained Learning</h3>
<ul>
<li><strong>Authors: </strong>Shervin Khalafi, Ignacio Hounie, Dongsheng Ding, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19104">https://arxiv.org/abs/2508.19104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19104">https://arxiv.org/pdf/2508.19104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19104]] Composition and Alignment of Diffusion Models using Constrained Learning(https://arxiv.org/abs/2508.19104)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have become prevalent in generative modeling due to their ability to sample from complex distributions. To improve the quality of generated samples and their compliance with user requirements, two commonly used methods are: (i) Alignment, which involves fine-tuning a diffusion model to align it with a reward; and (ii) Composition, which combines several pre-trained diffusion models, each emphasizing a desirable attribute in the generated outputs. However, trade-offs often arise when optimizing for multiple rewards or combining multiple models, as they can often represent competing properties. Existing methods cannot guarantee that the resulting model faithfully generates samples with all the desired properties. To address this gap, we propose a constrained optimization framework that unifies alignment and composition of diffusion models by enforcing that the aligned model satisfies reward constraints and/or remains close to (potentially multiple) pre-trained models. We provide a theoretical characterization of the solutions to the constrained alignment and composition problems and develop a Lagrangian-based primal-dual training algorithm to approximate these solutions. Empirically, we demonstrate the effectiveness and merits of our proposed approach in image generation, applying it to alignment and composition, and show that our aligned or composed model satisfies constraints effectively, and improves on the equally-weighted approach. Our implementation can be found at this https URL.</li>
</ul>

<h3>Title: Saddle Hierarchy in Dense Associative Memory</h3>
<ul>
<li><strong>Authors: </strong>Robin Thériault, Daniele Tantari</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19151">https://arxiv.org/abs/2508.19151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19151">https://arxiv.org/pdf/2508.19151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19151]] Saddle Hierarchy in Dense Associative Memory(https://arxiv.org/abs/2508.19151)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Dense associative memory (DAM) models have been attracting renewed attention since they were shown to be robust to adversarial examples and closely related to state-of-the-art machine learning paradigms, such as the attention mechanisms in transformers and generative diffusion models. We study a DAM built upon a three-layer Boltzmann machine with Potts hidden units, which represent data clusters and classes. Through a statistical mechanics analysis, we derive saddle-point equations that characterize both the stationary points of DAMs trained on real data and the fixed points of DAMs trained on synthetic data within a teacher-student framework. Based on these results, we propose a novel regularization scheme that makes training significantly more stable. Moreover, we show empirically that our DAM learns interpretable solutions to both supervised and unsupervised classification problems. Pushing our theoretical analysis further, we find that the weights learned by relatively small DAMs correspond to unstable saddle points in larger DAMs. We implement a network-growing algorithm that leverages this saddle-point hierarchy to drastically reduce the computational cost of training dense associative memory.</li>
</ul>

<h3>Title: RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yan Chen, Yi Wen, Wei Li, Junchao Liu, Yong Guo, Jie Hu, Xinghao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19154">https://arxiv.org/abs/2508.19154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19154">https://arxiv.org/pdf/2508.19154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19154]] RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration(https://arxiv.org/abs/2508.19154)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present the RAW domain diffusion model (RDDM), an end-to-end diffusion model that restores photo-realistic images directly from the sensor RAW data. While recent sRGB-domain diffusion methods achieve impressive results, they are caught in a dilemma between high fidelity and realistic generation. As these models process lossy sRGB inputs and neglect the accessibility of the sensor RAW images in many scenarios, e.g., in image and video capturing in edge devices, resulting in sub-optimal performance. RDDM bypasses this limitation by directly restoring images in the RAW domain, replacing the conventional two-stage image signal processing (ISP) + IR pipeline. However, a simple adaptation of pre-trained diffusion models to the RAW domain confronts the out-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE (RVAE) learning optimal latent representations, (2) a differentiable Post Tone Processing (PTP) module enabling joint RAW and sRGB space optimization. To compensate for the deficiency in the dataset, we develop a scalable degradation pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for large-scale training. Furthermore, we devise a configurable multi-bayer (CMB) LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive experiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion methods, yielding higher fidelity results with fewer artifacts.</li>
</ul>

<h3>Title: All-in-One Slider for Attribute Manipulation in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Weixin Ye, Hongguang Zhu, Wei Wang, Yahui Liu, Mengyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19195">https://arxiv.org/abs/2508.19195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19195">https://arxiv.org/pdf/2508.19195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19195]] All-in-One Slider for Attribute Manipulation in Diffusion Models(https://arxiv.org/abs/2508.19195)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have made significant strides in generating high-quality images. However, progressively manipulating certain attributes of generated images to meet the desired user expectations remains challenging, particularly for content with rich details, such as human faces. Some studies have attempted to address this by training slider modules. However, they follow a One-for-One manner, where an independent slider is trained for each attribute, requiring additional training whenever a new attribute is introduced. This not only results in parameter redundancy accumulated by sliders but also restricts the flexibility of practical applications and the scalability of attribute manipulation. To address this issue, we introduce the All-in-One Slider, a lightweight module that decomposes the text embedding space into sparse, semantically meaningful attribute directions. Once trained, it functions as a general-purpose slider, enabling interpretable and fine-grained continuous control over various attributes. Moreover, by recombining the learned directions, the All-in-One Slider supports zero-shot manipulation of unseen attributes (e.g., races and celebrities) and the composition of multiple attributes. Extensive experiments demonstrate that our method enables accurate and scalable attribute manipulation, achieving notable improvements compared to previous methods. Furthermore, our method can be extended to integrate with the inversion framework to perform attribute manipulation on real images, broadening its applicability to various real-world scenarios. The code and trained model will be released at: this https URL.</li>
</ul>

<h3>Title: Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19202">https://arxiv.org/abs/2508.19202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19202">https://arxiv.org/pdf/2508.19202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19202]] Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning(https://arxiv.org/abs/2508.19202)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning.</li>
</ul>

<h3>Title: LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding</h3>
<ul>
<li><strong>Authors: </strong>Julian Ost, Andrea Ramazzina, Amogh Joshi, Maximilian Bömer, Mario Bijelic, Felix Heide</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19204">https://arxiv.org/abs/2508.19204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19204">https://arxiv.org/pdf/2508.19204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19204]] LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding(https://arxiv.org/abs/2508.19204)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale scene data is essential for training and testing in robot learning. Neural reconstruction methods have promised the capability of reconstructing large physically-grounded outdoor scenes from captured sensor data. However, these methods have baked-in static environments and only allow for limited scene control -- they are functionally constrained in scene and trajectory diversity by the captures from which they are reconstructed. In contrast, generating driving data with recent image or video diffusion models offers control, however, at the cost of geometry grounding and causality. In this work, we aim to bridge this gap and present a method that directly generates large-scale 3D driving scenes with accurate geometry, allowing for causal novel view synthesis with object permanence and explicit 3D geometry estimation. The proposed method combines the generation of a proxy geometry and environment representation with score distillation from learned 2D image priors. We find that this approach allows for high controllability, enabling the prompt-guided geometry and high-fidelity texture and structure that can be conditioned on map layouts -- producing realistic and geometrically consistent 3D generations of complex driving scenes.</li>
</ul>

<h3>Title: VibeVoice Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, Shaohan Huang, Yan Xia, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19205">https://arxiv.org/abs/2508.19205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19205">https://arxiv.org/pdf/2508.19205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19205]] VibeVoice Technical Report(https://arxiv.org/abs/2508.19205)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.</li>
</ul>

<h3>Title: Generative Interfaces for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19227">https://arxiv.org/abs/2508.19227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19227">https://arxiv.org/pdf/2508.19227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19227]] Generative Interfaces for Language Models(https://arxiv.org/abs/2508.19227)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with humans preferring them in over 70% of cases. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.</li>
</ul>

<h3>Title: Autoregressive Universal Video Segmentation Model</h3>
<ul>
<li><strong>Authors: </strong>Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19242">https://arxiv.org/abs/2508.19242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19242">https://arxiv.org/pdf/2508.19242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19242]] Autoregressive Universal Video Segmentation Model(https://arxiv.org/abs/2508.19242)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent video foundation models such as SAM2 excel at prompted video segmentation by treating masks as a general-purpose primitive. However, many real-world settings require unprompted segmentation that aims to detect and track all objects in a video without external cues, leaving today's landscape fragmented across task-specific models and pipelines. We recast streaming video segmentation as sequential mask prediction, analogous to language modeling, and introduce the Autoregressive Universal Segmentation Model (AUSM), a single architecture that unifies both prompted and unprompted video segmentation. Built on recent state-space models, AUSM maintains a fixed-size spatial state and scales to video streams of arbitrary length. Furthermore, all components of AUSM are designed for parallel training across frames, yielding substantial speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS 2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training on 16-frame sequences.</li>
</ul>

<h3>Title: Articulate3D: Zero-Shot Text-Driven 3D Object Posing</h3>
<ul>
<li><strong>Authors: </strong>Oishi Deb, Anjun Hu, Ashkan Khakzar, Philip Torr, Christian Rupprecht</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19244">https://arxiv.org/abs/2508.19244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19244">https://arxiv.org/pdf/2508.19244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19244]] Articulate3D: Zero-Shot Text-Driven 3D Object Posing(https://arxiv.org/abs/2508.19244)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a training-free method, Articulate3D, to pose a 3D asset through language control. Despite advances in vision and language models, this task remains surprisingly challenging. To achieve this goal, we decompose the problem into two steps. We modify a powerful image-generator to create target images conditioned on the input image and a text instruction. We then align the mesh to the target images through a multi-view pose optimisation step. In detail, we introduce a self-attention rewiring mechanism (RSActrl) that decouples the source structure from pose within an image generative model, allowing it to maintain a consistent structure across varying poses. We observed that differentiable rendering is an unreliable signal for articulation optimisation; instead, we use keypoints to establish correspondences between input and target images. The effectiveness of Articulate3D is demonstrated across a diverse range of 3D objects and free-form text prompts, successfully manipulating poses while maintaining the original identity of the mesh. Quantitative evaluations and a comparative user study, in which our method was preferred over 85\% of the time, confirm its superiority over existing approaches. Project page:this https URL</li>
</ul>

<h3>Title: VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space</h3>
<ul>
<li><strong>Authors: </strong>Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.19247">https://arxiv.org/abs/2508.19247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.19247">https://arxiv.org/pdf/2508.19247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.19247]] VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space(https://arxiv.org/abs/2508.19247)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
