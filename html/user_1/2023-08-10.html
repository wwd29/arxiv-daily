<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: 3D Scene Diffusion Guidance using Scene Graphs. (arXiv:2308.04468v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04468">http://arxiv.org/abs/2308.04468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04468]] 3D Scene Diffusion Guidance using Scene Graphs(http://arxiv.org/abs/2308.04468)</code></li>
<li>Summary: <p>Guided synthesis of high-quality 3D scenes is a challenging task. Diffusion
models have shown promise in generating diverse data, including 3D scenes.
However, current methods rely directly on text embeddings for controlling the
generation, limiting the incorporation of complex spatial relationships between
objects. We propose a novel approach for 3D scene diffusion guidance using
scene graphs. To leverage the relative spatial information the scene graphs
provide, we make use of relational graph convolutional blocks within our
denoising network. We show that our approach significantly improves the
alignment between scene description and generated scene.
</p></li>
</ul>

<h3>Title: IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models. (arXiv:2308.04995v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04995">http://arxiv.org/abs/2308.04995</a></li>
<li>Code URL: https://github.com/fdbtrs/IDiff-Face</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04995]] IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models(http://arxiv.org/abs/2308.04995)</code></li>
<li>Summary: <p>The availability of large-scale authentic face databases has been crucial to
the significant advances made in face recognition research over the past
decade. However, legal and ethical concerns led to the recent retraction of
many of these databases by their creators, raising questions about the
continuity of future face recognition research without one of its key
resources. Synthetic datasets have emerged as a promising alternative to
privacy-sensitive authentic data for face recognition development. However,
recent synthetic datasets that are used to train face recognition models suffer
either from limitations in intra-class diversity or cross-class (identity)
discrimination, leading to less optimal accuracies, far away from the
accuracies achieved by models trained on authentic data. This paper targets
this issue by proposing IDiff-Face, a novel approach based on conditional
latent diffusion models for synthetic identity generation with realistic
identity variations for face recognition training. Through extensive
evaluations, our proposed synthetic-based face recognition approach pushed the
limits of state-of-the-art performances, achieving, for example, 98.00%
accuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from the
recent synthetic-based face recognition solutions with 95.40% and bridging the
gap to authentic-based face recognition with 99.82% accuracy.
</p></li>
</ul>

<h3>Title: Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization. (arXiv:2308.05021v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05021">http://arxiv.org/abs/2308.05021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05021]] Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization(http://arxiv.org/abs/2308.05021)</code></li>
<li>Summary: <p>While diffusion models have achieved promising performances in data
synthesis, they might suffer error propagation because of their cascade
structure, where the distributional mismatch spreads and magnifies through the
chain of denoising modules. However, a strict analysis is expected since many
sequential models such as Conditional Random Field (CRF) are free from error
propagation. In this paper, we empirically and theoretically verify that
diffusion models are indeed affected by error propagation and we then propose a
regularization to address this problem. Our theoretical analysis reveals that
the question can be reduced to whether every denoising module of the diffusion
model is fault-tolerant. We derive insightful transition equations, indicating
that the module can't recover from input errors and even propagates additional
errors to the next module. Our analysis directly leads to a consistency
regularization scheme for diffusion models, which explicitly reduces the
distribution gap between forward and backward processes. We further introduce a
bootstrapping algorithm to reduce the computation cost of the regularizer. Our
experimental results on multiple image datasets show that our regularization
effectively handles error propagation and significantly improves the
performance of vanilla diffusion models.
</p></li>
</ul>

<h3>Title: LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation. (arXiv:2308.05095v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05095">http://arxiv.org/abs/2308.05095</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05095]] LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation(http://arxiv.org/abs/2308.05095)</code></li>
<li>Summary: <p>In the text-to-image generation field, recent remarkable progress in Stable
Diffusion makes it possible to generate rich kinds of novel photorealistic
images. However, current models still face misalignment issues (e.g.,
problematic spatial relation understanding and numeration failure) in complex
natural scenes, which impedes the high-faithfulness text-to-image generation.
Although recent efforts have been made to improve controllability by giving
fine-grained guidance (e.g., sketch and scribbles), this issue has not been
fundamentally tackled since users have to provide such guidance information
manually. In this work, we strive to synthesize high-fidelity images that are
semantically aligned with a given textual prompt without any guidance. Toward
this end, we propose a coarse-to-fine paradigm to achieve layout planning and
image generation. Concretely, we first generate the coarse-grained layout
conditioned on a given textual prompt via in-context learning based on Large
Language Models. Afterward, we propose a fine-grained object-interaction
diffusion method to synthesize high-faithfulness images conditioned on the
prompt and the automatically generated layout. Extensive experiments
demonstrate that our proposed method outperforms the state-of-the-art models in
terms of layout and image generation. Our code and settings are available at
\url{https://layoutllm-t2i.github.io}.
</p></li>
</ul>

<h3>Title: Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations. (arXiv:2308.04735v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04735">http://arxiv.org/abs/2308.04735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04735]] Going Deeper with Five-point Stencil Convolutions for Reaction-Diffusion Equations(http://arxiv.org/abs/2308.04735)</code></li>
<li>Summary: <p>Physics-informed neural networks have been widely applied to partial
differential equations with great success because the physics-informed loss
essentially requires no observations or discretization. However, it is
difficult to optimize model parameters, and these parameters must be trained
for each distinct initial condition. To overcome these challenges in
second-order reaction-diffusion type equations, a possible way is to use
five-point stencil convolutional neural networks (FCNNs). FCNNs are trained
using two consecutive snapshots, where the time step corresponds to the step
size of the given snapshots. Thus, the time evolution of FCNNs depends on the
time step, and the time step must satisfy its CFL condition to avoid blow-up
solutions. In this work, we propose deep FCNNs that have large receptive fields
to predict time evolutions with a time step larger than the threshold of the
CFL condition. To evaluate our models, we consider the heat, Fisher's, and
Allen-Cahn equations with diverse initial conditions. We demonstrate that deep
FCNNs retain certain accuracies, in contrast to FDMs that blow up.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Unsupervised Camouflaged Object Segmentation as Domain Adaptation. (arXiv:2308.04528v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04528">http://arxiv.org/abs/2308.04528</a></li>
<li>Code URL: https://github.com/Jun-Pu/UCOS-DA</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04528]] Unsupervised Camouflaged Object Segmentation as Domain Adaptation(http://arxiv.org/abs/2308.04528)</code></li>
<li>Summary: <p>Deep learning for unsupervised image segmentation remains challenging due to
the absence of human labels. The common idea is to train a segmentation head,
with the supervision of pixel-wise pseudo-labels generated based on the
representation of self-supervised backbones. By doing so, the model performance
depends much on the distance between the distributions of target datasets and
the pre-training dataset (e.g., ImageNet). In this work, we investigate a new
task, namely unsupervised camouflaged object segmentation (UCOS), where the
target objects own a common rarely-seen attribute, i.e., camouflage.
Unsurprisingly, we find that the state-of-the-art unsupervised models struggle
in adapting UCOS, due to the domain gap between the properties of generic and
camouflaged objects. To this end, we formulate the UCOS as a source-free
unsupervised domain adaptation task (UCOS-DA), where both source labels and
target labels are absent during the whole model training process. Specifically,
we define a source model consisting of self-supervised vision transformers
pre-trained on ImageNet. On the other hand, the target domain includes a simple
linear layer (i.e., our target model) and unlabeled camouflaged objects. We
then design a pipeline for foreground-background-contrastive self-adversarial
domain adaptation, to achieve robust UCOS. As a result, our baseline model
achieves superior segmentation performance when compared with competing
unsupervised models on the UCOS benchmark, with the training set which's scale
is only one tenth of the supervised COS counterpart.
</p></li>
</ul>

<h3>Title: Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction. (arXiv:2308.04589v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04589">http://arxiv.org/abs/2308.04589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04589]] Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction(http://arxiv.org/abs/2308.04589)</code></li>
<li>Summary: <p>The emerging field of action prediction plays a vital role in various
computer vision applications such as autonomous driving, activity analysis and
human-computer interaction. Despite significant advancements, accurately
predicting future actions remains a challenging problem due to high
dimensionality, complex dynamics and uncertainties inherent in video data.
Traditional supervised approaches require large amounts of labelled data, which
is expensive and time-consuming to obtain. This paper introduces a novel
self-supervised video strategy for enhancing action prediction inspired by DINO
(self-distillation with no labels). The Temporal-DINO approach employs two
models; a 'student' processing past frames; and a 'teacher' processing both
past and future frames, enabling a broader temporal context. During training,
the teacher guides the student to learn future context by only observing past
frames. The strategy is evaluated on ROAD dataset for the action prediction
downstream task using 3D-ResNet, Transformer, and LSTM architectures. The
experimental results showcase significant improvements in prediction
performance across these architectures, with our method achieving an average
enhancement of 9.9% Precision Points (PP), highlighting its effectiveness in
enhancing the backbones' capabilities of capturing long-term dependencies.
Furthermore, our approach demonstrates efficiency regarding the pretraining
dataset size and the number of epochs required. This method overcomes
limitations present in other approaches, including considering various backbone
architectures, addressing multiple prediction horizons, reducing reliance on
hand-crafted augmentations, and streamlining the pretraining process into a
single stage. These findings highlight the potential of our approach in diverse
video-based tasks such as activity recognition, motion planning, and scene
understanding.
</p></li>
</ul>

<h3>Title: GeoAdapt: Self-Supervised Test-Time Adaption in LiDAR Place Recognition Using Geometric Priors. (arXiv:2308.04638v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04638">http://arxiv.org/abs/2308.04638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04638]] GeoAdapt: Self-Supervised Test-Time Adaption in LiDAR Place Recognition Using Geometric Priors(http://arxiv.org/abs/2308.04638)</code></li>
<li>Summary: <p>LiDAR place recognition approaches based on deep learning suffer a
significant degradation in performance when there is a shift between the
distribution of the training and testing datasets, with re-training often
required to achieve top performance. However, obtaining accurate ground truth
on new environments can be prohibitively expensive, especially in complex or
GPS-deprived environments. To address this issue we propose GeoAdapt, which
introduces a novel auxiliary classification head to generate pseudo-labels for
re-training on unseen environments in a self-supervised manner. GeoAdapt uses
geometric consistency as a prior to improve the robustness of our generated
pseudo-labels against domain shift, improving the performance and reliability
of our Test-Time Adaptation approach. Comprehensive experiments show that
GeoAdapt significantly boosts place recognition performance across moderate to
severe domain shifts, and is competitive with fully supervised test-time
adaptation approaches. Our code will be available at
https://github.com/csiro-robotics/GeoAdapt.
</p></li>
</ul>

<h3>Title: Self-supervised Learning of Rotation-invariant 3D Point Set Features using Transformer and its Self-distillation. (arXiv:2308.04725v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04725">http://arxiv.org/abs/2308.04725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04725]] Self-supervised Learning of Rotation-invariant 3D Point Set Features using Transformer and its Self-distillation(http://arxiv.org/abs/2308.04725)</code></li>
<li>Summary: <p>Invariance against rotations of 3D objects is an important property in
analyzing 3D point set data. Conventional 3D point set DNNs having rotation
invariance typically obtain accurate 3D shape features via supervised learning
by using labeled 3D point sets as training samples. However, due to the rapid
increase in 3D point set data and the high cost of labeling, a framework to
learn rotation-invariant 3D shape features from numerous unlabeled 3D point
sets is required. This paper proposes a novel self-supervised learning
framework for acquiring accurate and rotation-invariant 3D point set features
at object-level. Our proposed lightweight DNN architecture decomposes an input
3D point set into multiple global-scale regions, called tokens, that preserve
the spatial layout of partial shapes composing the 3D object. We employ a
self-attention mechanism to refine the tokens and aggregate them into an
expressive rotation-invariant feature per 3D point set. Our DNN is effectively
trained by using pseudo-labels generated by a self-distillation framework. To
facilitate the learning of accurate features, we propose to combine multi-crop
and cut-mix data augmentation techniques to diversify 3D point sets for
training. Through a comprehensive evaluation, we empirically demonstrate that,
(1) existing rotation-invariant DNN architectures designed for supervised
learning do not necessarily learn accurate 3D shape features under a
self-supervised learning scenario, and (2) our proposed algorithm learns
rotation-invariant 3D point set features that are more accurate than those
learned by existing algorithms. Code will be available at
https://github.com/takahikof/RIPT_SDMM
</p></li>
</ul>

<h3>Title: Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization. (arXiv:2308.04767v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04767">http://arxiv.org/abs/2308.04767</a></li>
<li>Code URL: https://github.com/tahy1/avin</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04767]] Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization(http://arxiv.org/abs/2308.04767)</code></li>
<li>Summary: <p>Self-supervised sound source localization is usually challenged by the
modality inconsistency. In recent studies, contrastive learning based
strategies have shown promising to establish such a consistent correspondence
between audio and sound sources in visual scenarios. Unfortunately, the
insufficient attention to the heterogeneity influence in the different modality
features still limits this scheme to be further improved, which also becomes
the motivation of our work. In this study, an Induction Network is proposed to
bridge the modality gap more effectively. By decoupling the gradients of visual
and audio modalities, the discriminative visual representations of sound
sources can be learned with the designed Induction Vector in a bootstrap
manner, which also enables the audio modality to be aligned with the visual
modality consistently. In addition to a visual weighted contrastive loss, an
adaptive threshold selection strategy is introduced to enhance the robustness
of the Induction Network. Substantial experiments conducted on SoundNet-Flickr
and VGG-Sound Source datasets have demonstrated a superior performance compared
to other state-of-the-art works in different challenging scenarios. The code is
available at https://github.com/Tahy1/AVIN
</p></li>
</ul>

<h3>Title: Self-supervised Landmark Learning with Deformation Reconstruction and Cross-subject Consistency Objectives. (arXiv:2308.04987v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04987">http://arxiv.org/abs/2308.04987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04987]] Self-supervised Landmark Learning with Deformation Reconstruction and Cross-subject Consistency Objectives(http://arxiv.org/abs/2308.04987)</code></li>
<li>Summary: <p>A Point Distribution Model (PDM) is the basis of a Statistical Shape Model
(SSM) that relies on a set of landmark points to represent a shape and
characterize the shape variation. In this work, we present a self-supervised
approach to extract landmark points from a given registration model for the
PDMs. Based on the assumption that the landmarks are the points that have the
most influence on registration, existing works learn a point-based registration
model with a small number of points to estimate the landmark points that
influence the deformation the most. However, such approaches assume that the
deformation can be captured by point-based registration and quality landmarks
can be learned solely with the deformation capturing objective. We argue that
data with complicated deformations can not easily be modeled with point-based
registration when only a limited number of points is used to extract
influential landmark points. Further, landmark consistency is not assured in
existing approaches In contrast, we propose to extract landmarks based on a
given registration model, which is tailored for the target data, so we can
obtain more accurate correspondences. Secondly, to establish the anatomical
consistency of the predicted landmarks, we introduce a landmark discovery loss
to explicitly encourage the model to predict the landmarks that are
anatomically consistent across subjects. We conduct experiments on an
osteoarthritis progression prediction task and show our method outperforms
existing image-based and point-based approaches.
</p></li>
</ul>

<h3>Title: A degree of image identification at sub-human scales could be possible with more advanced clusters. (arXiv:2308.05092v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05092">http://arxiv.org/abs/2308.05092</a></li>
<li>Code URL: https://github.com/prateekjannu/imagescale2</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05092]] A degree of image identification at sub-human scales could be possible with more advanced clusters(http://arxiv.org/abs/2308.05092)</code></li>
<li>Summary: <p>The purpose of the research is to determine if currently available
self-supervised learning techniques can accomplish human level comprehension of
visual images using the same degree and amount of sensory input that people
acquire from. Initial research on this topic solely considered data volume
scaling. Here, we scale both the volume of data and the quality of the image.
This scaling experiment is a self-supervised learning method that may be done
without any outside financing. We find that scaling up data volume and picture
resolution at the same time enables human-level item detection performance at
sub-human sizes.We run a scaling experiment with vision transformers trained on
up to 200000 images up to 256 ppi.
</p></li>
</ul>

<h3>Title: SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning. (arXiv:2308.04673v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04673">http://arxiv.org/abs/2308.04673</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04673]] SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning(http://arxiv.org/abs/2308.04673)</code></li>
<li>Summary: <p>Self-supervised learning (SSL) which leverages unlabeled datasets for
pre-training powerful encoders has achieved significant success in recent
years. These encoders are commonly used as feature extractors for various
downstream tasks, requiring substantial data and computing resources for their
training process. With the deployment of pre-trained encoders in commercial
use, protecting the intellectual property of model owners and ensuring the
trustworthiness of the models becomes crucial. Recent research has shown that
encoders are threatened by backdoor attacks, adversarial attacks, etc.
Therefore, a scheme to verify the integrity of pre-trained encoders is needed
to protect users. In this paper, we propose SSL-Auth, the first fragile
watermarking scheme for verifying the integrity of encoders without
compromising model performance. Our method utilizes selected key samples as
watermark information and trains a verification network to reconstruct the
watermark information, thereby verifying the integrity of the encoder. By
comparing the reconstruction results of the key samples, malicious
modifications can be effectively detected, as altered models should not exhibit
similar reconstruction performance as the original models. Extensive
evaluations on various models and diverse datasets demonstrate the
effectiveness and fragility of our proposed SSL-Auth.
</p></li>
</ul>

<h3>Title: Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals. (arXiv:2308.04650v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04650">http://arxiv.org/abs/2308.04650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04650]] Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals(http://arxiv.org/abs/2308.04650)</code></li>
<li>Summary: <p>Heart failure is a debilitating condition that affects millions of people
worldwide and has a significant impact on their quality of life and mortality
rates. An objective assessment of cardiac pressures remains an important method
for the diagnosis and treatment prognostication for patients with heart
failure. Although cardiac catheterization is the gold standard for estimating
central hemodynamic pressures, it is an invasive procedure that carries
inherent risks, making it a potentially dangerous procedure for some patients.
Approaches that leverage non-invasive signals - such as electrocardiogram (ECG)
- have the promise to make the routine estimation of cardiac pressures feasible
in both inpatient and outpatient settings. Prior models trained to estimate
intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP))
in a supervised fashion have shown good discriminatory ability but have been
limited to the labeled dataset from the heart failure cohort. To address this
issue and build a robust representation, we apply deep metric learning (DML)
and propose a novel self-supervised DML with distance-based mining that
improves the performance of a model with limited labels. We use a dataset that
contains over 5.4 million ECGs without concomitant central pressure labels to
pre-train a self-supervised DML model which showed improved classification of
elevated mPCWP compared to self-supervised contrastive baselines. Additionally,
the supervised DML model that is using ECGs with access to 8,172 mPCWP labels
demonstrated significantly better performance on the mPCWP regression task
compared to the supervised baseline. Moreover, our data suggest that DML yields
models that are performant across patient subgroups, even when some patient
subgroups are under-represented in the dataset. Our code is available at
https://github.com/mandiehyewon/ssldml
</p></li>
</ul>

<h3>Title: DOST -- Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels. (arXiv:2308.05101v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05101">http://arxiv.org/abs/2308.05101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05101]] DOST -- Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels(http://arxiv.org/abs/2308.05101)</code></li>
<li>Summary: <p>The enormous demand for annotated data brought forth by deep learning
techniques has been accompanied by the problem of annotation noise. Although
this issue has been widely discussed in machine learning literature, it has
been relatively unexplored in the context of "multi-label classification" (MLC)
tasks which feature more complicated kinds of noise. Additionally, when the
domain in question has certain logical constraints, noisy annotations often
exacerbate their violations, making such a system unacceptable to an expert.
This paper studies the effect of label noise on domain rule violation incidents
in the MLC task, and incorporates domain rules into our learning algorithm to
mitigate the effect of noise. We propose the Domain Obedient Self-supervised
Training (DOST) paradigm which not only makes deep learning models more aligned
to domain rules, but also improves learning performance in key metrics and
minimizes the effect of annotation noise. This novel approach uses domain
guidance to detect offending annotations and deter rule-violating predictions
in a self-supervised manner, thus making it more "data efficient" and domain
compliant. Empirical studies, performed over two large scale multi-label
classification datasets, demonstrate that our method results in improvement
across the board, and often entirely counteracts the effect of noise.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data. (arXiv:2308.04553v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04553">http://arxiv.org/abs/2308.04553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04553]] From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data(http://arxiv.org/abs/2308.04553)</code></li>
<li>Summary: <p>Visual recognition models are prone to learning spurious correlations induced
by an imbalanced training set where certain groups (\eg Females) are
under-represented in certain classes (\eg Programmers). Generative models offer
a promising direction in mitigating this bias by generating synthetic data for
the minority samples and thus balancing the training set. However, prior work
that uses these approaches overlooks that visual recognition models could often
learn to differentiate between real and synthetic images and thus fail to
unlearn the bias in the original dataset. In our work, we propose a novel
two-stage pipeline to mitigate this issue where 1) we pre-train a model on a
balanced synthetic dataset and then 2) fine-tune on the real data. Using this
pipeline, we avoid training on both real and synthetic data, thus avoiding the
bias between real and synthetic data. Moreover, we learn robust features
against the bias in the first step that mitigate the bias in the second step.
Moreover, our pipeline naturally integrates with bias mitigation methods; they
can be simply applied to the fine-tuning step. As our experiments prove, our
pipeline can further improve the performance of bias mitigation methods
obtaining state-of-the-art performance on three large-scale datasets.
</p></li>
</ul>

<h3>Title: GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization. (arXiv:2308.04699v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04699">http://arxiv.org/abs/2308.04699</a></li>
<li>Code URL: https://github.com/ffhibnese/gifd</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04699]] GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization(http://arxiv.org/abs/2308.04699)</code></li>
<li>Summary: <p>Federated Learning (FL) has recently emerged as a promising distributed
machine learning framework to preserve clients' privacy, by allowing multiple
clients to upload the gradients calculated from their local data to a central
server. Recent studies find that the exchanged gradients also take the risk of
privacy leakage, e.g., an attacker can invert the shared gradients and recover
sensitive data against an FL system by leveraging pre-trained generative
adversarial networks (GAN) as prior knowledge. However, performing gradient
inversion attacks in the latent space of the GAN model limits their expression
ability and generalizability. To tackle these challenges, we propose
\textbf{G}radient \textbf{I}nversion over \textbf{F}eature \textbf{D}omains
(GIFD), which disassembles the GAN model and searches the feature domains of
the intermediate layers. Instead of optimizing only over the initial latent
code, we progressively change the optimized layer, from the initial latent
space to intermediate layers closer to the output images. In addition, we
design a regularizer to avoid unreal image generation by adding a small ${l_1}$
ball constraint to the searching range. We also extend GIFD to the
out-of-distribution (OOD) setting, which weakens the assumption that the
training sets of GANs and FL tasks obey the same data distribution. Extensive
experiments demonstrate that our method can achieve pixel-level reconstruction
and is superior to the existing methods. Notably, GIFD also shows great
generalizability under different defense strategy settings and batch sizes.
</p></li>
</ul>

<h3>Title: An End-to-End Framework of Road User Detection, Tracking, and Prediction from Monocular Images. (arXiv:2308.05026v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05026">http://arxiv.org/abs/2308.05026</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05026]] An End-to-End Framework of Road User Detection, Tracking, and Prediction from Monocular Images(http://arxiv.org/abs/2308.05026)</code></li>
<li>Summary: <p>Perception that involves multi-object detection and tracking, and trajectory
prediction are two major tasks of autonomous driving. However, they are
currently mostly studied separately, which results in most trajectory
prediction modules being developed based on ground truth trajectories without
taking into account that trajectories extracted from the detection and tracking
modules in real-world scenarios are noisy. These noisy trajectories can have a
significant impact on the performance of the trajectory predictor and can lead
to serious prediction errors. In this paper, we build an end-to-end framework
for detection, tracking, and trajectory prediction called ODTP (Online
Detection, Tracking and Prediction). It adopts the state-of-the-art online
multi-object tracking model, QD-3DT, for perception and trains the trajectory
predictor, DCENet++, directly based on the detection results without purely
relying on ground truth trajectories. We evaluate the performance of ODTP on
the widely used nuScenes dataset for autonomous driving. Extensive experiments
show that ODPT achieves high performance end-to-end trajectory prediction.
DCENet++, with the enhanced dynamic maps, predicts more accurate trajectories
than its base model. It is also more robust when compared with other generative
and deterministic trajectory prediction models trained on noisy detection
results.
</p></li>
</ul>

<h3>Title: Benchmarking LLM powered Chatbots: Methods and Metrics. (arXiv:2308.04624v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04624">http://arxiv.org/abs/2308.04624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04624]] Benchmarking LLM powered Chatbots: Methods and Metrics(http://arxiv.org/abs/2308.04624)</code></li>
<li>Summary: <p>Autonomous conversational agents, i.e. chatbots, are becoming an increasingly
common mechanism for enterprises to provide support to customers and partners.
In order to rate chatbots, especially ones powered by Generative AI tools like
Large Language Models (LLMs) we need to be able to accurately assess their
performance. This is where chatbot benchmarking becomes important. In this
paper, we propose the use of a novel benchmark that we call the E2E (End to
End) benchmark, and show how the E2E benchmark can be used to evaluate accuracy
and usefulness of the answers provided by chatbots, especially ones powered by
LLMs. We evaluate an example chatbot at different levels of sophistication
based on both our E2E benchmark, as well as other available metrics commonly
used in the state of art, and observe that the proposed benchmark show better
results compared to others. In addition, while some metrics proved to be
unpredictable, the metric associated with the E2E benchmark, which uses cosine
similarity performed well in evaluating chatbots. The performance of our best
models shows that there are several benefits of using the cosine similarity
score as a metric in the E2E benchmark.
</p></li>
</ul>

<h3>Title: VulLibGen: Identifying Vulnerable Third-Party Libraries via Generative Pre-Trained Model. (arXiv:2308.04662v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04662">http://arxiv.org/abs/2308.04662</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04662]] VulLibGen: Identifying Vulnerable Third-Party Libraries via Generative Pre-Trained Model(http://arxiv.org/abs/2308.04662)</code></li>
<li>Summary: <p>To avoid potential risks posed by vulnerabilities in third-party libraries,
security researchers maintain vulnerability databases (e.g., NVD) containing
vulnerability reports, each of which records the description of a vulnerability
and the name list of libraries affected by the vulnerability (a.k.a. vulnerable
libraries). However, recent studies on about 200,000 vulnerability reports in
NVD show that 53.3% of these reports do not include the name list of vulnerable
libraries, and 59.82% of the included name lists of vulnerable libraries are
incomplete or incorrect.
</p>
<p>To address the preceding issue, in this paper, we propose the first
generative approach named VulLibGen to generate the name list of vulnerable
libraries (out of all the existing libraries) for the given vulnerability by
utilizing recent enormous advances in Large Language Models (LLMs), in order to
achieve high accuracy. VulLibGen takes only the description of a vulnerability
as input and achieves high identification accuracy based on LLMs' prior
knowledge of all the existing libraries. VulLibGen also includes the input
augmentation technique to help identify zero-shot vulnerable libraries (those
not occurring during training) and the post-processing technique to help
address VulLibGen's hallucinations. We evaluate VulLibGen using three
state-of-the-art/practice approaches (LightXML, Chronos, and VulLibMiner) that
identify vulnerable libraries on an open-source dataset (VulLib). Our
evaluation results show that VulLibGen can accurately identify vulnerable
libraries with an average F1 score of 0.626 while the state-of-the-art/practice
approaches achieve only 0.561. The post-processing technique helps VulLibGen
achieve an average improvement of F1@1 by 9.3%. The input augmentation
technique helps VulLibGen achieve an average improvement of F1@1 by 39% in
identifying zero-shot libraries.
</p></li>
</ul>

<h3>Title: Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc. (arXiv:2308.04445v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04445">http://arxiv.org/abs/2308.04445</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04445]] Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc(http://arxiv.org/abs/2308.04445)</code></li>
<li>Summary: <p>Generative AI, the most popular current approach to AI, consists of large
language models (LLMs) that are trained to produce outputs that are plausible,
but not necessarily correct. Although their abilities are often uncanny, they
are lacking in aspects of reasoning, leading LLMs to be less than completely
trustworthy. Furthermore, their results tend to be both unpredictable and
uninterpretable.
</p>
<p>We lay out 16 desiderata for future AI, and discuss an alternative approach
to AI which could theoretically address many of the limitations associated with
current approaches: AI educated with curated pieces of explicit knowledge and
rules of thumb, enabling an inference engine to automatically deduce the
logical entailments of all that knowledge. Even long arguments produced this
way can be both trustworthy and interpretable, since the full step-by-step line
of reasoning is always available, and for each step the provenance of the
knowledge used can be documented and audited. There is however a catch: if the
logical language is expressive enough to fully represent the meaning of
anything we can say in English, then the inference engine runs much too slowly.
That's why symbolic AI systems typically settle for some fast but much less
expressive logic, such as knowledge graphs. We describe how one AI system, Cyc,
has developed ways to overcome that tradeoff and is able to reason in higher
order logic in real time.
</p>
<p>We suggest that any trustworthy general AI will need to hybridize the
approaches, the LLM approach and more formal approach, and lay out a path to
realizing that dream.
</p></li>
</ul>

<h3>Title: Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution. (arXiv:2308.04708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04708">http://arxiv.org/abs/2308.04708</a></li>
<li>Code URL: https://github.com/idesan/gpa</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04708]] Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution(http://arxiv.org/abs/2308.04708)</code></li>
<li>Summary: <p>We address the task of probabilistic anomaly attribution in the black-box
regression setting, where the goal is to compute the probability distribution
of the attribution score of each input variable, given an observed anomaly. The
training dataset is assumed to be unavailable. This task differs from the
standard XAI (explainable AI) scenario, since we wish to explain the anomalous
deviation from a black-box prediction rather than the black-box model itself.
</p>
<p>We begin by showing that mainstream model-agnostic explanation methods, such
as the Shapley values, are not suitable for this task because of their
``deviation-agnostic property.'' We then propose a novel framework for
probabilistic anomaly attribution that allows us to not only compute
attribution scores as the predictive mean but also quantify the uncertainty of
those scores. This is done by considering a generative process for
perturbations that counter-factually bring the observed anomalous observation
back to normalcy. We introduce a variational Bayes algorithm for deriving the
distributions of per variable attribution scores. To the best of our knowledge,
this is the first probabilistic anomaly attribution framework that is free from
being deviation-agnostic.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection. (arXiv:2308.04789v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04789">http://arxiv.org/abs/2308.04789</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04789]] Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection(http://arxiv.org/abs/2308.04789)</code></li>
<li>Summary: <p>Anomaly detection has gained considerable attention due to its broad range of
applications, particularly in industrial defect detection. To address the
challenges of data collection, researchers have introduced zero-/few-shot
anomaly detection techniques that require minimal normal images for each
category. However, complex industrial scenarios often involve multiple objects,
presenting a significant challenge. In light of this, we propose a
straightforward yet powerful multi-scale memory comparison framework for
zero-/few-shot anomaly detection. Our approach employs a global memory bank to
capture features across the entire image, while an individual memory bank
focuses on simplified scenes containing a single object. The efficacy of our
method is validated by its remarkable achievement of 4th place in the zero-shot
track and 2nd place in the few-shot track of the Visual Anomaly and Novelty
Detection (VAND) competition.
</p></li>
</ul>

<h3>Title: Gaussian Image Anomaly Detection with Greedy Eigencomponent Selection. (arXiv:2308.04944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04944">http://arxiv.org/abs/2308.04944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04944]] Gaussian Image Anomaly Detection with Greedy Eigencomponent Selection(http://arxiv.org/abs/2308.04944)</code></li>
<li>Summary: <p>Anomaly detection (AD) in images, identifying significant deviations from
normality, is a critical issue in computer vision. This paper introduces a
novel approach to dimensionality reduction for AD using pre-trained
convolutional neural network (CNN) that incorporate EfficientNet models. We
investigate the importance of component selection and propose two types of tree
search approaches, both employing a greedy strategy, for optimal eigencomponent
selection. Our study conducts three main experiments to evaluate the
effectiveness of our approach. The first experiment explores the influence of
test set performance on component choice, the second experiment examines the
performance when we train on one anomaly type and evaluate on all other types,
and the third experiment investigates the impact of using a minimum number of
images for training and selecting them based on anomaly types. Our approach
aims to find the optimal subset of components that deliver the highest
performance score, instead of focusing solely on the proportion of variance
explained by each component and also understand the components behaviour in
different settings. Our results indicate that the proposed method surpasses
both Principal Component Analysis (PCA) and Negated Principal Component
Analysis (NPCA) in terms of detection accuracy, even when using fewer
components. Thus, our approach provides a promising alternative to conventional
dimensionality reduction techniques in AD, and holds potential to enhance the
efficiency and effectiveness of AD systems.
</p></li>
</ul>

<h3>Title: Sparse Binary Transformers for Multivariate Time Series Modeling. (arXiv:2308.04637v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.04637">http://arxiv.org/abs/2308.04637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.04637]] Sparse Binary Transformers for Multivariate Time Series Modeling(http://arxiv.org/abs/2308.04637)</code></li>
<li>Summary: <p>Compressed Neural Networks have the potential to enable deep learning across
new applications and smaller computational environments. However, understanding
the range of learning tasks in which such models can succeed is not well
studied. In this work, we apply sparse and binary-weighted Transformers to
multivariate time series problems, showing that the lightweight models achieve
accuracy comparable to that of dense floating-point Transformers of the same
structure. Our model achieves favorable results across three time series
learning tasks: classification, anomaly detection, and single-step forecasting.
Additionally, to reduce the computational complexity of the attention
mechanism, we apply two modifications, which show little to no decline in model
performance: 1) in the classification task, we apply a fixed mask to the query,
key, and value activations, and 2) for forecasting and anomaly detection, which
rely on predicting outputs at a single point in time, we propose an attention
mask to allow computation only at the current time step. Together, each
compression technique and attention modification substantially reduces the
number of non-zero operations necessary in the Transformer. We measure the
computational savings of our approach over a range of metrics including
parameter count, bit size, and floating point operation (FLOPs) count, showing
up to a 53x reduction in storage size and up to 10.5x reduction in FLOPs.
</p></li>
</ul>

<h3>Title: Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories. (arXiv:2308.05011v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05011">http://arxiv.org/abs/2308.05011</a></li>
<li>Code URL: https://github.com/mperezcarrasco/AnomalyALeRCE</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05011]] Multi-Class Deep SVDD: Anomaly Detection Approach in Astronomy with Distinct Inlier Categories(http://arxiv.org/abs/2308.05011)</code></li>
<li>Summary: <p>With the increasing volume of astronomical data generated by modern survey
telescopes, automated pipelines and machine learning techniques have become
crucial for analyzing and extracting knowledge from these datasets. Anomaly
detection, i.e. the task of identifying irregular or unexpected patterns in the
data, is a complex challenge in astronomy. In this paper, we propose
Multi-Class Deep Support Vector Data Description (MCDSVDD), an extension of the
state-of-the-art anomaly detection algorithm One-Class Deep SVDD, specifically
designed to handle different inlier categories with distinct data
distributions. MCDSVDD uses a neural network to map the data into hyperspheres,
where each hypersphere represents a specific inlier category. The distance of
each sample from the centers of these hyperspheres determines the anomaly
score. We evaluate the effectiveness of MCDSVDD by comparing its performance
with several anomaly detection algorithms on a large dataset of astronomical
light-curves obtained from the Zwicky Transient Facility. Our results
demonstrate the efficacy of MCDSVDD in detecting anomalous sources while
leveraging the presence of different inlier categories. The code and the data
needed to reproduce our results are publicly available at
https://github.com/mperezcarrasco/AnomalyALeRCE.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.05061">http://arxiv.org/abs/2308.05061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.05061]] Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language(http://arxiv.org/abs/2308.05061)</code></li>
<li>Summary: <p>In the growing domain of scientific machine learning, in-context operator
learning has demonstrated notable potential in learning operators from prompted
data during inference stage without weight updates. However, the current
model's overdependence on sensor data, may inadvertently overlook the
invaluable human insight into the operator. To address this, we present a
transformation of in-context operator learning into a multi-modal paradigm. We
propose the use of "captions" to integrate human knowledge about the operator,
expressed through natural language descriptions and equations. We illustrate
how this method not only broadens the flexibility and generality of
physics-informed learning, but also significantly boosts learning performance
and reduces data needs. Furthermore, we introduce a more efficient neural
network architecture for multi-modal in-context operator learning, referred to
as "ICON-LM", based on a language-model-like architecture. We demonstrate the
viability of "ICON-LM" for scientific machine learning tasks, which creates a
new path for the application of language models.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
