<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-27</h1>
<h3>Title: Continuous, Subject-Specific Attribute Control in T2I Models by  Identifying Semantic Directions</h3>
<ul>
<li><strong>Authors: </strong>Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, Bj√∂rn Ommer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17064">https://arxiv.org/abs/2403.17064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17064">https://arxiv.org/pdf/2403.17064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17064]] Continuous, Subject-Specific Attribute Control in T2I Models by  Identifying Semantic Directions(https://arxiv.org/abs/2403.17064)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts. We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model. Project page: https://compvis.github.io/attribute-control. Code is available at https://github.com/CompVis/attribute-control.</li>
</ul>

<h3>Title: Machine Learning on Blockchain Data: A Systematic Mapping Study</h3>
<ul>
<li><strong>Authors: </strong>Georgios Palaiokrassas, Sarah Bouraga, Leandros Tassiulas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17081">https://arxiv.org/abs/2403.17081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17081">https://arxiv.org/pdf/2403.17081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17081]] Machine Learning on Blockchain Data: A Systematic Mapping Study(https://arxiv.org/abs/2403.17081)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Context: Blockchain technology has drawn growing attention in the literature and in practice. Blockchain technology generates considerable amounts of data and has thus been a topic of interest for Machine Learning (ML). Objective: The objective of this paper is to provide a comprehensive review of the state of the art on machine learning applied to blockchain data. This work aims to systematically identify, analyze, and classify the literature on ML applied to blockchain data. This will allow us to discover the fields where more effort should be placed in future research. Method: A systematic mapping study has been conducted to identify the relevant literature. Ultimately, 159 articles were selected and classified according to various dimensions, specifically, the domain use case, the blockchain, the data, and the machine learning models. Results: The majority of the papers (49.7%) fall within the Anomaly use case. Bitcoin (47.2%) was the blockchain that drew the most attention. A dataset consisting of more than 1.000.000 data points was used by 31.4% of the papers. And Classification (46.5%) was the ML task most applied to blockchain data. Conclusion: The results confirm that ML applied to blockchain data is a relevant and a growing topic of interest both in the literature and in practice. Nevertheless, some open challenges and gaps remain, which can lead to future research directions. Specifically, we identify novel machine learning algorithms, the lack of a standardization framework, blockchain scalability issues and cross-chain interactions as areas worth exploring in the future.</li>
</ul>

<h3>Title: The Strong Pull of Prior Knowledge in Large Language Models and Its  Impact on Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17125">https://arxiv.org/abs/2403.17125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17125">https://arxiv.org/pdf/2403.17125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17125]] The Strong Pull of Prior Knowledge in Large Language Models and Its  Impact on Emotion Recognition(https://arxiv.org/abs/2403.17125)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context Learning (ICL) has emerged as a powerful paradigm for performing natural language tasks with Large Language Models (LLM) without updating the models' parameters, in contrast to the traditional gradient-based finetuning. The promise of ICL is that the LLM can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost. The ability of LLMs to perform tasks in this few-shot manner relies on their background knowledge of the task (or task priors). However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition, where the mapping from text to emotions can differ widely due to variability in human annotations. In this work, we design experiments and propose measurements to explicitly quantify the consistency of proxies of LLM priors and their pull on the posteriors. We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that the larger the model, the stronger these effects become. Our results suggest that caution is needed when using ICL with larger LLMs for affect-centered tasks outside their pre-training domain and when interpreting ICL results.</li>
</ul>

<h3>Title: MetaAligner: Conditional Weak-to-Strong Correction for Generalizable  Multi-Objective Alignment of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kailai Yang, Zhiwei Liu, Qianqian Xie, Tianlin Zhang, Nirui Song, Jimin Huang, Ziyan Kuang, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17141">https://arxiv.org/abs/2403.17141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17141">https://arxiv.org/pdf/2403.17141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17141]] MetaAligner: Conditional Weak-to-Strong Correction for Generalizable  Multi-Objective Alignment of Language Models(https://arxiv.org/abs/2403.17141)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves significant and balanced improvements in multi-objective alignments on 11 policy models with up to 63x more parameters, and outperforms previous alignment methods with down to 22.27x less computational resources. The model also accurately aligns with unseen objectives, marking the first step towards generalizable multi-objective preference alignment.</li>
</ul>

<h3>Title: CYGENT: A cybersecurity conversational agent with log summarization  powered by GPT-3</h3>
<ul>
<li><strong>Authors: </strong>Prasasthy Balasubramanian, Justin Seby, Panos Kostakos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17160">https://arxiv.org/abs/2403.17160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17160">https://arxiv.org/pdf/2403.17160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17160]] CYGENT: A cybersecurity conversational agent with log summarization  powered by GPT-3(https://arxiv.org/abs/2403.17160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In response to the escalating cyber-attacks in the modern IT and IoT landscape, we developed CYGENT, a conversational agent framework powered by GPT-3.5 turbo model, designed to aid system administrators in ensuring optimal performance and uninterrupted resource availability. This study focuses on fine-tuning GPT-3 models for cybersecurity tasks, including conversational AI and generative AI tailored specifically for cybersecurity operations. CYGENT assists users by providing cybersecurity information, analyzing and summarizing uploaded log files, detecting specific events, and delivering essential instructions. The conversational agent was developed based on the GPT-3.5 turbo model. We fine-tuned and validated summarizer models (GPT3) using manually generated data points. Using this approach, we achieved a BERTscore of over 97%, indicating GPT-3's enhanced capability in summarizing log files into human-readable formats and providing necessary information to users. Furthermore, we conducted a comparative analysis of GPT-3 models with other Large Language Models (LLMs), including CodeT5-small, CodeT5-base, and CodeT5-base-multi-sum, with the objective of analyzing log analysis techniques. Our analysis consistently demonstrated that Davinci (GPT-3) model outperformed all other LLMs, showcasing higher performance. These findings are crucial for improving human comprehension of logs, particularly in light of the increasing numbers of IoT devices. Additionally, our research suggests that the CodeT5-base-multi-sum model exhibits comparable performance to Davinci to some extent in summarizing logs, indicating its potential as an offline model for this task.</li>
</ul>

<h3>Title: AnimateMe: 4D Facial Expressions via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Stefanos Zafeiriou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17213">https://arxiv.org/abs/2403.17213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17213">https://arxiv.org/pdf/2403.17213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17213]] AnimateMe: 4D Facial Expressions via Diffusion Models(https://arxiv.org/abs/2403.17213)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The field of photorealistic 3D avatar reconstruction and generation has garnered significant attention in recent years; however, animating such avatars remains challenging. Recent advances in diffusion models have notably enhanced the capabilities of generative models in 2D animation. In this work, we directly utilize these models within the 3D domain to achieve controllable and high-fidelity 4D facial animation. By integrating the strengths of diffusion processes and geometric deep learning, we employ Graph Neural Networks (GNNs) as denoising diffusion models in a novel approach, formulating the diffusion process directly on the mesh space and enabling the generation of 3D facial expressions. This facilitates the generation of facial deformations through a mesh-diffusion-based model. Additionally, to ensure temporal coherence in our animations, we propose a consistent noise sampling method. Under a series of both quantitative and qualitative experiments, we showcase that the proposed method outperforms prior work in 4D expression synthesis by generating high-fidelity extreme expressions. Furthermore, we applied our method to textured 4D facial expression generation, implementing a straightforward extension that involves training on a large-scale textured 4D facial expression database.</li>
</ul>

<h3>Title: DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face  Reenactment</h3>
<ul>
<li><strong>Authors: </strong>Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17217">https://arxiv.org/abs/2403.17217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17217">https://arxiv.org/pdf/2403.17217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17217]] DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face  Reenactment(https://arxiv.org/abs/2403.17217)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions. Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific fine-tuning. We compare against state-of-the-art GAN-, StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment performance.</li>
</ul>

<h3>Title: DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yuanze Lin, Ronald Clark, Philip Torr</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17237">https://arxiv.org/abs/2403.17237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17237">https://arxiv.org/pdf/2403.17237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17237]] DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric  Diffusion(https://arxiv.org/abs/2403.17237)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions. While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness. This problem becomes particularly noticeable for methods that work with text input alone. To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views. Initially, a coarse 3D generation undergoes refinement via geometric optimization. Subsequently, we use a ControlNet driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset. Empirical evaluations across diverse textual prompts spanning various object categories demonstrate the efficacy of DreamPolisher in generating consistent and realistic 3D objects, aligning closely with the semantics of the textual instructions.</li>
</ul>

<h3>Title: Diffusion-based Negative Sampling on Graphs for Link Prediction</h3>
<ul>
<li><strong>Authors: </strong>Trung-Kien Nguyen, Yuan Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17259">https://arxiv.org/abs/2403.17259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17259">https://arxiv.org/pdf/2403.17259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17259]] Diffusion-based Negative Sampling on Graphs for Link Prediction(https://arxiv.org/abs/2403.17259)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Link prediction is a fundamental task for graph analysis with important applications on the Web, such as social network analysis and recommendation systems, etc. Modern graph link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal. Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control. Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the graph, missing out on potentially more optimal samples in the latent space. To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable ``hardness'' levels from the latent space. Our method, called Conditional Diffusion-based Multi-level Negative Sampling (DMNS), leverages the Markov chain property of diffusion models to generate negative nodes in multiple levels of variable hardness and reconcile them for effective graph link prediction. We further demonstrate that DMNS follows the sub-linear positivity principle for robust negative sampling. Extensive experiments on several benchmark datasets demonstrate the effectiveness of DMNS.</li>
</ul>

<h3>Title: Decoding Probing: Revealing Internal Linguistic Structures in Neural  Language Models using Minimal Pairs</h3>
<ul>
<li><strong>Authors: </strong>Linyang He, Peili Chen, Ercong Nie, Yuanning Li, Jonathan R. Brennan</a></li>
<li><strong>Subjects: </strong>cs.CL, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17299">https://arxiv.org/abs/2403.17299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17299">https://arxiv.org/pdf/2403.17299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17299]] Decoding Probing: Revealing Internal Linguistic Structures in Neural  Language Models using Minimal Pairs(https://arxiv.org/abs/2403.17299)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Inspired by cognitive neuroscience studies, we introduce a novel `decoding probing' method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the `brain' and its representations as `neural activations', we decode grammaticality labels of minimal pairs from the intermediate layers' representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions.</li>
</ul>

<h3>Title: HILL: Hierarchy-aware Information Lossless Contrastive Learning for  Hierarchical Text Classification</h3>
<ul>
<li><strong>Authors: </strong>He Zhu, Junran Wu, Ruomei Liu, Yue Hou, Ze Yuan, Shangzhe Li, Yicheng Pan, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17307">https://arxiv.org/abs/2403.17307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17307">https://arxiv.org/pdf/2403.17307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17307]] HILL: Hierarchy-aware Information Lossless Contrastive Learning for  Hierarchical Text Classification(https://arxiv.org/abs/2403.17307)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Existing self-supervised methods in natural language processing (NLP), especially hierarchical text classification (HTC), mainly focus on self-supervised contrastive learning, extremely relying on human-designed augmentation rules to generate contrastive samples, which can potentially corrupt or distort the original information. In this paper, we tend to investigate the feasibility of a contrastive learning scheme in which the semantic and syntactic information inherent in the input sample is adequately reserved in the contrastive samples and fused during the learning process. Specifically, we propose an information lossless contrastive learning strategy for HTC, namely \textbf{H}ierarchy-aware \textbf{I}nformation \textbf{L}ossless contrastive \textbf{L}earning (HILL), which consists of a text encoder representing the input document, and a structure encoder directly generating the positive sample. The structure encoder takes the document embedding as input, extracts the essential syntactic information inherent in the label hierarchy with the principle of structural entropy minimization, and injects the syntactic information into the text representation via hierarchical representation learning. Experiments on three common datasets are conducted to verify the superiority of HILL.</li>
</ul>

<h3>Title: Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, Ning Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17336">https://arxiv.org/abs/2403.17336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17336">https://arxiv.org/pdf/2403.17336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17336]] Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of  Large Language Models(https://arxiv.org/abs/2403.17336)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative AI have enabled ubiquitous access to large language models (LLMs). Empowered by their exceptional capabilities to understand and generate human-like text, these models are being increasingly integrated into our society. At the same time, there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers. To overcome such protection, jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited. Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists. To gain a better understanding of the threat landscape of semantically meaningful jailbreak prompts, we systemized existing prompts and measured their jailbreak effectiveness empirically. Further, we conducted a user study involving 92 participants with diverse backgrounds to unveil the process of manually creating jailbreak prompts. We observed that users often succeeded in jailbreak prompts generation regardless of their expertise in LLMs. Building on the insights from the user study, we also developed a system using AI as the assistant to automate the process of jailbreak prompt generation.</li>
</ul>

<h3>Title: Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance</h3>
<ul>
<li><strong>Authors: </strong>Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17377">https://arxiv.org/abs/2403.17377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17377">https://arxiv.org/pdf/2403.17377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17377]] Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance(https://arxiv.org/abs/2403.17377)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms' ability to capture structural information, and guiding the denoising process away from these degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves sample quality in conditional and even unconditional scenarios. Moreover, PAG significantly improves the baseline performance in various downstream tasks where existing guidances such as CG or CFG cannot be fully utilized, including ControlNet with empty prompts and image restoration such as inpainting and deblurring.</li>
</ul>

<h3>Title: InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17422">https://arxiv.org/abs/2403.17422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17422">https://arxiv.org/pdf/2403.17422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17422]] InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse  Diffusion(https://arxiv.org/abs/2403.17422)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy.</li>
</ul>

<h3>Title: Robust and Scalable Model Editing for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17431">https://arxiv.org/abs/2403.17431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17431">https://arxiv.org/pdf/2403.17431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17431]] Robust and Scalable Model Editing for Large Language Models(https://arxiv.org/abs/2403.17431)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. To better evaluate the robustness of model editors, we collect a new dataset, that contains irrelevant questions that are more challenging than the ones in existing datasets. Empirical results show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa). The source code can be found at https://github.com/thunlp/EREN.</li>
</ul>

<h3>Title: LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated  Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17465">https://arxiv.org/abs/2403.17465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17465">https://arxiv.org/pdf/2403.17465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17465]] LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated  Image Detection(https://arxiv.org/abs/2403.17465)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times.</li>
</ul>

<h3>Title: DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on  360¬∞ Images</h3>
<ul>
<li><strong>Authors: </strong>Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai B√¢ce, Zhiming Hu, Andreas Bulling</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17477">https://arxiv.org/abs/2403.17477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17477">https://arxiv.org/pdf/2403.17477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17477]] DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on  360¬∞ Images(https://arxiv.org/abs/2403.17477)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present DiffGaze, a novel method for generating realistic and diverse continuous human gaze sequences on 360{\deg} images based on a conditional score-based denoising diffusion model. Generating human gaze on 360{\deg} images is important for various human-computer interaction and computer graphics applications, e.g. for creating large-scale eye tracking datasets or for realistic animation of virtual humans. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting crucial parts of natural gaze behaviour. Our method uses features extracted from 360{\deg} images as condition and uses two transformers to model the temporal and spatial dependencies of continuous human gaze. We evaluate DiffGaze on two 360{\deg} image benchmarks for gaze sequence generation as well as scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms state-of-the-art methods on all tasks on both benchmarks. We also report a 21-participant user study showing that our method generates gaze sequences that are indistinguishable from real human sequences.</li>
</ul>

<h3>Title: FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart  Electrical Grids</h3>
<ul>
<li><strong>Authors: </strong>Emad Efatinasab, Francesco Marchiori, Alessandro Brighente, Mirco Rampazzo, Mauro Conti</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17494">https://arxiv.org/abs/2403.17494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17494">https://arxiv.org/pdf/2403.17494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17494]] FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart  Electrical Grids(https://arxiv.org/abs/2403.17494)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Predicting and classifying faults in electricity networks is crucial for uninterrupted provision and keeping maintenance costs at a minimum. Thanks to the advancements in the field provided by the smart grid, several data-driven approaches have been proposed in the literature to tackle fault prediction tasks. Implementing these systems brought several improvements, such as optimal energy consumption and quick restoration. Thus, they have become an essential component of the smart grid. However, the robustness and security of these systems against adversarial attacks have not yet been extensively investigated. These attacks can impair the whole grid and cause additional damage to the infrastructure, deceiving fault detection systems and disrupting restoration. In this paper, we present FaultGuard, the first framework for fault type and zone classification resilient to adversarial attacks. To ensure the security of our system, we employ an Anomaly Detection System (ADS) leveraging a novel Generative Adversarial Network training layer to identify attacks. Furthermore, we propose a low-complexity fault prediction model and an online adversarial training technique to enhance robustness. We comprehensively evaluate the framework's performance against various adversarial attacks using the IEEE13-AdvAttack dataset, which constitutes the state-of-the-art for resilient fault prediction benchmarking. Our model outclasses the state-of-the-art even without considering adversaries, with an accuracy of up to 0.958. Furthermore, our ADS shows attack detection capabilities with an accuracy of up to 1.000. Finally, we demonstrate how our novel training layers drastically increase performances across the whole framework, with a mean increase of 154% in ADS accuracy and 118% in model accuracy.</li>
</ul>

<h3>Title: Boosting Few-Shot Learning with Disentangled Self-Supervised Learning  and Meta-Learning for Medical Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Eva Pachetti, Sotirios A. Tsaftaris, Sara Colantonio</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17530">https://arxiv.org/abs/2403.17530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17530">https://arxiv.org/pdf/2403.17530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17530]] Boosting Few-Shot Learning with Disentangled Self-Supervised Learning  and Meta-Learning for Medical Image Classification(https://arxiv.org/abs/2403.17530)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data. We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes. Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks. We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level. This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing. We demonstrate the effectiveness of the proposed approach through a series of experiments exploring several backbones, as well as diverse pre-training and fine-tuning schemes, on two distinct medical tasks, i.e., classification of prostate cancer aggressiveness from MRI data and classification of breast cancer malignity from microscopic images. Results: Our results indicate that the proposed approach consistently yields superior performance w.r.t. ablation experiments, maintaining competitiveness even when a distribution shift between training and evaluation data occurs. Conclusion: Extensive experiments demonstrate the effectiveness and wide applicability of the proposed approach. We hope that this work will add another solution to the arsenal of addressing learning issues in data-scarce imaging domains.</li>
</ul>

<h3>Title: ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent  Classifier and Slot Filler</h3>
<ul>
<li><strong>Authors: </strong>Paramita Mirza, Viju Sudhi, Soumya Ranjan Sahoo, Sinchana Ramakanth Bhat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17536">https://arxiv.org/abs/2403.17536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17536">https://arxiv.org/pdf/2403.17536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17536]] ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent  Classifier and Slot Filler(https://arxiv.org/abs/2403.17536)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks. This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples. We introduce ILLUMINER, an approach framing IC and SF as language generation tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work. A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot filling by 11.1--32.2 percentage points. Additionally, our in-depth ablation study demonstrates that parameter-efficient fine-tuning requires less than 6% of training data to yield comparable performance with traditional full-weight fine-tuning.</li>
</ul>

<h3>Title: Naive Bayes-based Context Extension for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianlin Su, Murtadha Ahmed, Wenbo, Luo Ao, Mingren Zhu, Yunfeng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17552">https://arxiv.org/abs/2403.17552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17552">https://arxiv.org/pdf/2403.17552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17552]] Naive Bayes-based Context Extension for Large Language Models(https://arxiv.org/abs/2403.17552)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM's maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes' theorem to generate the test task. Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods. The NBCE code will be made publicly accessible. The code NBCE is available at: https://github.com/amurtadha/NBCE-master</li>
</ul>

<h3>Title: Fake or JPEG? Revealing Common Biases in Generated Image Detection  Datasets</h3>
<ul>
<li><strong>Authors: </strong>Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17608">https://arxiv.org/abs/2403.17608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17608">https://arxiv.org/pdf/2403.17608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17608]] Fake or JPEG? Revealing Common Biases in Generated Image Detection  Datasets(https://arxiv.org/abs/2403.17608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and Swin-T detectors on the GenImage dataset, achieving state-of-the-art results. We provide the dataset and source codes of this paper on the anonymous website: https://www.unbiased-genimage.org</li>
</ul>

<h3>Title: AniArtAvatar: Animatable 3D Art Avatar from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Shaoxu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17631">https://arxiv.org/abs/2403.17631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17631">https://arxiv.org/pdf/2403.17631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17631]] AniArtAvatar: Animatable 3D Art Avatar from a Single Image(https://arxiv.org/abs/2403.17631)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a novel approach for generating animatable 3D-aware art avatars from a single image, with controllable facial expressions, head poses, and shoulder movements. Unlike previous reenactment methods, our approach utilizes a view-conditioned 2D diffusion model to synthesize multi-view images from a single art portrait with a neutral expression. With the generated colors and normals, we synthesize a static avatar using an SDF-based neural surface. For avatar animation, we extract control points, transfer the motion with these points, and deform the implicit canonical space. Firstly, we render the front image of the avatar, extract the 2D landmarks, and project them to the 3D space using a trained SDF network. We extract 3D driving landmarks using 3DMM and transfer the motion to the avatar landmarks. To animate the avatar pose, we manually set the body height and bound the head and torso of an avatar with two cages. The head and torso can be animated by transforming the two cages. Our approach is a one-shot pipeline that can be applied to various styles. Experiments demonstrate that our method can generate high-quality 3D art avatars with desired control over different motions.</li>
</ul>

<h3>Title: Language Models for Text Classification: Is In-Context Learning Enough?</h3>
<ul>
<li><strong>Authors: </strong>Aleksandra Edwards, Jose Camacho-Collados</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17661">https://arxiv.org/abs/2403.17661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17661">https://arxiv.org/pdf/2403.17661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17661]] Language Models for Text Classification: Is In-Context Learning Enough?(https://arxiv.org/abs/2403.17661)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.</li>
</ul>

<h3>Title: DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with  Space-sensitive Customization and Semantic Preservation</h3>
<ul>
<li><strong>Authors: </strong>Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17664">https://arxiv.org/abs/2403.17664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17664">https://arxiv.org/pdf/2403.17664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17664]] DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with  Space-sensitive Customization and Semantic Preservation(https://arxiv.org/abs/2403.17664)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph. In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference. To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient diffusion-based framework tailored for high-fidelity FAE. For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM). In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC). This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background. We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of diffusion model. Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing.</li>
</ul>

<h3>Title: Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to  Inform GenAI Copyright Disputes</h3>
<ul>
<li><strong>Authors: </strong>Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17691">https://arxiv.org/abs/2403.17691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17691">https://arxiv.org/pdf/2403.17691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17691]] Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to  Inform GenAI Copyright Disputes(https://arxiv.org/abs/2403.17691)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Sc\`enes \`a faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works. We propose a data-driven approach to identify the genericity of works created by GenAI, employing "data-driven bias" to assess the genericity of expressive compositions. This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset. The potential implications of measuring expressive genericity for copyright law are profound. Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals. More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI.</li>
</ul>

<h3>Title: Manifold-Guided Lyapunov Control with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Amartya Mukherjee, Thanin Quartz, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, math.DG, math.OC, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17692">https://arxiv.org/abs/2403.17692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17692">https://arxiv.org/pdf/2403.17692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17692]] Manifold-Guided Lyapunov Control with Diffusion Models(https://arxiv.org/abs/2403.17692)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to generating stabilizing controllers for a large class of dynamical systems using diffusion models. The core objective is to develop stabilizing control functions by identifying the closest asymptotically stable vector field relative to a predetermined manifold and adjusting the control function based on this finding. To achieve this, we employ a diffusion model trained on pairs consisting of asymptotically stable vector fields and their corresponding Lyapunov functions. Our numerical results demonstrate that this pre-trained model can achieve stabilization over previously unseen systems efficiently and rapidly, showcasing the potential of our approach in fast zero-shot control and generalizability.</li>
</ul>

<h3>Title: AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation</h3>
<ul>
<li><strong>Authors: </strong>Huawei Wei, Zejun Yang, Zhisheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17694">https://arxiv.org/abs/2403.17694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17694">https://arxiv.org/pdf/2403.17694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17694]] AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation(https://arxiv.org/abs/2403.17694)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at https://github.com/scutzzj/AniPortrait</li>
</ul>

<h3>Title: The Solution for the CVPR 2023 1st foundation model challenge-Track2</h3>
<ul>
<li><strong>Authors: </strong>Haonan Xu, Yurui Huang, Sishun Pan, Zhihao Guan, Yi Xu, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17702">https://arxiv.org/abs/2403.17702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17702">https://arxiv.org/pdf/2403.17702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17702]] The Solution for the CVPR 2023 1st foundation model challenge-Track2(https://arxiv.org/abs/2403.17702)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a solution for cross-modal transportation retrieval. Due to the cross-domain problem of traffic images, we divide the problem into two sub-tasks of pedestrian retrieval and vehicle retrieval through a simple strategy. In pedestrian retrieval tasks, we use IRRA as the base model and specifically design an Attribute Classification to mine the knowledge implied by attribute labels. More importantly, We use the strategy of Inclusion Relation Matching to make the image-text pairs with inclusion relation have similar representation in the feature space. For the vehicle retrieval task, we use BLIP as the base model. Since aligning the color attributes of vehicles is challenging, we introduce attribute-based object detection techniques to add color patch blocks to vehicle images for color data augmentation. This serves as strong prior information, helping the model perform the image-text alignment. At the same time, we incorporate labeled attributes into the image-text alignment loss to learn fine-grained alignment and prevent similar images and texts from being incorrectly separated. Our approach ranked first in the final B-board test with a score of 70.9.</li>
</ul>

<h3>Title: Masked Autoencoders are PDE Learners</h3>
<ul>
<li><strong>Authors: </strong>Anthony Zhou, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17728">https://arxiv.org/abs/2403.17728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17728">https://arxiv.org/pdf/2403.17728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17728]] Masked Autoencoders are PDE Learners(https://arxiv.org/abs/2403.17728)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.</li>
</ul>

<h3>Title: Leave No Patient Behind: Enhancing Medication Recommendation for Rare  Disease Patients</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhao, Yi Jing, Fuli Feng, Jiancan Wu, Chongming Gao, Xiangnan He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17745">https://arxiv.org/abs/2403.17745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17745">https://arxiv.org/pdf/2403.17745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17745]] Leave No Patient Behind: Enhancing Medication Recommendation for Rare  Disease Patients(https://arxiv.org/abs/2403.17745)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Medication recommendation systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients' clinical information. However, existing approaches often suffer from fairness issues, as recommendations tend to be more accurate for patients with common diseases compared to those with rare conditions. In this paper, we propose a novel model called Robust and Accurate REcommendations for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases. RAREMed employs a transformer encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes. Additionally, it introduces two self-supervised pre-training tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes. Experimental results on two real-world datasets demonstrate that RAREMed provides accurate drug sets for both rare and common disease patients, thereby mitigating unfairness in medication recommendation systems.</li>
</ul>

<h3>Title: Noise2Noise Denoising of CRISM Hyperspectral Data</h3>
<ul>
<li><strong>Authors: </strong>Robert Platt, Rossella Arcucci, C√©dric John</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17757">https://arxiv.org/abs/2403.17757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17757">https://arxiv.org/pdf/2403.17757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17757]] Noise2Noise Denoising of CRISM Hyperspectral Data(https://arxiv.org/abs/2403.17757)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.</li>
</ul>

<h3>Title: GenesisTex: Adapting Image Denoising Diffusion to Texture Space</h3>
<ul>
<li><strong>Authors: </strong>Chenjian Gao, Boyan Jiang, Xinghui Li, Yingpeng Zhang, Qian Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17782">https://arxiv.org/abs/2403.17782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17782">https://arxiv.org/pdf/2403.17782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17782]] GenesisTex: Adapting Image Denoising Diffusion to Texture Space(https://arxiv.org/abs/2403.17782)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present GenesisTex, a novel method for synthesizing textures for 3D geometries from text descriptions. GenesisTex adapts the pretrained image diffusion model to texture space by texture space sampling. Specifically, we maintain a latent texture map for each viewpoint, which is updated with predicted noise on the rendering of the corresponding viewpoint. The sampled latent texture maps are then decoded into a final texture map. During the sampling process, we focus on both global and local consistency across multiple viewpoints: global consistency is achieved through the integration of style consistency mechanisms within the noise prediction network, and low-level consistency is achieved by dynamically aligning latent textures. Finally, we apply reference-based inpainting and img2img on denser views for texture refinement. Our approach overcomes the limitations of slow optimization in distillation-based methods and instability in inpainting-based methods. Experiments on meshes from various sources demonstrate that our method surpasses the baseline methods quantitatively and qualitatively.</li>
</ul>

<h3>Title: Improving Text-to-Image Consistency via Automatic Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Oscar Ma√±as, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17804">https://arxiv.org/abs/2403.17804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17804">https://arxiv.org/pdf/2403.17804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17804]] Improving Text-to-Image Consistency via Automatic Prompt Optimization(https://arxiv.org/abs/2403.17804)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.</li>
</ul>

<h3>Title: Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Eyma√´l, Renaud Vandeghen, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17823">https://arxiv.org/abs/2403.17823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17823">https://arxiv.org/pdf/2403.17823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17823]] Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders(https://arxiv.org/abs/2403.17823)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised pre-training of image encoders is omnipresent in the literature, particularly following the introduction of Masked autoencoders (MAE). Current efforts attempt to learn object-centric representations from motion in videos. In particular, SiamMAE recently introduced a Siamese network, training a shared-weight encoder from two frames of a video with a high asymmetric masking ratio (95%). In this work, we propose CropMAE, an alternative approach to the Siamese pre-training introduced by SiamMAE. Our method specifically differs by exclusively considering pairs of cropped images sourced from the same image but cropped differently, deviating from the conventional pairs of frames extracted from a video. CropMAE therefore alleviates the need for video datasets, while maintaining competitive performances and drastically reducing pre-training time. Furthermore, we demonstrate that CropMAE learns similar object-centric representations without explicit motion, showing that current self-supervised learning methods do not learn objects from motion, but rather thanks to the Siamese architecture. Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling the reconstruction of images using only two visible patches. Our code is available at https://github.com/alexandre-eymael/CropMAE.</li>
</ul>

<h3>Title: DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from  Textual Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17827">https://arxiv.org/abs/2403.17827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17827">https://arxiv.org/pdf/2403.17827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17827]] DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from  Textual Descriptions(https://arxiv.org/abs/2403.17827)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance. Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose. Given a grasping motion from this stage, multiple different actions can be prompted in the interaction phase. For textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions. Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions. Moreover, we demonstrate the practicality of our framework by utilizing a hand pose estimate from an off-the-shelf pose estimator for guidance, and then sampling multiple different actions in the interaction stage.</li>
</ul>

<h3>Title: A foundation model utilizing chest CT volumes and radiology reports for  supervised-level zero-shot detection of abnormalities</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Dogan, Muhammed Furkan Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Mehmet K. Ozdemir, Bjoern Menze</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17834">https://arxiv.org/abs/2403.17834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17834">https://arxiv.org/pdf/2403.17834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17834]] A foundation model utilizing chest CT volumes and radiology reports for  supervised-level zero-shot detection of abnormalities(https://arxiv.org/abs/2403.17834)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>A major challenge in computational research in 3D medical imaging is the lack of comprehensive datasets. Addressing this issue, our study introduces CT-RATE, the first 3D medical imaging dataset that pairs images with textual reports. CT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188 through various reconstructions, from 21,304 unique patients, along with corresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP, a CT-focused contrastive language-image pre-training framework. As a versatile, self-supervised model, CT-CLIP is designed for broad application and does not require task-specific training. Remarkably, CT-CLIP outperforms state-of-the-art, fully supervised methods in multi-abnormality detection across all key metrics, thus eliminating the need for manual annotation. We also demonstrate its utility in case retrieval, whether using imagery or textual queries, thereby advancing knowledge dissemination. The open-source release of CT-RATE and CT-CLIP marks a significant advancement in medical AI, enhancing 3D imaging analysis and fostering innovation in healthcare.</li>
</ul>

<h3>Title: Using Domain Knowledge to Guide Dialog Structure Induction via Neural  Probabilistic Soft Logic</h3>
<ul>
<li><strong>Authors: </strong>Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17853">https://arxiv.org/abs/2403.17853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17853">https://arxiv.org/pdf/2403.17853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17853]] Using Domain Knowledge to Guide Dialog Structure Induction via Neural  Probabilistic Soft Logic(https://arxiv.org/abs/2403.17853)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.</li>
</ul>

<h3>Title: Boosting Diffusion Models with Moving Average Sampling in Frequency  Domain</h3>
<ul>
<li><strong>Authors: </strong>Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, Tao Mei</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17870">https://arxiv.org/abs/2403.17870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17870">https://arxiv.org/pdf/2403.17870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17870]] Boosting Diffusion Models with Moving Average Sampling in Frequency  Domain(https://arxiv.org/abs/2403.17870)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently brought a powerful revolution in image generation. Despite showing impressive generative capabilities, most of these models rely on the current sample to denoise the next one, possibly resulting in denoising instability. In this paper, we reinterpret the iterative denoising process as model optimization and leverage a moving average mechanism to ensemble all the prior samples. Instead of simply applying moving average to the denoised samples at different timesteps, we first map the denoised samples to data space and then perform moving average to avoid distribution shift across timesteps. In view that diffusion models evolve the recovery from low-frequency components to high-frequency details, we further decompose the samples into different frequency components and execute moving average separately on each component. We name the complete approach "Moving Average Sampling in Frequency domain (MASF)". MASF could be seamlessly integrated into mainstream pre-trained diffusion models and sampling schedules. Extensive experiments on both unconditional and conditional diffusion models demonstrate that our MASF leads to superior performances compared to the baselines, with almost negligible additional complexity cost.</li>
</ul>

<h3>Title: Compressed Multi-task embeddings for Data-Efficient Downstream training  and inference in Earth Observation</h3>
<ul>
<li><strong>Authors: </strong>Carlos Gomes, Thomas Brunschwiler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17886">https://arxiv.org/abs/2403.17886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17886">https://arxiv.org/pdf/2403.17886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17886]] Compressed Multi-task embeddings for Data-Efficient Downstream training  and inference in Earth Observation(https://arxiv.org/abs/2403.17886)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient yet performant approach for multi-task EO modelling.</li>
</ul>

<h3>Title: Leveraging Near-Field Lighting for Monocular Depth Estimation from  Endoscopy Videos</h3>
<ul>
<li><strong>Authors: </strong>Akshay Paruchuri, Samuel Ehrenstein, Shuxian Wang, Inbar Fried, Stephen M. Pizer, Marc Niethammer, Roni Sengupta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17915">https://arxiv.org/abs/2403.17915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17915">https://arxiv.org/pdf/2403.17915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17915]] Leveraging Near-Field Lighting for Monocular Depth Estimation from  Endoscopy Videos(https://arxiv.org/abs/2403.17915)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation in endoscopy videos can enable assistive and robotic surgery to obtain better coverage of the organ and detection of various health issues. Despite promising progress on mainstream, natural image depth estimation, techniques perform poorly on endoscopy images due to a lack of strong geometric features and challenging illumination effects. In this paper, we utilize the photometric cues, i.e., the light emitted from an endoscope and reflected by the surface, to improve monocular depth estimation. We first create two novel loss functions with supervised and self-supervised variants that utilize a per-pixel shading representation. We then propose a novel depth refinement network (PPSNet) that leverages the same per-pixel shading representation. Finally, we introduce teacher-student transfer learning to produce better depth maps from both synthetic data with supervision and clinical data with self-supervision. We achieve state-of-the-art results on the C3VD dataset while estimating high-quality depth maps from clinical data. Our code, pre-trained models, and supplementary materials can be found on our project page: https://ppsnet.github.io/</li>
</ul>

<h3>Title: AID: Attention Interpolation of Text-to-Image Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17924">https://arxiv.org/abs/2403.17924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17924">https://arxiv.org/pdf/2403.17924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17924]] AID: Attention Interpolation of Text-to-Image Diffusion(https://arxiv.org/abs/2403.17924)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at https://github.com/QY-H00/attention-interpolation-diffusion.</li>
</ul>

<h3>Title: Track Everything Everywhere Fast and Robustly</h3>
<ul>
<li><strong>Authors: </strong>Yunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, Kostas Daniilidis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17931">https://arxiv.org/abs/2403.17931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17931">https://arxiv.org/pdf/2403.17931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17931]] Track Everything Everywhere Fast and Robustly(https://arxiv.org/abs/2403.17931)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video. The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications. OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence. To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions. While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision foundation models. Our system utilizes monocular depth estimation to represent scene geometry and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process. Our experiments demonstrate a substantial improvement in training speed (more than \textbf{10 times} faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion.</li>
</ul>

<h3>Title: OmniVid: A Generative Framework for Universal Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17935">https://arxiv.org/abs/2403.17935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17935">https://arxiv.org/pdf/2403.17935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17935]] OmniVid: A Generative Framework for Universal Video Understanding(https://arxiv.org/abs/2403.17935)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution. Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats. In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as GPT-3, with extensive training corpora. Inspired by this, we seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens. In this way, a variety of video tasks could be formulated as video-grounded token generation. This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video question answering, and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework. Through comprehensive experiments, we demonstrate such a simple and straightforward idea is quite effective and can achieve state-of-the-art or competitive results on seven video benchmarks, providing a novel perspective for more universal video understanding. Code is available at https://github.com/wangjk666/OmniVid.</li>
</ul>

<h3>Title: ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture  Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17936">https://arxiv.org/abs/2403.17936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17936">https://arxiv.org/pdf/2403.17936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17936]] ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture  Synthesis(https://arxiv.org/abs/2403.17936)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Gestures play a key role in human communication. Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance. Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words. Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis. Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing. Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures. To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another. We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks. We urge the reader to watch our supplementary video at our website.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
