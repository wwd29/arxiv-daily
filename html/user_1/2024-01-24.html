<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-24</h1>
<h3>Title: Memorization in Self-Supervised Learning Improves Downstream  Generalization</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Wang, Muhammad Ahmad Kaleem, Adam Dziedzic, Michael Backes, Nicolas Papernot, Franziska Boenisch</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12233">https://arxiv.org/abs/2401.12233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12233">https://arxiv.org/pdf/2401.12233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12233]] Memorization in Self-Supervised Learning Improves Downstream  Generalization(https://arxiv.org/abs/2401.12233)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regularization techniques that reduce overfitting-still significant fractions of training data points experience high memorization. Through our empirical results, we show that this memorization is essential for encoders to achieve higher generalization performance on different downstream tasks.</li>
</ul>

<h3>Title: Large-scale Reinforcement Learning for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yinan Zhang, Eric Tzeng, Yilun Du, Dmitry Kislyuk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12244">https://arxiv.org/abs/2401.12244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12244">https://arxiv.org/pdf/2401.12244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12244]] Large-scale Reinforcement Learning for Diffusion Models(https://arxiv.org/abs/2401.12244)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models are a class of deep generative models that have demonstrated an impressive capacity for high-quality image generation. However, these models are susceptible to implicit biases that arise from web-scale text-image training pairs and may inaccurately model aspects of images we care about. This can result in suboptimal samples, model bias, and images that do not align with human ethics and preferences. In this paper, we present an effective scalable algorithm to improve diffusion models using Reinforcement Learning (RL) across a diverse set of reward functions, such as human preference, compositionality, and fairness over millions of images. We illustrate how our approach substantially outperforms existing methods for aligning diffusion models with human preferences. We further illustrate how this substantially improves pretrained Stable Diffusion (SD) models, generating samples that are preferred by humans 80.3% of the time over those from the base SD model while simultaneously improving both the composition and diversity of generated samples.</li>
</ul>

<h3>Title: Diffusion Representation for Asymmetric Kernels</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Almeida Gomez, Antonio Silva Neto, Jorge zubelli</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12251">https://arxiv.org/abs/2401.12251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12251">https://arxiv.org/pdf/2401.12251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12251]] Diffusion Representation for Asymmetric Kernels(https://arxiv.org/abs/2401.12251)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We extend the diffusion-map formalism to data sets that are induced by asymmetric kernels. Analytical convergence results of the resulting expansion are proved, and an algorithm is proposed to perform the dimensional reduction. In this work we study data sets in which its geometry structure is induced by an asymmetric kernel. We use a priori coordinate system to represent this geometry and, thus, be able to improve the computational complexity of reducing the dimensionality of data sets. A coordinate system connected to the tensor product of Fourier basis is used to represent the underlying geometric structure obtained by the diffusion-map, thus reducing the dimensionality of the data set and making use of the speedup provided by the two-dimensional Fast Fourier Transform algorithm (2-D FFT). We compare our results with those obtained by other eigenvalue expansions, and verify the efficiency of the algorithms with synthetic data, as well as with real data from applications including climate change studies.</li>
</ul>

<h3>Title: GRATH: Gradual Self-Truthifying for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weixin Chen, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12292">https://arxiv.org/abs/2401.12292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12292">https://arxiv.org/pdf/2401.12292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12292]] GRATH: Gradual Self-Truthifying for Large Language Models(https://arxiv.org/abs/2401.12292)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the truthfulness data and optimizes the model, leading to a gradual improvement in model truthfulness. Empirically, we evaluate GRATH using different 7B-LLMs and compare with LLMs with similar or even larger sizes on benchmark datasets. Our results show that GRATH effectively improves LLMs' truthfulness without compromising other core capabilities. Notably, GRATH achieves state-of-the-art performance on TruthfulQA, with MC1 accuracy as 54.71% and MC2 accuracy as 69.10%, which even surpass those on larger-scale models, such as Llama2-Chat-70B, by 23.62% and 24.18%, respectively.</li>
</ul>

<h3>Title: OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for  Generalized and Robust Retinal Disease Detection</h3>
<ul>
<li><strong>Authors: </strong>Fatema-E Jannat, Sina Gholami, Minhaj Nur Alam, Hamed Tabkhi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12344">https://arxiv.org/abs/2401.12344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12344">https://arxiv.org/pdf/2401.12344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12344]] OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for  Generalized and Robust Retinal Disease Detection(https://arxiv.org/abs/2401.12344)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Despite the revolutionary impact of AI and the development of locally trained algorithms, achieving widespread generalized learning from multi-modal data in medical AI remains a significant challenge. This gap hinders the practical deployment of scalable medical AI solutions. Addressing this challenge, our research contributes a self-supervised robust machine learning framework, OCT-SelfNet, for detecting eye diseases using optical coherence tomography (OCT) images. In this work, various data sets from various institutions are combined enabling a more comprehensive range of representation. Our method addresses the issue using a two-phase training approach that combines self-supervised pretraining and supervised fine-tuning with a mask autoencoder based on the SwinV2 backbone by providing a solution for real-world clinical deployment. Extensive experiments on three datasets with different encoder backbones, low data settings, unseen data settings, and the effect of augmentation show that our method outperforms the baseline model, Resnet-50 by consistently attaining AUC-ROC performance surpassing 77% across all tests, whereas the baseline model exceeds 54%. Moreover, in terms of the AUC-PR metric, our proposed method exceeded 42%, showcasing a substantial increase of at least 10% in performance compared to the baseline, which exceeded only 33%. This contributes to our understanding of our approach's potential and emphasizes its usefulness in clinical settings.</li>
</ul>

<h3>Title: Enhancing In-context Learning via Linear Probe Calibration</h3>
<ul>
<li><strong>Authors: </strong>Momin Abbas, Yi Zhou, Parikshit Ram, Nathalie Baracaldo, Horst Samulowitz, Theodoros Salonidis, Tianyi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12406">https://arxiv.org/abs/2401.12406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12406">https://arxiv.org/pdf/2401.12406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12406]] Enhancing In-context Learning via Linear Probe Calibration(https://arxiv.org/abs/2401.12406)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is a new paradigm for natural language processing that utilizes Generative Pre-trained Transformer (GPT)-like models. This approach uses prompts that include in-context demonstrations to generate the corresponding output for a new query input. However, applying ICL in real cases does not scale with the number of samples, and lacks robustness to different prompt templates and demonstration permutations. In this paper, we first show that GPT-like models using ICL result in unreliable predictions based on a new metric based on Shannon entropy. Then, to solve this problem, we propose a new technique called the Linear Probe Calibration (LinC), a method that calibrates the model's output probabilities, resulting in reliable predictions and improved performance, while requiring only minimal additional samples (as few as five labeled data samples). LinC significantly enhances the ICL test performance of GPT models on various benchmark datasets, with an average improvement of up to 21%, and up to a 50% improvement in some cases, and significantly boosts the performance of PEFT methods, especially in the low resource regime. Moreover, LinC achieves lower expected calibration error, and is highly robust to varying label proportions, prompt templates, and demonstration permutations. Our code is available at \url{https://github.com/mominabbass/LinC}.</li>
</ul>

<h3>Title: AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space</h3>
<ul>
<li><strong>Authors: </strong>Ali Mottaghi, Mohammad Abdullah Jamal, Serena Yeung, Omid Mohareri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12421">https://arxiv.org/abs/2401.12421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12421">https://arxiv.org/pdf/2401.12421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12421]] AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space(https://arxiv.org/abs/2401.12421)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Semi-supervised domain adaptation (SSDA) presents a critical hurdle in computer vision, especially given the frequent scarcity of labeled data in real-world settings. This scarcity often causes foundation models, trained on extensive datasets, to underperform when applied to new domains. AdaEmbed, our newly proposed methodology for SSDA, offers a promising solution to these challenges. Leveraging the potential of unlabeled data, AdaEmbed facilitates the transfer of knowledge from a labeled source domain to an unlabeled target domain by learning a shared embedding space. By generating accurate and uniform pseudo-labels based on the established embedding space, the model overcomes the limitations of conventional SSDA, thus enhancing performance significantly. Our method's effectiveness is validated through extensive experiments on benchmark datasets such as DomainNet, Office-Home, and VisDA-C, where AdaEmbed consistently outperforms all the baselines, setting a new state of the art for SSDA. With its straightforward implementation and high data efficiency, AdaEmbed stands out as a robust and pragmatic solution for real-world scenarios, where labeled data is scarce. To foster further research and application in this area, we are sharing the codebase of our unified framework for semi-supervised domain adaptation.</li>
</ul>

<h3>Title: A Novel Garment Transfer Method Supervised by Distilled Knowledge of  Virtual Try-on Model</h3>
<ul>
<li><strong>Authors: </strong>Naiyu Fang, Lemiao Qiu, Shuyou Zhang, Zili Wang, Kerui Hu, Jianrong Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12433">https://arxiv.org/abs/2401.12433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12433">https://arxiv.org/pdf/2401.12433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12433]] A Novel Garment Transfer Method Supervised by Distilled Knowledge of  Virtual Try-on Model(https://arxiv.org/abs/2401.12433)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>When a shopper chooses garments online, garment transfer technology wears the garment from the model image onto the shopper's image, allowing the shopper to decide whether the garment is suitable for them. As garment transfer leverages wild and cheap person image as garment condition, it has attracted tremendous community attention and holds vast commercial potential. However, since the ground truth of garment transfer is almost unavailable in reality, previous studies have treated garment transfer as either pose transfer or garment-pose disentanglement, and trained garment transfer in self-supervised learning, yet do not cover garment transfer intentions completely. Therefore, the training supervising the garment transfer is a rock-hard issue. Notably, virtual try-on technology has exhibited superior performance using self-supervised learning. We supervise the garment transfer training via knowledge distillation from virtual try-on. Specifically, we first train the transfer parsing reasoning model at multi-phases to provide shape guidance for downstream tasks. The transfer parsing reasoning model learns the response and feature knowledge from the try-on parsing reasoning model and absorbs the hard knowledge from the ground truth. By leveraging the warping knowledge from virtual try-on, we estimate a progressive flow to precisely warp the garment by learning the shape and content correspondence. To enhance transfer realism, we propose a well-designed arm regrowth task to infer exposed skin pixel content. Experiments demonstrate that our method has state-of-the-art performance in transferring garments between person compared with other virtual try-on and garment transfer methods.</li>
</ul>

<h3>Title: Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural  Calibration</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhang, Siyu Ren, Junhui Hou, Jinjian Wu, Guangming Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12452">https://arxiv.org/abs/2401.12452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12452">https://arxiv.org/pdf/2401.12452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12452]] Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural  Calibration(https://arxiv.org/abs/2401.12452)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel self-supervised learning framework for enhancing 3D perception in autonomous driving scenes. Specifically, our approach, named NCLR, focuses on 2D-3D neural calibration, a novel pretext task that estimates the rigid transformation aligning camera and LiDAR coordinate systems. First, we propose the learnable transformation alignment to bridge the domain gap between image and point cloud data, converting features into a unified representation space for effective comparison and matching. Second, we identify the overlapping area between the image and point cloud with the fused features. Third, we establish dense 2D-3D correspondences to estimate the rigid transformation. The framework not only learns fine-grained matching from points to pixels but also achieves alignment of the image and point cloud at a holistic level, understanding their relative pose. We demonstrate NCLR's efficacy by applying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D semantic segmentation, object detection, and panoptic segmentation. Comprehensive experiments on various datasets illustrate the superiority of NCLR over existing self-supervised methods. The results confirm that joint learning from different modalities significantly enhances the network's understanding abilities and effectiveness of learned representation. Code will be available at \url{https://github.com/Eaphan/NCLR}.</li>
</ul>

<h3>Title: Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT  and SimCLR</h3>
<ul>
<li><strong>Authors: </strong>Robert Turnbull, Evelyn Mannix</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12513">https://arxiv.org/abs/2401.12513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12513">https://arxiv.org/pdf/2401.12513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12513]] Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT  and SimCLR(https://arxiv.org/abs/2401.12513)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The capacity to isolate and recognize individual characters from facsimile images of papyrus manuscripts yields rich opportunities for digital analysis. For this reason the `ICDAR 2023 Competition on Detection and Recognition of Greek Letters on Papyri' was held as part of the 17th International Conference on Document Analysis and Recognition. This paper discusses our submission to the competition. We used an ensemble of YOLOv8 models to detect and classify individual characters and employed two different approaches for refining the character predictions, including a transformer based DeiT approach and a ResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a self-supervised learning method. Our submission won the recognition challenge with a mAP of 42.2%, and was runner-up in the detection challenge with a mean average precision (mAP) of 51.4%. At the more relaxed intersection over union threshold of 0.5, we achieved the highest mean average precision and mean average recall results for both detection and classification. We ran our prediction pipeline on more than 4,500 images from the Oxyrhynchus Papyri to illustrate the utility of our approach, and we release the results publicly in multiple formats.</li>
</ul>

<h3>Title: DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing  High-Quality Implicit Neural Representations</h3>
<ul>
<li><strong>Authors: </strong>Dogyun Park, Sihyeon Kim, Sojin Lee, Hyunwoo J. Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12517">https://arxiv.org/abs/2401.12517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12517">https://arxiv.org/pdf/2401.12517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12517]] DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing  High-Quality Implicit Neural Representations(https://arxiv.org/abs/2401.12517)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and the continuous signal functions in the shared latent space. Additionally, we introduce a novel conditioning mechanism for evaluating INRs with the hierarchically decomposed PEs to further enhance expressive power. Extensive experiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance Fields, and videos, with seven benchmark datasets, demonstrate the versatility of DDMI and its superior performance compared to the existing INR generative models.</li>
</ul>

<h3>Title: Self-Supervised Vision Transformers Are Efficient Segmentation Learners  for Imperfect Labels</h3>
<ul>
<li><strong>Authors: </strong>Seungho Lee, Seoungyoon Kang, Hyunjung Shim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12535">https://arxiv.org/abs/2401.12535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12535">https://arxiv.org/pdf/2401.12535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12535]] Self-Supervised Vision Transformers Are Efficient Segmentation Learners  for Imperfect Labels(https://arxiv.org/abs/2401.12535)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This study demonstrates a cost-effective approach to semantic segmentation using self-supervised vision transformers (SSVT). By freezing the SSVT backbone and training a lightweight segmentation head, our approach effectively utilizes imperfect labels, thereby improving robustness to label imperfections. Empirical experiments show significant performance improvements over existing methods for various annotation types, including scribble, point-level, and image-level labels. The research highlights the effectiveness of self-supervised vision transformers in dealing with imperfect labels, providing a practical and efficient solution for semantic segmentation while reducing annotation costs. Through extensive experiments, we confirm that our method outperforms baseline models for all types of imperfect labels. Especially under the zero-shot vision-language-model-based label, our model exhibits 11.5\%p performance gain compared to the baseline.</li>
</ul>

<h3>Title: Multi-Party Private Set Intersection: A Circuit-Based Protocol with  Jaccard Similarity for Secure and Efficient Anomaly Detection in Network  Traffic</h3>
<ul>
<li><strong>Authors: </strong>Jiuheng Su, Zhili Chen, Xiaomin Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12542">https://arxiv.org/abs/2401.12542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12542">https://arxiv.org/pdf/2401.12542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12542]] Multi-Party Private Set Intersection: A Circuit-Based Protocol with  Jaccard Similarity for Secure and Efficient Anomaly Detection in Network  Traffic(https://arxiv.org/abs/2401.12542)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We present a new circuit-based protocol for multi-party private set intersection (PSI) that allows m parties to compute the intersection of their datasets without revealing any additional information about the items outside the intersection. Building upon the two-party Sort-Compare-Shuffle (SCS) protocol, we seamlessly extend it to a multi-party setting. Demonstrating its practicality through implementation, our protocol exhibits acceptable performance. Specifically, with 7 parties, each possessing a set size of 2^{12}, our protocol completes in just 19 seconds. Moreover, circuit-based protocols like ours have an advantage over using custom protocols to perform more complex computation. We substantiate this advantage by incorporating a module for calculating the Jaccard similarity metric of the private sets which can be used in the application domain of network traffic analysis for anomaly detection. This extension showcases the versatility of our protocol beyond set intersection computations, demonstrating its efficacy in preserving privacy while efficiently identifying abnormal patterns in network flow.</li>
</ul>

<h3>Title: Graph Contrastive Invariant Learning from the Causal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yanhu Mo, Xiao Wang, Shaohua Fan, Chuan Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12564">https://arxiv.org/abs/2401.12564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12564">https://arxiv.org/pdf/2401.12564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12564]] Graph Contrastive Invariant Learning from the Causal Perspective(https://arxiv.org/abs/2401.12564)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph contrastive learning (GCL), learning the node representation by contrasting two augmented graphs in a self-supervised way, has attracted considerable attention. GCL is usually believed to learn the invariant representation. However, does this understanding always hold in practice? In this paper, we first study GCL from the perspective of causality. By analyzing GCL with the structural causal model (SCM), we discover that traditional GCL may not well learn the invariant representations due to the non-causal information contained in the graph. How can we fix it and encourage the current GCL to learn better invariant representations? The SCM offers two requirements and motives us to propose a novel GCL method. Particularly, we introduce the spectral graph augmentation to simulate the intervention upon non-causal factors. Then we design the invariance objective and independence objective to better capture the causal factors. Specifically, (i) the invariance objective encourages the encoder to capture the invariant information contained in causal variables, and (ii) the independence objective aims to reduce the influence of confounders on the causal variables. Experimental results demonstrate the effectiveness of our approach on node classification tasks.</li>
</ul>

<h3>Title: ToDA: Target-oriented Diffusion Attacker against Recommendation System</h3>
<ul>
<li><strong>Authors: </strong>Xiaohao Liu, Zhulin Tao, Ting Jiang, He Chang, Yunshan Ma, Xianglin Huang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12578">https://arxiv.org/abs/2401.12578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12578">https://arxiv.org/pdf/2401.12578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12578]] ToDA: Target-oriented Diffusion Attacker against Recommendation System(https://arxiv.org/abs/2401.12578)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recommendation systems (RS) have become indispensable tools for web services to address information overload, thus enhancing user experiences and bolstering platforms' revenues. However, with their increasing ubiquity, security concerns have also emerged. As the public accessibility of RS, they are susceptible to specific malicious attacks where adversaries can manipulate user profiles, leading to biased recommendations. Recent research often integrates additional modules using generative models to craft these deceptive user profiles, ensuring them are imperceptible while causing the intended harm. Albeit their efficacy, these models face challenges of unstable training and the exploration-exploitation dilemma, which can lead to suboptimal results. In this paper, we pioneer to investigate the potential of diffusion models (DMs), for shilling attacks. Specifically, we propose a novel Target-oriented Diffusion Attack model (ToDA). It incorporates a pre-trained autoencoder that transforms user profiles into a high dimensional space, paired with a Latent Diffusion Attacker (LDA)-the core component of ToDA. LDA introduces noise into the profiles within this latent space, adeptly steering the approximation towards targeted items through cross-attention mechanisms. The global horizon, implemented by a bipartite graph, is involved in LDA and derived from the encoded user profile feature. This makes LDA possible to extend the generation outwards the on-processing user feature itself, and bridges the gap between diffused user features and target item features. Extensive experiments compared to several SOTA baselines demonstrate ToDA's effectiveness. Specific studies exploit the elaborative design of ToDA and underscore the potency of advanced generative models in such contexts.</li>
</ul>

<h3>Title: UniHDA: Towards Universal Hybrid Domain Adaptation of Image Generators</h3>
<ul>
<li><strong>Authors: </strong>Hengjia Li, Yang Liu, Yuqi Lin, Zhanwei Zhang, Yibo Zhao, weihang Pan, Tu Zheng, Zheng Yang, Yuchun Jiang, Boxi Wu, Deng Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12596">https://arxiv.org/abs/2401.12596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12596">https://arxiv.org/pdf/2401.12596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12596]] UniHDA: Towards Universal Hybrid Domain Adaptation of Image Generators(https://arxiv.org/abs/2401.12596)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative domain adaptation has achieved remarkable progress, enabling us to adapt a pre-trained generator to a new target domain. However, existing methods simply adapt the generator to a single target domain and are limited to a single modality, either text-driven or image-driven. Moreover, they are prone to overfitting domain-specific attributes, which inevitably compromises cross-domain consistency. In this paper, we propose UniHDA, a unified and versatile framework for generative hybrid domain adaptation with multi-modal references from multiple domains. We use CLIP encoder to project multi-modal references into a unified embedding space and then linear interpolate the direction vectors from multiple target domains to achieve hybrid domain adaptation. To ensure the cross-domain consistency, we propose a novel cross-domain spatial structure (CSS) loss that maintains detailed spatial structure information between source and target generator. Experiments show that the adapted generator can synthesise realistic images with various attribute compositions. Additionally, our framework is versatile to multiple generators, \eg, StyleGAN2 and Diffusion Models.</li>
</ul>

<h3>Title: Prompt Smells: An Omen for Undesirable Generative AI Outputs</h3>
<ul>
<li><strong>Authors: </strong>Krishna Ronanki, Beatriz Cabrero-Daniel, Christian Berger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12611">https://arxiv.org/abs/2401.12611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12611">https://arxiv.org/pdf/2401.12611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12611]] Prompt Smells: An Omen for Undesirable Generative AI Outputs(https://arxiv.org/abs/2401.12611)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent Generative Artificial Intelligence (GenAI) trends focus on various applications, including creating stories, illustrations, poems, articles, computer code, music compositions, and videos. Extrinsic hallucinations are a critical limitation of such GenAI, which can lead to significant challenges in achieving and maintaining the trustworthiness of GenAI. In this paper, we propose two new concepts that we believe will aid the research community in addressing limitations associated with the application of GenAI models. First, we propose a definition for the "desirability" of GenAI outputs and three factors which are observed to influence it. Second, drawing inspiration from Martin Fowler's code smells, we propose the concept of "prompt smells" and the adverse effects they are observed to have on the desirability of GenAI outputs. We expect our work will contribute to the ongoing conversation about the desirability of GenAI outputs and help advance the field in a meaningful way.</li>
</ul>

<h3>Title: ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, Tao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12665">https://arxiv.org/abs/2401.12665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12665">https://arxiv.org/pdf/2401.12665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12665]] ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation(https://arxiv.org/abs/2401.12665)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.</li>
</ul>

<h3>Title: Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical  Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhishuai Li, Yunhao Nie, Ziyue Li, Lei Bai, Yisheng Lv, Rui Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12681">https://arxiv.org/abs/2401.12681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12681">https://arxiv.org/pdf/2401.12681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12681]] Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical  Learning(https://arxiv.org/abs/2401.12681)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Kriging aims at estimating the attributes of unsampled geo-locations from observations in the spatial vicinity or physical connections, which helps mitigate skewed monitoring caused by under-deployed sensors. Existing works assume that neighbors' information offers the basis for estimating the attributes of the unobserved target while ignoring non-neighbors. However, non-neighbors could also offer constructive information, and neighbors could also be misleading. To this end, we propose ``Contrastive-Prototypical'' self-supervised learning for Kriging (KCP) to refine valuable information from neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we conduct the Kriging task from a new perspective of representation: we aim to first learn robust and general representations and then recover attributes from representations. A neighboring contrastive module is designed that coarsely learns the representations by narrowing the representation distance between the target and its neighbors while pushing away the non-neighbors. In parallel, a prototypical module is introduced to identify similar representations via exchanged prediction, thus refining the misleading neighbors and recycling the useful non-neighbors from the neighboring contrast component. As a result, not all the neighbors and some of the non-neighbors will be used to infer the target. To encourage the two modules above to learn general and robust representations, we design an adaptive augmentation module that incorporates data-driven attribute augmentation and centrality-based topology augmentation over the spatiotemporal Kriging graph data. Extensive experiments on real-world datasets demonstrate the superior performance of KCP compared to its peers with 6% improvements and exceptional transferability and robustness. The code is available at https://github.com/bonaldli/KCP</li>
</ul>

<h3>Title: A Comprehensive View of the Biases of Toxicity and Sentiment Analysis  Methods Towards Utterances with African American English Expressions</h3>
<ul>
<li><strong>Authors: </strong>Guilherme H. Resende, Luiz F. Nery, Fabr√≠cio Benevenuto, Savvas Zannettou, Flavio Figueiredo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12720">https://arxiv.org/abs/2401.12720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12720">https://arxiv.org/pdf/2401.12720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12720]] A Comprehensive View of the Biases of Toxicity and Sentiment Analysis  Methods Towards Utterances with African American English Expressions(https://arxiv.org/abs/2401.12720)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Language is a dynamic aspect of our culture that changes when expressed in different technologies/communities. Online social networks have enabled the diffusion and evolution of different dialects, including African American English (AAE). However, this increased usage is not without barriers. One particular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity (Google's Perspective and the open-source Detoxify) methods present biases towards utterances with AAE expressions. Consider Google's Perspective to understand bias. Here, an utterance such as ``All n*ggers deserve to die respectfully. The police murder us.'' it reaches a higher toxicity than ``African-Americans deserve to die respectfully. The police murder us.''. This score difference likely arises because the tool cannot understand the re-appropriation of the term ``n*gger''. One explanation for this bias is that AI models are trained on limited datasets, and using such a term in training data is more likely to appear in a toxic utterance. While this may be plausible, the tool will make mistakes regardless. Here, we study bias on two Web-based (YouTube and Twitter) datasets and two spoken English datasets. Our analysis shows how most models present biases towards AAE in most settings. We isolate the impact of AAE expression usage via linguistic control features from the Linguistic Inquiry and Word Count (LIWC) software, grammatical control features extracted via Part-of-Speech (PoS) tagging from Natural Language Processing (NLP) models, and the semantic of utterances by comparing sentence embeddings from recent language models. We present consistent results on how a heavy usage of AAE expressions may cause the speaker to be considered substantially more toxic, even when speaking about nearly the same subject. Our study complements similar analyses focusing on small datasets and/or one method only.</li>
</ul>

<h3>Title: DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for  Alleviating Over-squashing</h3>
<ul>
<li><strong>Authors: </strong>Li Sun, Zhenhao Huang, Hua Wu, Junda Ye, Hao Peng, Zhengtao Yu, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12780">https://arxiv.org/abs/2401.12780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12780">https://arxiv.org/pdf/2401.12780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12780]] DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for  Alleviating Over-squashing(https://arxiv.org/abs/2401.12780)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have shown great power for learning and mining on graphs, and Graph Structure Learning (GSL) plays an important role in boosting GNNs with a refined graph. In the literature, most GSL solutions either primarily focus on structure refinement with task-specific supervision (i.e., node classification), or overlook the inherent weakness of GNNs themselves (e.g., over-squashing), resulting in suboptimal performance despite sophisticated designs. In light of these limitations, we propose to study self-supervised graph structure-feature co-refinement for effectively alleviating the issue of over-squashing in typical GNNs. In this paper, we take a fundamentally different perspective of the Ricci curvature in Riemannian geometry, in which we encounter the challenges of modeling, utilizing and computing Ricci curvature. To tackle these challenges, we present a self-supervised Riemannian model, DeepRicci. Specifically, we introduce a latent Riemannian space of heterogeneous curvatures to model various Ricci curvatures, and propose a gyrovector feature mapping to utilize Ricci curvature for typical GNNs. Thereafter, we refine node features by geometric contrastive learning among different geometric views, and simultaneously refine graph structure by backward Ricci flow based on a novel formulation of differentiable Ricci curvature. Finally, extensive experiments on public datasets show the superiority of DeepRicci, and the connection between backward Ricci flow and over-squashing. Codes of our work are given in https://github.com/RiemanGraph/.</li>
</ul>

<h3>Title: DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained  Self-supervised Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Sonal Kumar, Arijit Sur, Rashmi Dutta Baruah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12820">https://arxiv.org/abs/2401.12820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12820">https://arxiv.org/pdf/2401.12820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12820]] DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained  Self-supervised Vision Transformer(https://arxiv.org/abs/2401.12820)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Successive proposals of several self-supervised training schemes continue to emerge, taking one step closer to developing a universal foundation model. In this process, the unsupervised downstream tasks are recognized as one of the evaluation methods to validate the quality of visual features learned with a self-supervised training scheme. However, unsupervised dense semantic segmentation has not been explored as a downstream task, which can utilize and evaluate the quality of semantic information introduced in patch-level feature representations during self-supervised training of a vision transformer. Therefore, this paper proposes a novel data-driven approach for unsupervised semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates semantically consistent and dense pseudo annotate segmentation masks for the unlabeled image dataset without using any visual-prior or synchronized data. We compare these pseudo-annotated segmentation masks with ground truth masks for evaluating recent self-supervised training schemes to learn shared semantic properties at the patch level and discriminative semantic properties at the segment level. Finally, we evaluate existing state-of-the-art self-supervised training schemes with our proposed downstream task, i.e., DatUS^2. Also, the best version of DatUS^2 outperforms the existing state-of-the-art method for the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47% Pixel accuracy on the SUIM dataset. It also achieves a competitive level of accuracy for a large-scale and complex dataset, i.e., the COCO dataset.</li>
</ul>

<h3>Title: FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units</h3>
<ul>
<li><strong>Authors: </strong>Shaoheng Fang, Rui Ye, Wenhao Wang, Zuhong Liu, Yuxiao Wang, Yafei Wang, Siheng Chen, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12862">https://arxiv.org/abs/2401.12862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12862">https://arxiv.org/pdf/2401.12862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12862]] FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units(https://arxiv.org/abs/2401.12862)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Roadside unit (RSU) can significantly improve the safety and robustness of autonomous vehicles through Vehicle-to-Everything (V2X) communication. Currently, the usage of a single RSU mainly focuses on real-time inference and V2X collaboration, while neglecting the potential value of the high-quality data collected by RSU sensors. Integrating the vast amounts of data from numerous RSUs can provide a rich source of data for model training. However, the absence of ground truth annotations and the difficulty of transmitting enormous volumes of data are two inevitable barriers to fully exploiting this hidden value. In this paper, we introduce FedRSU, an innovative federated learning framework for self-supervised scene flow estimation. In FedRSU, we present a recurrent self-supervision training paradigm, where for each RSU, the scene flow prediction of points at every timestamp can be supervised by its subsequent future multi-modality observation. Another key component of FedRSU is federated learning, where multiple devices collaboratively train an ML model while keeping the training data local and private. With the power of the recurrent self-supervised learning paradigm, FL is able to leverage innumerable underutilized data from RSU. To verify the FedRSU framework, we construct a large-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU clients, covering various scenarios, modalities, and sensor settings. Based on RSU-SF, we show that FedRSU can greatly improve model performance in ITS and provide a comprehensive benchmark under diverse FL scenarios. To the best of our knowledge, we provide the first real-world LiDAR-camera multi-modal dataset and benchmark for the FL community.</li>
</ul>

<h3>Title: Lumiere: A Space-Time Diffusion Model for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, Inbar Mosseri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12945">https://arxiv.org/abs/2401.12945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12945">https://arxiv.org/pdf/2401.12945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12945]] Lumiere: A Space-Time Diffusion Model for Video Generation(https://arxiv.org/abs/2401.12945)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Lumiere -- a text-to-video diffusion model designed for synthesizing videos that portray realistic, diverse and coherent motion -- a pivotal challenge in video synthesis. To this end, we introduce a Space-Time U-Net architecture that generates the entire temporal duration of the video at once, through a single pass in the model. This is in contrast to existing video models which synthesize distant keyframes followed by temporal super-resolution -- an approach that inherently makes global temporal consistency difficult to achieve. By deploying both spatial and (importantly) temporal down- and up-sampling and leveraging a pre-trained text-to-image diffusion model, our model learns to directly generate a full-frame-rate, low-resolution video by processing it in multiple space-time scales. We demonstrate state-of-the-art text-to-video generation results, and show that our design easily facilitates a wide range of content creation tasks and video editing applications, including image-to-video, video inpainting, and stylized generation.</li>
</ul>

<h3>Title: Transformer-Based Models Are Not Yet Perfect At Learning to Emulate  Structural Recursion</h3>
<ul>
<li><strong>Authors: </strong>Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, Talia Ringer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.FL, cs.LO, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12947">https://arxiv.org/abs/2401.12947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12947">https://arxiv.org/pdf/2401.12947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12947]] Transformer-Based Models Are Not Yet Perfect At Learning to Emulate  Structural Recursion(https://arxiv.org/abs/2401.12947)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture. With our framework as a powerful conceptual tool, we identify different issues under various set-ups. The models trained to emulate recursive computations cannot fully capture the recursion yet instead fit short-cut algorithms and thus cannot solve certain edge cases that are under-represented in the training distribution. In addition, it is difficult for state-of-the-art large language models (LLMs) to mine recursive rules from in-context demonstrations. Meanwhile, these LLMs fail in interesting ways when emulating reduction (step-wise computation) of the recursive function.</li>
</ul>

<h3>Title: Raidar: geneRative AI Detection viA Rewriting</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Mao, Carl Vondrick, Hao Wang, Junfeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12970">https://arxiv.org/abs/2401.12970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12970">https://arxiv.org/pdf/2401.12970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12970]] Raidar: geneRative AI Detection viA Rewriting(https://arxiv.org/abs/2401.12970)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar. Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.</li>
</ul>

<h3>Title: In-Context Language Learning: Arhitectures and Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Ekin Aky√ºrek, Bailin Wang, Yoon Kim, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12973">https://arxiv.org/abs/2401.12973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12973">https://arxiv.org/pdf/2401.12973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12973]] In-Context Language Learning: Arhitectures and Algorithms(https://arxiv.org/abs/2401.12973)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the "real" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including several RNNs, Transformers, and state-space model variants) on regular ICLL tasks, aiming to answer three questions: (1) Which model classes are empirically capable of ICLL? (2) What algorithmic solutions do successful models implement to perform ICLL? (3) What architectural changes can improve ICLL in less performant models? We first show that Transformers significantly outperform neural sequence models with recurrent or convolutional representations on ICLL tasks. Next, we provide evidence that their ability to do so relies on specialized "n-gram heads" (higher-order variants of induction heads) that compute input-conditional next-token distributions. Finally, we show that hard-wiring these heads into recurrent and convolutional models improves performance not just on ICLL, but natural language modeling -- improving the perplexity of 340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.</li>
</ul>

<h3>Title: Zero-Shot Learning for the Primitives of 3D Affordance in General  Objects</h3>
<ul>
<li><strong>Authors: </strong>Hyeonwoo Kim, Sookwan Han, Patrick Kwon, Hanbyul Joo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12978">https://arxiv.org/abs/2401.12978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12978">https://arxiv.org/pdf/2401.12978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12978]] Zero-Shot Learning for the Primitives of 3D Affordance in General  Objects(https://arxiv.org/abs/2401.12978)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>One of the major challenges in AI is teaching machines to precisely respond and utilize environmental functionalities, thereby achieving the affordance awareness that humans possess. Despite its importance, the field has been lagging in terms of learning, especially in 3D, as annotating affordance accompanies a laborious process due to the numerous variations of human-object interaction. The low availability of affordance data limits the learning in terms of generalization for object categories, and also simplifies the representation of affordance, capturing only a fraction of the affordance. To overcome these challenges, we propose a novel, self-supervised method to generate the 3D affordance examples given only a 3D object, without any manual annotations. The method starts by capturing the 3D object into images and creating 2D affordance images by inserting humans into the image via inpainting diffusion models, where we present the Adaptive Mask algorithm to enable human insertion without altering the original details of the object. The method consequently lifts inserted humans back to 3D to create 3D human-object pairs, where the depth ambiguity is resolved within a depth optimization framework that utilizes pre-generated human postures from multiple viewpoints. We also provide a novel affordance representation defined on relative orientations and proximity between dense human and object points, that can be easily aggregated from any 3D HOI datasets. The proposed representation serves as a primitive that can be manifested to conventional affordance representations via simple transformations, ranging from physically exerted affordances to nonphysical ones. We demonstrate the efficacy of our method and representation by generating the 3D affordance samples and deriving high-quality affordance examples from the representation, including contact, orientation, and spatial occupancies.</li>
</ul>

<h3>Title: GALA: Generating Animatable Layered Assets from a Single Scan</h3>
<ul>
<li><strong>Authors: </strong>Taeksoo Kim, Byungjun Kim, Shunsuke Saito, Hanbyul Joo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.12979">https://arxiv.org/abs/2401.12979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.12979">https://arxiv.org/pdf/2401.12979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.12979]] GALA: Generating Animatable Layered Assets from a Single Scan(https://arxiv.org/abs/2401.12979)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present GALA, a framework that takes as input a single-layer clothed 3D human mesh and decomposes it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed human avatars with any pose. Existing reconstruction approaches often treat clothed humans as a single-layer of geometry and overlook the inherent compositionality of humans with hairstyles, clothing, and accessories, thereby limiting the utility of the meshes for downstream applications. Decomposing a single-layer mesh into separate layers is a challenging task because it requires the synthesis of plausible geometry and texture for the severely occluded regions. Moreover, even with successful decomposition, meshes are not normalized in terms of poses and body shapes, failing coherent composition with novel identities and poses. To address these challenges, we propose to leverage the general knowledge of a pretrained 2D diffusion model as geometry and appearance prior for humans and other assets. We first separate the input mesh using the 3D surface segmentation extracted from multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and reanimation with novel poses. Our experiments demonstrate the effectiveness of our approach for decomposition, canonicalization, and composition tasks compared to existing solutions.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
