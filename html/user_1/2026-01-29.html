<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-29</h1>
<h3>Title: Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Boxiang Zhao, Qince Li, Zhonghao Wang, Zelin Cao, Yi Wang, Peng Cheng, Bo Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19923">https://arxiv.org/abs/2601.19923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19923">https://arxiv.org/pdf/2601.19923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19923]] Table-BiEval: A Self-Supervised, Dual-Track Framework for Decoupling Structure and Content in LLM Evaluation(https://arxiv.org/abs/2601.19923)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) evolve into autonomous agents, the capability to faithfully translate natural language into rigorous structured formats-essential for tool invocation-and to convert complex tabular information into machine-readable specifications has become paramount. However, current evaluations lack effective methodologies to measure this structural fidelity without costly human intervention, as traditional text metrics fail to detect semantic drift in code-like outputs. This paper proposes Table-BiEval, a novel approach based on a human-free, self-supervised evaluation framework, to assess LLMs performance quantitatively. By leveraging deterministic Intermediate Representations, our framework calculates Content Semantic Accuracy and Normalized Tree Edit Distance to decouple structure from content. Also, it empirically evaluates 15 state-of-the-art LLMs across dual topological dimensions-hierarchical structures and flat tables. The results reveal substantial variability, highlighting that mid-sized models can surprisingly outperform larger counterparts in structural efficiency and confirming that deep recursive nesting remains a universal bottleneck for current architectures.</li>
</ul>

<h3>Title: oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hyunmin Kim, Yukun Zhou, Rahul A. Jonas, Lie Ju, Sunjin Hwang, Pearse A. Keane, Siegfried K. Wagner</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19939">https://arxiv.org/abs/2601.19939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19939">https://arxiv.org/pdf/2601.19939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19939]] oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction(https://arxiv.org/abs/2601.19939)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.</li>
</ul>

<h3>Title: Classifier Calibration at Scale: An Empirical Study of Model-Agnostic Post-Hoc Methods</h3>
<ul>
<li><strong>Authors: </strong>Valery Manokhin, Daniel Gr√∏nhaug</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19944">https://arxiv.org/abs/2601.19944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19944">https://arxiv.org/pdf/2601.19944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19944]] Classifier Calibration at Scale: An Empirical Study of Model-Agnostic Post-Hoc Methods(https://arxiv.org/abs/2601.19944)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We study model-agnostic post-hoc calibration methods intended to improve probabilistic predictions in supervised binary classification on real i.i.d. tabular data, with particular emphasis on conformal and Venn-based approaches that provide distribution-free validity guarantees under exchangeability. We benchmark 21 widely used classifiers, including linear models, SVMs, tree ensembles (CatBoost, XGBoost, LightGBM), and modern tabular neural and foundation models, on binary tasks from the TabArena-v0.1 suite using randomized, stratified five-fold cross-validation with a held-out test fold. Five calibrators; Isotonic regression, Platt scaling, Beta calibration, Venn-Abers predictors, and Pearsonify are trained on a separate calibration split and applied to test predictions. Calibration is evaluated using proper scoring rules (log-loss and Brier score) and diagnostic measures (Spiegelhalter's Z, ECE, and ECI), alongside discrimination (AUC-ROC) and standard classification metrics. Across tasks and architectures, Venn-Abers predictors achieve the largest average reductions in log-loss, followed closely by Beta calibration, while Platt scaling exhibits weaker and less consistent effects. Beta calibration improves log-loss most frequently across tasks, whereas Venn-Abers displays fewer instances of extreme degradation and slightly more instances of extreme improvement. Importantly, we find that commonly used calibration procedures, most notably Platt scaling and isotonic regression, can systematically degrade proper scoring performance for strong modern tabular models. Overall classification performance is often preserved, but calibration effects vary substantially across datasets and architectures, and no method dominates uniformly. In expectation, all methods except Pearsonify slightly increase accuracy, but the effect is marginal, with the largest expected gain about 0.008%.</li>
</ul>

<h3>Title: MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference</h3>
<ul>
<li><strong>Authors: </strong>Huanlin Gao, Ping Chen, Fuyuan Shi, Ruijia Wu, Li YanTao, Qiang Hui, Yuren You, Ting Lu, Chao Tan, Shaoan Zhao, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19961">https://arxiv.org/abs/2601.19961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19961">https://arxiv.org/pdf/2601.19961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19961]] MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference(https://arxiv.org/abs/2601.19961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.</li>
</ul>

<h3>Title: BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Soham Sarkar, Tanmay Sen, Sayantan Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.19992">https://arxiv.org/abs/2601.19992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.19992">https://arxiv.org/pdf/2601.19992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.19992]] BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection(https://arxiv.org/abs/2601.19992)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.</li>
</ul>

<h3>Title: Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method</h3>
<ul>
<li><strong>Authors: </strong>Pragatheeswaran Vipulanandan, Kamal Premaratne, Dilip Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20026">https://arxiv.org/abs/2601.20026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20026">https://arxiv.org/pdf/2601.20026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20026]] Semantic Uncertainty Quantification of Hallucinations in LLMs: A Quantum Tensor Network Based Method(https://arxiv.org/abs/2601.20026)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit strong generative capabilities but remain vulnerable to confabulations, fluent yet unreliable outputs that vary arbitrarily even under identical prompts. Leveraging a quantum tensor network based pipeline, we propose a quantum physics inspired uncertainty quantification framework that accounts for aleatoric uncertainty in token sequence probability for semantic equivalence based clustering of LLM generations. This offers a principled and interpretable scheme for hallucination detection. We further introduce an entropy maximization strategy that prioritizes high certainty, semantically coherent outputs and highlights entropy regions where LLM decisions are likely to be unreliable, offering practical guidelines for when human oversight is warranted. We evaluate the robustness of our scheme under different generation lengths and quantization levels, dimensions overlooked in prior studies, demonstrating that our approach remains reliable even in resource constrained deployments. A total of 116 experiments on TriviaQA, NQ, SVAMP, and SQuAD across multiple architectures including Mistral-7B, Mistral-7B-instruct, Falcon-rw-1b, LLaMA-3.2-1b, LLaMA-2-13b-chat, LLaMA-2-7b-chat, LLaMA-2-13b, and LLaMA-2-7b show consistent improvements in AUROC and AURAC over state of the art baselines.</li>
</ul>

<h3>Title: In-Context Reinforcement Learning From Suboptimal Historical Data</h3>
<ul>
<li><strong>Authors: </strong>Juncheng Dong, Moyang Guo, Ethan X. Fang, Zhuoran Yang, Vahid Tarokh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20116">https://arxiv.org/abs/2601.20116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20116">https://arxiv.org/pdf/2601.20116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20116]] In-Context Reinforcement Learning From Suboptimal Historical Data(https://arxiv.org/abs/2601.20116)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.</li>
</ul>

<h3>Title: Membership Inference Attacks Against Fine-tuned Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuetian Chen, Kaiyuan Zhang, Yuntao Du, Edoardo Stoppa, Charles Fleming, Ashish Kundu, Bruno Ribeiro, Ninghui Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20125">https://arxiv.org/abs/2601.20125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20125">https://arxiv.org/pdf/2601.20125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20125]] Membership Inference Attacks Against Fine-tuned Diffusion Language Models(https://arxiv.org/abs/2601.20125)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.</li>
</ul>

<h3>Title: Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR</h3>
<ul>
<li><strong>Authors: </strong>Zilai Wang, Natarajan Balaji Shankar, Kaiyuan Zhang, Zihan Wang, Abeer Alwan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20142">https://arxiv.org/abs/2601.20142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20142">https://arxiv.org/pdf/2601.20142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20142]] Mind the Shift: Using Delta SSL Embeddings to Enhance Child ASR(https://arxiv.org/abs/2601.20142)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) models have achieved impressive results across many speech tasks, yet child automatic speech recognition (ASR) remains challenging due to limited data and pretraining domain mismatch. Fine-tuning SSL models on child speech induces shifts in the representation space. We hypothesize that delta SSL embeddings, defined as the differences between embeddings from a finetuned model and those from its pretrained counterpart, encode task-specific information that complements finetuned features from another SSL model. We evaluate multiple fusion strategies on the MyST childrens corpus using different models. Results show that delta embedding fusion with WavLM yields up to a 10 percent relative WER reduction for HuBERT and a 4.4 percent reduction for W2V2, compared to finetuned embedding fusion. Notably, fusing WavLM with delta W2V2 embeddings achieves a WER of 9.64, setting a new state of the art among SSL models on the MyST corpus. These findings demonstrate the effectiveness of delta embeddings and highlight feature fusion as a promising direction for advancing child ASR.</li>
</ul>

<h3>Title: Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Bo Dai, Na Li, Dale Schuurmans</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20154">https://arxiv.org/abs/2601.20154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20154">https://arxiv.org/pdf/2601.20154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20154]] Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning(https://arxiv.org/abs/2601.20154)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. In this paper, we are therefore compelled to develop a principled foundation of representation learning. We first theoretically investigate the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications.</li>
</ul>

<h3>Title: Efficient Token Pruning for LLaDA-V</h3>
<ul>
<li><strong>Authors: </strong>Zhewen Wan, Tianchen Song, Chen Lin, Zhiyong Zhao, Xianpeng Lang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20168">https://arxiv.org/abs/2601.20168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20168">https://arxiv.org/pdf/2601.20168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20168]] Efficient Token Pruning for LLaDA-V(https://arxiv.org/abs/2601.20168)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.</li>
</ul>

<h3>Title: MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zeyang Huang, Takanori Fujiwara, Angelos Chatzimparmpas, Wandrille Duchemin, Andreas Kerren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20173">https://arxiv.org/abs/2601.20173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20173">https://arxiv.org/pdf/2601.20173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20173]] MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis(https://arxiv.org/abs/2601.20173)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We present a new nonlinear dimensionality reduction method, MAPLE, that enhances UMAP by improving manifold modeling. MAPLE employs a self-supervised learning approach to more efficiently encode low-dimensional manifold geometry. Central to this approach are maximum manifold capacity representations (MMCRs), which help untangle complex manifolds by compressing variances among locally similar data points while amplifying variance among dissimilar data points. This design is particularly effective for high-dimensional data with substantial intra-cluster variance and curved manifold structures, such as biological or image data. Our qualitative and quantitative evaluations demonstrate that MAPLE can produce clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.</li>
</ul>

<h3>Title: TeleStyle: Content-Preserving Style Transfer in Images and Videos</h3>
<ul>
<li><strong>Authors: </strong>Shiwen Zhang, Xiaoyan Yang, Bojia Zi, Haibin Huang, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20175">https://arxiv.org/abs/2601.20175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20175">https://arxiv.org/pdf/2601.20175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20175]] TeleStyle: Content-Preserving Style Transfer in Images and Videos(https://arxiv.org/abs/2601.20175)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at this https URL</li>
</ul>

<h3>Title: Securing AI Agents in Cyber-Physical Systems: A Survey of Environmental Interactions, Deepfake Threats, and Defenses</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Hatami, Van Tuan Pham, Hozefa Lakadawala, Yu Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20184">https://arxiv.org/abs/2601.20184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20184">https://arxiv.org/pdf/2601.20184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20184]] Securing AI Agents in Cyber-Physical Systems: A Survey of Environmental Interactions, Deepfake Threats, and Defenses(https://arxiv.org/abs/2601.20184)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing integration of AI agents into cyber-physical systems (CPS) introduces new security risks that extend beyond traditional cyber or physical threat models. Recent advances in generative AI enable deepfake and semantic manipulation attacks that can compromise agent perception, reasoning, and interaction with the physical environment, while emerging protocols such as the Model Context Protocol (MCP) further expand the attack surface through dynamic tool use and cross-domain context sharing. This survey provides a comprehensive review of security threats targeting AI agents in CPS, with a particular focus on environmental interactions, deepfake-driven attacks, and MCP-mediated vulnerabilities. We organize the literature using the SENTINEL framework, a lifecycle-aware methodology that integrates threat characterization, feasibility analysis under CPS constraints, defense selection, and continuous validation. Through an end-to-end case study grounded in a real-world smart grid deployment, we quantitatively illustrate how timing, noise, and false-positive costs constrain deployable defenses, and why detection mechanisms alone are insufficient as decision authorities in safety-critical CPS. The survey highlights the role of provenance- and physics-grounded trust mechanisms and defense-in-depth architectures, and outlines open challenges toward trustworthy AI-enabled CPS.</li>
</ul>

<h3>Title: DeRaDiff: Denoising Time Realignment of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ratnavibusena Don Shahain Manujith, Yang Zhang, Teoh Tze Tzun, Kenji Kawaguchi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20198">https://arxiv.org/abs/2601.20198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20198">https://arxiv.org/pdf/2601.20198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20198]] DeRaDiff: Denoising Time Realignment of Diffusion Models(https://arxiv.org/abs/2601.20198)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to "reward hacking". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.</li>
</ul>

<h3>Title: Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization</h3>
<ul>
<li><strong>Authors: </strong>Julian Gutierrez, Redouane Silvente</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20226">https://arxiv.org/abs/2601.20226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20226">https://arxiv.org/pdf/2601.20226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20226]] Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization(https://arxiv.org/abs/2601.20226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.</li>
</ul>

<h3>Title: ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zichao Yu, Ming Li, Wenyi Zhang, Difan Zou, Weiguo Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20227">https://arxiv.org/abs/2601.20227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20227">https://arxiv.org/pdf/2601.20227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20227]] ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance(https://arxiv.org/abs/2601.20227)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.</li>
</ul>

<h3>Title: BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning</h3>
<ul>
<li><strong>Authors: </strong>Jan Niklas Kolf, Ozan Tezcan, Justin Theiss, Hyung Jun Kim, Wentao Bao, Bhargav Bhushanam, Khushi Gupta, Arun Kejariwal, Naser Damer, Fadi Boutros</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20246">https://arxiv.org/abs/2601.20246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20246">https://arxiv.org/pdf/2601.20246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20246]] BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning(https://arxiv.org/abs/2601.20246)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.</li>
</ul>

<h3>Title: Order-Optimal Sample Complexity of Rectified Flows</h3>
<ul>
<li><strong>Authors: </strong>Hari Krishna Sahoo, Mudit Gaur, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20250">https://arxiv.org/abs/2601.20250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20250">https://arxiv.org/pdf/2601.20250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20250]] Order-Optimal Sample Complexity of Rectified Flows(https://arxiv.org/abs/2601.20250)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$. This improves on the best known $O(\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.</li>
</ul>

<h3>Title: SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility</h3>
<ul>
<li><strong>Authors: </strong>Xuanyu Su, Diana Inkpen, Nathalie Japkowicz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20256">https://arxiv.org/abs/2601.20256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20256">https://arxiv.org/pdf/2601.20256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20256]] SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility(https://arxiv.org/abs/2601.20256)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Online hate on social media ranges from overt slurs and threats (\emph{hard hate speech}) to \emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \textbf{\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \emph{Argumentum Model of Topics} (AMT) and \emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \textbf{7} sociocultural domains and \textbf{28} target groups, comprising \textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \textcolor{red}{\textbf{Disclaimer.} Contains offensive examples used solely for research.}</li>
</ul>

<h3>Title: C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding</h3>
<ul>
<li><strong>Authors: </strong>Jinren Ding, Xuejian Xu, Shen Jiang, Zhitong Hao, Jinhui Yang, Peng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20257">https://arxiv.org/abs/2601.20257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20257">https://arxiv.org/pdf/2601.20257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20257]] C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding(https://arxiv.org/abs/2601.20257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: this https URL.</li>
</ul>

<h3>Title: Reversible Efficient Diffusion for Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Xingxin Xu, Bing Cao, DongDong Li, Qinghua Hu, Pengfei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20260">https://arxiv.org/abs/2601.20260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20260">https://arxiv.org/pdf/2601.20260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20260]] Reversible Efficient Diffusion for Image Fusion(https://arxiv.org/abs/2601.20260)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.</li>
</ul>

<h3>Title: Artifact-Aware Evaluation for High-Quality Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhu, Jiashu Zhu, Yanxun Li, Meiqi Wu, Bingze Song, Chubin Chen, Jiahong Wu, Xiangxiang Chu, Yangang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20297">https://arxiv.org/abs/2601.20297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20297">https://arxiv.org/pdf/2601.20297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20297]] Artifact-Aware Evaluation for High-Quality Video Generation(https://arxiv.org/abs/2601.20297)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.</li>
</ul>

<h3>Title: MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Jing Xu, Minglin Wu, Xueyuan Chen, Xixin Wu, Helen Meng</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20300">https://arxiv.org/abs/2601.20300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20300">https://arxiv.org/pdf/2601.20300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20300]] MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting(https://arxiv.org/abs/2601.20300)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.</li>
</ul>

<h3>Title: Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Genyuan Zhang, Zihao Wang, Zhifan Gao, Lei Xu, Zhen Zhou, Haijun Yu, Jianjia Zhang, Xiujian Liu, Weiwei Zhang, Shaoyu Wang, Huazhu Fu, Fenglin Liu, Weiwen Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20304">https://arxiv.org/abs/2601.20304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20304">https://arxiv.org/pdf/2601.20304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20304]] Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction(https://arxiv.org/abs/2601.20304)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.</li>
</ul>

<h3>Title: TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yanjie Tu, Qingsen Yan, Axi Niu, Jiacong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20306">https://arxiv.org/abs/2601.20306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20306">https://arxiv.org/pdf/2601.20306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20306]] TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration(https://arxiv.org/abs/2601.20306)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: this https URL.</li>
</ul>

<h3>Title: OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Shuoyan Wei, Feng Li, Chen Zhou, Runmin Cong, Yao Zhao, Huihui Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20308">https://arxiv.org/abs/2601.20308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20308">https://arxiv.org/pdf/2601.20308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20308]] OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion(https://arxiv.org/abs/2601.20308)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.</li>
</ul>

<h3>Title: SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks</h3>
<ul>
<li><strong>Authors: </strong>Xin Zhang, Zijin Yang, Kejiang Chen, Linfeng Ma, Weiming Zhang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20310">https://arxiv.org/abs/2601.20310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20310">https://arxiv.org/pdf/2601.20310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20310]] SemBind: Binding Diffusion Watermarks to Semantics Against Black-Box Forgery Attacks(https://arxiv.org/abs/2601.20310)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Latent-based watermarks, integrated into the generation process of latent diffusion models (LDMs), simplify detection and attribution of generated images. However, recent black-box forgery attacks, where an attacker needs at least one watermarked image and black-box access to the provider's model, can embed the provider's watermark into images not produced by the provider, posing outsized risk to provenance and trust. We propose SemBind, the first defense framework for latent-based watermarks that resists black-box forgery by binding latent signals to image semantics via a learned semantic masker. Trained with contrastive learning, the masker yields near-invariant codes for the same prompt and near-orthogonal codes across prompts; these codes are reshaped and permuted to modulate the target latent before any standard latent-based watermark. SemBind is generally compatible with existing latent-based watermarking schemes and keeps image quality essentially unchanged, while a simple mask-ratio parameter offers a tunable trade-off between anti-forgery strength and robustness. Across four mainstream latent-based watermark methods, our SemBind-enabled anti-forgery variants markedly reduce false acceptance under black-box forgery while providing a controllable robustness-security balance.</li>
</ul>

<h3>Title: CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Hu, Yancheng He, Weixun Wang, Tao Feng, Li Lin, Jiashun Liu, Wenbo Su, Bo Zheng, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20327">https://arxiv.org/abs/2601.20327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20327">https://arxiv.org/pdf/2601.20327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20327]] CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria(https://arxiv.org/abs/2601.20327)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.</li>
</ul>

<h3>Title: Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching</h3>
<ul>
<li><strong>Authors: </strong>Fengrui Zuo, Zhiwei Ke, Yiming Liu, Wenqi Lou, Chao Wang, Xvehai Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20332">https://arxiv.org/abs/2601.20332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20332">https://arxiv.org/pdf/2601.20332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20332]] Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching(https://arxiv.org/abs/2601.20332)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at this https URL.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.</li>
</ul>

<h3>Title: Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining</h3>
<ul>
<li><strong>Authors: </strong>Ali Zia, Usman Ali, Umer Ramzan, Abdul Rehman, Abdelwahed Khamis, Wei Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20333">https://arxiv.org/abs/2601.20333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20333">https://arxiv.org/pdf/2601.20333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20333]] Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining(https://arxiv.org/abs/2601.20333)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.</li>
</ul>

<h3>Title: Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space</h3>
<ul>
<li><strong>Authors: </strong>Yangyi Shen, Tianjian Feng, Jiaqi Han, Wen Wang, Tianlang Chen, Chunhua Shen, Jure Leskovec, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20339">https://arxiv.org/abs/2601.20339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20339">https://arxiv.org/pdf/2601.20339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20339]] Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space(https://arxiv.org/abs/2601.20339)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.</li>
</ul>

<h3>Title: Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Zengbin Wang, Xuecai Hu, Yong Wang, Feng Xiong, Man Zhang, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20354">https://arxiv.org/abs/2601.20354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20354">https://arxiv.org/pdf/2601.20354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20354]] Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models(https://arxiv.org/abs/2601.20354)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.</li>
</ul>

<h3>Title: Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku</h3>
<ul>
<li><strong>Authors: </strong>Mariia Drozdova</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20363">https://arxiv.org/abs/2601.20363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20363">https://arxiv.org/pdf/2601.20363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20363]] Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku(https://arxiv.org/abs/2601.20363)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.</li>
</ul>

<h3>Title: RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zhen Liu, Diedong Feng, Hai Jiang, Liaoyuan Zeng, Hao Wang, Chaoyu Feng, Lei Lei, Bing Zeng, Shuaicheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20364">https://arxiv.org/abs/2601.20364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20364">https://arxiv.org/pdf/2601.20364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20364]] RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching(https://arxiv.org/abs/2601.20364)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.</li>
</ul>

<h3>Title: Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Qing Lyu, Zhe Fu, Alexandre Bayen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20367">https://arxiv.org/abs/2601.20367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20367">https://arxiv.org/pdf/2601.20367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20367]] Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models(https://arxiv.org/abs/2601.20367)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.</li>
</ul>

<h3>Title: LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Anda Cheng, Yinggui Wang, Lei Wang, Tao Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20375">https://arxiv.org/abs/2601.20375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20375">https://arxiv.org/pdf/2601.20375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20375]] LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning(https://arxiv.org/abs/2601.20375)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.</li>
</ul>

<h3>Title: HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Mengge Liu, Yan Di, Gu Wang, Yun Qu, Dekai Zhu, Yanyan Li, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20383">https://arxiv.org/abs/2601.20383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20383">https://arxiv.org/pdf/2601.20383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20383]] HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation(https://arxiv.org/abs/2601.20383)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.</li>
</ul>

<h3>Title: SpeechMapper: Speech-to-text Embedding Projector for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Biswesh Mohapatra, Marcely Zanon Boito, Ioan Calapodescu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20417">https://arxiv.org/abs/2601.20417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20417">https://arxiv.org/pdf/2601.20417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20417]] SpeechMapper: Speech-to-text Embedding Projector for LLMs(https://arxiv.org/abs/2601.20417)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.</li>
</ul>

<h3>Title: Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance</h3>
<ul>
<li><strong>Authors: </strong>Chenliang Zhou, Fangcheng Zhong, Weihao Xia, Albert Miao, Canberk Baykal, Cengiz Oztireli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20425">https://arxiv.org/abs/2601.20425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20425">https://arxiv.org/pdf/2601.20425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20425]] Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance(https://arxiv.org/abs/2601.20425)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.</li>
</ul>

<h3>Title: Nonlinear Dimensionality Reduction with Diffusion Maps in Practice</h3>
<ul>
<li><strong>Authors: </strong>S√∂nke Beier, Paula Pirker-D√≠az, Friedrich Pagenkopf, Karoline Wiesner</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20428">https://arxiv.org/abs/2601.20428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20428">https://arxiv.org/pdf/2601.20428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20428]] Nonlinear Dimensionality Reduction with Diffusion Maps in Practice(https://arxiv.org/abs/2601.20428)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.</li>
</ul>

<h3>Title: MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Xu, Wei Lu, Xiangyang Luo, Jiantao Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20433">https://arxiv.org/abs/2601.20433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20433">https://arxiv.org/pdf/2601.20433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20433]] MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models(https://arxiv.org/abs/2601.20433)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.</li>
</ul>

<h3>Title: Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Yanzhu Liu, Xiao Liu, Yuexuan Wang, Mondal Soumik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20461">https://arxiv.org/abs/2601.20461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20461">https://arxiv.org/pdf/2601.20461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20461]] Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection(https://arxiv.org/abs/2601.20461)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.</li>
</ul>

<h3>Title: Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch</h3>
<ul>
<li><strong>Authors: </strong>Evanfiya Logacheva, Arto Hellas, Tsvetomila Mihaylova, Juha Sorva, Ava Heinonen, Juho Leinonen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20476">https://arxiv.org/abs/2601.20476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20476">https://arxiv.org/pdf/2601.20476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20476]] Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch(https://arxiv.org/abs/2601.20476)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.</li>
</ul>

<h3>Title: An explainable framework for the relationship between dementia and glucose metabolism patterns</h3>
<ul>
<li><strong>Authors: </strong>C. V√°zquez-Garc√≠a, F. J. Mart√≠nez-Murcia, F. Segovia Rom√°n, A. Forte, J. Ram√≠rez, I. Ill√°n, A. Hern√°ndez-Segura, C. Jim√©nez-Mesa, Juan M. G√≥rriz</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20480">https://arxiv.org/abs/2601.20480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20480">https://arxiv.org/pdf/2601.20480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20480]] An explainable framework for the relationship between dementia and glucose metabolism patterns(https://arxiv.org/abs/2601.20480)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.</li>
</ul>

<h3>Title: Efficient Autoregressive Video Diffusion with Dummy Head</h3>
<ul>
<li><strong>Authors: </strong>Hang Guo, Zhaoyang Jia, Jiahao Li, Bin Li, Yuanhao Cai, Jiangshan Wang, Yawei Li, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20499">https://arxiv.org/abs/2601.20499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20499">https://arxiv.org/pdf/2601.20499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20499]] Efficient Autoregressive Video Diffusion with Dummy Head(https://arxiv.org/abs/2601.20499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at this https URL.</li>
</ul>

<h3>Title: Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V</h3>
<ul>
<li><strong>Authors: </strong>Meiqi Wu, Bingze Song, Ruimin Lin, Chen Zhu, Xiaokun Feng, Jiahong Wu, Xiangxiang Chu, Kaiqi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20504">https://arxiv.org/abs/2601.20504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20504">https://arxiv.org/pdf/2601.20504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20504]] Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V(https://arxiv.org/abs/2601.20504)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.</li>
</ul>

<h3>Title: Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective</h3>
<ul>
<li><strong>Authors: </strong>Qiyan Zhao, Xiaofeng Zhang, Shuochen Chang, Qianyu Chen, Xiaosong Yuan, Xuhang Chen, Luoqi Liu, Jiajun Zhang, Xu-Yao Zhang, Da-Han Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20520">https://arxiv.org/abs/2601.20520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20520">https://arxiv.org/pdf/2601.20520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20520]] Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective(https://arxiv.org/abs/2601.20520)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at this https URL</li>
</ul>

<h3>Title: AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors</h3>
<ul>
<li><strong>Authors: </strong>Matic Fuƒçka, Vitjan Zavrtanik, Danijel Skoƒçaj</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20524">https://arxiv.org/abs/2601.20524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20524">https://arxiv.org/pdf/2601.20524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20524]] AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors(https://arxiv.org/abs/2601.20524)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: this https URL</li>
</ul>

<h3>Title: DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression</h3>
<ul>
<li><strong>Authors: </strong>Wenzhuo Ma, Zhenzhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20564">https://arxiv.org/abs/2601.20564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20564">https://arxiv.org/pdf/2601.20564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20564]] DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression(https://arxiv.org/abs/2601.20564)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.</li>
</ul>

<h3>Title: Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?</h3>
<ul>
<li><strong>Authors: </strong>Lakshman Balasubramanian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20598">https://arxiv.org/abs/2601.20598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20598">https://arxiv.org/pdf/2601.20598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20598]] Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?(https://arxiv.org/abs/2601.20598)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: this https URL.</li>
</ul>

<h3>Title: GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection</h3>
<ul>
<li><strong>Authors: </strong>Shuguang Zhang, Junhong Lian, Guoxin Yu, Baoxun Xu, Xiang Ao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20618">https://arxiv.org/abs/2601.20618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20618">https://arxiv.org/pdf/2601.20618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20618]] GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection(https://arxiv.org/abs/2601.20618)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.</li>
</ul>

<h3>Title: A Foundation Model for Virtual Sensors</h3>
<ul>
<li><strong>Authors: </strong>Leon G√∂tz, Lars Frederik Peiss, Erik Sauer, Andreas Udo Sass, Thorsten Bagdonat, Stephan G√ºnnemann, Leo Schwinn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20634">https://arxiv.org/abs/2601.20634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20634">https://arxiv.org/pdf/2601.20634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20634]] A Foundation Model for Virtual Sensors(https://arxiv.org/abs/2601.20634)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.</li>
</ul>

<h3>Title: Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability</h3>
<ul>
<li><strong>Authors: </strong>Rohan Asthana, Vasileios Belagiannis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20642">https://arxiv.org/abs/2601.20642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20642">https://arxiv.org/pdf/2601.20642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20642]] Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability(https://arxiv.org/abs/2601.20642)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.</li>
</ul>

<h3>Title: Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification</h3>
<ul>
<li><strong>Authors: </strong>Xin Jin, Jinming Liu, Yuntao Wei, Junyan Lin, Zhicheng Wang, Jianguo Huang, Xudong Yang, Yanxiao Liu, Wenjun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20742">https://arxiv.org/abs/2601.20742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20742">https://arxiv.org/pdf/2601.20742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20742]] Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification(https://arxiv.org/abs/2601.20742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>"Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.</li>
</ul>

<h3>Title: Supervised Guidance Training for Infinite-Dimensional Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Elizabeth L. Baker, Alexander Denker, Jes Frellsen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20756">https://arxiv.org/abs/2601.20756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20756">https://arxiv.org/pdf/2601.20756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20756]] Supervised Guidance Training for Infinite-Dimensional Diffusion Models(https://arxiv.org/abs/2601.20756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.</li>
</ul>

<h3>Title: FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haonan Zhong, Wei Song, Tingxu Han, Maurice Pagnucco, Jingling Xue, Yang Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20791">https://arxiv.org/abs/2601.20791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20791">https://arxiv.org/pdf/2601.20791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20791]] FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models(https://arxiv.org/abs/2601.20791)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos. Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.</li>
</ul>

<h3>Title: Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yiran Huang, Karsten Roth, Quentin Bouniot, Wenjia Xu, Zeynep Akata</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20796">https://arxiv.org/abs/2601.20796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20796">https://arxiv.org/pdf/2601.20796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20796]] Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers(https://arxiv.org/abs/2601.20796)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.</li>
</ul>

<h3>Title: Reinforcement Learning via Self-Distillation</h3>
<ul>
<li><strong>Authors: </strong>Jonas H√ºbotter, Frederike L√ºbeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20802">https://arxiv.org/abs/2601.20802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20802">https://arxiv.org/pdf/2601.20802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20802]] Reinforcement Learning via Self-Distillation(https://arxiv.org/abs/2601.20802)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.</li>
</ul>

<h3>Title: Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Aunabil Chakma, Mihai Surdeanu, Eduardo Blanco</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20803">https://arxiv.org/abs/2601.20803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20803">https://arxiv.org/pdf/2601.20803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20803]] Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction(https://arxiv.org/abs/2601.20803)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This paper presents several strategies to automatically obtain additional examples for in-context learning of one-shot relation extraction. Specifically, we introduce a novel strategy for example selection, in which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example. We show that this method results in complementary word choices and sentence structures when compared to LLM-generated examples. When these strategies are combined, the resulting hybrid system achieves a more holistic picture of the relations of interest than either method alone. Our framework transfers well across datasets (FS-TACRED and FS-FewRel) and LLM families (Qwen and Gemma). Overall, our hybrid selection method consistently outperforms alternative strategies and achieves state-of-the-art performance on FS-TACRED and strong gains on a customized FewRel subset.</li>
</ul>

<h3>Title: PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20845">https://arxiv.org/abs/2601.20845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20845">https://arxiv.org/pdf/2601.20845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20845]] PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting(https://arxiv.org/abs/2601.20845)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.</li>
</ul>

<h3>Title: Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>An√≠bal Silva, Mois√©s Santos, Andr√© Restivo, Carlos Soares</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20854">https://arxiv.org/abs/2601.20854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20854">https://arxiv.org/pdf/2601.20854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20854]] Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation(https://arxiv.org/abs/2601.20854)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</li>
</ul>

<h3>Title: FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Zhou, Zisen Shao, Sheng Miao, Pan Wang, Dongfeng Bai, Bingbing Liu, Yiyi Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20857">https://arxiv.org/abs/2601.20857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20857">https://arxiv.org/pdf/2601.20857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20857]] FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models(https://arxiv.org/abs/2601.20857)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
