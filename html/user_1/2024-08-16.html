<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-16</h1>
<h3>Title: Impact of Inaccurate Contamination Ratio on Robust Unsupervised Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jordan F. Masakuna, DJeff Kanda Nkashama, Arian Soltani, Marc Frappier, Pierre-Martin Tardif, Froduald Kabanza</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07718">https://arxiv.org/abs/2408.07718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07718">https://arxiv.org/pdf/2408.07718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07718]] Impact of Inaccurate Contamination Ratio on Robust Unsupervised Anomaly Detection(https://arxiv.org/abs/2408.07718)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Training data sets intended for unsupervised anomaly detection, typically presumed to be anomaly-free, often contain anomalies (or contamination), a challenge that significantly undermines model performance. Most robust unsupervised anomaly detection models rely on contamination ratio information to tackle contamination. However, in reality, contamination ratio may be inaccurate. We investigate on the impact of inaccurate contamination ratio information in robust unsupervised anomaly detection. We verify whether they are resilient to misinformed contamination ratios. Our investigation on 6 benchmark data sets reveals that such models are not adversely affected by exposure to misinformation. In fact, they can exhibit improved performance when provided with such inaccurate contamination ratios.</li>
</ul>

<h3>Title: Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies</h3>
<ul>
<li><strong>Authors: </strong>Peiran Wang, Qiyu Li, Longxuan Yu, Ziyao Wang, Ang Li, Haojian Jin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07728">https://arxiv.org/abs/2408.07728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07728">https://arxiv.org/pdf/2408.07728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07728]] Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies(https://arxiv.org/abs/2408.07728)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present Moderator, a policy-based model management system that allows administrators to specify fine-grained content moderation policies and modify the weights of a text-to-image (TTI) model to make it significantly more challenging for users to produce images that violate the policies. In contrast to existing general-purpose model editing techniques, which unlearn concepts without considering the associated contexts, Moderator allows admins to specify what content should be moderated, under which context, how it should be moderated, and why moderation is necessary. Given a set of policies, Moderator first prompts the original model to generate images that need to be moderated, then uses these self-generated images to reverse fine-tune the model to compute task vectors for moderation and finally negates the original model with the task vectors to decrease its performance in generating moderated content. We evaluated Moderator with 14 participants to play the role of admins and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations. Our experiment with 32 stable diffusion users suggested that Moderator can prevent 65% of users from generating moderated content under 15 attempts and require the remaining users an average of 8.3 times more attempts to generate undesired content.</li>
</ul>

<h3>Title: MedTsLLM: Leveraging LLMs for Multimodal Medical Time Series Analysis</h3>
<ul>
<li><strong>Authors: </strong>Nimeesha Chan, Felix Parker, William Bennett, Tianyi Wu, Mung Yao Jia, James Fackler, Kimia Ghobadi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07773">https://arxiv.org/abs/2408.07773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07773">https://arxiv.org/pdf/2408.07773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07773]] MedTsLLM: Leveraging LLMs for Multimodal Medical Time Series Analysis(https://arxiv.org/abs/2408.07773)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The complexity and heterogeneity of data in many real-world applications pose significant challenges for traditional machine learning and signal processing techniques. For instance, in medicine, effective analysis of diverse physiological signals is crucial for patient monitoring and clinical decision-making and yet highly challenging. We introduce MedTsLLM, a general multimodal large language model (LLM) framework that effectively integrates time series data and rich contextual information in the form of text to analyze physiological signals, performing three tasks with clinical relevance: semantic segmentation, boundary detection, and anomaly detection in time series. These critical tasks enable deeper analysis of physiological signals and can provide actionable insights for clinicians. We utilize a reprogramming layer to align embeddings of time series patches with a pretrained LLM's embedding space and make effective use of raw time series, in conjunction with textual context. Given the multivariate nature of medical datasets, we develop methods to handle multiple covariates. We additionally tailor the text prompt to include patient-specific information. Our model outperforms state-of-the-art baselines, including deep learning models, other LLMs, and clinical methods across multiple medical domains, specifically electrocardiograms and respiratory waveforms. MedTsLLM presents a promising step towards harnessing the power of LLMs for medical time series analysis that can elevate data-driven tools for clinicians and improve patient outcomes.</li>
</ul>

<h3>Title: Cropper: Vision-Language Model for Image Cropping through In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Seung Hyun Lee, Junjie Ke, Yinxiao Li, Junfeng He, Steven Hickson, Katie Datsenko, Sangpil Kim, Ming-Hsuan Yang, Irfan Essa, Feng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07790">https://arxiv.org/abs/2408.07790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07790">https://arxiv.org/pdf/2408.07790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07790]] Cropper: Vision-Language Model for Image Cropping through In-Context Learning(https://arxiv.org/abs/2408.07790)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The goal of image cropping is to identify visually appealing crops within an image. Conventional methods rely on specialized architectures trained on specific datasets, which struggle to be adapted to new requirements. Recent breakthroughs in large vision-language models (VLMs) have enabled visual in-context learning without explicit training. However, effective strategies for vision downstream tasks with VLMs remain largely unclear and underexplored. In this paper, we propose an effective approach to leverage VLMs for better image cropping. First, we propose an efficient prompt retrieval mechanism for image cropping to automate the selection of in-context examples. Second, we introduce an iterative refinement strategy to iteratively enhance the predicted crops. The proposed framework, named Cropper, is applicable to a wide range of cropping tasks, including free-form cropping, subject-aware cropping, and aspect ratio-aware cropping. Extensive experiments and a user study demonstrate that Cropper significantly outperforms state-of-the-art methods across several benchmarks.</li>
</ul>

<h3>Title: SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Osman, Daniel Z. Kaplan, Tamer Nadeem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07851">https://arxiv.org/abs/2408.07851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07851">https://arxiv.org/pdf/2408.07851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07851]] SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition(https://arxiv.org/abs/2408.07851)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction.</li>
</ul>

<h3>Title: A Systematic Evaluation of Generated Time Series and Their Effects in Self-Supervised Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Audrey Der, Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, Zhongfang Zhuang, Vivian Lai, Junpeng Wang, Liang Wang, Wei Zhang, Eamonn Keogh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07869">https://arxiv.org/abs/2408.07869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07869">https://arxiv.org/pdf/2408.07869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07869]] A Systematic Evaluation of Generated Time Series and Their Effects in Self-Supervised Pretraining(https://arxiv.org/abs/2408.07869)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised Pretrained Models (PTMs) have demonstrated remarkable performance in computer vision and natural language processing tasks. These successes have prompted researchers to design PTMs for time series data. In our experiments, most self-supervised time series PTMs were surpassed by simple supervised models. We hypothesize this undesired phenomenon may be caused by data scarcity. In response, we test six time series generation methods, use the generated data in pretraining in lieu of the real data, and examine the effects on classification performance. Our results indicate that replacing a real-data pretraining set with a greater volume of only generated samples produces noticeable improvement.</li>
</ul>

<h3>Title: Instruct Large Language Models to Generate Scientific Literature Survey Step by Step</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Lai, Yupeng Wu, Yidan Wang, Wenpeng Hu, Chen Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07884">https://arxiv.org/abs/2408.07884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07884">https://arxiv.org/pdf/2408.07884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07884]] Instruct Large Language Models to Generate Scientific Literature Survey Step by Step(https://arxiv.org/abs/2408.07884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Abstract. Automatically generating scientific literature surveys is a valuable task that can significantly enhance research efficiency. However, the diverse and complex nature of information within a literature survey poses substantial challenges for generative models. In this paper, we design a series of prompts to systematically leverage large language models (LLMs), enabling the creation of comprehensive literature surveys through a step-by-step approach. Specifically, we design prompts to guide LLMs to sequentially generate the title, abstract, hierarchical headings, and the main content of the literature survey. We argue that this design enables the generation of the headings from a high-level perspective. During the content generation process, this design effectively harnesses relevant information while minimizing costs by restricting the length of both input and output content in LLM queries. Our implementation with Qwen-long achieved third place in the NLPCC 2024 Scientific Literature Survey Generation evaluation task, with an overall score only 0.03% lower than the second-place team. Additionally, our soft heading recall is 95.84%, the second best among the submissions. Thanks to the efficient prompt design and the low cost of the Qwen-long API, our method reduces the expense for generating each literature survey to 0.1 RMB, enhancing the practical value of our method.</li>
</ul>

<h3>Title: MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Xie, Gaochen Wu, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07930">https://arxiv.org/abs/2408.07930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07930">https://arxiv.org/pdf/2408.07930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07930]] MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL(https://arxiv.org/abs/2408.07930)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Recent In-Context Learning based methods have achieved remarkable success in Text-to-SQL task. However, there is still a large gap between the performance of these models and human performance on datasets with complex database schema and difficult questions, such as BIRD. Besides, existing work has neglected to supervise intermediate steps when solving questions iteratively with question decomposition methods, and the schema linking methods used in these works are very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent generative approach with soft schema linking and iterative Sub-SQL refinement. In our framework, an entity-based method with tables' summary is used to select the columns in database, and a novel targets-conditions decomposition method is introduced to decompose those complex questions. Additionally, we build a iterative generating module which includes a Sub-SQL Generator and Sub-SQL Refiner, introducing external oversight for each step of generation. Through a series of ablation studies, the effectiveness of each agent in our framework has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL achieves an execution accuracy of 61.08\%, compared to the baseline accuracy of 46.35\% for vanilla GPT-4 and the baseline accuracy of 57.56\% for MAC-SQL. Besides, our approach makes similar progress on Spider.</li>
</ul>

<h3>Title: ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Faris Hijazi (1), Somayah AlHarbi (1), Abdulaziz AlHussein (1), Harethah Abu Shairah (2), Reem AlZahrani (2), Hebah AlShamlan (1), Omar Knio (2), George Turkiyyah (2) ((1) THIQAH, (2) KAUST)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07983">https://arxiv.org/abs/2408.07983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07983">https://arxiv.org/pdf/2408.07983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07983]] ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models(https://arxiv.org/abs/2408.07983)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The rapid advancements in Large Language Models (LLMs) have led to significant improvements in various natural language processing tasks. However, the evaluation of LLMs' legal knowledge, particularly in non-English languages such as Arabic, remains under-explored. To address this gap, we introduce ArabLegalEval, a multitask benchmark dataset for assessing the Arabic legal knowledge of LLMs. Inspired by the MMLU and LegalBench datasets, ArabLegalEval consists of multiple tasks sourced from Saudi legal documents and synthesized questions. In this work, we aim to analyze the capabilities required to solve legal problems in Arabic and benchmark the performance of state-of-the-art LLMs. We explore the impact of in-context learning and investigate various evaluation methods. Additionally, we explore workflows for generating questions with automatic validation to enhance the dataset's quality. We benchmark multilingual and Arabic-centric LLMs, such as GPT-4 and Jais, respectively. We also share our methodology for creating the dataset and validation, which can be generalized to other domains. We hope to accelerate AI research in the Arabic Legal domain by releasing the ArabLegalEval dataset and code: this https URL</li>
</ul>

<h3>Title: Causal Discovery from Time-Series Data with Short-Term Invariance-Based Convolutional Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Rujia Shen, Boran Wang, Chao Zhao, Yi Guan, Jingchi Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08023">https://arxiv.org/abs/2408.08023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08023">https://arxiv.org/pdf/2408.08023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08023]] Causal Discovery from Time-Series Data with Short-Term Invariance-Based Convolutional Neural Networks(https://arxiv.org/abs/2408.08023)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Causal discovery from time-series data aims to capture both intra-slice (contemporaneous) and inter-slice (time-lagged) causality between variables within the temporal chain, which is crucial for various scientific disciplines. Compared to causal discovery from non-time-series data, causal discovery from time-series data necessitates more serialized samples with a larger amount of observed time steps. To address the challenges, we propose a novel gradient-based causal discovery approach STIC, which focuses on \textbf{S}hort-\textbf{T}erm \textbf{I}nvariance using \textbf{C}onvolutional neural networks to uncover the causal relationships from time-series data. Specifically, STIC leverages both the short-term time and mechanism invariance of causality within each window observation, which possesses the property of independence, to enhance sample efficiency. Furthermore, we construct two causal convolution kernels, which correspond to the short-term time and mechanism invariance respectively, to estimate the window causal graph. To demonstrate the necessity of convolutional neural networks for causal discovery from time-series data, we theoretically derive the equivalence between convolution and the underlying generative principle of time-series data under the assumption that the additive noise model is identifiable. Experimental evaluations conducted on both synthetic and FMRI benchmark datasets demonstrate that our STIC outperforms baselines significantly and achieves the state-of-the-art performance, particularly when the datasets contain a limited number of observed time steps. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: The Clever Hans Effect in Unsupervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Jacob Kauffmann, Jonas Dippel, Lukas Ruff, Wojciech Samek, Klaus-Robert Müller, Grégoire Montavon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08041">https://arxiv.org/abs/2408.08041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08041">https://arxiv.org/pdf/2408.08041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08041]] The Clever Hans Effect in Unsupervised Learning(https://arxiv.org/abs/2408.08041)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Unsupervised learning has become an essential building block of AI systems. The representations it produces, e.g. in foundation models, are critical to a wide variety of downstream applications. It is therefore important to carefully examine unsupervised models to ensure not only that they produce accurate predictions, but also that these predictions are not "right for the wrong reasons", the so-called Clever Hans (CH) effect. Using specially developed Explainable AI techniques, we show for the first time that CH effects are widespread in unsupervised learning. Our empirical findings are enriched by theoretical insights, which interestingly point to inductive biases in the unsupervised learning machine as a primary source of CH effects. Overall, our work sheds light on unexplored risks associated with practical applications of unsupervised learning and suggests ways to make unsupervised learning more robust.</li>
</ul>

<h3>Title: Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Stefano Woerner, Christian F. Baumgartner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08058">https://arxiv.org/abs/2408.08058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08058">https://arxiv.org/pdf/2408.08058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08058]] Navigating Data Scarcity using Foundation Models: A Benchmark of Few-Shot and Zero-Shot Learning Approaches in Medical Imaging(https://arxiv.org/abs/2408.08058)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Data scarcity is a major limiting factor for applying modern machine learning techniques to clinical tasks. Although sufficient data exists for some well-studied medical tasks, there remains a long tail of clinically relevant tasks with poor data availability. Recently, numerous foundation models have demonstrated high suitability for few-shot learning (FSL) and zero-shot learning (ZSL), potentially making them more accessible to practitioners. However, it remains unclear which foundation model performs best on FSL medical image analysis tasks and what the optimal methods are for learning from limited data. We conducted a comprehensive benchmark study of ZSL and FSL using 16 pretrained foundation models on 19 diverse medical imaging datasets. Our results indicate that BiomedCLIP, a model pretrained exclusively on medical data, performs best on average for very small training set sizes, while very large CLIP models pretrained on LAION-2B perform best with slightly more training samples. However, simply fine-tuning a ResNet-18 pretrained on ImageNet performs similarly with more than five training examples per class. Our findings also highlight the need for further research on foundation models specifically tailored for medical applications and the collection of more datasets to train these models.</li>
</ul>

<h3>Title: MambaMIM: Pre-training Mamba with State Space Token-interpolation</h3>
<ul>
<li><strong>Authors: </strong>Fenghe Tang, Bingkun Nian, Yingtai Li, Jie Yang, Liu Wei, S. Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08070">https://arxiv.org/abs/2408.08070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08070">https://arxiv.org/pdf/2408.08070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08070]] MambaMIM: Pre-training Mamba with State Space Token-interpolation(https://arxiv.org/abs/2408.08070)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Generative self-supervised learning demonstrates outstanding representation learning capabilities in both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, there are currently no generative pre-training methods related to selective state space models (Mamba) that can handle long-range dependencies effectively. To address this challenge, we introduce a generative self-supervised learning method for Mamba (MambaMIM) based on Selective Structure State Space Sequence Token-interpolation (S6T), a general-purpose pre-training method for arbitrary Mamba architectures. Our method, MambaMIM, incorporates a bottom-up 3D hybrid masking strategy in the encoder to maintain masking consistency across different architectures. Additionally, S6T is employed to learn causal relationships between the masked sequence in the state space. MambaMIM can be used on any single or hybrid Mamba architectures to enhance the Mamba long-range representation capability. Extensive downstream experiments reveal the feasibility and advancement of using Mamba for pre-training medical image tasks. The code is available at: this https URL</li>
</ul>

<h3>Title: ColorMamba: Towards High-quality NIR-to-RGB Spectral Translation with Mamba</h3>
<ul>
<li><strong>Authors: </strong>Huiyu Zhai, Guang Jin, Xingxing Yang, Guosheng Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08087">https://arxiv.org/abs/2408.08087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08087">https://arxiv.org/pdf/2408.08087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08087]] ColorMamba: Towards High-quality NIR-to-RGB Spectral Translation with Mamba(https://arxiv.org/abs/2408.08087)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Translating NIR to the visible spectrum is challenging due to cross-domain complexities. Current models struggle to balance a broad receptive field with computational efficiency, limiting practical use. Although the Selective Structured State Space Model, especially the improved version, Mamba, excels in generative tasks by capturing long-range dependencies with linear complexity, its default approach of converting 2D images into 1D sequences neglects local context. In this work, we propose a simple but effective backbone, dubbed ColorMamba, which first introduces Mamba into spectral translation tasks. To explore global long-range dependencies and local context for efficient spectral translation, we introduce learnable padding tokens to enhance the distinction of image boundaries and prevent potential confusion within the sequence model. Furthermore, local convolutional enhancement and agent attention are designed to improve the vanilla Mamba. Moreover, we exploit the HSV color to provide multi-scale guidance in the reconstruction process for more accurate spectral translation. Extensive experiments show that our ColorMamba achieves a 1.02 improvement in terms of PSNR compared with the state-of-the-art method. Our code is available at this https URL.</li>
</ul>

<h3>Title: When Video Coding Meets Multimodal Large Language Models: A Unified Paradigm for Video Coding</h3>
<ul>
<li><strong>Authors: </strong>Pingping Zhang, Jinlong Li, Meng Wang, Nicu Sebe, Sam Kwong, Shiqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08093">https://arxiv.org/abs/2408.08093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08093">https://arxiv.org/pdf/2408.08093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08093]] When Video Coding Meets Multimodal Large Language Models: A Unified Paradigm for Video Coding(https://arxiv.org/abs/2408.08093)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing codecs are designed to eliminate intrinsic redundancies to create a compact representation for compression. However, strong external priors from Multimodal Large Language Models (MLLMs) have not been explicitly explored in video compression. Herein, we introduce a unified paradigm for Cross-Modality Video Coding (CMVC), which is a pioneering approach to explore multimodality representation and video generative models in video coding. Specifically, on the encoder side, we disentangle a video into spatial content and motion components, which are subsequently transformed into distinct modalities to achieve very compact representation by leveraging MLLMs. During decoding, previously encoded components and video generation models are leveraged to create multiple encoding-decoding modes that optimize video reconstruction quality for specific decoding requirements, including Text-Text-to-Video (TT2V) mode to ensure high-quality semantic information and Image-Text-to-Video (IT2V) mode to achieve superb perceptual consistency. In addition, we propose an efficient frame interpolation model for IT2V mode via Low-Rank Adaption (LoRA) tuning to guarantee perceptual quality, which allows the generated motion cues to behave smoothly. Experiments on benchmarks indicate that TT2V achieves effective semantic reconstruction, while IT2V exhibits competitive perceptual consistency. These results highlight potential directions for future research in video coding.</li>
</ul>

<h3>Title: Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Adi Haviv, Shahar Sarfaty, Uri Hacohen, Niva Elkin-Koren, Roi Livni, Amit H Bermano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08184">https://arxiv.org/abs/2408.08184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08184">https://arxiv.org/pdf/2408.08184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08184]] Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion(https://arxiv.org/abs/2408.08184)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This work addresses the challenge of quantifying originality in text-to-image (T2I) generative diffusion models, with a focus on copyright originality. We begin by evaluating T2I models' ability to innovate and generalize through controlled experiments, revealing that stable diffusion models can effectively recreate unseen elements with sufficiently diverse training data. Then, our key insight is that concepts and combinations of image elements the model is familiar with, and saw more during training, are more concisly represented in the model's latent space. We hence propose a method that leverages textual inversion to measure the originality of an image based on the number of tokens required for its reconstruction by the model. Our approach is inspired by legal definitions of originality and aims to assess whether a model can produce original content without relying on specific prompts or having the training data of the model. We demonstrate our method using both a pre-trained stable diffusion model and a synthetic dataset, showing a correlation between the number of tokens and image originality. This work contributes to the understanding of originality in generative models and has implications for copyright infringement cases.</li>
</ul>

<h3>Title: Heavy Labels Out! Dataset Distillation with Label Space Lightening</h3>
<ul>
<li><strong>Authors: </strong>Ruonan Yu, Songhua Liu, Zigeng Chen, Jingwen Ye, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08201">https://arxiv.org/abs/2408.08201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08201">https://arxiv.org/pdf/2408.08201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08201]] Heavy Labels Out! Dataset Distillation with Label Space Lightening(https://arxiv.org/abs/2408.08201)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Dataset distillation or condensation aims to condense a large-scale training dataset into a much smaller synthetic one such that the training performance of distilled and original sets on neural networks are similar. Although the number of training samples can be reduced substantially, current state-of-the-art methods heavily rely on enormous soft labels to achieve satisfactory performance. As a result, the required storage can be comparable even to original datasets, especially for large-scale ones. To solve this problem, instead of storing these heavy labels, we propose a novel label-lightening framework termed HeLlO aiming at effective image-to-label projectors, with which synthetic labels can be directly generated online from synthetic images. Specifically, to construct such projectors, we leverage prior knowledge in open-source foundation models, e.g., CLIP, and introduce a LoRA-like fine-tuning strategy to mitigate the gap between pre-trained and target distributions, so that original models for soft-label generation can be distilled into a group of low-rank matrices. Moreover, an effective image optimization method is proposed to further mitigate the potential error between the original and distilled label generators. Extensive experiments demonstrate that with only about 0.003% of the original storage required for a complete set of soft labels, we achieve comparable performance to current state-of-the-art dataset distillation methods on large-scale datasets. Our code will be available.</li>
</ul>

<h3>Title: The Dawn of KAN in Image-to-Image (I2I) Translation: Integrating Kolmogorov-Arnold Networks with GANs for Unpaired I2I Translation</h3>
<ul>
<li><strong>Authors: </strong>Arpan Mahara, Naphtali D. Rishe, Liangdong Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08216">https://arxiv.org/abs/2408.08216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08216">https://arxiv.org/pdf/2408.08216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08216]] The Dawn of KAN in Image-to-Image (I2I) Translation: Integrating Kolmogorov-Arnold Networks with GANs for Unpaired I2I Translation(https://arxiv.org/abs/2408.08216)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image-to-Image translation in Generative Artificial Intelligence (Generative AI) has been a central focus of research, with applications spanning healthcare, remote sensing, physics, chemistry, photography, and more. Among the numerous methodologies, Generative Adversarial Networks (GANs) with contrastive learning have been particularly successful. This study aims to demonstrate that the Kolmogorov-Arnold Network (KAN) can effectively replace the Multi-layer Perceptron (MLP) method in generative AI, particularly in the subdomain of image-to-image translation, to achieve better generative quality. Our novel approach replaces the two-layer MLP with a two-layer KAN in the existing Contrastive Unpaired Image-to-Image Translation (CUT) model, developing the KAN-CUT model. This substitution favors the generation of more informative features in low-dimensional vector representations, which contrastive learning can utilize more effectively to produce high-quality images in the target domain. Extensive experiments, detailed in the results section, demonstrate the applicability of KAN in conjunction with contrastive learning and GANs in Generative AI, particularly for image-to-image translation. This work suggests that KAN could be a valuable component in the broader generative AI domain.</li>
</ul>

<h3>Title: Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding</h3>
<ul>
<li><strong>Authors: </strong>Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Aviv Regev, Sergey Levine, Masatoshi Uehara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08252">https://arxiv.org/abs/2408.08252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08252">https://arxiv.org/pdf/2408.08252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08252]] Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding(https://arxiv.org/abs/2408.08252)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require ``differentiable'' proxy models (\textit{e.g.}, classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (\textit{e.g.}, classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. The code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Snuffy: Efficient Whole Slide Image Classifier</h3>
<ul>
<li><strong>Authors: </strong>Hossein Jafarinia, Alireza Alipanah, Danial Hamdi, Saeed Razavi, Nahal Mirzaie, Mohammad Hossein Rohban</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.NE, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08258">https://arxiv.org/abs/2408.08258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08258">https://arxiv.org/pdf/2408.08258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08258]] Snuffy: Efficient Whole Slide Image Classifier(https://arxiv.org/abs/2408.08258)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Whole Slide Image (WSI) classification with multiple instance learning (MIL) in digital pathology faces significant computational challenges. Current methods mostly rely on extensive self-supervised learning (SSL) for satisfactory performance, requiring long training periods and considerable computational resources. At the same time, no pre-training affects performance due to domain shifts from natural images to WSIs. We introduce \textbf{\textit{Snuffy}} architecture, a novel MIL-pooling method based on sparse transformers that mitigates performance loss with limited pre-training and enables continual few-shot pre-training as a competitive option. Our sparsity pattern is tailored for pathology and is theoretically proven to be a universal approximator with the tightest probabilistic sharp bound on the number of layers for sparse transformers, to date. We demonstrate Snuffy's effectiveness on CAMELYON16 and TCGA Lung cancer datasets, achieving superior WSI and patch-level accuracies. The code is available on \url{this https URL}.</li>
</ul>

<h3>Title: mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental Health Text Analysis</h3>
<ul>
<li><strong>Authors: </strong>Dae-young Kim (1), Rebecca Hwa (2), Muhammad Mahbubur Rahman (1) ((1) Children's National Hospital, Washington, DC, (2) George Washington University, Washington, DC)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08261">https://arxiv.org/abs/2408.08261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08261">https://arxiv.org/pdf/2408.08261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08261]] mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental Health Text Analysis(https://arxiv.org/abs/2408.08261)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces mhGPT, a lightweight generative pre-trained transformer trained on mental health-related social media and PubMed articles. Fine-tuned for specific mental health tasks, mhGPT was evaluated under limited hardware constraints and compared with state-of-the-art models like MentaLLaMA and Gemma. Despite having only 1.98 billion parameters and using just 5% of the dataset, mhGPT outperformed larger models and matched the performance of models trained on significantly more data. The key contributions include integrating diverse mental health data, creating a custom tokenizer, and optimizing a smaller architecture for low-resource settings. This research could advance AI-driven mental health care, especially in areas with limited computing power.</li>
</ul>

<h3>Title: Understanding the Local Geometry of Generative Model Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Imtiaz Humayun, Ibtihel Amara, Candice Schumann, Golnoosh Farnadi, Negar Rostamzadeh, Mohammad Havaei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.08307">https://arxiv.org/abs/2408.08307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.08307">https://arxiv.org/pdf/2408.08307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.08307]] Understanding the Local Geometry of Generative Model Manifolds(https://arxiv.org/abs/2408.08307)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models learn continuous representations of complex data manifolds using a finite number of samples during training. For a pre-trained generative model, the common way to evaluate the quality of the manifold representation learned, is by computing global metrics like Fréchet Inception Distance using a large number of generated and real samples. However, generative model performance is not uniform across the learned manifold, e.g., for \textit{foundation models} like Stable Diffusion generation performance can vary significantly based on the conditioning or initial noise vector being denoised. In this paper we study the relationship between the \textit{local geometry of the learned manifold} and downstream generation. Based on the theory of continuous piecewise-linear (CPWL) generators, we use three geometric descriptors - scaling ($\psi$), rank ($\nu$), and complexity ($\delta$) - to characterize a pre-trained generative model manifold locally. We provide quantitative and qualitative evidence showing that for a given latent, the local descriptors are correlated with generation aesthetics, artifacts, uncertainty, and even memorization. Finally we demonstrate that training a \textit{reward model} on the local geometry can allow controlling the likelihood of a generated sample under the learned distribution.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
