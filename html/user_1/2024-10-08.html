<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-08</h1>
<h3>Title: LCM: Log Conformal Maps for Robust Representation Learning to Mitigate Perspective Distortion</h3>
<ul>
<li><strong>Authors: </strong>Meenakshi Subhash Chippa, Prakash Chandra Chhipa, Kanjar De, Marcus Liwicki, Rajkumar Saini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03686">https://arxiv.org/abs/2410.03686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03686">https://arxiv.org/pdf/2410.03686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03686]] LCM: Log Conformal Maps for Robust Representation Learning to Mitigate Perspective Distortion(https://arxiv.org/abs/2410.03686)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Perspective distortion (PD) leads to substantial alterations in the shape, size, orientation, angles, and spatial relationships of visual elements in images. Accurately determining camera intrinsic and extrinsic parameters is challenging, making it hard to synthesize perspective distortion effectively. The current distortion correction methods involve removing distortion and learning vision tasks, thus making it a multi-step process, often compromising performance. Recent work leverages the Möbius transform for mitigating perspective distortions (MPD) to synthesize perspective distortions without estimating camera parameters. An essential downside of using the Möbius transform is that it requires tuning multiple interdependent and interrelated parameters and involving complex arithmetic operations, leading to substantial computational complexity. To address these challenges, we propose Log Conformal Maps (LCM), a method leveraging the logarithmic function to approximate perspective distortions with fewer parameters and reduced computational complexity. We provide a theoretical foundation complemented with experiments to demonstrate that LCM with fewer parameters approximates the MPD. We show that LCM integrates well with supervised and self-supervised representation learning, outperform standard models, and matches the state-of-the-art performance in mitigating perspective distortion over multiple benchmarks, namely Imagenet-PD, Imagenet-E, and Imagenet-X. Further LCM demonstrate seamless integration with person re-identification and improved the performance. Source code shall be released soon.</li>
</ul>

<h3>Title: Thematic Analysis with Open-Source Generative AI and Machine Learning: A New Method for Inductive Qualitative Codebook Development</h3>
<ul>
<li><strong>Authors: </strong>Andrew Katz, Gabriella Coloyan Fleming, Joyce Main</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03721">https://arxiv.org/abs/2410.03721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03721">https://arxiv.org/pdf/2410.03721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03721]] Thematic Analysis with Open-Source Generative AI and Machine Learning: A New Method for Inductive Qualitative Codebook Development(https://arxiv.org/abs/2410.03721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper aims to answer one central question: to what extent can open-source generative text models be used in a workflow to approximate thematic analysis in social science research? To answer this question, we present the Generative AI-enabled Theme Organization and Structuring (GATOS) workflow, which uses open-source machine learning techniques, natural language processing tools, and generative text models to facilitate thematic analysis. To establish validity of the method, we present three case studies applying the GATOS workflow, leveraging these models and techniques to inductively create codebooks similar to traditional procedures using thematic analysis. Specifically, we investigate the extent to which a workflow comprising open-source models and tools can inductively produce codebooks that approach the known space of themes and sub-themes. To address the challenge of gleaning insights from these texts, we combine open-source generative text models, retrieval-augmented generation, and prompt engineering to identify codes and themes in large volumes of text, i.e., generate a qualitative codebook. The process mimics an inductive coding process that researchers might use in traditional thematic analysis by reading text one unit of analysis at a time, considering existing codes already in the codebook, and then deciding whether or not to generate a new code based on whether the extant codebook provides adequate thematic coverage. We demonstrate this workflow using three synthetic datasets from hypothetical organizational research settings: a study of teammate feedback in teamwork settings, a study of organizational cultures of ethical behavior, and a study of employee perspectives about returning to their offices after the pandemic. We show that the GATOS workflow is able to identify themes in the text that were used to generate the original synthetic datasets.</li>
</ul>

<h3>Title: Unsupervised Human Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Sumuk Shashidhar, Abhinav Chinta, Vaibhav Sahai, Dilek Hakkani Tur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03731">https://arxiv.org/abs/2410.03731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03731">https://arxiv.org/pdf/2410.03731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03731]] Unsupervised Human Preference Learning(https://arxiv.org/abs/2410.03731)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Large language models demonstrate impressive reasoning abilities but struggle to provide personalized content due to their lack of individual user preference information. Existing methods, such as in-context learning and parameter-efficient fine-tuning, fall short in capturing the complexity of human preferences, especially given the small, personal datasets individuals possess. In this paper, we propose a novel approach utilizing small parameter models as preference agents to generate natural language rules that guide a larger, pre-trained model, enabling efficient personalization. Our method involves a small, local "steering wheel" model that directs the outputs of a much larger foundation model, producing content tailored to an individual's preferences while leveraging the extensive knowledge and capabilities of the large model. Importantly, this personalization is achieved without the need to fine-tune the large model. Experimental results on email and article datasets, demonstrate that our technique significantly outperforms baseline personalization methods. By allowing foundation models to adapt to individual preferences in a data and compute-efficient manner, our approach paves the way for highly personalized language model applications.</li>
</ul>

<h3>Title: Beyond Scalar Reward Model: Learning Generative Judge from Preference Data</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03742">https://arxiv.org/abs/2410.03742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03742">https://arxiv.org/pdf/2410.03742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03742]] Beyond Scalar Reward Model: Learning Generative Judge from Preference Data(https://arxiv.org/abs/2410.03742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning from preference feedback is a common practice for aligning large language models~(LLMs) with human value. Conventionally, preference data is learned and encoded into a scalar reward model that connects a value head with an LLM to produce a scalar score as preference or reward. However, scalar models lack interpretability and are known to be susceptible to biases in datasets. This paper investigates leveraging the generation capability of LLMs to address both limitations in one shot. Specifically, we prompt the pre-trained LLM to generate positive and negative judgments, both supported with rationales in natural language form. The self-generated contrastive judgment pairs are used to train the generative judge with Direct Preference Optimization (DPO). This proposal of training the generative Judge using self-generated Contrastive judgments (Con-J) ensures natural interpretability due to the generated rationales together with the judgments, as well as high robustness against bias without the need for an additional reward head. Experimental results show that the performance of Con-J is comparable to the scalar reward model trained on the same collection of preference data, and demonstrate its superior interpretability and robustness in encoding human preferences.</li>
</ul>

<h3>Title: Khattat: Enhancing Readability and Concept Representation of Semantic Typography</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Hussein, Alaa Elsetohy, Sama Hadhoud, Tameem Bakr, Yasser Rohaim, Badr AlKhamissi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03748">https://arxiv.org/abs/2410.03748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03748">https://arxiv.org/pdf/2410.03748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03748]] Khattat: Enhancing Readability and Concept Representation of Semantic Typography(https://arxiv.org/abs/2410.03748)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Designing expressive typography that visually conveys a word's meaning while maintaining readability is a complex task, known as semantic typography. It involves selecting an idea, choosing an appropriate font, and balancing creativity with legibility. We introduce an end-to-end system that automates this process. First, a Large Language Model (LLM) generates imagery ideas for the word, useful for abstract concepts like freedom. Then, the FontCLIP pre-trained model automatically selects a suitable font based on its semantic understanding of font attributes. The system identifies optimal regions of the word for morphing and iteratively transforms them using a pre-trained diffusion model. A key feature is our OCR-based loss function, which enhances readability and enables simultaneous stylization of multiple characters. We compare our method with other baselines, demonstrating great readability enhancement and versatility across multiple languages and writing scripts.</li>
</ul>

<h3>Title: SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Juan Pablo Muñoz, Jinjie Yuan, Nilesh Jain</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03750">https://arxiv.org/abs/2410.03750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03750">https://arxiv.org/pdf/2410.03750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03750]] SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models(https://arxiv.org/abs/2410.03750)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large pre-trained models (LPMs), such as large language models, have become ubiquitous and are employed in many applications. These models are often adapted to a desired domain or downstream task through a fine-tuning stage. This paper proposes SQFT, an end-to-end solution for low-precision sparse parameter-efficient fine-tuning of LPMs, allowing for effective model manipulation in resource-constrained environments. Additionally, an innovative strategy enables the merging of sparse weights with low-rank adapters without losing sparsity and accuracy, overcoming the limitations of previous approaches. SQFT also addresses the challenge of having quantized weights and adapters with different numerical precisions, enabling merging in the desired numerical format without sacrificing accuracy. Multiple adaptation scenarios, models, and comprehensive sparsity levels demonstrate the effectiveness of SQFT. Models and code are available at this https URL.</li>
</ul>

<h3>Title: Denoising with a Joint-Embedding Predictive Architecture</h3>
<ul>
<li><strong>Authors: </strong>Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03755">https://arxiv.org/abs/2410.03755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03755">https://arxiv.org/pdf/2410.03755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03755]] Denoising with a Joint-Embedding Predictive Architecture(https://arxiv.org/abs/2410.03755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Joint-embedding predictive architectures (JEPAs) have shown substantial promise in self-supervised representation learning, yet their application in generative modeling remains underexplored. Conversely, diffusion models have demonstrated significant efficacy in modeling arbitrary probability distributions. In this paper, we introduce Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), pioneering the integration of JEPA within generative modeling. By recognizing JEPA as a form of masked image modeling, we reinterpret it as a generalized next-token prediction strategy, facilitating data generation in an auto-regressive manner. Furthermore, we incorporate diffusion loss to model the per-token probability distribution, enabling data generation in a continuous space. We also adapt flow matching loss as an alternative to diffusion loss, thereby enhancing the flexibility of D-JEPA. Empirically, with increased GFLOPs, D-JEPA consistently achieves lower FID scores with fewer training epochs, indicating its good scalability. Our base, large, and huge models outperform all previous generative models across all scales on class-conditional ImageNet benchmarks. Beyond image generation, D-JEPA is well-suited for other continuous data modeling, including video and audio.</li>
</ul>

<h3>Title: Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yohan Mathew, Ollie Matthews, Robert McCarthy, Joan Velja, Christian Schroeder de Witt, Dylan Cope, Nandi Schoots</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03768">https://arxiv.org/abs/2410.03768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03768">https://arxiv.org/pdf/2410.03768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03768]] Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs(https://arxiv.org/abs/2410.03768)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render collusion practically undetectable. This underscores the need for evaluation frameworks to monitor and mitigate steganographic collusion capabilities. We address a crucial gap in the literature by demonstrating, for the first time, that robust steganographic collusion in LLMs can arise indirectly from optimization pressure. To investigate this problem we design two approaches -- a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method -- for reliably eliciting sophisticated LLM-generated linguistic text steganography. Importantly, we find that emergent steganographic collusion can be robust to both passive steganalytic oversight of model outputs and active mitigation through communication paraphrasing. We contribute a novel model evaluation framework and discuss limitations and future work. Our findings imply that effective risk mitigation from steganographic collusion post-deployment requires innovation in passive and active oversight techniques.</li>
</ul>

<h3>Title: DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Changdae Oh, Yixuan Li, Kyungwoo Song, Sangdoo Yun, Dongyoon Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03782">https://arxiv.org/abs/2410.03782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03782">https://arxiv.org/pdf/2410.03782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03782]] DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation(https://arxiv.org/abs/2410.03782)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Adapting a pre-trained foundation model on downstream tasks should ensure robustness against distribution shifts without the need to retrain the whole model. Although existing weight interpolation methods are simple yet effective, we argue their static nature limits downstream performance while achieving efficiency. In this work, we propose DaWin, a training-free dynamic weight interpolation method that leverages the entropy of individual models over each unlabeled test sample to assess model expertise, and compute per-sample interpolation coefficients dynamically. Unlike previous works that typically rely on additional training to learn such coefficients, our approach requires no training. Then, we propose a mixture modeling approach that greatly reduces inference overhead raised by dynamic interpolation. We validate DaWin on the large-scale visual recognition benchmarks, spanning 14 tasks across robust fine-tuning -- ImageNet and derived five distribution shift benchmarks -- and multi-task learning with eight classification tasks. Results demonstrate that DaWin achieves significant performance gain in considered settings, with minimal computational overhead. We further discuss DaWin's analytic behavior to explain its empirical success.</li>
</ul>

<h3>Title: Improving Neural Optimal Transport via Displacement Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Jaemoo Choi, Yongxin Chen, Jaewoong Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03783">https://arxiv.org/abs/2410.03783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03783">https://arxiv.org/pdf/2410.03783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03783]] Improving Neural Optimal Transport via Displacement Interpolation(https://arxiv.org/abs/2410.03783)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optimal Transport (OT) theory investigates the cost-minimizing transport map that moves a source distribution to a target distribution. Recently, several approaches have emerged for learning the optimal transport map for a given cost function using neural networks. We refer to these approaches as the OT Map. OT Map provides a powerful tool for diverse machine learning tasks, such as generative modeling and unpaired image-to-image translation. However, existing methods that utilize max-min optimization often experience training instability and sensitivity to hyperparameters. In this paper, we propose a novel method to improve stability and achieve a better approximation of the OT Map by exploiting displacement interpolation, dubbed Displacement Interpolation Optimal Transport Model (DIOTM). We derive the dual formulation of displacement interpolation at specific time $t$ and prove how these dual problems are related across time. This result allows us to utilize the entire trajectory of displacement interpolation in learning the OT Map. Our method improves the training stability and achieves superior results in estimating optimal transport maps. We demonstrate that DIOTM outperforms existing OT-based models on image-to-image translation tasks.</li>
</ul>

<h3>Title: Repurposing Foundation Model for Generalizable Medical Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Nan Huang, Haishuai Wang, Zihuai He, Marinka Zitnik, Xiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03794">https://arxiv.org/abs/2410.03794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03794">https://arxiv.org/pdf/2410.03794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03794]] Repurposing Foundation Model for Generalizable Medical Time Series Classification(https://arxiv.org/abs/2410.03794)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Medical time series (MedTS) classification is critical for a wide range of healthcare applications such as Alzheimer's Disease diagnosis. However, its real-world deployment is severely challenged by poor generalizability due to inter- and intra-dataset heterogeneity in MedTS, including variations in channel configurations, time series lengths, and diagnostic tasks. Here, we propose FORMED, a foundation classification model that leverages a pre-trained backbone and tackles these challenges through re-purposing. FORMED integrates the general representation learning enabled by the backbone foundation model and the medical domain knowledge gained on a curated cohort of MedTS datasets. FORMED can adapt seamlessly to unseen MedTS datasets, regardless of the number of channels, sample lengths, or medical tasks. Experimental results show that, without any task-specific adaptation, the repurposed FORMED achieves performance that is competitive with, and often superior to, 11 baseline models trained specifically for each dataset. Furthermore, FORMED can effectively adapt to entirely new, unseen datasets, with lightweight parameter updates, consistently outperforming baselines. Our results highlight FORMED as a versatile and scalable model for a wide range of MedTS classification tasks, positioning it as a strong foundation model for future research in MedTS analysis.</li>
</ul>

<h3>Title: Text-guided Diffusion Model for 3D Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Yanchen Luo, Junfeng Fang, Sihang Li, Zhiyuan Liu, Jiancan Wu, An Zhang, Wenjie Du, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03803">https://arxiv.org/abs/2410.03803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03803">https://arxiv.org/pdf/2410.03803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03803]] Text-guided Diffusion Model for 3D Molecule Generation(https://arxiv.org/abs/2410.03803)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The de novo generation of molecules with targeted properties is crucial in biology, chemistry, and drug discovery. Current generative models are limited to using single property values as conditions, struggling with complex customizations described in detailed human language. To address this, we propose the text guidance instead, and introduce TextSMOG, a new Text-guided Small Molecule Generation Approach via 3D Diffusion Model which integrates language and diffusion models for text-guided small molecule generation. This method uses textual conditions to guide molecule generation, enhancing both stability and diversity. Experimental results show TextSMOG's proficiency in capturing and utilizing information from textual descriptions, making it a powerful tool for generating 3D molecular structures in response to complex textual customizations.</li>
</ul>

<h3>Title: Unsupervised Prior Learning: Discovering Categorical Pose Priors from Videos</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Wang, Shuangpeng Han, Mike Zheng Shou, Mengmi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03858">https://arxiv.org/abs/2410.03858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03858">https://arxiv.org/pdf/2410.03858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03858]] Unsupervised Prior Learning: Discovering Categorical Pose Priors from Videos(https://arxiv.org/abs/2410.03858)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>A prior represents a set of beliefs or assumptions about a system, aiding inference and decision-making. In this work, we introduce the challenge of unsupervised prior learning in pose estimation, where AI models learn pose priors of animate objects from videos in a self-supervised manner. These videos present objects performing various actions, providing crucial information about their keypoints and connectivity. While priors are effective in pose estimation, acquiring them can be difficult. We propose a novel method, named Pose Prior Learner (PPL), to learn general pose priors applicable to any object category. PPL uses a hierarchical memory to store compositional parts of prototypical poses, from which we distill a general pose prior. This prior enhances pose estimation accuracy through template transformation and image reconstruction. PPL learns meaningful pose priors without any additional human annotations or interventions, outperforming competitive baselines on both human and animal pose estimation datasets. Notably, our experimental results reveal the effectiveness of PPL using learnt priors for pose estimation on occluded images. Through iterative inference, PPL leverages priors to refine estimated poses, regressing them to any prototypical poses stored in memory. Our code, model, and data will be publicly available.</li>
</ul>

<h3>Title: MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Leo Bringer, Joey Wilson, Kira Barton, Maani Ghaffari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03860">https://arxiv.org/abs/2410.03860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03860">https://arxiv.org/pdf/2410.03860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03860]] MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty(https://arxiv.org/abs/2410.03860)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a Multi-modal Diffusion model for Motion Prediction (MDMP) that integrates and synchronizes skeletal data and textual descriptions of actions to generate refined long-term motion predictions with quantifiable uncertainty. Existing methods for motion forecasting or motion generation rely solely on either prior motions or text prompts, facing limitations with precision or control, particularly over extended durations. The multi-modal nature of our approach enhances the contextual understanding of human motion, while our graph-based transformer framework effectively capture both spatial and temporal motion dynamics. As a result, our model consistently outperforms existing generative techniques in accurately predicting long-term motions. Additionally, by leveraging diffusion models' ability to capture different modes of prediction, we estimate uncertainty, significantly improving spatial awareness in human-robot interactions by incorporating zones of presence with varying confidence levels for each body joint.</li>
</ul>

<h3>Title: Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Wang, Kuiyi Gao, Zihan Jia, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03869">https://arxiv.org/abs/2410.03869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03869">https://arxiv.org/pdf/2410.03869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03869]] Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step(https://arxiv.org/abs/2410.03869)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-based image generation models, such as Stable Diffusion and DALL-E 3, hold significant potential in content creation and publishing workflows, making them the focus in recent years. Despite their remarkable capability to generate diverse and vivid images, considerable efforts are being made to prevent the generation of harmful content, such as abusive, violent, or pornographic material. To assess the safety of existing models, we introduce a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process. Specifically, for malicious queries that cannot bypass the safeguards with a single prompt, we intentionally decompose the query into multiple sub-queries. The image generation models are then prompted to generate and iteratively edit images based on these sub-queries. To evaluate the effectiveness of our CoJ attack method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine safety scenarios, three types of editing operations, and three editing elements. Experiments on four widely-used image generation services provided by GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack method can successfully bypass the safeguards of models for over 60% cases, which significantly outperforms other jailbreaking methods (i.e., 14%). Further, to enhance these models' safety against our CoJ attack method, we also propose an effective prompting-based method, Think Twice Prompting, that can successfully defend over 95% of CoJ attack. We release our dataset and code to facilitate the AI safety research.</li>
</ul>

<h3>Title: Towards Cost Sensitive Decision Making</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Junier Oliva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03892">https://arxiv.org/abs/2410.03892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03892">https://arxiv.org/pdf/2410.03892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03892]] Towards Cost Sensitive Decision Making(https://arxiv.org/abs/2410.03892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many real-world situations allow for the acquisition of additional relevant information when making decisions with limited or uncertain data. However, traditional RL approaches either require all features to be acquired beforehand (e.g. in a MDP) or regard part of them as missing data that cannot be acquired (e.g. in a POMDP). In this work, we consider RL models that may actively acquire features from the environment to improve the decision quality and certainty, while automatically balancing the cost of feature acquisition process and the reward of task decision process. We propose the Active-Acquisition POMDP and identify two types of the acquisition process for different application domains. In order to assist the agent in the actively-acquired partially-observed environment and alleviate the exploration-exploitation dilemma, we develop a model-based approach, where a deep generative model is utilized to capture the dependencies of the features and impute the unobserved features. The imputations essentially represent the beliefs of the agent. Equipped with the dynamics model, we develop hierarchical RL algorithms to resolve both types of the AA-POMDPs. Empirical results demonstrate that our approach achieves considerably better performance than existing POMDP-RL solutions.</li>
</ul>

<h3>Title: Improving Node Representation by Boosting Target-Aware Contrastive Loss</h3>
<ul>
<li><strong>Authors: </strong>Ying-Chun Lin, Jennifer Neville</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03901">https://arxiv.org/abs/2410.03901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03901">https://arxiv.org/pdf/2410.03901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03901]] Improving Node Representation by Boosting Target-Aware Contrastive Loss(https://arxiv.org/abs/2410.03901)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graphs model complex relationships between entities, with nodes and edges capturing intricate connections. Node representation learning involves transforming nodes into low-dimensional embeddings. These embeddings are typically used as features for downstream tasks. Therefore, their quality has a significant impact on task performance. Existing approaches for node representation learning span (semi-)supervised, unsupervised, and self-supervised paradigms. In graph domains, (semi-)supervised learning often only optimizes models based on class labels, neglecting other abundant graph signals, which limits generalization. While self-supervised or unsupervised learning produces representations that better capture underlying graph signals, the usefulness of these captured signals for downstream target tasks can vary. To bridge this gap, we introduce Target-Aware Contrastive Learning (Target-aware CL) which aims to enhance target task performance by maximizing the mutual information between the target task and node representations with a self-supervised learning process. This is achieved through a sampling function, XGBoost Sampler (XGSampler), to sample proper positive examples for the proposed Target-Aware Contrastive Loss (XTCL). By minimizing XTCL, Target-aware CL increases the mutual information between the target task and node representations, such that model generalization is improved. Additionally, XGSampler enhances the interpretability of each signal by showing the weights for sampling the proper positive examples. We show experimentally that XTCL significantly improves the performance on two target tasks: node classification and link prediction tasks, compared to state-of-the-art models.</li>
</ul>

<h3>Title: PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lemei Zhang, Peng Liu, Marcus Tiedemann Oekland Henriksboe, Even W. Lauvrak, Jon Atle Gulla, Heri Ramampiaro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03905">https://arxiv.org/abs/2410.03905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03905">https://arxiv.org/pdf/2410.03905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03905]] PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models(https://arxiv.org/abs/2410.03905)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Natural Language Processing in recent years, numerous studies have shown that generic summaries generated by Large Language Models (LLMs) can sometimes surpass those annotated by experts, such as journalists, according to human evaluations. However, there is limited research on whether these generic summaries meet the individual needs of ordinary people. The biggest obstacle is the lack of human-annotated datasets from the general public. Existing work on personalized summarization often relies on pseudo datasets created from generic summarization datasets or controllable tasks that focus on specific named entities or other aspects, such as the length and specificity of generated summaries, collected from hypothetical tasks without the annotators' initiative. To bridge this gap, we propose a high-quality, personalized, manually annotated abstractive summarization dataset called PersonalSum. This dataset is the first to investigate whether the focus of public readers differs from the generic summaries generated by LLMs. It includes user profiles, personalized summaries accompanied by source sentences from given articles, and machine-generated generic summaries along with their sources. We investigate several personal signals - entities/topics, plot, and structure of articles - that may affect the generation of personalized summaries using LLMs in a few-shot in-context learning scenario. Our preliminary results and analysis indicate that entities/topics are merely one of the key factors that impact the diverse preferences of users, and personalized summarization remains a significant challenge for existing LLMs.</li>
</ul>

<h3>Title: Distribution Guided Active Feature Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Junier Oliva</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03915">https://arxiv.org/abs/2410.03915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03915">https://arxiv.org/pdf/2410.03915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03915]] Distribution Guided Active Feature Acquisition(https://arxiv.org/abs/2410.03915)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human agents routinely reason on instances with incomplete and muddied data (and weigh the cost of obtaining further features). In contrast, much of ML is devoted to the unrealistic, sterile environment where all features are observed and further information on an instance is obviated. Here we extend past static ML and develop an active feature acquisition (AFA) framework that interacts with the environment to obtain new information on-the-fly and can: 1) make inferences on an instance in the face of incomplete features, 2) determine a plan for feature acquisitions to obtain additional information on the instance at hand. We build our AFA framework on a backbone of understanding the information and conditional dependencies that are present in the data. First, we show how to build generative models that can capture dependencies over arbitrary subsets of features and employ these models for acquisitions in a greedy scheme. After, we show that it is possible to guide the training of RL agents for AFA via side-information and auxiliary rewards stemming from our generative models. We also examine two important factors for deploying AFA models in real-world scenarios, namely interpretability and robustness. Extensive experiments demonstrate the state-of-the-art performance of our AFA framework.</li>
</ul>

<h3>Title: Online Posterior Sampling with a Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Branislav Kveton, Boris Oreshkin, Youngsuk Park, Aniket Deshmukh, Rui Song</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03919">https://arxiv.org/abs/2410.03919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03919">https://arxiv.org/pdf/2410.03919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03919]] Online Posterior Sampling with a Diffusion Prior(https://arxiv.org/abs/2410.03919)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Posterior sampling in contextual bandits with a Gaussian prior can be implemented exactly or approximately using the Laplace approximation. The Gaussian prior is computationally efficient but it cannot describe complex distributions. In this work, we propose approximate posterior sampling algorithms for contextual bandits with a diffusion model prior. The key idea is to sample from a chain of approximate conditional posteriors, one for each stage of the reverse process, which are estimated in a closed form using the Laplace approximation. Our approximations are motivated by posterior sampling with a Gaussian prior, and inherit its simplicity and efficiency. They are asymptotically consistent and perform well empirically on a variety of contextual bandit problems.</li>
</ul>

<h3>Title: Clustering Alzheimer's Disease Subtypes via Similarity Learning and Graph Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Wei, Shu Yang, Davoud Ataee Tarzanagh, Jingxuan Bao, Jia Xu, Patryk Orzechowski, Joost B. Wagenaar, Qi Long, Li Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03937">https://arxiv.org/abs/2410.03937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03937">https://arxiv.org/pdf/2410.03937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03937]] Clustering Alzheimer's Disease Subtypes via Similarity Learning and Graph Diffusion(https://arxiv.org/abs/2410.03937)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is a complex neurodegenerative disorder that affects millions of people worldwide. Due to the heterogeneous nature of AD, its diagnosis and treatment pose critical challenges. Consequently, there is a growing research interest in identifying homogeneous AD subtypes that can assist in addressing these challenges in recent years. In this study, we aim to identify subtypes of AD that represent distinctive clinical features and underlying pathology by utilizing unsupervised clustering with graph diffusion and similarity learning. We adopted SIMLR, a multi-kernel similarity learning framework, and graph diffusion to perform clustering on a group of 829 patients with AD and mild cognitive impairment (MCI, a prodromal stage of AD) based on their cortical thickness measurements extracted from magnetic resonance imaging (MRI) scans. Although the clustering approach we utilized has not been explored for the task of AD subtyping before, it demonstrated significantly better performance than several commonly used clustering methods. Specifically, we showed the power of graph diffusion in reducing the effects of noise in the subtype detection. Our results revealed five subtypes that differed remarkably in their biomarkers, cognitive status, and some other clinical features. To evaluate the resultant subtypes further, a genetic association study was carried out and successfully identified potential genetic underpinnings of different AD subtypes. Our source code is available at: this https URL.</li>
</ul>

<h3>Title: AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Artur Kasymov, Marcin Sendera, Michał Stypułkowski, Maciej Zięba, Przemysław Spurek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03941">https://arxiv.org/abs/2410.03941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03941">https://arxiv.org/pdf/2410.03941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03941]] AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models(https://arxiv.org/abs/2410.03941)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Low-rank adaptation (LoRA) is a fine-tuning technique that can be applied to conditional generative diffusion models. LoRA utilizes a small number of context examples to adapt the model to a specific domain, character, style, or concept. However, due to the limited data utilized during training, the fine-tuned model performance is often characterized by strong context bias and a low degree of variability in the generated images. To solve this issue, we introduce AutoLoRA, a novel guidance technique for diffusion models fine-tuned with the LoRA approach. Inspired by other guidance techniques, AutoLoRA searches for a trade-off between consistency in the domain represented by LoRA weights and sample diversity from the base conditional diffusion model. Moreover, we show that incorporating classifier-free guidance for both LoRA fine-tuned and base models leads to generating samples with higher diversity and better quality. The experimental results for several fine-tuned LoRA domains show superiority over existing guidance techniques on selected metrics.</li>
</ul>

<h3>Title: LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity</h3>
<ul>
<li><strong>Authors: </strong>Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Ling Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03953">https://arxiv.org/abs/2410.03953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03953">https://arxiv.org/pdf/2410.03953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03953]] LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity(https://arxiv.org/abs/2410.03953)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Combining large language models during training or at inference time has shown substantial performance gain over component LLMs. This paper presents LLM-TOPLA, a diversity-optimized LLM ensemble method with three unique properties: (i) We introduce the focal diversity metric to capture the diversity-performance correlation among component LLMs of an ensemble. (ii) We develop a diversity-optimized ensemble pruning algorithm to select the top-k sub-ensembles from a pool of $N$ base LLMs. Our pruning method recommends top-performing LLM subensembles of size $S$, often much smaller than $N$. (iii) We generate new output for each prompt query by utilizing a learn-to-ensemble approach, which learns to detect and resolve the output inconsistency among all component LLMs of an ensemble. Extensive evaluation on four different benchmarks shows good performance gain over the best LLM ensemble methods: (i) In constrained solution set problems, LLM-TOPLA outperforms the best-performing ensemble (Mixtral) by 2.2\% in accuracy on MMLU and the best-performing LLM ensemble (MoreAgent) on GSM8k by 2.1\%. (ii) In generative tasks, LLM-TOPLA outperforms the top-2 performers (Llama70b/Mixtral) on SearchQA by $3.9\mathrm{x}$ in F1, and on XSum by more than $38$ in ROUGE-1. Our code and dataset, which contains outputs of 8 modern LLMs on 4 benchmarks is available at this https URL</li>
</ul>

<h3>Title: Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions</h3>
<ul>
<li><strong>Authors: </strong>Jianxin Zhang, Josh Viktorov, Doosan Jung, Emily Pitler</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03973">https://arxiv.org/abs/2410.03973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03973">https://arxiv.org/pdf/2410.03973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03973]] Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions(https://arxiv.org/abs/2410.03973)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural Stochastic Differential Equations (Neural SDEs) have emerged as powerful mesh-free generative models for continuous stochastic processes, with critical applications in fields such as finance, physics, and biology. Previous state-of-the-art methods have relied on adversarial training, such as GANs, or on minimizing distance measures between processes using signature kernels. However, GANs suffer from issues like instability, mode collapse, and the need for specialized training techniques, while signature kernel-based methods require solving linear PDEs and backpropagating gradients through the solver, whose computational complexity scales quadratically with the discretization steps. In this paper, we identify a novel class of strictly proper scoring rules for comparing continuous Markov processes. This theoretical finding naturally leads to a novel approach called Finite Dimensional Matching (FDM) for training Neural SDEs. Our method leverages the Markov property of SDEs to provide a computationally efficient training objective. This scoring rule allows us to bypass the computational overhead associated with signature kernels and reduces the training complexity from $O(D^2)$ to $O(D)$ per epoch, where $D$ represents the number of discretization steps of the process. We demonstrate that FDM achieves superior performance, consistently outperforming existing methods in terms of both computational efficiency and generative quality.</li>
</ul>

<h3>Title: SyllableLM: Learning Coarse Semantic Units for Speech Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alan Baade, Puyuan Peng, David Harwath</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04029">https://arxiv.org/abs/2410.04029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04029">https://arxiv.org/pdf/2410.04029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04029]] SyllableLM: Learning Coarse Semantic Units for Speech Language Models(https://arxiv.org/abs/2410.04029)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.</li>
</ul>

<h3>Title: BlockFound: Customized blockchain foundation model for anomaly detection</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Yu, Xian Wu, Hao Liu, Wenbo Guo, Xinyu Xing</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04039">https://arxiv.org/abs/2410.04039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04039">https://arxiv.org/pdf/2410.04039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04039]] BlockFound: Customized blockchain foundation model for anomaly detection(https://arxiv.org/abs/2410.04039)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>We propose BlockFound, a customized foundation model for anomaly blockchain transaction detection. Unlike existing methods that rely on rule-based systems or directly apply off-the-shelf large language models, BlockFound introduces a series of customized designs to model the unique data structure of blockchain transactions. First, a blockchain transaction is multi-modal, containing blockchain-specific tokens, texts, and numbers. We design a modularized tokenizer to handle these multi-modal inputs, balancing the information across different modalities. Second, we design a customized mask language learning mechanism for pretraining with RoPE embedding and FlashAttention for handling longer sequences. After training the foundation model, we further design a novel detection method for anomaly detection. Extensive evaluations on Ethereum and Solana transactions demonstrate BlockFound's exceptional capability in anomaly detection while maintaining a low false positive rate. Remarkably, BlockFound is the only method that successfully detects anomalous transactions on Solana with high accuracy, whereas all other approaches achieved very low or zero detection recall scores. This work not only provides new foundation models for blockchain but also sets a new benchmark for applying LLMs in blockchain data.</li>
</ul>

<h3>Title: Beyond Forecasting: Compositional Time Series Reasoning for End-to-End Task Execution</h3>
<ul>
<li><strong>Authors: </strong>Wen Ye, Yizhou Zhang, Wei Yang, Lumingyuan Tang, Defu Cao, Jie Cai, Yan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04047">https://arxiv.org/abs/2410.04047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04047">https://arxiv.org/pdf/2410.04047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04047]] Beyond Forecasting: Compositional Time Series Reasoning for End-to-End Task Execution(https://arxiv.org/abs/2410.04047)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In recent decades, there has been substantial advances in time series models and benchmarks across various individual tasks, such as time series forecasting, classification, and anomaly detection. Meanwhile, compositional reasoning in time series prevalent in real-world applications (e.g., decision-making and compositional question answering) is in great demand. Unlike simple tasks that primarily focus on predictive accuracy, compositional reasoning emphasizes the synthesis of diverse information from both time series data and various domain knowledge, making it distinct and extremely more challenging. In this paper, we introduce Compositional Time Series Reasoning, a new task of handling intricate multistep reasoning tasks from time series data. Specifically, this new task focuses on various question instances requiring structural and compositional reasoning abilities on time series data, such as decision-making and compositional question answering. As an initial attempt to tackle this novel task, we developed TS-Reasoner, a program-aided approach that utilizes large language model (LLM) to decompose a complex task into steps of programs that leverage existing time series models and numerical subroutines. Unlike existing reasoning work which only calls off-the-shelf modules, TS-Reasoner allows for the creation of custom modules and provides greater flexibility to incorporate domain knowledge as well as user-specified constraints. We demonstrate the effectiveness of our method through a comprehensive set of experiments. These promising results indicate potential opportunities in the new task of time series reasoning and highlight the need for further research.</li>
</ul>

<h3>Title: Enhancing Graph Self-Supervised Learning with Graph Interplay</h3>
<ul>
<li><strong>Authors: </strong>Xinjian Zhao, Wei Pang, Xiangru Jian, Yaoyao Xu, Chaolong Ying, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04061">https://arxiv.org/abs/2410.04061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04061">https://arxiv.org/pdf/2410.04061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04061]] Enhancing Graph Self-Supervised Learning with Graph Interplay(https://arxiv.org/abs/2410.04061)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph self-supervised learning (GSSL) has emerged as a compelling framework for extracting informative representations from graph-structured data without extensive reliance on labeled inputs. In this study, we introduce Graph Interplay (GIP), an innovative and versatile approach that significantly enhances the performance equipped with various existing GSSL methods. To this end, GIP advocates direct graph-level communications by introducing random inter-graph edges within standard batches. Against GIP's simplicity, we further theoretically show that \textsc{GIP} essentially performs a principled manifold separation via combining inter-graph message passing and GSSL, bringing about more structured embedding manifolds and thus benefits a series of downstream tasks. Our empirical study demonstrates that GIP surpasses the performance of prevailing GSSL methods across multiple benchmarks by significant margins, highlighting its potential as a breakthrough approach. Besides, GIP can be readily integrated into a series of GSSL methods and consistently offers additional performance gain. This advancement not only amplifies the capability of GSSL but also potentially sets the stage for a novel graph learning paradigm in a broader sense.</li>
</ul>

<h3>Title: $\epsilon$-VAE: Denoising as Visual Decoding</h3>
<ul>
<li><strong>Authors: </strong>Long Zhao, Sanghyun Woo, Ziyu Wan, Yandong Li, Han Zhang, Boqing Gong, Hartwig Adam, Xuhui Jia, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04081">https://arxiv.org/abs/2410.04081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04081">https://arxiv.org/pdf/2410.04081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04081]] $\epsilon$-VAE: Denoising as Visual Decoding(https://arxiv.org/abs/2410.04081)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approach. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.</li>
</ul>

<h3>Title: From Hospital to Portables: A Universal ECG Foundation Model Built on 10+ Million Diverse Recordings</h3>
<ul>
<li><strong>Authors: </strong>Jun Li, Aaron Aguirre, Junior Moura, Che Liu, Lanhai Zhong, Chenxi Sun, Gari Clifford, Brandon Westover, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04133">https://arxiv.org/abs/2410.04133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04133">https://arxiv.org/pdf/2410.04133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04133]] From Hospital to Portables: A Universal ECG Foundation Model Built on 10+ Million Diverse Recordings(https://arxiv.org/abs/2410.04133)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) has shown great promise in electrocardiogram (ECG) analysis and cardiovascular disease detection. However, developing a general AI-ECG model has been challenging due to inter-individual variability and the diversity of ECG diagnoses, limiting existing models to specific diagnostic tasks and datasets. Moreover, current AI-ECG models struggle to achieve comparable performance between single-lead and 12-lead ECGs, limiting the application of AI-ECG to portable and wearable ECG devices. To address these limitations, we introduce an ECG Foundation Model (ECGFounder), a general-purpose model that leverages real-world ECG annotations from cardiology experts to broaden the diagnostic capabilities of ECG analysis. ECGFounder is trained on over 10 million ECGs with 150 label categories from the Harvard-Emory ECG Database, enabling comprehensive cardiovascular disease diagnosis through ECG analysis. The model is designed to be both effective out-of-the-box and fine-tunable for downstream tasks, maximizing usability. More importantly, we extend its application to single-lead ECGs, enabling complex condition diagnoses and supporting various downstream tasks in mobile and remote monitoring scenarios. Experimental results demonstrate that ECGFounder achieves expert-level performance on internal validation sets for both 12-lead and single-lead ECGs, while also exhibiting strong classification performance and generalization across various diagnoses on external validation sets. When fine-tuned, ECGFounder outperforms baseline models in demographics detection, clinical event detection, and cross-modality cardiac rhythm diagnosis. The trained model and data will be publicly released upon publication through the this http URL. Our code is available at this https URL.</li>
</ul>

<h3>Title: Applying Quantum Autoencoders for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Robin Frehner, Kurt Stockinger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04154">https://arxiv.org/abs/2410.04154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04154">https://arxiv.org/pdf/2410.04154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04154]] Applying Quantum Autoencoders for Time Series Anomaly Detection(https://arxiv.org/abs/2410.04154)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection is an important problem with applications in various domains such as fraud detection, pattern recognition or medical diagnosis. Several algorithms have been introduced using classical computing approaches. However, using quantum computing for solving anomaly detection problems in time series data is a widely unexplored research field. This paper explores the application of quantum autoencoders to time series anomaly detection. We investigate two primary techniques for classifying anomalies: (1) Analyzing the reconstruction error generated by the quantum autoencoder and (2) latent representation analysis. Our simulated experimental results, conducted across various ansaetze, demonstrate that quantum autoencoders consistently outperform classical deep learning-based autoencoders across multiple datasets. Specifically, quantum autoencoders achieve superior anomaly detection performance while utilizing 60-230 times fewer parameters and requiring five times fewer training iterations. In addition, we implement our quantum encoder on real quantum hardware. Our experimental results demonstrate that quantum autoencoders achieve anomaly detection performance on par with their simulated counterparts.</li>
</ul>

<h3>Title: Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Keda Tao, Jinjin Gu, Yulun Zhang, Xiucheng Wang, Nan Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04161">https://arxiv.org/abs/2410.04161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04161">https://arxiv.org/pdf/2410.04161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04161]] Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model(https://arxiv.org/abs/2410.04161)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR) technique designed to improve the quality of facial image restoration from low-quality inputs. Leveraging a blend of attribute text prompts, high-quality reference images, and identity information, MGFR can mitigate the generation of false facial attributes and identities often associated with generative face restoration methods. By incorporating a dual-control adapter and a two-stage training strategy, our method effectively utilizes multi-modal prior information for targeted restoration tasks. We also present the Reface-HQ dataset, comprising over 23,000 high-resolution facial images across 5,000 identities, to address the need for reference face training images. Our approach achieves superior visual quality in restoring facial details under severe degradation and allows for controlled restoration processes, enhancing the accuracy of identity preservation and attribute correction. Including negative quality samples and attribute prompts in the training further refines the model's ability to generate detailed and perceptually accurate images.</li>
</ul>

<h3>Title: Preference Optimization as Probabilistic Inference</h3>
<ul>
<li><strong>Authors: </strong>Abbas Abdolmaleki, Bilal Piot, Bobak Shahriari, Jost Tobias Springenberg, Tim Hertweck, Rishabh Joshi, Junhyuk Oh, Michael Bloesch, Thomas Lampe, Nicolas Heess, Jonas Buchli, Martin Riedmiller</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04166">https://arxiv.org/abs/2410.04166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04166">https://arxiv.org/pdf/2410.04166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04166]] Preference Optimization as Probabilistic Inference(https://arxiv.org/abs/2410.04166)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing preference optimization methods are mainly designed for directly learning from human feedback with the assumption that paired examples (preferred vs. dis-preferred) are available. In contrast, we propose a method that can leverage unpaired preferred or dis-preferred examples, and works even when only one type of feedback (positive or negative) is available. This flexibility allows us to apply it in scenarios with varying forms of feedback and models, including training generative language models based on human feedback as well as training policies for sequential decision-making problems, where learned (value) functions are available. Our approach builds upon the probabilistic framework introduced in (Dayan and Hinton, 1997), which proposes to use expectation-maximization (EM) to directly optimize the probability of preferred outcomes (as opposed to classic expected reward maximization). To obtain a practical algorithm, we identify and address a key limitation in current EM-based methods: when applied to preference optimization, they solely maximize the likelihood of preferred examples, while neglecting dis-preferred samples. We show how one can extend EM algorithms to explicitly incorporate dis-preferred outcomes, leading to a novel, theoretically grounded, preference optimization algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback.</li>
</ul>

<h3>Title: IV-Mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Shitong Shao, Zikai Zhou, Lichen Bai, Haoyi Xiond, Zeke Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04171">https://arxiv.org/abs/2410.04171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04171">https://arxiv.org/pdf/2410.04171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04171]] IV-Mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis(https://arxiv.org/abs/2410.04171)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The multi-step sampling mechanism, a key feature of visual diffusion models, has significant potential to replicate the success of OpenAI's Strawberry in enhancing performance by increasing the inference computational cost. Sufficient prior studies have demonstrated that correctly scaling up computation in the sampling process can successfully lead to improved generation quality, enhanced image editing, and compositional generalization. While there have been rapid advancements in developing inference-heavy algorithms for improved image generation, relatively little work has explored inference scaling laws in video diffusion models (VDMs). Furthermore, existing research shows only minimal performance gains that are perceptible to the naked eye. To address this, we design a novel training-free algorithm IV-Mixed Sampler that leverages the strengths of image diffusion models (IDMs) to assist VDMs surpass their current capabilities. The core of IV-Mixed Sampler is to use IDMs to significantly enhance the quality of each video frame and VDMs ensure the temporal coherence of the video during the sampling process. Our experiments have demonstrated that IV-Mixed Sampler achieves state-of-the-art performance on 4 benchmarks including UCF-101-FVD, MSR-VTT-FVD, Chronomagic-Bench-150, and Chronomagic-Bench-1649. For example, the open-source Animatediff with IV-Mixed Sampler reduces the UMT-FVD score from 275.2 to 228.6, closing to 223.1 from the closed-source Pika-2.0.</li>
</ul>

<h3>Title: Accelerating Diffusion Models with One-to-Many Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Linfeng Zhang, Kaisheng Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04191">https://arxiv.org/abs/2410.04191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04191">https://arxiv.org/pdf/2410.04191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04191]] Accelerating Diffusion Models with One-to-Many Knowledge Distillation(https://arxiv.org/abs/2410.04191)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Significant advancements in image generation have been made with diffusion models. Nevertheless, when contrasted with previous generative models, diffusion models face substantial computational overhead, leading to failure in real-time generation. Recent approaches have aimed to accelerate diffusion models by reducing the number of sampling steps through improved sampling techniques or step distillation. However, the methods to diminish the computational cost for each timestep remain a relatively unexplored area. Observing the fact that diffusion models exhibit varying input distributions and feature distributions at different timesteps, we introduce one-to-many knowledge distillation (O2MKD), which distills a single teacher diffusion model into multiple student diffusion models, where each student diffusion model is trained to learn the teacher's knowledge for a subset of continuous timesteps. Experiments on CIFAR10, LSUN Church, CelebA-HQ with DDPM and COCO30K with Stable Diffusion show that O2MKD can be applied to previous knowledge distillation and fast sampling methods to achieve significant acceleration. Codes will be released in Github.</li>
</ul>

<h3>Title: Learning on LoRAs: GL-Equivariant Processing of Low-Rank Weight Spaces for Large Finetuned Models</h3>
<ul>
<li><strong>Authors: </strong>Theo (Moe)Putterman, Derek Lim, Yoav Gelberg, Stefanie Jegelka, Haggai Maron</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04207">https://arxiv.org/abs/2410.04207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04207">https://arxiv.org/pdf/2410.04207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04207]] Learning on LoRAs: GL-Equivariant Processing of Low-Rank Weight Spaces for Large Finetuned Models(https://arxiv.org/abs/2410.04207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Low-rank adaptations (LoRAs) have revolutionized the finetuning of large foundation models, enabling efficient adaptation even with limited computational resources. The resulting proliferation of LoRAs presents exciting opportunities for applying machine learning techniques that take these low-rank weights themselves as inputs. In this paper, we investigate the potential of Learning on LoRAs (LoL), a paradigm where LoRA weights serve as input to machine learning models. For instance, an LoL model that takes in LoRA weights as inputs could predict the performance of the finetuned model on downstream tasks, detect potentially harmful finetunes, or even generate novel model edits without traditional training methods. We first identify the inherent parameter symmetries of low rank decompositions of weights, which differ significantly from the parameter symmetries of standard neural networks. To efficiently process LoRA weights, we develop several symmetry-aware invariant or equivariant LoL models, using tools such as canonicalization, invariant featurization, and equivariant layers. We finetune thousands of text-to-image diffusion models and language models to collect datasets of LoRAs. In numerical experiments on these datasets, we show that our LoL architectures are capable of processing low rank weight decompositions to predict CLIP score, finetuning data attributes, finetuning data membership, and accuracy on downstream tasks.</li>
</ul>

<h3>Title: TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Liu, Xingchao Yang, Tomoya Akiyama, Yuantian Huang, Qiaoge Li, Shigeru Kuriyama, Takafumi Taketomi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04221">https://arxiv.org/abs/2410.04221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04221">https://arxiv.org/pdf/2410.04221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04221]] TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation(https://arxiv.org/abs/2410.04221)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present TANGO, a framework for generating co-speech body-gesture videos. Given a few-minute, single-speaker reference video and target speech audio, TANGO produces high-fidelity videos with synchronized body gestures. TANGO builds on Gesture Video Reenactment (GVR), which splits and retrieves video clips using a directed graph structure - representing video frames as nodes and valid transitions as edges. We address two key limitations of GVR: audio-motion misalignment and visual artifacts in GAN-generated transition frames. In particular, (i) we propose retrieving gestures using latent feature distance to improve cross-modal alignment. To ensure the latent features could effectively model the relationship between speech audio and gesture motion, we implement a hierarchical joint embedding space (AuMoCLIP); (ii) we introduce the diffusion-based model to generate high-quality transition frames. Our diffusion model, Appearance Consistent Interpolation (ACInterp), is built upon AnimateAnyone and includes a reference motion module and homography background flow to preserve appearance consistency between generated and reference videos. By integrating these components into the graph-based retrieval framework, TANGO reliably produces realistic, audio-synchronized videos and outperforms all existing generative and retrieval methods. Our codes and pretrained models are available: \url{this https URL}</li>
</ul>

<h3>Title: Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning</h3>
<ul>
<li><strong>Authors: </strong>Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, Jie Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04223">https://arxiv.org/abs/2410.04223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04223">https://arxiv.org/pdf/2410.04223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04223]] Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning(https://arxiv.org/abs/2410.04223)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.</li>
</ul>

<h3>Title: Distillation-Free One-Step Diffusion for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Jianze Li, Jiezhang Cao, Zichen Zou, Xiongfei Su, Xin Yuan, Yulun Zhang, Yong Guo, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04224">https://arxiv.org/abs/2410.04224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04224">https://arxiv.org/pdf/2410.04224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04224]] Distillation-Free One-Step Diffusion for Real-World Image Super-Resolution(https://arxiv.org/abs/2410.04224)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have been achieving excellent performance for real-world image super-resolution (Real-ISR) with considerable computational costs. Current approaches are trying to derive one-step diffusion models from multi-step counterparts through knowledge distillation. However, these methods incur substantial training costs and may constrain the performance of the student model by the teacher's limitations. To tackle these issues, we propose DFOSD, a Distillation-Free One-Step Diffusion model. Specifically, we propose a noise-aware discriminator (NAD) to participate in adversarial training, further enhancing the authenticity of the generated content. Additionally, we improve the perceptual loss with edge-aware DISTS (EA-DISTS) to enhance the model's ability to generate fine details. Our experiments demonstrate that, compared with previous diffusion-based methods requiring dozens or even hundreds of steps, our DFOSD attains comparable or even superior results in both quantitative metrics and qualitative evaluations. Our DFOSD also abtains higher performance and efficiency compared with other one-step diffusion methods. We will release code and models at \url{this https URL}.</li>
</ul>

<h3>Title: DeFoG: Discrete Flow Matching for Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiming Qin, Manuel Madeira, Dorina Thanou, Pascal Frossard</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04263">https://arxiv.org/abs/2410.04263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04263">https://arxiv.org/pdf/2410.04263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04263]] DeFoG: Discrete Flow Matching for Graph Generation(https://arxiv.org/abs/2410.04263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph generation is fundamental in diverse scientific applications, due to its ability to reveal the underlying distribution of complex data, and eventually generate new, realistic data points. Despite the success of diffusion models in this domain, those face limitations in sampling efficiency and flexibility, stemming from the tight coupling between the training and sampling stages. To address this, we propose DeFoG, a novel framework using discrete flow matching for graph generation. DeFoG employs a flow-based approach that features an efficient linear interpolation noising process and a flexible denoising process based on a continuous-time Markov chain formulation. We leverage an expressive graph transformer and ensure desirable node permutation properties to respect graph symmetry. Crucially, our framework enables a disentangled design of the training and sampling stages, enabling more effective and efficient optimization of model performance. We navigate this design space by introducing several algorithmic improvements that boost the model performance, consistently surpassing existing diffusion models. We also theoretically demonstrate that, for general discrete data, discrete flow models can faithfully replicate the ground truth distribution - a result that naturally extends to graph data and reinforces DeFoG's foundations. Extensive experiments show that DeFoG achieves state-of-the-art results on synthetic and molecular datasets, improving both training and sampling efficiency over diffusion models, and excels in conditional generation on a digital pathology dataset.</li>
</ul>

<h3>Title: Constructing Cloze Questions Generatively</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Sun (1), Jie Wang (2)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04266">https://arxiv.org/abs/2410.04266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04266">https://arxiv.org/pdf/2410.04266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04266]] Constructing Cloze Questions Generatively(https://arxiv.org/abs/2410.04266)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a generative method called CQG for constructing cloze questions from a given article using neural networks and WordNet, with an emphasis on generating multigram distractors. Built on sense disambiguation, text-to-text transformation, WordNet's synset taxonomies and lexical labels, CQG selects an answer key for a given sentence, segments it into a sequence of instances, generates instance-level distractor candidates (IDCs) using a transformer and sibling this http URL then removes inappropriate IDCs, ranks the remaining IDCs based on contextual embedding similarities, as well as synset and lexical relatedness, forms distractor candidates by combinatorially replacing instances with the corresponding top-ranked IDCs, and checks if they are legitimate phrases. Finally, it selects top-ranked distractor candidates based on contextual semantic similarities to the answer key. Experiments show that this method significantly outperforms SOTA results. Human judges also confirm the high qualities of the generated distractors.</li>
</ul>

<h3>Title: Mechanistic Behavior Editing of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Joykirat Singh, Subhabrata Dutta, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04277">https://arxiv.org/abs/2410.04277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04277">https://arxiv.org/pdf/2410.04277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04277]] Mechanistic Behavior Editing of Language Models(https://arxiv.org/abs/2410.04277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models trained on web-scale text acquire language generation abilities that can solve a wide range of tasks, particularly when task knowledge is refined into the generative prior using in-context examples. However, spurious features learned from noisy data hinder their generalizability. Supervised finetuning can introduce task specificity, but introduce data inefficiency. Prior studies indicate that (i) noisy neural circuitries coexist with generalizable ones within LLMs, and (ii) finetuning typically enhances (or suppresses) existing abilities without introducing newer ones. Building upon these, we propose TaRot, a novel method for task adaptation. TaRot intervenes in the neural circuitries using learnable rotation matrices that are optimized using Bayesian Optimization, on labelled samples in the order of standard few-shot prompting examples. Experiments on multiple classification and generation tasks using LLMs of varying sizes reveal the efficacy of TaRot, improving upon both zero- as well as few-shot performance, with average improvements (across models and tasks) of 23.81% and 11.15%, respectively. The source code is available at this https URL</li>
</ul>

<h3>Title: Self-Supervised Anomaly Detection in the Wild: Favor Joint Embeddings Methods</h3>
<ul>
<li><strong>Authors: </strong>Daniel Otero, Rafael Mateus, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04289">https://arxiv.org/abs/2410.04289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04289">https://arxiv.org/pdf/2410.04289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04289]] Self-Supervised Anomaly Detection in the Wild: Favor Joint Embeddings Methods(https://arxiv.org/abs/2410.04289)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Accurate anomaly detection is critical in vision-based infrastructure inspection, where it helps prevent costly failures and enhances safety. Self-Supervised Learning (SSL) offers a promising approach by learning robust representations from unlabeled data. However, its application in anomaly detection remains underexplored. This paper addresses this gap by providing a comprehensive evaluation of SSL methods for real-world anomaly detection, focusing on sewer infrastructure. Using the Sewer-ML dataset, we evaluate lightweight models such as ViT-Tiny and ResNet-18 across SSL frameworks, including BYOL, Barlow Twins, SimCLR, DINO, and MAE, under varying class imbalance levels. Through 250 experiments, we rigorously assess the performance of these SSL methods to ensure a robust and comprehensive evaluation. Our findings highlight the superiority of joint-embedding methods like SimCLR and Barlow Twins over reconstruction-based approaches such as MAE, which struggle to maintain performance under class imbalance. Furthermore, we find that the SSL model choice is more critical than the backbone architecture. Additionally, we emphasize the need for better label-free assessments of SSL representations, as current methods like RankMe fail to adequately evaluate representation quality, making cross-validation without labels infeasible. Despite the remaining performance gap between SSL and supervised models, these findings highlight the potential of SSL to enhance anomaly detection, paving the way for further research in this underexplored area of SSL applications.</li>
</ul>

<h3>Title: Test-Time Adaptation for Keypoint-Based Spacecraft Pose Estimation Based on Predicted-View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Juan Ignacio Bravo Pérez-Villar, Álvaro García-Martín, Jesús Bescós, Juan C. SanMiguel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04298">https://arxiv.org/abs/2410.04298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04298">https://arxiv.org/pdf/2410.04298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04298]] Test-Time Adaptation for Keypoint-Based Spacecraft Pose Estimation Based on Predicted-View Synthesis(https://arxiv.org/abs/2410.04298)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Due to the difficulty of replicating the real conditions during training, supervised algorithms for spacecraft pose estimation experience a drop in performance when trained on synthetic data and applied to real operational data. To address this issue, we propose a test-time adaptation approach that leverages the temporal redundancy between images acquired during close proximity operations. Our approach involves extracting features from sequential spacecraft images, estimating their poses, and then using this information to synthesise a reconstructed view. We establish a self-supervised learning objective by comparing the synthesised view with the actual one. During training, we supervise both pose estimation and image synthesis, while at test-time, we optimise the self-supervised objective. Additionally, we introduce a regularisation loss to prevent solutions that are not consistent with the keypoint structure of the spacecraft. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Inference Scaling for Long-Context Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04343">https://arxiv.org/abs/2410.04343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04343">https://arxiv.org/pdf/2410.04343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04343]] Inference Scaling for Long-Context Retrieval Augmented Generation(https://arxiv.org/abs/2410.04343)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.</li>
</ul>

<h3>Title: VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide</h3>
<ul>
<li><strong>Authors: </strong>Dohun Lee, Bryan S Kim, Geon Yeong Park, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04364">https://arxiv.org/abs/2410.04364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04364">https://arxiv.org/pdf/2410.04364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04364]] VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide(https://arxiv.org/abs/2410.04364)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: this http URL</li>
</ul>

<h3>Title: Algorithmic Capabilities of Random Transformers</h3>
<ul>
<li><strong>Authors: </strong>Ziqian Zhong, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04368">https://arxiv.org/abs/2410.04368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04368">https://arxiv.org/pdf/2410.04368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04368]] Algorithmic Capabilities of Random Transformers(https://arxiv.org/abs/2410.04368)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Trained transformer models have been found to implement interpretable procedures for tasks like arithmetic and associative recall, but little is understood about how the circuits that implement these procedures originate during training. To what extent do they depend on the supervisory signal provided to models, and to what extent are they attributable to behavior already present in models at the beginning of training? To investigate these questions, we investigate what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input--output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model. We find that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation. Our results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained. Code is available at this https URL.</li>
</ul>

<h3>Title: DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Ke Sun, Shen Chen, Taiping Yao, Hong Liu, Xiaoshuai Sun, Shouhong Ding, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04372">https://arxiv.org/abs/2410.04372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04372">https://arxiv.org/pdf/2410.04372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04372]] DiffusionFake: Enhancing Generalization in Deepfake Detection via Guided Stable Diffusion(https://arxiv.org/abs/2410.04372)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of Deepfake technology has made face swapping highly realistic, raising concerns about the malicious use of fabricated facial content. Existing methods often struggle to generalize to unseen domains due to the diverse nature of facial manipulations. In this paper, we revisit the generation process and identify a universal principle: Deepfake images inherently contain information from both source and target identities, while genuine faces maintain a consistent identity. Building upon this insight, we introduce DiffusionFake, a novel plug-and-play framework that reverses the generative process of face forgeries to enhance the generalization of detection models. DiffusionFake achieves this by injecting the features extracted by the detection model into a frozen pre-trained Stable Diffusion model, compelling it to reconstruct the corresponding target and source images. This guided reconstruction process constrains the detection network to capture the source and target related features to facilitate the reconstruction, thereby learning rich and disentangled representations that are more resilient to unseen forgeries. Extensive experiments demonstrate that DiffusionFake significantly improves cross-domain generalization of various detector architectures without introducing additional parameters during inference. Our Codes are available in this https URL.</li>
</ul>

<h3>Title: Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Li, Guohao Li, Zhibin Lan, Xue Xu, Wanru Zhuang, Jiachen Liu, Xinyan Xiao, Jinsong Su</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04439">https://arxiv.org/abs/2410.04439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04439">https://arxiv.org/pdf/2410.04439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04439]] Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training(https://arxiv.org/abs/2410.04439)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image models have demonstrated impressive achievements in diversity and aesthetics but struggle to generate images with legible visual texts. Existing backbone models have limitations such as misspelling, failing to generate texts, and lack of support for Chinese text, but their development shows promising potential. In this paper, we propose a series of methods, aiming to empower backbone models to generate visual texts in English and Chinese. We first conduct a preliminary study revealing that Byte Pair Encoding (BPE) tokenization and the insufficient learning of cross-attention modules restrict the performance of the backbone models. Based on these observations, we make the following improvements: (1) We design a mixed granularity input strategy to provide more suitable text representations; (2) We propose to augment the conventional training objective with three glyph-aware training losses, which enhance the learning of cross-attention modules and encourage the model to focus on visual texts. Through experiments, we demonstrate that our methods can effectively empower backbone models to generate semantic relevant, aesthetically appealing, and accurate visual text images, while maintaining their fundamental image generation quality.</li>
</ul>

<h3>Title: Attention Shift: Steering AI Away from Unsafe Content</h3>
<ul>
<li><strong>Authors: </strong>Shivank Garg, Manyana Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04447">https://arxiv.org/abs/2410.04447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04447">https://arxiv.org/pdf/2410.04447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04447]] Attention Shift: Steering AI Away from Unsafe Content(https://arxiv.org/abs/2410.04447)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study investigates the generation of unsafe or harmful content in state-of-the-art generative models, focusing on methods for restricting such generations. We introduce a novel training-free approach using attention reweighing to remove unsafe concepts without additional training during inference. We compare our method against existing ablation methods, evaluating the performance on both, direct and adversarial jailbreak prompts, using qualitative and quantitative metrics. We hypothesize potential reasons for the observed results and discuss the limitations and broader implications of content restriction.</li>
</ul>

<h3>Title: Video Summarization Techniques: A Comprehensive Review</h3>
<ul>
<li><strong>Authors: </strong>Toqa Alaa, Ahmad Mongy, Assem Bakr, Mariam Diab, Walid Gomaa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04449">https://arxiv.org/abs/2410.04449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04449">https://arxiv.org/pdf/2410.04449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04449]] Video Summarization Techniques: A Comprehensive Review(https://arxiv.org/abs/2410.04449)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid expansion of video content across a variety of industries, including social media, education, entertainment, and surveillance, has made video summarization an essential field of study. The current work is a survey that explores the various approaches and methods created for video summarizing, emphasizing both abstractive and extractive strategies. The process of extractive summarization involves the identification of key frames or segments from the source video, utilizing methods such as shot boundary recognition, and clustering. On the other hand, abstractive summarization creates new content by getting the essential content from the video, using machine learning models like deep neural networks and natural language processing, reinforcement learning, attention mechanisms, generative adversarial networks, and multi-modal learning. We also include approaches that incorporate the two methodologies, along with discussing the uses and difficulties encountered in real-world implementations. The paper also covers the datasets used to benchmark these techniques. This review attempts to provide a state-of-the-art thorough knowledge of the current state and future directions of video summarization research.</li>
</ul>

<h3>Title: Revisiting In-context Learning Inference Circuit in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hakaze Cho, Mariko Kato, Yoshihiro Sakai, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04468">https://arxiv.org/abs/2410.04468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04468">https://arxiv.org/pdf/2410.04468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04468]] Revisiting In-context Learning Inference Circuit in Large Language Models(https://arxiv.org/abs/2410.04468)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Summarize: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit.</li>
</ul>

<h3>Title: MECFormer: Multi-task Whole Slide Image Classification with Expert Consultation Network</h3>
<ul>
<li><strong>Authors: </strong>Doanh C. Bui, Jin Tae Kwak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04507">https://arxiv.org/abs/2410.04507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04507">https://arxiv.org/pdf/2410.04507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04507]] MECFormer: Multi-task Whole Slide Image Classification with Expert Consultation Network(https://arxiv.org/abs/2410.04507)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Whole slide image (WSI) classification is a crucial problem for cancer diagnostics in clinics and hospitals. A WSI, acquired at gigapixel size, is commonly tiled into patches and processed by multiple-instance learning (MIL) models. Previous MIL-based models designed for this problem have only been evaluated on individual tasks for specific organs, and the ability to handle multiple tasks within a single model has not been investigated. In this study, we propose MECFormer, a generative Transformer-based model designed to handle multiple tasks within one model. To leverage the power of learning multiple tasks simultaneously and to enhance the model's effectiveness in focusing on each individual task, we introduce an Expert Consultation Network, a projection layer placed at the beginning of the Transformer-based model. Additionally, to enable flexible classification, autoregressive decoding is incorporated by a language decoder for WSI classification. Through extensive experiments on five datasets involving four different organs, one cancer classification task, and four cancer subtyping tasks, MECFormer demonstrates superior performance compared to individual state-of-the-art multiple-instance learning models.</li>
</ul>

<h3>Title: Pullback Flow Matching on Data Manifolds</h3>
<ul>
<li><strong>Authors: </strong>Friso de Kruiff, Erik Bekkers, Ozan Öktem, Carola-Bibiane Schönlieb, Willem Diepeveen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04543">https://arxiv.org/abs/2410.04543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04543">https://arxiv.org/pdf/2410.04543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04543]] Pullback Flow Matching on Data Manifolds(https://arxiv.org/abs/2410.04543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose Pullback Flow Matching (PFM), a novel framework for generative modeling on data manifolds. Unlike existing methods that assume or learn restrictive closed-form manifold mappings for training Riemannian Flow Matching (RFM) models, PFM leverages pullback geometry and isometric learning to preserve the underlying manifold's geometry while enabling efficient generation and precise interpolation in latent space. This approach not only facilitates closed-form mappings on the data manifold but also allows for designable latent spaces, using assumed metrics on both data and latent manifolds. By enhancing isometric learning through Neural ODEs and proposing a scalable training objective, we achieve a latent space more suitable for interpolation, leading to improved manifold learning and generative performance. We demonstrate PFM's effectiveness through applications in synthetic data, protein dynamics and protein sequence data, generating novel proteins with specific properties. This method shows strong potential for drug discovery and materials science, where generating novel samples with specific properties is of great interest.</li>
</ul>

<h3>Title: How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyan Li, Chen Liang, Jing Peng, Ming Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04545">https://arxiv.org/abs/2410.04545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04545">https://arxiv.org/pdf/2410.04545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04545]] How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?(https://arxiv.org/abs/2410.04545)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative AI technologies like large language models have boosted the incorporation of AI assistance in writing workflows, leading to the rise of a new paradigm of human-AI co-creation in writing. To understand how people perceive writings that are produced under this paradigm, in this paper, we conduct an experimental study to understand whether and how the disclosure of the level and type of AI assistance in the writing process would affect people's perceptions of the writing on various aspects, including their evaluation on the quality of the writing and their ranking of different writings. Our results suggest that disclosing the AI assistance in the writing process, especially if AI has provided assistance in generating new content, decreases the average quality ratings for both argumentative essays and creative stories. This decrease in the average quality ratings often comes with an increased level of variations in different individuals' quality evaluations of the same writing. Indeed, factors such as an individual's writing confidence and familiarity with AI writing assistants are shown to moderate the impact of AI assistance disclosure on their writing quality evaluations. We also find that disclosing the use of AI assistance may significantly reduce the proportion of writings produced with AI's content generation assistance among the top-ranked writings.</li>
</ul>

<h3>Title: $\texttt{dattri}$: A Library for Efficient Data Attribution</h3>
<ul>
<li><strong>Authors: </strong>Junwei Deng, Ting-Wei Li, Shiyuan Zhang, Shixuan Liu, Yijun Pan, Hao Huang, Xinhe Wang, Pingbang Hu, Xingjian Zhang, Jiaqi W. Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04555">https://arxiv.org/abs/2410.04555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04555">https://arxiv.org/pdf/2410.04555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04555]] $\texttt{dattri}$: A Library for Efficient Data Attribution(https://arxiv.org/abs/2410.04555)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data attribution methods aim to quantify the influence of individual training samples on the prediction of artificial intelligence (AI) models. As training data plays an increasingly crucial role in the modern development of large-scale AI models, data attribution has found broad applications in improving AI performance and safety. However, despite a surge of new data attribution methods being developed recently, there lacks a comprehensive library that facilitates the development, benchmarking, and deployment of different data attribution methods. In this work, we introduce $\texttt{dattri}$, an open-source data attribution library that addresses the above needs. Specifically, $\texttt{dattri}$ highlights three novel design features. Firstly, $\texttt{dattri}$ proposes a unified and easy-to-use API, allowing users to integrate different data attribution methods into their PyTorch-based machine learning pipeline with a few lines of code changed. Secondly, $\texttt{dattri}$ modularizes low-level utility functions that are commonly used in data attribution methods, such as Hessian-vector product, inverse-Hessian-vector product or random projection, making it easier for researchers to develop new data attribution methods. Thirdly, $\texttt{dattri}$ provides a comprehensive benchmark framework with pre-trained models and ground truth annotations for a variety of benchmark settings, including generative AI settings. We have implemented a variety of state-of-the-art efficient data attribution methods that can be applied to large-scale neural network models, and will continuously update the library in the future. Using the developed $\texttt{dattri}$ library, we are able to perform a comprehensive and fair benchmark analysis across a wide range of data attribution methods. The source code of $\texttt{dattri}$ is available at this https URL.</li>
</ul>

<h3>Title: GAMformer: In-Context Learning for Generalized Additive Models</h3>
<ul>
<li><strong>Authors: </strong>Andreas Mueller, Julien Siems, Harsha Nori, David Salinas, Arber Zela, Rich Caruana, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04560">https://arxiv.org/abs/2410.04560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04560">https://arxiv.org/pdf/2410.04560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04560]] GAMformer: In-Context Learning for Generalized Additive Models(https://arxiv.org/abs/2410.04560)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Generalized Additive Models (GAMs) are widely recognized for their ability to create fully interpretable machine learning models for tabular data. Traditionally, training GAMs involves iterative learning algorithms, such as splines, boosted trees, or neural networks, which refine the additive components through repeated error reduction. In this paper, we introduce GAMformer, the first method to leverage in-context learning to estimate shape functions of a GAM in a single forward pass, representing a significant departure from the conventional iterative approaches to GAM fitting. Building on previous research applying in-context learning to tabular data, we exclusively use complex, synthetic data to train GAMformer, yet find it extrapolates well to real-world data. Our experiments show that GAMformer performs on par with other leading GAMs across various classification benchmarks while generating highly interpretable shape functions.</li>
</ul>

<h3>Title: EnsemW2S: Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?</h3>
<ul>
<li><strong>Authors: </strong>Aakriti Agrawal, Mucong Ding, Zora Che, Chenghao Deng, Anirudh Satheesh, John Langford, Furong Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04571">https://arxiv.org/abs/2410.04571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04571">https://arxiv.org/pdf/2410.04571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04571]] EnsemW2S: Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?(https://arxiv.org/abs/2410.04571)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>How can we harness the collective capabilities of multiple Large Language Models (LLMs) to create an even more powerful model? This question forms the foundation of our research, where we propose an innovative approach to weak-to-strong (w2s) generalization-a critical problem in AI alignment. Our work introduces an easy-to-hard (e2h) framework for studying the feasibility of w2s generalization, where weak models trained on simpler tasks collaboratively supervise stronger models on more complex tasks. This setup mirrors real-world challenges, where direct human supervision is limited. To achieve this, we develop a novel AdaBoost-inspired ensemble method, demonstrating that an ensemble of weak supervisors can enhance the performance of stronger LLMs across classification and generative tasks on difficult QA datasets. In several cases, our ensemble approach matches the performance of models trained on ground-truth data, establishing a new benchmark for w2s generalization. We observe an improvement of up to 14% over existing baselines and average improvements of 5% and 4% for binary classification and generative tasks, respectively. This research points to a promising direction for enhancing AI through collective supervision, especially in scenarios where labeled data is sparse or insufficient.</li>
</ul>

<h3>Title: Hammer: Robust Function-Calling for On-Device Language Models via Function Masking</h3>
<ul>
<li><strong>Authors: </strong>Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, Weinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04587">https://arxiv.org/abs/2410.04587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04587">https://arxiv.org/pdf/2410.04587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04587]] Hammer: Robust Function-Calling for On-Device Language Models via Function Masking(https://arxiv.org/abs/2410.04587)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function calling capabilities. This paper identifies a critical gap in existing function calling models, where performance varies significantly across benchmarks, often due to being misled by specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models' sensitivity to irrelevant functions and incorporates function masking techniques to minimize misleading. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving sota results. Our open source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function calling performance.</li>
</ul>

<h3>Title: Towards Unsupervised Blind Face Restoration using Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Tianshu Kuai, Sina Honari, Igor Gilitschenski, Alex Levinshtein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04618">https://arxiv.org/abs/2410.04618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04618">https://arxiv.org/pdf/2410.04618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04618]] Towards Unsupervised Blind Face Restoration using Diffusion Prior(https://arxiv.org/abs/2410.04618)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Blind face restoration methods have shown remarkable performance, particularly when trained on large-scale synthetic datasets with supervised learning. These datasets are often generated by simulating low-quality face images with a handcrafted image degradation pipeline. The models trained on such synthetic degradations, however, cannot deal with inputs of unseen degradations. In this paper, we address this issue by using only a set of input images, with unknown degradations and without ground truth targets, to fine-tune a restoration model that learns to map them to clean and contextually consistent outputs. We utilize a pre-trained diffusion model as a generative prior through which we generate high quality images from the natural image distribution while maintaining the input image content through consistency constraints. These generated images are then used as pseudo targets to fine-tune a pre-trained restoration model. Unlike many recent approaches that employ diffusion models at test time, we only do so during training and thus maintain an efficient inference-time performance. Extensive experiments show that the proposed approach can consistently improve the perceptual quality of pre-trained blind face restoration models while maintaining great consistency with the input contents. Our best model also achieves the state-of-the-art results on both synthetic and real-world datasets.</li>
</ul>

<h3>Title: AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional Semantic Diffusion for Retinal Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Dewei Hu, Hao Li, Han Liu, Jiacheng Wang, Xing Yao, Daiwei Lu, Ipek Oguz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04648">https://arxiv.org/abs/2410.04648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04648">https://arxiv.org/pdf/2410.04648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04648]] AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional Semantic Diffusion for Retinal Vessel Segmentation(https://arxiv.org/abs/2410.04648)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep learning has shown remarkable performance in medical image segmentation. However, despite its promise, deep learning has many challenges in practice due to its inability to effectively transition to unseen domains, caused by the inherent data distribution shift and the lack of manual annotations to guide domain adaptation. To tackle this problem, we present an unsupervised domain adaptation (UDA) method named AdaptDiff that enables a retinal vessel segmentation network trained on fundus photography (FP) to produce satisfactory results on unseen modalities (e.g., OCT-A) without any manual labels. For all our target domains, we first adopt a segmentation model trained on the source domain to create pseudo-labels. With these pseudo-labels, we train a conditional semantic diffusion probabilistic model to represent the target domain distribution. Experimentally, we show that even with low quality pseudo-labels, the diffusion model can still capture the conditional semantic information. Subsequently, we sample on the target domain with binary vessel masks from the source domain to get paired data, i.e., target domain synthetic images conditioned on the binary vessel map. Finally, we fine-tune the pre-trained segmentation network using the synthetic paired data to mitigate the domain gap. We assess the effectiveness of AdaptDiff on seven publicly available datasets across three distinct modalities. Our results demonstrate a significant improvement in segmentation performance across all unseen datasets. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains</h3>
<ul>
<li><strong>Authors: </strong>Shane E. Loeffler, Zan Ahmad, Syed Yusuf Ali, Carolyna Yamamoto, Dan M. Popescu, Alana Yee, Yash Lal, Natalia Trayanova, Mauro Maggioni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.SP, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04655">https://arxiv.org/abs/2410.04655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04655">https://arxiv.org/pdf/2410.04655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04655]] Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains(https://arxiv.org/abs/2410.04655)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Predicting time-dependent dynamics of complex systems governed by non-linear partial differential equations (PDEs) with varying parameters and domains is a challenging task motivated by applications across various fields. We introduce a novel family of neural operators based on our Graph Fourier Neural Kernels, designed to learn solution generators for nonlinear PDEs in which the highest-order term is diffusive, across multiple domains and parameters. G-FuNK combines components that are parameter- and domain-adapted with others that are not. The domain-adapted components are constructed using a weighted graph on the discretized domain, where the graph Laplacian approximates the highest-order diffusive term, ensuring boundary condition compliance and capturing the parameter and domain-specific behavior. Meanwhile, the learned components transfer across domains and parameters via Fourier Neural Operators. This approach naturally embeds geometric and directional information, improving generalization to new test domains without need for retraining the network. To handle temporal dynamics, our method incorporates an integrated ODE solver to predict the evolution of the system. Experiments show G-FuNK's capability to accurately approximate heat, reaction diffusion, and cardiac electrophysiology equations across various geometries and anisotropic diffusivity fields. G-FuNK achieves low relative errors on unseen domains and fiber fields, significantly accelerating predictions compared to traditional finite-element solvers.</li>
</ul>

<h3>Title: Federated Learning Nodes Can Reconstruct Peers' Image Data</h3>
<ul>
<li><strong>Authors: </strong>Ethan Wilson, Kai Yue, Chau-Wai Wong, Huaiyu Dai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04661">https://arxiv.org/abs/2410.04661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04661">https://arxiv.org/pdf/2410.04661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04661]] Federated Learning Nodes Can Reconstruct Peers' Image Data(https://arxiv.org/abs/2410.04661)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a privacy-preserving machine learning framework that enables multiple nodes to train models on their local data and periodically average weight updates to benefit from other nodes' training. Each node's goal is to collaborate with other nodes to improve the model's performance while keeping its training data private. However, this framework does not guarantee data privacy. Prior work has shown that the gradient-sharing steps in FL can be vulnerable to data reconstruction attacks from an honest-but-curious central server. In this work, we show that an honest-but-curious node/client can also launch attacks to reconstruct peers' image data in a centralized system, presenting a severe privacy risk. We demonstrate that a single client can silently reconstruct other clients' private images using diluted information available within consecutive updates. We leverage state-of-the-art diffusion models to enhance the perceptual quality and recognizability of the reconstructed images, further demonstrating the risk of information leakage at a semantic level. This highlights the need for more robust privacy-preserving mechanisms that protect against silent client-side attacks during federated training.</li>
</ul>

<h3>Title: CAR: Controllable Autoregressive Modeling for Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Yao, Jialin Li, Yifeng Zhou, Yong Liu, Xi Jiang, Chengjie Wang, Feng Zheng, Yuexian Zou, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04671">https://arxiv.org/abs/2410.04671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04671">https://arxiv.org/pdf/2410.04671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04671]] CAR: Controllable Autoregressive Modeling for Visual Generation(https://arxiv.org/abs/2410.04671)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Controllable generation, which enables fine-grained control over generated outputs, has emerged as a critical focus in visual generative models. Currently, there are two primary technical approaches in visual generation: diffusion models and autoregressive models. Diffusion models, as exemplified by ControlNet and T2I-Adapter, offer advanced control mechanisms, whereas autoregressive models, despite showcasing impressive generative quality and scalability, remain underexplored in terms of controllability and flexibility. In this study, we introduce Controllable AutoRegressive Modeling (CAR), a novel, plug-and-play framework that integrates conditional control into multi-scale latent variable modeling, enabling efficient control generation within a pre-trained visual autoregressive model. CAR progressively refines and captures control representations, which are injected into each autoregressive step of the pre-trained model to guide the generation process. Our approach demonstrates excellent controllability across various types of conditions and delivers higher image quality compared to previous methods. Additionally, CAR achieves robust generalization with significantly fewer training resources compared to those required for pre-training the model. To the best of our knowledge, we are the first to propose a control framework for pre-trained autoregressive visual generation models.</li>
</ul>

<h3>Title: Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Yin, Xuzheng He, Luoao Deng, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04691">https://arxiv.org/abs/2410.04691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04691">https://arxiv.org/pdf/2410.04691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04691]] Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning(https://arxiv.org/abs/2410.04691)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Fine-tuning and in-context learning (ICL) are two prevalent methods in imbuing large language models with task-specific knowledge. It is commonly believed that fine-tuning can surpass ICL given sufficient training samples as it allows the model to adjust its internal parameters based on the data. However, this paper presents a counterintuitive finding: For tasks with implicit patterns, ICL captures these patterns significantly better than fine-tuning. We developed several datasets featuring implicit patterns, such as sequences determining answers through parity or identifying reducible terms in calculations. We then evaluated the models' understanding of these patterns under both fine-tuning and ICL across models ranging from 0.5B to 7B parameters. The results indicate that models employing ICL can quickly grasp deep patterns and significantly improve accuracy. In contrast, fine-tuning, despite utilizing thousands of times more training samples than ICL, achieved only limited improvements. We also proposed circuit shift theory from a mechanistic interpretability's view to explain why ICL wins.</li>
</ul>

<h3>Title: Neural Fourier Modelling: A Highly Compact Approach to Time-Series Analysis</h3>
<ul>
<li><strong>Authors: </strong>Minjung Kim, Yusuke Hioka, Michael Witbrock</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04703">https://arxiv.org/abs/2410.04703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04703">https://arxiv.org/pdf/2410.04703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04703]] Neural Fourier Modelling: A Highly Compact Approach to Time-Series Analysis(https://arxiv.org/abs/2410.04703)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Neural time-series analysis has traditionally focused on modeling data in the time domain, often with some approaches incorporating equivalent Fourier domain representations as auxiliary spectral features. In this work, we shift the main focus to frequency representations, modeling time-series data fully and directly in the Fourier domain. We introduce Neural Fourier Modelling (NFM), a compact yet powerful solution for time-series analysis. NFM is grounded in two key properties of the Fourier transform (FT): (i) the ability to model finite-length time series as functions in the Fourier domain, treating them as continuous-time elements in function space, and (ii) the capacity for data manipulation (such as resampling and timespan extension) within the Fourier domain. We reinterpret Fourier-domain data manipulation as frequency extrapolation and interpolation, incorporating this as a core learning mechanism in NFM, applicable across various tasks. To support flexible frequency extension with spectral priors and effective modulation of frequency representations, we propose two learning modules: Learnable Frequency Tokens (LFT) and Implicit Neural Fourier Filters (INFF). These modules enable compact and expressive modeling in the Fourier domain. Extensive experiments demonstrate that NFM achieves state-of-the-art performance on a wide range of tasks (forecasting, anomaly detection, and classification), including challenging time-series scenarios with previously unseen sampling rates at test time. Moreover, NFM is highly compact, requiring fewer than 40K parameters in each task, with time-series lengths ranging from 100 to 16K.</li>
</ul>

<h3>Title: ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction</h3>
<ul>
<li><strong>Authors: </strong>Hyungjin Chung, Dohun Lee, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04721">https://arxiv.org/abs/2410.04721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04721">https://arxiv.org/pdf/2410.04721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04721]] ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction(https://arxiv.org/abs/2410.04721)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive models (ARMs) and diffusion models (DMs) represent two leading paradigms in generative modeling, each excelling in distinct areas: ARMs in global context modeling and long-sequence generation, and DMs in generating high-quality local contexts, especially for continuous data such as images and short videos. However, ARMs often suffer from exponential error accumulation over long sequences, leading to physically implausible results, while DMs are limited by their local context generation capabilities. In this work, we introduce Autoregressive Coherent multimodal generation with Diffusion Correction (ACDC), a zero-shot approach that combines the strengths of both ARMs and DMs at the inference stage without the need for additional fine-tuning. ACDC leverages ARMs for global context generation and memory-conditioned DMs for local correction, ensuring high-quality outputs by correcting artifacts in generated multimodal tokens. In particular, we propose a memory module based on large language models (LLMs) that dynamically adjusts the conditioning texts for the DMs, preserving crucial global context information. Our experiments on multimodal tasks, including coherent multi-frame story generation and autoregressive video generation, demonstrate that ACDC effectively mitigates the accumulation of errors and significantly enhances the quality of generated outputs, achieving superior performance while remaining agnostic to specific ARM and DM architectures. Project page: this https URL</li>
</ul>

<h3>Title: Diffusion Models in 3D Vision: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhen Wang, Dongyuan Li, Renhe Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04738">https://arxiv.org/abs/2410.04738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04738">https://arxiv.org/pdf/2410.04738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04738]] Diffusion Models in 3D Vision: A Survey(https://arxiv.org/abs/2410.04738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, 3D vision has become a crucial field within computer vision, powering a wide range of applications such as autonomous driving, robotics, augmented reality (AR), and medical imaging. This field relies on the accurate perception, understanding, and reconstruction of 3D scenes from 2D data sources like images and videos. Diffusion models, originally designed for 2D generative tasks, offer the potential for more flexible, probabilistic approaches that can better capture the variability and uncertainty present in real-world 3D data. However, traditional methods often struggle with efficiency and scalability. In this paper, we review the state-of-the-art approaches that leverage diffusion models for 3D visual tasks, including but not limited to 3D object generation, shape completion, point cloud reconstruction, and scene understanding. We provide an in-depth discussion of the underlying mathematical principles of diffusion models, outlining their forward and reverse processes, as well as the various architectural advancements that enable these models to work with 3D datasets. We also discuss the key challenges in applying diffusion models to 3D vision, such as handling occlusions and varying point densities, and the computational demands of high-dimensional data. Finally, we discuss potential solutions, including improving computational efficiency, enhancing multimodal fusion, and exploring the use of large-scale pretraining for better generalization across 3D tasks. This paper serves as a foundation for future exploration and development in this rapidly evolving field.</li>
</ul>

<h3>Title: Double Oracle Neural Architecture Search for Game Theoretic Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Aye Phyu Phyu Aung, Xinrun Wang, Ruiyu Wang, Hau Chan, Bo An, Xiaoli Li, J. Senthilnath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04764">https://arxiv.org/abs/2410.04764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04764">https://arxiv.org/pdf/2410.04764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04764]] Double Oracle Neural Architecture Search for Game Theoretic Deep Learning Models(https://arxiv.org/abs/2410.04764)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new approach to train deep learning models using game theory concepts including Generative Adversarial Networks (GANs) and Adversarial Training (AT) where we deploy a double-oracle framework using best response oracles. GAN is essentially a two-player zero-sum game between the generator and the discriminator. The same concept can be applied to AT with attacker and classifier as players. Training these models is challenging as a pure Nash equilibrium may not exist and even finding the mixed Nash equilibrium is difficult as training algorithms for both GAN and AT have a large-scale strategy space. Extending our preliminary model DO-GAN, we propose the methods to apply the double oracle framework concept to Adversarial Neural Architecture Search (NAS for GAN) and Adversarial Training (NAS for AT) algorithms. We first generalize the players' strategies as the trained models of generator and discriminator from the best response oracles. We then compute the meta-strategies using a linear program. For scalability of the framework where multiple network models of best responses are stored in the memory, we prune the weakly-dominated players' strategies to keep the oracles from becoming intractable. Finally, we conduct experiments on MNIST, CIFAR-10 and TinyImageNet for DONAS-GAN. We also evaluate the robustness under FGSM and PGD attacks on CIFAR-10, SVHN and TinyImageNet for DONAS-AT. We show that all our variants have significant improvements in both subjective qualitative evaluation and quantitative metrics, compared with their respective base architectures.</li>
</ul>

<h3>Title: Transforming Color: A Novel Image Colorization Method</h3>
<ul>
<li><strong>Authors: </strong>Hamza Shafiq, Bumshik Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04799">https://arxiv.org/abs/2410.04799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04799">https://arxiv.org/pdf/2410.04799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04799]] Transforming Color: A Novel Image Colorization Method(https://arxiv.org/abs/2410.04799)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel method for image colorization that utilizes a color transformer and generative adversarial networks (GANs) to address the challenge of generating visually appealing colorized images. Conventional approaches often struggle with capturing long-range dependencies and producing realistic colorizations. The proposed method integrates a transformer architecture to capture global information and a GAN framework to improve visual quality. In this study, a color encoder that utilizes a random normal distribution to generate color features is applied. These features are then integrated with grayscale image features to enhance the overall representation of the images. Our method demonstrates superior performance compared with existing approaches by utilizing the capacity of the transformer, which can capture long-range dependencies and generate a realistic colorization of the GAN. Experimental results show that the proposed network significantly outperforms other state-of-the-art colorization techniques, highlighting its potential for image colorization. This research opens new possibilities for precise and visually compelling image colorization in domains such as digital restoration and historical image analysis.</li>
</ul>

<h3>Title: Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention Engineering</h3>
<ul>
<li><strong>Authors: </strong>Kazumoto Nakamura, Yuji Nozawa, Yu-Chieh Lin, Kengo Nakata, Youyang Ng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04801">https://arxiv.org/abs/2410.04801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04801">https://arxiv.org/pdf/2410.04801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04801]] Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention Engineering(https://arxiv.org/abs/2410.04801)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The goal of this paper is to improve the performance of pretrained Vision Transformer (ViT) models, particularly DINOv2, in image clustering task without requiring re-training or fine-tuning. As model size increases, high-norm artifacts anomaly appears in the patches of multi-head attention. We observe that this anomaly leads to reduced accuracy in zero-shot image clustering. These artifacts are characterized by disproportionately large values in the attention map compared to other patch tokens. To address these artifacts, we propose an approach called Inference-Time Attention Engineering (ITAE), which manipulates attention function during inference. Specifically, we identify the artifacts by investigating one of the Query-Key-Value (QKV) patches in the multi-head attention and attenuate their corresponding attention values inside the pretrained models. ITAE shows improved clustering accuracy on multiple datasets by exhibiting more expressive features in latent space. Our findings highlight the potential of ITAE as a practical solution for reducing artifacts in pretrained ViT models and improving model performance in clustering tasks without the need for re-training or fine-tuning.</li>
</ul>

<h3>Title: Timer-XL: Long-Context Transformers for Unified Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04803">https://arxiv.org/abs/2410.04803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04803">https://arxiv.org/pdf/2410.04803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04803]] Timer-XL: Long-Context Transformers for Unified Time Series Forecasting(https://arxiv.org/abs/2410.04803)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Timer-XL, a generative Transformer for unified time series forecasting. To uniformly predict 1D and 2D time series, we generalize next token prediction, predominantly adopted for causal generation of 1D sequences, to multivariate next token prediction. The proposed paradigm uniformly formulates various forecasting scenarios as a long-context generation problem. We opt for the generative Transformer, which can capture global-range and causal dependencies while providing contextual flexibility, to implement unified forecasting on univariate series characterized by non-stationarity, multivariate time series with complicated dynamics and correlations, and covariate-informed contexts that include both endogenous and exogenous variables. Technically, we propose a universal TimeAttention to facilitate generative Transformers on time series, which can effectively capture fine-grained intra- and inter-series dependencies of flattened time series tokens (patches) and is further strengthened by position embeddings in both temporal and variable dimensions. Timer-XL achieves state-of-the-art performance across challenging forecasting benchmarks through a unified approach. As a large time series model, it demonstrates notable model transferability by large-scale pre-training, as well as contextual flexibility in token lengths, positioning it as a one-for-all forecaster.</li>
</ul>

<h3>Title: FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haokun Chen, Hang Li, Yao Zhang, Gengyuan Zhang, Jinhe Bi, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.DC, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04810">https://arxiv.org/abs/2410.04810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04810">https://arxiv.org/pdf/2410.04810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04810]] FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models(https://arxiv.org/abs/2410.04810)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>One-Shot Federated Learning (OSFL), a special decentralized machine learning paradigm, has recently gained significant attention. OSFL requires only a single round of client data or model upload, which reduces communication costs and mitigates privacy threats compared to traditional FL. Despite these promising prospects, existing methods face challenges due to client data heterogeneity and limited data quantity when applied to real-world OSFL systems. Recently, Latent Diffusion Models (LDM) have shown remarkable advancements in synthesizing high-quality images through pretraining on large-scale datasets, thereby presenting a potential solution to overcome these issues. However, directly applying pretrained LDM to heterogeneous OSFL results in significant distribution shifts in synthetic data, leading to performance degradation in classification models trained on such data. This issue is particularly pronounced in rare domains, such as medical imaging, which are underrepresented in LDM's pretraining data. To address this challenge, we propose Federated Bi-Level Personalization (FedBiP), which personalizes the pretrained LDM at both instance-level and concept-level. Hereby, FedBiP synthesizes images following the client's local data distribution without compromising the privacy regulations. FedBiP is also the first approach to simultaneously address feature space heterogeneity and client data scarcity in OSFL. Our method is validated through extensive experiments on three OSFL benchmarks with feature space heterogeneity, as well as on challenging medical and satellite image datasets with label heterogeneity. The results demonstrate the effectiveness of FedBiP, which substantially outperforms other OSFL methods.</li>
</ul>

<h3>Title: Learning Efficient and Effective Trajectories for Differential Equation-based Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Zhu, Jinhui Hou, Hui Liu, Huanqiang Zeng, Junhui Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04811">https://arxiv.org/abs/2410.04811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04811">https://arxiv.org/pdf/2410.04811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04811]] Learning Efficient and Effective Trajectories for Differential Equation-based Image Restoration(https://arxiv.org/abs/2410.04811)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The differential equation-based image restoration approach aims to establish learnable trajectories connecting high-quality images to a tractable distribution, e.g., low-quality images or a Gaussian distribution. In this paper, we reformulate the trajectory optimization of this kind of method, focusing on enhancing both reconstruction quality and efficiency. Initially, we navigate effective restoration paths through a reinforcement learning process, gradually steering potential trajectories toward the most precise options. Additionally, to mitigate the considerable computational burden associated with iterative sampling, we propose cost-aware trajectory distillation to streamline complex paths into several manageable steps with adaptable sizes. Moreover, we fine-tune a foundational diffusion model (FLUX) with 12B parameters by using our algorithms, producing a unified framework for handling 7 kinds of image restoration tasks. Extensive experiments showcase the significant superiority of the proposed method, achieving a maximum PSNR improvement of 2.1 dB over state-of-the-art methods, while also greatly enhancing visual perceptual quality. Project page: \url{this https URL}.</li>
</ul>

<h3>Title: Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Manuel Brenner, Elias Weber, Georgia Koppe, Daniel Durstewitz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DS, nlin.CD, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04814">https://arxiv.org/abs/2410.04814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04814">https://arxiv.org/pdf/2410.04814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04814]] Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data(https://arxiv.org/abs/2410.04814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In science, we are often interested in obtaining a generative model of the underlying system dynamics from observed time series. While powerful methods for dynamical systems reconstruction (DSR) exist when data come from a single domain, how to best integrate data from multiple dynamical regimes and leverage it for generalization is still an open question. This becomes particularly important when individual time series are short, and group-level information may help to fill in for gaps in single-domain data. At the same time, averaging is not an option in DSR, as it will wipe out crucial dynamical properties (e.g., limit cycles in one domain vs. chaos in another). Hence, a framework is needed that enables to efficiently harvest group-level (multi-domain) information while retaining all single-domain dynamical characteristics. Here we provide such a hierarchical approach and showcase it on popular DSR benchmarks, as well as on neuroscientific and medical time series. In addition to faithful reconstruction of all individual dynamical regimes, our unsupervised methodology discovers common low-dimensional feature spaces in which datasets with similar dynamics cluster. The features spanning these spaces were further dynamically highly interpretable, surprisingly in often linear relation to control parameters that govern the dynamics of the underlying system. Finally, we illustrate transfer learning and generalization to new parameter regimes.</li>
</ul>

<h3>Title: A Simple Image Segmentation Framework via In-Context Examples</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Chenchen Jing, Hengtao Li, Muzhi Zhu, Hao Chen, Xinlong Wang, Chunhua Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04842">https://arxiv.org/abs/2410.04842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04842">https://arxiv.org/pdf/2410.04842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04842]] A Simple Image Segmentation Framework via In-Context Examples(https://arxiv.org/abs/2410.04842)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recently, there have been explorations of generalist segmentation models that can effectively tackle a variety of image segmentation tasks within a unified in-context learning framework. However, these methods still struggle with task ambiguity in in-context segmentation, as not all in-context examples can accurately convey the task information. In order to address this issue, we present SINE, a simple image Segmentation framework utilizing in-context examples. Our approach leverages a Transformer encoder-decoder structure, where the encoder provides high-quality image representations, and the decoder is designed to yield multiple task-specific output masks to effectively eliminate task ambiguity. Specifically, we introduce an In-context Interaction module to complement in-context information and produce correlations between the target image and the in-context example and a Matching Transformer that uses fixed matching and a Hungarian algorithm to eliminate differences between different tasks. In addition, we have further perfected the current evaluation system for in-context image segmentation, aiming to facilitate a holistic appraisal of these models. Experiments on various segmentation tasks show the effectiveness of the proposed method.</li>
</ul>

<h3>Title: PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Feng Tian, Yixuan Li, Yichao Yan, Shanyan Guan, Yanhao Ge, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04844">https://arxiv.org/abs/2410.04844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04844">https://arxiv.org/pdf/2410.04844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04844]] PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing(https://arxiv.org/abs/2410.04844)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the field of image editing, three core challenges persist: controllability, background preservation, and efficiency. Inversion-based methods rely on time-consuming optimization to preserve the features of the initial images, which results in low efficiency due to the requirement for extensive network inference. Conversely, inversion-free methods lack theoretical support for background similarity, as they circumvent the issue of maintaining initial features to achieve efficiency. As a consequence, none of these methods can achieve both high efficiency and background consistency. To tackle the challenges and the aforementioned disadvantages, we introduce PostEdit, a method that incorporates a posterior scheme to govern the diffusion sampling process. Specifically, a corresponding measurement term related to both the initial features and Langevin dynamics is introduced to optimize the estimated image generated by the given target prompt. Extensive experimental results indicate that the proposed PostEdit achieves state-of-the-art editing performance while accurately preserving unedited regions. Furthermore, the method is both inversion- and training-free, necessitating approximately 1.5 seconds and 18 GB of GPU memory to generate high-quality results.</li>
</ul>

<h3>Title: Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models</h3>
<ul>
<li><strong>Authors: </strong>Dehong Kong, Siyuan Liang, Xiaopeng Zhu, Yuansheng Zhong, Wenqi Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04884">https://arxiv.org/abs/2410.04884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04884">https://arxiv.org/pdf/2410.04884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04884]] Patch is Enough: Naturalistic Adversarial Patch against Vision-Language Pre-training Models(https://arxiv.org/abs/2410.04884)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Visual language pre-training (VLP) models have demonstrated significant success across various domains, yet they remain vulnerable to adversarial attacks. Addressing these adversarial vulnerabilities is crucial for enhancing security in multimodal learning. Traditionally, adversarial methods targeting VLP models involve simultaneously perturbing images and text. However, this approach faces notable challenges: first, adversarial perturbations often fail to translate effectively into real-world scenarios; second, direct modifications to the text are conspicuously visible. To overcome these limitations, we propose a novel strategy that exclusively employs image patches for attacks, thus preserving the integrity of the original text. Our method leverages prior knowledge from diffusion models to enhance the authenticity and naturalness of the perturbations. Moreover, to optimize patch placement and improve the efficacy of our attacks, we utilize the cross-attention mechanism, which encapsulates intermodal interactions by generating attention maps to guide strategic patch placements. Comprehensive experiments conducted in a white-box setting for image-to-text scenarios reveal that our proposed method significantly outperforms existing techniques, achieving a 100% attack success rate. Additionally, it demonstrates commendable performance in transfer tasks involving text-to-image configurations.</li>
</ul>

<h3>Title: Low-Rank Continual Personalization of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Łukasz Staniszewski, Katarzyna Zaleska, Kamil Deja</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04891">https://arxiv.org/abs/2410.04891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04891">https://arxiv.org/pdf/2410.04891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04891]] Low-Rank Continual Personalization of Diffusion Models(https://arxiv.org/abs/2410.04891)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent personalization methods for diffusion models, such as Dreambooth, allow fine-tuning pre-trained models to generate new concepts. However, applying these techniques across multiple tasks in order to include, e.g., several new objects or styles, leads to mutual interference between their adapters. While recent studies attempt to mitigate this issue by combining trained adapters across tasks after fine-tuning, we adopt a more rigorous regime and investigate the personalization of large diffusion models under a continual learning scenario, where such interference leads to catastrophic forgetting of previous knowledge. To that end, we evaluate the naïve continual fine-tuning of customized models and compare this approach with three methods for consecutive adapters' training: sequentially merging new adapters, merging orthogonally initialized adapters, and updating only relevant parameters according to the task. In our experiments, we show that the proposed approaches mitigate forgetting when compared to the naïve approach.</li>
</ul>

<h3>Title: Intent Classification for Bank Chatbots through LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Bibiána Lajčinová, Patrik Valábek, Michal Spišiak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04925">https://arxiv.org/abs/2410.04925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04925">https://arxiv.org/pdf/2410.04925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04925]] Intent Classification for Bank Chatbots through LLM Fine-Tuning(https://arxiv.org/abs/2410.04925)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study evaluates the application of large language models (LLMs) for intent classification within a chatbot with predetermined responses designed for banking industry websites. Specifically, the research examines the effectiveness of fine-tuning SlovakBERT compared to employing multilingual generative models, such as Llama 8b instruct and Gemma 7b instruct, in both their pre-trained and fine-tuned versions. The findings indicate that SlovakBERT outperforms the other models in terms of in-scope accuracy and out-of-scope false positive rate, establishing it as the benchmark for this application.</li>
</ul>

<h3>Title: PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion</h3>
<ul>
<li><strong>Authors: </strong>Sijie Wang, Qiyu Kang, Rui She, Kai Zhao, Yang Song, Wee Peng Tay</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04939">https://arxiv.org/abs/2410.04939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04939">https://arxiv.org/pdf/2410.04939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04939]] PRFusion: Toward Effective and Robust Multi-Modal Place Recognition with Image and Point Cloud Fusion(https://arxiv.org/abs/2410.04939)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Place recognition plays a crucial role in the fields of robotics and computer vision, finding applications in areas such as autonomous driving, mapping, and localization. Place recognition identifies a place using query sensor data and a known database. One of the main challenges is to develop a model that can deliver accurate results while being robust to environmental variations. We propose two multi-modal place recognition models, namely PRFusion and PRFusion++. PRFusion utilizes global fusion with manifold metric attention, enabling effective interaction between features without requiring camera-LiDAR extrinsic calibrations. In contrast, PRFusion++ assumes the availability of extrinsic calibrations and leverages pixel-point correspondences to enhance feature learning on local windows. Additionally, both models incorporate neural diffusion layers, which enable reliable operation even in challenging environments. We verify the state-of-the-art performance of both models on three large-scale benchmarks. Notably, they outperform existing models by a substantial margin of +3.0 AR@1 on the demanding Boreas dataset. Furthermore, we conduct ablation studies to validate the effectiveness of our proposed methods. The codes are available at: this https URL</li>
</ul>

<h3>Title: Failure-Proof Non-Contrastive Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Sansone, Tim Lebailly, Tinne Tuytelaars</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04959">https://arxiv.org/abs/2410.04959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04959">https://arxiv.org/pdf/2410.04959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04959]] Failure-Proof Non-Contrastive Self-Supervised Learning(https://arxiv.org/abs/2410.04959)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We identify sufficient conditions to avoid known failure modes, including representation, dimensional, cluster and intracluster collapses, occurring in non-contrastive self-supervised learning. Based on these findings, we propose a principled design for the projector and loss function. We theoretically demonstrate that this design introduces an inductive bias that promotes learning representations that are both decorrelated and clustered without explicit enforcing these properties and leading to improved generalization. To the best of our knowledge, this is the first solution that achieves robust training with respect to these failure modes while guaranteeing enhanced generalization performance in downstream tasks. We validate our theoretical findings on image datasets including SVHN, CIFAR10, CIFAR100 and ImageNet-100, and show that our solution, dubbed FALCON, outperforms existing feature decorrelation and cluster-based self-supervised learning methods in terms of generalization to clustering and linear classification tasks.</li>
</ul>

<h3>Title: Revealing Directions for Text-guided 3D Face Editing</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Chen, Yichao Yan, Sehngqi Liu, Yuhao Cheng, Weiming Zhao, Lincheng Li, Mengxiao Bi, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04965">https://arxiv.org/abs/2410.04965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04965">https://arxiv.org/pdf/2410.04965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04965]] Revealing Directions for Text-guided 3D Face Editing(https://arxiv.org/abs/2410.04965)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D face editing is a significant task in multimedia, aimed at the manipulation of 3D face models across various control signals. The success of 3D-aware GAN provides expressive 3D models learned from 2D single-view images only, encouraging researchers to discover semantic editing directions in its latent space. However, previous methods face challenges in balancing quality, efficiency, and generalization. To solve the problem, we explore the possibility of introducing the strength of diffusion model into 3D-aware GANs. In this paper, we present Face Clan, a fast and text-general approach for generating and manipulating 3D faces based on arbitrary attribute descriptions. To achieve disentangled editing, we propose to diffuse on the latent space under a pair of opposite prompts to estimate the mask indicating the region of interest on latent codes. Based on the mask, we then apply denoising to the masked latent codes to reveal the editing direction. Our method offers a precisely controllable manipulation method, allowing users to intuitively customize regions of interest with the text description. Experiments demonstrate the effectiveness and generalization of our Face Clan for various pre-trained GANs. It offers an intuitive and wide application for text-guided face editing that contributes to the landscape of multimedia content creation.</li>
</ul>

<h3>Title: L-C4: Language-Based Video Colorization for Creative and Consistent Color</h3>
<ul>
<li><strong>Authors: </strong>Zheng Chang, Shuchen Weng, Huan Ouyang, Yu Li, Si Li, Boxin Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04972">https://arxiv.org/abs/2410.04972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04972">https://arxiv.org/pdf/2410.04972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04972]] L-C4: Language-Based Video Colorization for Creative and Consistent Color(https://arxiv.org/abs/2410.04972)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automatic video colorization is inherently an ill-posed problem because each monochrome frame has multiple optional color candidates. Previous exemplar-based video colorization methods restrict the user's imagination due to the elaborate retrieval process. Alternatively, conditional image colorization methods combined with post-processing algorithms still struggle to maintain temporal consistency. To address these issues, we present Language-based video Colorization for Creative and Consistent Colors (L-C4) to guide the colorization process using user-provided language descriptions. Our model is built upon a pre-trained cross-modality generative model, leveraging its comprehensive language understanding and robust color representation abilities. We introduce the cross-modality pre-fusion module to generate instance-aware text embeddings, enabling the application of creative colors. Additionally, we propose temporally deformable attention to prevent flickering or color shifts, and cross-clip fusion to maintain long-term color consistency. Extensive experimental results demonstrate that L-C4 outperforms relevant methods, achieving semantically accurate colors, unrestricted creative correspondence, and temporally robust consistency.</li>
</ul>

<h3>Title: Conditional Variational Autoencoders for Probabilistic Pose Regression</h3>
<ul>
<li><strong>Authors: </strong>Fereidoon Zangeneh, Leonard Bruns, Amit Dekel, Alessandro Pieropan, Patric Jensfelt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.04989">https://arxiv.org/abs/2410.04989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.04989">https://arxiv.org/pdf/2410.04989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.04989]] Conditional Variational Autoencoders for Probabilistic Pose Regression(https://arxiv.org/abs/2410.04989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Robots rely on visual relocalization to estimate their pose from camera images when they lose track. One of the challenges in visual relocalization is repetitive structures in the operation environment of the robot. This calls for probabilistic methods that support multiple hypotheses for robot's pose. We propose such a probabilistic method to predict the posterior distribution of camera poses given an observed image. Our proposed training strategy results in a generative model of camera poses given an image, which can be used to draw samples from the pose posterior distribution. Our method is streamlined and well-founded in theory and outperforms existing methods on localization in presence of ambiguities.</li>
</ul>

<h3>Title: SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness</h3>
<ul>
<li><strong>Authors: </strong>Jens-Joris Decorte, Jeroen Van Hautte, Thomas Demeester, Chris Develder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05006">https://arxiv.org/abs/2410.05006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05006">https://arxiv.org/pdf/2410.05006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05006]] SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness(https://arxiv.org/abs/2410.05006)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurately modeling the relationships between skills is a crucial part of human resources processes such as recruitment and employee development. Yet, no benchmarks exist to evaluate such methods directly. We construct and release SkillMatch, a benchmark for the task of skill relatedness, based on expert knowledge mining from millions of job ads. Additionally, we propose a scalable self-supervised learning technique to adapt a Sentence-BERT model based on skill co-occurrence in job ads. This new method greatly surpasses traditional models for skill relatedness as measured on SkillMatch. By releasing SkillMatch publicly, we aim to contribute a foundation for research towards increased accuracy and transparency of skill-based recommendation systems.</li>
</ul>

<h3>Title: T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Hugo Thimonier, José Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05016">https://arxiv.org/abs/2410.05016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05016">https://arxiv.org/pdf/2410.05016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05016]] T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data(https://arxiv.org/abs/2410.05016)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervision is often used for pre-training to foster performance on a downstream task by constructing meaningful representations of samples. Self-supervised learning (SSL) generally involves generating different views of the same sample and thus requires data augmentations that are challenging to construct for tabular data. This constitutes one of the main challenges of self-supervision for structured data. In the present work, we propose a novel augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on a Joint Embedding Predictive Architecture (JEPA) and is akin to mask reconstruction in the latent space. It involves predicting the latent representation of one subset of features from the latent representation of a different subset within the same sample, thereby learning rich representations without augmentations. We use our method as a pre-training technique and train several deep classifiers on the obtained representation. Our experimental results demonstrate a substantial improvement in both classification and regression tasks, outperforming models trained directly on samples in their original data space. Moreover, T-JEPA enables some methods to consistently outperform or match the performance of traditional methods likes Gradient Boosted Decision Trees. To understand why, we extensively characterize the obtained representations and show that T-JEPA effectively identifies relevant features for downstream tasks without access to the labels. Additionally, we introduce regularization tokens, a novel regularization method critical for training of JEPA-based models on structured data.</li>
</ul>

<h3>Title: A test suite of prompt injection attacks for LLM-based machine translation</h3>
<ul>
<li><strong>Authors: </strong>Antonio Valerio Miceli-Barone, Zhifan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05047">https://arxiv.org/abs/2410.05047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05047">https://arxiv.org/pdf/2410.05047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05047]] A test suite of prompt injection attacks for LLM-based machine translation(https://arxiv.org/abs/2410.05047)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>LLM-based NLP systems typically work by embedding their input data into prompt templates which contain instructions and/or in-context examples, creating queries which are submitted to a LLM, and then parsing the LLM response in order to generate the system outputs. Prompt Injection Attacks (PIAs) are a type of subversion of these systems where a malicious user crafts special inputs which interfere with the prompt templates, causing the LLM to respond in ways unintended by the system designer. Recently, Sun and Miceli-Barone proposed a class of PIAs against LLM-based machine translation. Specifically, the task is to translate questions from the TruthfulQA test suite, where an adversarial prompt is prepended to the questions, instructing the system to ignore the translation instruction and answer the questions instead. In this test suite, we extend this approach to all the language pairs of the WMT 2024 General Machine Translation task. Moreover, we include additional attack formats in addition to the one originally studied.</li>
</ul>

<h3>Title: HE-Drive: Human-Like End-to-End Driving with Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junming Wang, Xingyu Zhang, Zebin Xing, Songen Gu, Xiaoyang Guo, Yang Hu, Ziying Song, Qian Zhang, Xiaoxiao Long, Wei Yin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05051">https://arxiv.org/abs/2410.05051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05051">https://arxiv.org/pdf/2410.05051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05051]] HE-Drive: Human-Like End-to-End Driving with Vision Language Models(https://arxiv.org/abs/2410.05051)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose HE-Drive: the first human-like-centric end-to-end autonomous driving system to generate trajectories that are both temporally consistent and comfortable. Recent studies have shown that imitation learning-based planners and learning-based trajectory scorers can effectively generate and select accuracy trajectories that closely mimic expert demonstrations. However, such trajectory planners and scorers face the dilemma of generating temporally inconsistent and uncomfortable trajectories. To solve the above problems, Our HE-Drive first extracts key 3D spatial representations through sparse perception, which then serves as conditional inputs for a Conditional Denoising Diffusion Probabilistic Models (DDPMs)-based motion planner to generate temporal consistency multi-modal trajectories. A Vision-Language Models (VLMs)-guided trajectory scorer subsequently selects the most comfortable trajectory from these candidates to control the vehicle, ensuring human-like end-to-end driving. Experiments show that HE-Drive not only achieves state-of-the-art performance (i.e., reduces the average collision rate by 71% than VAD) and efficiency (i.e., 1.9X faster than SparseDrive) on the challenging nuScenes and OpenScene datasets but also provides the most comfortable driving experience on real-world this http URL more information, visit the project website: this https URL.</li>
</ul>

<h3>Title: SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Benjamin Feuer, Jiawei Xu, Niv Cohen, Patrick Yubeaton, Govind Mittal, Chinmay Hegde</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05057">https://arxiv.org/abs/2410.05057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05057">https://arxiv.org/pdf/2410.05057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05057]] SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification(https://arxiv.org/abs/2410.05057)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT, the first large-scale benchmark of curation strategies for image classification. In order to generate baseline methods for the SELECT benchmark, we create a new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K to date. Our dataset extends ImageNet with 5 new training-data shifts, each approximately the size of ImageNet-1K itself, and each assembled using a distinct curation strategy. We evaluate our data curation baselines in two ways: (i) using each training-data shift to train identical image classification models from scratch (ii) using the data itself to fit a pretrained self-supervised representation. Our findings show interesting trends, particularly pertaining to recent methods for data curation such as synthetic data generation and lookup based on CLIP embeddings. We show that although these strategies are highly competitive for certain tasks, the curation strategy used to assemble the original ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark can illuminate the path for new methods to further reduce the gap. We release our checkpoints, code, documentation, and a link to our dataset at this https URL.</li>
</ul>

<h3>Title: TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05076">https://arxiv.org/abs/2410.05076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05076">https://arxiv.org/pdf/2410.05076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05076]] TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention(https://arxiv.org/abs/2410.05076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.</li>
</ul>

<h3>Title: Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data</h3>
<ul>
<li><strong>Authors: </strong>David Heurtel-Depeiges, Anian Ruoss, Joel Veness, Tim Genewein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05078">https://arxiv.org/abs/2410.05078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05078">https://arxiv.org/pdf/2410.05078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05078]] Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data(https://arxiv.org/abs/2410.05078)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have recently been shown to be strong data compressors. However, when accounting for their excessive parameter count, their compression ratios are actually inferior to standard compression algorithms. Moreover, naively reducing the number of parameters may not necessarily help as it leads to worse predictions and thus weaker compression. In this paper, we conduct a large-scale empirical study to investigate whether there is a sweet spot where competitive compression ratios with pre-trained vanilla transformers are possible. To this end, we train families of models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality. We find that relatively small models (i.e., millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG 2000, FLAC) - even when factoring in parameter count. We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we investigate the effect of unimodal versus multimodal training. We find that even small models can be trained to perform well on multiple modalities, but, in contrast to previously reported results with large-scale foundation models, transfer to unseen modalities is generally weak.</li>
</ul>

<h3>Title: DreamSat: Towards a General 3D Model for Novel View Synthesis of Space Objects</h3>
<ul>
<li><strong>Authors: </strong>Nidhi Mathihalli, Audrey Wei, Giovanni Lavezzi, Peng Mun Siew, Victor Rodriguez-Fernandez, Hodei Urrutxua, Richard Linares</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05097">https://arxiv.org/abs/2410.05097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05097">https://arxiv.org/pdf/2410.05097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05097]] DreamSat: Towards a General 3D Model for Novel View Synthesis of Space Objects(https://arxiv.org/abs/2410.05097)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Novel view synthesis (NVS) enables to generate new images of a scene or convert a set of 2D images into a comprehensive 3D model. In the context of Space Domain Awareness, since space is becoming increasingly congested, NVS can accurately map space objects and debris, improving the safety and efficiency of space operations. Similarly, in Rendezvous and Proximity Operations missions, 3D models can provide details about a target object's shape, size, and orientation, allowing for better planning and prediction of the target's behavior. In this work, we explore the generalization abilities of these reconstruction techniques, aiming to avoid the necessity of retraining for each new scene, by presenting a novel approach to 3D spacecraft reconstruction from single-view images, DreamSat, by fine-tuning the Zero123 XL, a state-of-the-art single-view reconstruction model, on a high-quality dataset of 190 high-quality spacecraft models and integrating it into the DreamGaussian framework. We demonstrate consistent improvements in reconstruction quality across multiple metrics, including Contrastive Language-Image Pretraining (CLIP) score (+0.33%), Peak Signal-to-Noise Ratio (PSNR) (+2.53%), Structural Similarity Index (SSIM) (+2.38%), and Learned Perceptual Image Patch Similarity (LPIPS) (+0.16%) on a test set of 30 previously unseen spacecraft images. Our method addresses the lack of domain-specific 3D reconstruction tools in the space industry by leveraging state-of-the-art diffusion models and 3D Gaussian splatting techniques. This approach maintains the efficiency of the DreamGaussian framework while enhancing the accuracy and detail of spacecraft reconstructions. The code for this work can be accessed on GitHub (this https URL).</li>
</ul>

<h3>Title: AI-Enhanced Ethical Hacking: A Linux-Focused Experiment</h3>
<ul>
<li><strong>Authors: </strong>Haitham S. Al-Sinani, Chris J. Mitchell</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05105">https://arxiv.org/abs/2410.05105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05105">https://arxiv.org/pdf/2410.05105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05105]] AI-Enhanced Ethical Hacking: A Linux-Focused Experiment(https://arxiv.org/abs/2410.05105)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This technical report investigates the integration of generative AI (GenAI), specifically ChatGPT, into the practice of ethical hacking through a comprehensive experimental study and conceptual analysis. Conducted in a controlled virtual environment, the study evaluates GenAI's effectiveness across the key stages of penetration testing on Linux-based target machines operating within a virtual local area network (LAN), including reconnaissance, scanning and enumeration, gaining access, maintaining access, and covering tracks. The findings confirm that GenAI can significantly enhance and streamline the ethical hacking process while underscoring the importance of balanced human-AI collaboration rather than the complete replacement of human input. The report also critically examines potential risks such as misuse, data biases, hallucination, and over-reliance on AI. This research contributes to the ongoing discussion on the ethical use of AI in cybersecurity and highlights the need for continued innovation to strengthen security defences.</li>
</ul>

<h3>Title: Hyper-Representations: Learning from Populations of Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Schürholt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05107">https://arxiv.org/abs/2410.05107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05107">https://arxiv.org/pdf/2410.05107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05107]] Hyper-Representations: Learning from Populations of Neural Networks(https://arxiv.org/abs/2410.05107)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>This thesis addresses the challenge of understanding Neural Networks through the lens of their most fundamental component: the weights, which encapsulate the learned information and determine the model behavior. At the core of this thesis is a fundamental question: Can we learn general, task-agnostic representations from populations of Neural Network models? The key contribution of this thesis to answer that question are hyper-representations, a self-supervised method to learn representations of NN weights. Work in this thesis finds that trained NN models indeed occupy meaningful structures in the weight space, that can be learned and used. Through extensive experiments, this thesis demonstrates that hyper-representations uncover model properties, such as their performance, state of training, or hyperparameters. Moreover, the identification of regions with specific properties in hyper-representation space allows to sample and generate model weights with targeted properties. This thesis demonstrates applications for fine-tuning, and transfer learning to great success. Lastly, it presents methods that allow hyper-representations to generalize beyond model sizes, architectures, and tasks. The practical implications of that are profound, as it opens the door to foundation models of Neural Networks, which aggregate and instantiate their knowledge across models and architectures. Ultimately, this thesis contributes to the deeper understanding of Neural Networks by investigating structures in their weights which leads to more interpretable, efficient, and adaptable models. By laying the groundwork for representation learning of NN weights, this research demonstrates the potential to change the way Neural Networks are developed, analyzed, and used.</li>
</ul>

<h3>Title: Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization</h3>
<ul>
<li><strong>Authors: </strong>Rohan Reddy Mekala, Frederik Pahde, Simon Baur, Sneha Chandrashekar, Madeline Diep, Markus Wenzel, Eric L. Wisotzky, Galip Ümit Yolcu, Sebastian Lapuschkin, Jackie Ma, Peter Eisert, Mikael Lindvall, Adam Porter, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05114">https://arxiv.org/abs/2410.05114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05114">https://arxiv.org/pdf/2410.05114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05114]] Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization(https://arxiv.org/abs/2410.05114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the realm of dermatological diagnoses, where the analysis of dermatoscopic and microscopic skin lesion images is pivotal for the accurate and early detection of various medical conditions, the costs associated with creating diverse and high-quality annotated datasets have hampered the accuracy and generalizability of machine learning models. We propose an innovative unsupervised augmentation solution that harnesses Generative Adversarial Network (GAN) based models and associated techniques over their latent space to generate controlled semiautomatically-discovered semantic variations in dermatoscopic images. We created synthetic images to incorporate the semantic variations and augmented the training data with these images. With this approach, we were able to increase the performance of machine learning models and set a new benchmark amongst non-ensemble based models in skin lesion classification on the HAM10000 dataset; and used the observed analytics and generated models for detailed studies on model explainability, affirming the effectiveness of our solution.</li>
</ul>

<h3>Title: Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05116">https://arxiv.org/abs/2410.05116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05116">https://arxiv.org/pdf/2410.05116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05116]] Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning(https://arxiv.org/abs/2410.05116)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Controllable generation through Stable Diffusion (SD) fine-tuning aims to improve fidelity, safety, and alignment with human guidance. Existing reinforcement learning from human feedback methods usually rely on predefined heuristic reward functions or pretrained reward models built on large-scale datasets, limiting their applicability to scenarios where collecting such data is costly or difficult. To effectively and efficiently utilize human feedback, we develop a framework, HERO, which leverages online human feedback collected on the fly during model learning. Specifically, HERO features two key mechanisms: (1) Feedback-Aligned Representation Learning, an online training method that captures human feedback and provides informative learning signals for fine-tuning, and (2) Feedback-Guided Image Generation, which involves generating images from SD's refined initialization samples, enabling faster convergence towards the evaluator's intent. We demonstrate that HERO is 4x more efficient in online feedback for body part anomaly correction compared to the best existing method. Additionally, experiments show that HERO can effectively handle tasks like reasoning, counting, personalization, and reducing NSFW content with only 0.5K online feedback.</li>
</ul>

<h3>Title: Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side Information</h3>
<ul>
<li><strong>Authors: </strong>Timofey Efimov, Harry Dong, Megna Shah, Jeff Simmons, Sean Donegan, Yuejie Chi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05143">https://arxiv.org/abs/2410.05143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05143">https://arxiv.org/pdf/2410.05143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05143]] Leveraging Multimodal Diffusion Models to Accelerate Imaging with Side Information(https://arxiv.org/abs/2410.05143)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have found phenomenal success as expressive priors for solving inverse problems, but their extension beyond natural images to more structured scientific domains remains limited. Motivated by applications in materials science, we aim to reduce the number of measurements required from an expensive imaging modality of interest, by leveraging side information from an auxiliary modality that is much cheaper to obtain. To deal with the non-differentiable and black-box nature of the forward model, we propose a framework to train a multimodal diffusion model over the joint modalities, turning inverse problems with black-box forward models into simple linear inpainting problems. Numerically, we demonstrate the feasibility of training diffusion models over materials imagery data, and show that our approach achieves superior image reconstruction by leveraging the available side information, requiring significantly less amount of data from the expensive microscopy modality.</li>
</ul>

<h3>Title: Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mehrdad Farahani, Richard Johansson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05162">https://arxiv.org/abs/2410.05162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05162">https://arxiv.org/pdf/2410.05162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05162]] Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models(https://arxiv.org/abs/2410.05162)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative language models often struggle with specialized or less-discussed knowledge. A potential solution is found in Retrieval-Augmented Generation (RAG) models which act like retrieving information before generating responses. In this study, we explore how the \textsc{Atlas} approach, a RAG model, decides between what it already knows (parametric) and what it retrieves (non-parametric). We use causal mediation analysis and controlled experiments to examine how internal representations influence information processing. Our findings disentangle the effects of parametric knowledge and the retrieved context. They indicate that in cases where the model can choose between both types of information (parametric and non-parametric), it relies more on the context than the parametric knowledge. Furthermore, the analysis investigates the computations involved in \emph{how} the model uses the information from the context. We find that multiple mechanisms are active within the model and can be detected with mediation analysis: first, the decision of \emph{whether the context is relevant}, and second, how the encoder computes output representations to support copying when relevant.</li>
</ul>

<h3>Title: A Simulation-Free Deep Learning Approach to Stochastic Optimal Control</h3>
<ul>
<li><strong>Authors: </strong>Mengjian Hua, Matthieu Laurière, Eric Vanden-Eijnden</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05163">https://arxiv.org/abs/2410.05163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05163">https://arxiv.org/pdf/2410.05163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05163]] A Simulation-Free Deep Learning Approach to Stochastic Optimal Control(https://arxiv.org/abs/2410.05163)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a simulation-free algorithm for the solution of generic problems in stochastic optimal control (SOC). Unlike existing methods, our approach does not require the solution of an adjoint problem, but rather leverages Girsanov theorem to directly calculate the gradient of the SOC objective on-policy. This allows us to speed up the optimization of control policies parameterized by neural networks since it completely avoids the expensive back-propagation step through stochastic differential equations (SDEs) used in the Neural SDE framework. In particular, it enables us to solve SOC problems in high dimension and on long time horizons. We demonstrate the efficiency of our approach in various domains of applications, including standard stochastic optimal control problems, sampling from unnormalized distributions via construction of a Schrödinger-Föllmer process, and fine-tuning of pre-trained diffusion models. In all cases our method is shown to outperform the existing methods in both the computing time and memory efficiency.</li>
</ul>

<h3>Title: Density estimation with LLMs: a geometric investigation of in-context learning trajectories</h3>
<ul>
<li><strong>Authors: </strong>Toni J.B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05218">https://arxiv.org/abs/2410.05218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05218">https://arxiv.org/pdf/2410.05218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05218]] Density estimation with LLMs: a geometric investigation of in-context learning trajectories(https://arxiv.org/abs/2410.05218)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate remarkable emergent abilities to perform in-context learning across various tasks, including time series forecasting. This work investigates LLMs' ability to estimate probability density functions (PDFs) from data observed in-context; such density estimation (DE) is a fundamental task underlying many probabilistic modeling problems. We leverage the Intensive Principal Component Analysis (InPCA) to visualize and analyze the in-context learning dynamics of LLaMA-2 models. Our main finding is that these LLMs all follow similar learning trajectories in a low-dimensional InPCA space, which are distinct from those of traditional density estimation methods like histograms and Gaussian kernel density estimation (KDE). We interpret the LLaMA in-context DE process as a KDE with an adaptive kernel width and shape. This custom kernel model captures a significant portion of LLaMA's behavior despite having only two parameters. We further speculate on why LLaMA's kernel width and shape differs from classical algorithms, providing insights into the mechanism of in-context probabilistic reasoning in LLMs.</li>
</ul>

<h3>Title: Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates</h3>
<ul>
<li><strong>Authors: </strong>Avanika Narayan, Mayee F. Chen, Kush Bhatia, Christopher Ré</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05224">https://arxiv.org/abs/2410.05224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05224">https://arxiv.org/pdf/2410.05224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05224]] Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates(https://arxiv.org/abs/2410.05224)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) on instruction datasets is a common way to improve their generative capabilities. However, instruction datasets can be expensive and time-consuming to manually curate, and while LLM-generated data is less labor-intensive, it may violate user privacy agreements or terms of service of LLM providers. Therefore, we seek a way of constructing instruction datasets with samples that are not generated by humans or LLMs but still improve LLM generative capabilities. In this work, we introduce Cookbook, a framework that programmatically generates training data consisting of simple patterns over random tokens, resulting in a scalable, cost-effective approach that avoids legal and privacy issues. First, Cookbook uses a template -- a data generating Python function -- to produce training data that encourages the model to learn an explicit pattern-based rule that corresponds to a desired task. We find that fine-tuning on Cookbook-generated data is able to improve performance on its corresponding task by up to 52.7 accuracy points. Second, since instruction datasets improve performance on multiple downstream tasks simultaneously, Cookbook algorithmically learns how to mix data from various templates to optimize performance on multiple tasks. On the standard multi-task GPT4ALL evaluation suite, Mistral-7B fine-tuned using a Cookbook-generated dataset attains the best accuracy on average compared to other 7B parameter instruction-tuned models and is the best performing model on 3 out of 8 tasks. Finally, we analyze when and why Cookbook improves performance and present a metric that allows us to verify that the improvement is largely explained by the model's generations adhering better to template rules.</li>
</ul>

<h3>Title: DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields in Unsupervised Deformable Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Yongtai Zhuo, Yiqing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05234">https://arxiv.org/abs/2410.05234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05234">https://arxiv.org/pdf/2410.05234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05234]] DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields in Unsupervised Deformable Image Registration(https://arxiv.org/abs/2410.05234)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deformable image registration aims to precisely align medical images from different modalities or times. Traditional deep learning methods, while effective, often lack interpretability, real-time observability and adjustment capacity during registration inference. Denoising diffusion models present an alternative by reformulating registration as iterative image denoising. However, existing diffusion registration approaches do not fully harness capabilities, neglecting the critical sampling phase that enables continuous observability during the inference. Hence, we introduce DiffuseReg, an innovative diffusion-based method that denoises deformation fields instead of images for improved transparency. We also propose a novel denoising network upon Swin Transformer, which better integrates moving and fixed images with diffusion time step throughout the denoising process. Furthermore, we enhance control over the denoising registration process with a novel similarity consistency regularization. Experiments on ACDC datasets demonstrate DiffuseReg outperforms existing diffusion registration methods by 1.32 in Dice score. The sampling process in DiffuseReg enables real-time output observability and adjustment unmatched by previous deep models.</li>
</ul>

<h3>Title: SePPO: Semi-Policy Preference Optimization for Diffusion Alignment</h3>
<ul>
<li><strong>Authors: </strong>Daoan Zhang, Guangchen Lan, Dong-Jun Han, Wenlin Yao, Xiaoman Pan, Hongming Zhang, Mingxiao Li, Pengcheng Chen, Yu Dong, Christopher Brinton, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05255">https://arxiv.org/abs/2410.05255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05255">https://arxiv.org/pdf/2410.05255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05255]] SePPO: Semi-Policy Preference Optimization for Diffusion Alignment(https://arxiv.org/abs/2410.05255)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace "losing images" in preference pairs. This approach allows us to optimize using only off-policy "winning images." Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in this https URL.</li>
</ul>

<h3>Title: Differential Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05258">https://arxiv.org/abs/2410.05258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05258">https://arxiv.org/pdf/2410.05258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05258]] Differential Transformer(https://arxiv.org/abs/2410.05258)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.</li>
</ul>

<h3>Title: GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yukang Cao, Masoud Hadi, Liang Pan, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05259">https://arxiv.org/abs/2410.05259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05259">https://arxiv.org/pdf/2410.05259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05259]] GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting(https://arxiv.org/abs/2410.05259)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based 2D virtual try-on (VTON) techniques have recently demonstrated strong performance, while the development of 3D VTON has largely lagged behind. Despite recent advances in text-guided 3D scene editing, integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains challenging. The reasons are twofold. First, text prompts cannot provide sufficient details in describing clothing. Second, 2D VTON results generated from different viewpoints of the same 3D scene lack coherence and spatial relationships, hence frequently leading to appearance inconsistencies and geometric distortions. To resolve these problems, we introduce an image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained knowledge from 2D VTON models to 3D while improving cross-view consistency. (1) Specifically, we propose a personalized diffusion model that utilizes low-rank adaptation (LoRA) fine-tuning to incorporate personalized information into pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a reference-driven image editing approach that enables the simultaneous editing of multi-view images while ensuring consistency. (2) Furthermore, we propose a persona-aware 3DGS editing framework to facilitate effective editing while maintaining consistent cross-view appearance and high-quality 3D geometry. (3) Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which facilitates comprehensive qualitative and quantitative 3D VTON evaluations. Through extensive experiments and comparative analyses with existing methods, the proposed \OM has demonstrated superior fidelity and advanced editing capabilities, affirming its effectiveness for 3D VTON.</li>
</ul>

<h3>Title: DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</h3>
<ul>
<li><strong>Authors: </strong>Kaifeng Zhao, Gen Li, Siyu Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.05260">https://arxiv.org/abs/2410.05260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.05260">https://arxiv.org/pdf/2410.05260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.05260]] DART: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control(https://arxiv.org/abs/2410.05260)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose DART, a Diffusion-based Autoregressive motion primitive model for Real-time Text-driven motion control. Our model, DART, effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model's versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability. Video results are available on the project page: this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
