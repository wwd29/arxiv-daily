<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-14</h1>
<h3>Title: Teaching Humans Subtle Differences with DIFFusion</h3>
<ul>
<li><strong>Authors: </strong>Mia Chiquier, Orr Avrech, Yossi Gandelsman, Berthy Feng, Katherine Bouman, Carl Vondrick</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08046">https://arxiv.org/abs/2504.08046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08046">https://arxiv.org/pdf/2504.08046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08046]] Teaching Humans Subtle Differences with DIFFusion(https://arxiv.org/abs/2504.08046)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Human expertise depends on the ability to recognize subtle visual differences, such as distinguishing diseases, species, or celestial phenomena. We propose a new method to teach novices how to differentiate between nuanced categories in specialized domains. Our method uses generative models to visualize the minimal change in features to transition between classes, i.e., counterfactuals, and performs well even in domains where data is sparse, examples are unpaired, and category boundaries are not easily explained by text. By manipulating the conditioning space of diffusion models, our proposed method DIFFusion disentangles category structure from instance identity, enabling high-fidelity synthesis even in challenging domains. Experiments across six domains show accurate transitions even with limited and unpaired examples across categories. User studies confirm that our generated counterfactuals outperform unpaired examples in teaching perceptual expertise, showing the potential of generative models for specialized visual learning.</li>
</ul>

<h3>Title: Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery</h3>
<ul>
<li><strong>Authors: </strong>Angelina Ibarra, Joshua Peeples</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08049">https://arxiv.org/abs/2504.08049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08049">https://arxiv.org/pdf/2504.08049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08049]] Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery(https://arxiv.org/abs/2504.08049)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This work presents a new approach to anomaly detection and localization in synthetic aperture radar imagery (SAR), expanding upon the existing patch distribution modeling framework (PaDiM). We introduce the adaptive cosine estimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at inference, an unbounded metric. ACE instead uses the cosine similarity metric, providing bounded anomaly detection scores. The proposed method is evaluated across multiple SAR datasets, with performance metrics including the area under the receiver operating curve (AUROC) at the image and pixel level, aiming for increased performance in anomaly detection and localization of SAR imagery. The code is publicly available: this https URL.</li>
</ul>

<h3>Title: Compositional Flows for 3D Molecule and Synthesis Pathway Co-design</h3>
<ul>
<li><strong>Authors: </strong>Tony Shen, Seonghwan Seo, Ross Irwin, Kieran Didi, Simon Olsson, Woo Youn Kim, Martin Ester</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08051">https://arxiv.org/abs/2504.08051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08051">https://arxiv.org/pdf/2504.08051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08051]] Compositional Flows for 3D Molecule and Synthesis Pathway Co-design(https://arxiv.org/abs/2504.08051)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many generative applications, such as synthesis-based 3D molecular design, involve constructing compositional objects with continuous features. Here, we introduce Compositional Generative Flows (CGFlow), a novel framework that extends flow matching to generate objects in compositional steps while modeling continuous states. Our key insight is that modeling compositional state transitions can be formulated as a straightforward extension of the flow matching interpolation process. We further build upon the theoretical foundations of generative flow networks (GFlowNets), enabling reward-guided sampling of compositional structures. We apply CGFlow to synthesizable drug design by jointly designing the molecule's synthetic pathway with its 3D binding pose. Our approach achieves state-of-the-art binding affinity on all 15 targets from the LIT-PCBA benchmark, and 5.8$\times$ improvement in sampling efficiency compared to 2D synthesis-based baseline. To our best knowledge, our method is also the first to achieve state of-art-performance in both Vina Dock (-9.38) and AiZynth success rate (62.2\%) on the CrossDocked benchmark.</li>
</ul>

<h3>Title: ContrastiveGaussian: High-Fidelity 3D Generation with Contrastive Learning and Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Junbang Liu, Enpei Huang, Dongxing Mao, Hui Zhang, Xinyuan Song, Yongxin Ni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08100">https://arxiv.org/abs/2504.08100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08100">https://arxiv.org/pdf/2504.08100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08100]] ContrastiveGaussian: High-Fidelity 3D Generation with Contrastive Learning and Gaussian Splatting(https://arxiv.org/abs/2504.08100)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Creating 3D content from single-view images is a challenging problem that has attracted considerable attention in recent years. Current approaches typically utilize score distillation sampling (SDS) from pre-trained 2D diffusion models to generate multi-view 3D representations. Although some methods have made notable progress by balancing generation speed and model quality, their performance is often limited by the visual inconsistencies of the diffusion model outputs. In this work, we propose ContrastiveGaussian, which integrates contrastive learning into the generative process. By using a perceptual loss, we effectively differentiate between positive and negative samples, leveraging the visual inconsistencies to improve 3D generation quality. To further enhance sample differentiation and improve contrastive learning, we incorporate a super-resolution model and introduce another Quantity-Aware Triplet Loss to address varying sample distributions during training. Our experiments demonstrate that our approach achieves superior texture fidelity and improved geometric consistency.</li>
</ul>

<h3>Title: POEM: Precise Object-level Editing via MLLM control</h3>
<ul>
<li><strong>Authors: </strong>Marco Schouten, Mehmet Onurcan Kaya, Serge Belongie, Dim P. Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08111">https://arxiv.org/abs/2504.08111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08111">https://arxiv.org/pdf/2504.08111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08111]] POEM: Precise Object-level Editing via MLLM control(https://arxiv.org/abs/2504.08111)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have significantly improved text-to-image generation, producing high-quality, realistic images from textual descriptions. Beyond generation, object-level image editing remains a challenging problem, requiring precise modifications while preserving visual coherence. Existing text-based instructional editing methods struggle with localized shape and layout transformations, often introducing unintended global changes. Image interaction-based approaches offer better accuracy but require manual human effort to provide precise guidance. To reduce this manual effort while maintaining a high image editing accuracy, in this paper, we propose POEM, a framework for Precise Object-level Editing using Multimodal Large Language Models (MLLMs). POEM leverages MLLMs to analyze instructional prompts and generate precise object masks before and after transformation, enabling fine-grained control without extensive user input. This structured reasoning stage guides the diffusion-based editing process, ensuring accurate object localization and transformation. To evaluate our approach, we introduce VOCEdits, a benchmark dataset based on PASCAL VOC 2012, augmented with instructional edit prompts, ground-truth transformations, and precise object masks. Experimental results show that POEM outperforms existing text-based image editing approaches in precision and reliability while reducing manual effort compared to interaction-based methods.</li>
</ul>

<h3>Title: Benchmarking Suite for Synthetic Aperture Radar Imagery Anomaly Detection (SARIAD) Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Lucian Chauvina, Somil Guptac, Angelina Ibarrac, Joshua Peeples</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08115">https://arxiv.org/abs/2504.08115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08115">https://arxiv.org/pdf/2504.08115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08115]] Benchmarking Suite for Synthetic Aperture Radar Imagery Anomaly Detection (SARIAD) Algorithms(https://arxiv.org/abs/2504.08115)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection is a key research challenge in computer vision and machine learning with applications in many fields from quality control to radar imaging. In radar imaging, specifically synthetic aperture radar (SAR), anomaly detection can be used for the classification, detection, and segmentation of objects of interest. However, there is no method for developing and benchmarking these methods on SAR imagery. To address this issue, we introduce SAR imagery anomaly detection (SARIAD). In conjunction with Anomalib, a deep-learning library for anomaly detection, SARIAD provides a comprehensive suite of algorithms and datasets for assessing and developing anomaly detection approaches on SAR imagery. SARIAD specifically integrates multiple SAR datasets along with tools to effectively apply various anomaly detection algorithms to SAR imagery. Several anomaly detection metrics and visualizations are available. Overall, SARIAD acts as a central package for benchmarking SAR models and datasets to allow for reproducible research in the field of anomaly detection in SAR imagery. This package is publicly available: this https URL.</li>
</ul>

<h3>Title: Impact of Language Guidance: A Reproducibility Study</h3>
<ul>
<li><strong>Authors: </strong>Cherish Puniani, Advika Sinha, Shree Singhi, Aayan Yadav</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08140">https://arxiv.org/abs/2504.08140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08140">https://arxiv.org/pdf/2504.08140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08140]] Impact of Language Guidance: A Reproducibility Study(https://arxiv.org/abs/2504.08140)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Modern deep-learning architectures need large amounts of data to produce state-of-the-art results. Annotating such huge datasets is time-consuming, expensive, and prone to human error. Recent advances in self-supervised learning allow us to train huge models without explicit annotation. Contrastive learning is a popular paradigm in self-supervised learning. Recent works like SimCLR and CLIP rely on image augmentations or directly minimizing cross-modal loss between image and text. Banani et al. (2023) propose to use language guidance to sample view pairs. They claim that language enables better conceptual similarity, eliminating the effects of visual variability. We reproduce their experiments to verify their claims and find that their dataset, RedCaps, contains low-quality captions. We use an off-the-shelf image captioning model, BLIP-2, to replace the captions and improve performance, and we also devise a new metric to evaluate the semantic capabilities of self-supervised models based on interpretability methods.</li>
</ul>

<h3>Title: LoRAX: LoRA eXpandable Networks for Continual Synthetic Image Attribution</h3>
<ul>
<li><strong>Authors: </strong>Danielle Sullivan-Pao, Nicole Tian, Pooya Khorrami</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08149">https://arxiv.org/abs/2504.08149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08149">https://arxiv.org/pdf/2504.08149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08149]] LoRAX: LoRA eXpandable Networks for Continual Synthetic Image Attribution(https://arxiv.org/abs/2504.08149)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As generative AI image technologies become more widespread and advanced, there is a growing need for strong attribution models. These models are crucial for verifying the authenticity of images and identifying the architecture of their originating generative models-key to maintaining media integrity. However, attribution models struggle to generalize to unseen models, and traditional fine-tuning methods for updating these models have shown to be impractical in real-world settings. To address these challenges, we propose LoRA eXpandable Networks (LoRAX), a parameter-efficient class incremental algorithm that adapts to novel generative image models without the need for full retraining. Our approach trains an extremely parameter-efficient feature extractor per continual learning task via Low Rank Adaptation. Each task-specific feature extractor learns distinct features while only requiring a small fraction of the parameters present in the underlying feature extractor's backbone model. Our extensive experimentation shows LoRAX outperforms or remains competitive with state-of-the-art class incremental learning algorithms on the Continual Deepfake Detection benchmark across all training scenarios and memory settings, while requiring less than 3% of the number of trainable parameters per feature extractor compared to the full-rank implementation. LoRAX code is available at: this https URL.</li>
</ul>

<h3>Title: Investigating Vision-Language Model for Point Cloud-based Vehicle Classification</h3>
<ul>
<li><strong>Authors: </strong>Yiqiao Li, Jie Wei, Camille Kamga</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08154">https://arxiv.org/abs/2504.08154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08154">https://arxiv.org/pdf/2504.08154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08154]] Investigating Vision-Language Model for Point Cloud-based Vehicle Classification(https://arxiv.org/abs/2504.08154)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Heavy-duty trucks pose significant safety challenges due to their large size and limited maneuverability compared to passenger vehicles. A deeper understanding of truck characteristics is essential for enhancing the safety perspective of cooperative autonomous driving. Traditional LiDAR-based truck classification methods rely on extensive manual annotations, which makes them labor-intensive and costly. The rapid advancement of large language models (LLMs) trained on massive datasets presents an opportunity to leverage their few-shot learning capabilities for truck classification. However, existing vision-language models (VLMs) are primarily trained on image datasets, which makes it challenging to directly process point cloud data. This study introduces a novel framework that integrates roadside LiDAR point cloud data with VLMs to facilitate efficient and accurate truck classification, which supports cooperative and safe driving environments. This study introduces three key innovations: (1) leveraging real-world LiDAR datasets for model development, (2) designing a preprocessing pipeline to adapt point cloud data for VLM input, including point cloud registration for dense 3D rendering and mathematical morphological techniques to enhance feature representation, and (3) utilizing in-context learning with few-shot prompting to enable vehicle classification with minimally labeled training data. Experimental results demonstrate encouraging performance of this method and present its potential to reduce annotation efforts while improving classification accuracy.</li>
</ul>

<h3>Title: Learning Object Focused Attention</h3>
<ul>
<li><strong>Authors: </strong>Vivek Trivedy, Amani Almalki, Longin Jan Latecki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08166">https://arxiv.org/abs/2504.08166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08166">https://arxiv.org/pdf/2504.08166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08166]] Learning Object Focused Attention(https://arxiv.org/abs/2504.08166)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>We propose an adaptation to the training of Vision Transformers (ViTs) that allows for an explicit modeling of objects during the attention computation. This is achieved by adding a new branch to selected attention layers that computes an auxiliary loss which we call the object-focused attention (OFA) loss. We restrict the attention to image patches that belong to the same object class, which allows ViTs to gain a better understanding of configural (or holistic) object shapes by focusing on intra-object patches instead of other patches such as those in the background. Our proposed inductive bias fits easily into the attention framework of transformers since it only adds an auxiliary loss over selected attention layers. Furthermore, our approach has no additional overhead during inference. We also experiment with multiscale masking to further improve the performance of our OFA model and give a path forward for self-supervised learning with our method. Our experimental results demonstrate that ViTs with OFA achieve better classification results than their base models, exhibit a stronger generalization ability to out-of-distribution (OOD) and adversarially corrupted images, and learn representations based on object shapes rather than spurious correlations via general textures. For our OOD setting, we generate a novel dataset using the COCO dataset and Stable Diffusion inpainting which we plan to share with the community.</li>
</ul>

<h3>Title: On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jinfeng Zhuang, Yinrui Li, Runze Su, Ke Xu, Zhixuan Shao, Kungang Li, Ling Leng, Han Sun, Meng Qi, Yixiong Meng, Yang Tang, Zhifang Liu, Qifei Shen, Aayush Mudgal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08169">https://arxiv.org/abs/2504.08169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08169">https://arxiv.org/pdf/2504.08169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08169]] On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction(https://arxiv.org/abs/2504.08169)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. However, its performance for CVR prediction is unclear in the conversion ads setting, where an ad bids for the probability of a user's off-site actions on a third party website or app, including purchase, add to cart, sign up, etc. A few challenges in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve the best trade-off between efficiency and efficacy? 3) What hyper-parameters to choose in each feature-crossing module? Orthogonal to the model architecture, the input personalization features also significantly impact model performance with a high degree of freedom. In this paper, we attack this problem and present our contributions biased to the applied data science side, including: First, we propose a multitask learning framework with DHEN as the single backbone model architecture to predict all CVR tasks, with a detailed study on how to make DHEN work effectively in practice; Second, we build both on-site real-time user behavior sequences and off-site conversion event sequences for CVR prediction purposes, and conduct ablation study on its importance; Last but not least, we propose a self-supervised auxiliary loss to predict future actions in the input sequence, to help resolve the label sparseness issue in CVR prediction. Our method achieves state-of-the-art performance compared to previous single feature crossing modules with pre-trained user personalization features.</li>
</ul>

<h3>Title: GenXSS: an AI-Driven Framework for Automated Detection of XSS Attacks in WAFs</h3>
<ul>
<li><strong>Authors: </strong>Vahid Babaey, Arun Ravindran</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08176">https://arxiv.org/abs/2504.08176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08176">https://arxiv.org/pdf/2504.08176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08176]] GenXSS: an AI-Driven Framework for Automated Detection of XSS Attacks in WAFs(https://arxiv.org/abs/2504.08176)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>The increasing reliance on web services has led to a rise in cybersecurity threats, particularly Cross-Site Scripting (XSS) attacks, which target client-side layers of web applications by injecting malicious scripts. Traditional Web Application Firewalls (WAFs) struggle to detect highly obfuscated and complex attacks, as their rules require manual updates. This paper presents a novel generative AI framework that leverages Large Language Models (LLMs) to enhance XSS mitigation. The framework achieves two primary objectives: (1) generating sophisticated and syntactically validated XSS payloads using in-context learning, and (2) automating defense mechanisms by testing these attacks against a vulnerable application secured by a WAF, classifying bypassing attacks, and generating effective WAF security rules. Experimental results using GPT-4o demonstrate the framework's effectiveness generating 264 XSS payloads, 83% of which were validated, with 80% bypassing ModSecurity WAF equipped with an industry standard security rule set developed by the Open Web Application Security Project (OWASP) to protect against web vulnerabilities. Through rule generation, 86% of previously successful attacks were blocked using only 15 new rules. In comparison, Google Gemini Pro achieved a lower bypass rate of 63%, highlighting performance differences across LLMs.</li>
</ul>

<h3>Title: TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruineng Li, Daitao Xing, Huiming Sun, Yuanzhou Ha, Jinglin Shen, Chiuman Ho</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08181">https://arxiv.org/abs/2504.08181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08181">https://arxiv.org/pdf/2504.08181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08181]] TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation(https://arxiv.org/abs/2504.08181)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human-centric motion control in video generation remains a critical challenge, particularly when jointly controlling camera movements and human poses in scenarios like the iconic Grammy Glambot moment. While recent video diffusion models have made significant progress, existing approaches struggle with limited motion representations and inadequate integration of camera and human motion controls. In this work, we present TokenMotion, the first DiT-based video diffusion framework that enables fine-grained control over camera motion, human motion, and their joint interaction. We represent camera trajectories and human poses as spatio-temporal tokens to enable local control granularity. Our approach introduces a unified modeling framework utilizing a decouple-and-fuse strategy, bridged by a human-aware dynamic mask that effectively handles the spatially-and-temporally varying nature of combined motion signals. Through extensive experiments, we demonstrate TokenMotion's effectiveness across both text-to-video and image-to-video paradigms, consistently outperforming current state-of-the-art methods in human-centric motion control tasks. Our work represents a significant advancement in controllable video generation, with particular relevance for creative production applications.</li>
</ul>

<h3>Title: DaemonSec: Examining the Role of Machine Learning for Daemon Security in Linux Environments</h3>
<ul>
<li><strong>Authors: </strong>Sheikh Muhammad Farjad</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08227">https://arxiv.org/abs/2504.08227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08227">https://arxiv.org/pdf/2504.08227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08227]] DaemonSec: Examining the Role of Machine Learning for Daemon Security in Linux Environments(https://arxiv.org/abs/2504.08227)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>DaemonSec is an early-stage startup exploring machine learning (ML)-based security for Linux daemons, a critical yet often overlooked attack surface. While daemon security remains underexplored, conventional defenses struggle against adaptive threats and zero-day exploits. To assess the perspectives of IT professionals on ML-driven daemon protection, a systematic interview study based on semi-structured interviews was conducted with 22 professionals from industry and academia. The study evaluates adoption, feasibility, and trust in ML-based security solutions. While participants recognized the potential of ML for real-time anomaly detection, findings reveal skepticism toward full automation, limited security awareness among non-security roles, and concerns about patching delays creating attack windows. This paper presents the methods, key findings, and implications for advancing ML-driven daemon security in industry.</li>
</ul>

<h3>Title: Understanding the Impact of Data Domain Extraction on Synthetic Data Privacy</h3>
<ul>
<li><strong>Authors: </strong>Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08254">https://arxiv.org/abs/2504.08254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08254">https://arxiv.org/pdf/2504.08254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08254]] Understanding the Impact of Data Domain Extraction on Synthetic Data Privacy(https://arxiv.org/abs/2504.08254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Privacy attacks, particularly membership inference attacks (MIAs), are widely used to assess the privacy of generative models for tabular synthetic data, including those with Differential Privacy (DP) guarantees. These attacks often exploit outliers, which are especially vulnerable due to their position at the boundaries of the data domain (e.g., at the minimum and maximum values). However, the role of data domain extraction in generative models and its impact on privacy attacks have been overlooked. In this paper, we examine three strategies for defining the data domain: assuming it is externally provided (ideally from public data), extracting it directly from the input data, and extracting it with DP mechanisms. While common in popular implementations and libraries, we show that the second approach breaks end-to-end DP guarantees and leaves models vulnerable. While using a provided domain (if representative) is preferable, extracting it with DP can also defend against popular MIAs, even at high privacy budgets.</li>
</ul>

<h3>Title: CoProSketch: Controllable and Progressive Sketch Generation with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Ruohao Zhan, Yijin Li, Yisheng He, Shuo Chen, Yichen Shen, Xinyu Chen, Zilong Dong, Zhaoyang Huang, Guofeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08259">https://arxiv.org/abs/2504.08259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08259">https://arxiv.org/pdf/2504.08259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08259]] CoProSketch: Controllable and Progressive Sketch Generation with Diffusion Model(https://arxiv.org/abs/2504.08259)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Sketches serve as fundamental blueprints in artistic creation because sketch editing is easier and more intuitive than pixel-level RGB image editing for painting artists, yet sketch generation remains unexplored despite advancements in generative models. We propose a novel framework CoProSketch, providing prominent controllability and details for sketch generation with diffusion models. A straightforward method is fine-tuning a pretrained image generation diffusion model with binarized sketch images. However, we find that the diffusion models fail to generate clear binary images, which makes the produced sketches chaotic. We thus propose to represent the sketches by unsigned distance field (UDF), which is continuous and can be easily decoded to sketches through a lightweight network. With CoProSketch, users generate a rough sketch from a bounding box and a text prompt. The rough sketch can be manually edited and fed back into the model for iterative refinement and will be decoded to a detailed sketch as the final result. Additionally, we curate the first large-scale text-sketch paired dataset as the training data. Experiments demonstrate superior semantic consistency and controllability over baselines, offering a practical solution for integrating user feedback into generative workflows.</li>
</ul>

<h3>Title: Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Yonchanok Khaokaew, Flora D. Salim, Andreas ZÃ¼fle, Hao Xue, Taylor Anderson, Matthew Scotch, David J Heslop</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08260">https://arxiv.org/abs/2504.08260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08260">https://arxiv.org/pdf/2504.08260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08260]] Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare(https://arxiv.org/abs/2504.08260)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies.</li>
</ul>

<h3>Title: To See or Not to See -- Fingerprinting Devices in Adversarial Environments Amid Advanced Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Justin Feng, Nader Sehatbakhsh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08264">https://arxiv.org/abs/2504.08264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08264">https://arxiv.org/pdf/2504.08264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08264]] To See or Not to See -- Fingerprinting Devices in Adversarial Environments Amid Advanced Machine Learning(https://arxiv.org/abs/2504.08264)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing use of the Internet of Things raises security concerns. To address this, device fingerprinting is often employed to authenticate devices, detect adversaries, and identify eavesdroppers in an environment. This requires the ability to discern between legitimate and malicious devices which is achieved by analyzing the unique physical and/or operational characteristics of IoT devices. In the era of the latest progress in machine learning, particularly generative models, it is crucial to methodically examine the current studies in device fingerprinting. This involves explaining their approaches and underscoring their limitations when faced with adversaries armed with these ML tools. To systematically analyze existing methods, we propose a generic, yet simplified, model for device fingerprinting. Additionally, we thoroughly investigate existing methods to authenticate devices and detect eavesdropping, using our proposed model. We further study trends and similarities between works in authentication and eavesdropping detection and present the existing threats and attacks in these domains. Finally, we discuss future directions in fingerprinting based on these trends to develop more secure IoT fingerprinting schemes.</li>
</ul>

<h3>Title: Palmprint De-Identification Using Diffusion Model for High-Quality and Diverse Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Licheng Yan, Bob Zhang, Andrew Beng Jin Teoh, Lu Leng, Shuyi Li, Yuqi Wang, Ziyuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08272">https://arxiv.org/abs/2504.08272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08272">https://arxiv.org/pdf/2504.08272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08272]] Palmprint De-Identification Using Diffusion Model for High-Quality and Diverse Synthesis(https://arxiv.org/abs/2504.08272)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Palmprint recognition techniques have advanced significantly in recent years, enabling reliable recognition even when palmprints are captured in uncontrolled or challenging environments. However, this strength also introduces new risks, as publicly available palmprint images can be misused by adversaries for malicious activities. Despite this growing concern, research on methods to obscure or anonymize palmprints remains largely unexplored. Thus, it is essential to develop a palmprint de-identification technique capable of removing identity-revealing features while retaining the image's utility and preserving non-sensitive information. In this paper, we propose a training-free framework that utilizes pre-trained diffusion models to generate diverse, high-quality palmprint images that conceal identity features for de-identification purposes. To ensure greater stability and controllability in the synthesis process, we incorporate a semantic-guided embedding fusion alongside a prior interpolation mechanism. We further propose the de-identification ratio, a novel metric for intuitive de-identification assessment. Extensive experiments across multiple palmprint datasets and recognition methods demonstrate that our method effectively conceals identity-related traits with significant diversity across de-identified samples. The de-identified samples preserve high visual fidelity and maintain excellent usability, achieving a balance between de-identification and retaining non-identity information.</li>
</ul>

<h3>Title: DreamFuse: Adaptive Image Fusion with Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Junjia Huang, Pengxiang Yan, Jiyang Liu, Jie Wu, Zhao Wang, Yitong Wang, Liang Lin, Guanbin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08291">https://arxiv.org/abs/2504.08291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08291">https://arxiv.org/pdf/2504.08291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08291]] DreamFuse: Adaptive Image Fusion with Diffusion Transformer(https://arxiv.org/abs/2504.08291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image fusion seeks to seamlessly integrate foreground objects with background scenes, producing realistic and harmonious fused images. Unlike existing methods that directly insert objects into the background, adaptive and interactive fusion remains a challenging yet appealing task. It requires the foreground to adjust or interact with the background context, enabling more coherent integration. To address this, we propose an iterative human-in-the-loop data generation pipeline, which leverages limited initial data with diverse textual prompts to generate fusion datasets across various scenarios and interactions, including placement, holding, wearing, and style transfer. Building on this, we introduce DreamFuse, a novel approach based on the Diffusion Transformer (DiT) model, to generate consistent and harmonious fused images with both foreground and background information. DreamFuse employs a Positional Affine mechanism to inject the size and position of the foreground into the background, enabling effective foreground-background interaction through shared attention. Furthermore, we apply Localized Direct Preference Optimization guided by human feedback to refine DreamFuse, enhancing background consistency and foreground harmony. DreamFuse achieves harmonious fusion while generalizing to text-driven attribute editing of the fused results. Experimental results demonstrate that our method outperforms state-of-the-art approaches across multiple metrics.</li>
</ul>

<h3>Title: Generative AI for Film Creation: A Survey of Recent Advances</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao Kong, Nix Liu Xin, Shanshan Jiang, Praagya Bahuguna, Mark Chan, Khushi Hora, Lijian Yang, Yongqi Liang, Runhe Bian, Yunlei Liu, Isabela Campillo Valencia, Patricia Morales Tredinick, Ilia Kozlov, Sijia Jiang, Peiwen Huang, Na Chen, Xuanxuan Liu, Anyi Rao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08296">https://arxiv.org/abs/2504.08296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08296">https://arxiv.org/pdf/2504.08296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08296]] Generative AI for Film Creation: A Survey of Recent Advances(https://arxiv.org/abs/2504.08296)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) is transforming filmmaking, equipping artists with tools like text-to-image and image-to-video diffusion, neural radiance fields, avatar generation, and 3D synthesis. This paper examines the adoption of these technologies in filmmaking, analyzing workflows from recent AI-driven films to understand how GenAI contributes to character creation, aesthetic styling, and narration. We explore key strategies for maintaining character consistency, achieving stylistic coherence, and ensuring motion continuity. Additionally, we highlight emerging trends such as the growing use of 3D generation and the integration of real footage with AI-generated elements. Beyond technical advancements, we examine how GenAI is enabling new artistic expressions, from generating hard-to-shoot footage to dreamlike diffusion-based morphing effects, abstract visuals, and unworldly objects. We also gather artists' feedback on challenges and desired improvements, including consistency, controllability, fine-grained editing, and motion refinement. Our study provides insights into the evolving intersection of AI and filmmaking, offering a roadmap for researchers and artists navigating this rapidly expanding field.</li>
</ul>

<h3>Title: EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video Generation Based on Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Renda Li, Xiaohua Qi, Qiang Ling, Jun Yu, Ziyi Chen, Peng Chang, Mei HanJing Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08344">https://arxiv.org/abs/2504.08344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08344">https://arxiv.org/pdf/2504.08344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08344]] EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video Generation Based on Diffusion Model(https://arxiv.org/abs/2504.08344)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Audio-driven cospeech video generation typically involves two stages: speech-to-gesture and gesture-to-video. While significant advances have been made in speech-to-gesture generation, synthesizing natural expressions and gestures remains challenging in gesture-to-video systems. In order to improve the generation effect, previous works adopted complex input and training strategies and required a large amount of data sets for pre-training, which brought inconvenience to practical applications. We propose a simple one-stage training method and a temporal inference method based on a diffusion model to synthesize realistic and continuous gesture videos without the need for additional training of temporal this http URL entire model makes use of existing pre-trained weights, and only a few thousand frames of data are needed for each character at a time to complete fine-tuning. Built upon the video generator, we introduce a new audio-to-video pipeline to synthesize co-speech videos, using 2D human skeleton as the intermediate motion representation. Our experiments show that our method outperforms existing GAN-based and diffusion-based methods.</li>
</ul>

<h3>Title: Geometric Consistency Refinement for Single Image Novel View Synthesis via Test-Time Adaptation of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Josef Bengtson, David Nilsson, Fredrik Kahl</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08348">https://arxiv.org/abs/2504.08348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08348">https://arxiv.org/pdf/2504.08348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08348]] Geometric Consistency Refinement for Single Image Novel View Synthesis via Test-Time Adaptation of Diffusion Models(https://arxiv.org/abs/2504.08348)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models for single image novel view synthesis (NVS) can generate highly realistic and plausible images, but they are limited in the geometric consistency to the given relative poses. The generated images often show significant errors with respect to the epipolar constraints that should be fulfilled, as given by the target pose. In this paper we address this issue by proposing a methodology to improve the geometric correctness of images generated by a diffusion model for single image NVS. We formulate a loss function based on image matching and epipolar constraints, and optimize the starting noise in a diffusion sampling process such that the generated image should both be a realistic image and fulfill geometric constraints derived from the given target pose. Our method does not require training data or fine-tuning of the diffusion models, and we show that we can apply it to multiple state-of-the-art models for single image NVS. The method is evaluated on the MegaScenes dataset and we show that geometric consistency is improved compared to the baseline models while retaining the quality of the generated images.</li>
</ul>

<h3>Title: MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft</h3>
<ul>
<li><strong>Authors: </strong>Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08388">https://arxiv.org/abs/2504.08388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08388">https://arxiv.org/pdf/2504.08388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08388]] MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft(https://arxiv.org/abs/2504.08388)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate $4$ to $7$ frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.</li>
</ul>

<h3>Title: GeoTexBuild: 3D Building Model Generation from Map Footprints</h3>
<ul>
<li><strong>Authors: </strong>Ruizhe Wang, Junyan Yang, Qiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08419">https://arxiv.org/abs/2504.08419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08419">https://arxiv.org/pdf/2504.08419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08419]] GeoTexBuild: 3D Building Model Generation from Map Footprints(https://arxiv.org/abs/2504.08419)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce GeoTexBuild, a modular generative framework for creating 3D building models from map footprints. The proposed framework employs a three-stage process comprising height map generation, geometry reconstruction, and appearance stylization, culminating in building models with intricate geometry and appearance attributes. By integrating customized ControlNet and Text2Mesh models, we explore effective methods for controlling both geometric and visual attributes during the generation process. By this, we eliminate the problem of structural variations behind a single facade photo of the existing 3D generation techniques. Experimental results at each stage validate the capability of GeoTexBuild to generate detailed and accurate building models from footprints derived from site planning or map designs. Our framework significantly reduces manual labor in modeling buildings and can offer inspiration for designers.</li>
</ul>

<h3>Title: Customizing Spider Silk: Generative Models with Mechanical Property Conditioning for Protein Engineering</h3>
<ul>
<li><strong>Authors: </strong>Neeru Dubey, Elin Karlsson, Miguel Angel Redondo, Johan ReimegÃ¥rd, Anna Rising, Hedvig KjellstrÃ¶m</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08437">https://arxiv.org/abs/2504.08437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08437">https://arxiv.org/pdf/2504.08437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08437]] Customizing Spider Silk: Generative Models with Mechanical Property Conditioning for Protein Engineering(https://arxiv.org/abs/2504.08437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The remarkable mechanical properties of spider silk, including its tensile strength and extensibility, are primarily governed by the repetitive regions of the proteins that constitute the fiber, the major ampullate spidroins (MaSps). However, establishing correlations between mechanical characteristics and repeat sequences is challenging due to the intricate sequence-structure-function relationships of MaSps and the limited availability of annotated datasets. In this study, we present a novel computational framework for designing MaSp repeat sequences with customizable mechanical properties. To achieve this, we developed a lightweight GPT-based generative model by distilling the pre-trained ProtGPT2 protein language model. The distilled model was subjected to multilevel fine-tuning using curated subsets of the Spider Silkome dataset. Specifically, we adapt the model for MaSp repeat generation using 6,000 MaSp repeat sequences and further refine it with 572 repeats associated with experimentally determined fiber-level mechanical properties. Our model generates biologically plausible MaSp repeat regions tailored to specific mechanical properties while also predicting those properties for given sequences. Validation includes sequence-level analysis, assessing physicochemical attributes and expected distribution of key motifs as well as secondary structure compositions. A correlation study using BLAST on the Spider Silkome dataset and a test set of MaSp repeats with known mechanical properties further confirmed the predictive accuracy of the model. This framework advances the rational design of spider silk-inspired biomaterials, offering a versatile tool for engineering protein sequences with tailored mechanical attributes.</li>
</ul>

<h3>Title: SARFormer -- An Acquisition Parameter Aware Vision Transformer for Synthetic Aperture Radar Data</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Prexl, Michael Recla, Michael Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08441">https://arxiv.org/abs/2504.08441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08441">https://arxiv.org/pdf/2504.08441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08441]] SARFormer -- An Acquisition Parameter Aware Vision Transformer for Synthetic Aperture Radar Data(https://arxiv.org/abs/2504.08441)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This manuscript introduces SARFormer, a modified Vision Transformer (ViT) architecture designed for processing one or multiple synthetic aperture radar (SAR) images. Given the complex image geometry of SAR data, we propose an acquisition parameter encoding module that significantly guides the learning process, especially in the case of multiple images, leading to improved performance on downstream tasks. We further explore self-supervised pre-training, conduct experiments with limited labeled data, and benchmark our contribution and adaptations thoroughly in ablation experiments against a baseline, where the model is tested on tasks such as height reconstruction and segmentation. Our approach achieves up to 17% improvement in terms of RMSE over baseline models</li>
</ul>

<h3>Title: Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via Optimized Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Weiye Chen, Qingen Zhu, Qian Long</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08451">https://arxiv.org/abs/2504.08451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08451">https://arxiv.org/pdf/2504.08451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08451]] Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via Optimized Latent Diffusion(https://arxiv.org/abs/2504.08451)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in visual synthesis have leveraged diffusion models and attention mechanisms to achieve high-fidelity artistic style transfer and photorealistic text-to-image generation. However, real-time deployment on edge devices remains challenging due to computational and memory constraints. We propose Muon-AD, a co-designed framework that integrates the Muon optimizer with attention distillation for real-time edge synthesis. By eliminating gradient conflicts through orthogonal parameter updates and dynamic pruning, Muon-AD achieves 3.2 times faster convergence compared to Stable Diffusion-TensorRT, while maintaining synthesis quality (15% lower FID, 4% higher SSIM). Our framework reduces peak memory to 7GB on Jetson Orin and enables 24FPS real-time generation through mixed-precision quantization and curriculum learning. Extensive experiments on COCO-Stuff and ImageNet-Texture demonstrate Muon-AD's Pareto-optimal efficiency-quality trade-offs. Here, we show a 65% reduction in communication overhead during distributed training and real-time 10s/image generation on edge GPUs. These advancements pave the way for democratizing high-quality visual synthesis in resource-constrained environments.</li>
</ul>

<h3>Title: Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Bram Vanherle, Brent Zoomers, Jeroen Put, Frank Van Reeth, Nick Michiels</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08473">https://arxiv.org/abs/2504.08473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08473">https://arxiv.org/pdf/2504.08473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08473]] Cut-and-Splat: Leveraging Gaussian Splatting for Synthetic Data Generation(https://arxiv.org/abs/2504.08473)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating synthetic images is a useful method for cheaply obtaining labeled data for training computer vision models. However, obtaining accurate 3D models of relevant objects is necessary, and the resulting images often have a gap in realism due to challenges in simulating lighting effects and camera artifacts. We propose using the novel view synthesis method called Gaussian Splatting to address these challenges. We have developed a synthetic data pipeline for generating high-quality context-aware instance segmentation training data for specific objects. This process is fully automated, requiring only a video of the target object. We train a Gaussian Splatting model of the target object and automatically extract the object from the video. Leveraging Gaussian Splatting, we then render the object on a random background image, and monocular depth estimation is employed to place the object in a believable pose. We introduce a novel dataset to validate our approach and show superior performance over other data generation approaches, such as Cut-and-Paste and Diffusion model-based generation.</li>
</ul>

<h3>Title: Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Tommaso Galliena, Tommaso Apicella, Stefano Rosa, Pietro Morerio, Alessio Del Bue, Lorenzo Natale</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08531">https://arxiv.org/abs/2504.08531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08531">https://arxiv.org/pdf/2504.08531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08531]] Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions(https://arxiv.org/abs/2504.08531)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at this https URL</li>
</ul>

<h3>Title: Discriminator-Free Direct Preference Optimization for Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Haoran Cheng, Qide Dong, Liang Peng, Zhizhou Sha, Weiguo Feng, Jinghui Xie, Zhao Song, Shilei Wen, Xiaofei He, Boxi Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08542">https://arxiv.org/abs/2504.08542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08542">https://arxiv.org/pdf/2504.08542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08542]] Discriminator-Free Direct Preference Optimization for Video Diffusion(https://arxiv.org/abs/2504.08542)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO), which aligns models with human preferences through win/lose data pairs, has achieved remarkable success in language and image generation. However, applying DPO to video diffusion models faces critical challenges: (1) Data inefficiency. Generating thousands of videos per DPO iteration incurs prohibitive costs; (2) Evaluation uncertainty. Human annotations suffer from subjective bias, and automated discriminators fail to detect subtle temporal artifacts like flickering or motion incoherence. To address these, we propose a discriminator-free video DPO framework that: (1) Uses original real videos as win cases and their edited versions (e.g., reversed, shuffled, or noise-corrupted clips) as lose cases; (2) Trains video diffusion models to distinguish and avoid artifacts introduced by editing. This approach eliminates the need for costly synthetic video comparisons, provides unambiguous quality signals, and enables unlimited training data expansion through simple editing operations. We theoretically prove the framework's effectiveness even when real videos and model-generated videos follow different distributions. Experiments on CogVideoX demonstrate the efficiency of the proposed method.</li>
</ul>

<h3>Title: Slicing the Gaussian Mixture Wasserstein Distance</h3>
<ul>
<li><strong>Authors: </strong>Moritz Piening, Robert Beinert</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08544">https://arxiv.org/abs/2504.08544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08544">https://arxiv.org/pdf/2504.08544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08544]] Slicing the Gaussian Mixture Wasserstein Distance(https://arxiv.org/abs/2504.08544)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (MW) distance adapts the Wasserstein metric to GMMs and has been applied in various domains, including domain adaptation, dataset comparison, and reinforcement learning. However, its high computational cost -- arising from repeated Wasserstein distance computations involving matrix square root estimations and an expensive linear program -- limits its scalability to high-dimensional and large-scale problems. To address this, we propose multiple novel slicing-based approximations to the MW distance that significantly reduce computational complexity while preserving key optimal transport properties. From a theoretical viewpoint, we establish several weak and strong equivalences between the introduced metrics, and show the relations to the original MW distance and the well-established sliced Wasserstein distance. Furthermore, we validate the effectiveness of our approach through numerical experiments, demonstrating computational efficiency and applications in clustering, perceptual image comparison, and GMM minimization</li>
</ul>

<h3>Title: Boosting multi-demographic federated learning for chest x-ray analysis using general-purpose self-supervised representations</h3>
<ul>
<li><strong>Authors: </strong>Mahshad Lotfinia, Arash Tayebiarasteh, Samaneh Samiei, Mehdi Joodaki, Soroosh Tayebi Arasteh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08584">https://arxiv.org/abs/2504.08584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08584">https://arxiv.org/pdf/2504.08584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08584]] Boosting multi-demographic federated learning for chest x-ray analysis using general-purpose self-supervised representations(https://arxiv.org/abs/2504.08584)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non-independent and identically distributed (non-IID) settings, where institutions with more representative data may experience degraded performance. Moreover, existing large-scale FL studies have been limited to adult datasets, neglecting the unique challenges posed by pediatric data, which introduces additional non-IID variability. To address these limitations, we analyzed n=398,523 adult chest radiographs from diverse institutions across multiple countries and n=9,125 pediatric images, leveraging transfer learning from general-purpose self-supervised image representations to classify pneumonia and cases with no abnormality. Using state-of-the-art vision transformers, we found that FL improved performance only for smaller adult datasets (P<0.001) but degraded performance for larger datasets (P<0.064) and pediatric cases (P=0.242). However, equipping FL with self-supervised weights significantly enhanced outcomes across pediatric cases (P=0.031) and most adult datasets (P<0.008), except the largest dataset (P=0.052). These findings underscore the potential of easily deployable general-purpose self-supervised image representations to address non-IID challenges in clinical FL applications and highlight their promise for enhancing patient outcomes and advancing pediatric healthcare, where data scarcity and variability remain persistent obstacles.</li>
</ul>

<h3>Title: ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Yongsheng Yu, Haitian Zheng, Zhifei Zhang, Jianming Zhang, Yuqian Zhou, Connelly Barnes, Yuchen Liu, Wei Xiong, Zhe Lin, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08591">https://arxiv.org/abs/2504.08591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08591">https://arxiv.org/pdf/2504.08591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08591]] ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration(https://arxiv.org/abs/2504.08591)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces a critical trade-off between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, a novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs a highly compressed latent representation that compresses image 32x, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that structures the latent space into sub-bands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs.</li>
</ul>

<h3>Title: Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Gabriele Lozupone, Alessandro Bria, Francesco Fontanella, Frederick J.A. Meijer, Claudio De Stefano, Henkjan Huisman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08635">https://arxiv.org/abs/2504.08635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08635">https://arxiv.org/pdf/2504.08635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08635]] Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging(https://arxiv.org/abs/2504.08635)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at this https URL</li>
</ul>

<h3>Title: Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization</h3>
<ul>
<li><strong>Authors: </strong>Jialu Li, Shoubin Yu, Han Lin, Jaemin Cho, Jaehong Yoon, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08641">https://arxiv.org/abs/2504.08641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08641">https://arxiv.org/pdf/2504.08641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08641]] Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization(https://arxiv.org/abs/2504.08641)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. A recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt a large T2V model as a backbone. To address this, we introduce Video-MSG, a training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. Video-MSG consists of three steps, where in the first two steps, Video-MSG creates Video Sketch, a fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, Video-MSG guides a downstream T2V diffusion model with Video Sketch through noise inversion and denoising. Notably, Video-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. Video-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation.</li>
</ul>

<h3>Title: The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Masashi Hatano, Zhifan Zhu, Hideo Saito, Dima Damen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08654">https://arxiv.org/abs/2504.08654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08654">https://arxiv.org/pdf/2504.08654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08654]] The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation(https://arxiv.org/abs/2504.08654)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Forecasting hand motion and pose from an egocentric perspective is essential for understanding human intention. However, existing methods focus solely on predicting positions without considering articulation, and only when the hands are visible in the field of view. This limitation overlooks the fact that approximate hand positions can still be inferred even when they are outside the camera's view. In this paper, we propose a method to forecast the 3D trajectories and poses of both hands from an egocentric video, both in and out of the field of view. We propose a diffusion-based transformer architecture for Egocentric Hand Forecasting, EgoH4, which takes as input the observation sequence and camera poses, then predicts future 3D motion and poses for both hands of the camera wearer. We leverage full-body pose information, allowing other joints to provide constraints on hand motion. We denoise the hand and body joints along with a visibility predictor for hand joints and a 3D-to-2D reprojection loss that minimizes the error when hands are in-view. We evaluate EgoH4 on the Ego-Exo4D dataset, combining subsets with body and hand annotations. We train on 156K sequences and evaluate on 34K sequences, respectively. EgoH4 improves the performance by 3.4cm and 5.1cm over the baseline in terms of ADE for hand trajectory forecasting and MPJPE for hand pose forecasting. Project page: this https URL</li>
</ul>

<h3>Title: Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Zhiwu Qing, Fei Xiao, Meng Wei, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08685">https://arxiv.org/abs/2504.08685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08685">https://arxiv.org/pdf/2504.08685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08685]] Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model(https://arxiv.org/abs/2504.08685)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at this https URL</li>
</ul>

<h3>Title: Generating Fine Details of Entity Interactions</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Gu, Jiayuan Mao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.08714">https://arxiv.org/abs/2504.08714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.08714">https://arxiv.org/pdf/2504.08714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.08714]] Generating Fine Details of Entity Interactions(https://arxiv.org/abs/2504.08714)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Images not only depict objects but also encapsulate rich interactions between them. However, generating faithful and high-fidelity images involving multiple entities interacting with each other, is a long-standing challenge. While pre-trained text-to-image models are trained on large-scale datasets to follow diverse text instructions, they struggle to generate accurate interactions, likely due to the scarcity of training data for uncommon object interactions. This paper introduces InterActing, an interaction-focused dataset with 1000 fine-grained prompts covering three key scenarios: (1) functional and action-based interactions, (2) compositional spatial relationships, and (3) multi-subject interactions. To address interaction generation challenges, we propose a decomposition-augmented refinement procedure. Our approach, DetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose interactions into finer-grained concepts, uses a VLM to critique generated images, and applies targeted interventions within the diffusion process in refinement. Automatic and human evaluations show significantly improved image quality, demonstrating the potential of enhanced inference strategies. Our dataset and code are available at this https URL to facilitate future exploration of interaction-rich image generation.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
