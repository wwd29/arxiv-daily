<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions. (arXiv:2308.13142v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13142">http://arxiv.org/abs/2308.13142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13142]] A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions(http://arxiv.org/abs/2308.13142)</code></li>
<li>Summary: <p>Recently, there has been significant progress in the development of large
models. Following the success of ChatGPT, numerous language models have been
introduced, demonstrating remarkable performance. Similar advancements have
also been observed in image generation models, such as Google's Imagen model,
OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive
capabilities in generating images. However, similar to large language models,
these models still encounter unresolved challenges. Fortunately, the
availability of open-source stable diffusion models and their underlying
mathematical principles has enabled the academic community to extensively
analyze the performance of current image generation models and make
improvements based on this stable diffusion framework. This survey aims to
examine the existing issues and the current solutions pertaining to image
generation models.
</p></li>
</ul>

<h3>Title: Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model. (arXiv:2308.13164v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13164">http://arxiv.org/abs/2308.13164</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13164]] Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model(http://arxiv.org/abs/2308.13164)</code></li>
<li>Summary: <p>In this paper, we rethink the low-light image enhancement task and propose a
physically explainable and generative diffusion model for low-light image
enhancement, termed as Diff-Retinex. We aim to integrate the advantages of the
physical model and the generative network. Furthermore, we hope to supplement
and even deduce the information missing in the low-light image through the
generative network. Therefore, Diff-Retinex formulates the low-light image
enhancement problem into Retinex decomposition and conditional image
generation. In the Retinex decomposition, we integrate the superiority of
attention in Transformer and meticulously design a Retinex Transformer
decomposition network (TDN) to decompose the image into illumination and
reflectance maps. Then, we design multi-path generative diffusion networks to
reconstruct the normal-light Retinex probability distribution and solve the
various degradations in these components respectively, including dark
illumination, noise, color deviation, loss of scene contents, etc. Owing to
generative diffusion model, Diff-Retinex puts the restoration of low-light
subtle detail into practice. Extensive experiments conducted on real-world
low-light datasets qualitatively and quantitatively demonstrate the
effectiveness, superiority, and generalization of the proposed method.
</p></li>
</ul>

<h3>Title: EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior. (arXiv:2308.13223v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13223">http://arxiv.org/abs/2308.13223</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13223]] EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior(http://arxiv.org/abs/2308.13223)</code></li>
<li>Summary: <p>While the image diffusion model has made significant strides in text-driven
3D content creation, it often falls short in accurately capturing the intended
meaning of the text prompt, particularly with respect to direction information.
This shortcoming gives rise to the Janus problem, where multi-faced 3D models
are produced with the guidance of such diffusion models. In this paper, we
present a robust pipeline for generating high-fidelity 3D content with
orthogonal-view image guidance. Specifically, we introduce a novel 2D diffusion
model that generates an image consisting of four orthogonal-view sub-images for
the given text prompt. The 3D content is then created with this diffusion
model, which enhances 3D consistency and provides strong structured semantic
priors. This addresses the infamous Janus problem and significantly promotes
generation efficiency. Additionally, we employ a progressive 3D synthesis
strategy that results in substantial improvement in the quality of the created
3D contents. Both quantitative and qualitative evaluations show that our method
demonstrates a significant improvement over previous text-to-3D techniques.
</p></li>
</ul>

<h3>Title: Distribution-Aligned Diffusion for Human Mesh Recovery. (arXiv:2308.13369v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13369">http://arxiv.org/abs/2308.13369</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13369]] Distribution-Aligned Diffusion for Human Mesh Recovery(http://arxiv.org/abs/2308.13369)</code></li>
<li>Summary: <p>Recovering a 3D human mesh from a single RGB image is a challenging task due
to depth ambiguity and self-occlusion, resulting in a high degree of
uncertainty. Meanwhile, diffusion models have recently seen much success in
generating high-quality outputs by progressively denoising noisy inputs.
Inspired by their capability, we explore a diffusion-based approach for human
mesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework which
frames mesh recovery as a reverse diffusion process. We also propose a
Distribution Alignment Technique (DAT) that injects input-specific distribution
information into the diffusion process, and provides useful prior knowledge to
simplify the mesh recovery task. Our method achieves state-of-the-art
performance on three widely used datasets. Project page:
https://gongjia0208.github.io/HMDiff/.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: SurGNN: Explainable visual scene understanding and assessment of surgical skill using graph neural networks. (arXiv:2308.13073v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13073">http://arxiv.org/abs/2308.13073</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13073]] SurGNN: Explainable visual scene understanding and assessment of surgical skill using graph neural networks(http://arxiv.org/abs/2308.13073)</code></li>
<li>Summary: <p>This paper explores how graph neural networks (GNNs) can be used to enhance
visual scene understanding and surgical skill assessment. By using GNNs to
analyze the complex visual data of surgical procedures represented as graph
structures, relevant features can be extracted and surgical skill can be
predicted. Additionally, GNNs provide interpretable results, revealing the
specific actions, instruments, or anatomical structures that contribute to the
predicted skill metrics. This can be highly beneficial for surgical educators
and trainees, as it provides valuable insights into the factors that contribute
to successful surgical performance and outcomes. SurGNN proposes two concurrent
approaches -- one supervised and the other self-supervised. The paper also
briefly discusses other automated surgical skill evaluation techniques and
highlights the limitations of hand-crafted features in capturing the
intricacies of surgical expertise. We use the proposed methods to achieve
state-of-the-art results on EndoVis19, and custom datasets. The working
implementation of the code can be found at https://github.com/&lt;redacted&gt;.
</p></li>
</ul>

<h3>Title: Preserving Modality Structure Improves Multi-Modal Learning. (arXiv:2308.13077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13077">http://arxiv.org/abs/2308.13077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13077]] Preserving Modality Structure Improves Multi-Modal Learning(http://arxiv.org/abs/2308.13077)</code></li>
<li>Summary: <p>Self-supervised learning on large-scale multi-modal datasets allows learning
semantically meaningful embeddings in a joint multi-modal representation space
without relying on human annotations. These joint embeddings enable zero-shot
cross-modal tasks like retrieval and classification. However, these methods
often struggle to generalize well on out-of-domain data as they ignore the
semantic structure present in modality-specific embeddings. In this context, we
propose a novel Semantic-Structure-Preserving Consistency approach to improve
generalizability by preserving the modality-specific relationships in the joint
embedding space. To capture modality-specific semantic relationships between
samples, we propose to learn multiple anchors and represent the multifaceted
relationship between samples with respect to their relationship with these
anchors. To assign multiple anchors to each sample, we propose a novel
Multi-Assignment Sinkhorn-Knopp algorithm. Our experimentation demonstrates
that our proposed approach learns semantically meaningful anchors in a
self-supervised manner. Furthermore, our evaluation on MSR-VTT and YouCook2
datasets demonstrates that our proposed multi-anchor assignment based solution
achieves state-of-the-art performance and generalizes to both inand
out-of-domain datasets. Code: https://github.com/Swetha5/Multi_Sinkhorn_Knopp
</p></li>
</ul>

<h3>Title: Self-supervised Scene Text Segmentation with Object-centric Layered Representations Augmented by Text Regions. (arXiv:2308.13178v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13178">http://arxiv.org/abs/2308.13178</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13178]] Self-supervised Scene Text Segmentation with Object-centric Layered Representations Augmented by Text Regions(http://arxiv.org/abs/2308.13178)</code></li>
<li>Summary: <p>Text segmentation tasks have a very wide range of application values, such as
image editing, style transfer, watermark removal, etc.However, existing public
datasets are of poor quality of pixel-level labels that have been shown to be
notoriously costly to acquire, both in terms of money and time. At the same
time, when pretraining is performed on synthetic datasets, the data
distribution of the synthetic datasets is far from the data distribution in the
real scene. These all pose a huge challenge to the current pixel-level text
segmentation algorithms.To alleviate the above problems, we propose a
self-supervised scene text segmentation algorithm with layered decoupling of
representations derived from the object-centric manner to segment images into
texts and background. In our method, we propose two novel designs which include
Region Query Module and Representation Consistency Constraints adapting to the
unique properties of text as complements to Auto Encoder, which improves the
network's sensitivity to texts.For this unique design, we treat the
polygon-level masks predicted by the text localization model as extra input
information, and neither utilize any pixel-level mask annotations for training
stage nor pretrain on synthetic datasets.Extensive experiments show the
effectiveness of the method proposed. On several public scene text datasets,
our method outperforms the state-of-the-art unsupervised segmentation
algorithms.
</p></li>
</ul>

<h3>Title: Self-supervised learning for hotspot detection and isolation from thermal images. (arXiv:2308.13204v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13204">http://arxiv.org/abs/2308.13204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13204]] Self-supervised learning for hotspot detection and isolation from thermal images(http://arxiv.org/abs/2308.13204)</code></li>
<li>Summary: <p>Hotspot detection using thermal imaging has recently become essential in
several industrial applications, such as security applications, health
applications, and equipment monitoring applications. Hotspot detection is of
utmost importance in industrial safety where equipment can develop anomalies.
Hotspots are early indicators of such anomalies. We address the problem of
hotspot detection in thermal images by proposing a self-supervised learning
approach. Self-supervised learning has shown potential as a competitive
alternative to their supervised learning counterparts but their application to
thermography has been limited. This has been due to lack of diverse data
availability, domain specific pre-trained models, standardized benchmarks, etc.
We propose a self-supervised representation learning approach followed by
fine-tuning that improves detection of hotspots by classification. The SimSiam
network based ensemble classifier decides whether an image contains hotspots or
not. Detection of hotspots is followed by precise hotspot isolation. By doing
so, we are able to provide a highly accurate and precise hotspot
identification, applicable to a wide range of applications. We created a novel
large thermal image dataset to address the issue of paucity of easily
accessible thermal images. Our experiments with the dataset created by us and a
publicly available segmentation dataset show the potential of our approach for
hotspot detection and its ability to isolate hotspots with high accuracy. We
achieve a Dice Coefficient of 0.736, the highest when compared with existing
hotspot identification techniques. Our experiments also show self-supervised
learning as a strong contender of supervised learning, providing competitive
metrics for hotspot detection, with the highest accuracy of our approach being
97%.
</p></li>
</ul>

<h3>Title: Self-Supervised Representation Learning with Cross-Context Learning between Global and Hypercolumn Features. (arXiv:2308.13392v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13392">http://arxiv.org/abs/2308.13392</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13392]] Self-Supervised Representation Learning with Cross-Context Learning between Global and Hypercolumn Features(http://arxiv.org/abs/2308.13392)</code></li>
<li>Summary: <p>Whilst contrastive learning yields powerful representations by matching
different augmented views of the same instance, it lacks the ability to capture
the similarities between different instances. One popular way to address this
limitation is by learning global features (after the global pooling) to capture
inter-instance relationships based on knowledge distillation, where the global
features of the teacher are used to guide the learning of the global features
of the student. Inspired by cross-modality learning, we extend this existing
framework that only learns from global features by encouraging the global
features and intermediate layer features to learn from each other. This leads
to our novel self-supervised framework: cross-context learning between global
and hypercolumn features (CGH), that enforces the consistency of instance
relations between low- and high-level semantics. Specifically, we stack the
intermediate feature maps to construct a hypercolumn representation so that we
can measure instance relations using two contexts (hypercolumn and global
feature) separately, and then use the relations of one context to guide the
learning of the other. This cross-context learning allows the model to learn
from the differences between the two contexts. The experimental results on
linear classification and downstream tasks show that our method outperforms the
state-of-the-art methods.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13320">http://arxiv.org/abs/2308.13320</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13320]] Fine-tuning can cripple your foundation model; preserving features may be the solution(http://arxiv.org/abs/2308.13320)</code></li>
<li>Summary: <p>Pre-trained foundation models, owing primarily to their enormous capacity and
exposure to vast amount of training data scraped from the internet, enjoy the
advantage of storing knowledge about plenty of real-world concepts. Such models
are typically fine-tuned on downstream datasets to produce remarkable
state-of-the-art performances. While various fine-tuning methods have been
devised and are shown to be highly effective, we observe that a fine-tuned
model's ability to recognize concepts on tasks $\textit{different}$ from the
downstream one is reduced significantly compared to its pre-trained
counterpart. This is clearly undesirable as a huge amount of time and money
went into learning those very concepts in the first place. We call this
undesirable phenomenon "concept forgetting" and via experiments show that most
end-to-end fine-tuning approaches suffer heavily from this side effect. To this
end, we also propose a rather simple fix to this problem by designing a method
called LDIFS (short for $\ell_2$ distance in feature space) that simply
preserves the features of the original foundation model during fine-tuning. We
show that LDIFS significantly reduces concept forgetting without having
noticeable impact on the downstream task performance.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon. (arXiv:2308.13182v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13182">http://arxiv.org/abs/2308.13182</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13182]] Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon(http://arxiv.org/abs/2308.13182)</code></li>
<li>Summary: <p>With the advent of digital scanners and deep learning, diagnostic operations
may move from a microscope to a desktop. Hematoxylin and Eosin (H&amp;E) staining
is one of the most frequently used stains for disease analysis, diagnosis, and
grading, but pathologists do need different immunohistochemical (IHC) stains to
analyze specific structures or cells. Obtaining all of these stains (H&amp;E and
different IHCs) on a single specimen is a tedious and time-consuming task.
Consequently, virtual staining has emerged as an essential research direction.
Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for
synthesizing IHC stains from H&amp;E images, and vice versa. Our method expressly
incorporates structural information in the form of edges (in addition to color
data) and employs attention modules exclusively in the decoder of the proposed
generator model. This integration enhances feature localization and preserves
contextual information during the generation process. In addition, a structural
loss is incorporated to ensure accurate structure alignment between the
generated and input markers. To demonstrate the efficacy of the proposed model,
experiments are conducted with two IHC markers emphasizing distinct structures
of glands in the colon: the nucleus of epithelial cells (CDX2) and the
cytoplasm (CK818). Quantitative metrics such as FID and SSIM are frequently
used for the analysis of generative models, but they do not correlate
explicitly with higher-quality virtual staining results. Therefore, we propose
two new quantitative metrics that correlate directly with the virtual staining
specificity of IHC markers.
</p></li>
</ul>

<h3>Title: Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map. (arXiv:2308.13245v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13245">http://arxiv.org/abs/2308.13245</a></li>
<li>Code URL: https://github.com/naughtyzz/3d_facial_shape_attribute_translation_ssgmap</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13245]] Unpaired Multi-domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map(http://arxiv.org/abs/2308.13245)</code></li>
<li>Summary: <p>While impressive progress has recently been made in image-oriented facial
attribute translation, shape-oriented 3D facial attribute translation remains
an unsolved issue. This is primarily limited by the lack of 3D generative
models and ineffective usage of 3D facial data. We propose a learning framework
for 3D facial attribute translation to relieve these limitations. Firstly, we
customize a novel geometric map for 3D shape representation and embed it in an
end-to-end generative adversarial network. The geometric map represents 3D
shapes symmetrically on a square image grid, while preserving the neighboring
relationship of 3D vertices in a local least-square sense. This enables
effective learning for the latent representation of data with different
attributes. Secondly, we employ a unified and unpaired learning framework for
multi-domain attribute translation. It not only makes effective usage of data
correlation from multiple domains, but also mitigates the constraint for hardly
accessible paired data. Finally, we propose a hierarchical architecture for the
discriminator to guarantee robust results against both global and local
artifacts. We conduct extensive experiments to demonstrate the advantage of the
proposed framework over the state-of-the-art in generating high-fidelity facial
shapes. Given an input 3D facial shape, the proposed framework is able to
synthesize novel shapes of different attributes, which covers some downstream
applications, such as expression transfer, gender translation, and aging. Code
at https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap.
</p></li>
</ul>

<h3>Title: ARTIST: ARTificial Intelligence for Simplified Text. (arXiv:2308.13458v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13458">http://arxiv.org/abs/2308.13458</a></li>
<li>Code URL: https://github.com/delftcrowd/artist</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13458]] ARTIST: ARTificial Intelligence for Simplified Text(http://arxiv.org/abs/2308.13458)</code></li>
<li>Summary: <p>Complex text is a major barrier for many citizens when accessing public
information and knowledge. While often done manually, Text Simplification is a
key Natural Language Processing task that aims for reducing the linguistic
complexity of a text while preserving the original meaning. Recent advances in
Generative Artificial Intelligence (AI) have enabled automatic text
simplification both on the lexical and syntactical levels. However, as
applications often focus on English, little is understood about the
effectiveness of Generative AI techniques on low-resource languages such as
Dutch. For this reason, we carry out empirical studies to understand the
benefits and limitations of applying generative technologies for text
simplification and provide the following outcomes: 1) the design and
implementation for a configurable text simplification pipeline that
orchestrates state-of-the-art generative text simplification models, domain and
reader adaptation, and visualisation modules; 2) insights and lessons learned,
showing the strengths of automatic text simplification while exposing the
challenges in handling cultural and commonsense knowledge. These outcomes
represent a first step in the exploration of Dutch text simplification and shed
light on future endeavours both for research and practice.
</p></li>
</ul>

<h3>Title: ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching. (arXiv:2308.13062v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13062">http://arxiv.org/abs/2308.13062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13062]] ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching(http://arxiv.org/abs/2308.13062)</code></li>
<li>Summary: <p>Security critical software, e.g., OpenSSL, comes with numerous side-channel
leakages left unpatched due to a lack of resources or experts. The situation
will only worsen as the pace of code development accelerates, with developers
relying on Large Language Models (LLMs) to automatically generate code. In this
work, we explore the use of LLMs in generating patches for vulnerable code with
microarchitectural side-channel leakages. For this, we investigate the
generative abilities of powerful LLMs by carefully crafting prompts following a
zero-shot learning approach. All generated code is dynamically analyzed by
leakage detection tools, which are capable of pinpointing information leakage
at the instruction level leaked either from secret dependent accesses or
branches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts
are used to generate candidate replacements for vulnerable code, which are then
analyzed for correctness and for leakage resilience. From a cost/performance
perspective, the GPT4-based configuration costs in API calls a mere few cents
per vulnerability fixed. Our results show that LLM-based patching is far more
cost-effective and thus provides a scalable solution. Finally, the framework we
propose will improve in time, especially as vulnerability detection tools and
LLMs mature.
</p></li>
</ul>

<h3>Title: Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13265">http://arxiv.org/abs/2308.13265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13265]] Heterogeneous Federated Learning via Personalized Generative Networks(http://arxiv.org/abs/2308.13265)</code></li>
<li>Summary: <p>Federated Learning (FL) allows several clients to construct a common global
machine-learning model without having to share their data. FL, however, faces
the challenge of statistical heterogeneity between the client's data, which
degrades performance and slows down the convergence toward the global model. In
this paper, we provide theoretical proof that minimizing heterogeneity between
clients facilitates the convergence of a global model for every single client.
This becomes particularly important under empirical concept shifts among
clients, rather than merely considering imbalanced classes, which have been
studied until now. Therefore, we propose a method for knowledge transfer
between clients where the server trains client-specific generators. Each
generator generates samples for the corresponding client to remove the conflict
with other clients' models. Experiments conducted on synthetic and real data,
along with a theoretical study, support the effectiveness of our method in
constructing a well-generalizable global model by reducing the conflict between
local models.
</p></li>
</ul>

<h3>Title: Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13278">http://arxiv.org/abs/2308.13278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13278]] Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity(http://arxiv.org/abs/2308.13278)</code></li>
<li>Summary: <p>Quality-Diversity is a branch of stochastic optimization that is often
applied to problems from the Reinforcement Learning and control domains in
order to construct repertoires of well-performing policies/skills that exhibit
diversity with respect to a behavior space. Such archives are usually composed
of a finite number of reactive agents which are each associated to a unique
behavior descriptor, and instantiating behavior descriptors outside of that
coarsely discretized space is not straight-forward. While a few recent works
suggest solutions to that issue, the trajectory that is generated is not easily
customizable beyond the specification of a target behavior descriptor. We
propose to jointly solve those problems in environments where semantic
information about static scene elements is available by leveraging a Large
Language Model to augment the repertoire with natural language descriptions of
trajectories, and training a policy conditioned on those descriptions. Thus,
our method allows a user to not only specify an arbitrary target behavior
descriptor, but also provide the model with a high-level textual prompt to
shape the generated trajectory. We also propose an LLM-based approach to
evaluating the performance of such generative agents. Furthermore, we develop a
benchmark based on simulated robot navigation in a 2d maze that we use for
experimental validation.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Burnt area extraction from high-resolution satellite images based on anomaly detection. (arXiv:2308.13367v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13367">http://arxiv.org/abs/2308.13367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13367]] Burnt area extraction from high-resolution satellite images based on anomaly detection(http://arxiv.org/abs/2308.13367)</code></li>
<li>Summary: <p>Wildfire detection using satellite images is a widely studied task in remote
sensing with many applications to fire delineation and mapping. Recently, deep
learning methods have become a scalable solution to automate this task,
especially in the field of unsupervised learning where no training data is
available. This is particularly important in the context of emergency risk
monitoring where fast and effective detection is needed, generally based on
high-resolution satellite data. Among various approaches, Anomaly Detection
(AD) appears to be highly potential thanks to its broad applications in
computer vision, medical imaging, as well as remote sensing. In this work, we
build upon the framework of Vector Quantized Variational Autoencoder (VQ-VAE),
a popular reconstruction-based AD method with discrete latent spaces, to
perform unsupervised burnt area extraction. We integrate VQ-VAE into an
end-to-end framework with an intensive post-processing step using dedicated
vegetation, water and brightness indexes. Our experiments conducted on
high-resolution SPOT-6/7 images provide promising results of the proposed
technique, showing its high potential in future research on unsupervised burnt
area extraction.
</p></li>
</ul>

<h3>Title: Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13068">http://arxiv.org/abs/2308.13068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13068]] Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology(http://arxiv.org/abs/2308.13068)</code></li>
<li>Summary: <p>Multivariate Time Series (MVTS) anomaly detection is a long-standing and
challenging research topic that has attracted tremendous research effort from
both industry and academia recently. However, a careful study of the literature
makes us realize that 1) the community is active but not as organized as other
sibling machine learning communities such as Computer Vision (CV) and Natural
Language Processing (NLP), and 2) most proposed solutions are evaluated using
either inappropriate or highly flawed protocols, with an apparent lack of
scientific foundation. So flawed is one very popular protocol, the so-called
\pa protocol, that a random guess can be shown to systematically outperform
\emph{all} algorithms developed so far. In this paper, we review and evaluate
many recent algorithms using more robust protocols and discuss how a normally
good protocol may have weaknesses in the context of MVTS anomaly detection and
how to mitigate them. We also share our concerns about benchmark datasets,
experiment design and evaluation methodology we observe in many works.
Furthermore, we propose a simple, yet challenging, baseline algorithm based on
Principal Components Analysis (PCA) that surprisingly outperforms many recent
Deep Learning (DL) based approaches on popular benchmark datasets. The main
objective of this work is to stimulate more effort towards important aspects of
the research such as data, experiment design, evaluation methodology and result
interpretability, instead of putting the highest weight on the design of
increasingly more complex and "fancier" algorithms.
</p></li>
</ul>

<h3>Title: A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13352">http://arxiv.org/abs/2308.13352</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13352]] A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data(http://arxiv.org/abs/2308.13352)</code></li>
<li>Summary: <p>Anomaly detection (AD) tasks have been solved using machine learning
algorithms in various domains and applications. The great majority of these
algorithms use normal data to train a residual-based model, and assign anomaly
scores to unseen samples based on their dissimilarity with the learned normal
regime. The underlying assumption of these approaches is that anomaly-free data
is available for training. This is, however, often not the case in real-world
operational settings, where the training data may be contaminated with a
certain fraction of abnormal samples. Training with contaminated data, in turn,
inevitably leads to a deteriorated AD performance of the residual-based
algorithms.
</p>
<p>In this paper we introduce a framework for a fully unsupervised refinement of
contaminated training data for AD tasks. The framework is generic and can be
applied to any residual-based machine learning model. We demonstrate the
application of the framework to two public datasets of multivariate time series
machine data from different application fields. We show its clear superiority
over the naive approach of training with contaminated data without refinement.
Moreover, we compare it to the ideal, unrealistic reference in which
anomaly-free data would be available for training. Since the approach exploits
information from the anomalies, and not only from the normal regime, it is
comparable and often outperforms the ideal baseline as well.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.13259">http://arxiv.org/abs/2308.13259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.13259]] Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering(http://arxiv.org/abs/2308.13259)</code></li>
<li>Summary: <p>Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown
impressive reasoning ability in various downstream tasks. Even so, suffering
from hallucinations and the inability to access external knowledge, LLMs often
come with incorrect or unfaithful intermediate reasoning steps, especially in
the context of answering knowledge-intensive tasks such as KBQA. To alleviate
this issue, we propose a framework called Knowledge-Driven Chain-of-Thought
(KD-CoT) to verify and modify reasoning traces in CoT via interaction with
external knowledge, and thus overcome the hallucinations and error propagation.
Concretely, we formulate the CoT rationale process of LLMs into a structured
multi-round QA format. In each round, LLMs interact with a QA system that
retrieves external knowledge and produce faithful reasoning traces based on
retrieved precise answers. The structured CoT reasoning of LLMs is facilitated
by our developed KBQA CoT collection, which serves as in-context learning
demonstrations and can also be utilized as feedback augmentation to train a
robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion
datasets demonstrate the effectiveness of proposed KD-CoT in task-solving
reasoning generation, which outperforms the vanilla CoT ICL with an absolute
success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented
retriever outperforms the state-of-the-art baselines for retrieving knowledge,
achieving significant improvement in Hit performance.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
