<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-03</h1>
<h3>Title: Text Clustering as Classification with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chen Huang, Guoxiu He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00927">https://arxiv.org/abs/2410.00927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00927">https://arxiv.org/pdf/2410.00927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00927]] Text Clustering as Classification with LLMs(https://arxiv.org/abs/2410.00927)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Text clustering remains valuable in real-world applications where manual labeling is cost-prohibitive. It facilitates efficient organization and analysis of information by grouping similar texts based on their representations. However, implementing this approach necessitates fine-tuned embedders for downstream data and sophisticated similarity metrics. To address this issue, this study presents a novel framework for text clustering that effectively leverages the in-context learning capacity of Large Language Models (LLMs). Instead of fine-tuning embedders, we propose to transform the text clustering into a classification task via LLM. First, we prompt LLM to generate potential labels for a given dataset. Second, after integrating similar labels generated by the LLM, we prompt the LLM to assign the most appropriate label to each sample in the dataset. Our framework has been experimentally proven to achieve comparable or superior performance to state-of-the-art clustering methods that employ embeddings, without requiring complex fine-tuning or clustering algorithms. We make our code available to the public for utilization at this https URL.</li>
</ul>

<h3>Title: Towards Full-parameter and Parameter-efficient Self-learning For Endoscopic Camera Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Shuting Zhao, Chenkang Du, Kristin Qi, Xinrong Chen, Xinhan Di</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00979">https://arxiv.org/abs/2410.00979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00979">https://arxiv.org/pdf/2410.00979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00979]] Towards Full-parameter and Parameter-efficient Self-learning For Endoscopic Camera Depth Estimation(https://arxiv.org/abs/2410.00979)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Adaptation methods are developed to adapt depth foundation models to endoscopic depth estimation recently. However, such approaches typically under-perform training since they limit the parameter search to a low-rank subspace and alter the training dynamics. Therefore, we propose a full-parameter and parameter-efficient learning framework for endoscopic depth estimation. At the first stage, the subspace of attention, convolution and multi-layer perception are adapted simultaneously within different sub-spaces. At the second stage, a memory-efficient optimization is proposed for subspace composition and the performance is further improved in the united sub-space. Initial experiments on the SCARED dataset demonstrate that results at the first stage improves the performance from 10.2% to 4.1% for Sq Rel, Abs Rel, RMSE and RMSE log in the comparison with the state-of-the-art models.</li>
</ul>

<h3>Title: Robust Guided Diffusion for Offline Black-Box Optimization</h3>
<ul>
<li><strong>Authors: </strong>Can (Sam)Chen, Christopher Beckham, Zixuan Liu, Xue Liu, Christopher Pal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00983">https://arxiv.org/abs/2410.00983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00983">https://arxiv.org/pdf/2410.00983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00983]] Robust Guided Diffusion for Offline Black-Box Optimization(https://arxiv.org/abs/2410.00983)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Offline black-box optimization aims to maximize a black-box function using an offline dataset of designs and their measured properties. Two main approaches have emerged: the forward approach, which learns a mapping from input to its value, thereby acting as a proxy to guide optimization, and the inverse approach, which learns a mapping from value to input for conditional generation. (a) Although proxy-free~(classifier-free) diffusion shows promise in robustly modeling the inverse mapping, it lacks explicit guidance from proxies, essential for generating high-performance samples beyond the training distribution. Therefore, we propose \textit{proxy-enhanced sampling} which utilizes the explicit guidance from a trained proxy to bolster proxy-free diffusion with enhanced sampling control. (b) Yet, the trained proxy is susceptible to out-of-distribution issues. To address this, we devise the module \textit{diffusion-based proxy refinement}, which seamlessly integrates insights from proxy-free diffusion back into the proxy for refinement. To sum up, we propose \textit{\textbf{R}obust \textbf{G}uided \textbf{D}iffusion for Offline Black-box Optimization}~(\textbf{RGD}), combining the advantages of proxy~(explicit guidance) and proxy-free diffusion~(robustness) for effective conditional generation. RGD achieves state-of-the-art results on various design-bench tasks, underscoring its efficacy. Our code is at this https URL.</li>
</ul>

<h3>Title: LaDTalk: Latent Denoising for Synthesizing Talking Head Videos with High Frequency Details</h3>
<ul>
<li><strong>Authors: </strong>Jian Yang, Xukun Wang, Wentao Wang, Guoming Li, Qihang Fang, Ruihong Yuan, Tianyang Wang, Jason Zhaoxin Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.00990">https://arxiv.org/abs/2410.00990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.00990">https://arxiv.org/pdf/2410.00990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.00990]] LaDTalk: Latent Denoising for Synthesizing Talking Head Videos with High Frequency Details(https://arxiv.org/abs/2410.00990)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Audio-driven talking head generation is a pivotal area within film-making and Virtual Reality. Although existing methods have made significant strides following the end-to-end paradigm, they still encounter challenges in producing videos with high-frequency details due to their limited expressivity in this domain. This limitation has prompted us to explore an effective post-processing approach to synthesize photo-realistic talking head videos. Specifically, we employ a pretrained Wav2Lip model as our foundation model, leveraging its robust audio-lip alignment capabilities. Drawing on the theory of Lipschitz Continuity, we have theoretically established the noise robustness of Vector Quantised Auto Encoders (VQAEs). Our experiments further demonstrate that the high-frequency texture deficiency of the foundation model can be temporally consistently recovered by the Space-Optimised Vector Quantised Auto Encoder (SOVQAE) we introduced, thereby facilitating the creation of realistic talking head videos. We conduct experiments on both the conventional dataset and the High-Frequency TalKing head (HFTK) dataset that we curated. The results indicate that our method, LaDTalk, achieves new state-of-the-art video quality and out-of-domain lip synchronization performance.</li>
</ul>

<h3>Title: Back to Bayesics: Uncovering Human Mobility Distributions and Anomalies with an Integrated Statistical and Neural Framework</h3>
<ul>
<li><strong>Authors: </strong>Minxuan Duan, Yinlong Qian, Lingyi Zhao, Zihao Zhou, Zeeshan Rasheed, Rose Yu, Khurram Shafique</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01011">https://arxiv.org/abs/2410.01011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01011">https://arxiv.org/pdf/2410.01011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01011]] Back to Bayesics: Uncovering Human Mobility Distributions and Anomalies with an Integrated Statistical and Neural Framework(https://arxiv.org/abs/2410.01011)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Existing methods for anomaly detection often fall short due to their inability to handle the complexity, heterogeneity, and high dimensionality inherent in real-world mobility data. In this paper, we propose DeepBayesic, a novel framework that integrates Bayesian principles with deep neural networks to model the underlying multivariate distributions from sparse and complex datasets. Unlike traditional models, DeepBayesic is designed to manage heterogeneous inputs, accommodating both continuous and categorical data to provide a more comprehensive understanding of mobility patterns. The framework features customized neural density estimators and hybrid architectures, allowing for flexibility in modeling diverse feature distributions and enabling the use of specialized neural networks tailored to different data types. Our approach also leverages agent embeddings for personalized anomaly detection, enhancing its ability to distinguish between normal and anomalous behaviors for individual agents. We evaluate our approach on several mobility datasets, demonstrating significant improvements over state-of-the-art anomaly detection methods. Our results indicate that incorporating personalization and advanced sequence modeling techniques can substantially enhance the ability to detect subtle and complex anomalies in spatiotemporal event sequences.</li>
</ul>

<h3>Title: MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages</h3>
<ul>
<li><strong>Authors: </strong>Marco Gaido, Sara Papi, Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, Matteo Negri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01036">https://arxiv.org/abs/2410.01036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01036">https://arxiv.org/pdf/2410.01036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01036]] MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages(https://arxiv.org/abs/2410.01036)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The rise of foundation models (FMs), coupled with regulatory efforts addressing their risks and impacts, has sparked significant interest in open-source models. However, existing speech FMs (SFMs) fall short of full compliance with the open-source principles, even if claimed otherwise, as no existing SFM has model weights, code, and training data publicly available under open-source terms. In this work, we take the first step toward filling this gap by focusing on the 24 official languages of the European Union (EU). We collect suitable training data by surveying automatic speech recognition datasets and unlabeled speech corpora under open-source compliant licenses, for a total of 950k hours. Additionally, we release automatic transcripts for 441k hours of unlabeled data under the permissive CC-BY license, thereby facilitating the creation of open-source SFMs for the EU languages.</li>
</ul>

<h3>Title: Pose Estimation of Buried Deep-Sea Objects using 3D Vision Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Jerry Yan, Chinmay Talegaonkar, Nicholas Antipa, Eric Terrill, Sophia Merrifield</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01061">https://arxiv.org/abs/2410.01061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01061">https://arxiv.org/pdf/2410.01061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01061]] Pose Estimation of Buried Deep-Sea Objects using 3D Vision Deep Learning Models(https://arxiv.org/abs/2410.01061)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present an approach for pose and burial fraction estimation of debris field barrels found on the seabed in the Southern California San Pedro Basin. Our computational workflow leverages recent advances in foundation models for segmentation and a vision transformer-based approach to estimate the point cloud which defines the geometry of the barrel. We propose BarrelNet for estimating the 6-DOF pose and radius of buried barrels from the barrel point clouds as input. We train BarrelNet using synthetically generated barrel point clouds, and qualitatively demonstrate the potential of our approach using remotely operated vehicle (ROV) video footage of barrels found at a historic dump site. We compare our method to a traditional least squares fitting approach and show significant improvement according to our defined benchmarks.</li>
</ul>

<h3>Title: From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems</h3>
<ul>
<li><strong>Authors: </strong>Ali Mohammadjafari, Anthony S. Maida, Raju Gottumukkala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01066">https://arxiv.org/abs/2410.01066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01066">https://arxiv.org/pdf/2410.01066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01066]] From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems(https://arxiv.org/abs/2410.01066)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Since the onset of LLMs, translating natural language queries to structured SQL commands is assuming increasing. Unlike the previous reviews, this survey provides a comprehensive study of the evolution of LLM-based text-to-SQL systems, from early rule-based models to advanced LLM approaches, and how LLMs impacted this field. We discuss benchmarks, evaluation methods and evaluation metrics. Also, we uniquely study the role of integration of knowledge graphs for better contextual accuracy and schema linking in these systems. The current techniques fall into two categories: in-context learning of corpus and fine-tuning, which then leads to approaches such as zero-shot, few-shot learning from the end, and data augmentation. Finally, we highlight key challenges such as computational efficiency, model robustness, and data privacy with perspectives toward their development and improvements in potential areas for future of LLM-based text-to-SQL system.</li>
</ul>

<h3>Title: Inferring Kernel $\epsilon$-Machines: Discovering Structure in Complex Systems</h3>
<ul>
<li><strong>Authors: </strong>Alexandra M. Jurgens, Nicolas Brodu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01076">https://arxiv.org/abs/2410.01076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01076">https://arxiv.org/pdf/2410.01076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01076]] Inferring Kernel $\epsilon$-Machines: Discovering Structure in Complex Systems(https://arxiv.org/abs/2410.01076)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Previously, we showed that computational mechanic's causal states -- predictively-equivalent trajectory classes for a stochastic dynamical system -- can be cast into a reproducing kernel Hilbert space. The result is a widely-applicable method that infers causal structure directly from very different kinds of observations and systems. Here, we expand this method to explicitly introduce the causal diffusion components it produces. These encode the kernel causal-state estimates as a set of coordinates in a reduced dimension space. We show how each component extracts predictive features from data and demonstrate their application on four examples: first, a simple pendulum -- an exactly solvable system; second, a molecular-dynamic trajectory of $n$-butane -- a high-dimensional system with a well-studied energy landscape; third, the monthly sunspot sequence -- the longest-running available time series of direct observations; and fourth, multi-year observations of an active crop field -- a set of heterogeneous observations of the same ecosystem taken for over a decade. In this way, we demonstrate that the empirical kernel causal-states algorithm robustly discovers predictive structures for systems with widely varying dimensionality and stochasticity.</li>
</ul>

<h3>Title: Embedding-based statistical inference on generative models</h3>
<ul>
<li><strong>Authors: </strong>Hayden Helm, Aranyak Acharyya, Brandon Duderstadt, Youngser Park, Carey E. Priebe</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01106">https://arxiv.org/abs/2410.01106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01106">https://arxiv.org/pdf/2410.01106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01106]] Embedding-based statistical inference on generative models(https://arxiv.org/abs/2410.01106)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>The recent cohort of publicly available generative models can produce human expert level content across a variety of topics and domains. Given a model in this cohort as a base model, methods such as parameter efficient fine-tuning, in-context learning, and constrained decoding have further increased generative capabilities and improved both computational and data efficiency. Entire collections of derivative models have emerged as a byproduct of these methods and each of these models has a set of associated covariates such as a score on a benchmark, an indicator for if the model has (or had) access to sensitive information, etc. that may or may not be available to the user. For some model-level covariates, it is possible to use "similar" models to predict an unknown covariate. In this paper we extend recent results related to embedding-based representations of generative models -- the data kernel perspective space -- to classical statistical inference settings. We demonstrate that using the perspective space as the basis of a notion of "similar" is effective for multiple model-level inference tasks.</li>
</ul>

<h3>Title: Uncertainty-Guided Enhancement on Driving Perception System via Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Yang, Yuxin Hu, Mao Ye, Zaiwei Zhang, Zhichao Lu, Yi Xu, Ufuk Topcu, Ben Snyder</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01144">https://arxiv.org/abs/2410.01144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01144">https://arxiv.org/pdf/2410.01144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01144]] Uncertainty-Guided Enhancement on Driving Perception System via Foundation Models(https://arxiv.org/abs/2410.01144)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multimodal foundation models offer promising advancements for enhancing driving perception systems, but their high computational and financial costs pose challenges. We develop a method that leverages foundation models to refine predictions from existing driving perception models -- such as enhancing object classification accuracy -- while minimizing the frequency of using these resource-intensive models. The method quantitatively characterizes uncertainties in the perception model's predictions and engages the foundation model only when these uncertainties exceed a pre-specified threshold. Specifically, it characterizes uncertainty by calibrating the perception model's confidence scores into theoretical lower bounds on the probability of correct predictions using conformal prediction. Then, it sends images to the foundation model and queries for refining the predictions only if the theoretical bound of the perception model's outcome is below the threshold. Additionally, we propose a temporal inference mechanism that enhances prediction accuracy by integrating historical predictions, leading to tighter theoretical bounds. The method demonstrates a 10 to 15 percent improvement in prediction accuracy and reduces the number of queries to the foundation model by 50 percent, based on quantitative evaluations from driving datasets.</li>
</ul>

<h3>Title: Text2PDE: Latent Diffusion Models for Accessible Physics Simulation</h3>
<ul>
<li><strong>Authors: </strong>Anthony Zhou, Zijie Li, Michael Schneier, John R Buchanan Jr, Amir Barati Farimani</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01153">https://arxiv.org/abs/2410.01153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01153">https://arxiv.org/pdf/2410.01153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01153]] Text2PDE: Latent Diffusion Models for Accessible Physics Simulation(https://arxiv.org/abs/2410.01153)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning have inspired numerous works on data-driven solutions to partial differential equation (PDE) problems. These neural PDE solvers can often be much faster than their numerical counterparts; however, each presents its unique limitations and generally balances training cost, numerical accuracy, and ease of applicability to different problem setups. To address these limitations, we introduce several methods to apply latent diffusion models to physics simulation. Firstly, we introduce a mesh autoencoder to compress arbitrarily discretized PDE data, allowing for efficient diffusion training across various physics. Furthermore, we investigate full spatio-temporal solution generation to mitigate autoregressive error accumulation. Lastly, we investigate conditioning on initial physical quantities, as well as conditioning solely on a text prompt to introduce text2PDE generation. We show that language can be a compact, interpretable, and accurate modality for generating physics simulations, paving the way for more usable and accessible PDE solvers. Through experiments on both uniform and structured grids, we show that the proposed approach is competitive with current neural PDE solvers in both accuracy and efficiency, with promising scaling behavior up to $\sim$3 billion parameters. By introducing a scalable, accurate, and usable physics simulator, we hope to bring neural PDE solvers closer to practical use.</li>
</ul>

<h3>Title: UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Hasnat Md Abdullah, Tian Liu, Kangda Wei, Shu Kong, Ruihong Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01180">https://arxiv.org/abs/2410.01180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01180">https://arxiv.org/pdf/2410.01180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01180]] UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark(https://arxiv.org/abs/2410.01180)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Localizing unusual activities, such as human errors or surveillance incidents, in videos holds practical significance. However, current video understanding models struggle with localizing these unusual events likely because of their insufficient representation in models' pretraining datasets. To explore foundation models' capability in localizing unusual activity, we introduce UAL-Bench, a comprehensive benchmark for unusual activity localization, featuring three video datasets: UAG-OOPS, UAG-SSBD, UAG-FunQA, and an instruction-tune dataset: OOPS-UAG-Instruct, to improve model capabilities. UAL-Bench evaluates three approaches: Video-Language Models (Vid-LLMs), instruction-tuned Vid-LLMs, and a novel integration of Vision-Language Models and Large Language Models (VLM-LLM). Our results show the VLM-LLM approach excels in localizing short-span unusual events and predicting their onset (start time) more accurately than Vid-LLMs. We also propose a new metric, R@1, TD <= p, to address limitations in existing evaluation methods. Our findings highlight the challenges posed by long-duration videos, particularly in autism diagnosis scenarios, and the need for further advancements in localization techniques. Our work not only provides a benchmark for unusual activity localization but also outlines the key challenges for existing foundation models, suggesting future research directions on this important task.</li>
</ul>

<h3>Title: Towards Native Generative Model for 3D Head Avatar</h3>
<ul>
<li><strong>Authors: </strong>Yiyu Zhuang, Yuxiao He, Jiawei Zhang, Yanwen Wang, Jiahe Zhu, Yao Yao, Siyu Zhu, Xun Cao, Hao Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01226">https://arxiv.org/abs/2410.01226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01226">https://arxiv.org/pdf/2410.01226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01226]] Towards Native Generative Model for 3D Head Avatar(https://arxiv.org/abs/2410.01226)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Creating 3D head avatars is a significant yet challenging task for many applicated scenarios. Previous studies have set out to learn 3D human head generative models using massive 2D image data. Although these models are highly generalizable for human appearance, their result models are not 360$^\circ$-renderable, and the predicted 3D geometry is unreliable. Therefore, such results cannot be used in VR, game modeling, and other scenarios that require 360$^\circ$-renderable 3D head models. An intuitive idea is that 3D head models with limited amount but high 3D accuracy are more reliable training data for a high-quality 3D generative model. In this vein, we delve into how to learn a native generative model for 360$^\circ$ full head from a limited 3D head dataset. Specifically, three major problems are studied: 1) how to effectively utilize various representations for generating the 360$^\circ$-renderable human head; 2) how to disentangle the appearance, shape, and motion of human faces to generate a 3D head model that can be edited by appearance and driven by motion; 3) and how to extend the generalization capability of the generative model to support downstream tasks. Comprehensive experiments are conducted to verify the effectiveness of the proposed model. We hope the proposed models and artist-designed dataset can inspire future research on learning native generative 3D head models from limited 3D datasets.</li>
</ul>

<h3>Title: Aggregation of Multi Diffusion Models for Enhancing Learned Representations</h3>
<ul>
<li><strong>Authors: </strong>Conghan Yue, Zhengwei Peng, Shiyan Du, Zhi Ji, Dongyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01262">https://arxiv.org/abs/2410.01262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01262">https://arxiv.org/pdf/2410.01262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01262]] Aggregation of Multi Diffusion Models for Enhancing Learned Representations(https://arxiv.org/abs/2410.01262)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in image generation, particularly with the various applications of classifier-free guidance conditional diffusion models. While many diffusion models perform well when controlling for particular aspect among style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper introduces a novel algorithm, Aggregation of Multi Diffusion Models (AMDM), which synthesizes features from multiple diffusion models into a specified model, enhancing its learned representations to activate specific features for fine-grained control. AMDM consists of two key components: spherical aggregation and manifold optimization. Spherical aggregation merges intermediate variables from different diffusion models with minimal manifold deviation, while manifold optimization refines these variables to align with the intermediate data manifold, enhancing sampling quality. Experimental results demonstrate that AMDM significantly improves fine-grained control without additional training or inference time, proving its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional control generation in diffusion models: We can fully utilize existing conditional diffusion models that control specific aspects, or develop new ones, and then aggregate them using the AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs. Code is available at: this https URL</li>
</ul>

<h3>Title: Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Can Demircan, Tankred Saanum, Akshay K. Jagadish, Marcel Binz, Eric Schulz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01280">https://arxiv.org/abs/2410.01280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01280">https://arxiv.org/pdf/2410.01280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01280]] Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models(https://arxiv.org/abs/2410.01280)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning, the ability to adapt based on a few examples in the input prompt, is a ubiquitous feature of large language models (LLMs). However, as LLMs' in-context learning abilities continue to improve, understanding this phenomenon mechanistically becomes increasingly important. In particular, it is not well-understood how LLMs learn to solve specific classes of problems, such as reinforcement learning (RL) problems, in-context. Through three different tasks, we first show that Llama $3$ $70$B can solve simple RL problems in-context. We then analyze the residual stream of Llama using Sparse Autoencoders (SAEs) and find representations that closely match temporal difference (TD) errors. Notably, these representations emerge despite the model only being trained to predict the next token. We verify that these representations are indeed causally involved in the computation of TD errors and $Q$-values by performing carefully designed interventions on them. Taken together, our work establishes a methodology for studying and manipulating in-context learning with SAEs, paving the way for a more mechanistic understanding.</li>
</ul>

<h3>Title: Mitigating Copy Bias in In-Context Learning through Neuron Pruning</h3>
<ul>
<li><strong>Authors: </strong>Ameen Ali, Lior Wolf, Ivan Titov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01288">https://arxiv.org/abs/2410.01288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01288">https://arxiv.org/pdf/2410.01288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01288]] Mitigating Copy Bias in In-Context Learning through Neuron Pruning(https://arxiv.org/abs/2410.01288)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated impressive few-shot in-context learning (ICL) abilities. Still, we show that they are sometimes prone to a `copying bias', where they copy answers from provided examples instead of learning the underlying patterns. In this work, we propose a novel and simple method to mitigate such copying bias. First, we create a synthetic task and use the Integrated Gradients method to identify neurons that prioritize copying over generalization. We demonstrate that pruning these neurons consistently improves performance across a diverse set of ICL tasks. We also show that our method is applicable across various LLM architectures, including Transformers and State-Space Models, without requiring modifications. In our analysis, we adopt a task-recognition perspective on ICL and examine task vectors (Hendel et al., 2023) induced by the model. We find that pruning enhances the quality of these vectors, suggesting that the pruned neurons previously hindered effective task recognition.</li>
</ul>

<h3>Title: LaGeM: A Large Geometry Model for 3D Representation Learning and Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Biao Zhang, Peter Wonka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01295">https://arxiv.org/abs/2410.01295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01295">https://arxiv.org/pdf/2410.01295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01295]] LaGeM: A Large Geometry Model for 3D Representation Learning and Diffusion(https://arxiv.org/abs/2410.01295)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel hierarchical autoencoder that maps 3D models into a highly compressed latent space. The hierarchical autoencoder is specifically designed to tackle the challenges arising from large-scale datasets and generative modeling using diffusion. Different from previous approaches that only work on a regular image or volume grid, our hierarchical autoencoder operates on unordered sets of vectors. Each level of the autoencoder controls different geometric levels of detail. We show that the model can be used to represent a wide range of 3D models while faithfully representing high-resolution geometry details. The training of the new architecture takes 0.70x time and 0.58x memory compared to the baseline. We also explore how the new representation can be used for generative modeling. Specifically, we propose a cascaded diffusion framework where each stage is conditioned on the previous stage. Our design extends existing cascaded designs for image and volume grids to vector sets.</li>
</ul>

<h3>Title: Sampling from Energy-based Policies using Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Vineet Jain, Tara Akhound-Sadegh, Siamak Ravanbakhsh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01312">https://arxiv.org/abs/2410.01312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01312">https://arxiv.org/pdf/2410.01312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01312]] Sampling from Energy-based Policies using Diffusion(https://arxiv.org/abs/2410.01312)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Energy-based policies offer a flexible framework for modeling complex, multimodal behaviors in reinforcement learning (RL). In maximum entropy RL, the optimal policy is a Boltzmann distribution derived from the soft Q-function, but direct sampling from this distribution in continuous action spaces is computationally intractable. As a result, existing methods typically use simpler parametric distributions, like Gaussians, for policy representation - limiting their ability to capture the full complexity of multimodal action distributions. In this paper, we introduce a diffusion-based approach for sampling from energy-based policies, where the negative Q-function defines the energy function. Based on this approach, we propose an actor-critic method called Diffusion Q-Sampling (DQS) that enables more expressive policy representations, allowing stable learning in diverse environments. We show that our approach enhances exploration and captures multimodal behavior in continuous control tasks, addressing key limitations of existing methods.</li>
</ul>

<h3>Title: Finetuning Pre-trained Model with Limited Data for LiDAR-based 3D Object Detection by Bridging Domain Gaps</h3>
<ul>
<li><strong>Authors: </strong>Jiyun Jang, Mincheol Chang, Jongwon Park, Jinkyu Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01319">https://arxiv.org/abs/2410.01319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01319">https://arxiv.org/pdf/2410.01319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01319]] Finetuning Pre-trained Model with Limited Data for LiDAR-based 3D Object Detection by Bridging Domain Gaps(https://arxiv.org/abs/2410.01319)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>LiDAR-based 3D object detectors have been largely utilized in various applications, including autonomous vehicles or mobile robots. However, LiDAR-based detectors often fail to adapt well to target domains with different sensor configurations (e.g., types of sensors, spatial resolution, or FOVs) and location shifts. Collecting and annotating datasets in a new setup is commonly required to reduce such gaps, but it is often expensive and time-consuming. Recent studies suggest that pre-trained backbones can be learned in a self-supervised manner with large-scale unlabeled LiDAR frames. However, despite their expressive representations, they remain challenging to generalize well without substantial amounts of data from the target domain. Thus, we propose a novel method, called Domain Adaptive Distill-Tuning (DADT), to adapt a pre-trained model with limited target data (approximately 100 LiDAR frames), retaining its representation power and preventing it from overfitting. Specifically, we use regularizers to align object-level and context-level representations between the pre-trained and finetuned models in a teacher-student architecture. Our experiments with driving benchmarks, i.e., Waymo Open dataset and KITTI, confirm that our method effectively finetunes a pre-trained model, achieving significant gains in accuracy.</li>
</ul>

<h3>Title: Forte : Finding Outliers with Representation Typicality Estimation</h3>
<ul>
<li><strong>Authors: </strong>Debargha Ganguly, Warren Morningstar, Andrew Yu, Vipin Chaudhary</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01322">https://arxiv.org/abs/2410.01322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01322">https://arxiv.org/pdf/2410.01322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01322]] Forte : Finding Outliers with Representation Typicality Estimation(https://arxiv.org/abs/2410.01322)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Generative models can now produce photorealistic synthetic data which is virtually indistinguishable from the real data used to train it. This is a significant evolution over previous models which could produce reasonable facsimiles of the training data, but ones which could be visually distinguished from the training data by human evaluation. Recent work on OOD detection has raised doubts that generative model likelihoods are optimal OOD detectors due to issues involving likelihood misestimation, entropy in the generative process, and typicality. We speculate that generative OOD detectors also failed because their models focused on the pixels rather than the semantic content of the data, leading to failures in near-OOD cases where the pixels may be similar but the information content is significantly different. We hypothesize that estimating typical sets using self-supervised learners leads to better OOD detectors. We introduce a novel approach that leverages representation learning, and informative summary statistics based on manifold estimation, to address all of the aforementioned issues. Our method outperforms other unsupervised approaches and achieves state-of-the art performance on well-established challenging benchmarks, and new synthetic data detection tasks.</li>
</ul>

<h3>Title: Unveiling Language Skills under Circuits</h3>
<ul>
<li><strong>Authors: </strong>Hang Chen, Jiaying Zhu, Xinyu Yang, Wenya Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01334">https://arxiv.org/abs/2410.01334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01334">https://arxiv.org/pdf/2410.01334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01334]] Unveiling Language Skills under Circuits(https://arxiv.org/abs/2410.01334)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The exploration of language skills in language models (LMs) has always been one of the central goals in mechanistic interpretability. However, existing circuit analyses often fall short in representing the full functional scope of these models, primarily due to the exclusion of Feed-Forward layers. Additionally, isolating the effect of a single language skill from a text, which inherently involves multiple entangled skills, poses a significant challenge. To address these gaps, we introduce a novel concept, Memory Circuit, a minimum unit that fully and independently manipulates the memory-reading functionality of a language model, and disentangle the transformer model precisely into a circuit graph which is an ensemble of paths connecting different memory circuits. Based on this disentanglement, we identify salient circuit paths, named as skill paths, responsible for three crucial language skills, i.e., the Previous Token Skill, Induction Skill and In-Context Learning (ICL) Skill, leveraging causal effect estimation through interventions and counterfactuals. Our experiments on various datasets confirm the correspondence between our identified skill paths and language skills, and validate three longstanding hypotheses: 1) Language skills are identifiable through circuit dissection; 2) Simple language skills reside in shallow layers, whereas complex language skills are found in deeper layers; 3) Complex language skills are formed on top of simpler language skills. Our codes are available at: this https URL.</li>
</ul>

<h3>Title: PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems</h3>
<ul>
<li><strong>Authors: </strong>Bocheng Zeng, Qi Wang, Mengtao Yan, Yang Liu, Ruizhi Chengze, Yi Zhang, Hongsheng Liu, Zidong Wang, Hao Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01337">https://arxiv.org/abs/2410.01337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01337">https://arxiv.org/pdf/2410.01337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01337]] PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems(https://arxiv.org/abs/2410.01337)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains.</li>
</ul>

<h3>Title: Harnessing the Latent Diffusion Model for Training-Free Image Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Kento Masui, Mayu Otani, Masahiro Nomura, Hideki Nakayama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01366">https://arxiv.org/abs/2410.01366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01366">https://arxiv.org/pdf/2410.01366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01366]] Harnessing the Latent Diffusion Model for Training-Free Image Style Transfer(https://arxiv.org/abs/2410.01366)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently shown the ability to generate high-quality images. However, controlling its generation process still poses challenges. The image style transfer task is one of those challenges that transfers the visual attributes of a style image to another content image. Typical obstacle of this task is the requirement of additional training of a pre-trained model. We propose a training-free style transfer algorithm, Style Tracking Reverse Diffusion Process (STRDP) for a pretrained Latent Diffusion Model (LDM). Our algorithm employs Adaptive Instance Normalization (AdaIN) function in a distinct manner during the reverse diffusion process of an LDM while tracking the encoding history of the style image. This algorithm enables style transfer in the latent space of LDM for reduced computational cost, and provides compatibility for various LDM models. Through a series of experiments and a user study, we show that our method can quickly transfer the style of an image without additional training. The speed, compatibility, and training-free aspect of our algorithm facilitates agile experiments with combinations of styles and LDMs for extensive application.</li>
</ul>

<h3>Title: AgriCLIP: Adapting CLIP for Agriculture and Livestock via Domain-Specialized Cross-Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Umair Nawaz, Muhammad Awais, Hanan Gani, Muzammal Naseer, Fahad Khan, Salman Khan, Rao Muhammad Anwer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01407">https://arxiv.org/abs/2410.01407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01407">https://arxiv.org/pdf/2410.01407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01407]] AgriCLIP: Adapting CLIP for Agriculture and Livestock via Domain-Specialized Cross-Model Alignment(https://arxiv.org/abs/2410.01407)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Capitalizing on vast amount of image-text data, large-scale vision-language pre-training has demonstrated remarkable zero-shot capabilities and has been utilized in several applications. However, models trained on general everyday web-crawled data often exhibit sub-optimal performance for specialized domains, likely due to domain shift. Recent works have tackled this problem for some domains (e.g., healthcare) by constructing domain-specialized image-text data. However, constructing a dedicated large-scale image-text dataset for sustainable area of agriculture and livestock is still open to research. Further, this domain desires fine-grained feature learning due to the subtle nature of the downstream tasks (e.g, nutrient deficiency detection, livestock breed classification). To address this we present AgriCLIP, a vision-language foundational model dedicated to the domain of agriculture and livestock. First, we propose a large-scale dataset, named ALive, that leverages customized prompt generation strategy to overcome the scarcity of expert annotations. Our ALive dataset covers crops, livestock, and fishery, with around 600,000 image-text pairs. Second, we propose a training pipeline that integrates both contrastive and self-supervised learning to learn both global semantic and local fine-grained domain-specialized features. Experiments on diverse set of 20 downstream tasks demonstrate the effectiveness of AgriCLIP framework, achieving an absolute gain of 7.8\% in terms of average zero-shot classification accuracy, over the standard CLIP adaptation via domain-specialized ALive dataset. Our ALive dataset and code can be accessible at \href{this https URL}{Github}.</li>
</ul>

<h3>Title: Fair4Free: Generating High-fidelity Fair Synthetic Samples using Data Free Distillation</h3>
<ul>
<li><strong>Authors: </strong>Md Fahim Sikder, Daniel de Leng, Fredrik Heintz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01423">https://arxiv.org/abs/2410.01423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01423">https://arxiv.org/pdf/2410.01423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01423]] Fair4Free: Generating High-fidelity Fair Synthetic Samples using Data Free Distillation(https://arxiv.org/abs/2410.01423)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work presents Fair4Free, a novel generative model to generate synthetic fair data using data-free distillation in the latent space. Fair4Free can work on the situation when the data is private or inaccessible. In our approach, we first train a teacher model to create fair representation and then distil the knowledge to a student model (using a smaller architecture). The process of distilling the student model is data-free, i.e. the student model does not have access to the training dataset while distilling. After the distillation, we use the distilled model to generate fair synthetic samples. Our extensive experiments show that our synthetic samples outperform state-of-the-art models in all three criteria (fairness, utility and synthetic quality) with a performance increase of 5% for fairness, 8% for utility and 12% in synthetic quality for both tabular and image datasets.</li>
</ul>

<h3>Title: Adaptive teachers for amortized samplers</h3>
<ul>
<li><strong>Authors: </strong>Minsu Kim, Sanghyeok Choi, Taeyoung Yun, Emmanuel Bengio, Leo Feng, Jarrid Rector-Brooks, Sungsoo Ahn, Jinkyoo Park, Nikolay Malkin, Yoshua Bengio</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01432">https://arxiv.org/abs/2410.01432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01432">https://arxiv.org/pdf/2410.01432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01432]] Adaptive teachers for amortized samplers(https://arxiv.org/abs/2410.01432)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Amortized inference is the task of training a parametric model, such as a neural network, to approximate a distribution with a given unnormalized density where exact sampling is intractable. When sampling is implemented as a sequential decision-making process, reinforcement learning (RL) methods, such as generative flow networks, can be used to train the sampling policy. Off-policy RL training facilitates the discovery of diverse, high-reward candidates, but existing methods still face challenges in efficient exploration. We propose to use an adaptive training distribution (the Teacher) to guide the training of the primary amortized sampler (the Student) by prioritizing high-loss regions. The Teacher, an auxiliary behavior model, is trained to sample high-error regions of the Student and can generalize across unexplored modes, thereby enhancing mode coverage by providing an efficient training curriculum. We validate the effectiveness of this approach in a synthetic environment designed to present an exploration challenge, two diffusion-based sampling tasks, and four biochemical discovery tasks demonstrating its ability to improve sample efficiency and mode coverage.</li>
</ul>

<h3>Title: Information-Theoretical Principled Trade-off between Jailbreakability and Stealthiness on Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ching-Chia Kao, Chia-Mu Yu, Chun-Shien Lu, Chu-Song Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01438">https://arxiv.org/abs/2410.01438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01438">https://arxiv.org/pdf/2410.01438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01438]] Information-Theoretical Principled Trade-off between Jailbreakability and Stealthiness on Vision Language Models(https://arxiv.org/abs/2410.01438)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, Vision-Language Models (VLMs) have demonstrated significant advancements in artificial intelligence, transforming tasks across various domains. Despite their capabilities, these models are susceptible to jailbreak attacks, which can compromise their safety and reliability. This paper explores the trade-off between jailbreakability and stealthiness in VLMs, presenting a novel algorithm to detect non-stealthy jailbreak attacks and enhance model robustness. We introduce a stealthiness-aware jailbreak attack using diffusion models, highlighting the challenge of detecting AI-generated content. Our approach leverages Fano's inequality to elucidate the relationship between attack success rates and stealthiness scores, providing an explainable framework for evaluating these threats. Our contributions aim to fortify AI systems against sophisticated attacks, ensuring their outputs remain aligned with ethical standards and user expectations.</li>
</ul>

<h3>Title: Decorrelation-based Self-Supervised Visual Representation Learning for Writer Identification</h3>
<ul>
<li><strong>Authors: </strong>Arkadip Maitra, Shree Mitra, Siladittya Manna, Saumik Bhattacharya, Umapada Pal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01441">https://arxiv.org/abs/2410.01441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01441">https://arxiv.org/pdf/2410.01441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01441]] Decorrelation-based Self-Supervised Visual Representation Learning for Writer Identification(https://arxiv.org/abs/2410.01441)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has developed rapidly over the last decade and has been applied in many areas of computer vision. Decorrelation-based self-supervised pretraining has shown great promise among non-contrastive algorithms, yielding performance at par with supervised and contrastive self-supervised baselines. In this work, we explore the decorrelation-based paradigm of self-supervised learning and apply the same to learning disentangled stroke features for writer identification. Here we propose a modified formulation of the decorrelation-based framework named SWIS which was proposed for signature verification by standardizing the features along each dimension on top of the existing framework. We show that the proposed framework outperforms the contemporary self-supervised learning framework on the writer identification benchmark and also outperforms several supervised methods as well. To the best of our knowledge, this work is the first of its kind to apply self-supervised learning for learning representations for writer verification tasks.</li>
</ul>

<h3>Title: Agent-Driven Large Language Models for Mandarin Lyric Generation</h3>
<ul>
<li><strong>Authors: </strong>Hong-Hsiang Liu, Yi-Wen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01450">https://arxiv.org/abs/2410.01450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01450">https://arxiv.org/pdf/2410.01450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01450]] Agent-Driven Large Language Models for Mandarin Lyric Generation(https://arxiv.org/abs/2410.01450)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, in-context</a></li>
<li><strong>Abstract: </strong>Generative Large Language Models have shown impressive in-context learning abilities, performing well across various tasks with just a prompt. Previous melody-to-lyric research has been limited by scarce high-quality aligned data and unclear standard for creativeness. Most efforts focused on general themes or emotions, which are less valuable given current language model capabilities. In tonal contour languages like Mandarin, pitch contours are influenced by both melody and tone, leading to variations in lyric-melody fit. Our study, validated by the Mpop600 dataset, confirms that lyricists and melody writers consider this fit during their composition process. In this research, we developed a multi-agent system that decomposes the melody-to-lyric task into sub-tasks, with each agent controlling rhyme, syllable count, lyric-melody alignment, and consistency. Listening tests were conducted via a diffusion-based singing voice synthesizer to evaluate the quality of lyrics generated by different agent groups.</li>
</ul>

<h3>Title: Discrete Diffusion Schr\"odinger Bridge Matching for Graph Transformation</h3>
<ul>
<li><strong>Authors: </strong>Jun Hyeong Kim, Seonghwan Kim, Seokhyun Moon, Hyeongwoo Kim, Jeheon Woo, Woo Youn Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01500">https://arxiv.org/abs/2410.01500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01500">https://arxiv.org/pdf/2410.01500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01500]] Discrete Diffusion Schr\"odinger Bridge Matching for Graph Transformation(https://arxiv.org/abs/2410.01500)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Transporting between arbitrary distributions is a fundamental goal in generative modeling. Recently proposed diffusion bridge models provide a potential solution, but they rely on a joint distribution that is difficult to obtain in practice. Furthermore, formulations based on continuous domains limit their applicability to discrete domains such as graphs. To overcome these limitations, we propose Discrete Diffusion Schrödinger Bridge Matching (DDSBM), a novel framework that utilizes continuous-time Markov chains to solve the SB problem in a high-dimensional discrete state space. Our approach extends Iterative Markovian Fitting to discrete domains, and we have proved its convergence to the SB. Furthermore, we adapt our framework for the graph transformation and show that our design choice of underlying dynamics characterized by independent modifications of nodes and edges can be interpreted as the entropy-regularized version of optimal transport with a cost function described by the graph edit distance. To demonstrate the effectiveness of our framework, we have applied DDSBM to molecular optimization in the field of chemistry. Experimental results demonstrate that DDSBM effectively optimizes molecules' property-of-interest with minimal graph transformation, successfully retaining other features.</li>
</ul>

<h3>Title: LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion</h3>
<ul>
<li><strong>Authors: </strong>Dexuan Ding, Lei Wang, Liyun Zhu, Tom Gedeon, Piotr Koniusz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01506">https://arxiv.org/abs/2410.01506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01506">https://arxiv.org/pdf/2410.01506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01506]] LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion(https://arxiv.org/abs/2410.01506)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In computer vision tasks, features often come from diverse representations, domains, and modalities, such as text, images, and videos. Effectively fusing these features is essential for robust performance, especially with the availability of powerful pre-trained models like vision-language models. However, common fusion methods, such as concatenation, element-wise operations, and non-linear techniques, often fail to capture structural relationships, deep feature interactions, and suffer from inefficiency or misalignment of features across domains. In this paper, we shift from high-dimensional feature space to a lower-dimensional, interpretable graph space by constructing similarity graphs that encode feature relationships at different levels, e.g., clip, frame, patch, token, etc. To capture deeper interactions, we use graph power expansions and introduce a learnable graph fusion operator to combine these graph powers for more effective fusion. Our approach is relationship-centric, operates in a homogeneous space, and is mathematically principled, resembling element-wise similarity score aggregation via multilinear polynomials. We demonstrate the effectiveness of our graph-based fusion method on video anomaly detection, showing strong performance across multi-representational, multi-modal, and multi-domain feature fusion tasks.</li>
</ul>

<h3>Title: Disentangling Latent Shifts of In-Context Learning Through Self-Training</h3>
<ul>
<li><strong>Authors: </strong>Josip Jukić, Jan Šnajder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01508">https://arxiv.org/abs/2410.01508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01508">https://arxiv.org/pdf/2410.01508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01508]] Disentangling Latent Shifts of In-Context Learning Through Self-Training(https://arxiv.org/abs/2410.01508)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has become essential in natural language processing, particularly with autoregressive large language models capable of learning from demonstrations provided within the prompt. However, ICL faces challenges with stability and long contexts, especially as the number of demonstrations grows, leading to poor generalization and inefficient inference. To address these issues, we introduce STICL (Self-Training ICL), an approach that disentangles the latent shifts of demonstrations from the latent shift of the query through self-training. STICL employs a teacher model to generate pseudo-labels and trains a student model using these labels, encoded in an adapter module. The student model exhibits weak-to-strong generalization, progressively refining its predictions over time. Our empirical results show that STICL improves generalization and stability, consistently outperforming traditional ICL methods and other disentangling strategies across both in-domain and out-of-domain data.</li>
</ul>

<h3>Title: Multi-Scale Fusion for Object Representation</h3>
<ul>
<li><strong>Authors: </strong>Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01539">https://arxiv.org/abs/2410.01539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01539">https://arxiv.org/pdf/2410.01539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01539]] Multi-Scale Fusion for Object Representation(https://arxiv.org/abs/2410.01539)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Representing images or videos as object-level feature vectors, rather than pixel-level feature maps, facilitates advanced visual tasks. Object-Centric Learning (OCL) primarily achieves this by reconstructing the input under the guidance of Variational Autoencoder (VAE) intermediate representation to drive so-called \textit{slots} to aggregate as much object information as possible. However, existing VAE guidance does not explicitly address that objects can vary in pixel sizes while models typically excel at specific pattern scales. We propose \textit{Multi-Scale Fusion} (MSF) to enhance VAE guidance for OCL training. To ensure objects of all sizes fall within VAE's comfort zone, we adopt the \textit{image pyramid}, which produces intermediate representations at multiple scales; To foster scale-invariance/variance in object super-pixels, we devise \textit{inter}/\textit{intra-scale fusion}, which augments low-quality object super-pixels of one scale with corresponding high-quality super-pixels from another scale. On standard OCL benchmarks, our technique improves mainstream methods, including state-of-the-art diffusion-based ones. The source code is available in the supplemental material.</li>
</ul>

<h3>Title: Edge-preserving noise for diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Jente Vandersanden, Sascha Holl, Xingchang Huang, Gurprit Singh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01540">https://arxiv.org/abs/2410.01540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01540">https://arxiv.org/pdf/2410.01540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01540]] Edge-preserving noise for diffusion models(https://arxiv.org/abs/2410.01540)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Classical generative diffusion models learn an isotropic Gaussian denoising process, treating all spatial regions uniformly, thus neglecting potentially valuable structural information in the data. Inspired by the long-established work on anisotropic diffusion in image processing, we present a novel edge-preserving diffusion model that is a generalization of denoising diffusion probablistic models (DDPM). In particular, we introduce an edge-aware noise scheduler that varies between edge-preserving and isotropic Gaussian noise. We show that our model's generative process converges faster to results that more closely match the target distribution. We demonstrate its capability to better learn the low-to-mid frequencies within the dataset, which plays a crucial role in representing shapes and structural information. Our edge-preserving diffusion process consistently outperforms state-of-the-art baselines in unconditional image generation. It is also more robust for generative tasks guided by a shape-based prior, such as stroke-to-image generation. We present qualitative and quantitative results showing consistent improvements (FID score) of up to 30% for both tasks.</li>
</ul>

<h3>Title: In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar Tasks</h3>
<ul>
<li><strong>Authors: </strong>Dingzirui Wang, Xuangliang Zhang, Qiguang Chen, Longxu Dou, Xiao Xu, Rongyu Cao, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01548">https://arxiv.org/abs/2410.01548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01548">https://arxiv.org/pdf/2410.01548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01548]] In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar Tasks(https://arxiv.org/abs/2410.01548)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is an effective approach to help large language models (LLMs) adapt to various tasks by providing demonstrations of the target task. Considering the high cost of labeling demonstrations, many methods propose synthesizing demonstrations from scratch using LLMs. However, the quality of the demonstrations synthesized from scratch is limited by the capabilities and knowledge of LLMs. To address this, inspired by transfer learning, we propose In-Context Transfer Learning (ICTL), which synthesizes target task demonstrations by transferring labeled demonstrations from similar source tasks. ICTL consists of two steps: source sampling and target transfer. First, we define an optimization objective, which minimizes transfer error to sample source demonstrations similar to the target task. Then, we employ LLMs to transfer the sampled source demonstrations to the target task, matching the definition and format of the target task. Experiments on Super-NI show that ICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the effectiveness of our method.</li>
</ul>

<h3>Title: Bayes' Power for Explaining In-Context Learning Generalizations</h3>
<ul>
<li><strong>Authors: </strong>Samuel Müller, Noah Hollmann, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01565">https://arxiv.org/abs/2410.01565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01565">https://arxiv.org/pdf/2410.01565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01565]] Bayes' Power for Explaining In-Context Learning Generalizations(https://arxiv.org/abs/2410.01565)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, in-context</a></li>
<li><strong>Abstract: </strong>Traditionally, neural network training has been primarily viewed as an approximation of maximum likelihood estimation (MLE). This interpretation originated in a time when training for multiple epochs on small datasets was common and performance was data bound; but it falls short in the era of large-scale single-epoch trainings ushered in by large self-supervised setups, like language models. In this new setup, performance is compute-bound, but data is readily available. As models became more powerful, in-context learning (ICL), i.e., learning in a single forward-pass based on the context, emerged as one of the dominant paradigms. In this paper, we argue that a more useful interpretation of neural network behavior in this era is as an approximation of the true posterior, as defined by the data-generating process. We demonstrate this interpretations' power for ICL and its usefulness to predict generalizations to previously unseen tasks. We show how models become robust in-context learners by effectively composing knowledge from their training data. We illustrate this with experiments that reveal surprising generalizations, all explicable through the exact posterior. Finally, we show the inherent constraints of the generalization capabilities of posteriors and the limitations of neural networks in approximating these posteriors.</li>
</ul>

<h3>Title: Fake It Until You Break It: On the Adversarial Robustness of AI-generated Image Detectors</h3>
<ul>
<li><strong>Authors: </strong>Sina Mavali, Jonas Ricker, David Pape, Yash Sharma, Asja Fischer, Lea Schoenherr</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01574">https://arxiv.org/abs/2410.01574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01574">https://arxiv.org/pdf/2410.01574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01574]] Fake It Until You Break It: On the Adversarial Robustness of AI-generated Image Detectors(https://arxiv.org/abs/2410.01574)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While generative AI (GenAI) offers countless possibilities for creative and productive tasks, artificially generated media can be misused for fraud, manipulation, scams, misinformation campaigns, and more. To mitigate the risks associated with maliciously generated media, forensic classifiers are employed to identify AI-generated content. However, current forensic classifiers are often not evaluated in practically relevant scenarios, such as the presence of an attacker or when real-world artifacts like social media degradations affect images. In this paper, we evaluate state-of-the-art AI-generated image (AIGI) detectors under different attack scenarios. We demonstrate that forensic classifiers can be effectively attacked in realistic settings, even when the attacker does not have access to the target model and post-processing occurs after the adversarial examples are created, which is standard on social media platforms. These attacks can significantly reduce detection accuracy to the extent that the risks of relying on detectors outweigh their benefits. Finally, we propose a simple defense mechanism to make CLIP-based detectors, which are currently the best-performing detectors, robust against these attacks.</li>
</ul>

<h3>Title: MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Mingzhen Sun, Weining Wang, Yanyuan Qiao, Jiahui Sun, Zihan Qin, Longteng Guo, Xinxin Zhu, Jing Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01594">https://arxiv.org/abs/2410.01594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01594">https://arxiv.org/pdf/2410.01594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01594]] MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation(https://arxiv.org/abs/2410.01594)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Sounding Video Generation (SVG) is an audio-video joint generation task challenged by high-dimensional signal spaces, distinct data formats, and different patterns of content information. To address these issues, we introduce a novel multi-modal latent diffusion model (MM-LDM) for the SVG task. We first unify the representation of audio and video data by converting them into a single or a couple of images. Then, we introduce a hierarchical multi-modal autoencoder that constructs a low-level perceptual latent space for each modality and a shared high-level semantic feature space. The former space is perceptually equivalent to the raw signal space of each modality but drastically reduces signal dimensions. The latter space serves to bridge the information gap between modalities and provides more insightful cross-modal guidance. Our proposed method achieves new state-of-the-art results with significant quality and efficiency gains. Specifically, our method achieves a comprehensive improvement on all evaluation metrics and a faster training and sampling speed on Landscape and AIST++ datasets. Moreover, we explore its performance on open-domain sounding video generation, long sounding video generation, audio continuation, video continuation, and conditional single-modal generation tasks for a comprehensive evaluation, where our MM-LDM demonstrates exciting adaptability and generalization ability.</li>
</ul>

<h3>Title: KnobGen: Controlling the Sophistication of Artwork in Sketch-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pouyan Navard, Amin Karimi Monsefi, Mengxi Zhou, Wei-Lun Chao, Alper Yilmaz, Rajiv Ramnath</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01595">https://arxiv.org/abs/2410.01595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01595">https://arxiv.org/pdf/2410.01595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01595]] KnobGen: Controlling the Sophistication of Artwork in Sketch-Based Diffusion Models(https://arxiv.org/abs/2410.01595)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly improved text-to-image (T2I) generation, but they often struggle to balance fine-grained precision with high-level control. Methods like ControlNet and T2I-Adapter excel at following sketches by seasoned artists but tend to be overly rigid, replicating unintentional flaws in sketches from novice users. Meanwhile, coarse-grained methods, such as sketch-based abstraction frameworks, offer more accessible input handling but lack the precise control needed for detailed, professional use. To address these limitations, we propose KnobGen, a dual-pathway framework that democratizes sketch-based image generation by seamlessly adapting to varying levels of sketch complexity and user skill. KnobGen uses a Coarse-Grained Controller (CGC) module for high-level semantics and a Fine-Grained Controller (FGC) module for detailed refinement. The relative strength of these two modules can be adjusted through our knob inference mechanism to align with the user's specific needs. These mechanisms ensure that KnobGen can flexibly generate images from both novice sketches and those drawn by seasoned artists. This maintains control over the final output while preserving the natural appearance of the image, as evidenced on the MultiGen-20M dataset and a newly collected sketch dataset.</li>
</ul>

<h3>Title: ENTP: Encoder-only Next Token Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ethan Ewer, Daewon Chae, Thomas Zeng, Jinkyu Kim, Kangwook Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01600">https://arxiv.org/abs/2410.01600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01600">https://arxiv.org/pdf/2410.01600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01600]] ENTP: Encoder-only Next Token Prediction(https://arxiv.org/abs/2410.01600)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Next-token prediction models have predominantly relied on decoder-only Transformers with causal attention, driven by the common belief that causal attention is essential to prevent "cheating" by masking future tokens. We challenge this widely accepted notion and argue that this design choice is about efficiency rather than necessity. While decoder-only Transformers are still a good choice for practical reasons, they are not the only viable option. In this work, we introduce Encoder-only Next Token Prediction (ENTP). We explore the differences between ENTP and decoder-only Transformers in expressive power and complexity, highlighting potential advantages of ENTP. We introduce the Triplet-Counting task and show, both theoretically and experimentally, that while ENTP can perform this task easily, a decoder-only Transformer cannot. Finally, we empirically demonstrate ENTP's superior performance across various realistic tasks, such as length generalization and in-context learning.</li>
</ul>

<h3>Title: Automated Red Teaming with GOAT: the Generative Offensive Agent Tester</h3>
<ul>
<li><strong>Authors: </strong>Maya Pavlova, Erik Brinkman, Krithika Iyer, Vitor Albiero, Joanna Bitton, Hailey Nguyen, Joe Li, Cristian Canton Ferrer, Ivan Evtimov, Aaron Grattafiori</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01606">https://arxiv.org/abs/2410.01606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01606">https://arxiv.org/pdf/2410.01606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01606]] Automated Red Teaming with GOAT: the Generative Offensive Agent Tester(https://arxiv.org/abs/2410.01606)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Red teaming assesses how large language models (LLMs) can produce content that violates norms, policies, and rules set during their safety training. However, most existing automated methods in the literature are not representative of the way humans tend to interact with AI models. Common users of AI models may not have advanced knowledge of adversarial machine learning methods or access to model internals, and they do not spend a lot of time crafting a single highly effective adversarial prompt. Instead, they are likely to make use of techniques commonly shared online and exploit the multiturn conversational nature of LLMs. While manual testing addresses this gap, it is an inefficient and often expensive process. To address these limitations, we introduce the Generative Offensive Agent Tester (GOAT), an automated agentic red teaming system that simulates plain language adversarial conversations while leveraging multiple adversarial prompting techniques to identify vulnerabilities in LLMs. We instantiate GOAT with 7 red teaming attacks by prompting a general-purpose model in a way that encourages reasoning through the choices of methods available, the current target model's response, and the next steps. Our approach is designed to be extensible and efficient, allowing human testers to focus on exploring new areas of risk while automation covers the scaled adversarial stress-testing of known risk territory. We present the design and evaluation of GOAT, demonstrating its effectiveness in identifying vulnerabilities in state-of-the-art LLMs, with an ASR@10 of 97% against Llama 3.1 and 88% against GPT-4 on the JailbreakBench dataset.</li>
</ul>

<h3>Title: Intent Detection in the Age of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Arora, Shreya Jain, Srujana Merugu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01627">https://arxiv.org/abs/2410.01627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01627">https://arxiv.org/pdf/2410.01627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01627]] Intent Detection in the Age of LLMs(https://arxiv.org/abs/2410.01627)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Intent detection is a critical component of task-oriented dialogue systems (TODS) which enables the identification of suitable actions to address user utterances at each dialog turn. Traditional approaches relied on computationally efficient supervised sentence transformer encoder models, which require substantial training data and struggle with out-of-scope (OOS) detection. The emergence of generative large language models (LLMs) with intrinsic world knowledge presents new opportunities to address these challenges. In this work, we adapt 7 SOTA LLMs using adaptive in-context learning and chain-of-thought prompting for intent detection, and compare their performance with contrastively fine-tuned sentence transformer (SetFit) models to highlight prediction quality and latency tradeoff. We propose a hybrid system using uncertainty based routing strategy to combine the two approaches that along with negative data augmentation results in achieving the best of both worlds ( i.e. within 2% of native LLM accuracy with 50% less latency). To better understand LLM OOS detection capabilities, we perform controlled experiments revealing that this capability is significantly influenced by the scope of intent labels and the size of the label space. We also introduce a two-step approach utilizing internal LLM representations, demonstrating empirical gains in OOS detection accuracy and F1-score by >5% for the Mistral-7B model.</li>
</ul>

<h3>Title: Efficient Long-range Language Modeling with Self-supervised Causal Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Xiang Hu, Zhihao Teng, Wei Wu, Kewei Tu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01651">https://arxiv.org/abs/2410.01651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01651">https://arxiv.org/pdf/2410.01651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01651]] Efficient Long-range Language Modeling with Self-supervised Causal Retrieval(https://arxiv.org/abs/2410.01651)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recently, retrieval-based language models (RLMs) have received much attention. However, most of them leverage a pre-trained retriever with fixed parameters, which may not adapt well to causal language models. In this work, we propose Grouped Cross-Attention, a novel module enabling joint pre-training of the retriever and causal LM, and apply it to long-context modeling. For a given input sequence, we split it into chunks and use the current chunk to retrieve past chunks for subsequent text generation. Our innovation allows the retriever to learn how to retrieve past chunks that better minimize the auto-regressive loss of subsequent tokens in an end-to-end manner. By integrating top-$k$ retrieval, our model can be pre-trained efficiently from scratch with context lengths up to 64K tokens. Our experiments show our model, compared with long-range LM baselines, can achieve lower perplexity with comparable or lower pre-training and inference costs.</li>
</ul>

<h3>Title: Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering</h3>
<ul>
<li><strong>Authors: </strong>Klaus-Rudolf Kladny, Bernhard Schölkopf, Michael Muehlebach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01660">https://arxiv.org/abs/2410.01660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01660">https://arxiv.org/pdf/2410.01660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01660]] Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering(https://arxiv.org/abs/2410.01660)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models lack rigorous statistical guarantees for their outputs and are therefore unreliable in safety-critical applications. In this work, we propose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a sequential conformal prediction method producing prediction sets that satisfy a rigorous statistical guarantee called conformal admissibility control. This guarantee states that with high probability, the prediction sets contain at least one admissible (or valid) example. To this end, our method first samples an initial set of i.i.d. examples from a black box generative model. Then, this set is iteratively pruned via so-called greedy filters. As a consequence of the iterative generation procedure, admissibility of the final prediction set factorizes as a Markov chain. This factorization is crucial, because it allows to control each factor separately, using conformal prediction. In comparison to prior work, our method demonstrates a large reduction in the number of admissibility evaluations during calibration. This reduction is important in safety-critical applications, where these evaluations must be conducted manually by domain experts and are therefore costly and time consuming. We highlight the advantages of our method in terms of admissibility evaluations and cardinality of the prediction sets through experiments in natural language generation and molecular graph extension tasks.</li>
</ul>

<h3>Title: PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation</h3>
<ul>
<li><strong>Authors: </strong>Mike Ranzinger, Jon Barker, Greg Heinrich, Pavlo Molchanov, Bryan Catanzaro, Andrew Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01680">https://arxiv.org/abs/2410.01680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01680">https://arxiv.org/pdf/2410.01680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01680]] PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation(https://arxiv.org/abs/2410.01680)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Various visual foundation models have distinct strengths and weaknesses, both of which can be improved through heterogeneous multi-teacher knowledge distillation without labels, termed "agglomerative models." We build upon this body of work by studying the effect of the teachers' activation statistics, particularly the impact of the loss function on the resulting student model quality. We explore a standard toolkit of statistical normalization techniques to better align the different distributions and assess their effects. Further, we examine the impact on downstream teacher-matching metrics, which motivates the use of Hadamard matrices. With these matrices, we demonstrate useful properties, showing how they can be used for isotropic standardization, where each dimension of a multivariate distribution is standardized using the same scale. We call this technique "PHI Standardization" (PHI-S) and empirically demonstrate that it produces the best student model across the suite of methods studied.</li>
</ul>

<h3>Title: An Exploration of Self-Supervised Mutual Information Alignment for Multi-Task Settings</h3>
<ul>
<li><strong>Authors: </strong>Soham Govande</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01704">https://arxiv.org/abs/2410.01704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01704">https://arxiv.org/pdf/2410.01704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01704]] An Exploration of Self-Supervised Mutual Information Alignment for Multi-Task Settings(https://arxiv.org/abs/2410.01704)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>There is a growing need for pluralistic alignment methods that can steer language models towards individual attributes and preferences. One such method, Self-Supervised Alignment with Mutual Information (SAMI), uses conditional mutual information to encourage the connection between behavioral preferences and model responses. We conduct two experiments exploring SAMI in multi-task settings. First, we compare SAMI to Direct Preference Optimization (DPO) on a multi-task benchmark (MT-Bench), using a stronger model to generate training data for a weaker one across diverse categories (humanities, STEM, extraction, coding, math, reasoning, and roleplay). Our results indicate that one iteration of SAMI has a 57% win rate against DPO, with significant variation in performance between task categories. Second, we examine SAMI's impact on mathematical accuracy (GSM-8K) relative to supervised fine-tuning (SFT). While SAMI increases zero-shot performance by 1.1%, SFT is more effective with a 3.2% boost. However, SAMI shows interesting scaling trends. When given 10 attempts, SAMI improves accuracy by 3.9%, while SFT achieves a 10.1% increase. Combining SAMI with SFT yields an additional improvement of 1.3% in multi-attempt settings, though single-attempt accuracy remains unchanged.</li>
</ul>

<h3>Title: Examining the Role of Relationship Alignment in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kristen M. Altenburger, Hongda Jiang, Robert E. Kraut, Yi-Chia Wang, Jane Dwivedi-Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01708">https://arxiv.org/abs/2410.01708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01708">https://arxiv.org/pdf/2410.01708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01708]] Examining the Role of Relationship Alignment in Large Language Models(https://arxiv.org/abs/2410.01708)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid development and deployment of Generative AI in social settings raise important questions about how to optimally personalize them for users while maintaining accuracy and realism. Based on a Facebook public post-comment dataset, this study evaluates the ability of Llama 3.0 (70B) to predict the semantic tones across different combinations of a commenter's and poster's gender, age, and friendship closeness and to replicate these differences in LLM-generated comments. The study consists of two parts: Part I assesses differences in semantic tones across social relationship categories, and Part II examines the similarity between comments generated by Llama 3.0 (70B) and human comments from Part I given public Facebook posts as input. Part I results show that including social relationship information improves the ability of a model to predict the semantic tone of human comments. However, Part II results show that even without including social context information in the prompt, LLM-generated comments and human comments are equally sensitive to social context, suggesting that LLMs can comprehend semantics from the original post alone. When we include all social relationship information in the prompt, the similarity between human comments and LLM-generated comments decreases. This inconsistency may occur because LLMs did not include social context information as part of their training data. Together these results demonstrate the ability of LLMs to comprehend semantics from the original post and respond similarly to human comments, but also highlights their limitations in generalizing personalized comments through prompting alone.</li>
</ul>

<h3>Title: Meta-TTT: A Meta-learning Minimax Framework For Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Chen Tao, Li Shen, Soumik Mondal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01709">https://arxiv.org/abs/2410.01709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01709">https://arxiv.org/pdf/2410.01709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01709]] Meta-TTT: A Meta-learning Minimax Framework For Test-Time Training(https://arxiv.org/abs/2410.01709)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Test-time domain adaptation is a challenging task that aims to adapt a pre-trained model to limited, unlabeled target data during inference. Current methods that rely on self-supervision and entropy minimization underperform when the self-supervised learning (SSL) task does not align well with the primary objective. Additionally, minimizing entropy can lead to suboptimal solutions when there is limited diversity within minibatches. This paper introduces a meta-learning minimax framework for test-time training on batch normalization (BN) layers, ensuring that the SSL task aligns with the primary task while addressing minibatch overfitting. We adopt a mixed-BN approach that interpolates current test batch statistics with the statistics from source domains and propose a stochastic domain synthesizing method to improve model generalization and robustness to domain shifts. Extensive experiments demonstrate that our method surpasses state-of-the-art techniques across various domain adaptation and generalization benchmarks, significantly enhancing the pre-trained model's robustness on unseen domains.</li>
</ul>

<h3>Title: COMUNI: Decomposing Common and Unique Video Signals for Diffusion-based Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Mingzhen Sun, Weining Wang, Xinxin Zhu, Jing Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01718">https://arxiv.org/abs/2410.01718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01718">https://arxiv.org/pdf/2410.01718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01718]] COMUNI: Decomposing Common and Unique Video Signals for Diffusion-based Video Generation(https://arxiv.org/abs/2410.01718)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Since videos record objects moving coherently, adjacent video frames have commonness (similar object appearances) and uniqueness (slightly changed postures). To prevent redundant modeling of common video signals, we propose a novel diffusion-based framework, named COMUNI, which decomposes the COMmon and UNIque video signals to enable efficient video generation. Our approach separates the decomposition of video signals from the task of video generation, thus reducing the computation complexity of generative models. In particular, we introduce CU-VAE to decompose video signals and encode them into latent features. To train CU-VAE in a self-supervised manner, we employ a cascading merge module to reconstitute video signals and a time-agnostic video decoder to reconstruct video frames. Then we propose CU-LDM to model latent features for video generation, which adopts two specific diffusion streams to simultaneously model the common and unique latent features. We further utilize additional joint modules for cross modeling of the common and unique latent features, and a novel position embedding method to ensure the content consistency and motion coherence of generated videos. The position embedding method incorporates spatial and temporal absolute position information into the joint modules. Extensive experiments demonstrate the necessity of decomposing common and unique video signals for video generation and the effectiveness and efficiency of our proposed method.</li>
</ul>

<h3>Title: HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01723">https://arxiv.org/abs/2410.01723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01723">https://arxiv.org/pdf/2410.01723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01723]] HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration(https://arxiv.org/abs/2410.01723)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have gained prominence for outstanding scalability and extraordinary performance in generative tasks. However, their considerable inference costs impede practical deployment. The feature cache mechanism, which involves storing and retrieving redundant computations across timesteps, holds promise for reducing per-step inference time in diffusion models. Most existing caching methods for DiT are manually designed. Although the learning-based approach attempts to optimize strategies adaptively, it suffers from discrepancies between training and inference, which hampers both the performance and acceleration ratio. Upon detailed analysis, we pinpoint that these discrepancies primarily stem from two aspects: (1) Prior Timestep Disregard, where training ignores the effect of cache usage at earlier timesteps, and (2) Objective Mismatch, where the training target (align predicted noise in each timestep) deviates from the goal of inference (generate the high-quality image). To alleviate these discrepancies, we propose HarmoniCa, a novel method that Harmonizes training and inference with a novel learning-based Caching framework built upon Step-Wise Denoising Training (SDT) and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional training paradigm, the newly proposed SDT maintains the continuity of the denoising process, enabling the model to leverage information from prior timesteps during training, similar to the way it operates during inference. Furthermore, we design IEPO, which integrates an efficient proxy mechanism to approximate the final image error caused by reusing the cached feature. Therefore, IEPO helps balance final image quality and cache utilization, resolving the issue of training that only considers the impact of cache usage on the predicted output at each timestep.</li>
</ul>

<h3>Title: RADAR: Robust Two-stage Modality-incomplete Industrial Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Bingchen Miao, Wenqiao Zhang, Juncheng Li, Siliang Tang, Zhaocheng Li, Haochen Shi, Jun Xiao, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01737">https://arxiv.org/abs/2410.01737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01737">https://arxiv.org/pdf/2410.01737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01737]] RADAR: Robust Two-stage Modality-incomplete Industrial Anomaly Detection(https://arxiv.org/abs/2410.01737)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Multimodal Industrial Anomaly Detection (MIAD), utilizing 3D point clouds and 2D RGB images to identify the abnormal region of products, plays a crucial role in industrial quality inspection. However, the conventional MIAD setting presupposes that all 2D and 3D modalities are paired, overlooking the fact that multimodal data collected from the real world is often imperfect due to missing modalities. Consequently, MIAD models that demonstrate robustness against modal-incomplete data are highly desirable in practice. To address this practical challenge, we introduce a first-of-its-kind study that comprehensively investigates Modality-Incomplete Industrial Anomaly Detection (MIIAD), to consider the imperfect learning environment in which the multimodal information may be incomplete. Not surprisingly, we discovered that most existing MIAD approaches are inadequate for addressing MIIAD challenges, leading to significant performance degradation on the MIIAD benchmark we developed. In this paper, we propose a novel two-stage Robust modAlity-imcomplete fusing and Detecting frAmewoRk, abbreviated as RADAR. Our bootstrapping philosophy is to enhance two stages in MIIAD, improving the robustness of the Multimodal Transformer: i) In feature fusion, we first explore learning modality-incomplete instruction, guiding the pre-trained Multimodal Transformer to robustly adapt to various modality-incomplete scenarios, and implement adaptive parameter learning based on a HyperNetwork; ii) In anomaly detection, we construct a real-pseudo hybrid module to highlight the distinctiveness of modality combinations, further enhancing the robustness of the MIIAD model. Our experimental results demonstrate that the proposed RADAR significantly surpasses conventional MIAD methods in terms of effectiveness and robustness on our newly created MIIAD dataset, underscoring its practical application value.</li>
</ul>

<h3>Title: VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kailai Feng, Yabo Zhang, Haodong Yu, Zhilong Ji, Jinfeng Bai, Hongzhi Zhang, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01738">https://arxiv.org/abs/2410.01738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01738">https://arxiv.org/pdf/2410.01738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01738]] VitaGlyph: Vitalizing Artistic Typography with Flexible Dual-branch Diffusion Models(https://arxiv.org/abs/2410.01738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Artistic typography is a technique to visualize the meaning of input character in an imaginable and readable manner. With powerful text-to-image diffusion models, existing methods directly design the overall geometry and texture of input character, making it challenging to ensure both creativity and legibility. In this paper, we introduce a dual-branch and training-free method, namely VitaGlyph, enabling flexible artistic typography along with controllable geometry change to maintain the readability. The key insight of VitaGlyph is to treat input character as a scene composed of Subject and Surrounding, followed by rendering them under varying degrees of geometry transformation. The subject flexibly expresses the essential concept of input character, while the surrounding enriches relevant background without altering the shape. Specifically, we implement VitaGlyph through a three-phase framework: (i) Knowledge Acquisition leverages large language models to design text descriptions of subject and surrounding. (ii) Regional decomposition detects the part that most matches the subject description and divides input glyph image into subject and surrounding regions. (iii) Typography Stylization firstly refines the structure of subject region via Semantic Typography, and then separately renders the textures of Subject and Surrounding regions through Controllable Compositional Generation. Experimental results demonstrate that VitaGlyph not only achieves better artistry and readability, but also manages to depict multiple customize concepts, facilitating more creative and pleasing artistic typography generation. Our code will be made publicly at this https URL.</li>
</ul>

<h3>Title: ImageFolder: Autoregressive Image Generation with Folded Tokens</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, Zhe Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01756">https://arxiv.org/abs/2410.01756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01756">https://arxiv.org/pdf/2410.01756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01756]] ImageFolder: Autoregressive Image Generation with Folded Tokens(https://arxiv.org/abs/2410.01756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Image tokenizers are crucial for visual generative models, e.g., diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling. Increasing token length is a common approach to improve the image reconstruction quality. However, tokenizers with longer token lengths are not guaranteed to achieve better generation quality. There exists a trade-off between reconstruction and generation quality regarding token length. In this paper, we investigate the impact of token length on both image reconstruction and generation and provide a flexible solution to the tradeoff. We propose ImageFolder, a semantic tokenizer that provides spatially aligned image tokens that can be folded during autoregressive modeling to improve both generation efficiency and quality. To enhance the representative capability without increasing token length, we leverage dual-branch product quantization to capture different contexts of images. Specifically, semantic regularization is introduced in one branch to encourage compacted semantic information while another branch is designed to capture the remaining pixel-level details. Extensive experiments demonstrate the superior quality of image generation and shorter token length with ImageFolder tokenizer.</li>
</ul>

<h3>Title: Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context</h3>
<ul>
<li><strong>Authors: </strong>Spencer Frei, Gal Vardi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01774">https://arxiv.org/abs/2410.01774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01774">https://arxiv.org/pdf/2410.01774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01774]] Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context(https://arxiv.org/abs/2410.01774)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Transformers have the capacity to act as supervised learning algorithms: by properly encoding a set of labeled training ("in-context") examples and an unlabeled test example into an input sequence of vectors of the same dimension, the forward pass of the transformer can produce predictions for that unlabeled test example. A line of recent work has shown that when linear transformers are pre-trained on random instances for linear regression tasks, these trained transformers make predictions using an algorithm similar to that of ordinary least squares. In this work, we investigate the behavior of linear transformers trained on random linear classification tasks. Via an analysis of the implicit regularization of gradient descent, we characterize how many pre-training tasks and in-context examples are needed for the trained transformer to generalize well at test-time. We further show that in some settings, these trained transformers can exhibit "benign overfitting in-context": when in-context examples are corrupted by label flipping noise, the transformer memorizes all of its in-context examples (including those with noisy labels) yet still generalizes near-optimally for clean test examples.</li>
</ul>

<h3>Title: Bellman Diffusion: Generative Modeling as Learning a Linear Operator in the Distribution Space</h3>
<ul>
<li><strong>Authors: </strong>Yangming Li, Chieh-Hsin Lai, Carola-Bibiane Schönlieb, Yuki Mitsufuji, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01796">https://arxiv.org/abs/2410.01796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01796">https://arxiv.org/pdf/2410.01796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01796]] Bellman Diffusion: Generative Modeling as Learning a Linear Operator in the Distribution Space(https://arxiv.org/abs/2410.01796)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep Generative Models (DGMs), including Energy-Based Models (EBMs) and Score-based Generative Models (SGMs), have advanced high-fidelity data generation and complex continuous distribution approximation. However, their application in Markov Decision Processes (MDPs), particularly in distributional Reinforcement Learning (RL), remains underexplored, with conventional histogram-based methods dominating the field. This paper rigorously highlights that this application gap is caused by the nonlinearity of modern DGMs, which conflicts with the linearity required by the Bellman equation in MDPs. For instance, EBMs involve nonlinear operations such as exponentiating energy functions and normalizing constants. To address this, we introduce Bellman Diffusion, a novel DGM framework that maintains linearity in MDPs through gradient and scalar field modeling. With divergence-based training techniques to optimize neural network proxies and a new type of stochastic differential equation (SDE) for sampling, Bellman Diffusion is guaranteed to converge to the target distribution. Our empirical results show that Bellman Diffusion achieves accurate field estimations and is a capable image generator, converging 1.5x faster than the traditional histogram-based baseline in distributional RL tasks. This work enables the effective integration of DGMs into MDP applications, unlocking new avenues for advanced decision-making frameworks.</li>
</ul>

<h3>Title: FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images</h3>
<ul>
<li><strong>Authors: </strong>Cheng Zhang, Yuanhao Wang, Francisco Vicente Carrasco, Chenglei Wu, Jinlong Yang, Thabo Beeler, Fernando De la Torre</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.01801">https://arxiv.org/abs/2410.01801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.01801">https://arxiv.org/pdf/2410.01801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.01801]] FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments Generation from In-The-Wild Clothing Images(https://arxiv.org/abs/2410.01801)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce FabricDiffusion, a method for transferring fabric textures from a single clothing image to 3D garments of arbitrary shapes. Existing approaches typically synthesize textures on the garment surface through 2D-to-3D texture mapping or depth-aware inpainting via generative models. Unfortunately, these methods often struggle to capture and preserve texture details, particularly due to challenging occlusions, distortions, or poses in the input image. Inspired by the observation that in the fashion industry, most garments are constructed by stitching sewing patterns with flat, repeatable textures, we cast the task of clothing texture transfer as extracting distortion-free, tileable texture materials that are subsequently mapped onto the UV space of the garment. Building upon this insight, we train a denoising diffusion model with a large-scale synthetic dataset to rectify distortions in the input texture image. This process yields a flat texture map that enables a tight coupling with existing Physically-Based Rendering (PBR) material generation pipelines, allowing for realistic relighting of the garment under various lighting conditions. We show that FabricDiffusion can transfer various features from a single clothing image including texture patterns, material properties, and detailed prints and logos. Extensive experiments demonstrate that our model significantly outperforms state-to-the-art methods on both synthetic data and real-world, in-the-wild clothing images while generalizing to unseen textures and garment shapes.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
