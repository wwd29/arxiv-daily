<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: MVDream: Multi-view Diffusion for 3D Generation. (arXiv:2308.16512v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16512">http://arxiv.org/abs/2308.16512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16512]] MVDream: Multi-view Diffusion for 3D Generation(http://arxiv.org/abs/2308.16512)</code></li>
<li>Summary: <p>We propose MVDream, a multi-view diffusion model that is able to generate
geometrically consistent multi-view images from a given text prompt. By
leveraging image diffusion models pre-trained on large-scale web datasets and a
multi-view dataset rendered from 3D assets, the resulting multi-view diffusion
model can achieve both the generalizability of 2D diffusion and the consistency
of 3D data. Such a model can thus be applied as a multi-view prior for 3D
generation via Score Distillation Sampling, where it greatly improves the
stability of existing 2D-lifting methods by solving the 3D consistency problem.
Finally, we show that the multi-view diffusion model can also be fine-tuned
under a few shot setting for personalized 3D generation, i.e. DreamBooth3D
application, where the consistency can be maintained after learning the subject
identity.
</p></li>
</ul>

<h3>Title: Any-Size-Diffusion: Toward Efficient Text-Driven Synthesis for Any-Size HD Images. (arXiv:2308.16582v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16582">http://arxiv.org/abs/2308.16582</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16582]] Any-Size-Diffusion: Toward Efficient Text-Driven Synthesis for Any-Size HD Images(http://arxiv.org/abs/2308.16582)</code></li>
<li>Summary: <p>Stable diffusion, a generative model used in text-to-image synthesis,
frequently encounters resolution-induced composition problems when generating
images of varying sizes. This issue primarily stems from the model being
trained on pairs of single-scale images and their corresponding text
descriptions. Moreover, direct training on images of unlimited sizes is
unfeasible, as it would require an immense number of text-image pairs and
entail substantial computational expenses. To overcome these challenges, we
propose a two-stage pipeline named Any-Size-Diffusion (ASD), designed to
efficiently generate well-composed images of any size, while minimizing the
need for high-memory GPU resources. Specifically, the initial stage, dubbed Any
Ratio Adaptability Diffusion (ARAD), leverages a selected set of images with a
restricted range of ratios to optimize the text-conditional diffusion model,
thereby improving its ability to adjust composition to accommodate diverse
image sizes. To support the creation of images at any desired size, we further
introduce a technique called Fast Seamless Tiled Diffusion (FSTD) at the
subsequent stage. This method allows for the rapid enlargement of the ASD
output to any high-resolution size, avoiding seaming artifacts or memory
overloads. Experimental results on the LAION-COCO and MM-CelebA-HQ benchmarks
demonstrate that ASD can produce well-structured images of arbitrary sizes,
cutting down the inference time by 2x compared to the traditional tiled
algorithm.
</p></li>
</ul>

<h3>Title: Detecting Out-of-Context Image-Caption Pairs in News: A Counter-Intuitive Method. (arXiv:2308.16611v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16611">http://arxiv.org/abs/2308.16611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16611]] Detecting Out-of-Context Image-Caption Pairs in News: A Counter-Intuitive Method(http://arxiv.org/abs/2308.16611)</code></li>
<li>Summary: <p>The growth of misinformation and re-contextualized media in social media and
news leads to an increasing need for fact-checking methods. Concurrently, the
advancement in generative models makes cheapfakes and deepfakes both easier to
make and harder to detect. In this paper, we present a novel approach using
generative image models to our advantage for detecting Out-of-Context (OOC) use
of images-caption pairs in news. We present two new datasets with a total of
$6800$ images generated using two different generative models including (1)
DALL-E 2, and (2) Stable-Diffusion. We are confident that the method proposed
in this paper can further research on generative models in the field of
cheapfake detection, and that the resulting datasets can be used to train and
evaluate new models aimed at detecting cheapfakes. We run a preliminary
qualitative and quantitative analysis to evaluate the performance of each image
generation model for this task, and evaluate a handful of methods for computing
image similarity.
</p></li>
</ul>

<h3>Title: MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model. (arXiv:2308.16635v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16635">http://arxiv.org/abs/2308.16635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16635]] MFR-Net: Multi-faceted Responsive Listening Head Generation via Denoising Diffusion Model(http://arxiv.org/abs/2308.16635)</code></li>
<li>Summary: <p>Face-to-face communication is a common scenario including roles of speakers
and listeners. Most existing research methods focus on producing speaker
videos, while the generation of listener heads remains largely overlooked.
Responsive listening head generation is an important task that aims to model
face-to-face communication scenarios by generating a listener head video given
a speaker video and a listener head image. An ideal generated responsive
listening video should respond to the speaker with attitude or viewpoint
expressing while maintaining diversity in interaction patterns and accuracy in
listener identity information. To achieve this goal, we propose the
\textbf{M}ulti-\textbf{F}aceted \textbf{R}esponsive Listening Head Generation
Network (MFR-Net). Specifically, MFR-Net employs the probabilistic denoising
diffusion model to predict diverse head pose and expression features. In order
to perform multi-faceted response to the speaker video, while maintaining
accurate listener identity preservation, we design the Feature Aggregation
Module to boost listener identity features and fuse them with other
speaker-related features. Finally, a renderer finetuned with identity
consistency loss produces the final listening head videos. Our extensive
experiments demonstrate that MFR-Net not only achieves multi-faceted responses
in diversity and speaker identity information but also in attitude and
viewpoint expression.
</p></li>
</ul>

<h3>Title: Generate Your Own Scotland: Satellite Image Generation Conditioned on Maps. (arXiv:2308.16648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16648">http://arxiv.org/abs/2308.16648</a></li>
<li>Code URL: https://github.com/miquel-espinosa/map-sat</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16648]] Generate Your Own Scotland: Satellite Image Generation Conditioned on Maps(http://arxiv.org/abs/2308.16648)</code></li>
<li>Summary: <p>Despite recent advancements in image generation, diffusion models still
remain largely underexplored in Earth Observation. In this paper we show that
state-of-the-art pretrained diffusion models can be conditioned on cartographic
data to generate realistic satellite images. We provide two large datasets of
paired OpenStreetMap images and satellite views over the region of Mainland
Scotland and the Central Belt. We train a ControlNet model and qualitatively
evaluate the results, demonstrating that both image quality and map fidelity
are possible. Finally, we provide some insights on the opportunities and
challenges of applying these models for remote sensing. Our model weights and
code for creating the dataset are publicly available at
https://github.com/miquel-espinosa/map-sat.
</p></li>
</ul>

<h3>Title: Diffusion Inertial Poser: Human Motion Reconstruction from Arbitrary Sparse IMU Configurations. (arXiv:2308.16682v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16682">http://arxiv.org/abs/2308.16682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16682]] Diffusion Inertial Poser: Human Motion Reconstruction from Arbitrary Sparse IMU Configurations(http://arxiv.org/abs/2308.16682)</code></li>
<li>Summary: <p>Motion capture from a limited number of inertial measurement units (IMUs) has
important applications in health, human performance, and virtual reality.
Real-world limitations and application-specific goals dictate different IMU
configurations (i.e., number of IMUs and chosen attachment body segments),
trading off accuracy and practicality. Although recent works were successful in
accurately reconstructing whole-body motion from six IMUs, these systems only
work with a specific IMU configuration. Here we propose a single diffusion
generative model, Diffusion Inertial Poser (DiffIP), which reconstructs human
motion in real-time from arbitrary IMU configurations. We show that DiffIP has
the benefit of flexibility with respect to the IMU configuration while being as
accurate as the state-of-the-art for the commonly used six IMU configuration.
Our system enables selecting an optimal configuration for different
applications without retraining the model. For example, when only four IMUs are
available, DiffIP found that the configuration that minimizes errors in joint
kinematics instruments the thighs and forearms. However, global translation
reconstruction is better when instrumenting the feet instead of the thighs.
Although our approach is agnostic to the underlying model, we built DiffIP
based on physiologically realistic musculoskeletal models to enable use in
biomedical research and health applications.
</p></li>
</ul>

<h3>Title: Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance. (arXiv:2308.16725v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16725">http://arxiv.org/abs/2308.16725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16725]] Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance(http://arxiv.org/abs/2308.16725)</code></li>
<li>Summary: <p>Sketch-based terrain generation seeks to create realistic landscapes for
virtual environments in various applications such as computer games, animation
and virtual reality. Recently, deep learning based terrain generation has
emerged, notably the ones based on generative adversarial networks (GAN).
However, these methods often struggle to fulfill the requirements of flexible
user control and maintain generative diversity for realistic terrain.
Therefore, we propose a novel diffusion-based method, namely terrain diffusion
network (TDN), which actively incorporates user guidance for enhanced
controllability, taking into account terrain features like rivers, ridges,
basins, and peaks. Instead of adhering to a conventional monolithic denoising
process, which often compromises the fidelity of terrain details or the
alignment with user control, a multi-level denoising scheme is proposed to
generate more realistic terrains by taking into account fine-grained details,
particularly those related to climatic patterns influenced by erosion and
tectonic activities. Specifically, three terrain synthesisers are designed for
structural, intermediate, and fine-grained level denoising purposes, which
allow each synthesiser concentrate on a distinct terrain aspect. Moreover, to
maximise the efficiency of our TDN, we further introduce terrain and sketch
latent spaces for the synthesizers with pre-trained terrain autoencoders.
Comprehensive experiments on a new dataset constructed from NASA Topology
Images clearly demonstrate the effectiveness of our proposed method, achieving
the state-of-the-art performance. Our code and dataset will be publicly
available.
</p></li>
</ul>

<h3>Title: Diffusion Models for Interferometric Satellite Aperture Radar. (arXiv:2308.16847v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16847">http://arxiv.org/abs/2308.16847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16847]] Diffusion Models for Interferometric Satellite Aperture Radar(http://arxiv.org/abs/2308.16847)</code></li>
<li>Summary: <p>Probabilistic Diffusion Models (PDMs) have recently emerged as a very
promising class of generative models, achieving high performance in natural
image generation. However, their performance relative to non-natural images,
like radar-based satellite data, remains largely unknown. Generating large
amounts of synthetic (and especially labelled) satellite data is crucial to
implement deep-learning approaches for the processing and analysis of
(interferometric) satellite aperture radar data. Here, we leverage PDMs to
generate several radar-based satellite image datasets. We show that PDMs
succeed in generating images with complex and realistic structures, but that
sampling time remains an issue. Indeed, accelerated sampling strategies, which
work well on simple image datasets like MNIST, fail on our radar datasets. We
provide a simple and versatile open-source
https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and
evaluate PDMs using any dataset on a single GPU.
</p></li>
</ul>

<h3>Title: InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion. (arXiv:2308.16905v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16905">http://arxiv.org/abs/2308.16905</a></li>
<li>Code URL: https://github.com/Sirui-Xu/InterDiff</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16905]] InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion(http://arxiv.org/abs/2308.16905)</code></li>
<li>Summary: <p>This paper addresses a novel task of anticipating 3D human-object
interactions (HOIs). Most existing research on HOI synthesis lacks
comprehensive whole-body interactions with dynamic objects, e.g., often limited
to manipulating small or static objects. Our task is significantly more
challenging, as it requires modeling dynamic objects with various shapes,
capturing whole-body motion, and ensuring physically valid interactions. To
this end, we propose InterDiff, a framework comprising two key steps: (i)
interaction diffusion, where we leverage a diffusion model to encode the
distribution of future human-object interactions; (ii) interaction correction,
where we introduce a physics-informed predictor to correct denoised HOIs in a
diffusion step. Our key insight is to inject prior knowledge that the
interactions under reference with respect to contact points follow a simple
pattern and are easily predictable. Experiments on multiple human-object
interaction datasets demonstrate the effectiveness of our method for this task,
capable of producing realistic, vivid, and remarkably long-term 3D HOI
predictions.
</p></li>
</ul>

<h3>Title: Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16818">http://arxiv.org/abs/2308.16818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16818]] Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network(http://arxiv.org/abs/2308.16818)</code></li>
<li>Summary: <p>Accurate traffic forecasting at intersections governed by intelligent traffic
signals is critical for the advancement of an effective intelligent traffic
signal control system. However, due to the irregular traffic time series
produced by intelligent intersections, the traffic forecasting task becomes
much more intractable and imposes three major new challenges: 1) asynchronous
spatial dependency, 2) irregular temporal dependency among traffic data, and 3)
variable-length sequence to be predicted, which severely impede the performance
of current traffic forecasting methods. To this end, we propose an Asynchronous
Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic
states of the lanes entering intelligent intersections in a future time window.
Specifically, by linking lanes via a traffic diffusion graph, we first propose
an Asynchronous Graph Diffusion Network to model the asynchronous spatial
dependency between the time-misaligned traffic state measurements of lanes.
After that, to capture the temporal dependency within irregular traffic state
sequence, a learnable personalized time encoding is devised to embed the
continuous time for each lane. Then we propose a Transformable Time-aware
Convolution Network that learns meta-filters to derive time-aware convolution
filters with transformable filter sizes for efficient temporal convolution on
the irregular sequence. Furthermore, a Semi-Autoregressive Prediction Network
consisting of a state evolution unit and a semiautoregressive predictor is
designed to effectively and efficiently predict variable-length traffic state
sequences. Extensive experiments on two real-world datasets demonstrate the
effectiveness of ASeer in six metrics.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Emergence of Segmentation with Minimalistic White-Box Transformers. (arXiv:2308.16271v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16271">http://arxiv.org/abs/2308.16271</a></li>
<li>Code URL: https://github.com/ma-lab-berkeley/crate</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16271]] Emergence of Segmentation with Minimalistic White-Box Transformers(http://arxiv.org/abs/2308.16271)</code></li>
<li>Summary: <p>Transformer-like models for vision tasks have recently proven effective for a
wide range of downstream applications such as segmentation and detection.
Previous works have shown that segmentation properties emerge in vision
transformers (ViTs) trained using self-supervised methods such as DINO, but not
in those trained on supervised classification tasks. In this study, we probe
whether segmentation emerges in transformer-based models solely as a result of
intricate self-supervised learning mechanisms, or if the same emergence can be
achieved under much broader conditions through proper design of the model
architecture. Through extensive experimental results, we demonstrate that when
employing a white-box transformer-like architecture known as CRATE, whose
design explicitly models and pursues low-dimensional structures in the data
distribution, segmentation properties, at both the whole and parts levels,
already emerge with a minimalistic supervised training recipe. Layer-wise
finer-grained analysis reveals that the emergent properties strongly
corroborate the designed mathematical functions of the white-box network. Our
results suggest a path to design white-box foundation models that are
simultaneously highly performant and mathematically fully interpretable. Code
is at \url{https://github.com/Ma-Lab-Berkeley/CRATE}.
</p></li>
</ul>

<h3>Title: Catalog Phrase Grounding (CPG): Grounding of Product Textual Attributes in Product Images for e-commerce Vision-Language Applications. (arXiv:2308.16354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16354">http://arxiv.org/abs/2308.16354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16354]] Catalog Phrase Grounding (CPG): Grounding of Product Textual Attributes in Product Images for e-commerce Vision-Language Applications(http://arxiv.org/abs/2308.16354)</code></li>
<li>Summary: <p>We present Catalog Phrase Grounding (CPG), a model that can associate product
textual data (title, brands) into corresponding regions of product images
(isolated product region, brand logo region) for e-commerce vision-language
applications. We use a state-of-the-art modulated multimodal transformer
encoder-decoder architecture unifying object detection and phrase-grounding. We
train the model in self-supervised fashion with 2.3 million image-text pairs
synthesized from an e-commerce site. The self-supervision data is annotated
with high-confidence pseudo-labels generated with a combination of teacher
models: a pre-trained general domain phrase grounding model (e.g. MDETR) and a
specialized logo detection model. This allows CPG, as a student model, to
benefit from transfer knowledge from these base models combining general-domain
knowledge and specialized knowledge. Beyond immediate catalog phrase grounding
tasks, we can benefit from CPG representations by incorporating them as ML
features into downstream catalog applications that require deep semantic
understanding of products. Our experiments on product-brand matching, a
challenging e-commerce application, show that incorporating CPG representations
into the existing production ensemble system leads to on average 5% recall
improvement across all countries globally (with the largest lift of 11% in a
single country) at fixed 95% precision, outperforming other alternatives
including a logo detection teacher model and ResNet50.
</p></li>
</ul>

<h3>Title: Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16481">http://arxiv.org/abs/2308.16481</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16481]] Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning(http://arxiv.org/abs/2308.16481)</code></li>
<li>Summary: <p>We present Point-TTA, a novel test-time adaptation framework for point cloud
registration (PCR) that improves the generalization and the performance of
registration models. While learning-based approaches have achieved impressive
progress, generalization to unknown testing environments remains a major
challenge due to the variations in 3D scans. Existing methods typically train a
generic model and the same trained model is applied on each instance during
testing. This could be sub-optimal since it is difficult for the same model to
handle all the variations during testing. In this paper, we propose a test-time
adaptation approach for PCR. Our model can adapt to unseen distributions at
test-time without requiring any prior knowledge of the test data. Concretely,
we design three self-supervised auxiliary tasks that are optimized jointly with
the primary PCR task. Given a test instance, we adapt our model using these
auxiliary tasks and the updated model is used to perform the inference. During
training, our model is trained using a meta-auxiliary learning approach, such
that the adapted model via auxiliary tasks improves the accuracy of the primary
task. Experimental results demonstrate the effectiveness of our approach in
improving generalization of point cloud registration and outperforming other
state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16572">http://arxiv.org/abs/2308.16572</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16572]] CL-MAE: Curriculum-Learned Masked Autoencoders(http://arxiv.org/abs/2308.16572)</code></li>
<li>Summary: <p>Masked image modeling has been demonstrated as a powerful pretext task for
generating robust representations that can be effectively generalized across
multiple downstream tasks. Typically, this approach involves randomly masking
patches (tokens) in input images, with the masking strategy remaining unchanged
during training. In this paper, we propose a curriculum learning approach that
updates the masking strategy to continually increase the complexity of the
self-supervised reconstruction task. We conjecture that, by gradually
increasing the task complexity, the model can learn more sophisticated and
transferable representations. To facilitate this, we introduce a novel
learnable masking module that possesses the capability to generate masks of
different complexities, and integrate the proposed module into masked
autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting
its behavior during training, transitioning from a partner to the MAE
(optimizing the same reconstruction loss) to an adversary (optimizing the
opposite loss), while passing through a neutral state. The transition between
these behaviors is smooth, being regulated by a factor that is multiplied with
the reconstruction loss of the masking module. The resulting training procedure
generates an easy-to-hard curriculum. We train our Curriculum-Learned Masked
Autoencoder (CL-MAE) on ImageNet and show that it exhibits superior
representation learning capabilities compared to MAE. The empirical results on
five downstream tasks confirm our conjecture, demonstrating that curriculum
learning can be successfully used to self-supervise masked autoencoders.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art. (arXiv:2308.16316v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16316">http://arxiv.org/abs/2308.16316</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16316]] Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art(http://arxiv.org/abs/2308.16316)</code></li>
<li>Summary: <p>Since their inception in 2014, Generative Adversarial Networks (GANs) have
rapidly emerged as powerful tools for generating realistic and diverse data
across various domains, including computer vision and other applied areas.
Consisting of a discriminative network and a generative network engaged in a
Minimax game, GANs have revolutionized the field of generative modeling. In
February 2018, GAN secured the leading spot on the ``Top Ten Global
Breakthrough Technologies List'' issued by the Massachusetts Science and
Technology Review. Over the years, numerous advancements have been proposed,
leading to a rich array of GAN variants, such as conditional GAN, Wasserstein
GAN, CycleGAN, and StyleGAN, among many others. This survey aims to provide a
general overview of GANs, summarizing the latent architecture, validation
metrics, and application areas of the most widely recognized variants. We also
delve into recent theoretical developments, exploring the profound connection
between the adversarial principle underlying GAN and Jensen-Shannon divergence,
while discussing the optimality characteristics of the GAN framework. The
efficiency of GAN variants and their model architectures will be evaluated
along with training obstacles as well as training solutions. In addition, a
detailed discussion will be provided, examining the integration of GANs with
newly developed deep learning frameworks such as Transformers, Physics-Informed
Neural Networks, Large Language models, and Diffusion models. Finally, we
reveal several issues as well as future research outlines in this field.
</p></li>
</ul>

<h3>Title: Latent Painter. (arXiv:2308.16490v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16490">http://arxiv.org/abs/2308.16490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16490]] Latent Painter(http://arxiv.org/abs/2308.16490)</code></li>
<li>Summary: <p>Latent diffusers revolutionized the generative AI and inspired creative art.
When denoising the latent, the predicted original image at each step
collectively animates the formation. However, the animation is limited by the
denoising nature of the diffuser, and only renders a sharpening process. This
work presents Latent Painter, which uses the latent as the canvas, and the
diffuser predictions as the plan, to generate painting animation. Latent
Painter also transits one generated image to another, which can happen between
images from two different sets of checkpoints.
</p></li>
</ul>

<h3>Title: Robust GAN inversion. (arXiv:2308.16510v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16510">http://arxiv.org/abs/2308.16510</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16510]] Robust GAN inversion(http://arxiv.org/abs/2308.16510)</code></li>
<li>Summary: <p>Recent advancements in real image editing have been attributed to the
exploration of Generative Adversarial Networks (GANs) latent space. However,
the main challenge of this procedure is GAN inversion, which aims to map the
image to the latent space accurately. Existing methods that work on extended
latent space $W+$ are unable to achieve low distortion and high editability
simultaneously. To address this issue, we propose an approach which works in
native latent space $W$ and tunes the generator network to restore missing
image details. We introduce a novel regularization strategy with learnable
coefficients obtained by training randomized StyleGAN 2 model - WRanGAN. This
method outperforms traditional approaches in terms of reconstruction quality
and computational efficiency, achieving the lowest distortion with 4 times
fewer parameters. Furthermore, we observe a slight improvement in the quality
of constructing hyperplanes corresponding to binary image attributes. We
demonstrate the effectiveness of our approach on two complex datasets:
Flickr-Faces-HQ and LSUN Church.
</p></li>
</ul>

<h3>Title: Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models. (arXiv:2308.16777v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16777">http://arxiv.org/abs/2308.16777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16777]] Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models(http://arxiv.org/abs/2308.16777)</code></li>
<li>Summary: <p>Zero-shot referring image segmentation is a challenging task because it aims
to find an instance segmentation mask based on the given referring
descriptions, without training on this type of paired data. Current zero-shot
methods mainly focus on using pre-trained discriminative models (e.g., CLIP).
However, we have observed that generative models (e.g., Stable Diffusion) have
potentially understood the relationships between various visual elements and
text descriptions, which are rarely investigated in this task. In this work, we
introduce a novel Referring Diffusional segmentor (Ref-Diff) for this task,
which leverages the fine-grained multi-modal information from generative
models. We demonstrate that without a proposal generator, a generative model
alone can achieve comparable performance to existing SOTA weakly-supervised
models. When we combine both generative and discriminative models, our Ref-Diff
outperforms these competing methods by a significant margin. This indicates
that generative models are also beneficial for this task and can complement
discriminative models for better referring segmentation. Our code is publicly
available at https://github.com/kodenii/Ref-Diff.
</p></li>
</ul>

<h3>Title: PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16911">http://arxiv.org/abs/2308.16911</a></li>
<li>Code URL: https://github.com/openrobotlab/pointllm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16911]] PointLLM: Empowering Large Language Models to Understand Point Clouds(http://arxiv.org/abs/2308.16911)</code></li>
<li>Summary: <p>The unprecedented advancements in Large Language Models (LLMs) have created a
profound impact on natural language processing but are yet to fully embrace the
realm of 3D understanding. This paper introduces PointLLM, a preliminary effort
to fill this gap, thereby enabling LLMs to understand point clouds and offering
a new avenue beyond 2D visual data. PointLLM processes colored object point
clouds with human instructions and generates contextually appropriate
responses, illustrating its grasp of point clouds and common sense.
Specifically, it leverages a point cloud encoder with a powerful LLM to
effectively fuse geometric, appearance, and linguistic information. We collect
a novel dataset comprising 660K simple and 70K complex point-text instruction
pairs to enable a two-stage training strategy: initially aligning latent spaces
and subsequently instruction-tuning the unified model. To rigorously evaluate
our model's perceptual abilities and its generalization capabilities, we
establish two benchmarks: Generative 3D Object Classification and 3D Object
Captioning, assessed through three different methods, including human
evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment
results show that PointLLM demonstrates superior performance over existing 2D
baselines. Remarkably, in human-evaluated object captioning tasks, PointLLM
outperforms human annotators in over 50% of the samples. Codes, datasets, and
benchmarks are available at https://github.com/OpenRobotLab/PointLLM .
</p></li>
</ul>

<h3>Title: Unsupervised Text Style Transfer with Deep Generative Models. (arXiv:2308.16584v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16584">http://arxiv.org/abs/2308.16584</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16584]] Unsupervised Text Style Transfer with Deep Generative Models(http://arxiv.org/abs/2308.16584)</code></li>
<li>Summary: <p>We present a general framework for unsupervised text style transfer with deep
generative models. The framework models each sentence-label pair in the
non-parallel corpus as partially observed from a complete quadruplet which
additionally contains two latent codes representing the content and style,
respectively. These codes are learned by exploiting dependencies inside the
observed data. Then a sentence is transferred by manipulating them. Our
framework is able to unify previous embedding and prototype methods as two
special forms. It also provides a principled perspective to explain previously
proposed techniques in the field such as aligned encoder and adversarial
training. We further conduct experiments on three benchmarks. Both automatic
and human evaluation results show that our methods achieve better or
competitive results compared to several strong baselines.
</p></li>
</ul>

<h3>Title: Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16534">http://arxiv.org/abs/2308.16534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16534]] Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints(http://arxiv.org/abs/2308.16534)</code></li>
<li>Summary: <p>Score-based and diffusion models have emerged as effective approaches for
both conditional and unconditional generation. Still conditional generation is
based on either a specific training of a conditional model or classifier
guidance, which requires training a noise-dependent classifier, even when the
classifier for uncorrupted data is given. We propose an approach to sample from
unconditional score-based generative models enforcing arbitrary logical
constraints, without any additional training. Firstly, we show how to
manipulate the learned score in order to sample from an un-normalized
distribution conditional on a user-defined constraint. Then, we define a
flexible and numerically stable neuro-symbolic framework for encoding soft
logical constraints. Combining these two ingredients we obtain a general, but
approximate, conditional sampling algorithm. We further developed effective
heuristics aimed at improving the approximation. Finally, we show the
effectiveness of our approach for various types of constraints and data:
tabular data, images and time series.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Classification of Anomalies in Telecommunication Network KPI Time Series. (arXiv:2308.16279v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16279">http://arxiv.org/abs/2308.16279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16279]] Classification of Anomalies in Telecommunication Network KPI Time Series(http://arxiv.org/abs/2308.16279)</code></li>
<li>Summary: <p>The increasing complexity and scale of telecommunication networks have led to
a growing interest in automated anomaly detection systems. However, the
classification of anomalies detected on network Key Performance Indicators
(KPI) has received less attention, resulting in a lack of information about
anomaly characteristics and classification processes. To address this gap, this
paper proposes a modular anomaly classification framework. The framework
assumes separate entities for the anomaly classifier and the detector, allowing
for a distinct treatment of anomaly detection and classification tasks on time
series. The objectives of this study are (1) to develop a time series simulator
that generates synthetic time series resembling real-world network KPI
behavior, (2) to build a detection model to identify anomalies in the time
series, (3) to build classification models that accurately categorize detected
anomalies into predefined classes (4) to evaluate the classification framework
performance on simulated and real-world network KPI time series. This study has
demonstrated the good performance of the anomaly classification models trained
on simulated anomalies when applied to real-world network time series data.
</p></li>
</ul>

<h2>in-context</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
