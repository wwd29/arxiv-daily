<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-30</h1>
<h3>Title: Dissecting CLIP: Decomposition with a Schur Complement-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Azim Ospanov, Mohammad Jalali, Farzan Farnia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18645">https://arxiv.org/abs/2412.18645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18645">https://arxiv.org/pdf/2412.18645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18645]] Dissecting CLIP: Decomposition with a Schur Complement-based Approach(https://arxiv.org/abs/2412.18645)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The use of CLIP embeddings to assess the alignment of samples produced by text-to-image generative models has been extensively explored in the literature. While the widely adopted CLIPScore, derived from the cosine similarity of text and image embeddings, effectively measures the relevance of a generated image, it does not quantify the diversity of images generated by a text-to-image model. In this work, we extend the application of CLIP embeddings to quantify and interpret the intrinsic diversity of text-to-image models, which is responsible for generating diverse images from similar text prompts. To achieve this, we propose a decomposition of the CLIP-based kernel covariance matrix of image data into text-based and non-text-based components. Using the Schur complement of the joint image-text kernel covariance matrix, we perform this decomposition and define the matrix-based entropy of the decomposed component as the \textit{Schur Complement Entropy (SCE)} score, a measure of the intrinsic diversity of a text-to-image model based on data collected with varying text prompts. Additionally, we demonstrate the use of the Schur complement-based decomposition to nullify the influence of a given prompt in the CLIP embedding of an image, enabling focus or defocus of embeddings on specific objects or properties for downstream tasks. We present several numerical results that apply our Schur complement-based approach to evaluate text-to-image models and modify CLIP image embeddings. The codebase is available at this https URL</li>
</ul>

<h3>Title: Comparing analytic and data-driven approaches to parameter identifiability: A power systems case study</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Evangelou, Alexander M. Stankovic, Ioannis G. Kevrekidis, Mark K. Transtrum</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18663">https://arxiv.org/abs/2412.18663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18663">https://arxiv.org/pdf/2412.18663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18663]] Comparing analytic and data-driven approaches to parameter identifiability: A power systems case study(https://arxiv.org/abs/2412.18663)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Parameter identifiability refers to the capability of accurately inferring the parameter values of a model from its observations (data). Traditional analysis methods exploit analytical properties of the closed form model, in particular sensitivity analysis, to quantify the response of the model predictions to variations in parameters. Techniques developed to analyze data, specifically manifold learning methods, have the potential to complement, and even extend the scope of the traditional analytical approaches. We report on a study comparing and contrasting analytical and data-driven approaches to quantify parameter identifiability and, importantly, perform parameter reduction tasks. We use the infinite bus synchronous generator model, a well-understood model from the power systems domain, as our benchmark problem. Our traditional analysis methods use the Fisher Information Matrix to quantify parameter identifiability analysis, and the Manifold Boundary Approximation Method to perform parameter reduction. We compare these results to those arrived at through data-driven manifold learning schemes: Output - Diffusion Maps and Geometric Harmonics. For our test case, we find that the two suites of tools (analytical when a model is explicitly available, as well as data-driven when the model is lacking and only measurement data are available) give (correct) comparable results; these results are also in agreement with traditional analysis based on singular perturbation theory. We then discuss the prospects of using data-driven methods for such model analysis.</li>
</ul>

<h3>Title: Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Faraz Waseem, Muhammad Shahzad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18688">https://arxiv.org/abs/2412.18688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18688">https://arxiv.org/pdf/2412.18688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18688]] Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation(https://arxiv.org/abs/2412.18688)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>An image may convey a thousand words, but a video composed of hundreds or thousands of image frames tells a more intricate story. Despite significant progress in multimodal large language models (MLLMs), generating extended videos remains a formidable challenge. As of this writing, OpenAI's Sora, the current state-of-the-art system, is still limited to producing videos that are up to one minute in length. This limitation stems from the complexity of long video generation, which requires more than generative AI techniques for approximating density functions essential aspects such as planning, story development, and maintaining spatial and temporal consistency present additional hurdles. Integrating generative AI with a divide-and-conquer approach could improve scalability for longer videos while offering greater control. In this survey, we examine the current landscape of long video generation, covering foundational techniques like GANs and diffusion models, video generation strategies, large-scale training datasets, quality metrics for evaluating long videos, and future research areas to address the limitations of the existing video generation capabilities. We believe it would serve as a comprehensive foundation, offering extensive information to guide future advancements and research in the field of long video generation.</li>
</ul>

<h3>Title: Elucidating Flow Matching ODE Dynamics with respect to Data Geometries</h3>
<ul>
<li><strong>Authors: </strong>Gal Mishne, Zhengchao Wan, Qingsong Wang, Yusu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18730">https://arxiv.org/abs/2412.18730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18730">https://arxiv.org/pdf/2412.18730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18730]] Elucidating Flow Matching ODE Dynamics with respect to Data Geometries(https://arxiv.org/abs/2412.18730)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative models have become the standard for image generation. ODE-based samplers and flow matching models improve efficiency, in comparison to diffusion models, by reducing sampling steps through learned vector fields. However, the theoretical foundations of flow matching models remain limited, particularly regarding the convergence of individual sample trajectories at terminal time - a critical property that impacts sample quality and being critical assumption for models like the consistency model. In this paper, we advance the theory of flow matching models through a comprehensive analysis of sample trajectories, centered on the denoiser that drives ODE dynamics. We establish the existence, uniqueness and convergence of ODE trajectories at terminal time, ensuring stable sampling outcomes under minimal assumptions. Our analysis reveals how trajectories evolve from capturing global data features to local structures, providing the geometric characterization of per-sample behavior in flow matching models. We also explain the memorization phenomenon in diffusion-based training through our terminal time analysis. These findings bridge critical gaps in understanding flow matching models, with practical implications for sampling stability and model design.</li>
</ul>

<h3>Title: Protective Perturbations against Unauthorized Data Usage in Diffusion-based Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Sen Peng, Jijia Yang, Mingyue Wang, Jianfei He, Xiaohua Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18791">https://arxiv.org/abs/2412.18791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18791">https://arxiv.org/pdf/2412.18791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18791]] Protective Perturbations against Unauthorized Data Usage in Diffusion-based Image Generation(https://arxiv.org/abs/2412.18791)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image models have shown immense potential for various image-related tasks. However, despite their prominence and popularity, customizing these models using unauthorized data also brings serious privacy and intellectual property issues. Existing methods introduce protective perturbations based on adversarial attacks, which are applied to the customization samples. In this systematization of knowledge, we present a comprehensive survey of protective perturbation methods designed to prevent unauthorized data usage in diffusion-based image generation. We establish the threat model and categorize the downstream tasks relevant to these methods, providing a detailed analysis of their designs. We also propose a completed evaluation framework for these perturbation techniques, aiming to advance research in this field.</li>
</ul>

<h3>Title: DRDM: A Disentangled Representations Diffusion Model for Synthesizing Realistic Person Images</h3>
<ul>
<li><strong>Authors: </strong>Enbo Huang, Yuan Zhang, Faliang Huang, Guangyu Zhang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18797">https://arxiv.org/abs/2412.18797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18797">https://arxiv.org/pdf/2412.18797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18797]] DRDM: A Disentangled Representations Diffusion Model for Synthesizing Realistic Person Images(https://arxiv.org/abs/2412.18797)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Person image synthesis with controllable body poses and appearances is an essential task owing to the practical needs in the context of virtual try-on, image editing and video production. However, existing methods face significant challenges with details missing, limbs distortion and the garment style deviation. To address these issues, we propose a Disentangled Representations Diffusion Model (DRDM) to generate photo-realistic images from source portraits in specific desired poses and appearances. First, a pose encoder is responsible for encoding pose features into a high-dimensional space to guide the generation of person images. Second, a body-part subspace decoupling block (BSDB) disentangles features from the different body parts of a source figure and feeds them to the various layers of the noise prediction block, thereby supplying the network with rich disentangled features for generating a realistic target image. Moreover, during inference, we develop a parsing map-based disentangled classifier-free guided sampling method, which amplifies the conditional signals of texture and pose. Extensive experimental results on the Deepfashion dataset demonstrate the effectiveness of our approach in achieving pose transfer and appearance control.</li>
</ul>

<h3>Title: DebiasDiff: Debiasing Text-to-image Diffusion Models with Self-discovering Latent Attribute Directions</h3>
<ul>
<li><strong>Authors: </strong>Yilei Jiang, Weihong Li, Yiyuan Zhang, Minghong Cai, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18810">https://arxiv.org/abs/2412.18810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18810">https://arxiv.org/pdf/2412.18810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18810]] DebiasDiff: Debiasing Text-to-image Diffusion Models with Self-discovering Latent Attribute Directions(https://arxiv.org/abs/2412.18810)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>While Diffusion Models (DM) exhibit remarkable performance across various image generative tasks, they nonetheless reflect the inherent bias presented in the training set. As DMs are now widely used in real-world applications, these biases could perpetuate a distorted worldview and hinder opportunities for minority groups. Existing methods on debiasing DMs usually requires model re-training with a human-crafted reference dataset or additional classifiers, which suffer from two major limitations: (1) collecting reference datasets causes expensive annotation cost; (2) the debiasing performance is heavily constrained by the quality of the reference dataset or the additional classifier. To address the above limitations, we propose DebiasDiff, a plug-and-play method that learns attribute latent directions in a self-discovering manner, thus eliminating the reliance on such reference dataset. Specifically, DebiasDiff consists of two parts: a set of attribute adapters and a distribution indicator. Each adapter in the set aims to learn an attribute latent direction, and is optimized via noise composition through a self-discovering process. Then, the distribution indicator is multiplied by the set of adapters to guide the generation process towards the prescribed distribution. Our method enables debiasing multiple attributes in DMs simultaneously, while remaining lightweight and easily integrable with other DMs, eliminating the need for re-training. Extensive experiments on debiasing gender, racial, and their intersectional biases show that our method outperforms previous SOTA by a large margin.</li>
</ul>

<h3>Title: CausalTAD: Causal Implicit Generative Model for Debiased Online Trajectory Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenbin Li, Di Yao, Chang Gong, Xiaokai Chu, Quanliang Jing, Xiaolei Zhou, Yuxuan Zhang, Yunxia Fan, Jingping Bi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18820">https://arxiv.org/abs/2412.18820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18820">https://arxiv.org/pdf/2412.18820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18820]] CausalTAD: Causal Implicit Generative Model for Debiased Online Trajectory Anomaly Detection(https://arxiv.org/abs/2412.18820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Trajectory anomaly detection, aiming to estimate the anomaly risk of trajectories given the Source-Destination (SD) pairs, has become a critical problem for many real-world applications. Existing solutions directly train a generative model for observed trajectories and calculate the conditional generative probability $P({T}|{C})$ as the anomaly risk, where ${T}$ and ${C}$ represent the trajectory and SD pair respectively. However, we argue that the observed trajectories are confounded by road network preference which is a common cause of both SD distribution and trajectories. Existing methods ignore this issue limiting their generalization ability on out-of-distribution trajectories. In this paper, we define the debiased trajectory anomaly detection problem and propose a causal implicit generative model, namely CausalTAD, to solve it. CausalTAD adopts do-calculus to eliminate the confounding bias of road network preference and estimates $P({T}|do({C}))$ as the anomaly criterion. Extensive experiments show that CausalTAD can not only achieve superior performance on trained trajectories but also generally improve the performance of out-of-distribution data, with improvements of $2.1\% \sim 5.7\%$ and $10.6\% \sim 32.7\%$ respectively.</li>
</ul>

<h3>Title: DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering</h3>
<ul>
<li><strong>Authors: </strong>Ruohong Yang, Peng Hu, Xi Peng, Xiting Liu, Yunfan Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18838">https://arxiv.org/abs/2412.18838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18838">https://arxiv.org/pdf/2412.18838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18838]] DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering(https://arxiv.org/abs/2412.18838)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Fine-grained clustering is a practical yet challenging task, whose essence lies in capturing the subtle differences between instances of different classes. Such subtle differences can be easily disrupted by data augmentation or be overwhelmed by redundant information in data, leading to significant performance degradation for existing clustering methods. In this work, we introduce DiFiC a fine-grained clustering method building upon the conditional diffusion model. Distinct from existing works that focus on extracting discriminative features from images, DiFiC resorts to deducing the textual conditions used for image generation. To distill more precise and clustering-favorable object semantics, DiFiC further regularizes the diffusion target and guides the distillation process utilizing neighborhood similarity. Extensive experiments demonstrate that DiFiC outperforms both state-of-the-art discriminative and generative clustering methods on four fine-grained image clustering benchmarks. We hope the success of DiFiC will inspire future research to unlock the potential of diffusion models in tasks beyond generation. The code will be released.</li>
</ul>

<h3>Title: SWAG: Long-term Surgical Workflow Prediction with Generative-based Anticipation</h3>
<ul>
<li><strong>Authors: </strong>Maxence Boels, Yang Liu, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18849">https://arxiv.org/abs/2412.18849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18849">https://arxiv.org/pdf/2412.18849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18849]] SWAG: Long-term Surgical Workflow Prediction with Generative-based Anticipation(https://arxiv.org/abs/2412.18849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While existing recognition approaches excel at identifying current surgical phases, they provide limited foresight into future procedural steps, restricting their intraoperative utility. Similarly, current anticipation methods are constrained to predicting short-term events or singular future occurrences, neglecting the dynamic and sequential nature of surgical workflows. To address these limitations, we propose SWAG (Surgical Workflow Anticipative Generation), a unified framework for phase recognition and long-term anticipation of surgical workflows. SWAG employs two generative decoding methods -- single-pass (SP) and auto-regressive (AR) -- to predict sequences of future surgical phases. A novel prior knowledge embedding mechanism enhances the accuracy of anticipatory predictions. The framework addresses future phase classification and remaining time regression tasks. Additionally, a regression-to-classification (R2C) method is introduced to map continuous predictions to discrete temporal segments. SWAG's performance was evaluated on the Cholec80 and AutoLaparo21 datasets. The single-pass classification model with prior knowledge embeddings (SWAG-SP\*) achieved 53.5\% accuracy in 15-minute anticipation on AutoLaparo21, while the R2C model reached 60.8\% accuracy on Cholec80. SWAG's single-pass regression approach outperformed existing methods for remaining time prediction, achieving weighted mean absolute errors of 0.32 and 0.48 minutes for 2- and 3-minute horizons, respectively. SWAG demonstrates versatility across classification and regression tasks, offering robust tools for real-time surgical workflow anticipation. By unifying recognition and anticipatory capabilities, SWAG provides actionable predictions to enhance intraoperative decision-making.</li>
</ul>

<h3>Title: Accelerating Diffusion Transformers with Dual Feature Caching</h3>
<ul>
<li><strong>Authors: </strong>Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18911">https://arxiv.org/abs/2412.18911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18911">https://arxiv.org/pdf/2412.18911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18911]] Accelerating Diffusion Transformers with Dual Feature Caching(https://arxiv.org/abs/2412.18911)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. However, on the one hand, aggressively reusing all the features cached in previous timesteps leads to a severe drop in generation quality. On the other hand, conservatively caching only the features in the redundant layers or tokens but still computing the important ones successfully preserves the generation quality but results in reductions in acceleration ratios. Observing such a tradeoff between generation quality and acceleration performance, this paper begins by quantitatively studying the accumulated error from cached features. Surprisingly, we find that aggressive caching does not introduce significantly more caching errors in the caching step, and the conservative feature caching can fix the error introduced by aggressive caching. Thereby, we propose a dual caching strategy that adopts aggressive and conservative caching iteratively, leading to significant acceleration and high generation quality at the same time. Besides, we further introduce a V-caching strategy for token-wise conservative caching, which is compatible with flash attention and requires no training and calibration data. Our codes have been released in Github: \textbf{Code: \href{this https URL}{\texttt{\textcolor{cyan}{this https URL}}}}</li>
</ul>

<h3>Title: Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of Vision-Language Multiway Transformer Model</h3>
<ul>
<li><strong>Authors: </strong>Yi-Chia Chen, Wei-Hua Li, Chu-Song Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18917">https://arxiv.org/abs/2412.18917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18917">https://arxiv.org/pdf/2412.18917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18917]] Open-Vocabulary Panoptic Segmentation Using BERT Pre-Training of Vision-Language Multiway Transformer Model(https://arxiv.org/abs/2412.18917)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Open-vocabulary panoptic segmentation remains a challenging problem. One of the biggest difficulties lies in training models to generalize to an unlimited number of classes using limited categorized training data. Recent popular methods involve large-scale vision-language pre-trained foundation models, such as CLIP. In this paper, we propose OMTSeg for open-vocabulary segmentation using another large-scale vision-language pre-trained model called BEiT-3 and leveraging the cross-modal attention between visual and linguistic features in BEiT-3 to achieve better performance. Experiments result demonstrates that OMTSeg performs favorably against state-of-the-art models.</li>
</ul>

<h3>Title: Generative Face Parsing Map Guided 3D Face Reconstruction Under Occluded Scenes</h3>
<ul>
<li><strong>Authors: </strong>Dapeng Zhao, Yue Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18920">https://arxiv.org/abs/2412.18920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18920">https://arxiv.org/pdf/2412.18920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18920]] Generative Face Parsing Map Guided 3D Face Reconstruction Under Occluded Scenes(https://arxiv.org/abs/2412.18920)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Over the past few years, single-view 3D face reconstruction methods can produce beautiful 3D models. Nevertheless,the input of these works is unobstructed this http URL describe a system designed to reconstruct convincing face texture in the case of this http URL by parsing facial features,we propose a complete face parsing map generation method guided by this http URL estimate the 2D face structure of the reasonable position of the occlusion area,which is used for the construction of 3D this http URL excellent anti-occlusion face reconstruction method should ensure the authenticity of the output,including the topological structure between the eyes,nose, and mouth. We extensively tested our method and its components, qualitatively demonstrating the rationality of our estimated facial structure. We conduct extensive experiments on general 3D face reconstruction tasks as concrete examples to demonstrate the method's superior regulation ability over existing methods often break this http URL further provide numerous quantitative examples showing that our method advances both the quality and the robustness of 3D face reconstruction under occlusion scenes.</li>
</ul>

<h3>Title: Exemplar-condensed Federated Class-incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Rui Sun, Yumin Zhang, Varun Ojha, Tejal Shah, Haoran Duan, Bo Wei, Rajiv Ranjan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18926">https://arxiv.org/abs/2412.18926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18926">https://arxiv.org/pdf/2412.18926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18926]] Exemplar-condensed Federated Class-incremental Learning(https://arxiv.org/abs/2412.18926)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose Exemplar-Condensed federated class-incremental learning (ECoral) to distil the training characteristics of real images from streaming data into informative rehearsal exemplars. The proposed method eliminates the limitations of exemplar selection in replay-based approaches for mitigating catastrophic forgetting in federated continual learning (FCL). The limitations particularly related to the heterogeneity of information density of each summarized data. Our approach maintains the consistency of training gradients and the relationship to past tasks for the summarized exemplars to represent the streaming data compared to the original images effectively. Additionally, our approach reduces the information-level heterogeneity of the summarized data by inter-client sharing of the disentanglement generative model. Extensive experiments show that our ECoral outperforms several state-of-the-art methods and can be seamlessly integrated with many existing approaches to enhance performance.</li>
</ul>

<h3>Title: UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal Transformer for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Lunhao Duan, Shanshan Zhao, Wenjun Yan, Yinglun Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Mingming Gong, Gui-Song Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18928">https://arxiv.org/abs/2412.18928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18928">https://arxiv.org/pdf/2412.18928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18928]] UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal Transformer for Image Generation(https://arxiv.org/abs/2412.18928)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, text-to-image generation models have achieved remarkable advancements, particularly with diffusion models facilitating high-quality image synthesis from textual descriptions. However, these models often struggle with achieving precise control over pixel-level layouts, object appearances, and global styles when using text prompts alone. To mitigate this issue, previous works introduce conditional images as auxiliary inputs for image generation, enhancing control but typically necessitating specialized models tailored to different types of reference inputs. In this paper, we explore a new approach to unify controllable generation within a single framework. Specifically, we propose the unified image-instruction adapter (UNIC-Adapter) built on the Multi-Modal-Diffusion Transformer architecture, to enable flexible and controllable generation across diverse conditions without the need for multiple specialized models. Our UNIC-Adapter effectively extracts multi-modal instruction information by incorporating both conditional images and task instructions, injecting this information into the image generation process through a cross-attention mechanism enhanced by Rotary Position Embedding. Experimental results across a variety of tasks, including pixel-level spatial control, subject-driven image generation, and style-image-based image synthesis, demonstrate the effectiveness of our UNIC-Adapter in unified controllable image generation.</li>
</ul>

<h3>Title: Single Trajectory Distillation for Accelerating Image and Video Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Sijie Xu, Runqi Wang, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, Yao Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.18945">https://arxiv.org/abs/2412.18945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.18945">https://arxiv.org/pdf/2412.18945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.18945]] Single Trajectory Distillation for Accelerating Image and Video Style Transfer(https://arxiv.org/abs/2412.18945)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based stylization methods typically denoise from a specific partial noise state for image-to-image and video-to-video tasks. This multi-step diffusion process is computationally expensive and hinders real-world application. A promising solution to speed up the process is to obtain few-step consistency models through trajectory distillation. However, current consistency models only force the initial-step alignment between the probability flow ODE (PF-ODE) trajectories of the student and the imperfect teacher models. This training strategy can not ensure the consistency of whole trajectories. To address this issue, we propose single trajectory distillation (STD) starting from a specific partial noise state. We introduce a trajectory bank to store the teacher model's trajectory states, mitigating the time cost during training. Besides, we use an asymmetric adversarial loss to enhance the style and quality of the generated images. Extensive experiments on image and video stylization demonstrate that our method surpasses existing acceleration models in terms of style similarity and aesthetic evaluations. Our code and results will be available on the project page: this https URL.</li>
</ul>

<h3>Title: MGAN-CRCM: A Novel Multiple Generative Adversarial Network and Coarse-Refinement Based Cognizant Method for Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Nafiz Al Asad, Md. Appel Mahmud Pranto, Shbiruzzaman Shiam, Musaddeq Mahmud Akand, Mohammad Abu Yousuf, Khondokar Fida Hasan, Mohammad Ali Moni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19000">https://arxiv.org/abs/2412.19000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19000">https://arxiv.org/pdf/2412.19000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19000]] MGAN-CRCM: A Novel Multiple Generative Adversarial Network and Coarse-Refinement Based Cognizant Method for Image Inpainting(https://arxiv.org/abs/2412.19000)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image inpainting is a widely used technique in computer vision for reconstructing missing or damaged pixels in images. Recent advancements with Generative Adversarial Networks (GANs) have demonstrated superior performance over traditional methods due to their deep learning capabilities and adaptability across diverse image domains. Residual Networks (ResNet) have also gained prominence for their ability to enhance feature representation and compatibility with other architectures. This paper introduces a novel architecture combining GAN and ResNet models to improve image inpainting outcomes. Our framework integrates three components: Transpose Convolution-based GAN for guided and blind inpainting, Fast ResNet-Convolutional Neural Network (FR-CNN) for object removal, and Co-Modulation GAN (Co-Mod GAN) for refinement. The model's performance was evaluated on benchmark datasets, achieving accuracies of 96.59% on Image-Net, 96.70% on Places2, and 96.16% on CelebA. Comparative analyses demonstrate that the proposed architecture outperforms existing methods, highlighting its effectiveness in both qualitative and quantitative evaluations.</li>
</ul>

<h3>Title: FACEMUG: A Multimodal Generative and Fusion Framework for Local Facial Editing</h3>
<ul>
<li><strong>Authors: </strong>Wanglong Lu, Jikai Wang, Xiaogang Jin, Xianta Jiang, Hanli Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19009">https://arxiv.org/abs/2412.19009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19009">https://arxiv.org/pdf/2412.19009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19009]] FACEMUG: A Multimodal Generative and Fusion Framework for Local Facial Editing(https://arxiv.org/abs/2412.19009)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Existing facial editing methods have achieved remarkable results, yet they often fall short in supporting multimodal conditional local facial editing. One of the significant evidences is that their output image quality degrades dramatically after several iterations of incremental editing, as they do not support local editing. In this paper, we present a novel multimodal generative and fusion framework for globally-consistent local facial editing (FACEMUG) that can handle a wide range of input modalities and enable fine-grained and semantic manipulation while remaining unedited parts unchanged. Different modalities, including sketches, semantic maps, color maps, exemplar images, text, and attribute labels, are adept at conveying diverse conditioning details, and their combined synergy can provide more explicit guidance for the editing process. We thus integrate all modalities into a unified generative latent space to enable multimodal local facial edits. Specifically, a novel multimodal feature fusion mechanism is proposed by utilizing multimodal aggregation and style fusion blocks to fuse facial priors and multimodalities in both latent and feature spaces. We further introduce a novel self-supervised latent warping algorithm to rectify misaligned facial features, efficiently transferring the pose of the edited image to the given latent codes. We evaluate our FACEMUG through extensive experiments and comparisons to state-of-the-art (SOTA) methods. The results demonstrate the superiority of FACEMUG in terms of editing quality, flexibility, and semantic control, making it a promising solution for a wide range of local facial editing tasks.</li>
</ul>

<h3>Title: Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability</h3>
<ul>
<li><strong>Authors: </strong>Ruixi Lin, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19018">https://arxiv.org/abs/2412.19018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19018">https://arxiv.org/pdf/2412.19018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19018]] Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability(https://arxiv.org/abs/2412.19018)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning, which allows large language models to perform diverse tasks with a few demonstrations, is found to have imbalanced per-class prediction accuracy on multi-class text classification. Although notable output correction methods have been developed to tackle the issue and simultaneously improve downstream prediction accuracy, they may fail to answer the core interpretability challenges: why and which certain classes need corrections, and more importantly, a tailored correction for per-sample, per-class's probability. To address such interpretability gaps, we first find that the imbalance arises from certain classes consistently receiving high ICL output probabilities, whereas others receiving lower or mixed ranges, so the former is more frequently chosen, resulting in higher accuracy; more crucially, we find that these ranges have significantly varying degrees of influence on the accuracy bias, highlighting the need for precise, interpretable probability corrections by range. Motivated by this, we propose FuRud, a Fuzzy Rule Optimization based Debiasing method, that (1) detects which classes need corrections, and (2) for each correction-needed class, detects its probability ranges and applies asymmetric amplifications or reductions to correct them interpretably. Notably, across seven benchmark datasets, FuRud reduces the pairwise class accuracy bias (COBias) by more than half (56%), while achieving a relative increase of 21% in accuracy, outperforming state-of-the-art debiasing methods. Moreover, FuRud can optimize downstream tasks with as few as 10 optimization examples. Furthermore, FuRud can work for prompt formats that lead to highly skewed predictions. For example, FuRud greatly improves ICL outputs which use letter options, with 44% relative accuracy increase and 54% relative COBias reduction.</li>
</ul>

<h3>Title: Mask Factory: Towards High-quality Synthetic Data Generation for Dichotomous Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Haotian Qian, YD Chen, Shengtao Lou, Fahad Shahbaz Khan, Xiaogang Jin, Deng-Ping Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19080">https://arxiv.org/abs/2412.19080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19080">https://arxiv.org/pdf/2412.19080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19080]] Mask Factory: Towards High-quality Synthetic Data Generation for Dichotomous Image Segmentation(https://arxiv.org/abs/2412.19080)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Dichotomous Image Segmentation (DIS) tasks require highly precise annotations, and traditional dataset creation methods are labor intensive, costly, and require extensive domain expertise. Although using synthetic data for DIS is a promising solution to these challenges, current generative models and techniques struggle with the issues of scene deviations, noise-induced errors, and limited training sample variability. To address these issues, we introduce a novel approach, \textbf{\ourmodel{}}, which provides a scalable solution for generating diverse and precise datasets, markedly reducing preparation time and costs. We first introduce a general mask editing method that combines rigid and non-rigid editing techniques to generate high-quality synthetic masks. Specially, rigid editing leverages geometric priors from diffusion models to achieve precise viewpoint transformations under zero-shot conditions, while non-rigid editing employs adversarial training and self-attention mechanisms for complex, topologically consistent modifications. Then, we generate pairs of high-resolution image and accurate segmentation mask using a multi-conditional control generation method. Finally, our experiments on the widely-used DIS5K dataset benchmark demonstrate superior performance in quality and efficiency compared to existing methods. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Integrating Artificial Open Generative Artificial Intelligence into Software Supply Chain Security</h3>
<ul>
<li><strong>Authors: </strong>Vasileios Alevizos, George A Papakostas, Akebu Simasiku, Dimitra Malliarou, Antonis Messinis, Sabrina Edralin, Clark Xu, Zongliang Yue</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19088">https://arxiv.org/abs/2412.19088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19088">https://arxiv.org/pdf/2412.19088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19088]] Integrating Artificial Open Generative Artificial Intelligence into Software Supply Chain Security(https://arxiv.org/abs/2412.19088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While new technologies emerge, human errors always looming. Software supply chain is increasingly complex and intertwined, the security of a service has become paramount to ensuring the integrity of products, safeguarding data privacy, and maintaining operational continuity. In this work, we conducted experiments on the promising open Large Language Models (LLMs) into two main software security challenges: source code language errors and deprecated code, with a focus on their potential to replace conventional static and dynamic security scanners that rely on predefined rules and patterns. Our findings suggest that while LLMs present some unexpected results, they also encounter significant limitations, particularly in memory complexity and the management of new and unfamiliar data patterns. Despite these challenges, the proactive application of LLMs, coupled with extensive security databases and continuous updates, holds the potential to fortify Software Supply Chain (SSC) processes against emerging threats.</li>
</ul>

<h3>Title: Improving Generative Pre-Training: An In-depth Study of Masked Image Modeling and Denoising Models</h3>
<ul>
<li><strong>Authors: </strong>Hyesong Choi, Daeun Kim, Sungmin Cha, Kwang Moo Yi, Dongbo Min</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19104">https://arxiv.org/abs/2412.19104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19104">https://arxiv.org/pdf/2412.19104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19104]] Improving Generative Pre-Training: An In-depth Study of Masked Image Modeling and Denoising Models(https://arxiv.org/abs/2412.19104)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this work, we dive deep into the impact of additive noise in pre-training deep networks. While various methods have attempted to use additive noise inspired by the success of latent denoising diffusion models, when used in combination with masked image modeling, their gains have been marginal when it comes to recognition tasks. We thus investigate why this would be the case, in an attempt to find effective ways to combine the two ideas. Specifically, we find three critical conditions: corruption and restoration must be applied within the encoder, noise must be introduced in the feature space, and an explicit disentanglement between noised and masked tokens is necessary. By implementing these findings, we demonstrate improved pre-training performance for a wide range of recognition tasks, including those that require fine-grained, high-frequency information to solve.</li>
</ul>

<h3>Title: Graph Mixture of Experts and Memory-augmented Routers for Multivariate Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Huang (1 and 2), Weidong Chen (1), Bo Hu (1), Zhendong Mao (1)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19108">https://arxiv.org/abs/2412.19108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19108">https://arxiv.org/pdf/2412.19108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19108]] Graph Mixture of Experts and Memory-augmented Routers for Multivariate Time Series Anomaly Detection(https://arxiv.org/abs/2412.19108)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Multivariate time series (MTS) anomaly detection is a critical task that involves identifying abnormal patterns or events in data that consist of multiple interrelated time series. In order to better model the complex interdependence between entities and the various inherent characteristics of each entity, the GNN based methods are widely adopted by existing methods. In each layer of GNN, node features aggregate information from their neighboring nodes to update their information. In doing so, from shallow layer to deep layer in GNN, original individual node features continue to be weakened and more structural information,i.e., from short-distance neighborhood to long-distance neighborhood, continues to be enhanced. However, research to date has largely ignored the understanding of how hierarchical graph information is represented and their characteristics that can benefit anomaly detection. Existing methods simply leverage the output from the last layer of GNN for anomaly estimation while neglecting the essential information contained in the intermediate GNN layers. To address such limitations, in this paper, we propose a Graph Mixture of Experts (Graph-MoE) network for multivariate time series anomaly detection, which incorporates the mixture of experts (MoE) module to adaptively represent and integrate hierarchical multi-layer graph information into entity representations. It is worth noting that our Graph-MoE can be integrated into any GNN-based MTS anomaly detection method in a plug-and-play manner. In addition, the memory-augmented routers are proposed in this paper to capture the correlation temporal information in terms of the global historical features of MTS to adaptively weigh the obtained entity representations to achieve successful anomaly estimation. Extensive experiments on five challenging datasets prove the superiority of our approach and each proposed module.</li>
</ul>

<h3>Title: SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing Values</h3>
<ul>
<li><strong>Authors: </strong>Yunfan Zhang, Changlun Li, Yuyu Luo, Nan Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DB, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19113">https://arxiv.org/abs/2412.19113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19113">https://arxiv.org/pdf/2412.19113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19113]] SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing Values(https://arxiv.org/abs/2412.19113)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Missing value is a critical issue in data science, significantly impacting the reliability of analyses and predictions. Missing value imputation (MVI) is a longstanding problem because it highly relies on domain knowledge. Large language models (LLMs) have emerged as a promising tool for data cleaning, including MVI for tabular data, offering advanced capabilities for understanding and generating content. However, despite their promise, existing LLM techniques such as in-context learning and Chain-of-Thought (CoT) often fall short in guiding LLMs to perform complex reasoning for MVI, particularly when imputing derived missing values, which require mathematical formulas and data relationships across rows and columns. This gap underscores the need for further advancements in LLM methodologies to enhance their reasoning capabilities for more reliable imputation outcomes. To fill this gap, we propose SketchFill, a novel sketch-based method to guide LLMs in generating accurate formulas to impute missing numerical values. Our experimental results demonstrate that SketchFill significantly outperforms state-of-the-art methods, achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher accuracy than MetaGPT. This sets a new standard for automated data cleaning and advances the field of MVI for numerical values.</li>
</ul>

<h3>Title: Discrete vs. Continuous Trade-offs for Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jathin Korrapati, Tanish Baranwal, Rahul Shah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19114">https://arxiv.org/abs/2412.19114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19114">https://arxiv.org/pdf/2412.19114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19114]] Discrete vs. Continuous Trade-offs for Generative Models(https://arxiv.org/abs/2412.19114)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This work explores the theoretical and practical foundations of denoising diffusion probabilistic models (DDPMs) and score-based generative models, which leverage stochastic processes and Brownian motion to model complex data distributions. These models employ forward and reverse diffusion processes defined through stochastic differential equations (SDEs) to iteratively add and remove noise, enabling high-quality data generation. By analyzing the performance bounds of these models, we demonstrate how score estimation errors propagate through the reverse process and bound the total variation distance using discrete Girsanov transformations, Pinsker's inequality, and the data processing inequality (DPI) for an information theoretic lens.</li>
</ul>

<h3>Title: Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for Robustness, Generalizability, and Multi-Domain Impact</h3>
<ul>
<li><strong>Authors: </strong>Valay Bundele, Ouz Ata al, Bora Kargi, Karahan Sarta, Kvan Tezren, Zohreh Ghaderi, Hendrik Lensch</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19124">https://arxiv.org/abs/2412.19124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19124">https://arxiv.org/pdf/2412.19124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19124]] Evaluating Self-Supervised Learning in Medical Imaging: A Benchmark for Robustness, Generalizability, and Multi-Domain Impact(https://arxiv.org/abs/2412.19124)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has emerged as a promising paradigm in medical imaging, addressing the chronic challenge of limited labeled data in healthcare settings. While SSL has shown impressive results, existing studies in the medical domain are often limited in scope, focusing on specific datasets or modalities, or evaluating only isolated aspects of model performance. This fragmented evaluation approach poses a significant challenge, as models deployed in critical medical settings must not only achieve high accuracy but also demonstrate robust performance and generalizability across diverse datasets and varying conditions. To address this gap, we present a comprehensive evaluation of SSL methods within the medical domain, with a particular focus on robustness and generalizability. Using the MedMNIST dataset collection as a standardized benchmark, we evaluate 8 major SSL methods across 11 different medical datasets. Our study provides an in-depth analysis of model performance in both in-domain scenarios and the detection of out-of-distribution (OOD) samples, while exploring the effect of various initialization strategies, model architectures, and multi-domain pre-training. We further assess the generalizability of SSL methods through cross-dataset evaluations and the in-domain performance with varying label proportions (1%, 10%, and 100%) to simulate real-world scenarios with limited supervision. We hope this comprehensive benchmark helps practitioners and researchers make more informed decisions when applying SSL methods to medical applications.</li>
</ul>

<h3>Title: Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot Quantization in Edge Computing</h3>
<ul>
<li><strong>Authors: </strong>Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19125">https://arxiv.org/abs/2412.19125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19125">https://arxiv.org/pdf/2412.19125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19125]] Advanced Knowledge Transfer: Refined Feature Distillation for Zero-Shot Quantization in Edge Computing(https://arxiv.org/abs/2412.19125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce AKT (Advanced Knowledge Transfer), a novel method to enhance the training ability of low-bit quantized (Q) models in the field of zero-shot quantization (ZSQ). Existing research in ZSQ has focused on generating high-quality data from full-precision (FP) models. However, these approaches struggle with reduced learning ability in low-bit quantization due to its limited information capacity. To overcome this limitation, we propose effective training strategy compared to data generation. Particularly, we analyzed that refining feature maps in the feature distillation process is an effective way to transfer knowledge to the Q model. Based on this analysis, AKT efficiently transfer core information from the FP model to the Q model. AKT is the first approach to utilize both spatial and channel attention information in feature distillation in ZSQ. Our method addresses the fundamental gradient exploding problem in low-bit Q models. Experiments on CIFAR-10 and CIFAR-100 datasets demonstrated the effectiveness of the AKT. Our method led to significant performance enhancement in existing generative models. Notably, AKT achieved significant accuracy improvements in low-bit Q models, achieving state-of-the-art in the 3,5bit scenarios on CIFAR-10. The code is available at this https URL.</li>
</ul>

<h3>Title: SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19140">https://arxiv.org/abs/2412.19140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19140">https://arxiv.org/pdf/2412.19140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19140]] SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis(https://arxiv.org/abs/2412.19140)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In recent years, fine-grained sentiment analysis in finance has gained significant attention, but the scarcity of entity-level datasets remains a key challenge. To address this, we have constructed the largest English and Chinese financial entity-level sentiment analysis datasets to date. Building on this foundation, we propose a novel two-stage sentiment analysis approach called Self-aware In-context Learning Correction (SILC). The first stage involves fine-tuning a base large language model to generate pseudo-labeled data specific to our task. In the second stage, we train a correction model using a GNN-based example retriever, which is informed by the pseudo-labeled data. This two-stage strategy has allowed us to achieve state-of-the-art performance on the newly constructed datasets, advancing the field of financial sentiment analysis. In a case study, we demonstrate the enhanced practical utility of our data and methods in monitoring the cryptocurrency market. Our datasets and code are available at this https URL.</li>
</ul>

<h3>Title: Generating Editable Head Avatars with 3D Gaussian GANs</h3>
<ul>
<li><strong>Authors: </strong>Guohao Li, Hongyu Yang, Yifang Men, Di Huang, Weixin Li, Ruijie Yang, Yunhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19149">https://arxiv.org/abs/2412.19149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19149">https://arxiv.org/pdf/2412.19149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19149]] Generating Editable Head Avatars with 3D Gaussian GANs(https://arxiv.org/abs/2412.19149)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating animatable and editable 3D head avatars is essential for various applications in computer vision and graphics. Traditional 3D-aware generative adversarial networks (GANs), often using implicit fields like Neural Radiance Fields (NeRF), achieve photorealistic and view-consistent 3D head synthesis. However, these methods face limitations in deformation flexibility and editability, hindering the creation of lifelike and easily modifiable 3D heads. We propose a novel approach that enhances the editability and animation control of 3D head avatars by incorporating 3D Gaussian Splatting (3DGS) as an explicit 3D representation. This method enables easier illumination control and improved editability. Central to our approach is the Editable Gaussian Head (EG-Head) model, which combines a 3D Morphable Model (3DMM) with texture maps, allowing precise expression control and flexible texture editing for accurate animation while preserving identity. To capture complex non-facial geometries like hair, we use an auxiliary set of 3DGS and tri-plane features. Extensive experiments demonstrate that our approach delivers high-quality 3D-aware synthesis with state-of-the-art controllability. Our code and models are available at this https URL.</li>
</ul>

<h3>Title: Mask Approximation Net: Merging Feature Extraction and Distribution Learning for Remote Sensing Change Captioning</h3>
<ul>
<li><strong>Authors: </strong>Dongwei Sun, Xiangyong Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19179">https://arxiv.org/abs/2412.19179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19179">https://arxiv.org/pdf/2412.19179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19179]] Mask Approximation Net: Merging Feature Extraction and Distribution Learning for Remote Sensing Change Captioning(https://arxiv.org/abs/2412.19179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Remote sensing image change description, as a novel multimodal task in the field of remote sensing processing, not only enables the detection of changes in surface conditions but also provides detailed descriptions of these changes, thereby enhancing human interpretability and interactivity. However, previous methods mainly employed Convolutional Neural Network (CNN) architectures to extract bitemporal image features. This approach often leads to an overemphasis on designing specific network architectures and limits the captured feature distributions to the current dataset, resulting in poor generalizability and robustness when applied to other datasets or real-world scenarios. To address these limitations, this paper proposes a novel approach for remote sensing image change detection and description that integrates diffusion models, aiming to shift the focus from conventional feature learning paradigms to data distribution learning. The proposed method primarily includes a simple multi-scale change detection module, whose output features are subsequently refined using a diffusion model. Additionally, we introduce a frequency-guided complex filter module to handle high-frequency noise during the diffusion process, which helps to maintain model performance. Finally, we validate the effectiveness of our proposed method on several remote sensing change detection description datasets, demonstrating its superior performance. The code available at MaskApproxNet.</li>
</ul>

<h3>Title: Towards Better Spherical Sliced-Wasserstein Distance Learning with Data-Adaptive Discriminative Projection Direction</h3>
<ul>
<li><strong>Authors: </strong>Hongliang Zhang, Shuo Chen, Lei Luo, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19212">https://arxiv.org/abs/2412.19212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19212">https://arxiv.org/pdf/2412.19212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19212]] Towards Better Spherical Sliced-Wasserstein Distance Learning with Data-Adaptive Discriminative Projection Direction(https://arxiv.org/abs/2412.19212)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Spherical Sliced-Wasserstein (SSW) has recently been proposed to measure the discrepancy between spherical data distributions in various fields, such as geology, medical domains, computer vision, and deep representation learning. However, in the original SSW, all projection directions are treated equally, which is too idealistic and cannot accurately reflect the importance of different projection directions for various data distributions. To address this issue, we propose a novel data-adaptive Discriminative Spherical Sliced-Wasserstein (DSSW) distance, which utilizes a projected energy function to determine the discriminative projection direction for SSW. In our new DSSW, we introduce two types of projected energy functions to generate the weights for projection directions with complete theoretical guarantees. The first type employs a non-parametric deterministic function that transforms the projected Wasserstein distance into its corresponding weight in each projection direction. This improves the performance of the original SSW distance with negligible additional computational overhead. The second type utilizes a neural network-induced function that learns the projection direction weight through a parameterized neural network based on data projections. This further enhances the performance of the original SSW distance with less extra computational overhead. Finally, we evaluate the performance of our proposed DSSW by comparing it with several state-of-the-art methods across a variety of machine learning tasks, including gradient flows, density estimation on real earth data, and self-supervised learning.</li>
</ul>

<h3>Title: Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Yan, Zhengxue Wang, Kun Wang, Jun Li, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19225">https://arxiv.org/abs/2412.19225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19225">https://arxiv.org/pdf/2412.19225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19225]] Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion(https://arxiv.org/abs/2412.19225)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce the Selective Image Guided Network (SigNet), a novel degradation-aware framework that transforms depth completion into depth enhancement for the first time. Moving beyond direct completion using convolutional neural networks (CNNs), SigNet initially densifies sparse depth data through non-CNN densification tools to obtain coarse yet dense depth. This approach eliminates the mismatch and ambiguity caused by direct convolution over irregularly sampled sparse data. Subsequently, SigNet redefines completion as enhancement, establishing a self-supervised degradation bridge between the coarse depth and the targeted dense depth for effective RGB-D fusion. To achieve this, SigNet leverages the implicit degradation to adaptively select high-frequency components (e.g., edges) of RGB data to compensate for the coarse depth. This degradation is further integrated into a multi-modal conditional Mamba, dynamically generating the state parameters to enable efficient global high-frequency information interaction. We conduct extensive experiments on the NYUv2, DIML, SUN RGBD, and TOFDC datasets, demonstrating the state-of-the-art (SOTA) performance of SigNet.</li>
</ul>

<h3>Title: Learning Cross-Domain Representations for Transferable Drug Perturbations on Single-Cell Transcriptional Responses</h3>
<ul>
<li><strong>Authors: </strong>Hui Liu, Shikai Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19228">https://arxiv.org/abs/2412.19228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19228">https://arxiv.org/pdf/2412.19228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19228]] Learning Cross-Domain Representations for Transferable Drug Perturbations on Single-Cell Transcriptional Responses(https://arxiv.org/abs/2412.19228)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Phenotypic drug discovery has attracted widespread attention because of its potential to identify bioactive molecules. Transcriptomic profiling provides a comprehensive reflection of phenotypic changes in cellular responses to external perturbations. In this paper, we propose XTransferCDR, a novel generative framework designed for feature decoupling and transferable representation learning across domains. Given a pair of perturbed expression profiles, our approach decouples the perturbation representations from basal states through domain separation encoders and then cross-transfers them in the latent space. The transferred representations are then used to reconstruct the corresponding perturbed expression profiles via a shared decoder. This cross-transfer constraint effectively promotes the learning of transferable drug perturbation representations. We conducted extensive evaluations of our model on multiple datasets, including single-cell transcriptional responses to drugs and single- and combinatorial genetic perturbations. The experimental results show that XTransferCDR achieved better performance than current state-of-the-art methods, showcasing its potential to advance phenotypic drug discovery.</li>
</ul>

<h3>Title: SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Li, Danfeng Hong, Chenyu Li, Jocelyn Chanussot</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19237">https://arxiv.org/abs/2412.19237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19237">https://arxiv.org/pdf/2412.19237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19237]] SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model(https://arxiv.org/abs/2412.19237)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Remote Sensing (RS) data contains a wealth of multi-dimensional information crucial for Earth observation. Owing to its vast volume, diverse sources, and temporal properties, RS data is highly suitable for the development of large Visual Foundation Models (VFMs). VFMs act as robust feature extractors, learning from extensive RS data, and are subsequently fine-tuned for deployment in various geoscientific tasks. However, current VFMs in the RS domain are predominantly pretrained and tailored exclusively for specific characteristics of RS imagery, neglecting the potential of utilizing the multi-dimensional properties of RS data. Therefore, in this work, we propose SeaMo, a pioneering visual foundation model that integrates multi-seasonal and multimodal information in the RS field. SeaMo is designed to harness multiple properties of RS data. Within the masked image modeling framework, we employ non-aligned cropping techniques to extract spatial properties, use multi-source inputs for multimodal integration, and incorporate temporal-multimodal fusion blocks for effective assimilation of multi-seasonal data. SeaMo explicitly models the multi-dimensional properties of RS data, making the model more comprehensive, robust, and versatile. We applied SeaMo to several downstream geoscience tasks, which demonstrated exceptional performance. Extensive ablation studies were conducted to validate the model's superiority.</li>
</ul>

<h3>Title: PearSAN: A Machine Learning Method for Inverse Design using Pearson Correlated Surrogate Annealing</h3>
<ul>
<li><strong>Authors: </strong>Michael Bezick, Blake A. Wilson, Vaishnavi Iyer, Yuheng Chen, Vladimir M. Shalaev, Sabre Kais, Alexander V. Kildishev, Alexandra Boltasseva, Brad Lackey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19284">https://arxiv.org/abs/2412.19284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19284">https://arxiv.org/pdf/2412.19284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19284]] PearSAN: A Machine Learning Method for Inverse Design using Pearson Correlated Surrogate Annealing(https://arxiv.org/abs/2412.19284)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>PearSAN is a machine learning-assisted optimization algorithm applicable to inverse design problems with large design spaces, where traditional optimizers struggle. The algorithm leverages the latent space of a generative model for rapid sampling and employs a Pearson correlated surrogate model to predict the figure of merit of the true design metric. As a showcase example, PearSAN is applied to thermophotovoltaic (TPV) metasurface design by matching the working bands between a thermal radiator and a photovoltaic cell. PearSAN can work with any pretrained generative model with a discretized latent space, making it easy to integrate with VQ-VAEs and binary autoencoders. Its novel Pearson correlational loss can be used as both a latent regularization method, similar to batch and layer normalization, and as a surrogate training loss. We compare both to previous energy matching losses, which are shown to enforce poor regularization and performance, even with upgraded affine parameters. PearSAN achieves a state-of-the-art maximum design efficiency of 97%, and is at least an order of magnitude faster than previous methods, with an improved maximum figure-of-merit gain.</li>
</ul>

<h3>Title: Time Series Foundational Models: Their Role in Anomaly Detection and Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chathurangi Shyalika, Harleen Kaur Bagga, Ahan Bhatt, Renjith Prasad, Alaa Al Ghazo, Amit Sheth</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19286">https://arxiv.org/abs/2412.19286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19286">https://arxiv.org/pdf/2412.19286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19286]] Time Series Foundational Models: Their Role in Anomaly Detection and Prediction(https://arxiv.org/abs/2412.19286)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series foundational models (TSFM) have gained prominence in time series forecasting, promising state-of-the-art performance across various applications. However, their application in anomaly detection and prediction remains underexplored, with growing concerns regarding their black-box nature, lack of interpretability and applicability. This paper critically evaluates the efficacy of TSFM in anomaly detection and prediction tasks. We systematically analyze TSFM across multiple datasets, including those characterized by the absence of discernible patterns, trends and seasonality. Our analysis shows that while TSFMs can be extended for anomaly detection and prediction, traditional statistical and deep learning models often match or outperform TSFM in these tasks. Additionally, TSFMs require high computational resources but fail to capture sequential dependencies effectively or improve performance in few-shot or zero-shot scenarios. \noindent The preprocessed datasets, codes to reproduce the results and supplementary materials are available at this https URL.</li>
</ul>

<h3>Title: Manga Generation via Layout-controllable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Siyu Chen, Dengjie Li, Zenghao Bao, Yao Zhou, Lingfeng Tan, Yujie Zhong, Zheng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19303">https://arxiv.org/abs/2412.19303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19303">https://arxiv.org/pdf/2412.19303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19303]] Manga Generation via Layout-controllable Diffusion(https://arxiv.org/abs/2412.19303)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating comics through text is widely studied. However, there are few studies on generating multi-panel Manga (Japanese comics) solely based on plain text. Japanese manga contains multiple panels on a single page, with characteristics such as coherence in storytelling, reasonable and diverse page layouts, consistency in characters, and semantic correspondence between panel drawings and panel scripts. Therefore, generating manga poses a significant challenge. This paper presents the manga generation task and constructs the Manga109Story dataset for studying manga generation solely from plain text. Additionally, we propose MangaDiffusion to facilitate the intra-panel and inter-panel information interaction during the manga generation process. The results show that our method particularly ensures the number of panels, reasonable and diverse page layouts. Based on our approach, there is potential to converting a large amount of textual stories into more engaging manga readings, leading to significant application prospects.</li>
</ul>

<h3>Title: MINIMA: Modality Invariant Image Matching</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Jiang, Jiangwei Ren, Zizhuo Li, Xin Zhou, Dingkang Liang, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19412">https://arxiv.org/abs/2412.19412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19412">https://arxiv.org/pdf/2412.19412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19412]] MINIMA: Modality Invariant Image Matching(https://arxiv.org/abs/2412.19412)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image matching for both cross-view and cross-modality plays a critical role in multimodal perception. In practice, the modality gap caused by different imaging systems/styles poses great challenges to the matching task. Existing works try to extract invariant features for specific modalities and train on limited datasets, showing poor generalization. In this paper, we present MINIMA, a unified image matching framework for multiple cross-modal cases. Without pursuing fancy modules, our MINIMA aims to enhance universal performance from the perspective of data scaling up. For such purpose, we propose a simple yet effective data engine that can freely produce a large dataset containing multiple modalities, rich scenarios, and accurate matching labels. Specifically, we scale up the modalities from cheap but rich RGB-only matching data, by means of generative models. Under this setting, the matching labels and rich diversity of the RGB dataset are well inherited by the generated multimodal data. Benefiting from this, we construct MD-syn, a new comprehensive dataset that fills the data gap for general multimodal image matching. With MD-syn, we can directly train any advanced matching pipeline on randomly selected modality pairs to obtain cross-modal ability. Extensive experiments on in-domain and zero-shot matching tasks, including $19$ cross-modal cases, demonstrate that our MINIMA can significantly outperform the baselines and even surpass modality-specific methods. The dataset and code are available at this https URL .</li>
</ul>

<h3>Title: Multi-scale Latent Point Consistency Models for 3D Shape Generation</h3>
<ul>
<li><strong>Authors: </strong>Bi'an Du, Wei Hu, Renjie Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19413">https://arxiv.org/abs/2412.19413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19413">https://arxiv.org/pdf/2412.19413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19413]] Multi-scale Latent Point Consistency Models for 3D Shape Generation(https://arxiv.org/abs/2412.19413)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Consistency Models (CMs) have significantly accelerated the sampling process in diffusion models, yielding impressive results in synthesizing high-resolution images. To explore and extend these advancements to point-cloud-based 3D shape generation, we propose a novel Multi-scale Latent Point Consistency Model (MLPCM). Our MLPCM follows a latent diffusion framework and introduces hierarchical levels of latent representations, ranging from point-level to super-point levels, each corresponding to a different spatial resolution. We design a multi-scale latent integration module along with 3D spatial attention to effectively denoise the point-level latent representations conditioned on those from multiple super-point levels. Additionally, we propose a latent consistency model, learned through consistency distillation, that compresses the prior into a one-step generator. This significantly improves sampling efficiency while preserving the performance of the original teacher model. Extensive experiments on standard benchmarks ShapeNet and ShapeNet-Vol demonstrate that MLPCM achieves a 100x speedup in the generation process, while surpassing state-of-the-art diffusion models in terms of both shape quality and diversity.</li>
</ul>

<h3>Title: Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression Profiles via Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Chen Li, Yuki Matsukiyo, Yoshihiro Yamanishi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19422">https://arxiv.org/abs/2412.19422</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19422">https://arxiv.org/pdf/2412.19422</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19422]] Gx2Mol: De Novo Generation of Hit-like Molecules from Gene Expression Profiles via Deep Learning(https://arxiv.org/abs/2412.19422)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>De novo generation of hit-like molecules is a challenging task in the drug discovery process. Most methods in previous studies learn the semantics and syntax of molecular structures by analyzing molecular graphs or simplified molecular input line entry system (SMILES) strings; however, they do not take into account the drug responses of the biological systems consisting of genes and proteins. In this study we propose a deep generative model, Gx2Mol, which utilizes gene expression profiles to generate molecular structures with desirable phenotypes for arbitrary target proteins. In the algorithm, a variational autoencoder is employed as a feature extractor to learn the latent feature distribution of the gene expression profiles. Then, a long short-term memory is leveraged as the chemical generator to produce syntactically valid SMILES strings that satisfy the feature conditions of the gene expression profile extracted by the feature extractor. Experimental results and case studies demonstrate that the proposed Gx2Mol model can produce new molecules with potential bioactivities and drug-like properties.</li>
</ul>

<h3>Title: Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuo Wang, Chihang Wang, Jia Gao, Zhen Qi, Hongye Zheng, Xiaoxuan Liao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19449">https://arxiv.org/abs/2412.19449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19449">https://arxiv.org/pdf/2412.19449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19449]] Feature Alignment-Based Knowledge Distillation for Efficient Compression of Large Language Models(https://arxiv.org/abs/2412.19449)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This study proposes a knowledge distillation algorithm based on large language models and feature alignment, aiming to effectively transfer the knowledge of large pre-trained models into lightweight student models, thereby reducing computational costs while maintaining high model performance. Different from the traditional soft label distillation method, this method introduces a multi-layer feature alignment strategy to deeply align the intermediate features and attention mechanisms of the teacher model and the student model, maximally retaining the semantic expression ability and context modeling ability of the teacher model. In terms of method design, a multi-task loss function is constructed, including feature matching loss, attention alignment loss, and output distribution matching loss, to ensure multi-level information transfer through joint optimization. The experiments were comprehensively evaluated on the GLUE data set and various natural language processing tasks. The results show that the proposed model performs very close to the state-of-the-art GPT-4 model in terms of evaluation indicators such as perplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline models such as DeBERTa, XLNet, and GPT-3, showing significant performance improvements and computing efficiency advantages. Research results show that the feature alignment distillation strategy is an effective model compression method that can significantly reduce computational overhead and storage requirements while maintaining model capabilities. Future research can be further expanded in the directions of self-supervised learning, cross-modal feature alignment, and multi-task transfer learning to provide more flexible and efficient solutions for the deployment and optimization of deep learning models.</li>
</ul>

<h3>Title: NijiGAN: Transform What You See into Anime with Contrastive Semi-Supervised Learning and Neural Ordinary Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Kevin Putra Santoso, Anny Yuniarti, Dwiyasa Nakula, Dimas Prihady Setyawan, Adam Haidar Azizi, Jeany Aurellia P. Dewati, Farah Dhia Fadhila, Maria T. Elvara Bumbungan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19455">https://arxiv.org/abs/2412.19455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19455">https://arxiv.org/pdf/2412.19455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19455]] NijiGAN: Transform What You See into Anime with Contrastive Semi-Supervised Learning and Neural Ordinary Differential Equations(https://arxiv.org/abs/2412.19455)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI has transformed the animation industry. Several models have been developed for image-to-image translation, particularly focusing on converting real-world images into anime through unpaired translation. Scenimefy, a notable approach utilizing contrastive learning, achieves high fidelity anime scene translation by addressing limited paired data through semi-supervised training. However, it faces limitations due to its reliance on paired data from a fine-tuned StyleGAN in the anime domain, often producing low-quality datasets. Additionally, Scenimefy's high parameter architecture presents opportunities for computational optimization. This research introduces NijiGAN, a novel model incorporating Neural Ordinary Differential Equations (NeuralODEs), which offer unique advantages in continuous transformation modeling compared to traditional residual networks. NijiGAN successfully transforms real-world scenes into high fidelity anime visuals using half of Scenimefy's parameters. It employs pseudo-paired data generated through Scenimefy for supervised training, eliminating dependence on low-quality paired data and improving the training process. Our comprehensive evaluation includes ablation studies, qualitative, and quantitative analysis comparing NijiGAN to similar models. The testing results demonstrate that NijiGAN produces higher-quality images compared to AnimeGAN, as evidenced by a Mean Opinion Score (MOS) of 2.192, it surpasses AnimeGAN's MOS of 2.160. Furthermore, our model achieved a Frechet Inception Distance (FID) score of 58.71, outperforming Scenimefy's FID score of 60.32. These results demonstrate that NijiGAN achieves competitive performance against existing state-of-the-arts, especially Scenimefy as the baseline model.</li>
</ul>

<h3>Title: DriveEditor: A Unified 3D Information-Guided Framework for Controllable Object Editing in Driving Scenes</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Liang, Zhiying Yan, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19458">https://arxiv.org/abs/2412.19458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19458">https://arxiv.org/pdf/2412.19458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19458]] DriveEditor: A Unified 3D Information-Guided Framework for Controllable Object Editing in Driving Scenes(https://arxiv.org/abs/2412.19458)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Vision-centric autonomous driving systems require diverse data for robust training and evaluation, which can be augmented by manipulating object positions and appearances within existing scene captures. While recent advancements in diffusion models have shown promise in video editing, their application to object manipulation in driving scenarios remains challenging due to imprecise positional control and difficulties in preserving high-fidelity object appearances. To address these challenges in position and appearance control, we introduce DriveEditor, a diffusion-based framework for object editing in driving videos. DriveEditor offers a unified framework for comprehensive object editing operations, including repositioning, replacement, deletion, and insertion. These diverse manipulations are all achieved through a shared set of varying inputs, processed by identical position control and appearance maintenance modules. The position control module projects the given 3D bounding box while preserving depth information and hierarchically injects it into the diffusion process, enabling precise control over object position and orientation. The appearance maintenance module preserves consistent attributes with a single reference image by employing a three-tiered approach: low-level detail preservation, high-level semantic maintenance, and the integration of 3D priors from a novel view synthesis model. Extensive qualitative and quantitative evaluations on the nuScenes dataset demonstrate DriveEditor's exceptional fidelity and controllability in generating diverse driving scene edits, as well as its remarkable ability to facilitate downstream tasks.</li>
</ul>

<h3>Title: Generative Adversarial Network on Motion-Blur Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhengdong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19479">https://arxiv.org/abs/2412.19479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19479">https://arxiv.org/pdf/2412.19479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19479]] Generative Adversarial Network on Motion-Blur Image Restoration(https://arxiv.org/abs/2412.19479)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In everyday life, photographs taken with a camera often suffer from motion blur due to hand vibrations or sudden movements. This phenomenon can significantly detract from the quality of the images captured, making it an interesting challenge to develop a deep learning model that utilizes the principles of adversarial networks to restore clarity to these blurred pixels. In this project, we will focus on leveraging Generative Adversarial Networks (GANs) to effectively deblur images affected by motion blur. A GAN-based Tensorflow model is defined, training and evaluating by GoPro dataset which comprises paired street view images featuring both clear and blurred versions. This adversarial training process between Discriminator and Generator helps to produce increasingly realistic images over time. Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) are the two evaluation metrics used to provide quantitative measures of image quality, allowing us to evaluate the effectiveness of the deblurring process. Mean PSNR in 29.1644 and mean SSIM in 0.7459 with average 4.6921 seconds deblurring time are achieved in this project. The blurry pixels are sharper in the output of GAN model shows a good image restoration effect in real world applications.</li>
</ul>

<h3>Title: Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for Legal Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Shiwen Ni, Hao Cheng, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19482">https://arxiv.org/abs/2412.19482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19482">https://arxiv.org/pdf/2412.19482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19482]] Pre-training, Fine-tuning and Re-ranking: A Three-Stage Framework for Legal Question Answering(https://arxiv.org/abs/2412.19482)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Legal question answering (QA) has attracted increasing attention from people seeking legal advice, which aims to retrieve the most applicable answers from a large-scale database of question-answer pairs. Previous methods mainly use a dual-encoder architecture to learn dense representations of both questions and answers. However, these methods could suffer from lacking domain knowledge and sufficient labeled training data. In this paper, we propose a three-stage (\underline{p}re-training, \underline{f}ine-tuning and \underline{r}e-ranking) framework for \underline{l}egal \underline{QA} (called PFR-LQA), which promotes the fine-grained text representation learning and boosts the performance of dense retrieval with the dual-encoder architecture. Concretely, we first conduct domain-specific pre-training on legal questions and answers through a self-supervised training objective, allowing the pre-trained model to be adapted to the legal domain. Then, we perform task-specific fine-tuning of the dual-encoder on legal question-answer pairs by using the supervised learning objective, leading to a high-quality dual-encoder for the specific downstream QA task. Finally, we employ a contextual re-ranking objective to further refine the output representations of questions produced by the document encoder, which uses contextual similarity to increase the discrepancy between the anchor and hard negative samples for better question re-ranking. We conduct extensive experiments on a manually annotated legal QA dataset. Experimental results show that our PFR-LQA method achieves better performance than the strong competitors for legal question answering.</li>
</ul>

<h3>Title: RAIN: Real-time Animation of Infinite Video Stream</h3>
<ul>
<li><strong>Authors: </strong>Zhilei Shu, Ruili Feng, Yang Cao, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19489">https://arxiv.org/abs/2412.19489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19489">https://arxiv.org/pdf/2412.19489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19489]] RAIN: Real-time Animation of Infinite Video Stream(https://arxiv.org/abs/2412.19489)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Live animation has gained immense popularity for enhancing online engagement, yet achieving high-quality, real-time, and stable animation with diffusion models remains challenging, especially on consumer-grade GPUs. Existing methods struggle with generating long, consistent video streams efficiently, often being limited by latency issues and degraded visual quality over extended periods. In this paper, we introduce RAIN, a pipeline solution capable of animating infinite video streams in real-time with low latency using a single RTX 4090 GPU. The core idea of RAIN is to efficiently compute frame-token attention across different noise levels and long time-intervals while simultaneously denoising a significantly larger number of frame-tokens than previous stream-based methods. This design allows RAIN to generate video frames with much shorter latency and faster speed, while maintaining long-range attention over extended video streams, resulting in enhanced continuity and consistency. Consequently, a Stable Diffusion model fine-tuned with RAIN in just a few epochs can produce video streams in real-time and low latency without much compromise in quality or consistency, up to infinite long. Despite its advanced capabilities, the RAIN only introduces a few additional 1D attention blocks, imposing minimal additional burden. Experiments in benchmark datasets and generating super-long videos demonstrating that RAIN can animate characters in real-time with much better quality, accuracy, and consistency than competitors while costing less latency. All code and models will be made publicly available.</li>
</ul>

<h3>Title: Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19513">https://arxiv.org/abs/2412.19513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19513">https://arxiv.org/pdf/2412.19513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19513]] Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs(https://arxiv.org/abs/2412.19513)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can correct their self-generated responses, but a decline in accuracy after self-correction is also witnessed. To have a deeper understanding of self-correction, we endeavor to decompose, evaluate, and analyze the self-correction behaviors of LLMs. By enumerating and analyzing answer correctness before and after self-correction, we decompose the self-correction capability into confidence (being confident to correct answers) and critique (turning wrong answers to correct) capabilities, and propose two metrics from a probabilistic perspective to measure these 2 capabilities, along with another metric for overall self-correction capability evaluation. Based on our decomposition and evaluation metrics, we conduct extensive experiments and draw some empirical conclusions. For example, we find different models can exhibit distinct behaviors: some models are confident while others are more critical. We also find the trade-off between the two capabilities (i.e. improving one can lead to a decline in the other) when manipulating model self-correction behavior by prompts or in-context learning. Further, we find a simple yet efficient strategy to improve self-correction capability by transforming Supervision Fine-Tuning (SFT) data format, and our strategy outperforms vanilla SFT in both capabilities and achieves much higher accuracy after self-correction. Our code will be publicly available on GitHub.</li>
</ul>

<h3>Title: Estimation of System Parameters Including Repeated Cross-Sectional Data through Emulator-Informed Deep Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Hyunwoo Cho, Sung Woong Cho, Hyeontae Jo, Hyung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA, q-bio.PE, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19517">https://arxiv.org/abs/2412.19517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19517">https://arxiv.org/pdf/2412.19517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19517]] Estimation of System Parameters Including Repeated Cross-Sectional Data through Emulator-Informed Deep Generative Model(https://arxiv.org/abs/2412.19517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Differential equations (DEs) are crucial for modeling the evolution of natural or engineered systems. Traditionally, the parameters in DEs are adjusted to fit data from system observations. However, in fields such as politics, economics, and biology, available data are often independently collected at distinct time points from different subjects (i.e., repeated cross-sectional (RCS) data). Conventional optimization techniques struggle to accurately estimate DE parameters when RCS data exhibit various heterogeneities, leading to a significant loss of information. To address this issue, we propose a new estimation method called the emulator-informed deep-generative model (EIDGM), designed to handle RCS data. Specifically, EIDGM integrates a physics-informed neural network-based emulator that immediately generates DE solutions and a Wasserstein generative adversarial network-based parameter generator that can effectively mimic the RCS data. We evaluated EIDGM on exponential growth, logistic population models, and the Lorenz system, demonstrating its superior ability to accurately capture parameter distributions. Additionally, we applied EIDGM to an experimental dataset of Amyloid beta 40 and beta 42, successfully capturing diverse parameter distribution shapes. This shows that EIDGM can be applied to model a wide range of systems and extended to uncover the operating principles of systems based on limited data.</li>
</ul>

<h3>Title: P3S-Diffusion:A Selective Subject-driven Generation Framework via Point Supervision</h3>
<ul>
<li><strong>Authors: </strong>Junjie Hu (1), Shuyong Gao (1), Lingyi Hong (1), Qishan Wang (1), Yuzhou Zhao (1), Yan Wang (1), Wenqiang Zhang (1) ((1) Fudan university)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19533">https://arxiv.org/abs/2412.19533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19533">https://arxiv.org/pdf/2412.19533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19533]] P3S-Diffusion:A Selective Subject-driven Generation Framework via Point Supervision(https://arxiv.org/abs/2412.19533)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent research in subject-driven generation increasingly emphasizes the importance of selective subject features. Nevertheless, accurately selecting the content in a given reference image still poses challenges, especially when selecting the similar subjects in an image (e.g., two different dogs). Some methods attempt to use text prompts or pixel masks to isolate specific elements. However, text prompts often fall short in precisely describing specific content, and pixel masks are often expensive. To address this, we introduce P3S-Diffusion, a novel architecture designed for context-selected subject-driven generation via point supervision. P3S-Diffusion leverages minimal cost label (e.g., points) to generate subject-driven images. During fine-tuning, it can generate an expanded base mask from these points, obviating the need for additional segmentation models. The mask is employed for inpainting and aligning with subject representation. The P3S-Diffusion preserves fine features of the subjects through Multi-layers Condition Injection. Enhanced by the Attention Consistency Loss for improved training, extensive experiments demonstrate its excellent feature preservation and image generation capabilities.</li>
</ul>

<h3>Title: StyleRWKV: High-Quality and High-Efficiency Style Transfer with RWKV-like Architecture</h3>
<ul>
<li><strong>Authors: </strong>Miaomiao Dai, Qianyu Zhou, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19535">https://arxiv.org/abs/2412.19535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19535">https://arxiv.org/pdf/2412.19535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19535]] StyleRWKV: High-Quality and High-Efficiency Style Transfer with RWKV-like Architecture(https://arxiv.org/abs/2412.19535)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Style transfer aims to generate a new image preserving the content but with the artistic representation of the style source. Most of the existing methods are based on Transformers or diffusion models, however, they suffer from quadratic computational complexity and high inference time. RWKV, as an emerging deep sequence models, has shown immense potential for long-context sequence modeling in NLP tasks. In this work, we present a novel framework StyleRWKV, to achieve high-quality style transfer with limited memory usage and linear time complexity. Specifically, we propose a Recurrent WKV (Re-WKV) attention mechanism, which incorporates bidirectional attention to establish a global receptive field. Additionally, we develop a Deformable Shifting (Deform-Shifting) layer that introduces learnable offsets to the sampling grid of the convolution kernel, allowing tokens to shift flexibly and adaptively from the region of interest, thereby enhancing the model's ability to capture local dependencies. Finally, we propose a Skip Scanning (S-Scanning) method that effectively establishes global contextual dependencies. Extensive experiments with analysis including qualitative and quantitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of stylization quality, model complexity, and inference efficiency.</li>
</ul>

<h3>Title: Diverse Rare Sample Generation with Pretrained GANs</h3>
<ul>
<li><strong>Authors: </strong>Subeen Lee, Jiyeon Han, Soyeon Kim, Jaesik Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19543">https://arxiv.org/abs/2412.19543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19543">https://arxiv.org/pdf/2412.19543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19543]] Diverse Rare Sample Generation with Pretrained GANs(https://arxiv.org/abs/2412.19543)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models are proficient in generating realistic data but struggle with producing rare samples in low density regions due to their scarcity of training datasets and the mode collapse problem. While recent methods aim to improve the fidelity of generated samples, they often reduce diversity and coverage by ignoring rare and novel samples. This study proposes a novel approach for generating diverse rare samples from high-resolution image datasets with pretrained GANs. Our method employs gradient-based optimization of latent vectors within a multi-objective framework and utilizes normalizing flows for density estimation on the feature space. This enables the generation of diverse rare images, with controllable parameters for rarity, diversity, and similarity to a reference image. We demonstrate the effectiveness of our approach both qualitatively and quantitatively across various datasets and GANs without retraining or fine-tuning the pretrained GANs.</li>
</ul>

<h3>Title: TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data</h3>
<ul>
<li><strong>Authors: </strong>Xiang Huang, Jiayu Shen, Shanshan Huang, Sitao Cheng, Xiaxia Wang, Yuzhong Qu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19544">https://arxiv.org/abs/2412.19544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19544">https://arxiv.org/pdf/2412.19544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19544]] TARGA: Targeted Synthetic Data Generation for Practical Reasoning over Structured Data(https://arxiv.org/abs/2412.19544)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Semantic parsing, which converts natural language questions into logic forms, plays a crucial role in reasoning within structured environments. However, existing methods encounter two significant challenges: reliance on extensive manually annotated datasets and limited generalization capability to unseen examples. To tackle these issues, we propose Targeted Synthetic Data Generation (TARGA), a practical framework that dynamically generates high-relevance synthetic data without manual annotation. Starting from the pertinent entities and relations of a given question, we probe for the potential relevant queries through layer-wise expansion and cross-layer combination. Then we generate corresponding natural language questions for these constructed queries to jointly serve as the synthetic demonstrations for in-context learning. Experiments on multiple knowledge base question answering (KBQA) datasets demonstrate that TARGA, using only a 7B-parameter model, substantially outperforms existing non-fine-tuned methods that utilize close-sourced model, achieving notable improvements in F1 scores on GrailQA(+7.7) and KBQA-Agent(+12.2). Furthermore, TARGA also exhibits superior sample efficiency, robustness, and generalization capabilities under non-I.I.D. settings.</li>
</ul>

<h3>Title: VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tao Wu, Yong Zhang, Xiaodong Cun, Zhongang Qi, Junfu Pu, Huanzhang Dou, Guangcong Zheng, Ying Shan, Xi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19645">https://arxiv.org/abs/2412.19645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19645">https://arxiv.org/pdf/2412.19645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19645]] VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models(https://arxiv.org/abs/2412.19645)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Zero-shot customized video generation has gained significant attention due to its substantial application potential. Existing methods rely on additional models to extract and inject reference subject features, assuming that the Video Diffusion Model (VDM) alone is insufficient for zero-shot customized video generation. However, these methods often struggle to maintain consistent subject appearance due to suboptimal feature extraction and injection techniques. In this paper, we reveal that VDM inherently possesses the force to extract and inject subject features. Departing from previous heuristic approaches, we introduce a novel framework that leverages VDM's inherent force to enable high-quality zero-shot customized video generation. Specifically, for feature extraction, we directly input reference images into VDM and use its intrinsic feature extraction process, which not only provides fine-grained features but also significantly aligns with VDM's pre-trained knowledge. For feature injection, we devise an innovative bidirectional interaction between subject features and generated content through spatial self-attention within VDM, ensuring that VDM has better subject fidelity while maintaining the diversity of the generated this http URL on both customized human and object video generation validate the effectiveness of our framework.</li>
</ul>

<h3>Title: FreStega: A Plug-and-Play Method for Boosting Imperceptibility and Capacity in Generative Linguistic Steganography for Real-World Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Kaiyi Pang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19652">https://arxiv.org/abs/2412.19652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19652">https://arxiv.org/pdf/2412.19652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19652]] FreStega: A Plug-and-Play Method for Boosting Imperceptibility and Capacity in Generative Linguistic Steganography for Real-World Scenarios(https://arxiv.org/abs/2412.19652)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Linguistic steganography embeds secret information in seemingly innocent texts, safeguarding privacy in surveillance environments. Generative linguistic steganography leverages the probability distribution of language models (LMs) and applies steganographic algorithms to generate stego tokens, gaining attention with recent Large Language Model (LLM) advancements. To enhance security, researchers develop distribution-preserving stego algorithms to minimize the gap between stego sampling and LM sampling. However, the reliance on language model distributions, coupled with deviations from real-world cover texts, results in insufficient imperceptibility when facing steganalysis detectors in real-world scenarios. Moreover, LLM distributions tend to be more deterministic, resulting in reduced entropy and, consequently, lower embedding capacity. In this paper, we propose FreStega, a plug-and-play method to reconstruct the distribution of language models used for generative linguistic steganography. FreStega dynamically adjusts token probabilities from the language model at each step of stegotext auto-regressive generation, leveraging both sequential and spatial dimensions. In sequential adjustment, the temperature is dynamically adjusted based on instantaneous entropy, enhancing the diversity of stego texts and boosting embedding capacity. In the spatial dimension, the distribution is aligned with guidance from the target domain corpus, closely mimicking real cover text in the target domain. By reforming the distribution, FreStega enhances the imperceptibility of stego text in practical scenarios and improves steganographic capacity by 15.41\%, all without compromising the quality of the generated text. FreStega serves as a plug-and-play remedy to enhance the imperceptibility and embedding capacity of existing distribution-preserving steganography methods in real-world scenarios.</li>
</ul>

<h3>Title: From Elements to Design: A Layered Approach for Automatic Graphic Design Composition</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Lin, Shizhao Sun, Danqing Huang, Ting Liu, Ji Li, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19712">https://arxiv.org/abs/2412.19712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19712">https://arxiv.org/pdf/2412.19712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19712]] From Elements to Design: A Layered Approach for Automatic Graphic Design Composition(https://arxiv.org/abs/2412.19712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we investigate automatic design composition from multimodal graphic elements. Although recent studies have developed various generative models for graphic design, they usually face the following limitations: they only focus on certain subtasks and are far from achieving the design composition task; they do not consider the hierarchical information of graphic designs during the generation process. To tackle these issues, we introduce the layered design principle into Large Multimodal Models (LMMs) and propose a novel approach, called LaDeCo, to accomplish this challenging task. Specifically, LaDeCo first performs layer planning for a given element set, dividing the input elements into different semantic layers according to their contents. Based on the planning results, it subsequently predicts element attributes that control the design composition in a layer-wise manner, and includes the rendered image of previously generated layers into the context. With this insightful design, LaDeCo decomposes the difficult task into smaller manageable steps, making the generation process smoother and clearer. The experimental results demonstrate the effectiveness of LaDeCo in design composition. Furthermore, we show that LaDeCo enables some interesting applications in graphic design, such as resolution adjustment, element filling, design variation, etc. In addition, it even outperforms the specialized models in some design subtasks without any task-specific training.</li>
</ul>

<h3>Title: Generative Pretrained Embedding and Hierarchical Irregular Time Series Representation for Daily Living Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Damien Bouchabou, Sao Mai Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19732">https://arxiv.org/abs/2412.19732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19732">https://arxiv.org/pdf/2412.19732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19732]] Generative Pretrained Embedding and Hierarchical Irregular Time Series Representation for Daily Living Activity Recognition(https://arxiv.org/abs/2412.19732)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Within the evolving landscape of smart homes, the precise recognition of daily living activities using ambient sensor data stands paramount. This paper not only aims to bolster existing algorithms by evaluating two distinct pretrained embeddings suited for ambient sensor activations but also introduces a novel hierarchical architecture. We delve into an architecture anchored on Transformer Decoder-based pre-trained embeddings, reminiscent of the GPT design, and contrast it with the previously established state-of-the-art (SOTA) ELMo embeddings for ambient sensors. Our proposed hierarchical structure leverages the strengths of each pre-trained embedding, enabling the discernment of activity dependencies and sequence order, thereby enhancing classification precision. To further refine recognition, we incorporate into our proposed architecture an hour-of-the-day embedding. Empirical evaluations underscore the preeminence of the Transformer Decoder embedding in classification endeavors. Additionally, our innovative hierarchical design significantly bolsters the efficacy of both pre-trained embeddings, notably in capturing inter-activity nuances. The integration of temporal aspects subtly but distinctively augments classification, especially for time-sensitive activities. In conclusion, our GPT-inspired hierarchical approach, infused with temporal insights, outshines the SOTA ELMo benchmark.</li>
</ul>

<h3>Title: Generative Video Propagation</h3>
<ul>
<li><strong>Authors: </strong>Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, Jiaya Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19761">https://arxiv.org/abs/2412.19761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19761">https://arxiv.org/pdf/2412.19761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19761]] Generative Video Propagation(https://arxiv.org/abs/2412.19761)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large-scale video generation models have the inherent ability to realistically model natural scenes. In this paper, we demonstrate that through a careful design of a generative video propagation framework, various video tasks can be addressed in a unified way by leveraging the generative power of such models. Specifically, our framework, GenProp, encodes the original video with a selective content encoder and propagates the changes made to the first frame using an image-to-video generation model. We propose a data generation scheme to cover multiple video tasks based on instance-level video segmentation datasets. Our model is trained by incorporating a mask prediction decoder head and optimizing a region-aware loss to aid the encoder to preserve the original content while the generation model propagates the modified region. This novel design opens up new possibilities: In editing scenarios, GenProp allows substantial changes to an object's shape; for insertion, the inserted objects can exhibit independent motion; for removal, GenProp effectively removes effects like shadows and reflections from the whole video; for tracking, GenProp is capable of tracking objects and their associated effects together. Experiment results demonstrate the leading performance of our model in various video tasks, and we further provide in-depth analyses of the proposed framework.</li>
</ul>

<h3>Title: Tensor Network Estimation of Distribution Algorithms</h3>
<ul>
<li><strong>Authors: </strong>John Gardiner, Javier Lopez-Piqueres</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19780">https://arxiv.org/abs/2412.19780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19780">https://arxiv.org/pdf/2412.19780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19780]] Tensor Network Estimation of Distribution Algorithms(https://arxiv.org/abs/2412.19780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tensor networks are a tool first employed in the context of many-body quantum physics that now have a wide range of uses across the computational sciences, from numerical methods to machine learning. Methods integrating tensor networks into evolutionary optimization algorithms have appeared in the recent literature. In essence, these methods can be understood as replacing the traditional crossover operation of a genetic algorithm with a tensor network-based generative model. We investigate these methods from the point of view that they are Estimation of Distribution Algorithms (EDAs). We find that optimization performance of these methods is not related to the power of the generative model in a straightforward way. Generative models that are better (in the sense that they better model the distribution from which their training data is drawn) do not necessarily result in better performance of the optimization algorithm they form a part of. This raises the question of how best to incorporate powerful generative models into optimization routines. In light of this we find that adding an explicit mutation operator to the output of the generative model often improves optimization performance.</li>
</ul>

<h3>Title: InfAlign: Inference-aware language model alignment</h3>
<ul>
<li><strong>Authors: </strong>Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.19792">https://arxiv.org/abs/2412.19792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.19792">https://arxiv.org/pdf/2412.19792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.19792]] InfAlign: Inference-aware language model alignment(https://arxiv.org/abs/2412.19792)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the win rate of a sample from the aligned model over a sample from the reference model is high, subject to a KL divergence constraint. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. However, the alignment objective does not capture such inference-time decoding procedures. We show that the existing alignment framework is sub-optimal in view of such inference-time methods. We then modify the alignment objective and propose a framework for inference-aware alignment (IAPO). We prove that for any inference-time decoding algorithm, the optimal solution that optimizes the inference-time win rate of the aligned policy against the reference policy is the solution to the typical RLHF problem with a transformation of the reward. This motivates us to provide the KL-regularized calibrate-and-transform RL (CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. We particularize our study to two important inference-time strategies: best-of-N sampling and best-of-N jailbreaking, where N responses are sampled from the model and the one with the highest or lowest reward is selected. We propose specific transformations for these strategies and demonstrate that our framework offers significant improvements over existing state-of-the-art methods for language model alignment. Empirically, we outperform baselines that are designed without taking inference-time decoding into consideration by 8-12% and 4-9% on inference-time win rates over the Anthropic helpfulness and harmlessness dialog benchmark datasets.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
