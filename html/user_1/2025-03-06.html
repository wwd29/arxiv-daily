<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-03-06</h1>
<h3>Title: Straight-Line Diffusion Model for Efficient 3D Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuyan Ni, Shikun Feng, Haohan Chi, Bowen Zheng, Huan-ang Gao, Wei-Ying Ma, Zhi-Ming Ma, Yanyan Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02918">https://arxiv.org/abs/2503.02918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02918">https://arxiv.org/pdf/2503.02918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02918]] Straight-Line Diffusion Model for Efficient 3D Molecular Generation(https://arxiv.org/abs/2503.02918)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based models have shown great promise in molecular generation but often require a large number of sampling steps to generate valid samples. In this paper, we introduce a novel Straight-Line Diffusion Model (SLDM) to tackle this problem, by formulating the diffusion process to follow a linear trajectory. The proposed process aligns well with the noise sensitivity characteristic of molecular structures and uniformly distributes reconstruction effort across the generative process, thus enhancing learning efficiency and efficacy. Consequently, SLDM achieves state-of-the-art performance on 3D molecule generation benchmarks, delivering a 100-fold improvement in sampling efficiency. Furthermore, experiments on toy data and image generation tasks validate the generality and robustness of SLDM, showcasing its potential across diverse generative modeling domains.</li>
</ul>

<h3>Title: Robust time series generation via Schrödinger Bridge: a comprehensive evaluation</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Alouadi, Baptiste Barreau, Laurent Carlier, Huyên Pham</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02943">https://arxiv.org/abs/2503.02943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02943">https://arxiv.org/pdf/2503.02943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02943]] Robust time series generation via Schrödinger Bridge: a comprehensive evaluation(https://arxiv.org/abs/2503.02943)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We investigate the generative capabilities of the Schrödinger Bridge (SB) approach for time series. The SB framework formulates time series synthesis as an entropic optimal interpolation transport problem between a reference probability measure on path space and a target joint distribution. This results in a stochastic differential equation over a finite horizon that accurately captures the temporal dynamics of the target time series. While the SB approach has been largely explored in fields like image generation, there is a scarcity of studies for its application to time series. In this work, we bridge this gap by conducting a comprehensive evaluation of the SB method's robustness and generative performance. We benchmark it against state-of-the-art (SOTA) time series generation methods across diverse datasets, assessing its strengths, limitations, and capacity to model complex temporal dependencies. Our results offer valuable insights into the SB framework's potential as a versatile and robust tool for time series generation.</li>
</ul>

<h3>Title: Revolutionizing Traffic Management with AI-Powered Machine Vision: A Step Toward Smart Cities</h3>
<ul>
<li><strong>Authors: </strong>Seyed Hossein Hosseini DolatAbadi, Sayyed Mohammad Hossein Hashemi, Mohammad Hosseini, Moein-Aldin AliHosseini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02967">https://arxiv.org/abs/2503.02967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02967">https://arxiv.org/pdf/2503.02967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02967]] Revolutionizing Traffic Management with AI-Powered Machine Vision: A Step Toward Smart Cities(https://arxiv.org/abs/2503.02967)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The rapid urbanization of cities and increasing vehicular congestion have posed significant challenges to traffic management and safety. This study explores the transformative potential of artificial intelligence (AI) and machine vision technologies in revolutionizing traffic systems. By leveraging advanced surveillance cameras and deep learning algorithms, this research proposes a system for real-time detection of vehicles, traffic anomalies, and driver behaviors. The system integrates geospatial and weather data to adapt dynamically to environmental conditions, ensuring robust performance in diverse scenarios. Using YOLOv8 and YOLOv11 models, the study achieves high accuracy in vehicle detection and anomaly recognition, optimizing traffic flow and enhancing road safety. These findings contribute to the development of intelligent traffic management solutions and align with the vision of creating smart cities with sustainable and efficient urban infrastructure.</li>
</ul>

<h3>Title: Integrating Predictive and Generative Capabilities by Latent Space Design via the DKL-VAE Model</h3>
<ul>
<li><strong>Authors: </strong>Boris N. Slautin, Utkarsh Pratiush, Doru C. Lupascu, Maxim A. Ziatdinov, Sergei V. Kalinin</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.02978">https://arxiv.org/abs/2503.02978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.02978">https://arxiv.org/pdf/2503.02978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.02978]] Integrating Predictive and Generative Capabilities by Latent Space Design via the DKL-VAE Model(https://arxiv.org/abs/2503.02978)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a Deep Kernel Learning Variational Autoencoder (VAE-DKL) framework that integrates the generative power of a Variational Autoencoder (VAE) with the predictive nature of Deep Kernel Learning (DKL). The VAE learns a latent representation of high-dimensional data, enabling the generation of novel structures, while DKL refines this latent space by structuring it in alignment with target properties through Gaussian Process (GP) regression. This approach preserves the generative capabilities of the VAE while enhancing its latent space for GP-based property prediction. We evaluate the framework on two datasets: a structured card dataset with predefined variational factors and the QM9 molecular dataset, where enthalpy serves as the target function for optimization. The model demonstrates high-precision property prediction and enables the generation of novel out-of-training subset structures with desired characteristics. The VAE-DKL framework offers a promising approach for high-throughput material discovery and molecular design, balancing structured latent space organization with generative flexibility.</li>
</ul>

<h3>Title: Network Anomaly Detection for IoT Using Hyperdimensional Computing on NSL-KDD</h3>
<ul>
<li><strong>Authors: </strong>Ghazal Ghajari, Ashutosh Ghimire, Elaheh Ghajari, Fathi Amsaad</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03031">https://arxiv.org/abs/2503.03031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03031">https://arxiv.org/pdf/2503.03031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03031]] Network Anomaly Detection for IoT Using Hyperdimensional Computing on NSL-KDD(https://arxiv.org/abs/2503.03031)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>With the rapid growth of IoT devices, ensuring robust network security has become a critical challenge. Traditional intrusion detection systems (IDSs) often face limitations in detecting sophisticated attacks within high-dimensional and complex data environments. This paper presents a novel approach to network anomaly detection using hyperdimensional computing (HDC) techniques, specifically applied to the NSL-KDD dataset. The proposed method leverages the efficiency of HDC in processing large-scale data to identify both known and unknown attack patterns. The model achieved an accuracy of 91.55% on the KDDTrain+ subset, outperforming traditional approaches. These comparative evaluations underscore the model's superior performance, highlighting its potential in advancing anomaly detection for IoT networks and contributing to more secure and intelligent cybersecurity solutions.</li>
</ul>

<h3>Title: Generative assimilation and prediction for weather and climate</h3>
<ul>
<li><strong>Authors: </strong>Shangshang Yang, Congyi Nai, Xinyan Liu, Weidong Li, Jie Chao, Jingnan Wang, Leyi Wang, Xichen Li, Xi Chen, Bo Lu, Ziniu Xiao, Niklas Boers, Huiling Yuan, Baoxiang Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03038">https://arxiv.org/abs/2503.03038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03038">https://arxiv.org/pdf/2503.03038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03038]] Generative assimilation and prediction for weather and climate(https://arxiv.org/abs/2503.03038)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning models have shown great success in predicting weather up to two weeks ahead, outperforming process-based benchmarks. However, existing approaches mostly focus on the prediction task, and do not incorporate the necessary data assimilation. Moreover, these models suffer from error accumulation in long roll-outs, limiting their applicability to seasonal predictions or climate projections. Here, we introduce Generative Assimilation and Prediction (GAP), a unified deep generative framework for assimilation and prediction of both weather and climate. By learning to quantify the probabilistic distribution of atmospheric states under observational, predictive, and external forcing constraints, GAP excels in a broad range of weather-climate related tasks, including data assimilation, seamless prediction, and climate simulation. In particular, GAP is competitive with state-of-the-art ensemble assimilation, probabilistic weather forecast and seasonal prediction, yields stable millennial simulations, and reproduces climate variability from daily to decadal time scales.</li>
</ul>

<h3>Title: Semi-Supervised In-Context Learning: A Baseline Study</h3>
<ul>
<li><strong>Authors: </strong>Zhengyao Gu, Henry Peng Zou, Yankai Chen, Aiwei Liu, Weizhi Zhang, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03062">https://arxiv.org/abs/2503.03062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03062">https://arxiv.org/pdf/2503.03062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03062]] Semi-Supervised In-Context Learning: A Baseline Study(https://arxiv.org/abs/2503.03062)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Most existing work in data selection for In-Context Learning (ICL) has focused on constructing demonstrations from ground truth annotations, with limited attention given to selecting reliable self-generated annotations. In this work, we propose a three-step semi-supervised ICL framework: annotation generation, demonstration selection, and semi-supervised inference. Our baseline, Naive-SemiICL, which prompts select high-confidence self-generated demonstrations for ICL prompting, outperforms a 16-shot baseline by an average of 9.94% across 16 datasets. We further introduce IterPSD, an annotation approach that refines pseudo-demonstrations iteratively, achieving up to 6.8% additional gains in classification tasks. Lastly, we reveal a scaling law for semi-supervised ICL, where models achieve optimal performance with over 1,000 demonstrations.</li>
</ul>

<h3>Title: WarmFed: Federated Learning with Warm-Start for Globalization and Personalization Via Personalized Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tao Feng, Jie Zhang, Xiangjian Li, Rong Huang, Huashan Liu, Zhijie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03110">https://arxiv.org/abs/2503.03110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03110">https://arxiv.org/pdf/2503.03110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03110]] WarmFed: Federated Learning with Warm-Start for Globalization and Personalization Via Personalized Diffusion Models(https://arxiv.org/abs/2503.03110)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) stands as a prominent distributed learning paradigm among multiple clients to achieve a unified global model without privacy leakage. In contrast to FL, Personalized federated learning aims at serving for each client in achieving persoanlized model. However, previous FL frameworks have grappled with a dilemma: the choice between developing a singular global model at the server to bolster globalization or nurturing personalized model at the client to accommodate personalization. Instead of making trade-offs, this paper commences its discourse from the pre-trained initialization, obtaining resilient global information and facilitating the development of both global and personalized models. Specifically, we propose a novel method called WarmFed to achieve this. WarmFed customizes Warm-start through personalized diffusion models, which are generated by local efficient fine-tunining (LoRA). Building upon the Warm-Start, we advance a server-side fine-tuning strategy to derive the global model, and propose a dynamic self-distillation (DSD) to procure more resilient personalized models simultaneously. Comprehensive experiments underscore the substantial gains of our approach across both global and personalized models, achieved within just one-shot and five communication(s).</li>
</ul>

<h3>Title: A Survey of Foundation Models for Environmental Science</h3>
<ul>
<li><strong>Authors: </strong>Runlong Yu, Shengyu Chen, Yiqun Xie, Xiaowei Jia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03142">https://arxiv.org/abs/2503.03142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03142">https://arxiv.org/pdf/2503.03142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03142]] A Survey of Foundation Models for Environmental Science(https://arxiv.org/abs/2503.03142)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Modeling environmental ecosystems is essential for effective resource management, sustainable development, and understanding complex ecological processes. However, traditional methods frequently struggle with the inherent complexity, interconnectedness, and limited data of such systems. Foundation models, with their large-scale pre-training and universal representations, offer transformative opportunities by integrating diverse data sources, capturing spatiotemporal dependencies, and adapting to a broad range of tasks. This survey presents a comprehensive overview of foundation model applications in environmental science, highlighting advancements in forward prediction, data generation, data assimilation, downscaling, model ensembling, and decision-making across domains. We also detail the development process of these models, covering data collection, architecture design, training, tuning, and evaluation. By showcasing these emerging methods, we aim to foster interdisciplinary collaboration and advance the integration of cutting-edge machine learning for sustainable solutions in environmental science.</li>
</ul>

<h3>Title: Position: Model Collapse Does Not Mean What You Think</h3>
<ul>
<li><strong>Authors: </strong>Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu, Sanmi Koyejo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03150">https://arxiv.org/abs/2503.03150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03150">https://arxiv.org/pdf/2503.03150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03150]] Position: Model Collapse Does Not Mean What You Think(https://arxiv.org/abs/2503.03150)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The proliferation of AI-generated content online has fueled concerns over \emph{model collapse}, a degradation in future generative models' performance when trained on synthetic data generated by earlier models. Industry leaders, premier research journals and popular science publications alike have prophesied catastrophic societal consequences stemming from model collapse. In this position piece, we contend this widespread narrative fundamentally misunderstands the scientific evidence. We highlight that research on model collapse actually encompasses eight distinct and at times conflicting definitions of model collapse, and argue that inconsistent terminology within and between papers has hindered building a comprehensive understanding of model collapse. To assess how significantly different interpretations of model collapse threaten future generative models, we posit what we believe are realistic conditions for studying model collapse and then conduct a rigorous assessment of the literature's methodologies through this lens. While we leave room for reasonable disagreement, our analysis of research studies, weighted by how faithfully each study matches real-world conditions, leads us to conclude that certain predicted claims of model collapse rely on assumptions and conditions that poorly match real-world conditions, and in fact several prominent collapse scenarios are readily avoidable. Altogether, this position paper argues that model collapse has been warped from a nuanced multifaceted consideration into an oversimplified threat, and that the evidence suggests specific harms more likely under society's current trajectory have received disproportionately less attention.</li>
</ul>

<h3>Title: Enhancing Cybersecurity in Critical Infrastructure with LLM-Assisted Explainable IoT Systems</h3>
<ul>
<li><strong>Authors: </strong>Ashutosh Ghimire, Ghazal Ghajari, Karma Gurung, Love K. Sah, Fathi Amsaad</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03180">https://arxiv.org/abs/2503.03180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03180">https://arxiv.org/pdf/2503.03180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03180]] Enhancing Cybersecurity in Critical Infrastructure with LLM-Assisted Explainable IoT Systems(https://arxiv.org/abs/2503.03180)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Ensuring the security of critical infrastructure has become increasingly vital with the proliferation of Internet of Things (IoT) systems. However, the heterogeneous nature of IoT data and the lack of human-comprehensible insights from anomaly detection models remain significant challenges. This paper presents a hybrid framework that combines numerical anomaly detection using Autoencoders with Large Language Models (LLMs) for enhanced preprocessing and interpretability. Two preprocessing approaches are implemented: a traditional method utilizing Principal Component Analysis (PCA) to reduce dimensionality and an LLM-assisted method where GPT-4 dynamically recommends feature selection, transformation, and encoding strategies. Experimental results on the KDDCup99 10% corrected dataset demonstrate that the LLM-assisted preprocessing pipeline significantly improves anomaly detection performance. The macro-average F1 score increased from 0.49 in the traditional PCA-based approach to 0.98 with LLM-driven insights. Additionally, the LLM generates natural language explanations for detected anomalies, providing contextual insights into their causes and implications. This framework highlights the synergy between numerical AI models and LLMs, delivering an accurate, interpretable, and efficient solution for IoT cybersecurity in critical infrastructure.</li>
</ul>

<h3>Title: An Analytical Theory of Power Law Spectral Bias in the Learning Dynamics of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Binxu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03206">https://arxiv.org/abs/2503.03206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03206">https://arxiv.org/pdf/2503.03206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03206]] An Analytical Theory of Power Law Spectral Bias in the Learning Dynamics of Diffusion Models(https://arxiv.org/abs/2503.03206)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We developed an analytical framework for understanding how the learned distribution evolves during diffusion model training. Leveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one- or two-layer linear denoiser settings with arbitrary data. Remarkably, these solutions allowed us to derive the generated distribution in closed form and its KL divergence through training. These analytical results expose a pronounced power-law spectral bias, i.e., for weights and distributions, the convergence time of a mode follows an inverse power law of its variance. Empirical experiments on both Gaussian and image datasets demonstrate that the power-law spectral bias remains robust even when using deeper or convolutional architectures. Our results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations for why earlier stopping could lead to incorrect details in image generative models.</li>
</ul>

<h3>Title: Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture</h3>
<ul>
<li><strong>Authors: </strong>Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03222">https://arxiv.org/abs/2503.03222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03222">https://arxiv.org/pdf/2503.03222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03222]] Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture(https://arxiv.org/abs/2503.03222)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recovering absolute poses in the world coordinate system from monocular views presents significant challenges. Two primary issues arise in this context. Firstly, existing methods rely on 3D motion data for training, which requires collection in limited environments. Acquiring such 3D labels for new actions in a timely manner is impractical, severely restricting the model's generalization capabilities. In contrast, 2D poses are far more accessible and easier to obtain. Secondly, estimating a person's absolute position in metric space from a single viewpoint is inherently more complex. To address these challenges, we introduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motions into 2D poses, leveraging 2D data to enhance 3D motion reconstruction in diverse scenarios and accurately predict absolute positions in the world coordinate system. We initially pretrain a single-view diffusion model with extensive 2D data, followed by fine-tuning a multi-view diffusion model for view consistency using publicly available 3D data. This strategy facilitates the effective use of large-scale 2D data. Additionally, we propose an innovative human motion representation that decouples local actions from global movements and encodes geometric priors of the ground, ensuring the generative model learns accurate motion priors from 2D data. During inference, this allows for the gradual recovery of global movements, resulting in more plausible positioning. We evaluate our model's performance on real-world datasets, demonstrating superior accuracy in motion and absolute human positioning compared to state-of-the-art methods, along with enhanced generalization and scalability. Our code will be made publicly available.</li>
</ul>

<h3>Title: Targeted Distillation for Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yice Zhang, Guangyu Xie, Jingjie Lin, Jianzhu Bao, Qianlong Wang, Xi Zeng, Ruifeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03225">https://arxiv.org/abs/2503.03225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03225">https://arxiv.org/pdf/2503.03225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03225]] Targeted Distillation for Sentiment Analysis(https://arxiv.org/abs/2503.03225)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This paper presents a compact model that achieves strong sentiment analysis capabilities through targeted distillation from advanced large language models (LLMs). Our methodology decouples the distillation target into two key components: sentiment-related knowledge and task alignment. To transfer these components, we propose a two-stage distillation framework. The first stage, knowledge-driven distillation (\textsc{KnowDist}), transfers sentiment-related knowledge to enhance fundamental sentiment analysis capabilities. The second stage, in-context learning distillation (\textsc{ICLDist}), transfers task-specific prompt-following abilities to optimize task alignment. For evaluation, we introduce \textsc{SentiBench}, a comprehensive sentiment analysis benchmark comprising 3 task categories across 12 datasets. Experiments on this benchmark demonstrate that our model effectively balances model size and performance, showing strong competitiveness compared to existing small-scale LLMs.</li>
</ul>

<h3>Title: Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs</h3>
<ul>
<li><strong>Authors: </strong>Runlin Lei, Jiarui Ji, Haipeng Ding, Lu Yi, Zhewei Wei, Yongchao Liu, Chuntao Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03258">https://arxiv.org/abs/2503.03258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03258">https://arxiv.org/pdf/2503.03258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03258]] Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs(https://arxiv.org/abs/2503.03258)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>With the rise of large language models (LLMs), there has been growing interest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging LLMs as predictors, GFMs have demonstrated impressive generalizability across various tasks and datasets. However, existing research on LLMs as predictors has predominantly focused on static graphs, leaving their potential in dynamic graph prediction unexplored. In this work, we pioneer using LLMs for predictive tasks on dynamic graphs. We identify two key challenges: the constraints imposed by context length when processing large-scale historical data and the significant variability in domain characteristics, both of which complicate the development of a unified predictor. To address these challenges, we propose the GraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages collaborative LLMs. In contrast to using a single LLM as the predictor, GAD incorporates global and local summary agents to generate domain-specific knowledge, enhancing its transferability across domains. Additionally, knowledge reflection agents enable adaptive updates to GAD's knowledge, maintaining a unified and self-consistent architecture. In experiments, GAD demonstrates performance comparable to or even exceeds that of full-supervised graph neural networks without dataset-specific training. Finally, to enhance the task-specific performance of LLM-based predictors, we discuss potential improvements, such as dataset-specific fine-tuning to LLMs. By developing tailored strategies for different tasks, we provide new insights for the future design of LLM-based predictors.</li>
</ul>

<h3>Title: Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions</h3>
<ul>
<li><strong>Authors: </strong>Yichong Zhao, Susumu Goto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03261">https://arxiv.org/abs/2503.03261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03261">https://arxiv.org/pdf/2503.03261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03261]] Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions(https://arxiv.org/abs/2503.03261)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can perform various natural language processing (NLP) tasks through in-context learning without relying on supervised data. However, multiple previous studies have reported suboptimal performance of LLMs in biological text mining. By analyzing failure patterns in these evaluations, we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs fail to learn implicit dataset-specific nuances from supervised data, (2) The common formatting requirements of discriminative tasks limit the reasoning capabilities of LLMs particularly for LLMs that lack test-time compute, and (3) LLMs struggle to adhere to annotation guidelines and match exact schemas, which hinders their ability to understand detailed annotation requirements which is essential in biomedical annotation workflow. To address these challenges, we experimented with prompt engineering techniques targeted to the above issues, and developed a pipeline that dynamically extracts instructions from annotation guidelines. Our findings show that frontier LLMs can approach or surpass the performance of state-of-the-art (SOTA) BERT-based models with minimal reliance on manually annotated data and without fine-tuning. Furthermore, we performed model distillation on a closed-source LLM, demonstrating that a BERT model trained exclusively on synthetic data annotated by LLMs can also achieve a practical performance. Based on these results, we explored the feasibility of partially replacing manual annotation with LLMs in production scenarios for biomedical text mining.</li>
</ul>

<h3>Title: Optimizing for the Shortest Path in Denoising Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Ping Chen, Xingpeng Zhang, Zhaoxiang Liu, Huan Hu, Xiang Liu, Kai Wang, Min Wang, Yanlin Qian, Shiguo Lian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03265">https://arxiv.org/abs/2503.03265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03265">https://arxiv.org/pdf/2503.03265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03265]] Optimizing for the Shortest Path in Denoising Diffusion Model(https://arxiv.org/abs/2503.03265)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this research, we propose a novel denoising diffusion model based on shortest-path modeling that optimizes residual propagation to enhance both denoising efficiency and this http URL on Denoising Diffusion Implicit Models (DDIM) and insights from graph theory, our model, termed the Shortest Path Diffusion Model (ShortDF), treats the denoising process as a shortest-path problem aimed at minimizing reconstruction error. By optimizing the initial residuals, we improve the efficiency of the reverse diffusion process and the quality of the generated this http URL experiments on multiple standard benchmarks demonstrate that ShortDF significantly reduces diffusion time (or steps) while enhancing the visual fidelity of generated samples compared to prior this http URL work, we suppose, paves the way for interactive diffusion-based applications and establishes a foundation for rapid data generation. Code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Jun Li, Che Liu, Wenjia Bai, Rossella Arcucci, Cosmin I. Bercea, Julia A. Schnabel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03278">https://arxiv.org/abs/2503.03278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03278">https://arxiv.org/pdf/2503.03278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03278]] Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions(https://arxiv.org/abs/2503.03278)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical this http URL evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.</li>
</ul>

<h3>Title: Label-Efficient LiDAR Semantic Segmentation with 2D-3D Vision Transformer Adapters</h3>
<ul>
<li><strong>Authors: </strong>Julia Hindel, Rohit Mohan, Jelena Bratulic, Daniele Cattaneo, Thomas Brox, Abhinav Valada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03299">https://arxiv.org/abs/2503.03299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03299">https://arxiv.org/pdf/2503.03299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03299]] Label-Efficient LiDAR Semantic Segmentation with 2D-3D Vision Transformer Adapters(https://arxiv.org/abs/2503.03299)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>LiDAR semantic segmentation models are typically trained from random initialization as universal pre-training is hindered by the lack of large, diverse datasets. Moreover, most point cloud segmentation architectures incorporate custom network layers, limiting the transferability of advances from vision-based architectures. Inspired by recent advances in universal foundation models, we propose BALViT, a novel approach that leverages frozen vision models as amodal feature encoders for learning strong LiDAR encoders. Specifically, BALViT incorporates both range-view and bird's-eye-view LiDAR encoding mechanisms, which we combine through a novel 2D-3D adapter. While the range-view features are processed through a frozen image backbone, our bird's-eye-view branch enhances them through multiple cross-attention interactions. Thereby, we continuously improve the vision network with domain-dependent knowledge, resulting in a strong label-efficient LiDAR encoding mechanism. Extensive evaluations of BALViT on the SemanticKITTI and nuScenes benchmarks demonstrate that it outperforms state-of-the-art methods on small data regimes. We make the code and models publicly available at: this http URL.</li>
</ul>

<h3>Title: LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xi Zhu, Haochen Xue, Ziwei Zhao, Wujiang Xu, Jingyuan Huang, Minghao Guo, Qifan Wang, Kaixiong Zhou, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03313">https://arxiv.org/abs/2503.03313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03313">https://arxiv.org/pdf/2503.03313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03313]] LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models(https://arxiv.org/abs/2503.03313)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Text-Attributed Graphs (TAGs), where each node is associated with text descriptions, are ubiquitous in real-world scenarios. They typically exhibit distinctive structure and domain-specific knowledge, motivating the development of a Graph Foundation Model (GFM) that generalizes across diverse graphs and tasks. Despite large efforts to integrate Large Language Models (LLMs) and Graph Neural Networks (GNNs) for TAGs, existing approaches suffer from decoupled architectures with two-stage alignment, limiting their synergistic potential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens to graph nodes, leading to graph-specific semantics, token explosion, and incompatibility with task-oriented prompt templates, which hinders cross-graph and cross-task transferability. To address these challenges, we propose PromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning. PromptGFM comprises two key components: (1) Graph Understanding Module, which explicitly prompts LLMs to replicate the finest GNN workflow within the text space, facilitating seamless GNN-LLM integration and elegant graph-text alignment; (2) Graph Inference Module, which establishes a language-based graph vocabulary ensuring expressiveness, transferability, and scalability, enabling readable instructions for LLM fine-tuning. Extensive experiments demonstrate our superiority and transferability across diverse graphs and tasks. The code is available at this: this https URL.</li>
</ul>

<h3>Title: Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and Anatomical Information</h3>
<ul>
<li><strong>Authors: </strong>Yiqiong Yang, Yitian Yuan, Baoxing Ren, Ye Wu, Yanqiu Feng, Xinyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03329">https://arxiv.org/abs/2503.03329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03329">https://arxiv.org/pdf/2503.03329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03329]] Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and Anatomical Information(https://arxiv.org/abs/2503.03329)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion MRI tractography technique enables non-invasive visualization of the white matter pathways in the brain. It plays a crucial role in neuroscience and clinical fields by facilitating the study of brain connectivity and neurological disorders. However, the accuracy of reconstructed tractograms has been a longstanding challenge. Recently, deep learning methods have been applied to improve tractograms for better white matter coverage, but often comes at the expense of generating excessive false-positive connections. This is largely due to their reliance on local information to predict long range streamlines. To improve the accuracy of streamline propagation predictions, we introduce a novel deep learning framework that integrates image-domain spatial information and anatomical information along tracts, with the former extracted through convolutional layers and the later modeled via a Transformer-decoder. Additionally, we employ a weighted loss function to address fiber class imbalance encountered during training. We evaluate the proposed method on the simulated ISMRM 2015 Tractography Challenge dataset, achieving a valid streamline rate of 66.2%, white matter coverage of 63.8%, and successfully reconstructing 24 out of 25 bundles. Furthermore, on the multi-site Tractoinferno dataset, the proposed method demonstrates its ability to handle various diffusion MRI acquisition schemes, achieving a 5.7% increase in white matter coverage and a 4.1% decrease in overreach compared to RNN-based methods.</li>
</ul>

<h3>Title: Video Super-Resolution: All You Need is a Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Zhan, Wang Pang, Xiang Zhu, Yechao Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03355">https://arxiv.org/abs/2503.03355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03355">https://arxiv.org/pdf/2503.03355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03355]] Video Super-Resolution: All You Need is a Video Diffusion Model(https://arxiv.org/abs/2503.03355)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a generic video super-resolution algorithm in this paper, based on the Diffusion Posterior Sampling framework with an unconditional video generation model in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Due to limited computational resources and training data, our experiments provide empirical evidence of the algorithm's strong super-resolution capabilities using synthetic data.</li>
</ul>

<h3>Title: Top-K Maximum Intensity Projection Priors for 3D Liver Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiaotong Zhang, Alexander Broersen, Gonnie CM van Erp, Silvia L. Pintea, Jouke Dijkstra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03367">https://arxiv.org/abs/2503.03367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03367">https://arxiv.org/pdf/2503.03367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03367]] Top-K Maximum Intensity Projection Priors for 3D Liver Vessel Segmentation(https://arxiv.org/abs/2503.03367)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Liver-vessel segmentation is an essential task in the pre-operative planning of liver resection. State-of-the-art 2D or 3D convolution-based methods focusing on liver vessel segmentation on 2D CT cross-sectional views, which do not take into account the global liver-vessel topology. To maintain this global vessel topology, we rely on the underlying physics used in the CT reconstruction process, and apply this to liver-vessel segmentation. Concretely, we introduce the concept of top-k maximum intensity projections, which mimics the CT reconstruction by replacing the integral along each projection direction, with keeping the top-k maxima along each projection direction. We use these top-k maximum projections to condition a diffusion model and generate 3D liver-vessel trees. We evaluate our 3D liver-vessel segmentation on the 3D-ircadb-01 dataset, and achieve the highest Dice coefficient, intersection-over-union (IoU), and Sensitivity scores compared to prior work.</li>
</ul>

<h3>Title: The Serendipity of Claude AI: Case of the 13 Low-Resource National Languages of Mali</h3>
<ul>
<li><strong>Authors: </strong>Alou Dembele, Nouhoum Souleymane Coulibaly, Michael Leventhal (RobotsMali AI4D Lab, Bamako, Mali)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03380">https://arxiv.org/abs/2503.03380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03380">https://arxiv.org/pdf/2503.03380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03380]] The Serendipity of Claude AI: Case of the 13 Low-Resource National Languages of Mali(https://arxiv.org/abs/2503.03380)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in artificial intelligence (AI) and natural language processing (NLP) have improved the representation of underrepresented languages. However, most languages, including Mali's 13 official national languages, continue to be poorly supported or unsupported by automatic translation and generative AI. This situation appears to have slightly improved with certain recent LLM releases. The study evaluated Claude AI's translation performance on each of the 13 national languages of Mali. In addition to ChrF2 and BLEU scores, human evaluators assessed translation accuracy, contextual consistency, robustness to dialect variations, management of linguistic bias, adaptation to a limited corpus, and ease of understanding. The study found that Claude AI performs robustly for languages with very modest language resources and, while unable to produce understandable and coherent texts for Malian languages with minimal resources, still manages to produce results which demonstrate the ability to mimic some elements of the language.</li>
</ul>

<h3>Title: AI-Driven Multi-Stage Computer Vision System for Defect Detection in Laser-Engraved Industrial Nameplates</h3>
<ul>
<li><strong>Authors: </strong>Adhish Anitha Vilasan, Stephan Jäger, Noah Klarmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03395">https://arxiv.org/abs/2503.03395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03395">https://arxiv.org/pdf/2503.03395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03395]] AI-Driven Multi-Stage Computer Vision System for Defect Detection in Laser-Engraved Industrial Nameplates(https://arxiv.org/abs/2503.03395)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Automated defect detection in industrial manufacturing is essential for maintaining product quality and minimizing production errors. In air disc brake manufacturing, ensuring the precision of laser-engraved nameplates is crucial for accurate product identification and quality control. Engraving errors, such as misprints or missing characters, can compromise both aesthetics and functionality, leading to material waste and production delays. This paper presents a proof of concept for an AI-driven computer vision system that inspects and verifies laser-engraved nameplates, detecting defects in logos and alphanumeric strings. The system integrates object detection using YOLOv7, optical character recognition (OCR) with Tesseract, and anomaly detection through a residual variational autoencoder (ResVAE) along with other computer vision methods to enable comprehensive inspections at multiple stages. Experimental results demonstrate the system's effectiveness, achieving 91.33% accuracy and 100% recall, ensuring that defective nameplates are consistently detected and addressed. This solution highlights the potential of AI-driven visual inspection to enhance quality control, reduce manual inspection efforts, and improve overall manufacturing efficiency.</li>
</ul>

<h3>Title: Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alessio Galatolo, Zhenbang Dai, Katie Winkle, Meriem Beloucif</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03460">https://arxiv.org/abs/2503.03460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03460">https://arxiv.org/pdf/2503.03460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03460]] Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models(https://arxiv.org/abs/2503.03460)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning LLMs with first-order methods like back-propagation is computationally intensive. Zeroth-Order (ZO) optimisation, using function evaluations instead of gradients, reduces memory usage but suffers from slow convergence in high-dimensional models. As a result, ZO research in LLMs has mostly focused on classification, overlooking more complex generative tasks. In this paper, we introduce ZOPrO, a novel ZO algorithm designed for \textit{Preference Optimisation} in LLMs. We begin by analysing the interplay between policy and reward models during traditional (first-order) Preference Optimisation, uncovering patterns in their relative updates. Guided by these insights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA) with a targeted sampling strategy to accelerate convergence. Through experiments on summarisation, machine translation, and conversational assistants, we demonstrate that our method consistently enhances reward signals while achieving convergence times comparable to first-order methods. While it falls short of some state-of-the-art methods, our work is the first to apply Zeroth-Order methods to Preference Optimisation in LLMs, going beyond classification tasks and paving the way for a largely unexplored research direction. Code and visualisations are available at this https URL</li>
</ul>

<h3>Title: TEDDY: A Family Of Foundation Models For Understanding Single Cell Biology</h3>
<ul>
<li><strong>Authors: </strong>Alexis Chevalier, Soumya Ghosh, Urvi Awasthi, James Watkins, Julia Bieniewska, Nichita Mitrea, Olga Kotova, Kirill Shkura, Andrew Noble, Michael Steinbaugh, Julien Delile, Christoph Meier, Leonid Zhukov, Iya Khalil, Srayanta Mukherjee, Judith Mueller</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03485">https://arxiv.org/abs/2503.03485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03485">https://arxiv.org/pdf/2503.03485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03485]] TEDDY: A Family Of Foundation Models For Understanding Single Cell Biology(https://arxiv.org/abs/2503.03485)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Understanding the biological mechanism of disease is critical for medicine, and in particular drug discovery. AI-powered analysis of genome-scale biological data hold great potential in this regard. The increasing availability of single-cell RNA sequencing data has enabled the development of large foundation models for disease biology. However, existing foundation models either do not improve or only modestly improve over task-specific models in downstream applications. Here, we explored two avenues for improving the state-of-the-art. First, we scaled the pre-training dataset to 116 million cells, which is larger than those used by previous models. Second, we leveraged the availability of large-scale biological annotations as a form of supervision during pre-training. We trained the TEDDY family of models comprising six transformer-based state-of-the-art single-cell foundation models with 70 million, 160 million, and 400 million parameters. We vetted our models on two downstream evaluation tasks -- identifying the underlying disease state of held-out donors not seen during training and distinguishing healthy cells from diseased ones for disease conditions and donors not seen during training. Scaling experiments showed that performance improved predictably with both data volume and parameter count. Our models showed substantial improvement over existing work on the first task and more muted improvements on the second.</li>
</ul>

<h3>Title: Rethinking Synthetic Data definitions: A privacy driven approach</h3>
<ul>
<li><strong>Authors: </strong>Vibeke Binz Vallevik, Serena Elizabeth Marshall, Aleksandar Babic, Jan Franz Nygaard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03506">https://arxiv.org/abs/2503.03506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03506">https://arxiv.org/pdf/2503.03506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03506]] Rethinking Synthetic Data definitions: A privacy driven approach(https://arxiv.org/abs/2503.03506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic data is gaining traction as a cost-effective solution for the increasing data demands of AI development and can be generated either from existing knowledge or derived data captured from real-world events. The source of the synthetic data generation and the technique used significantly impacts its residual privacy risk and therefore its opportunity for sharing. Traditional classification of synthetic data types no longer fit the newer generation techniques and there is a need to better align the classification with practical needs. We suggest a new way of grouping synthetic data types that better supports privacy evaluations to aid regulatory policymaking. Our novel classification provides flexibility to new advancements like deep generative methods and offers a more practical framework for future applications.</li>
</ul>

<h3>Title: A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Dragos Costea, Alina Marcu, Marius Leordeanu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03543">https://arxiv.org/abs/2503.03543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03543">https://arxiv.org/pdf/2503.03543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03543]] A self-supervised cyclic neural-analytic approach for novel view synthesis and 3D reconstruction(https://arxiv.org/abs/2503.03543)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Generating novel views from recorded videos is crucial for enabling autonomous UAV navigation. Recent advancements in neural rendering have facilitated the rapid development of methods capable of rendering new trajectories. However, these methods often fail to generalize well to regions far from the training data without an optimized flight path, leading to suboptimal reconstructions. We propose a self-supervised cyclic neural-analytic pipeline that combines high-quality neural rendering outputs with precise geometric insights from analytical methods. Our solution improves RGB and mesh reconstructions for novel view synthesis, especially in undersampled areas and regions that are completely different from the training dataset. We use an effective transformer-based architecture for image reconstruction to refine and adapt the synthesis process, enabling effective handling of novel, unseen poses without relying on extensive labeled datasets. Our findings demonstrate substantial improvements in rendering views of novel and also 3D reconstruction, which to the best of our knowledge is a first, setting a new standard for autonomous navigation in complex outdoor environments.</li>
</ul>

<h3>Title: Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03562">https://arxiv.org/abs/2503.03562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03562">https://arxiv.org/pdf/2503.03562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03562]] Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection(https://arxiv.org/abs/2503.03562)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are this http URL bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object this http URL benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our dataset and benchmark will be publicly available.</li>
</ul>

<h3>Title: Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias</h3>
<ul>
<li><strong>Authors: </strong>Rui Lu, Runzhe Wang, Kaifeng Lyu, Xitai Jiang, Gao Huang, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03595">https://arxiv.org/abs/2503.03595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03595">https://arxiv.org/pdf/2503.03595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03595]] Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias(https://arxiv.org/abs/2503.03595)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Score-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data. While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no meaning. This paper focuses on textual hallucinations, where diffusion models correctly generate individual symbols but assemble them in a nonsensical manner. Through experimental probing, we consistently observe that such phenomenon is attributed it to the network's local generation bias. Denoising networks tend to produce outputs that rely heavily on highly correlated local regions, particularly when different dimensions of the data distribution are nearly pairwise independent. This behavior leads to a generation process that decomposes the global distribution into separate, independent distributions for each symbol, ultimately failing to capture the global structure, including underlying grammar. Intriguingly, this bias persists across various denoising network architectures including MLP and transformers which have the structure to model global dependency. These findings also provide insights into understanding other types of hallucinations, extending beyond text, as a result of implicit biases in the denoising models. Additionally, we theoretically analyze the training dynamics for a specific case involving a two-layer MLP learning parity points on a hypercube, offering an explanation of its underlying mechanism.</li>
</ul>

<h3>Title: Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health Counseling</h3>
<ul>
<li><strong>Authors: </strong>Keqi Chen, Zekai Sun, Yuhua Wen, Huijun Lian, Yingming Gao, Ya Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03607">https://arxiv.org/abs/2503.03607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03607">https://arxiv.org/pdf/2503.03607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03607]] Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health Counseling(https://arxiv.org/abs/2503.03607)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The in-context learning capabilities of large language models (LLMs) show great potential in mental health support. However, the lack of counseling datasets, particularly in Chinese corpora, restricts their application in this field. To address this, we constructed Psy-Insight, the first mental health-oriented explainable multi-task bilingual dataset. We collected face-to-face multi-turn counseling dialogues, which are annotated with multi-task labels and conversation process explanations. Our annotations include psychotherapy, emotion, strategy, and topic labels, as well as turn-level reasoning and session-level guidance. Psy-Insight is not only suitable for tasks such as label recognition but also meets the need for training LLMs to act as empathetic counselors through logical reasoning. Experiments show that training LLMs on Psy-Insight enables the models to not only mimic the conversation style but also understand the underlying strategies and reasoning of counseling.</li>
</ul>

<h3>Title: DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhao, Weijia Mao, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03651">https://arxiv.org/abs/2503.03651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03651">https://arxiv.org/pdf/2503.03651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03651]] DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles(https://arxiv.org/abs/2503.03651)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at this https URL.</li>
</ul>

<h3>Title: Improving Neutral Point of View Text Generation through Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality Dataset</h3>
<ul>
<li><strong>Authors: </strong>Jessica Hoffmann, Christiane Ahlheim, Zac Yu, Aria Walfrand, Jarvis Jin, Marie Tano, Ahmad Beirami, Erin van Liemt, Nithum Thain, Hakim Sidahmed, Lucas Dixon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03654">https://arxiv.org/abs/2503.03654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03654">https://arxiv.org/pdf/2503.03654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03654]] Improving Neutral Point of View Text Generation through Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality Dataset(https://arxiv.org/abs/2503.03654)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper describes the construction of a dataset and the evaluation of training methods to improve generative large language models' (LLMs) ability to answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e., to provide significantly more informative, diverse and impartial answers. The dataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written quadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set of links to source texts elaborating the various points of view. The first key contribution of this paper is a new methodology to create such datasets through iterative rounds of human peer-critique and annotator training, which we release alongside the dataset. The second key contribution is the identification of a highly effective training regime for parameter-efficient reinforcement learning (PE-RL) to improve NPOV generation. We compare and extensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a strong baseline), SFT and RLHF. PE-RL not only improves on overall NPOV quality compared to the strongest baseline ($97.06\%\rightarrow 99.08\%$), but also scores much higher on features linguists identify as key to separating good answers from the best answers ($60.25\%\rightarrow 85.21\%$ for presence of supportive details, $68.74\%\rightarrow 91.43\%$ for absence of oversimplification). A qualitative analysis corroborates this. Finally, our evaluation finds no statistical differences between results on topics that appear in the training dataset and those on separated evaluation topics, which provides strong evidence that our approach to training PE-RL exhibits very effective out of topic generalization.</li>
</ul>

<h3>Title: A Generative Approach to High Fidelity 3D Reconstruction from Text Data</h3>
<ul>
<li><strong>Authors: </strong>Venkat Kumar R, Deepak Saravanan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03664">https://arxiv.org/abs/2503.03664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03664">https://arxiv.org/pdf/2503.03664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03664]] A Generative Approach to High Fidelity 3D Reconstruction from Text Data(https://arxiv.org/abs/2503.03664)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The convergence of generative artificial intelligence and advanced computer vision technologies introduces a groundbreaking approach to transforming textual descriptions into three-dimensional representations. This research proposes a fully automated pipeline that seamlessly integrates text-to-image generation, various image processing techniques, and deep learning methods for reflection removal and 3D reconstruction. By leveraging state-of-the-art generative models like Stable Diffusion, the methodology translates natural language inputs into detailed 3D models through a multi-stage workflow. The reconstruction process begins with the generation of high-quality images from textual prompts, followed by enhancement by a reinforcement learning agent and reflection removal using the Stable Delight model. Advanced image upscaling and background removal techniques are then applied to further enhance visual fidelity. These refined two-dimensional representations are subsequently transformed into volumetric 3D models using sophisticated machine learning algorithms, capturing intricate spatial relationships and geometric characteristics. This process achieves a highly structured and detailed output, ensuring that the final 3D models reflect both semantic accuracy and geometric precision. This approach addresses key challenges in generative reconstruction, such as maintaining semantic coherence, managing geometric complexity, and preserving detailed visual information. Comprehensive experimental evaluations will assess reconstruction quality, semantic accuracy, and geometric fidelity across diverse domains and varying levels of complexity. By demonstrating the potential of AI-driven 3D reconstruction techniques, this research offers significant implications for fields such as augmented reality (AR), virtual reality (VR), and digital content creation.</li>
</ul>

<h3>Title: Analogical Reasoning Inside Large Language Models: Concept Vectors and the Limits of Abstraction</h3>
<ul>
<li><strong>Authors: </strong>Gustaw Opiełka, Hannes Rosenbusch, Claire E. Stevenson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03666">https://arxiv.org/abs/2503.03666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03666">https://arxiv.org/pdf/2503.03666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03666]] Analogical Reasoning Inside Large Language Models: Concept Vectors and the Limits of Abstraction(https://arxiv.org/abs/2503.03666)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Analogical reasoning relies on conceptual abstractions, but it is unclear whether Large Language Models (LLMs) harbor such internal representations. We explore distilled representations from LLM activations and find that function vectors (FVs; Todd et al., 2024) - compact representations for in-context learning (ICL) tasks - are not invariant to simple input changes (e.g., open-ended vs. multiple-choice), suggesting they capture more than pure concepts. Using representational similarity analysis (RSA), we localize a small set of attention heads that encode invariant concept vectors (CVs) for verbal concepts like "antonym". These CVs function as feature detectors that operate independently of the final output - meaning that a model may form a correct internal representation yet still produce an incorrect output. Furthermore, CVs can be used to causally guide model behaviour. However, for more abstract concepts like "previous" and "next", we do not observe invariant linear representations, a finding we link to generalizability issues LLMs display within these domains.</li>
</ul>

<h3>Title: MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Rui Ye, Shuo Tang, Rui Ge, Yaxin Du, Zhenfei Yin, Siheng Chen, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03686">https://arxiv.org/abs/2503.03686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03686">https://arxiv.org/pdf/2503.03686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03686]] MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems(https://arxiv.org/abs/2503.03686)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs. In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS. To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high effectiveness, efficiency and strong generalization ability. Code will be available at this https URL.</li>
</ul>

<h3>Title: DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance</h3>
<ul>
<li><strong>Authors: </strong>Zhao Yang, Zezhong Qian, Xiaofan Li, Weixiang Xu, Gongpeng Zhao, Ruohong Yu, Lingsi Zhu, Longjun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03689">https://arxiv.org/abs/2503.03689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03689">https://arxiv.org/pdf/2503.03689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03689]] DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance(https://arxiv.org/abs/2503.03689)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate and high-fidelity driving scene reconstruction demands the effective utilization of comprehensive scene information as conditional inputs. Existing methods predominantly rely on 3D bounding boxes and BEV road maps for foreground and background control, which fail to capture the full complexity of driving scenes and adequately integrate multimodal information. In this work, we present DualDiff, a dual-branch conditional diffusion model designed to enhance driving scene generation across multiple views and video sequences. Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditional input, offering rich foreground and background semantics alongside 3D spatial geometry to precisely control the generation of both elements. To improve the synthesis of fine-grained foreground objects, particularly complex and distant ones, we propose a Foreground-Aware Mask (FGM) denoising loss function. Additionally, we develop the Semantic Fusion Attention (SFA) mechanism to dynamically prioritize relevant information and suppress noise, enabling more effective multimodal fusion. Finally, to ensure high-quality image-to-video generation, we introduce the Reward-Guided Diffusion (RGD) framework, which maintains global consistency and semantic coherence in generated videos. Extensive experiments demonstrate that DualDiff achieves state-of-the-art (SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiff reduces the FID score by 4.09% compared to the best baseline. In downstream tasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% and road mIoU by 1.70%, while in BEV 3D object detection, the foreground mAP increases by 1.46%. Code will be made available at this https URL.</li>
</ul>

<h3>Title: SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches</h3>
<ul>
<li><strong>Authors: </strong>Hiroyuki Deguchi, Go Kamoda, Yusuke Matsushita, Chihiro Taguchi, Kohei Suenaga, Masaki Waga, Sho Yokoi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03703">https://arxiv.org/abs/2503.03703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03703">https://arxiv.org/pdf/2503.03703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03703]] SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches(https://arxiv.org/abs/2503.03703)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Researchers and practitioners in natural language processing and computational linguistics frequently observe and analyze the real language usage in large-scale corpora. For that purpose, they often employ off-the-shelf pattern-matching tools, such as grep, and keyword-in-context concordancers, which is widely used in corpus linguistics for gathering examples. Nonetheless, these existing techniques rely on surface-level string matching, and thus they suffer from the major limitation of not being able to handle orthographic variations and paraphrasing -- notable and common phenomena in any natural language. In addition, existing continuous approaches such as dense vector search tend to be overly coarse, often retrieving texts that are unrelated but share similar topics. Given these challenges, we propose a novel algorithm that achieves \emph{soft} (or semantic) yet efficient pattern matching by relaxing a surface-level matching with word embeddings. Our algorithm is highly scalable with respect to the size of the corpus text utilizing inverted indexes. We have prepared an efficient implementation, and we provide an accessible web tool. Our experiments demonstrate that the proposed method (i) can execute searches on billion-scale corpora in less than a second, which is comparable in speed to surface-level string matching and dense vector search; (ii) can extract harmful instances that semantically match queries from a large set of English and Japanese Wikipedia articles; and (iii) can be effectively applied to corpus-linguistic analyses of Latin, a language with highly diverse inflections.</li>
</ul>

<h3>Title: Rethinking Video Tokenization: A Conditioned Diffusion-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03708">https://arxiv.org/abs/2503.03708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03708">https://arxiv.org/pdf/2503.03708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03708]] Rethinking Video Tokenization: A Conditioned Diffusion-based Approach(https://arxiv.org/abs/2503.03708)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Video tokenizers, which transform videos into compact latent representations, are key to video generation. Existing video tokenizers are based on the VAE architecture and follow a paradigm where an encoder compresses videos into compact latents, and a deterministic decoder reconstructs the original videos from these latents. In this paper, we propose a novel \underline{\textbf{C}}onditioned \underline{\textbf{D}}iffusion-based video \underline{\textbf{T}}okenizer entitled \textbf{\ourmethod}, which departs from previous methods by replacing the deterministic decoder with a 3D causal diffusion model. The reverse diffusion generative process of the decoder is conditioned on the latent representations derived via the encoder. With a feature caching and sampling acceleration, the framework efficiently reconstructs high-fidelity videos of arbitrary lengths. Results show that {\ourmethod} achieves state-of-the-art performance in video reconstruction tasks using just a single-step sampling. Even a smaller version of {\ourmethod} still achieves reconstruction results on par with the top two baselines. Furthermore, the latent video generation model trained using {\ourmethod} also shows superior performance.</li>
</ul>

<h3>Title: Handling Uncertainty in Health Data using Generative Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Arab Loodaricheh, Neh Majmudar, Anita Raja, Ansaf Salleb-Aouissi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03715">https://arxiv.org/abs/2503.03715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03715">https://arxiv.org/pdf/2503.03715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03715]] Handling Uncertainty in Health Data using Generative Algorithms(https://arxiv.org/abs/2503.03715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Understanding and managing uncertainty is crucial in machine learning, especially in high-stakes domains like healthcare, where class imbalance can impact predictions. This paper introduces RIGA, a novel pipeline that mitigates class imbalance using generative AI. By converting tabular healthcare data into images, RIGA leverages models like cGAN, VQVAE, and VQGAN to generate balanced samples, improving classification performance. These representations are processed by CNNs and later transformed back into tabular format for seamless integration. This approach enhances traditional classifiers like XGBoost, improves Bayesian structure learning, and strengthens ML model robustness by generating realistic synthetic data for underrepresented classes.</li>
</ul>

<h3>Title: Graph-Augmented LSTM for Forecasting Sparse Anomalies in Graph-Structured Time Series</h3>
<ul>
<li><strong>Authors: </strong>Sneh Pillai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03729">https://arxiv.org/abs/2503.03729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03729">https://arxiv.org/pdf/2503.03729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03729]] Graph-Augmented LSTM for Forecasting Sparse Anomalies in Graph-Structured Time Series(https://arxiv.org/abs/2503.03729)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in time series data is a critical task across many domains. The challenge intensifies when anomalies are sparse and the data are multivariate with relational dependencies across sensors or nodes. Traditional univariate anomaly detectors struggle to capture such cross-node dependencies, particularly in sparse anomaly settings. To address this, we propose a graph-augmented time series forecasting approach that explicitly integrates the graph of relationships among time series into an LSTM forecasting model. This enables the model to detect rare anomalies that might otherwise go unnoticed in purely univariate approaches. We evaluate the approach on two benchmark datasets - the Yahoo Webscope S5 anomaly dataset and the METR-LA traffic sensor network - and compare the performance of the Graph-Augmented LSTM against LSTM-only, ARIMA, and Prophet baselines. Results demonstrate that the graph-augmented model achieves significantly higher precision and recall, improving F1-score by up to 10% over the best baseline</li>
</ul>

<h3>Title: Rethinking Deep Clustering Paradigms: Self-Supervision Is All You Need</h3>
<ul>
<li><strong>Authors: </strong>Amal Shaheena, Nairouz Mrabahb, Riadh Ksantinia, Abdulla Alqaddoumia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03733">https://arxiv.org/abs/2503.03733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03733">https://arxiv.org/pdf/2503.03733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03733]] Rethinking Deep Clustering Paradigms: Self-Supervision Is All You Need(https://arxiv.org/abs/2503.03733)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The recent advances in deep clustering have been made possible by significant progress in self-supervised and pseudo-supervised learning. However, the trade-off between self-supervision and pseudo-supervision can give rise to three primary issues. The joint training causes Feature Randomness and Feature Drift, whereas the independent training causes Feature Randomness and Feature Twist. In essence, using pseudo-labels generates random and unreliable features. The combination of pseudo-supervision and self-supervision drifts the reliable clustering-oriented features. Moreover, moving from self-supervision to pseudo-supervision can twist the curved latent manifolds. This paper addresses the limitations of existing deep clustering paradigms concerning Feature Randomness, Feature Drift, and Feature Twist. We propose a new paradigm with a new strategy that replaces pseudo-supervision with a second round of self-supervision training. The new strategy makes the transition between instance-level self-supervision and neighborhood-level self-supervision smoother and less abrupt. Moreover, it prevents the drifting effect that is caused by the strong competition between instance-level self-supervision and clustering-level pseudo-supervision. Moreover, the absence of the pseudo-supervision prevents the risk of generating random features. With this novel approach, our paper introduces a Rethinking of the Deep Clustering Paradigms, denoted by R-DC. Our model is specifically designed to address three primary challenges encountered in Deep Clustering: Feature Randomness, Feature Drift, and Feature Twist. Experimental results conducted on six datasets have shown that the two-level self-supervision training yields substantial improvements.</li>
</ul>

<h3>Title: PacketCLIP: Multi-Modal Embedding of Network Traffic and Language for Cybersecurity Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ryozo Masukawa, Sanggeon Yun, Sungheon Jeong, Wenjun Huang, Yang Ni, Ian Bryant, Nathaniel D. Bastian, Mohsen Imani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03747">https://arxiv.org/abs/2503.03747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03747">https://arxiv.org/pdf/2503.03747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03747]] PacketCLIP: Multi-Modal Embedding of Network Traffic and Language for Cybersecurity Reasoning(https://arxiv.org/abs/2503.03747)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Traffic classification is vital for cybersecurity, yet encrypted traffic poses significant challenges. We present PacketCLIP, a multi-modal framework combining packet data with natural language semantics through contrastive pretraining and hierarchical Graph Neural Network (GNN) reasoning. PacketCLIP integrates semantic reasoning with efficient classification, enabling robust detection of anomalies in encrypted network flows. By aligning textual descriptions with packet behaviors, it offers enhanced interpretability, scalability, and practical applicability across diverse security scenarios. PacketCLIP achieves a 95% mean AUC, outperforms baselines by 11.6%, and reduces model size by 92%, making it ideal for real-time anomaly detection. By bridging advanced machine learning techniques and practical cybersecurity needs, PacketCLIP provides a foundation for scalable, efficient, and interpretable solutions to tackle encrypted traffic classification and network intrusion detection challenges in resource-constrained environments.</li>
</ul>

<h3>Title: GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control</h3>
<ul>
<li><strong>Authors: </strong>Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, Jun Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2503.03751">https://arxiv.org/abs/2503.03751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2503.03751">https://arxiv.org/pdf/2503.03751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2503.03751]] GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control(https://arxiv.org/abs/2503.03751)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
