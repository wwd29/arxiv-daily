<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-25</h1>
<h3>Title: ControlTraj: Controllable Trajectory Generation with  Topology-Constrained Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Qidong Liu, Yongchao Ye, Wei Chen, Zijian Zhang, Xuetao Wei, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15380">https://arxiv.org/abs/2404.15380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15380">https://arxiv.org/pdf/2404.15380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15380]] ControlTraj: Controllable Trajectory Generation with  Topology-Constrained Diffusion Model(https://arxiv.org/abs/2404.15380)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating trajectory data is among promising solutions to addressing privacy concerns, collection costs, and proprietary restrictions usually associated with human mobility analyses. However, existing trajectory generation methods are still in their infancy due to the inherent diversity and unpredictability of human activities, grappling with issues such as fidelity, flexibility, and generalizability. To overcome these obstacles, we propose ControlTraj, a Controllable Trajectory generation framework with the topology-constrained diffusion model. Distinct from prior approaches, ControlTraj utilizes a diffusion model to generate high-fidelity trajectories while integrating the structural constraints of road network topology to guide the geographical outcomes. Specifically, we develop a novel road segment autoencoder to extract fine-grained road segment embedding. The encoded features, along with trip attributes, are subsequently merged into the proposed geographic denoising UNet architecture, named GeoUNet, to synthesize geographic trajectories from white noise. Through experimentation across three real-world data settings, ControlTraj demonstrates its ability to produce human-directed, high-fidelity trajectory generation with adaptability to unexplored geographical contexts.</li>
</ul>

<h3>Title: Advances and Open Challenges in Federated Learning with Foundation  Models</h3>
<ul>
<li><strong>Authors: </strong>Chao Ren, Han Yu, Hongyi Peng, Xiaoli Tang, Anran Li, Yulan Gao, Alysa Ziying Tan, Bo Zhao, Xiaoxiao Li, Zengxiang Li, Qiang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15381">https://arxiv.org/abs/2404.15381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15381">https://arxiv.org/pdf/2404.15381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15381]] Advances and Open Challenges in Federated Learning with Foundation  Models(https://arxiv.org/abs/2404.15381)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The integration of Foundation Models (FMs) with Federated Learning (FL) presents a transformative paradigm in Artificial Intelligence (AI), offering enhanced capabilities while addressing concerns of privacy, data decentralization, and computational efficiency. This paper provides a comprehensive survey of the emerging field of Federated Foundation Models (FedFM), elucidating their synergistic relationship and exploring novel methodologies, challenges, and future directions that the FL research field needs to focus on in order to thrive in the age of foundation models. A systematic multi-tiered taxonomy is proposed, categorizing existing FedFM approaches for model training, aggregation, trustworthiness, and incentivization. Key challenges, including how to enable FL to deal with high complexity of computational demands, privacy considerations, contribution evaluation, and communication efficiency, are thoroughly discussed. Moreover, the paper explores the intricate challenges of communication, scalability and security inherent in training/fine-tuning FMs via FL, highlighting the potential of quantum computing to revolutionize the training, inference, optimization and data encryption processes. This survey underscores the importance of further research to propel innovation in FedFM, emphasizing the need for developing trustworthy solutions. It serves as a foundational guide for researchers and practitioners interested in contributing to this interdisciplinary and rapidly advancing field.</li>
</ul>

<h3>Title: XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>João Monteiro, Étienne Marcotte, Pierre-André Noël, Valentina Zantedeschi, David Vázquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15420">https://arxiv.org/abs/2404.15420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15420">https://arxiv.org/pdf/2404.15420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15420]] XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference(https://arxiv.org/abs/2404.15420)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) approaches typically leverage prompting to condition decoder-only language model generation on reference information. Just-in-time processing of a context is inefficient due to the quadratic cost of self-attention operations, and caching is desirable. However, caching transformer states can easily require almost as much space as the model parameters. When the right context isn't known in advance, caching ICL can be challenging. This work addresses these limitations by introducing models that, inspired by the encoder-decoder architecture, use cross-attention to condition generation on reference text without the prompt. More precisely, we leverage pre-trained decoder-only models and only train a small number of added layers. We use Question-Answering (QA) as a testbed to evaluate the ability of our models to perform conditional generation and observe that they outperform ICL, are comparable to fine-tuned prompted LLMs, and drastically reduce the space footprint relative to standard KV caching by two orders of magnitude.</li>
</ul>

<h3>Title: GLoD: Composing Global Contexts and Local Details in Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Moyuru Yamada</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15447">https://arxiv.org/abs/2404.15447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15447">https://arxiv.org/pdf/2404.15447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15447]] GLoD: Composing Global Contexts and Local Details in Image Generation(https://arxiv.org/abs/2404.15447)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated their capability to synthesize high-quality and diverse images from textual prompts. However, simultaneous control over both global contexts (e.g., object layouts and interactions) and local details (e.g., colors and emotions) still remains a significant challenge. The models often fail to understand complex descriptions involving multiple objects and reflect specified visual attributes to wrong targets or ignore them. This paper presents Global-Local Diffusion (\textit{GLoD}), a novel framework which allows simultaneous control over the global contexts and the local details in text-to-image generation without requiring training or fine-tuning. It assigns multiple global and local prompts to corresponding layers and composes their noises to guide a denoising process using pre-trained diffusion models. Our framework enables complex global-local compositions, conditioning objects in the global prompt with the local prompts while preserving other unspecified identities. Our quantitative and qualitative evaluations demonstrate that GLoD effectively generates complex images that adhere to both user-provided object interactions and object details.</li>
</ul>

<h3>Title: ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with  Reward Feedback Learning</h3>
<ul>
<li><strong>Authors: </strong>Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15449">https://arxiv.org/abs/2404.15449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15449">https://arxiv.org/pdf/2404.15449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15449]] ID-Aligner: Enhancing Identity-Preserving Text-to-Image Generation with  Reward Feedback Learning(https://arxiv.org/abs/2404.15449)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The rapid development of diffusion models has triggered diverse applications. Identity-preserving text-to-image generation (ID-T2I) particularly has received significant attention due to its wide range of application scenarios like AI portrait and advertising. While existing ID-T2I methods have demonstrated impressive results, several key challenges remain: (1) It is hard to maintain the identity characteristics of reference portraits accurately, (2) The generated images lack aesthetic appeal especially while enforcing identity retention, and (3) There is a limitation that cannot be compatible with LoRA-based and Adapter-based methods simultaneously. To address these issues, we present \textbf{ID-Aligner}, a general feedback learning framework to enhance ID-T2I performance. To resolve identity features lost, we introduce identity consistency reward fine-tuning to utilize the feedback from face detection and recognition models to improve generated identity preservation. Furthermore, we propose identity aesthetic reward fine-tuning leveraging rewards from human-annotated preference data and automatically constructed feedback on character structure generation to provide aesthetic tuning signals. Thanks to its universal feedback fine-tuning framework, our method can be readily applied to both LoRA and Adapter models, achieving consistent performance gains. Extensive experiments on SD1.5 and SDXL diffusion models validate the effectiveness of our approach. \textbf{Project Page: \url{https://idaligner.github.io/}}</li>
</ul>

<h3>Title: Metric3D v2: A Versatile Monocular Geometric Foundation Model for  Zero-shot Metric Depth and Surface Normal Estimation</h3>
<ul>
<li><strong>Authors: </strong>Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, Shaojie Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15506">https://arxiv.org/abs/2404.15506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15506">https://arxiv.org/pdf/2404.15506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15506]] Metric3D v2: A Versatile Monocular Geometric Foundation Model for  Zero-shot Metric Depth and Surface Normal Estimation(https://arxiv.org/abs/2404.15506)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce Metric3D v2, a geometric foundation model for zero-shot metric depth and surface normal estimation from a single image, which is crucial for metric 3D recovery. While depth and normal are geometrically related and highly complimentary, they present distinct challenges. SoTA monocular depth methods achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. Meanwhile, SoTA normal estimation methods have limited zero-shot performance due to the lack of large-scale labeled data. To tackle these issues, we propose solutions for both metric depth estimation and surface normal estimation. For metric depth estimation, we show that the key to a zero-shot single-view model lies in resolving the metric ambiguity from various camera models and large-scale data training. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problem and can be effortlessly plugged into existing monocular models. For surface normal estimation, we propose a joint depth-normal optimization module to distill diverse data knowledge from metric depth, enabling normal estimators to learn beyond normal labels. Equipped with these modules, our depth-normal models can be stably trained with over 16 million of images from thousands of camera models with different-type annotations, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. Our project page is at https://JUGGHM.github.io/Metric3Dv2.</li>
</ul>

<h3>Title: ToM-LM: Delegating Theory Of Mind Reasoning to External Symbolic  Executors in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weizhi Tang, Vaishak Belle</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15515">https://arxiv.org/abs/2404.15515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15515">https://arxiv.org/pdf/2404.15515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15515]] ToM-LM: Delegating Theory Of Mind Reasoning to External Symbolic  Executors in Large Language Models(https://arxiv.org/abs/2404.15515)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Theory of Mind (ToM) refers to the ability of individuals to attribute mental states to others. While Large Language Models (LLMs) have shown some promise with ToM ability, they still struggle with complex ToM reasoning. Our approach leverages an external symbolic executor, specifically the SMCDEL model checker, and fine-tuning to improve the ToM reasoning ability of LLMs. In our approach, an LLM is first fine-tuned through pairs of natural language and symbolic formulation representation of ToM problems and is then instructed to generate the symbolic formulation with a one-shot in-context example. The generated symbolic formulation is then executed by the SMCDEL model checker to perform transparent and verifiable ToM reasoning and give the final result. We demonstrate that our approach, ToM-LM, shows a significant improvement over all the constructed baselines. Our study proposes a novel view about externalizing a particular component of ToM reasoning, mainly reasoning about beliefs, and suggests generalizing it to other aspects of ToM reasoning.</li>
</ul>

<h3>Title: Can Foundational Large Language Models Assist with Conducting  Pharmaceuticals Manufacturing Investigations?</h3>
<ul>
<li><strong>Authors: </strong>Hossein Salami (1), Brandye Smith-Goettler (2), Vijay Yadav (2) ((1) Digital Services, MMD, Merck & Co., Inc., Rahway, NJ, USA, (2) Digital Services, MMD, Merck & Co., Inc., West Point, PA, USA)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15578">https://arxiv.org/abs/2404.15578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15578">https://arxiv.org/pdf/2404.15578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15578]] Can Foundational Large Language Models Assist with Conducting  Pharmaceuticals Manufacturing Investigations?(https://arxiv.org/abs/2404.15578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>General purpose Large Language Models (LLM) such as the Generative Pretrained Transformer (GPT) and Large Language Model Meta AI (LLaMA) have attracted much attention in recent years. There is strong evidence that these models can perform remarkably well in various natural language processing tasks. However, how to leverage them to approach domain-specific use cases and drive value remains an open question. In this work, we focus on a specific use case, pharmaceutical manufacturing investigations, and propose that leveraging historical records of manufacturing incidents and deviations in an organization can be beneficial for addressing and closing new cases, or de-risking new manufacturing campaigns. Using a small but diverse dataset of real manufacturing deviations selected from different product lines, we evaluate and quantify the power of three general purpose LLMs (GPT-3.5, GPT-4, and Claude-2) in performing tasks related to the above goal. In particular, (1) the ability of LLMs in automating the process of extracting specific information such as root cause of a case from unstructured data, as well as (2) the possibility of identifying similar or related deviations by performing semantic search on the database of historical records are examined. While our results point to the high accuracy of GPT-4 and Claude-2 in the information extraction task, we discuss cases of complex interplay between the apparent reasoning and hallucination behavior of LLMs as a risk factor. Furthermore, we show that semantic search on vector embedding of deviation descriptions can be used to identify similar records, such as those with a similar type of defect, with a high level of accuracy. We discuss further improvements to enhance the accuracy of similar record identification.</li>
</ul>

<h3>Title: MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image  Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Zhuang, Linshan Wu, Qiong Wang, Varut Vardhanabhuti, Lin Luo, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15580">https://arxiv.org/abs/2404.15580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15580">https://arxiv.org/pdf/2404.15580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15580]] MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image  Analysis(https://arxiv.org/abs/2404.15580)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>The Vision Transformer (ViT) has demonstrated remarkable performance in Self-Supervised Learning (SSL) for 3D medical image analysis. Mask AutoEncoder (MAE) for feature pre-training can further unleash the potential of ViT on various medical vision tasks. However, due to large spatial sizes with much higher dimensions of 3D medical images, the lack of hierarchical design for MAE may hinder the performance of downstream tasks. In this paper, we propose a novel \textit{Mask in Mask (MiM)} pre-training framework for 3D medical images, which aims to advance MAE by learning discriminative representation from hierarchical visual tokens across varying scales. We introduce multiple levels of granularity for masked inputs from the volume, which are then reconstructed simultaneously ranging at both fine and coarse levels. Additionally, a cross-level alignment mechanism is applied to adjacent level volumes to enforce anatomical similarity hierarchically. Furthermore, we adopt a hybrid backbone to enhance the hierarchical representation learning efficiently during the pre-training. MiM was pre-trained on a large scale of available 3D volumetric images, \textit{i.e.,} Computed Tomography (CT) images containing various body parts. Extensive experiments on thirteen public datasets demonstrate the superiority of MiM over other SSL methods in organ/lesion/tumor segmentation and disease classification. We further scale up the MiM to large pre-training datasets with more than 10k volumes, showing that large-scale pre-training can further enhance the performance of downstream tasks. The improvement also concluded that the research community should pay more attention to the scale of the pre-training dataset towards the healthcare foundation model for 3D medical images.</li>
</ul>

<h3>Title: Security Analysis of WiFi-based Sensing Systems: Threats from  Perturbation Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hangcheng Cao, Wenbin Huang, Guowen Xu, Xianhao Chen, Ziyang He, Jingyang Hu, Hongbo Jiang, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15587">https://arxiv.org/abs/2404.15587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15587">https://arxiv.org/pdf/2404.15587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15587]] Security Analysis of WiFi-based Sensing Systems: Threats from  Perturbation Attacks(https://arxiv.org/abs/2404.15587)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning technologies are pivotal in enhancing the performance of WiFi-based wireless sensing systems. However, they are inherently vulnerable to adversarial perturbation attacks, and regrettably, there is lacking serious attention to this security issue within the WiFi sensing community. In this paper, we elaborate such an attack, called WiIntruder, distinguishing itself with universality, robustness, and stealthiness, which serves as a catalyst to assess the security of existing WiFi-based sensing systems. This attack encompasses the following salient features: (1) Maximizing transferability by differentiating user-state-specific feature spaces across sensing models, leading to a universally effective perturbation attack applicable to common applications; (2) Addressing perturbation signal distortion caused by device synchronization and wireless propagation when critical parameters are optimized through a heuristic particle swarm-driven perturbation generation algorithm; and (3) Enhancing attack pattern diversity and stealthiness through random switching of perturbation surrogates generated by a generative adversarial network. Extensive experimental results confirm the practical threats of perturbation attacks to common WiFi-based services, including user authentication and respiratory monitoring.</li>
</ul>

<h3>Title: Optimizing OOD Detection in Molecular Graphs: A Novel Approach with  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, Xin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15625">https://arxiv.org/abs/2404.15625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15625">https://arxiv.org/pdf/2404.15625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15625]] Optimizing OOD Detection in Molecular Graphs: A Novel Approach with  Diffusion Models(https://arxiv.org/abs/2404.15625)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The open-world test dataset is often mixed with out-of-distribution (OOD) samples, where the deployed models will struggle to make accurate predictions. Traditional detection methods need to trade off OOD detection and in-distribution (ID) classification performance since they share the same representation learning model. In this work, we propose to detect OOD molecules by adopting an auxiliary diffusion model-based framework, which compares similarities between input molecules and reconstructed graphs. Due to the generative bias towards reconstructing ID training samples, the similarity scores of OOD molecules will be much lower to facilitate detection. Although it is conceptually simple, extending this vanilla framework to practical detection applications is still limited by two significant challenges. First, the popular similarity metrics based on Euclidian distance fail to consider the complex graph structure. Second, the generative model involving iterative denoising steps is time-consuming especially when it runs on the enormous pool of drugs. To address these challenges, our research pioneers an approach of Prototypical Graph Reconstruction for Molecular OOD Detection, dubbed as PGR-MOOD and hinges on three innovations: i) An effective metric to comprehensively quantify the matching degree of input and reconstructed molecules; ii) A creative graph generator to construct prototypical graphs that are in line with ID but away from OOD; iii) An efficient and scalable OOD detector to compare the similarity between test samples and pre-constructed prototypical graphs and omit the generative process on every new molecule. Extensive experiments on ten benchmark datasets and six baselines are conducted to demonstrate our superiority.</li>
</ul>

<h3>Title: Representing Part-Whole Hierarchies in Foundation Models by Learning  Localizability, Composability, and Decomposability from Anatomy via  Self-Supervision</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Reza Hosseinzadeh Taher, Michael B. Gotway, Jianming Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15672">https://arxiv.org/abs/2404.15672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15672">https://arxiv.org/pdf/2404.15672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15672]] Representing Part-Whole Hierarchies in Foundation Models by Learning  Localizability, Composability, and Decomposability from Anatomy via  Self-Supervision(https://arxiv.org/abs/2404.15672)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Humans effortlessly interpret images by parsing them into part-whole hierarchies; deep learning excels in learning multi-level feature spaces, but they often lack explicit coding of part-whole relations, a prominent property of medical imaging. To overcome this limitation, we introduce Adam-v2, a new self-supervised learning framework extending Adam [79] by explicitly incorporating part-whole hierarchies into its learning objectives through three key branches: (1) Localizability, acquiring discriminative representations to distinguish different anatomical patterns; (2) Composability, learning each anatomical structure in a parts-to-whole manner; and (3) Decomposability, comprehending each anatomical structure in a whole-to-parts manner. Experimental results across 10 tasks, compared to 11 baselines in zero-shot, few-shot transfer, and full fine-tuning settings, showcase Adam-v2's superior performance over large-scale medical models and existing SSL methods across diverse downstream tasks. The higher generality and robustness of Adam-v2's representations originate from its explicit construction of hierarchies for distinct anatomical structures from unlabeled medical images. Adam-v2 preserves a semantic balance of anatomical diversity and harmony in its embedding, yielding representations that are both generic and semantically meaningful, yet overlooked in existing SSL methods. All code and pretrained models are available at https://github.com/JLiangLab/Eden.</li>
</ul>

<h3>Title: CharacterFactory: Sampling Consistent Characters with GANs for Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Qinghe Wang, Baolu Li, Xiaomin Li, Bing Cao, Liqian Ma, Huchuan Lu, Xu Jia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15677">https://arxiv.org/abs/2404.15677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15677">https://arxiv.org/pdf/2404.15677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15677]] CharacterFactory: Sampling Consistent Characters with GANs for Diffusion  Models(https://arxiv.org/abs/2404.15677)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image models have opened new frontiers in human-centric generation. However, these models cannot be directly employed to generate images with consistent newly coined identities. In this work, we propose CharacterFactory, a framework that allows sampling new characters with consistent identities in the latent space of GANs for diffusion models. More specifically, we consider the word embeddings of celeb names as ground truths for the identity-consistent generation task and train a GAN model to learn the mapping from a latent space to the celeb embedding space. In addition, we design a context-consistent loss to ensure that the generated identity embeddings can produce identity-consistent images in various contexts. Remarkably, the whole model only takes 10 minutes for training, and can sample infinite characters end-to-end during inference. Extensive experiments demonstrate excellent performance of the proposed CharacterFactory on character creation in terms of identity consistency and editability. Furthermore, the generated characters can be seamlessly combined with the off-the-shelf image/video/3D diffusion models. We believe that the proposed CharacterFactory is an important step for identity-consistent character generation. Project page is available at: https://qinghew.github.io/CharacterFactory/.</li>
</ul>

<h3>Title: Automated Creation of Source Code Variants of a Cryptographic Hash  Function Implementation Using Generative Pre-Trained Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Elijah Pelofske, Vincent Urias, Lorie M. Liebrock</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15681">https://arxiv.org/abs/2404.15681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15681">https://arxiv.org/pdf/2404.15681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15681]] Automated Creation of Source Code Variants of a Cryptographic Hash  Function Implementation Using Generative Pre-Trained Transformer Models(https://arxiv.org/abs/2404.15681)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative pre-trained transformers (GPT's) are a type of large language machine learning model that are unusually adept at producing novel, and coherent, natural language. In this study the ability of GPT models to generate novel and correct versions, and notably very insecure versions, of implementations of the cryptographic hash function SHA-1 is examined. The GPT models Llama-2-70b-chat-h, Mistral-7B-Instruct-v0.1, and zephyr-7b-alpha are used. The GPT models are prompted to re-write each function using a modified version of the localGPT framework and langchain to provide word embedding context of the full source code and header files to the model, resulting in over 130,000 function re-write GPT output text blocks, approximately 40,000 of which were able to be parsed as C code and subsequently compiled. The generated code is analyzed for being compilable, correctness of the algorithm, memory leaks, compiler optimization stability, and character distance to the reference implementation. Remarkably, several generated function variants have a high implementation security risk of being correct for some test vectors, but incorrect for other test vectors. Additionally, many function implementations were not correct to the reference algorithm of SHA-1, but produced hashes that have some of the basic characteristics of hash functions. Many of the function re-writes contained serious flaws such as memory leaks, integer overflows, out of bounds accesses, use of uninitialised values, and compiler optimization instability. Compiler optimization settings and SHA-256 hash checksums of the compiled binaries are used to cluster implementations that are equivalent but may not have identical syntax - using this clustering over 100,000 novel and correct versions of the SHA-1 codebase were generated where each component C function of the reference implementation is different from the original code.</li>
</ul>

<h3>Title: AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models  for Brain MRI</h3>
<ul>
<li><strong>Authors: </strong>Yiming Che, Fazle Rafsani, Jay Shah, Md Mahfuzur Rahman Siddiquee, Teresa Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15683">https://arxiv.org/abs/2404.15683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15683">https://arxiv.org/pdf/2404.15683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15683]] AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models  for Brain MRI(https://arxiv.org/abs/2404.15683)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Weakly-supervised diffusion models (DM) in anomaly segmentation, leveraging image-level labels, have attracted significant attention for their superior performance compared to unsupervised methods. It eliminates the need for pixel-level labels in training, offering a more cost-effective alternative to supervised methods. However, existing methods are not fully weakly-supervised because they heavily rely on costly pixel-level labels for hyperparameter tuning in inference. To tackle this challenge, we introduce Anomaly Segmentation with Forward Process of Diffusion Models (AnoFPDM), a fully weakly-supervised framework that operates without the need for pixel-level labels. Leveraging the unguided forward process as a reference, we identify suitable hyperparameters, i.e., noise scale and threshold, for each input image. We aggregate anomaly maps from each step in the forward process, enhancing the signal strength of anomalous regions. Remarkably, our proposed method outperforms recent state-of-the-art weakly-supervised approaches, even without utilizing pixel-level labels.</li>
</ul>

<h3>Title: Deep Learning for Accelerated and Robust MRI Reconstruction: a Review</h3>
<ul>
<li><strong>Authors: </strong>Reinhard Heckel, Mathews Jacob, Akshay Chaudhari, Or Perlman, Efrat Shimron</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15692">https://arxiv.org/abs/2404.15692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15692">https://arxiv.org/pdf/2404.15692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15692]] Deep Learning for Accelerated and Robust MRI Reconstruction: a Review(https://arxiv.org/abs/2404.15692)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Deep learning (DL) has recently emerged as a pivotal technology for enhancing magnetic resonance imaging (MRI), a critical tool in diagnostic radiology. This review paper provides a comprehensive overview of recent advances in DL for MRI reconstruction. It focuses on DL approaches and architectures designed to improve image quality, accelerate scans, and address data-related challenges. These include end-to-end neural networks, pre-trained networks, generative models, and self-supervised methods. The paper also discusses the role of DL in optimizing acquisition protocols, enhancing robustness against distribution shifts, and tackling subtle bias. Drawing on the extensive literature and practical insights, it outlines current successes, limitations, and future directions for leveraging DL in MRI reconstruction, while emphasizing the potential of DL to significantly impact clinical imaging practices.</li>
</ul>

<h3>Title: DeepFeatureX Net: Deep Features eXtractors based Network for  discriminating synthetic from real images</h3>
<ul>
<li><strong>Authors: </strong>Orazio Pontorno (1), Luca Guarnera (1), Sebastiano Battiato (1) ((1) University of Catania)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15697">https://arxiv.org/abs/2404.15697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15697">https://arxiv.org/pdf/2404.15697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15697]] DeepFeatureX Net: Deep Features eXtractors based Network for  discriminating synthetic from real images(https://arxiv.org/abs/2404.15697)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deepfakes, synthetic images generated by deep learning algorithms, represent one of the biggest challenges in the field of Digital Forensics. The scientific community is working to develop approaches that can discriminate the origin of digital images (real or AI-generated). However, these methodologies face the challenge of generalization, that is, the ability to discern the nature of an image even if it is generated by an architecture not seen during training. This usually leads to a drop in performance. In this context, we propose a novel approach based on three blocks called Base Models, each of which is responsible for extracting the discriminative features of a specific image class (Diffusion Model-generated, GAN-generated, or real) as it is trained by exploiting deliberately unbalanced datasets. The features extracted from each block are then concatenated and processed to discriminate the origin of the input image. Experimental results showed that this approach not only demonstrates good robust capabilities to JPEG compression but also outperforms state-of-the-art methods in several generalization tests. Code, models and dataset are available at https://github.com/opontorno/block-based_deepfake-detection.</li>
</ul>

<h3>Title: What Makes Multimodal In-Context Learning Work?</h3>
<ul>
<li><strong>Authors: </strong>Folco Bertini Baldassini, Mustafa Shukor, Matthieu Cord, Laure Soulier, Benjamin Piwowarski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15736">https://arxiv.org/abs/2404.15736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15736">https://arxiv.org/pdf/2404.15736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15736]] What Makes Multimodal In-Context Learning Work?(https://arxiv.org/abs/2404.15736)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated remarkable performance across various tasks, exhibiting the capacity to swiftly acquire new skills, such as through In-Context Learning (ICL) with minimal demonstration examples. In this work, we present a comprehensive framework for investigating Multimodal ICL (M-ICL) in the context of Large Multimodal Models. We consider the best open-source multimodal models (e.g., IDEFICS, OpenFlamingo) and a wide range of multimodal tasks. Our study unveils several noteworthy findings: (1) M-ICL primarily relies on text-driven mechanisms, showing little to no influence from the image modality. (2) When used with advanced-ICL strategy (like RICES), M-ICL is not better than a simple strategy based on majority voting over context examples. Moreover, we identify several biases and limitations of M-ICL that warrant consideration prior to deployment. Code available at https://gitlab.com/folbaeni/multimodal-icl}{gitlab.com/folbaeni/multimodal-icl</li>
</ul>

<h3>Title: SRAGAN: Saliency Regularized and Attended Generative Adversarial Network  for Chinese Ink-wash Painting Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiang Gao, Yuqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15743">https://arxiv.org/abs/2404.15743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15743">https://arxiv.org/pdf/2404.15743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15743]] SRAGAN: Saliency Regularized and Attended Generative Adversarial Network  for Chinese Ink-wash Painting Generation(https://arxiv.org/abs/2404.15743)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper handles the problem of converting real pictures into traditional Chinese ink-wash paintings, i.e., Chinese ink-wash painting style transfer. Though this problem could be realized by a wide range of image-to-image translation models, a notable issue with all these methods is that the original image content details could be easily erased or corrupted due to transfer of ink-wash style elements. To solve or ameliorate this issue, we propose to incorporate saliency detection into the unpaired image-to-image translation framework to regularize content information of the generated paintings. The saliency map is utilized for content regularization from two aspects, both explicitly and implicitly: (\romannumeral1) we propose saliency IOU (SIOU) loss to explicitly regularize saliency consistency before and after stylization; (\romannumeral2) we propose saliency adaptive normalization (SANorm) which implicitly enhances content integrity of the generated paintings by injecting saliency information to the generator network to guide painting generation. Besides, we also propose saliency attended discriminator network which harnesses saliency mask to focus generative adversarial attention onto salient image regions, it contributes to producing finer ink-wash stylization effect for salient objects of images. Qualitative and quantitative experiments consistently demonstrate superiority of our model over related advanced methods for Chinese ink-wash painting style transfer.</li>
</ul>

<h3>Title: Unifying Bayesian Flow Networks and Diffusion Models through Stochastic  Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Kaiwen Xue, Yuhao Zhou, Shen Nie, Xu Min, Xiaolu Zhang, Jun Zhou, Chongxuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15766">https://arxiv.org/abs/2404.15766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15766">https://arxiv.org/pdf/2404.15766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15766]] Unifying Bayesian Flow Networks and Diffusion Models through Stochastic  Differential Equations(https://arxiv.org/abs/2404.15766)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Bayesian flow networks (BFNs) iteratively refine the parameters, instead of the samples in diffusion models (DMs), of distributions at various noise levels through Bayesian inference. Owing to its differentiable nature, BFNs are promising in modeling both continuous and discrete data, while simultaneously maintaining fast sampling capabilities. This paper aims to understand and enhance BFNs by connecting them with DMs through stochastic differential equations (SDEs). We identify the linear SDEs corresponding to the noise-addition processes in BFNs, demonstrate that BFN's regression losses are aligned with denoise score matching, and validate the sampler in BFN as a first-order solver for the respective reverse-time SDE. Based on these findings and existing recipes of fast sampling in DMs, we propose specialized solvers for BFNs that markedly surpass the original BFN sampler in terms of sample quality with a limited number of function evaluations (e.g., 10) on both image and text datasets. Notably, our best sampler achieves an increase in speed of 5~20 times for free. Our code is available at https://github.com/ML-GSAI/BFN-Solver.</li>
</ul>

<h3>Title: Toward Physics-Aware Deep Learning Architectures for LiDAR Intensity  Simulation</h3>
<ul>
<li><strong>Authors: </strong>Vivek Anand, Bharat Lohani, Gaurav Pandey, Rakesh Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15774">https://arxiv.org/abs/2404.15774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15774">https://arxiv.org/pdf/2404.15774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15774]] Toward Physics-Aware Deep Learning Architectures for LiDAR Intensity  Simulation(https://arxiv.org/abs/2404.15774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autonomous vehicles (AVs) heavily rely on LiDAR perception for environment understanding and navigation. LiDAR intensity provides valuable information about the reflected laser signals and plays a crucial role in enhancing the perception capabilities of AVs. However, accurately simulating LiDAR intensity remains a challenge due to the unavailability of material properties of the objects in the environment, and complex interactions between the laser beam and the environment. The proposed method aims to improve the accuracy of intensity simulation by incorporating physics-based modalities within the deep learning framework. One of the key entities that captures the interaction between the laser beam and the objects is the angle of incidence. In this work we demonstrate that the addition of the LiDAR incidence angle as a separate input to the deep neural networks significantly enhances the results. We present a comparative study between two prominent deep learning architectures: U-NET a Convolutional Neural Network (CNN), and Pix2Pix a Generative Adversarial Network (GAN). We implemented these two architectures for the intensity prediction task and used SemanticKITTI and VoxelScape datasets for experiments. The comparative analysis reveals that both architectures benefit from the incidence angle as an additional input. Moreover, the Pix2Pix architecture outperforms U-NET, especially when the incidence angle is incorporated.</li>
</ul>

<h3>Title: BASS: Batched Attention-optimized Speculative Sampling</h3>
<ul>
<li><strong>Authors: </strong>Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, Anoop Deoras</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15778">https://arxiv.org/abs/2404.15778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15778">https://arxiv.org/pdf/2404.15778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15778]] BASS: Batched Attention-optimized Speculative Sampling(https://arxiv.org/abs/2404.15778)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15X speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3X the highest of that of regular decoding and around 10X of single-sequence speculative decoding.</li>
</ul>

<h3>Title: MotionMaster: Training-free Camera Motion Transfer For Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, Lizhuang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15789">https://arxiv.org/abs/2404.15789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15789">https://arxiv.org/pdf/2404.15789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15789]] MotionMaster: Training-free Camera Motion Transfer For Video Generation(https://arxiv.org/abs/2404.15789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The emergence of diffusion models has greatly propelled the progress in image and video generation. Recently, some efforts have been made in controllable video generation, including text-to-video generation and video motion control, among which camera motion control is an important topic. However, existing camera motion control methods rely on training a temporal camera module, and necessitate substantial computation resources due to the large amount of parameters in video generation models. Moreover, existing methods pre-define camera motion types during training, which limits their flexibility in camera control. Therefore, to reduce training costs and achieve flexible camera control, we propose COMD, a novel training-free video motion transfer model, which disentangles camera motions and object motions in source videos and transfers the extracted camera motions to new videos. We first propose a one-shot camera motion disentanglement method to extract camera motion from a single source video, which separates the moving objects from the background and estimates the camera motion in the moving objects region based on the motion in the background by solving a Poisson equation. Furthermore, we propose a few-shot camera motion disentanglement method to extract the common camera motion from multiple videos with similar camera motions, which employs a window-based clustering technique to extract the common features in temporal attention maps of multiple videos. Finally, we propose a motion combination method to combine different types of camera motions together, enabling our model a more controllable and flexible camera control. Extensive experiments demonstrate that our training-free approach can effectively decouple camera-object motion and apply the decoupled camera motion to a wide range of controllable video generation tasks, achieving flexible and diverse camera motion control.</li>
</ul>

<h3>Title: Where to Mask: Structure-Guided Masking for Graph Masked Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Chuang Liu, Yuyao Wang, Yibing Zhan, Xueqi Ma, Dapeng Tao, Jia Wu, Wenbin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15806">https://arxiv.org/abs/2404.15806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15806">https://arxiv.org/pdf/2404.15806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15806]] Where to Mask: Structure-Guided Masking for Graph Masked Autoencoders(https://arxiv.org/abs/2404.15806)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph masked autoencoders (GMAE) have emerged as a significant advancement in self-supervised pre-training for graph-structured data. Previous GMAE models primarily utilize a straightforward random masking strategy for nodes or edges during training. However, this strategy fails to consider the varying significance of different nodes within the graph structure. In this paper, we investigate the potential of leveraging the graph's structural composition as a fundamental and unique prior in the masked pre-training process. To this end, we introduce a novel structure-guided masking strategy (i.e., StructMAE), designed to refine the existing GMAE models. StructMAE involves two steps: 1) Structure-based Scoring: Each node is evaluated and assigned a score reflecting its structural significance. Two distinct types of scoring manners are proposed: predefined and learnable scoring. 2) Structure-guided Masking: With the obtained assessment scores, we develop an easy-to-hard masking strategy that gradually increases the structural awareness of the self-supervised reconstruction task. Specifically, the strategy begins with random masking and progresses to masking structure-informative nodes based on the assessment scores. This design gradually and effectively guides the model in learning graph structural information. Furthermore, extensive experiments consistently demonstrate that our StructMAE method outperforms existing state-of-the-art GMAE models in both unsupervised and transfer learning tasks. Codes are available at https://github.com/LiuChuang0059/StructMAE.</li>
</ul>

<h3>Title: Fast Ensembling with Diffusion Schrödinger Bridge</h3>
<ul>
<li><strong>Authors: </strong>Hyunsu Kim, Jongmin Yoon, Juho Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15814">https://arxiv.org/abs/2404.15814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15814">https://arxiv.org/pdf/2404.15814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15814]] Fast Ensembling with Diffusion Schrödinger Bridge(https://arxiv.org/abs/2404.15814)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep Ensemble (DE) approach is a straightforward technique used to enhance the performance of deep neural networks by training them from different initial points, converging towards various local optima. However, a limitation of this methodology lies in its high computational overhead for inference, arising from the necessity to store numerous learned parameters and execute individual forward passes for each parameter during the inference stage. We propose a novel approach called Diffusion Bridge Network (DBN) to address this challenge. Based on the theory of the Schr\"odinger bridge, this method directly learns to simulate an Stochastic Differential Equation (SDE) that connects the output distribution of a single ensemble member to the output distribution of the ensembled model, allowing us to obtain ensemble prediction without having to invoke forward pass through all the ensemble models. By substituting the heavy ensembles with this lightweight neural network constructing DBN, we achieved inference with reduced computational cost while maintaining accuracy and uncertainty scores on benchmark datasets such as CIFAR-10, CIFAR-100, and TinyImageNet. Our implementation is available at https://github.com/kim-hyunsu/dbn.</li>
</ul>

<h3>Title: Sketch2Human: Deep Human Generation with Disentangled Geometry and  Appearance Control</h3>
<ul>
<li><strong>Authors: </strong>Linzi Qu, Jiaxiang Shang, Hui Ye, Xiaoguang Han, Hongbo Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15889">https://arxiv.org/abs/2404.15889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15889">https://arxiv.org/pdf/2404.15889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15889]] Sketch2Human: Deep Human Generation with Disentangled Geometry and  Appearance Control(https://arxiv.org/abs/2404.15889)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Geometry- and appearance-controlled full-body human image generation is an interesting but challenging task. Existing solutions are either unconditional or dependent on coarse conditions (e.g., pose, text), thus lacking explicit geometry and appearance control of body and garment. Sketching offers such editing ability and has been adopted in various sketch-based face generation and editing solutions. However, directly adapting sketch-based face generation to full-body generation often fails to produce high-fidelity and diverse results due to the high complexity and diversity in the pose, body shape, and garment shape and texture. Recent geometrically controllable diffusion-based methods mainly rely on prompts to generate appearance and it is hard to balance the realism and the faithfulness of their results to the sketch when the input is coarse. This work presents Sketch2Human, the first system for controllable full-body human image generation guided by a semantic sketch (for geometry control) and a reference image (for appearance control). Our solution is based on the latent space of StyleGAN-Human with inverted geometry and appearance latent codes as input. Specifically, we present a sketch encoder trained with a large synthetic dataset sampled from StyleGAN-Human's latent space and directly supervised by sketches rather than real images. Considering the entangled information of partial geometry and texture in StyleGAN-Human and the absence of disentangled datasets, we design a novel training scheme that creates geometry-preserved and appearance-transferred training data to tune a generator to achieve disentangled geometry and appearance control. Although our method is trained with synthetic data, it can handle hand-drawn sketches as well. Qualitative and quantitative evaluations demonstrate the superior performance of our method to state-of-the-art methods.</li>
</ul>

<h3>Title: OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Lizhi Wang, Feng Zhou, Jianqin Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15891">https://arxiv.org/abs/2404.15891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15891">https://arxiv.org/pdf/2404.15891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15891]] OMEGAS: Object Mesh Extraction from Large Scenes Guided by Gaussian  Segmentation(https://arxiv.org/abs/2404.15891)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in 3D reconstruction technologies have paved the way for high-quality and real-time rendering of complex 3D scenes. Despite these achievements, a notable challenge persists: it is difficult to precisely reconstruct specific objects from large scenes. Current scene reconstruction techniques frequently result in the loss of object detail textures and are unable to reconstruct object portions that are occluded or unseen in views. To address this challenge, we delve into the meticulous 3D reconstruction of specific objects within large scenes and propose a framework termed OMEGAS: Object Mesh Extraction from Large Scenes Guided by GAussian Segmentation. OMEGAS employs a multi-step approach, grounded in several excellent off-the-shelf methodologies. Specifically, initially, we utilize the Segment Anything Model (SAM) to guide the segmentation of 3D Gaussian Splatting (3DGS), thereby creating a basic 3DGS model of the target object. Then, we leverage large-scale diffusion priors to further refine the details of the 3DGS model, especially aimed at addressing invisible or occluded object portions from the original scene views. Subsequently, by re-rendering the 3DGS model onto the scene views, we achieve accurate object segmentation and effectively remove the background. Finally, these target-only images are used to improve the 3DGS model further and extract the definitive 3D object mesh by the SuGaR model. In various scenarios, our experiments demonstrate that OMEGAS significantly surpasses existing scene reconstruction methods. Our project page is at: https://github.com/CrystalWlz/OMEGAS</li>
</ul>

<h3>Title: Drawing the Line: Deep Segmentation for Extracting Art from Ancient  Etruscan Mirrors</h3>
<ul>
<li><strong>Authors: </strong>Rafael Sterzinger, Simon Brenner, Robert Sablatnig</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15903">https://arxiv.org/abs/2404.15903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15903">https://arxiv.org/pdf/2404.15903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15903]] Drawing the Line: Deep Segmentation for Extracting Art from Ancient  Etruscan Mirrors(https://arxiv.org/abs/2404.15903)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Etruscan mirrors constitute a significant category within Etruscan art and, therefore, undergo systematic examinations to obtain insights into ancient times. A crucial aspect of their analysis involves the labor-intensive task of manually tracing engravings from the backside. Additionally, this task is inherently challenging due to the damage these mirrors have sustained, introducing subjectivity into the process. We address these challenges by automating the process through photometric-stereo scanning in conjunction with deep segmentation networks which, however, requires effective usage of the limited data at hand. We accomplish this by incorporating predictions on a per-patch level, and various data augmentations, as well as exploring self-supervised learning. Compared to our baseline, we improve predictive performance w.r.t. the pseudo-F-Measure by around 16%. When assessing performance on complete mirrors against a human baseline, our approach yields quantitative similar performance to a human annotator and significantly outperforms existing binarization methods. With our proposed methodology, we streamline the annotation process, enhance its objectivity, and reduce overall workload, offering a valuable contribution to the examination of these historical artifacts and other non-traditional documents.</li>
</ul>

<h3>Title: Learning Long-form Video Prior via Generative Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Jinheng Xie, Jiajun Feng, Zhaoxu Tian, Kevin Qinghong Lin, Yawen Huang, Xi Xia, Nanxu Gong, Xu Zuo, Jiaqi Yang, Yefeng Zheng, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15909">https://arxiv.org/abs/2404.15909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15909">https://arxiv.org/pdf/2404.15909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15909]] Learning Long-form Video Prior via Generative Pre-Training(https://arxiv.org/abs/2404.15909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Concepts involved in long-form videos such as people, objects, and their interactions, can be viewed as following an implicit prior. They are notably complex and continue to pose challenges to be comprehensively learned. In recent years, generative pre-training (GPT) has exhibited versatile capacities in modeling any kind of text content even visual locations. Can this manner work for learning long-form video prior? Instead of operating on pixel space, it is efficient to employ visual locations like bounding boxes and keypoints to represent key information in videos, which can be simply discretized and then tokenized for consumption by GPT. Due to the scarcity of suitable data, we create a new dataset called \textbf{Storyboard20K} from movies to serve as a representative. It includes synopses, shot-by-shot keyframes, and fine-grained annotations of film sets and characters with consistent IDs, bounding boxes, and whole body keypoints. In this way, long-form videos can be represented by a set of tokens and be learned via generative pre-training. Experimental results validate that our approach has great potential for learning long-form video prior. Code and data will be released at \url{https://github.com/showlab/Long-form-Video-Prior}.</li>
</ul>

<h3>Title: Beyond Deepfake Images: Detecting AI-Generated Videos</h3>
<ul>
<li><strong>Authors: </strong>Danial Samadi Vahdati, Tai D. Nguyen, Aref Azizpour, Matthew C. Stamm</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15955">https://arxiv.org/abs/2404.15955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15955">https://arxiv.org/pdf/2404.15955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15955]] Beyond Deepfake Images: Detecting AI-Generated Videos(https://arxiv.org/abs/2404.15955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative AI have led to the development of techniques to generate visually realistic synthetic video. While a number of techniques have been developed to detect AI-generated synthetic images, in this paper we show that synthetic image detectors are unable to detect synthetic videos. We demonstrate that this is because synthetic video generators introduce substantially different traces than those left by image generators. Despite this, we show that synthetic video traces can be learned, and used to perform reliable synthetic video detection or generator source attribution even after H.264 re-compression. Furthermore, we demonstrate that while detecting videos from new generators through zero-shot transferability is challenging, accurate detection of videos from a new generator can be achieved through few-shot learning.</li>
</ul>

<h3>Title: HDDGAN: A Heterogeneous Dual-Discriminator Generative Adversarial  Network for Infrared and Visible Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Guosheng Lu, Zile Fang, Chunming He, Zhigang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.15992">https://arxiv.org/abs/2404.15992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.15992">https://arxiv.org/pdf/2404.15992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.15992]] HDDGAN: A Heterogeneous Dual-Discriminator Generative Adversarial  Network for Infrared and Visible Image Fusion(https://arxiv.org/abs/2404.15992)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Infrared and visible image fusion (IVIF) aims to preserve thermal radiation information from infrared images while integrating texture details from visible images, enabling the capture of important features and hidden details of subjects in complex scenes and disturbed environments. Consequently, IVIF offers distinct advantages in practical applications such as video surveillance, night navigation, and target recognition. However, prevailing methods often face challenges in simultaneously capturing thermal region features and detailed information due to the disparate characteristics of infrared and visible images. Consequently, fusion outcomes frequently entail a compromise between thermal target area information and texture details. In this study, we introduce a novel heterogeneous dual-discriminator generative adversarial network (HDDGAN) to address this issue. Specifically, the generator is structured as a multi-scale skip-connected structure, facilitating the extraction of essential features from different source images. To enhance the information representation ability of the fusion result, an attention mechanism is employed to construct the information fusion layer within the generator, leveraging the disparities between the source images. Moreover, recognizing the distinct learning requirements of information in infrared and visible images, we design two discriminators with differing structures. This approach aims to guide the model to learn salient information from infrared images while simultaneously capturing detailed information from visible images. Extensive experiments conducted on various public datasets demonstrate the superiority of our proposed HDDGAN over other state-of-the-art (SOTA) algorithms, highlighting its enhanced potential for practical applications.</li>
</ul>

<h3>Title: MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large  Vision-Language Models Towards Multitask AGI</h3>
<ul>
<li><strong>Authors: </strong>Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, Wenqi Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16006">https://arxiv.org/abs/2404.16006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16006">https://arxiv.org/pdf/2404.16006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16006]] MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large  Vision-Language Models Towards Multitask AGI(https://arxiv.org/abs/2404.16006)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.</li>
</ul>

<h3>Title: RetinaRegNet: A Versatile Approach for Retinal Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Vishal Balaji Sivaraman, Muhammad Imran, Qingyue Wei, Preethika Muralidharan, Michelle R. Tamplin, Isabella M . Grumbach, Randy H. Kardon, Jui-Kai Wang, Yuyin Zhou, Wei Shao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GT, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16017">https://arxiv.org/abs/2404.16017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16017">https://arxiv.org/pdf/2404.16017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16017]] RetinaRegNet: A Versatile Approach for Retinal Image Registration(https://arxiv.org/abs/2404.16017)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce the RetinaRegNet model, which can achieve state-of-the-art performance across various retinal image registration tasks. RetinaRegNet does not require training on any retinal images. It begins by establishing point correspondences between two retinal images using image features derived from diffusion models. This process involves the selection of feature points from the moving image using the SIFT algorithm alongside random point sampling. For each selected feature point, a 2D correlation map is computed by assessing the similarity between the feature vector at that point and the feature vectors of all pixels in the fixed image. The pixel with the highest similarity score in the correlation map corresponds to the feature point in the moving image. To remove outliers in the estimated point correspondences, we first applied an inverse consistency constraint, followed by a transformation-based outlier detector. This method proved to outperform the widely used random sample consensus (RANSAC) outlier detector by a significant margin. To handle large deformations, we utilized a two-stage image registration framework. A homography transformation was used in the first stage and a more accurate third-order polynomial transformation was used in the second stage. The model's effectiveness was demonstrated across three retinal image datasets: color fundus images, fluorescein angiography images, and laser speckle flowgraphy images. RetinaRegNet outperformed current state-of-the-art methods in all three datasets. It was especially effective for registering image pairs with large displacement and scaling deformations. This innovation holds promise for various applications in retinal image analysis. Our code is publicly available at https://github.com/mirthAI/RetinaRegNet.</li>
</ul>

<h3>Title: PuLID: Pure and Lightning ID Customization via Contrastive Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Qian He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16022">https://arxiv.org/abs/2404.16022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16022">https://arxiv.org/pdf/2404.16022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16022]] PuLID: Pure and Lightning ID Customization via Contrastive Alignment(https://arxiv.org/abs/2404.16022)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose Pure and Lightning ID customization (PuLID), a novel tuning-free ID customization method for text-to-image generation. By incorporating a Lightning T2I branch with a standard diffusion one, PuLID introduces both contrastive alignment loss and accurate ID loss, minimizing disruption to the original model and ensuring high ID fidelity. Experiments show that PuLID achieves superior performance in both ID fidelity and editability. Another attractive property of PuLID is that the image elements (e.g., background, lighting, composition, and style) before and after the ID insertion are kept as consistent as possible. Codes and models will be available at https://github.com/ToTheBeginning/PuLID</li>
</ul>

<h3>Title: Editable Image Elements for Controllable Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jiteng Mu, Michaël Gharbi, Richard Zhang, Eli Shechtman, Nuno Vasconcelos, Xiaolong Wang, Taesung Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.16029">https://arxiv.org/abs/2404.16029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.16029">https://arxiv.org/pdf/2404.16029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.16029]] Editable Image Elements for Controllable Synthesis(https://arxiv.org/abs/2404.16029)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have made significant advances in text-guided synthesis tasks. However, editing user-provided images remains challenging, as the high dimensional noise input space of diffusion models is not naturally suited for image inversion or spatial editing. In this work, we propose an image representation that promotes spatial editing of input images using a diffusion model. Concretely, we learn to encode an input into "image elements" that can faithfully reconstruct an input image. These elements can be intuitively edited by a user, and are decoded by a diffusion model into realistic images. We show the effectiveness of our representation on various image editing tasks, such as object resizing, rearrangement, dragging, de-occlusion, removal, variation, and image composition. Project page: https://jitengmu.github.io/Editable_Image_Elements/</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
