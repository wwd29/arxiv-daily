<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-20</h1>
<h3>Title: Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Meriem Zerkouk, Miloud Mihoubi, Belkacem Chikhaoui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13196">https://arxiv.org/abs/2508.13196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13196">https://arxiv.org/pdf/2508.13196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13196]] Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis(https://arxiv.org/abs/2508.13196)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach for multimodal sentiment analysis on social media, particularly in the context of natural disasters, where understanding public sentiment is crucial for effective crisis management. Unlike conventional methods that process text and image modalities separately, our approach seamlessly integrates Convolutional Neural Network (CNN) based image analysis with Large Language Model (LLM) based text processing, leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to extract sentiment relevant features from the CrisisMMD dataset. To effectively model intermodal relationships, we introduce a contextual attention mechanism within the fusion process. Leveraging contextual-attention layers, this mechanism effectively captures intermodality interactions, enhancing the model's comprehension of complex relationships between textual and visual data. The deep neural network architecture of our model learns from these fused features, leading to improved accuracy compared to existing baselines. Experimental results demonstrate significant advancements in classifying social media data into informative and noninformative categories across various natural disasters. Our model achieves a notable 2.43% increase in accuracy and 5.18% in F1-score, highlighting its efficacy in processing complex multimodal data. Beyond quantitative metrics, our approach provides deeper insight into the sentiments expressed during crises. The practical implications extend to real time disaster management, where enhanced sentiment analysis can optimize the accuracy of emergency interventions. By bridging the gap between multimodal analysis, LLM powered text understanding, and disaster response, our work presents a promising direction for Artificial Intelligence (AI) driven crisis management solutions. Keywords:</li>
</ul>

<h3>Title: MIRAGE: Towards AI-Generated Image Detection in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Cheng Xia, Manxi Lin, Jiexiang Tan, Xiaoxiong Du, Yang Qiu, Junjun Zheng, Xiangheng Kong, Yuning Jiang, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13223">https://arxiv.org/abs/2508.13223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13223">https://arxiv.org/pdf/2508.13223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13223]] MIRAGE: Towards AI-Generated Image Detection in the Wild(https://arxiv.org/abs/2508.13223)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The spreading of AI-generated images (AIGI), driven by advances in generative AI, poses a significant threat to information security and public trust. Existing AIGI detectors, while effective against images in clean laboratory settings, fail to generalize to in-the-wild scenarios. These real-world images are noisy, varying from ``obviously fake" images to realistic ones derived from multiple generative models and further edited for quality control. We address in-the-wild AIGI detection in this paper. We introduce Mirage, a challenging benchmark designed to emulate the complexity of in-the-wild AIGI. Mirage is constructed from two sources: (1) a large corpus of Internet-sourced AIGI verified by human experts, and (2) a synthesized dataset created through the collaboration between multiple expert generators, closely simulating the realistic AIGI in the wild. Building on this benchmark, we propose Mirage-R1, a vision-language model with heuristic-to-analytic reasoning, a reflective reasoning mechanism for AIGI detection. Mirage-R1 is trained in two stages: a supervised-fine-tuning cold start, followed by a reinforcement learning stage. By further adopting an inference-time adaptive thinking strategy, Mirage-R1 is able to provide either a quick judgment or a more robust and accurate conclusion, effectively balancing inference speed and performance. Extensive experiments show that our model leads state-of-the-art detectors by 5% and 10% on Mirage and the public benchmark, respectively. The benchmark and code will be made publicly available.</li>
</ul>

<h3>Title: RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Suhang Hu, Wei Hu, Yuhang Su, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13229">https://arxiv.org/abs/2508.13229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13229">https://arxiv.org/pdf/2508.13229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13229]] RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning(https://arxiv.org/abs/2508.13229)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) struggle with complex image annotation tasks, such as emotion classification and context-driven object detection, which demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses solely on annotation outcomes, ignoring underlying rationales, while Visual Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought (CoTs) due to the absence of high-quality, verified CoTs during pre-training. We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement learning-driven "annotation-reasoning-annotation" closed-loop generates visually grounded, logically consistent CoTs by verifying their ability to reconstruct original annotations without direct leakage. The Inspire and Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement fine-tuning to produce interpretable reasoning and accurate annotations, achieving Expertise in complex visual tasks. Evaluated on complex and simple image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and Visual-RFT, achieving robust performance and enhanced explainability. RISE offers a self-supervised solution for advancing VLM reasoning without requiring manually annotated CoTs.</li>
</ul>

<h3>Title: DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Qian Chen, Xianyin Zhang, Lifan Guo, Feng Chen, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13238">https://arxiv.org/abs/2508.13238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13238">https://arxiv.org/pdf/2508.13238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13238]] DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool Interleaved Vision-Language Model(https://arxiv.org/abs/2508.13238)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large vision-language models (LVLMs) have enabled a new paradigm of end-to-end document image parsing, excelling in Optical Character Recognition (OCR) tasks such as text, table, and formula recognition. However, generative LVLMs, similarly to large language models (LLMs), are prone to hallucinations--generating words that do not exist in input images. Furthermore, LVLMs are designed for general purposes and tend to be less effective on OCR tasks compared to expert models that are trained on domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a reasoning-enhanced framework designed to address these limitations through training reasoning-and-tool interleaved VLMs. Given a recognition instruction, our DianJin-OCR-R1 model first recognizes the content in the input image by its own OCR capabilities, and then calls other tools (i.e., other expert models) to obtain their results as references, finally looks again the image and rethinks about the reasoning process to provide the final recognized content. Since architectures of expert models are tailored for specific OCR tasks, which makes them less prone to hallucinations, their results can help VLMs mitigate hallucinations. Additionally, expert models are typically smaller in scale and easy to iterate, enabling performance improvements for VLMs at a lower cost. We evaluate our model on ReST and OmniDocBench, and experimental results show that our DianJin-OCR-R1 models consistently outperform their non-reasoning counterparts and expert OCR models, which proves the effectiveness of our method.</li>
</ul>

<h3>Title: CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification</h3>
<ul>
<li><strong>Authors: </strong>Zeynep Ozdemir, Hacer Yalim Keles, Omer Ozgur Tanriover</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13280">https://arxiv.org/abs/2508.13280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13280">https://arxiv.org/pdf/2508.13280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13280]] CLoE: Curriculum Learning on Endoscopic Images for Robust MES Classification(https://arxiv.org/abs/2508.13280)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Estimating disease severity from endoscopic images is essential in assessing ulcerative colitis, where the Mayo Endoscopic Subscore (MES) is widely used to grade inflammation. However, MES classification remains challenging due to label noise from inter-observer variability and the ordinal nature of the score, which standard models often ignore. We propose CLoE, a curriculum learning framework that accounts for both label reliability and ordinal structure. Image quality, estimated via a lightweight model trained on Boston Bowel Preparation Scale (BBPS) labels, is used as a proxy for annotation confidence to order samples from easy (clean) to hard (noisy). This curriculum is further combined with ResizeMix augmentation to improve robustness. Experiments on the LIMUC and HyperKvasir datasets, using both CNNs and Transformers, show that CLoE consistently improves performance over strong supervised and self-supervised baselines. For instance, ConvNeXt-Tiny reaches 82.5\% accuracy and a QWK of 0.894 on LIMUC with low computational cost. These results highlight the potential of difficulty-aware training strategies for improving ordinal classification under label uncertainty. Code will be released at this https URL.</li>
</ul>

<h3>Title: GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Sirshapan Mitra, Yogesh S. Rawat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13300">https://arxiv.org/abs/2508.13300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13300">https://arxiv.org/pdf/2508.13300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13300]] GaitCrafter: Diffusion Model for Biometric Preserving Gait Synthesis(https://arxiv.org/abs/2508.13300)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Gait recognition is a valuable biometric task that enables the identification of individuals from a distance based on their walking patterns. However, it remains limited by the lack of large-scale labeled datasets and the difficulty of collecting diverse gait samples for each individual while preserving privacy. To address these challenges, we propose GaitCrafter, a diffusion-based framework for synthesizing realistic gait sequences in the silhouette domain. Unlike prior works that rely on simulated environments or alternative generative models, GaitCrafter trains a video diffusion model from scratch, exclusively on gait silhouette data. Our approach enables the generation of temporally consistent and identity-preserving gait sequences. Moreover, the generation process is controllable-allowing conditioning on various covariates such as clothing, carried objects, and view angle. We show that incorporating synthetic samples generated by GaitCrafter into the gait recognition pipeline leads to improved performance, especially under challenging conditions. Additionally, we introduce a mechanism to generate novel identities-synthetic individuals not present in the original dataset-by interpolating identity embeddings. These novel identities exhibit unique, consistent gait patterns and are useful for training models while maintaining privacy of real subjects. Overall, our work takes an important step toward leveraging diffusion models for high-quality, controllable, and privacy-aware gait data generation.</li>
</ul>

<h3>Title: Counterfactual Probabilistic Diffusion with Expert Models</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Mu, Zhi Cao, Mehmed Uludag, Alexander Rodr√≠guez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13355">https://arxiv.org/abs/2508.13355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13355">https://arxiv.org/pdf/2508.13355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13355]] Counterfactual Probabilistic Diffusion with Expert Models(https://arxiv.org/abs/2508.13355)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Predicting counterfactual distributions in complex dynamical systems is essential for scientific modeling and decision-making in domains such as public health and medicine. However, existing methods often rely on point estimates or purely data-driven models, which tend to falter under data scarcity. We propose a time series diffusion-based framework that incorporates guidance from imperfect expert models by extracting high-level signals to serve as structured priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and data-driven approaches, enabling more reliable and interpretable causal inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations, synthetic pharmacological dynamics, and real-world case studies, demonstrating that it consistently outperforms strong baselines in both point prediction and distributional accuracy.</li>
</ul>

<h3>Title: Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp</h3>
<ul>
<li><strong>Authors: </strong>Nooshin Bahador, Milad Lankarany</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13406">https://arxiv.org/abs/2508.13406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13406">https://arxiv.org/pdf/2508.13406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13406]] Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp(https://arxiv.org/abs/2508.13406)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This study presents a quantitative framework for evaluating the spatial concordance between clinically defined seizure onset zones (SOZs) and statistically anomalous channels identified through time-frequency analysis of chirp events. The proposed pipeline employs a two-step methodology: (1) Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with adaptive neighborhood selection identifies anomalous channels based on spectro-temporal features of chirp (Onset frequency, offset frequency, and temporal duration); and (2) Spatial Correlation Analysis, which computes both exact co-occurrence metrics and weighted index similarity, incorporating hemispheric congruence and electrode proximity. Key findings demonstrate that the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects outliers, with index matching (weighted by channel proximity) outperforming exact matching in SOZ localization. Performance metrics (precision, recall, F1) were highest for seizure-free patients (Index Precision mean: 0.903) and those with successful surgical outcomes (Index Precision mean: 0.865), whereas failure cases exhibited lower concordance (Index Precision mean: 0.460). The key takeaway is that chirp-based outlier detection, combined with weighted spatial metrics, provides a complementary method for SOZ localization, particularly in patients with successful surgical outcomes.</li>
</ul>

<h3>Title: NovoMolGen: Rethinking Molecular Language Model Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Kamran Chitsaz, Roshan Balaji, Quentin Fournier, Nirav Pravinbhai Bhatt, Sarath Chandar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13408">https://arxiv.org/abs/2508.13408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13408">https://arxiv.org/pdf/2508.13408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13408]] NovoMolGen: Rethinking Molecular Language Model Pretraining(https://arxiv.org/abs/2508.13408)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Designing de-novo molecules with desired property profiles requires efficient exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$ possible synthesizable candidates. While various deep generative models have been developed to design small molecules using diverse input representations, Molecular Large Language Models (Mol-LLMs) based on string representations have emerged as a scalable approach capable of exploring billions of molecules. However, there remains limited understanding regarding how standard language modeling practices such as textual representations, tokenization strategies, model size, and dataset scale impact molecular generation performance. In this work, we systematically investigate these critical aspects by introducing NovoMolGen, a family of transformer-based foundation models pretrained on 1.5 billion molecules for de-novo molecule generation. Through extensive empirical analyses, we identify a weak correlation between performance metrics measured during pretraining and actual downstream performance, revealing important distinctions between molecular and general NLP training dynamics. NovoMolGen establishes new state-of-the-art results, substantially outperforming prior Mol-LLMs and specialized generative models in both unconstrained and goal-directed molecular generation tasks, thus providing a robust foundation for advancing efficient and effective molecular modeling strategies.</li>
</ul>

<h3>Title: EventTSF: Event-Aware Non-Stationary Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yunfeng Ge, Ming Jin, Yiji Zhao, Hongyan Li, Bo Du, Chang Xu, Shirui Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13434">https://arxiv.org/abs/2508.13434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13434">https://arxiv.org/pdf/2508.13434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13434]] EventTSF: Event-Aware Non-Stationary Time Series Forecasting(https://arxiv.org/abs/2508.13434)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Time series forecasting plays a vital role in critical domains like energy and transportation, where non-stationary dynamics are deeply intertwined with events in other modalities such as texts. However, incorporating natural language-based external events to improve non-stationary forecasting remains largely unexplored, as most approaches still rely on a single modality, resulting in limited contextual knowledge and model underperformance. Enabling fine-grained multimodal interactions between temporal and textual data is challenged by three fundamental issues: (1) the difficulty of fine-grained synchronization between time-varying discrete textual events and continuous time series; (2) the inherent temporal uncertainty introduced by textual semantics; and (3) the misalignment between textual event embeddings and multi-resolution temporal patterns. In this work, we address these challenges by introducing event-aware non-stationary time series forecasting (EventTSF), an autoregressive generation framework that integrates historical time series with textual events to make subsequent forecasts. Specifically, EventTSF uses autoregressive diffusion with flow matching at each step to capture nuanced temporal-event interactions. To handle event-induced uncertainty, flow matching timesteps are adaptively controlled according to event semantic signals. The underlying denoiser employs a multimodal U-shaped diffusion transformer that efficiently fuses temporal and textual modalities across different resolutions. Extensive experiments on 8 synthetic and real-world datasets show that EventTSF outperforms 12 baselines across diverse event-aware non-stationary time series forecasting scenarios, achieving substantial improvements of 10.7% higher forecasting accuracy and $1.13\times$ faster training efficiency.</li>
</ul>

<h3>Title: Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yanbiao Ma, Wei Dai, Bowei Liu, Jiayi Chen, Wenke Huang, Guancheng Wan, Zhiwu Lu, Junchi Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13518">https://arxiv.org/abs/2508.13518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13518">https://arxiv.org/pdf/2508.13518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13518]] Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency(https://arxiv.org/abs/2508.13518)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Despite the fast progress of deep learning, one standing challenge is the gap of the observed training samples and the underlying true distribution. There are multiple reasons for the causing of this gap e.g. sampling bias, noise etc. In the era of foundation models, we show that when leveraging the off-the-shelf (vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the geometric shapes of the resulting feature distributions exhibit remarkable transferability across domains and datasets. To verify its practical usefulness, we embody our geometric knowledge-guided distribution calibration framework in two popular and challenging settings: federated learning and long-tailed recognition. In the federated setting, we devise a technique of acquiring the global geometric shape under privacy constraints, then leverage this knowledge to generate new samples for clients, in the aim of bridging the gap between local and global observations. In long-tailed learning, it utilizes the geometric knowledge transferred from sample-rich categories to recover the true distribution for sample-scarce tail classes. Comprehensive experiments show that our proposed geometric knowledge-guided distribution calibration effectively overcomes information deficits caused by data heterogeneity and sample imbalance, with boosted performance across benchmarks.</li>
</ul>

<h3>Title: Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</h3>
<ul>
<li><strong>Authors: </strong>Hassan Barmandah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13525">https://arxiv.org/abs/2508.13525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13525">https://arxiv.org/pdf/2508.13525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13525]] Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation(https://arxiv.org/abs/2508.13525)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) for Arabic are still dominated by Modern Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi and Hijazi. This underrepresentation hinders their ability to capture authentic dialectal variation. Using a privately curated Saudi Dialect Instruction dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50 split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model developed in Saudi Arabia, for Saudi dialect generation. We investigate two variants: (i) Dialect-Token training, which prepends an explicit dialect tag to the instruction, and (ii) No-Token training, which omits the tag at formatting time. Evaluation on a held-out test set combines an external dialect classifier with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The Dialect-Token model achieves the best control, raising the Saudi rate from 47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and fidelity, while avoiding metadata-tag echoing that these baselines frequently exhibit. We do not release the dataset or any model weights/adapters; instead, we release training/evaluation/inference code and a detailed datasheet (schema and aggregate statistics) to support independent verification.</li>
</ul>

<h3>Title: EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors</h3>
<ul>
<li><strong>Authors: </strong>Shikun Zhang, Cunjian Chen, Yiqun Wang, Qiuhong Ke, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13537">https://arxiv.org/abs/2508.13537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13537">https://arxiv.org/pdf/2508.13537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13537]] EAvatar: Expression-Aware Head Avatar Reconstruction with Generative Geometry Priors(https://arxiv.org/abs/2508.13537)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-fidelity head avatar reconstruction plays a crucial role in AR/VR, gaming, and multimedia content creation. Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated effectiveness in modeling complex geometry with real-time rendering capability and are now widely used in high-fidelity head avatar reconstruction tasks. However, existing 3DGS-based methods still face significant challenges in capturing fine-grained facial expressions and preserving local texture continuity, especially in highly deformable regions. To mitigate these limitations, we propose a novel 3DGS-based framework termed EAvatar for head reconstruction that is both expression-aware and deformation-aware. Our method introduces a sparse expression control mechanism, where a small number of key Gaussians are used to influence the deformation of their neighboring Gaussians, enabling accurate modeling of local deformations and fine-scale texture transitions. Furthermore, we leverage high-quality 3D priors from pretrained generative models to provide a more reliable facial geometry, offering structural guidance that improves convergence stability and shape accuracy during training. Experimental results demonstrate that our method produces more accurate and visually coherent head reconstructions with improved expression controllability and detail fidelity.</li>
</ul>

<h3>Title: A Lightweight Dual-Mode Optimization for Generative Face Video Coding</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhang, Shanzhi Yin, Bolin Chen, Ru-Ling Liao, Shiqi Wang, Yan Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13547">https://arxiv.org/abs/2508.13547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13547">https://arxiv.org/pdf/2508.13547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13547]] A Lightweight Dual-Mode Optimization for Generative Face Video Coding(https://arxiv.org/abs/2508.13547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Face Video Coding (GFVC) achieves superior rate-distortion performance by leveraging the strong inference capabilities of deep generative models. However, its practical deployment is hindered by large model parameters and high computational costs. To address this, we propose a lightweight GFVC framework that introduces dual-mode optimization - combining architectural redesign and operational refinement - to reduce complexity whilst preserving reconstruction quality. Architecturally, we replace traditional 3 x 3 convolutions with slimmer and more efficient layers, reducing complexity without compromising feature expressiveness. Operationally, we develop a two-stage adaptive channel pruning strategy: (1) soft pruning during training identifies redundant channels via learnable thresholds, and (2) hard pruning permanently eliminates these channels post-training using a derived mask. This dual-phase approach ensures both training stability and inference efficiency. Experimental results demonstrate that the proposed lightweight dual-mode optimization for GFVC can achieve 90.4% parameter reduction and 88.9% computation saving compared to the baseline, whilst achieving superior performance compared to state-of-the-art video coding standard Versatile Video Coding (VVC) in terms of perceptual-level quality metrics. As such, the proposed method is expected to enable efficient GFVC deployment in resource-constrained environments such as mobile edge devices.</li>
</ul>

<h3>Title: DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup</h3>
<ul>
<li><strong>Authors: </strong>Zhen Qu, Xian Tao, Xinyi Gong, ShiChen Qu, Xiaopei Zhang, Xingang Wang, Fei Shen, Zhengtao Zhang, Mukesh Prasad, Guiguang Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13560">https://arxiv.org/abs/2508.13560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13560">https://arxiv.org/pdf/2508.13560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13560]] DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup(https://arxiv.org/abs/2508.13560)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Recent vision-language models (e.g., CLIP) have demonstrated remarkable class-generalizable ability to unseen classes in few-shot anomaly segmentation (FSAS), leveraging supervised prompt learning or fine-tuning on seen classes. However, their cross-category generalization largely depends on prior knowledge of real seen anomaly samples. In this paper, we propose a novel framework, namely DictAS, which enables a unified model to detect visual anomalies in unseen object categories without any retraining on the target data, only employing a few normal reference images as visual prompts. The insight behind DictAS is to transfer dictionary lookup capabilities to the FSAS task for unseen classes via self-supervised learning, instead of merely memorizing the normal and abnormal feature patterns from the training set. Specifically, DictAS mainly consists of three components: (1) **Dictionary Construction** - to simulate the index and content of a real dictionary using features from normal reference images. (2) **Dictionary Lookup** - to retrieve queried region features from the dictionary via a sparse lookup strategy. When a query feature cannot be retrieved, it is classified as an anomaly. (3) **Query Discrimination Regularization**- to enhance anomaly discrimination by making abnormal features harder to retrieve from the dictionary. To achieve this, Contrastive Query Constraint and Text Alignment Constraint are further proposed. Extensive experiments on seven public industrial and medical datasets demonstrate that DictAS consistently outperforms state-of-the-art FSAS methods.</li>
</ul>

<h3>Title: Prediction of Hospital Associated Infections During Continuous Hospital Stays</h3>
<ul>
<li><strong>Authors: </strong>Rituparna Datta, Methun Kamruzzaman, Eili Y. Klein, Gregory R Madden, Xinwei Deng, Anil Vullikanti, Parantapa Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13561">https://arxiv.org/abs/2508.13561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13561">https://arxiv.org/pdf/2508.13561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13561]] Prediction of Hospital Associated Infections During Continuous Hospital Stays(https://arxiv.org/abs/2508.13561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The US Centers for Disease Control and Prevention (CDC), in 2019, designated Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial resistance threat. The risk of acquiring MRSA and suffering life-threatening consequences due to it remains especially high for hospitalized patients due to a unique combination of factors, including: co-morbid conditions, immuno suppression, antibiotic use, and risk of contact with contaminated hospital workers and equipment. In this paper, we present a novel generative probabilistic model, GenHAI, for modeling sequences of MRSA test results outcomes for patients during a single hospitalization. This model can be used to answer many important questions from the perspectives of hospital administrators for mitigating the risk of MRSA infections. Our model is based on the probabilistic programming paradigm, and can be used to approximately answer a variety of predictive, causal, and counterfactual questions. We demonstrate the efficacy of our model by comparing it against discriminative and generative machine learning models using two real-world datasets.</li>
</ul>

<h3>Title: Generative Model-Based Feature Attention Module for Video Action Analysis</h3>
<ul>
<li><strong>Authors: </strong>Guiqin Wang, Peng Zhao, Cong Zhao, Jing Huang, Siyan Guo, Shusen Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13565">https://arxiv.org/abs/2508.13565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13565">https://arxiv.org/pdf/2508.13565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13565]] Generative Model-Based Feature Attention Module for Video Action Analysis(https://arxiv.org/abs/2508.13565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video action analysis is a foundational technology within the realm of intelligent video comprehension, particularly concerning its application in Internet of Things(IoT). However, existing methodologies overlook feature semantics in feature extraction and focus on optimizing action proposals, thus these solutions are unsuitable for widespread adoption in high-performance IoT applications due to the limitations in precision, such as autonomous driving, which necessitate robust and scalable intelligent video analytics analysis. To address this issue, we propose a novel generative attention-based model to learn the relation of feature semantics. Specifically, by leveraging the differences of actions' foreground and background, our model simultaneously learns the frame- and segment-dependencies of temporal action feature semantics, which takes advantage of feature semantics in the feature extraction effectively. To evaluate the effectiveness of our model, we conduct extensive experiments on two benchmark video task, action recognition and action detection. In the context of action detection tasks, we substantiate the superiority of our approach through comprehensive validation on widely recognized datasets. Moreover, we extend the validation of the effectiveness of our proposed method to a broader task, video action recognition. Our code is available at this https URL.</li>
</ul>

<h3>Title: Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Ruixin Zhang, Jiaqing Fan, Yifan Liao, Qian Qiao, Fanzhang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13584">https://arxiv.org/abs/2508.13584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13584">https://arxiv.org/pdf/2508.13584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13584]] Temporal-Conditional Referring Video Object Segmentation with Noise-Free Text-to-Video Diffusion Model(https://arxiv.org/abs/2508.13584)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Referring Video Object Segmentation (RVOS) aims to segment specific objects in a video according to textual descriptions. We observe that recent RVOS approaches often place excessive emphasis on feature extraction and temporal modeling, while relatively neglecting the design of the segmentation head. In fact, there remains considerable room for improvement in segmentation head design. To address this, we propose a Temporal-Conditional Referring Video Object Segmentation model, which innovatively integrates existing segmentation methods to effectively enhance boundary segmentation capability. Furthermore, our model leverages a text-to-video diffusion model for feature extraction. On top of this, we remove the traditional noise prediction module to avoid the randomness of noise from degrading segmentation accuracy, thereby simplifying the model while improving performance. Finally, to overcome the limited feature extraction capability of the VAE, we design a Temporal Context Mask Refinement (TCMR) module, which significantly improves segmentation quality without introducing complex designs. We evaluate our method on four public RVOS benchmarks, where it consistently achieves state-of-the-art performance.</li>
</ul>

<h3>Title: Bridging Clear and Adverse Driving Conditions</h3>
<ul>
<li><strong>Authors: </strong>Yoel Shapiro, Yahia Showgan, Koustav Mullick</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13592">https://arxiv.org/abs/2508.13592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13592">https://arxiv.org/pdf/2508.13592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13592]] Bridging Clear and Adverse Driving Conditions(https://arxiv.org/abs/2508.13592)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autonomous Driving (AD) systems exhibit markedly degraded performance under adverse environmental conditions, such as low illumination and precipitation. The underrepresentation of adverse conditions in AD datasets makes it challenging to address this deficiency. To circumvent the prohibitive cost of acquiring and annotating adverse weather data, we propose a novel Domain Adaptation (DA) pipeline that transforms clear-weather images into fog, rain, snow, and nighttime images. Here, we systematically develop and evaluate several novel data-generation pipelines, including simulation-only, GAN-based, and hybrid diffusion-GAN approaches, to synthesize photorealistic adverse images from labelled clear images. We leverage an existing DA GAN, extend it to support auxiliary inputs, and develop a novel training recipe that leverages both simulated and real images. The simulated images facilitate exact supervision by providing perfectly matched image pairs, while the real images help bridge the simulation-to-real (sim2real) gap. We further introduce a method to mitigate hallucinations and artifacts in Stable-Diffusion Image-to-Image (img2img) outputs by blending them adaptively with their progenitor images. We finetune downstream models on our synthetic data and evaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We achieve 1.85 percent overall improvement in semantic segmentation, and 4.62 percent on nighttime, demonstrating the efficacy of our hybrid method for robust AD perception under challenging conditions.</li>
</ul>

<h3>Title: A Generalized Learning Framework for Self-Supervised Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Lingyu Si, Jingyao Wang, Wenwen Qiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13596">https://arxiv.org/abs/2508.13596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13596">https://arxiv.org/pdf/2508.13596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13596]] A Generalized Learning Framework for Self-Supervised Contrastive Learning(https://arxiv.org/abs/2508.13596)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised contrastive learning (SSCL) has recently demonstrated superiority in multiple downstream tasks. In this paper, we generalize the standard SSCL methods to a Generalized Learning Framework (GLF) consisting of two parts: the aligning part and the constraining part. We analyze three existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be unified under GLF with different choices of the constraining part. We further propose empirical and theoretical analyses providing two insights into designing the constraining part of GLF: intra-class compactness and inter-class separability, which measure how well the feature space preserves the class information of the inputs. However, since SSCL can not use labels, it is challenging to design a constraining part that satisfies these properties. To address this issue, we consider inducing intra-class compactness and inter-class separability by iteratively capturing the dynamic relationship between anchor and other samples and propose a plug-and-play method called Adaptive Distribution Calibration (ADC) to ensure that samples that are near or far from the anchor point in the original input space are closer or further away from the anchor point in the feature space. Both the theoretical analysis and the empirical evaluation demonstrate the superiority of ADC.</li>
</ul>

<h3>Title: DiffIER: Optimizing Diffusion Models with Iterative Error Reduction</h3>
<ul>
<li><strong>Authors: </strong>Ao Chen, Lihe Ding, Tianfan Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13628">https://arxiv.org/abs/2508.13628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13628">https://arxiv.org/pdf/2508.13628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13628]] DiffIER: Optimizing Diffusion Models with Iterative Error Reduction(https://arxiv.org/abs/2508.13628)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable capabilities in generating high-quality samples and enhancing performance across diverse domains through Classifier-Free Guidance (CFG). However, the quality of generated samples is highly sensitive to the selection of the guidance weight. In this work, we identify a critical ``training-inference gap'' and we argue that it is the presence of this gap that undermines the performance of conditional generation and renders outputs highly sensitive to the guidance weight. We quantify this gap by measuring the accumulated error during the inference stage and establish a correlation between the selection of guidance weight and minimizing this gap. Furthermore, to mitigate this gap, we propose DiffIER, an optimization-based method for high-quality generation. We demonstrate that the accumulated error can be effectively reduced by an iterative error minimization at each step during inference. By introducing this novel plug-and-play optimization framework, we enable the optimization of errors at every single inference step and enhance generation quality. Empirical results demonstrate that our proposed method outperforms baseline approaches in conditional generation tasks. Furthermore, the method achieves consistent success in text-to-image generation, image super-resolution, and text-to-speech generation, underscoring its versatility and potential for broad applications in future research.</li>
</ul>

<h3>Title: Text2Weight: Bridging Natural Language and Neural Network Weight Spaces</h3>
<ul>
<li><strong>Authors: </strong>Bowen Tian, Wenshuo Chen, Zexi Li, Songning Lai, Jiemin Wu, Yutao Yue</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13633">https://arxiv.org/abs/2508.13633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13633">https://arxiv.org/pdf/2508.13633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13633]] Text2Weight: Bridging Natural Language and Neural Network Weight Spaces(https://arxiv.org/abs/2508.13633)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>How far are we really from automatically generating neural networks? While neural network weight generation shows promise, current approaches struggle with generalization to unseen tasks and practical application exploration. To address this, we propose T2W, a diffusion transformer framework that generates task-specific weights conditioned on natural language descriptions. T2W hierarchically processes network parameters into uniform blocks, integrates text embeddings from CLIP via a prior attention mechanism, and employs adversarial training with weight-space augmentation to enhance generalization. Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability to produce high-quality weights for unseen tasks, outperforming optimization-based initialization and enabling novel applications such as weight enhancement and text-guided model fusion. Our work bridges textual semantics with weight-space dynamics, supported by an open-source dataset of text-weight pairs, advancing the practicality of generative models in neural network parameter synthesis. Our code is available on Github.</li>
</ul>

<h3>Title: Personalized Subgraph Federated Learning with Sheaf Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Wenfei Liang, Yanan Zhao, Rui She, Yiming Li, Wee Peng Tay</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13642">https://arxiv.org/abs/2508.13642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13642">https://arxiv.org/pdf/2508.13642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13642]] Personalized Subgraph Federated Learning with Sheaf Collaboration(https://arxiv.org/abs/2508.13642)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph-structured data is prevalent in many applications. In subgraph federated learning (FL), this data is distributed across clients, each with a local subgraph. Personalized subgraph FL aims to develop a customized model for each client to handle diverse data distributions. However, performance variation across clients remains a key issue due to the heterogeneity of local subgraphs. To overcome the challenge, we propose FedSheafHN, a novel framework built on a sheaf collaboration mechanism to unify enhanced client descriptors with efficient personalized model generation. Specifically, FedSheafHN embeds each client's local subgraph into a server-constructed collaboration graph by leveraging graph-level embeddings and employing sheaf diffusion within the collaboration graph to enrich client representations. Subsequently, FedSheafHN generates customized client models via a server-optimized hypernetwork. Empirical evaluations demonstrate that FedSheafHN outperforms existing personalized subgraph FL methods on various graph datasets. Additionally, it exhibits fast model convergence and effectively generalizes to new clients.</li>
</ul>

<h3>Title: In-Context Decision Making for Optimizing Complex AutoML Pipelines</h3>
<ul>
<li><strong>Authors: </strong>Amir Rezaei Balef, Katharina Eggensperger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13657">https://arxiv.org/abs/2508.13657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13657">https://arxiv.org/pdf/2508.13657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13657]] In-Context Decision Making for Optimizing Complex AutoML Pipelines(https://arxiv.org/abs/2508.13657)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been fundamental to traditional AutoML systems. However, with the advancements of pre-trained models, modern ML workflows go beyond hyperparameter optimization and often require fine-tuning, ensembling, and other adaptation techniques. While the core challenge of identifying the best-performing model for a downstream task remains, the increasing heterogeneity of ML pipelines demands novel AutoML approaches. This work extends the CASH framework to select and adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to efficiently estimate the posterior distribution of the maximal value via in-context learning. We show how to extend this method to consider varying costs of pulling arms and to use different PFNs to model reward distributions individually per arm. Experimental results on one novel and two existing standard benchmark tasks demonstrate the superior performance of PS-PFN compared to other bandit and AutoML strategies. We make our code and data available at this https URL.</li>
</ul>

<h3>Title: SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Paul Grimal, Micha√´l Soumm, Herv√© Le Borgne, Olivier Ferret, Akihiro Sugimoto</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13866">https://arxiv.org/abs/2508.13866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13866">https://arxiv.org/pdf/2508.13866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13866]] SAGA: Learning Signal-Aligned Distributions for Improved Text-to-Image Generation(https://arxiv.org/abs/2508.13866)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>State-of-the-art text-to-image models produce visually impressive results but often struggle with precise alignment to text prompts, leading to missing critical elements or unintended blending of distinct concepts. We propose a novel approach that learns a high-success-rate distribution conditioned on a target prompt, ensuring that generated images faithfully reflect the corresponding prompts. Our method explicitly models the signal component during the denoising process, offering fine-grained control that mitigates over-optimization and out-of-distribution artifacts. Moreover, our framework is training-free and seamlessly integrates with both existing diffusion and flow matching architectures. It also supports additional conditioning modalities -- such as bounding boxes -- for enhanced spatial alignment. Extensive experiments demonstrate that our approach outperforms current state-of-the-art methods. The code is available at this https URL.</li>
</ul>

<h3>Title: Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation</h3>
<ul>
<li><strong>Authors: </strong>Thanh Nguyen, Chang D. Yoo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13904">https://arxiv.org/abs/2508.13904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13904">https://arxiv.org/pdf/2508.13904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13904]] Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation(https://arxiv.org/abs/2508.13904)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The generative power of diffusion models (DMs) has recently enabled high-performing decision-making algorithms in offline reinforcement learning (RL), achieving state-of-the-art results across standard benchmarks. Among them, Diffusion Q-Learning (DQL) stands out as a leading method for its consistently strong performance. Nevertheless, DQL remains limited in practice due to its reliance on multi-step denoising for action generation during both training and inference. Although one-step denoising is desirable, simply applying it to DQL leads to a drastic performance drop. In this work, we revisit DQL and identify its core limitations. We then propose One-Step Flow Q-Learning (OFQL), a novel framework that enables efficient one-step action generation during both training and inference, without requiring auxiliary models, distillation, or multi-phase training. Specifically, OFQL reformulates DQL within the sample-efficient Flow Matching (FM) framework. While conventional FM induces curved generative trajectories that impede one-step generation, OFQL instead learns an average velocity field that facilitates direct, accurate action generation. Collectively, OFQL eliminates the need for multi-step sampling and recursive gradient updates in DQL, resulting in faster and more robust training and inference. Extensive experiments on the D4RL benchmark demonstrate that OFQL outperforms DQL and other diffusion-based baselines, while substantially reducing both training and inference time compared to DQL.</li>
</ul>

<h3>Title: ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Xianda Guo, Ruijun Zhang, Yiqun Duan, Ruilin Wang, Keyuan Zhou, Wenzhao Zheng, Wenke Huang, Gangwei Xu, Mike Horton, Yuan Si, Hao Zhao, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13977">https://arxiv.org/abs/2508.13977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13977">https://arxiv.org/pdf/2508.13977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13977]] ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving(https://arxiv.org/abs/2508.13977)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Depth estimation is a fundamental task for 3D scene understanding in autonomous driving, robotics, and augmented reality. Existing depth datasets, such as KITTI, nuScenes, and DDAD, have advanced the field but suffer from limitations in diversity and scalability. As benchmark performance on these datasets approaches saturation, there is an increasing need for a new generation of large-scale, diverse, and cost-efficient datasets to support the era of foundation models and multi-modal learning. To address these challenges, we introduce a large-scale, diverse, frame-wise continuous dataset for depth estimation in dynamic outdoor driving environments, comprising 20K video frames to evaluate existing methods. Our lightweight acquisition pipeline ensures broad scene coverage at low cost, while sparse yet statistically sufficient ground truth enables robust training. Compared to existing datasets, ours presents greater diversity in driving scenarios and lower depth density, creating new challenges for generalization. Benchmark experiments with standard monocular depth estimation models validate the dataset's utility and highlight substantial performance gaps in challenging conditions, establishing a new platform for advancing depth estimation research.</li>
</ul>

<h3>Title: Self-Supervised Sparse Sensor Fusion for Long Range Perception</h3>
<ul>
<li><strong>Authors: </strong>Edoardo Palladin, Samuel Brucker, Filippo Ghilotti, Praveen Narayanan, Mario Bijelic, Felix Heide</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.13995">https://arxiv.org/abs/2508.13995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.13995">https://arxiv.org/pdf/2508.13995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.13995]] Self-Supervised Sparse Sensor Fusion for Long Range Perception(https://arxiv.org/abs/2508.13995)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Outside of urban hubs, autonomous cars and trucks have to master driving on intercity highways. Safe, long-distance highway travel at speeds exceeding 100 km/h demands perception distances of at least 250 m, which is about five times the 50-100m typically addressed in city driving, to allow sufficient planning and braking margins. Increasing the perception ranges also allows to extend autonomy from light two-ton passenger vehicles to large-scale forty-ton trucks, which need a longer planning horizon due to their high inertia. However, most existing perception approaches focus on shorter ranges and rely on Bird's Eye View (BEV) representations, which incur quadratic increases in memory and compute costs as distance grows. To overcome this limitation, we built on top of a sparse representation and introduced an efficient 3D encoding of multi-modal and temporal features, along with a novel self-supervised pre-training scheme that enables large-scale learning from unlabeled camera-LiDAR data. Our approach extends perception distances to 250 meters and achieves an 26.6% improvement in mAP in object detection and a decrease of 30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods, reaching distances up to 250 meters. Project Page: this https URL</li>
</ul>

<h3>Title: ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Abouagour, Eleftherios Garyfallidis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14006">https://arxiv.org/abs/2508.14006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14006">https://arxiv.org/pdf/2508.14006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14006]] ResPlan: A Large-Scale Vector-Graph Dataset of 17,000 Residential Floor Plans(https://arxiv.org/abs/2508.14006)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce ResPlan, a large-scale dataset of 17,000 detailed, structurally rich, and realistic residential floor plans, created to advance spatial AI research. Each plan includes precise annotations of architectural elements (walls, doors, windows, balconies) and functional spaces (such as kitchens, bedrooms, and bathrooms). ResPlan addresses key limitations of existing datasets such as RPLAN (Wu et al., 2019) and MSD (van Engelenburg et al., 2024) by offering enhanced visual fidelity and greater structural diversity, reflecting realistic and non-idealized residential layouts. Designed as a versatile, general-purpose resource, ResPlan supports a wide range of applications including robotics, reinforcement learning, generative AI, virtual and augmented reality, simulations, and game development. Plans are provided in both geometric and graph-based formats, enabling direct integration into simulation engines and fast 3D conversion. A key contribution is an open-source pipeline for geometry cleaning, alignment, and annotation refinement. Additionally, ResPlan includes structured representations of room connectivity, supporting graph-based spatial reasoning tasks. Finally, we present comparative analyses with existing benchmarks and outline several open benchmark tasks enabled by ResPlan. Ultimately, ResPlan offers a significant advance in scale, realism, and usability, providing a robust foundation for developing and benchmarking next-generation spatial intelligence systems.</li>
</ul>

<h3>Title: Typed Topological Structures Of Datasets</h3>
<ul>
<li><strong>Authors: </strong>Wanjun Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14008">https://arxiv.org/abs/2508.14008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14008">https://arxiv.org/pdf/2508.14008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14008]] Typed Topological Structures Of Datasets(https://arxiv.org/abs/2508.14008)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>A datatset $X$ on $R^2$ is a finite topological space. Current research of a dataset focuses on statistical methods and the algebraic topological method \cite{carlsson}. In \cite{hu}, the concept of typed topological space was introduced and showed to have the potential for studying finite topological spaces, such as a dataset. It is a new method from the general topology perspective. A typed topological space is a topological space whose open sets are assigned types. Topological concepts and methods can be redefined using open sets of certain types. In this article, we develop a special set of types and its related typed topology on a dataset $X$. Using it, we can investigate the inner structure of $X$. In particular, $R^2$ has a natural quotient space, in which $X$ is organized into tracks, and each track is split into components. Those components are in a order. Further, they can be represented by an integer sequence. Components crossing tracks form branches, and the relationship can be well represented by a type of pseudotree (called typed-II pseudotree). Such structures provide a platform for new algorithms for problems such as calculating convex hull, holes, clustering and anomaly detection.</li>
</ul>

<h3>Title: Backdooring Self-Supervised Contrastive Learning by Noisy Alignment</h3>
<ul>
<li><strong>Authors: </strong>Tuo Chen, Jie Gui, Minjing Dong, Ju Jia, Lanting Fang, Jian Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14015">https://arxiv.org/abs/2508.14015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14015">https://arxiv.org/pdf/2508.14015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14015]] Backdooring Self-Supervised Contrastive Learning by Noisy Alignment(https://arxiv.org/abs/2508.14015)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised contrastive learning (CL) effectively learns transferable representations from unlabeled data containing images or image-text pairs but suffers vulnerability to data poisoning backdoor attacks (DPCLs). An adversary can inject poisoned images into pretraining datasets, causing compromised CL encoders to exhibit targeted misbehavior in downstream tasks. Existing DPCLs, however, achieve limited efficacy due to their dependence on fragile implicit co-occurrence between backdoor and target object and inadequate suppression of discriminative features in backdoored images. We propose Noisy Alignment (NA), a DPCL method that explicitly suppresses noise components in poisoned images. Inspired by powerful training-controllable CL attacks, we identify and extract the critical objective of noisy alignment, adapting it effectively into data-poisoning scenarios. Our method implements noisy alignment by strategically manipulating contrastive learning's random cropping mechanism, formulating this process as an image layout optimization problem with theoretically derived optimal parameters. The resulting method is simple yet effective, achieving state-of-the-art performance compared to existing DPCLs, while maintaining clean-data accuracy. Furthermore, Noisy Alignment demonstrates robustness against common backdoor defenses. Codes can be found at this https URL.</li>
</ul>

<h3>Title: The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities</h3>
<ul>
<li><strong>Authors: </strong>Xiancheng Li, Georgios D. Karampatakis, Helen E. Wood, Chris J. Griffiths, Borislava Mihaylova, Neil S. Coulson, Alessio Pasinato, Pietro Panzarasa, Marco Viviani, Anna De Simoni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14032">https://arxiv.org/abs/2508.14032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14032">https://arxiv.org/pdf/2508.14032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14032]] The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities(https://arxiv.org/abs/2508.14032)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Digital health analytics face critical challenges nowadays. The sophisticated analysis of patient-generated health content, which contains complex emotional and medical contexts, requires scarce domain expertise, while traditional ML approaches are constrained by data shortage and privacy limitations in healthcare settings. Online Health Communities (OHCs) exemplify these challenges with mixed-sentiment posts, clinical terminology, and implicit emotional expressions that demand specialised knowledge for accurate Sentiment Analysis (SA). To address these challenges, this study explores how Large Language Models (LLMs) can integrate expert knowledge through in-context learning for SA, providing a scalable solution for sophisticated health data analysis. Specifically, we develop a structured codebook that systematically encodes expert interpretation guidelines, enabling LLMs to apply domain-specific knowledge through targeted prompting rather than extensive training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are compared with pre-trained language models (BioBERT variants) and lexicon-based methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior performance while demonstrating expert-level agreement. This high agreement, with no statistically significant difference from inter-expert agreement levels, suggests knowledge integration beyond surface-level pattern recognition. The consistent performance across diverse LLM models, supported by in-context learning, offers a promising solution for digital health analytics. This approach addresses the critical challenge of expert knowledge shortage in digital health research, enabling real-time, expert-quality analysis for patient monitoring, intervention assessment, and evidence-based health strategies.</li>
</ul>

<h3>Title: GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ken Deng, Yunhan Yang, Jingxiang Sun, Xihui Liu, Yebin Liu, Ding Liang, Yan-Pei Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.14036">https://arxiv.org/abs/2508.14036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.14036">https://arxiv.org/pdf/2508.14036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.14036]] GeoSAM2: Unleashing the Power of SAM2 for 3D Part Segmentation(https://arxiv.org/abs/2508.14036)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern 3D generation methods can rapidly create shapes from sparse or single views, but their outputs often lack geometric detail due to computational constraints. We present DetailGen3D, a generative approach specifically designed to enhance these generated 3D shapes. Our key insight is to model the coarse-to-fine transformation directly through data-dependent flows in latent space, avoiding the computational overhead of large-scale 3D generative models. We introduce a token matching strategy that ensures accurate spatial correspondence during refinement, enabling local detail synthesis while preserving global structure. By carefully designing our training data to match the characteristics of synthesized coarse shapes, our method can effectively enhance shapes produced by various 3D generation and reconstruction approaches, from single-view to sparse multi-view inputs. Extensive experiments demonstrate that DetailGen3D achieves high-fidelity geometric detail synthesis while maintaining efficiency in training.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
