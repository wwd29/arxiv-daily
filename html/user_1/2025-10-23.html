<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-23</h1>
<h3>Title: Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape</h3>
<ul>
<li><strong>Authors: </strong>Ziquan Wei, Tingting Dan, Guorong Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18910">https://arxiv.org/abs/2510.18910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18910">https://arxiv.org/pdf/2510.18910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18910]] Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape(https://arxiv.org/abs/2510.18910)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>A reliable foundation model of functional neuroimages is critical to promote clinical applications where the performance of current AI models is significantly impeded by a limited sample size. To that end, tremendous efforts have been made to pretraining large models on extensive unlabeled fMRI data using scalable self-supervised learning. Since self-supervision is not necessarily aligned with the brain-to-outcome relationship, most foundation models are suboptimal to the downstream task, such as predicting disease outcomes. By capitalizing on rich environmental variables and demographic data along with an unprecedented amount of functional neuroimages, we form the brain modeling as a multitask learning and present a scalable model architecture for (i) multitask pretraining by tokenizing multiple brain-environment interactions (BEI) and (ii) semi-supervised finetuning by assigning pseudo-labels of pretrained BEI. We have evaluated our foundation model on a variety of applications, including sex prediction, human behavior recognition, and disease early diagnosis of Autism, Parkinson's disease, Alzheimer's disease, and {Schizophrenia}, where promising results indicate the great potential to facilitate current neuroimaging applications in clinical routines.</li>
</ul>

<h3>Title: Dimensionality Reduction for Remote Sensing Data Analysis: A Systematic Review of Methods and Applications</h3>
<ul>
<li><strong>Authors: </strong>Nathan Mankovich, Kai-Hendrik Cohrs, Homer Durand, Vasileios Sitokonstantinou, Tristan Williams, Gustau Camps-Valls</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18935">https://arxiv.org/abs/2510.18935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18935">https://arxiv.org/pdf/2510.18935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18935]] Dimensionality Reduction for Remote Sensing Data Analysis: A Systematic Review of Methods and Applications(https://arxiv.org/abs/2510.18935)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Earth observation involves collecting, analyzing, and processing an ever-growing mass of data. Automatically harvesting information is crucial for addressing significant societal, economic, and environmental challenges, ranging from environmental monitoring to urban planning and disaster management. However, the high dimensionality of these data poses challenges in terms of sparsity, inefficiency, and the curse of dimensionality, which limits the effectiveness of machine learning models. Dimensionality reduction (DR) techniques, specifically feature extraction, address these challenges by preserving essential data properties while reducing complexity and enhancing tasks such as data compression, cleaning, fusion, visualization, anomaly detection, and prediction. This review provides a handbook for leveraging DR across the RS data value chain and identifies opportunities for under-explored DR algorithms and their application in future research.</li>
</ul>

<h3>Title: An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version</h3>
<ul>
<li><strong>Authors: </strong>Buang Zhang, Tung Kieu, Xiangfei Qiu, Chenjuan Guo, Jilin Hu, Aoying Zhou, Christian S. Jensen, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18998">https://arxiv.org/abs/2510.18998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18998">https://arxiv.org/pdf/2510.18998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18998]] An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version(https://arxiv.org/abs/2510.18998)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection is important in modern large-scale systems and is applied in a variety of domains to analyze and monitor the operation of diverse systems. Unsupervised approaches have received widespread interest, as they do not require anomaly labels during training, thus avoiding potentially high costs and having wider applications. Among these, autoencoders have received extensive attention. They use reconstruction errors from compressed representations to define anomaly scores. However, representations learned by autoencoders are sensitive to anomalies in training time series, causing reduced accuracy. We propose a novel encode-then-decompose paradigm, where we decompose the encoded representation into stable and auxiliary representations, thereby enhancing the robustness when training with contaminated time series. In addition, we propose a novel mutual information based metric to replace the reconstruction errors for identifying anomalies. Our proposal demonstrates competitive or state-of-the-art performance on eight commonly used multi- and univariate time series benchmarks and exhibits robustness to time series with different contamination ratios.</li>
</ul>

<h3>Title: Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records</h3>
<ul>
<li><strong>Authors: </strong>Saman Nessari, Ali Bozorgi-Amiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19014">https://arxiv.org/abs/2510.19014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19014">https://arxiv.org/pdf/2510.19014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19014]] Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records(https://arxiv.org/abs/2510.19014)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current medical practice depends on standardized treatment frameworks and empirical methodologies that neglect individual patient variations, leading to suboptimal health outcomes. We develop a comprehensive system integrating Large Language Models (LLMs), Conditional Tabular Generative Adversarial Networks (CTGAN), T-learner counterfactual models, and contextual bandit approaches to provide customized, data-informed clinical recommendations. The approach utilizes LLMs to process unstructured medical narratives into structured datasets (93.2% accuracy), uses CTGANs to produce realistic synthetic patient data (55% accuracy via two-sample verification), deploys T-learners to forecast patient-specific treatment responses (84.3% accuracy), and integrates prior-informed contextual bandits to enhance online therapeutic selection by effectively balancing exploration of new possibilities with exploitation of existing knowledge. Testing on stage III colon cancer datasets revealed that our KernelUCB approach obtained 0.60-0.61 average reward scores across 5,000 rounds, exceeding other reference methods. This comprehensive system overcomes cold-start limitations in online learning environments, improves computational effectiveness, and constitutes notable progress toward individualized medicine adapted to specific patient characteristics.</li>
</ul>

<h3>Title: MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Aritra Bhowmik, Denis Korzhenkov, Cees G. M. Snoek, Amirhossein Habibian, Mohsen Ghafoorian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19022">https://arxiv.org/abs/2510.19022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19022">https://arxiv.org/pdf/2510.19022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19022]] MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models(https://arxiv.org/abs/2510.19022)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-video diffusion models have enabled high-quality video synthesis, yet often fail to generate temporally coherent and physically plausible motion. A key reason is the models' insufficient understanding of complex motions that natural videos often entail. Recent works tackle this problem by aligning diffusion model features with those from pretrained video encoders. However, these encoders mix video appearance and dynamics into entangled features, limiting the benefit of such alignment. In this paper, we propose a motion-centric alignment framework that learns a disentangled motion subspace from a pretrained video encoder. This subspace is optimized to predict ground-truth optical flow, ensuring it captures true motion dynamics. We then align the latent features of a text-to-video diffusion model to this new subspace, enabling the generative model to internalize motion knowledge and generate more plausible videos. Our method improves the physical commonsense in a state-of-the-art video diffusion model, while preserving adherence to textual prompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench, and VBench-2.0, along with a user study.</li>
</ul>

<h3>Title: PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19060">https://arxiv.org/abs/2510.19060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19060">https://arxiv.org/pdf/2510.19060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19060]] PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions(https://arxiv.org/abs/2510.19060)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.</li>
</ul>

<h3>Title: Learning Peer Influence Probabilities with Linear Contextual Bandits</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Sayeed Faruk, Mohammad Shahverdikondori, Elena Zheleva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19119">https://arxiv.org/abs/2510.19119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19119">https://arxiv.org/pdf/2510.19119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19119]] Learning Peer Influence Probabilities with Linear Contextual Bandits(https://arxiv.org/abs/2510.19119)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In networked environments, users frequently share recommendations about content, products, services, and courses of action with others. The extent to which such recommendations are successful and adopted is highly contextual, dependent on the characteristics of the sender, recipient, their relationship, the recommended item, and the medium, which makes peer influence probabilities highly heterogeneous. Accurate estimation of these probabilities is key to understanding information diffusion processes and to improving the effectiveness of viral marketing strategies. However, learning these probabilities from data is challenging; static data may capture correlations between peer recommendations and peer actions but fails to reveal influence relationships. Online learning algorithms can learn these probabilities from interventions but either waste resources by learning from random exploration or optimize for rewards, thus favoring exploration of the space with higher influence probabilities. In this work, we study learning peer influence probabilities under a contextual linear bandit framework. We show that a fundamental trade-off can arise between regret minimization and estimation error, characterize all achievable rate pairs, and propose an uncertainty-guided exploration algorithm that, by tuning a parameter, attains any pair within this trade-off. Our experiments on semi-synthetic network datasets show the advantages of our method over static methods and contextual bandits that ignore this trade-off.</li>
</ul>

<h3>Title: Securing IoT Communications via Anomaly Traffic Detection: Synergy of Genetic Algorithm and Ensemble Method</h3>
<ul>
<li><strong>Authors: </strong>Behnam Seyedi, Octavian Postolache</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19121">https://arxiv.org/abs/2510.19121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19121">https://arxiv.org/pdf/2510.19121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19121]] Securing IoT Communications via Anomaly Traffic Detection: Synergy of Genetic Algorithm and Ensemble Method(https://arxiv.org/abs/2510.19121)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The rapid growth of the Internet of Things (IoT) has transformed industries by enabling seamless data exchange among connected devices. However, IoT networks remain vulnerable to security threats such as denial of service (DoS) attacks, anomalous traffic, and data manipulation due to decentralized architectures and limited resources. To address these issues, this paper proposes an advanced anomaly detection framework with three main phases. First, data preprocessing is performed using the Median KS Test to remove noise, handle missing values, and balance datasets for cleaner input. Second, a feature selection phase employs a Genetic Algorithm combined with eagle inspired search strategies to identify the most relevant features, reduce dimensionality, and improve efficiency without sacrificing accuracy. Finally, an ensemble classifier integrates Decision Tree, Random Forest, and XGBoost algorithms to achieve accurate and reliable anomaly detection. The proposed model demonstrates high adaptability and scalability across diverse IoT environments. Experimental results show that it outperforms existing methods by achieving 98 percent accuracy, 95 percent detection rate, and reductions in false positive (10 percent) and false negative (5 percent) rates. These results confirm the framework effectiveness and robustness in improving IoT network security against evolving cyber threats.</li>
</ul>

<h3>Title: Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression</h3>
<ul>
<li><strong>Authors: </strong>Paimon Goulart, Jordan Steinhauser, Kylene Shuler, Edward Korzus, Jia Chen, Evangelos E. Papalexakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19160">https://arxiv.org/abs/2510.19160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19160">https://arxiv.org/pdf/2510.19160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19160]] Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression(https://arxiv.org/abs/2510.19160)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Integration of diverse data will be a pivotal step towards improving scientific explorations in many disciplines. This work establishes a vision-language model (VLM) that encodes videos with text input in order to classify various behaviors of a mouse existing in and engaging with their environment. Importantly, this model produces a behavioral vector over time for each subject and for each session the subject undergoes. The output is a valuable dataset that few programs are able to produce with as high accuracy and with minimal user input. Specifically, we use the open-source Qwen2.5-VL model and enhance its performance through prompts, in-context learning (ICL) with labeled examples, and frame-level preprocessing. We found that each of these methods contributes to improved classification, and that combining them results in strong F1 scores across all behaviors, including rare classes like freezing and fleeing, without any model fine-tuning. Overall, this model will support interdisciplinary researchers studying mouse behavior by enabling them to integrate diverse behavioral features, measured across multiple time points and environments, into a comprehensive dataset that can address complex research questions.</li>
</ul>

<h3>Title: Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Takehiro Aoshima, Yusuke Shinohara, Park Byeongseon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19193">https://arxiv.org/abs/2510.19193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19193">https://arxiv.org/pdf/2510.19193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19193]] Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning(https://arxiv.org/abs/2510.19193)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reward-based fine-tuning of video diffusion models is an effective approach to improve the quality of generated videos, as it can fine-tune models without requiring real-world video datasets. However, it can sometimes be limited to specific performances because conventional reward functions are mainly aimed at enhancing the quality across the whole generated video sequence, such as aesthetic appeal and overall consistency. Notably, the temporal consistency of the generated video often suffers when applying previous approaches to image-to-video (I2V) generation tasks. To address this limitation, we propose Video Consistency Distance (VCD), a novel metric designed to enhance temporal consistency, and fine-tune a model with the reward-based fine-tuning framework. To achieve coherent temporal consistency relative to a conditioning image, VCD is defined in the frequency space of video frame features to capture frame information effectively through frequency-domain analysis. Experimental results across multiple I2V datasets demonstrate that fine-tuning a video generation model with VCD significantly enhances temporal consistency without degrading other performance compared to the previous method.</li>
</ul>

<h3>Title: An Active Diffusion Neural Network for Graphs</h3>
<ul>
<li><strong>Authors: </strong>Mengying Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19202">https://arxiv.org/abs/2510.19202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19202">https://arxiv.org/pdf/2510.19202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19202]] An Active Diffusion Neural Network for Graphs(https://arxiv.org/abs/2510.19202)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The analogy to heat diffusion has enhanced our understanding of information flow in graphs and inspired the development of Graph Neural Networks (GNNs). However, most diffusion-based GNNs emulate passive heat diffusion, which still suffers from over-smoothing and limits their ability to capture global graph information. Inspired by the heat death of the universe, which posits that energy distribution becomes uniform over time in a closed system, we recognize that, without external input, node representations in a graph converge to identical feature vectors as diffusion progresses. To address this issue, we propose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achieves active diffusion by integrating multiple external information sources that dynamically influence the diffusion process, effectively overcoming the over-smoothing problem. Furthermore, our approach realizes true infinite diffusion by directly calculating the closed-form solution of the active diffusion iterative formula. This allows nodes to preserve their unique characteristics while efficiently gaining comprehensive insights into the graph's global structure. We evaluate ADGNN against several state-of-the-art GNN models across various graph tasks. The results demonstrate that ADGNN significantly improves both accuracy and efficiency, highlighting its effectiveness in capturing global graph information and maintaining node distinctiveness.</li>
</ul>

<h3>Title: Understanding the Implicit Biases of Design Choices for Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Annan Yu, Danielle C. Maddix, Boran Han, Xiyuan Zhang, Abdul Fatir Ansari, Oleksandr Shchur, Christos Faloutsos, Andrew Gordon Wilson, Michael W. Mahoney, Yuyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19236">https://arxiv.org/abs/2510.19236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19236">https://arxiv.org/pdf/2510.19236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19236]] Understanding the Implicit Biases of Design Choices for Time Series Foundation Models(https://arxiv.org/abs/2510.19236)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series foundation models (TSFMs) are a class of potentially powerful, general-purpose tools for time series forecasting and related temporal tasks, but their behavior is strongly shaped by subtle inductive biases in their design. Rather than developing a new model and claiming that it is better than existing TSFMs, e.g., by winning on existing well-established benchmarks, our objective is to understand how the various ``knobs'' of the training process affect model quality. Using a mix of theory and controlled empirical evaluation, we identify several design choices (patch size, embedding choice, training objective, etc.) and show how they lead to implicit biases in fundamental model properties (temporal behavior, geometric structure, how aggressively or not the model regresses to the mean, etc.); and we show how these biases can be intuitive or very counterintuitive, depending on properties of the model and data. We also illustrate in a case study on outlier handling how multiple biases can interact in complex ways; and we discuss implications of our results for learning the bitter lesson and building TSFMs.</li>
</ul>

<h3>Title: Mixing Configurations for Downstream Prediction</h3>
<ul>
<li><strong>Authors: </strong>Juntang Wang, Hao Wu, Runkun Guo, Yihan Wang, Dongmian Zou, Shixin Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19248">https://arxiv.org/abs/2510.19248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19248">https://arxiv.org/pdf/2510.19248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19248]] Mixing Configurations for Downstream Prediction(https://arxiv.org/abs/2510.19248)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Humans possess an innate ability to group objects by similarity, a cognitive mechanism that clustering algorithms aim to emulate. Recent advances in community detection have enabled the discovery of configurations -- valid hierarchical clusterings across multiple resolution scales -- without requiring labeled data. In this paper, we formally characterize these configurations and identify similar emergent structures in register tokens within Vision Transformers. Unlike register tokens, configurations exhibit lower redundancy and eliminate the need for ad hoc selection. They can be learned through unsupervised or self-supervised methods, yet their selection or composition remains specific to the downstream task and input. Building on these insights, we introduce GraMixC, a plug-and-play module that extracts configurations, aligns them using our Reverse Merge/Split (RMS) technique, and fuses them via attention heads before forwarding them to any downstream predictor. On the DSN1 16S rRNA cultivation-media prediction task, GraMixC improves the R2 score from 0.6 to 0.9 across multiple methods, setting a new state of the art. We further validate GraMixC on standard tabular benchmarks, where it consistently outperforms single-resolution and static-feature baselines.</li>
</ul>

<h3>Title: Advances in 4D Representation: Geometry, Motion, and Interaction</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19255">https://arxiv.org/abs/2510.19255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19255">https://arxiv.org/pdf/2510.19255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19255]] Advances in 4D Representation: Geometry, Motion, and Interaction(https://arxiv.org/abs/2510.19255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a survey on 4D generation and reconstruction, a fast-evolving subfield of computer graphics whose developments have been propelled by recent advances in neural fields, geometric and motion deep learning, as well 3D generative artificial intelligence (GenAI). While our survey is not the first of its kind, we build our coverage of the domain from a unique and distinctive perspective of 4D representations\/}, to model 3D geometry evolving over time while exhibiting motion and interaction. Specifically, instead of offering an exhaustive enumeration of many works, we take a more selective approach by focusing on representative works to highlight both the desirable properties and ensuing challenges of each representation under different computation, application, and data scenarios. The main take-away message we aim to convey to the readers is on how to select and then customize the appropriate 4D representations for their tasks. Organizationally, we separate the 4D representations based on three key pillars: geometry, motion, and interaction. Our discourse will not only encompass the most popular representations of today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS), but also bring attention to relatively under-explored representations in the 4D context, such as structured models and long-range motions. Throughout our survey, we will reprise the role of large language models (LLMs) and video foundational models (VFMs) in a variety of 4D applications, while steering our discussion towards their current limitations and how they can be addressed. We also provide a dedicated coverage on what 4D datasets are currently available, as well as what is lacking, in driving the subfield forward. Project page:this https URL</li>
</ul>

<h3>Title: SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yun Kai Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19272">https://arxiv.org/abs/2510.19272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19272">https://arxiv.org/pdf/2510.19272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19272]] SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution(https://arxiv.org/abs/2510.19272)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Real-world image super-resolution (Real-ISR) must handle complex degradations and inherent reconstruction ambiguities. While generative models have improved perceptual quality, a key trade-off remains with computational cost. One-step diffusion models offer speed but often produce structural inaccuracies due to distillation artifacts. To address this, we propose a novel SR framework that enhances a one-step diffusion model using a ControlNet mechanism for semantic edge guidance. This integrates edge information to provide dynamic structural control during single-pass inference. We also introduce a hybrid loss combining L2, LPIPS, and an edge-aware AME loss to optimize for pixel accuracy, perceptual quality, and geometric precision. Experiments show our method effectively improves structural integrity and realism while maintaining the efficiency of one-step generation, achieving a superior balance between output quality and inference speed. The results of test datasets will be published at this https URL and the related code will be published at this https URL.</li>
</ul>

<h3>Title: D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Nobline Yoo, Olga Russakovsky, Ye Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19278">https://arxiv.org/abs/2510.19278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19278">https://arxiv.org/pdf/2510.19278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19278]] D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation(https://arxiv.org/abs/2510.19278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models have achieved strong performance in semantic alignment, yet they still struggle with generating the correct number of objects specified in prompts. Existing approaches typically incorporate auxiliary counting networks as external critics to enhance numeracy. However, since these critics must provide gradient guidance during generation, they are restricted to regression-based models that are inherently differentiable, thus excluding detector-based models with superior counting ability, whose count-via-enumeration nature is non-differentiable. To overcome this limitation, we propose Detector-to-Differentiable (D2D), a novel framework that transforms non-differentiable detection models into differentiable critics, thereby leveraging their superior counting ability to guide numeracy generation. Specifically, we design custom activation functions to convert detector logits into soft binary indicators, which are then used to optimize the noise prior at inference time with pre-trained T2I models. Our extensive experiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of varying complexity (low-density, high-density, and multi-object scenarios) demonstrate consistent and substantial improvements in object counting accuracy (e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark), with minimal degradation in overall image quality and computational overhead.</li>
</ul>

<h3>Title: Reliability and Resilience of AI-Driven Critical Network Infrastructure under Cyber-Physical Threats</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos A. Lizos, Leandros Maglaras, Elena Petrovik, Saied M. Abd El-atty, Georgios Tsachtsiris, Mohamed Amine Ferrag</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19295">https://arxiv.org/abs/2510.19295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19295">https://arxiv.org/pdf/2510.19295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19295]] Reliability and Resilience of AI-Driven Critical Network Infrastructure under Cyber-Physical Threats(https://arxiv.org/abs/2510.19295)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The increasing reliance on AI-driven 5G/6G network infrastructures for mission-critical services highlights the need for reliability and resilience against sophisticated cyber-physical threats. These networks are highly exposed to novel attack surfaces due to their distributed intelligence, virtualized resources, and cross-domain integration. This paper proposes a fault-tolerant and resilience-aware framework that integrates AI-driven anomaly detection, adaptive routing, and redundancy mechanisms to mitigate cascading failures under cyber-physical attack conditions. A comprehensive validation is carried out using NS-3 simulations, where key performance indicators such as reliability, latency, resilience index, and packet loss rate are analyzed under various attack scenarios. The deduced results demonstrate that the proposed framework significantly improves fault recovery, stabilizes packet delivery, and reduces service disruption compared to baseline approaches.</li>
</ul>

<h3>Title: Collaborative penetration testing suite for emerging generative AI algorithms</h3>
<ul>
<li><strong>Authors: </strong>Petar Radanliev</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.MA, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19303">https://arxiv.org/abs/2510.19303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19303">https://arxiv.org/pdf/2510.19303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19303]] Collaborative penetration testing suite for emerging generative AI algorithms(https://arxiv.org/abs/2510.19303)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Problem Space: AI Vulnerabilities and Quantum Threats Generative AI vulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum threats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative AI models against classical and quantum cyberattacks. Proposed Solution Collaborative Penetration Testing Suite Five Integrated Components: DAST SAST OWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with CI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs. Quantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations Adversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow for AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities identified across test environments. 70% reduction in high-severity issues within 2 weeks. 90% resolution efficiency for blockchain-logged vulnerabilities. Quantum-resistant cryptography maintained 100% integrity in tests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum Cryptography AI Red Teaming.</li>
</ul>

<h3>Title: Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Jo, Jaesik Yoon, Justin Deschenaux, Caglar Gulcehre, Sungjin Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19304">https://arxiv.org/abs/2510.19304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19304">https://arxiv.org/pdf/2510.19304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19304]] Loopholing Discrete Diffusion: Deterministic Bypass of the Sampling Wall(https://arxiv.org/abs/2510.19304)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models offer a promising alternative to autoregressive generation through parallel decoding, but they suffer from a sampling wall: once categorical sampling occurs, rich distributional information collapses into one-hot vectors and cannot be propagated across steps, forcing subsequent steps to operate with limited information. To mitigate this problem, we introduce Loopholing, a novel and simple mechanism that preserves this information via a deterministic latent pathway, leading to Loopholing Discrete Diffusion Models (LDDMs). Trained efficiently with a self-conditioning strategy, LDDMs achieve substantial gains-reducing generative perplexity by up to 61% over prior baselines, closing (and in some cases surpassing) the gap with autoregressive models, and producing more coherent text. Applied to reasoning tasks, LDDMs also improve performance on arithmetic benchmarks such as Countdown and Game of 24. These results also indicate that loopholing mitigates idle steps and oscillations, providing a scalable path toward high-quality non-autoregressive text generation.</li>
</ul>

<h3>Title: Unified Reinforcement and Imitation Learning for Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19307">https://arxiv.org/abs/2510.19307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19307">https://arxiv.org/pdf/2510.19307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19307]] Unified Reinforcement and Imitation Learning for Vision-Language Models(https://arxiv.org/abs/2510.19307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have achieved remarkable progress, yet their large scale often renders them impractical for resource-constrained environments. This paper introduces Unified Reinforcement and Imitation Learning (RIL), a novel and efficient training algorithm designed to create powerful, lightweight VLMs. RIL distinctively combines the strengths of reinforcement learning with adversarial imitation learning. This enables smaller student VLMs not only to mimic the sophisticated text generation of large teacher models but also to systematically improve their generative capabilities through reinforcement signals. Key to our imitation framework is an LLM-based discriminator that adeptly distinguishes between student and teacher outputs, complemented by guidance from multiple large teacher VLMs to ensure diverse learning. This unified learning strategy, leveraging both reinforcement and imitation, empowers student models to achieve significant performance gains, making them competitive with leading closed-source VLMs. Extensive experiments on diverse vision-language benchmarks demonstrate that RIL significantly narrows the performance gap with state-of-the-art open- and closed-source VLMs and, in several instances, surpasses them.</li>
</ul>

<h3>Title: Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization</h3>
<ul>
<li><strong>Authors: </strong>Junjie Song, Yiwen Liu, Dapeng Li, Yin Sun, Shukun Fu, Siqi Chen, Yuji Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19325">https://arxiv.org/abs/2510.19325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19325">https://arxiv.org/pdf/2510.19325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19325]] Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization(https://arxiv.org/abs/2510.19325)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Text summarization is a crucial task that requires the simultaneous optimization of multiple objectives, including consistency, coherence, relevance, and fluency, which presents considerable challenges. Although large language models (LLMs) have demonstrated remarkable performance, enhanced by reinforcement learning (RL), few studies have focused on optimizing the multi-objective problem of summarization through RL based on LLMs. In this paper, we introduce hypervolume optimization (HVO), a novel optimization strategy that dynamically adjusts the scores between groups during the reward process in RL by using the hypervolume method. This method guides the model's optimization to progressively approximate the pareto front, thereby generating balanced summaries across multiple objectives. Experimental results on several representative summarization datasets demonstrate that our method outperforms group relative policy optimization (GRPO) in overall scores and shows more balanced performance across different dimensions. Moreover, a 7B foundation model enhanced by HVO performs comparably to GPT-4 in the summarization task, while maintaining a shorter generation length. Our code is publicly available at this https URL</li>
</ul>

<h3>Title: Slot Filling as a Reasoning Task for SpeechLLMs</h3>
<ul>
<li><strong>Authors: </strong>Kadri Hacioglu, Manjunath K E, Andreas Stolcke</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19326">https://arxiv.org/abs/2510.19326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19326">https://arxiv.org/pdf/2510.19326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19326]] Slot Filling as a Reasoning Task for SpeechLLMs(https://arxiv.org/abs/2510.19326)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We propose integration of reasoning into speech large language models (speechLLMs) for the end-to-end slot-filling task. Inspired by the recent development of reasoning LLMs, we use a chain-of-thought framework to decompose the slot-filling task into multiple reasoning steps, create a reasoning dataset and apply the supervised fine-tuning strategy to a speechLLM. We distinguish between regular and reasoning speechLLMs and experiment with different types and sizes of LLMs as their text foundation models. We demonstrate performance improvements by introducing reasoning (intermediate) steps. However, we show that a reasoning textual LLM developed mainly for math, logic and coding domains might be inferior as a foundation model for a reasoning speechLLM. We further show that hybrid speechLLMs, built on a hybrid text foundation LLM and fine-tuned to preserve both direct and reasoning modes of operation, have better performance than those fine-tuned employing only one mode of operation.</li>
</ul>

<h3>Title: Foundation Model Forecasts: Form and Function</h3>
<ul>
<li><strong>Authors: </strong>Alvaro Perez-Diaz, James C. Loach, Danielle E. Toutoungi, Lee Middleton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19345">https://arxiv.org/abs/2510.19345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19345">https://arxiv.org/pdf/2510.19345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19345]] Foundation Model Forecasts: Form and Function(https://arxiv.org/abs/2510.19345)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet accuracy alone does not determine practical value. The form of a forecast -- point, quantile, parametric, or trajectory ensemble -- fundamentally constrains which operational tasks it can support. We survey recent TSFMs and find that two-thirds produce only point or parametric forecasts, while many operational tasks require trajectory ensembles that preserve temporal dependence. We establish when forecast types can be converted and when they cannot: trajectory ensembles convert to simpler forms via marginalization without additional assumptions, but the reverse requires imposing temporal dependence through copulas or conformal methods. We prove that marginals cannot determine path-dependent event probabilities -- infinitely many joint distributions share identical marginals but yield different answers to operational questions. We map six fundamental forecasting tasks to minimal sufficient forecast types and provide a task-aligned evaluation framework. Our analysis clarifies when forecast type, not accuracy, differentiates practical utility.</li>
</ul>

<h3>Title: Optimization Benchmark for Diffusion Models on Dynamical Systems</h3>
<ul>
<li><strong>Authors: </strong>Fabian Schaipp</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19376">https://arxiv.org/abs/2510.19376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19376">https://arxiv.org/pdf/2510.19376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19376]] Optimization Benchmark for Diffusion Models on Dynamical Systems(https://arxiv.org/abs/2510.19376)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The training of diffusion models is often absent in the evaluation of new optimization techniques. In this work, we benchmark recent optimization algorithms for training a diffusion model for denoising flow trajectories. We observe that Muon and SOAP are highly efficient alternatives to AdamW (18% lower final loss). We also revisit several recent phenomena related to the training of models for text or image applications in the context of diffusion model training. This includes the impact of the learning-rate schedule on the training dynamics, and the performance gap between Adam and SGD.</li>
</ul>

<h3>Title: Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19384">https://arxiv.org/abs/2510.19384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19384">https://arxiv.org/pdf/2510.19384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19384]] Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment(https://arxiv.org/abs/2510.19384)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs) is central to web-scale applications such as search, recommendation, and knowledge discovery. However, existing CLIP-style graph-text aligners face two key limitations: they assume strict one-to-one correspondences between nodes and texts, overlooking the inherent many-to-many relations in real-world graphs; and they rely on static alignment objectives that cannot adapt to varying data quality, making them brittle under noisy supervision. Together, these limitations expose a core dilemma: embracing expressive many-to-many alignment amplifies noise, while reverting to strict one-to-one strategies sacrifices semantic diversity and fails to handle inherently mismatched pairs. To address these challenges, we propose ADAligner, a dynamic, quality-aware graph-text alignment framework that dynamically adjusts between expressive many-to-many and conservative one-to-one objectives according to supervision quality. ADAligner estimates batch-level alignment reliability in real time and adapts its optimization accordingly, promoting soft, subgraph-level many-to-many alignment when supervision is clean, while emphasizing reliable one-to-one alignment by dynamically filtering low-confidence pairs under noise. Theoretically, we prove that this dynamic mechanism forms a stable negative feedback process, ensuring convergence and robustness. Comprehensive experiments on nine diverse TAG datasets demonstrate that ADAligner consistently outperforms prior graph-text aligners on zero-/few-shot node classification, link prediction and cross-modal retrieval tasks. It maintains strong robustness under noisy supervision and accelerates pre-training by approximately 2 to 3 times compared to multimodal baselines, establishing a scalable and reliable foundation for graph-text representation learning in real-world web environments.</li>
</ul>

<h3>Title: PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Ali Sadeghkhani, Brandon Bennett, Masoud Babaei, Arash Rabbani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19465">https://arxiv.org/abs/2510.19465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19465">https://arxiv.org/pdf/2510.19465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19465]] PCP-GAN: Property-Constrained Pore-scale image reconstruction via conditional Generative Adversarial Networks(https://arxiv.org/abs/2510.19465)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Obtaining truly representative pore-scale images that match bulk formation properties remains a fundamental challenge in subsurface characterization, as natural spatial heterogeneity causes extracted sub-images to deviate significantly from core-measured values. This challenge is compounded by data scarcity, where physical samples are only available at sparse well locations. This study presents a multi-conditional Generative Adversarial Network (cGAN) framework that generates representative pore-scale images with precisely controlled properties, addressing both the representativeness challenge and data availability constraints. The framework was trained on thin section samples from four depths (1879.50-1943.50 m) of a carbonate formation, simultaneously conditioning on porosity values and depth parameters within a single unified model. This approach captures both universal pore network principles and depth-specific geological characteristics, from grainstone fabrics with interparticle-intercrystalline porosity to crystalline textures with anhydrite inclusions. The model achieved exceptional porosity control (R^2=0.95) across all formations with mean absolute errors of 0.0099-0.0197. Morphological validation confirmed preservation of critical pore network characteristics including average pore radius, specific surface area, and tortuosity, with statistical differences remaining within acceptable geological tolerances. Most significantly, generated images demonstrated superior representativeness with dual-constraint errors of 1.9-11.3% compared to 36.4-578% for randomly extracted real sub-images. This capability provides transformative tools for subsurface characterization, particularly valuable for carbon storage, geothermal energy, and groundwater management applications where knowing the representative morphology of the pore space is critical for implementing digital rock physics.</li>
</ul>

<h3>Title: Predicting before Reconstruction: A generative prior framework for MRI acceleration</h3>
<ul>
<li><strong>Authors: </strong>Juhyung Park, Rokgi Hong, Roh-Eul Yoo, Jaehyeon Koo, Se Young Chun, Seung Hong Choi, Jongho Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19472">https://arxiv.org/abs/2510.19472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19472">https://arxiv.org/pdf/2510.19472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19472]] Predicting before Reconstruction: A generative prior framework for MRI acceleration(https://arxiv.org/abs/2510.19472)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in artificial intelligence have created transformative capabilities in image synthesis and generation, enabling diverse research fields to innovate at revolutionary speed and spectrum. In this study, we leverage this generative power to introduce a new paradigm for accelerating Magnetic Resonance Imaging (MRI), introducing a shift from image reconstruction to proactive predictive imaging. Despite being a cornerstone of modern patient care, MRI's lengthy acquisition times limit clinical throughput. Our novel framework addresses this challenge by first predicting a target contrast image, which then serves as a data-driven prior for reconstructing highly under-sampled data. This informative prior is predicted by a generative model conditioned on diverse data sources, such as other contrast images, previously scanned images, acquisition parameters, patient information. We demonstrate this approach with two key applications: (1) reconstructing FLAIR images using predictions from T1w and/or T2w scans, and (2) reconstructing T1w images using predictions from previously acquired T1w scans. The framework was evaluated on internal and multiple public datasets (total 14,921 scans; 1,051,904 slices), including multi-channel k-space data, for a range of high acceleration factors (x4, x8 and x12). The results demonstrate that our prediction-prior reconstruction method significantly outperforms other approaches, including those with alternative or no prior information. Through this framework we introduce a fundamental shift from image reconstruction towards a new paradigm of predictive imaging.</li>
</ul>

<h3>Title: Which Evaluation for Which Model? A Taxonomy for Speech Model Assessment</h3>
<ul>
<li><strong>Authors: </strong>Maureen de Seyssel, Eeshan Gunesh Dhekane</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19509">https://arxiv.org/abs/2510.19509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19509">https://arxiv.org/pdf/2510.19509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19509]] Which Evaluation for Which Model? A Taxonomy for Speech Model Assessment(https://arxiv.org/abs/2510.19509)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Speech foundation models have recently achieved remarkable capabilities across a wide range of tasks. However, their evaluation remains disjointed across tasks and model types. Different models excel at distinct aspects of speech processing and thus require different evaluation protocols. This paper proposes a unified taxonomy that addresses the question: Which evaluation is appropriate for which model? The taxonomy defines three orthogonal axes: the \textbf{evaluation aspect} being measured, the model capabilities required to attempt the task, and the task or protocol requirements needed to perform it. We classify a broad set of existing evaluations and benchmarks along these axes, spanning areas such as representation learning, speech generation, and interactive dialogue. By mapping each evaluation to the capabilities a model exposes (e.g., speech generation, real-time processing) and to its methodological demands (e.g., fine-tuning data, human judgment), the taxonomy provides a principled framework for aligning models with suitable evaluation methods. It also reveals systematic gaps, such as limited coverage of prosody, interaction, or reasoning, that highlight priorities for future benchmark design. Overall, this work offers a conceptual foundation and practical guide for selecting, interpreting, and extending evaluations of speech models.</li>
</ul>

<h3>Title: Conditions for Catastrophic Forgetting in Multilingual Translation</h3>
<ul>
<li><strong>Authors: </strong>Danni Liu, Jan Niehues</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19546">https://arxiv.org/abs/2510.19546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19546">https://arxiv.org/pdf/2510.19546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19546]] Conditions for Catastrophic Forgetting in Multilingual Translation(https://arxiv.org/abs/2510.19546)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Fine-tuning multilingual foundation models on specific languages often induces catastrophic forgetting, degrading performance on languages unseen in fine-tuning. While this phenomenon is widely-documented, the literature presents fragmented results about when forgetting occurs. To address this ambiguity, we conduct a systematic empirical study using machine translation as a testbed to identify the conditions that trigger catastrophic forgetting in multilingual fine-tuning. Through controlled experiments across different model architectures, data scales, and fine-tuning approaches, we reveal that the relative scale between model and data size is a primary determinant of forgetting. Moreover, we demonstrate that a model's instruction-following ability is more critical for retaining multilingual knowledge than its architecture. Contrary to assumptions, parameter-efficient fine-tuning offers no clear advantage over full fine-tuning in mitigating forgetting. Lastly, we show that cross-lingual alignment can mitigate forgetting while also facilitating positive transfer to unseen target languages.</li>
</ul>

<h3>Title: The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Zhang, Aaron Courville, Michal Drozdzal, Adriana Romero-Soriano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19557">https://arxiv.org/abs/2510.19557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19557">https://arxiv.org/pdf/2510.19557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19557]] The Intricate Dance of Prompt Complexity, Quality, Diversity, and Consistency in T2I Models(https://arxiv.org/abs/2510.19557)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) models offer great potential for creating virtually limitless synthetic data, a valuable resource compared to fixed and finite real datasets. Previous works evaluate the utility of synthetic data from T2I models on three key desiderata: quality, diversity, and consistency. While prompt engineering is the primary means of interacting with T2I models, the systematic impact of prompt complexity on these critical utility axes remains underexplored. In this paper, we first conduct synthetic experiments to motivate the difficulty of generalization w.r.t. prompt complexity and explain the observed difficulty with theoretical derivations. Then, we introduce a new evaluation framework that can compare the utility of real data and synthetic data, and present a comprehensive analysis of how prompt complexity influences the utility of synthetic data generated by commonly used T2I models. We conduct our study across diverse datasets, including CC12M, ImageNet-1k, and DCI, and evaluate different inference-time intervention methods. Our synthetic experiments show that generalizing to more general conditions is harder than the other way round, since the former needs an estimated likelihood that is not learned by diffusion models. Our large-scale empirical experiments reveal that increasing prompt complexity results in lower conditional diversity and prompt consistency, while reducing the synthetic-to-real distribution shift, which aligns with the synthetic experiments. Moreover, current inference-time interventions can augment the diversity of the generations at the expense of moving outside the support of real data. Among those interventions, prompt expansion, by deliberately using a pre-trained language model as a likelihood estimator, consistently achieves the highest performance in both image diversity and aesthetics, even higher than that of real data.</li>
</ul>

<h3>Title: Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Yu Wu, Ke Shu, Jonas Fischer, Lidia Pivovarova, David Rosson, Eetu Mkel, Mikko Tolonen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19585">https://arxiv.org/abs/2510.19585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19585">https://arxiv.org/pdf/2510.19585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19585]] Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark(https://arxiv.org/abs/2510.19585)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This paper presents a novel task of extracting Latin fragments from mixed-language historical documents with varied layouts. We benchmark and evaluate the performance of large foundation models against a multimodal dataset of 724 annotated pages. The results demonstrate that reliable Latin detection with contemporary models is achievable. Our study provides the first comprehensive analysis of these models' capabilities and limits for this task.</li>
</ul>

<h3>Title: CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization</h3>
<ul>
<li><strong>Authors: </strong>Zhou Lei, Pan Gang, Wang Jiahao, Sun Di</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19597">https://arxiv.org/abs/2510.19597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19597">https://arxiv.org/pdf/2510.19597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19597]] CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery Localization(https://arxiv.org/abs/2510.19597)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment.</li>
</ul>

<h3>Title: Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism</h3>
<ul>
<li><strong>Authors: </strong>Junfei Zhou, Penglin Dai, Quanmin Wei, Bingyi Liu, Xiao Wu, Jianping Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19618">https://arxiv.org/abs/2510.19618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19618">https://arxiv.org/pdf/2510.19618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19618]] Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism(https://arxiv.org/abs/2510.19618)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Multi-agent collaboration enhances the perception capabilities of individual agents through information sharing. However, in real-world applications, differences in sensors and models across heterogeneous agents inevitably lead to domain gaps during collaboration. Existing approaches based on adaptation and reconstruction fail to support pragmatic heterogeneous collaboration due to two key limitations: (1) Intrusive retraining of the encoder or core modules disrupts the established semantic consistency among agents; and (2) accommodating new agents incurs high computational costs, limiting scalability. To address these challenges, we present a novel Generative Communication mechanism (GenComm) that facilitates seamless perception across heterogeneous multi-agent systems through feature generation, without altering the original network, and employs lightweight numerical alignment of spatial information to efficiently integrate new agents at minimal cost. Specifically, a tailored Deformable Message Extractor is designed to extract spatial message for each collaborator, which is then transmitted in place of intermediate features. The Spatial-Aware Feature Generator, utilizing a conditional diffusion model, generates features aligned with the ego agent's semantic space while preserving the spatial information of the collaborators. These generated features are further refined by a Channel Enhancer before fusion. Experiments conducted on the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm outperforms existing state-of-the-art methods, achieving an 81\% reduction in both computational cost and parameter count when incorporating new agents. Our code is available at this https URL.</li>
</ul>

<h3>Title: Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jin Han, Zhe Zheng, Yi Gu, Jia-Rui Lin, Xin-Zheng Lu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19623">https://arxiv.org/abs/2510.19623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19623">https://arxiv.org/pdf/2510.19623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19623]] Learning and Simulating Building Evacuation Patterns for Enhanced Safety Design Using Generative Models(https://arxiv.org/abs/2510.19623)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Evacuation simulation is essential for building safety design, ensuring properly planned evacuation routes. However, traditional evacuation simulation relies heavily on refined modeling with extensive parameters, making it challenging to adopt such methods in a rapid iteration process in early design stages. Thus, this study proposes DiffEvac, a novel method to learn building evacuation patterns based on Generative Models (GMs), for efficient evacuation simulation and enhanced safety design. Initially, a dataset of 399 diverse functional layouts and corresponding evacuation heatmaps of buildings was established. Then, a decoupled feature representation is proposed to embed physical features like layouts and occupant density for GMs. Finally, a diffusion model based on image prompts is proposed to learn evacuation patterns from simulated evacuation heatmaps. Compared to existing research using Conditional GANs with RGB representation, DiffEvac achieves up to a 37.6% improvement in SSIM, 142% in PSNR, and delivers results 16 times faster, thereby cutting simulation time to 2 minutes. Case studies further demonstrate that the proposed method not only significantly enhances the rapid design iteration and adjustment process with efficient evacuation simulation but also offers new insights and technical pathways for future safety optimization in intelligent building design. The research implication is that the approach lowers the modeling burden, enables large-scale what-if exploration, and facilitates coupling with multi-objective design tools.</li>
</ul>

<h3>Title: Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them</h3>
<ul>
<li><strong>Authors: </strong>Hrittik Roy, Sren Hauberg, Nicholas Krmer</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19634">https://arxiv.org/abs/2510.19634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19634">https://arxiv.org/pdf/2510.19634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19634]] Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them(https://arxiv.org/abs/2510.19634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper argues that the method of least squares has significant unfulfilled potential in modern machine learning, far beyond merely being a tool for fitting linear models. To release its potential, we derive custom gradients that transform the solver into a differentiable operator, like a neural network layer, enabling many diverse applications. Empirically, we demonstrate: (i) scalability by enforcing weight sparsity on a 50 million parameter model; (ii) imposing conservativeness constraints in score-based generative models; and (iii) hyperparameter tuning of Gaussian processes based on predictive performance. By doing this, our work represents the next iteration in developing differentiable linear-algebra tools and making them widely accessible to machine learning practitioners.</li>
</ul>

<h3>Title: CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakage</h3>
<ul>
<li><strong>Authors: </strong>Nowfel Mashnoor, Mohammad Akyash, Hadi Kamali, Kimia Azar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19676">https://arxiv.org/abs/2510.19676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19676">https://arxiv.org/pdf/2510.19676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19676]] CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakage(https://arxiv.org/abs/2510.19676)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved remarkable success in generative tasks, including register-transfer level (RTL) hardware synthesis. However, their tendency to memorize training data poses critical risks when proprietary or security-sensitive designs are unintentionally exposed during inference. While prior work has examined memorization in natural language, RTL introduces unique challenges: In RTL, structurally different implementations (e.g., behavioral vs. gate-level descriptions) can realize the same hardware, leading to intellectual property (IP) leakage (full or partial) even without verbatim overlap. Conversely, even small syntactic variations (e.g., operator precedence or blocking vs. non-blocking assignments) can drastically alter circuit behavior, making correctness preservation especially challenging. In this work, we systematically study memorization in RTL code generation and propose CircuitGuard, a defense strategy that balances leakage reduction with correctness preservation. CircuitGuard (1) introduces a novel RTL-aware similarity metric that captures both structural and functional equivalence beyond surface-level overlap, and (2) develops an activation-level steering method that identifies and attenuates transformer components most responsible for memorization. Our empirical evaluation demonstrates that CircuitGuard identifies (and isolates) 275 memorization-critical features across layers 18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic similarity to proprietary patterns while maintaining generation quality. CircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling robust memorization mitigation across circuit categories without retraining.</li>
</ul>

<h3>Title: Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Cesar Gonzalez-Gutierrez, Dirk Hovy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19694">https://arxiv.org/abs/2510.19694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19694">https://arxiv.org/pdf/2510.19694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19694]] Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings(https://arxiv.org/abs/2510.19694)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Prompting is a common approach for leveraging LMs in zero-shot settings. However, the underlying mechanisms that enable LMs to perform diverse tasks without task-specific supervision remain poorly understood. Studying the relationship between prompting and the quality of internal representations can shed light on how pre-trained embeddings may support in-context task solving. In this empirical study, we conduct a series of probing experiments on prompt embeddings, analyzing various combinations of prompt templates for zero-shot classification. Our findings show that while prompting affects the quality of representations, these changes do not consistently correlate with the relevance of the prompts to the target task. This result challenges the assumption that more relevant prompts necessarily lead to better representations. We further analyze potential factors that may contribute to this unexpected behavior.</li>
</ul>

<h3>Title: SEMPO: Lightweight Foundation Models for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Hui He, Kun Yi, Yuanchi Ma, Qi Zhang, Zhendong Niu, Guansong Pang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19710">https://arxiv.org/abs/2510.19710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19710">https://arxiv.org/pdf/2510.19710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19710]] SEMPO: Lightweight Foundation Models for Time Series Forecasting(https://arxiv.org/abs/2510.19710)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The recent boom of large pre-trained models witnesses remarkable success in developing foundation models (FMs) for time series forecasting. Despite impressive performance across diverse downstream forecasting tasks, existing time series FMs possess massive network architectures and require substantial pre-training on large-scale datasets, which significantly hinders their deployment in resource-constrained environments. In response to this growing tension between versatility and affordability, we propose SEMPO, a novel lightweight foundation model that requires pretraining on relatively small-scale data, yet exhibits strong general time series forecasting. Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctral decomposition module, that substantially improves the utilization of pre-training data by modeling not only the high-energy frequency signals but also the low-energy yet informative frequency signals that are ignored in current methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns heterogeneous temporal patterns through small dataset-specific prompts and adaptively routes time series tokens to prompt-based experts for parameter-efficient model adaptation across different datasets and domains. Equipped with these modules, SEMPO significantly reduces both pre-training data scale and model size, while achieving strong generalization. Extensive experiments on two large-scale benchmarks covering 16 datasets demonstrate the superior performance of SEMPO in both zero-shot and few-shot forecasting scenarios compared with state-of-the-art methods. Code and data are available at this https URL.</li>
</ul>

<h3>Title: Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Ibrahim, Bart Elen, Chang Sun, Gkhan Ertaylan, Michel Dumontier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19728">https://arxiv.org/abs/2510.19728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19728">https://arxiv.org/pdf/2510.19728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19728]] Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series(https://arxiv.org/abs/2510.19728)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a novel framework for leveraging synthetic ICU time-series data not only to train but also to rigorously and trustworthily evaluate predictive models, both at the population level and within fine-grained demographic subgroups. Building on prior diffusion and VAE-based generators (TimeDiff, HealthGen, TimeAutoDiff), we introduce \textit{Enhanced TimeAutoDiff}, which augments the latent diffusion objective with distribution-alignment penalties. We extensively benchmark all models on MIMIC-III and eICU, on 24-hour mortality and binary length-of-stay tasks. Our results show that Enhanced TimeAutoDiff reduces the gap between real-on-synthetic and real-on-real evaluation (``TRTS gap'') by over 70\%, achieving $\Delta_{TRTS} \leq 0.014$ AUROC, while preserving training utility ($\Delta_{TSTR} \approx 0.01$). Crucially, for 32 intersectional subgroups, large synthetic cohorts cut subgroup-level AUROC estimation error by up to 50\% relative to small real test sets, and outperform them in 72--84\% of subgroups. This work provides a practical, privacy-preserving roadmap for trustworthy, granular model evaluation in critical care, enabling robust and reliable performance analysis across diverse patient populations without exposing sensitive EHR data, contributing to the overall trustworthiness of Medical AI.</li>
</ul>

<h3>Title: A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19755">https://arxiv.org/abs/2510.19755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19755">https://arxiv.org/pdf/2510.19755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19755]] A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation(https://arxiv.org/abs/2510.19755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation. Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis. Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.</li>
</ul>

<h3>Title: Adapting Multilingual Models to Code-Mixed Tasks via Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Prashant Kodali, Vaishnavi Shivkumar, Swarang Joshi, Monojit Choudhary, Ponnurangam Kumaraguru, Manish Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19782">https://arxiv.org/abs/2510.19782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19782">https://arxiv.org/pdf/2510.19782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19782]] Adapting Multilingual Models to Code-Mixed Tasks via Model Merging(https://arxiv.org/abs/2510.19782)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We study model merging as a practical alternative to conventional adaptation strategies for code-mixed NLP. Starting from a multilingual base model, we: (i) perform continued pre-training (CPT) on unlabeled code-mixed text to obtain an adapted checkpoint, (ii) merge checkpoint with the base model, and (iii) fine-tune (FT) on the downstream task data. We evaluate our approach for sentence classification (sentiment and hate speech) task in English-Hindi (En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our results show that merged models consistently outperform full fine-tuning and CPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2 points over CPT->FT, indicating that unlabeled data is leveraged more effectively via merging than via CPT alone. Zero-/few-shot prompting with larger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged checkpoints, underscoring limits of in-context learning for code-mixed inputs. We further test cross-pair transfer by training on En-Hi and evaluating on En-Ta and En-Ml: merged checkpoints transfer more strongly than monolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs 0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more reliable substrate for low-resource pairs. We conclude with adaptation recipes matched to common data regimes (labeled only; labeled+unlabeled; transfer-only) and discuss limitations and scaling considerations for broader tasks and larger models.</li>
</ul>

<h3>Title: OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19789">https://arxiv.org/abs/2510.19789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19789">https://arxiv.org/pdf/2510.19789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19789]] OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation(https://arxiv.org/abs/2510.19789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</li>
</ul>

<h3>Title: Transformers are almost optimal metalearners for linear classification</h3>
<ul>
<li><strong>Authors: </strong>Roey Magen, Gal Vardi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.19797">https://arxiv.org/abs/2510.19797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.19797">https://arxiv.org/pdf/2510.19797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.19797]] Transformers are almost optimal metalearners for linear classification(https://arxiv.org/abs/2510.19797)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Transformers have demonstrated impressive in-context learning (ICL) capabilities, raising the question of whether they can serve as metalearners that adapt to new tasks using only a small number of in-context examples, without any further training. While recent theoretical work has studied transformers' ability to perform ICL, most of these analyses do not address the formal metalearning setting, where the objective is to solve a collection of related tasks more efficiently than would be possible by solving each task individually. In this paper, we provide the first theoretical analysis showing that a simplified transformer architecture trained via gradient descent can act as a near-optimal metalearner in a linear classification setting. We consider a natural family of tasks where each task corresponds to a class-conditional Gaussian mixture model, with the mean vectors lying in a shared $k$-dimensional subspace of $R^d$. After training on a sufficient number of such tasks, we show that the transformer can generalize to a new task using only $O(k / R^4)$ in-context examples, where $R$ denotes the signal strength at test time. This performance (almost) matches that of an optimal learner that knows exactly the shared subspace and significantly outperforms any learner that only has access to the in-context data, which requires $\Omega(d / R^4)$ examples to generalize. Importantly, our bounds on the number of training tasks and examples per task needed to achieve this result are independent of the ambient dimension $d$.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
