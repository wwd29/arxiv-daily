<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-05</h1>
<h3>Title: On the Challenges and Opportunities in Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Laura Manduchi, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina Däubener, Sophie Fellenz, Asja Fischer, Thomas Gärtner, Matthias Kirchler, Marius Kloft, Yingzhen Li, Christoph Lippert, Gerard de Melo, Eric Nalisnick, Björn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich, Guy Van den Broeck, Julia E Vogt, Yixin Wang, Florian Wenzel, Frank Wood, Stephan Mandt, Vincent Fortuin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00025">https://arxiv.org/abs/2403.00025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00025">https://arxiv.org/pdf/2403.00025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00025]] On the Challenges and Opportunities in Generative AI(https://arxiv.org/abs/2403.00025)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI solutions.</li>
</ul>

<h3>Title: Learning to Deliver: a Foundation Model for the Montreal Capacitated  Vehicle Routing Problem</h3>
<ul>
<li><strong>Authors: </strong>Samuel J. K. Chin, Matthias Winkenbach, Akash Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00026">https://arxiv.org/abs/2403.00026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00026">https://arxiv.org/pdf/2403.00026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00026]] Learning to Deliver: a Foundation Model for the Montreal Capacitated  Vehicle Routing Problem(https://arxiv.org/abs/2403.00026)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In this paper, we present the Foundation Model for the Montreal Capacitated Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that approximates high-quality solutions to a variant of the Capacitated Vehicle Routing Problem (CVRP) that characterizes many real-world applications. The so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally described by Bengio et al. (2021), is defined on a fixed and finite graph, which is analogous to a city. Each MCVRP instance is essentially the sub-graph connecting a randomly sampled subset of the nodes in the fixed graph, which represent a set of potential addresses in a real-world delivery problem on a given day. Our work exploits this problem structure to frame the MCVRP as an analogous Natural Language Processing (NLP) task. Specifically, we leverage a Transformer architecture embedded in a Large Language Model (LLM) framework to train our model in a supervised manner on computationally inexpensive, sub-optimal MCVRP solutions obtained algorithmically. Through comprehensive computational experiments, we show that FM-MCVRP produces better MCVRP solutions than the training data and generalizes to larger sized problem instances not seen during training. Even when compared to near-optimal solutions from state-of-the-art heuristics, FM-MCVRP yields competitive results despite being trained on inferior data. For instance, for 400-customer problems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our results further demonstrate that unlike prior works in the literature, FM-MCVRP is a unified model, which performs consistently and reliably on a range of problem instance sizes and parameter values such as the vehicle capacity.</li>
</ul>

<h3>Title: UniTS: Building a Unified Time Series Model</h3>
<ul>
<li><strong>Authors: </strong>Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros Tsiligkaridis, Marinka Zitnik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00131">https://arxiv.org/abs/2403.00131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00131">https://arxiv.org/pdf/2403.00131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00131]] UniTS: Building a Unified Time Series Model(https://arxiv.org/abs/2403.00131)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrates superior performance compared to task-specific models and repurposed natural language-based LLMs. UNITS exhibits remarkable zero-shot, few-shot, and prompt learning capabilities when evaluated on new data domains and tasks. The source code and datasets are available at https://github.com/mims-harvard/UniTS.</li>
</ul>

<h3>Title: Non-Invasive Medical Digital Twins using Physics-Informed  Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Keying Kuang, Frances Dean, Jack B. Jedlicki, David Ouyang, Anthony Philippakis, David Sontag, Ahmed M. Alaa</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00177">https://arxiv.org/abs/2403.00177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00177">https://arxiv.org/pdf/2403.00177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00177]] Non-Invasive Medical Digital Twins using Physics-Informed  Self-Supervised Learning(https://arxiv.org/abs/2403.00177)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of solving the physical model equations. Subsequently, the model is trained to reconstruct low-dimensional health measurements from noninvasive modalities while being constrained by the physical equations learned in pretraining. We apply our method to identify digital twins of cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate its utility in unsupervised disease detection and in-silico clinical trials.</li>
</ul>

<h3>Title: Learning to Find Missing Video Frames with Synthetic Data Augmentation:  A General Framework and Application in Generating Thermal Images Using RGB  Cameras</h3>
<ul>
<li><strong>Authors: </strong>Mathias Viborg Andersen, Ross Greer, Andreas Møgelmose, Mohan Trivedi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00196">https://arxiv.org/abs/2403.00196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00196">https://arxiv.org/pdf/2403.00196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00196]] Learning to Find Missing Video Frames with Synthetic Data Augmentation:  A General Framework and Application in Generating Thermal Images Using RGB  Cameras(https://arxiv.org/abs/2403.00196)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on accurate driver perception within the vehicle cabin, often leveraging a combination of sensing modalities. However, these modalities operate at varying rates, posing challenges for real-time, comprehensive driver state monitoring. This paper addresses the issue of missing data due to sensor frame rate mismatches, introducing a generative model approach to create synthetic yet realistic thermal imagery. We propose using conditional generative adversarial networks (cGANs), specifically comparing the pix2pix and CycleGAN architectures. Experimental results demonstrate that pix2pix outperforms CycleGAN, and utilizing multi-view input styles, especially stacked views, enhances the accuracy of thermal image generation. Moreover, the study evaluates the model's generalizability across different subjects, revealing the importance of individualized training for optimal performance. The findings suggest the potential of generative models in addressing missing frames, advancing driver state monitoring for intelligent vehicles, and underscoring the need for continued research in model generalization and customization.</li>
</ul>

<h3>Title: MaskLRF: Self-supervised Pretraining via Masked Autoencoding of Local  Reference Frames for Rotation-invariant 3D Point Set Analysis</h3>
<ul>
<li><strong>Authors: </strong>Takahiko Furuya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00206">https://arxiv.org/abs/2403.00206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00206">https://arxiv.org/pdf/2403.00206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00206]] MaskLRF: Self-supervised Pretraining via Masked Autoencoding of Local  Reference Frames for Rotation-invariant 3D Point Set Analysis(https://arxiv.org/abs/2403.00206)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Following the successes in the fields of vision and language, self-supervised pretraining via masked autoencoding of 3D point set data, or Masked Point Modeling (MPM), has achieved state-of-the-art accuracy in various downstream tasks. However, current MPM methods lack a property essential for 3D point set analysis, namely, invariance against rotation of 3D objects/scenes. Existing MPM methods are thus not necessarily suitable for real-world applications where 3D point sets may have inconsistent orientations. This paper develops, for the first time, a rotation-invariant self-supervised pretraining framework for practical 3D point set analysis. The proposed algorithm, called MaskLRF, learns rotation-invariant and highly generalizable latent features via masked autoencoding of 3D points within Local Reference Frames (LRFs), which are not affected by rotation of 3D point sets. MaskLRF enhances the quality of latent features by integrating feature refinement using relative pose encoding and feature reconstruction using low-level but rich 3D geometry. The efficacy of MaskLRF is validated via extensive experiments on diverse downstream tasks including classification, segmentation, registration, and domain adaptation. I confirm that MaskLRF achieves new state-of-the-art accuracies in analyzing 3D point sets having inconsistent orientations. Code will be available at: https://github.com/takahikof/MaskLRF</li>
</ul>

<h3>Title: Transcription and translation of videos using fine-tuned XLSR Wav2Vec2  on custom dataset and mBART</h3>
<ul>
<li><strong>Authors: </strong>Aniket Tathe, Anand Kamble, Suyash Kumbharkar, Atharva Bhandare, Anirban C. Mitra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00212">https://arxiv.org/abs/2403.00212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00212">https://arxiv.org/pdf/2403.00212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00212]] Transcription and translation of videos using fine-tuned XLSR Wav2Vec2  on custom dataset and mBART(https://arxiv.org/abs/2403.00212)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.</li>
</ul>

<h3>Title: Robust Policy Learning via Offline Skill Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Woo Kyung Kim, Minjong Yoo, Honguk Woo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00225">https://arxiv.org/abs/2403.00225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00225">https://arxiv.org/pdf/2403.00225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00225]] Robust Policy Learning via Offline Skill Diffusion(https://arxiv.org/abs/2403.00225)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embedding space into two distinct representations, one for encapsulating domain-invariant behaviors and the other for delineating the factors that induce domain variations in the behaviors. Our DuSkill framework enhances the diversity of skills learned offline, thus enabling to accelerate the learning procedure of high-level policies for different domains. Through experiments, we show that DuSkill outperforms other skill-based imitation learning and RL algorithms for several long-horizon tasks, demonstrating its benefits in few-shot imitation and online RL.</li>
</ul>

<h3>Title: A Semantic Distance Metric Learning approach for Lexical Semantic Change  Detection</h3>
<ul>
<li><strong>Authors: </strong>Taichi Aida, Danushka Bollegala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00226">https://arxiv.org/abs/2403.00226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00226">https://arxiv.org/pdf/2403.00226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00226]] A Semantic Distance Metric Learning approach for Lexical Semantic Change  Detection(https://arxiv.org/abs/2403.00226)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Detecting temporal semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. Lexical Semantic Change Detection (SCD) task considers the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $C_1$ and $C_2$. For this purpose, we propose a supervised two-staged SCD method that uses existing Word-in-Context (WiC) datasets. In the first stage, for a target word $w$, we learn two sense-aware encoder that represents the meaning of $w$ in a given sentence selected from a corpus. Next, in the second stage, we learn a sense-aware distance metric that compares the semantic representations of a target word across all of its occurrences in $C_1$ and $C_2$. Experimental results on multiple benchmark datasets for SCD show that our proposed method consistently outperforms all previously proposed SCD methods for multiple languages, establishing a novel state-of-the-art for SCD. Interestingly, our findings imply that there are specialised dimensions that carry information related to semantic changes of words in the sense-aware embedding space. Source code is available at https://github.com/a1da4/svp-sdml .</li>
</ul>

<h3>Title: Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language  Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Haowei Liu, Yaya Shi, Haiyang Xu, Chunfeng Yuan, Qinghao Ye, Chenliang Li, Ming Yan, Ji Zhang, Fei Huang, Bing Li, Weiming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00249">https://arxiv.org/abs/2403.00249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00249">https://arxiv.org/pdf/2403.00249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00249]] Semantics-enhanced Cross-modal Masked Image Modeling for Vision-Language  Pre-training(https://arxiv.org/abs/2403.00249)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In vision-language pre-training (VLP), masked image modeling (MIM) has recently been introduced for fine-grained cross-modal alignment. However, in most existing methods, the reconstruction targets for MIM lack high-level semantics, and text is not sufficiently involved in masked modeling. These two drawbacks limit the effect of MIM in facilitating cross-modal semantic alignment. In this work, we propose a semantics-enhanced cross-modal MIM framework (SemMIM) for vision-language representation learning. Specifically, to provide more semantically meaningful supervision for MIM, we propose a local semantics enhancing approach, which harvest high-level semantics from global image features via self-supervised agreement learning and transfer them to local patch encodings by sharing the encoding space. Moreover, to achieve deep involvement of text during the entire MIM process, we propose a text-guided masking strategy and devise an efficient way of injecting textual information in both masked modeling and reconstruction target acquisition. Experimental results validate that our method improves the effectiveness of the MIM task in facilitating cross-modal semantic alignment. Compared to previous VLP models with similar model size and data scale, our SemMIM model achieves state-of-the-art or competitive performance on multiple downstream vision-language tasks.</li>
</ul>

<h3>Title: Parameter-Efficient Tuning of Large Convolutional Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Chen, Zichen Miao, Qiang Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00269">https://arxiv.org/abs/2403.00269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00269">https://arxiv.org/pdf/2403.00269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00269]] Parameter-Efficient Tuning of Large Convolutional Models(https://arxiv.org/abs/2403.00269)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>To address the high computational and parameter complexity associated with fine-tuning large pre-trained models, researchers have developed parameter-efficient methods, where only partial parameters are updated for downstream tasks. However, these works often overlook the distinct properties of convolutional kernels, which still remain essential elements in many large models, such as Stable Diffusion. In this study, we first introduce filter subspace by decomposing convolutional kernels within each network layer over a small set of filter subspace elements, referred to as filter atoms. We then fine-tune these models to extract task-specific representation by only adapting the filter atoms, a few hundred parameters typically. To potentially expand the parameter space for tuning, we further show a simple approach to generate an overcomplete filter subspace by recursively decomposing each filter atom over another set of filter atoms. The fine-tuning of filter atoms reshapes the filter subspace, enabling convolutional layers to adapt to diverse downstream tasks efficiently. Extensive experiments show that such a simple scheme surpasses previous tuning baselines for both discriminate and generative tasks. Our approach can potentially be complementary to many existing fine-tuning methods.</li>
</ul>

<h3>Title: CustomListener: Text-guided Responsive Interaction for User-friendly  Listening Head Generation</h3>
<ul>
<li><strong>Authors: </strong>Xi Liu, Ying Guo, Cheng Zhen, Tong Li, Yingying Ao, Pengfei Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00274">https://arxiv.org/abs/2403.00274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00274">https://arxiv.org/pdf/2403.00274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00274]] CustomListener: Text-guided Responsive Interaction for User-friendly  Listening Head Generation(https://arxiv.org/abs/2403.00274)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Listening head generation aims to synthesize a non-verbal responsive listener head by modeling the correlation between the speaker and the listener in dynamic conversion.The applications of listener agent generation in virtual interaction have promoted many works achieving the diverse and fine-grained motion generation. However, they can only manipulate motions through simple emotional labels, but cannot freely control the listener's motions. Since listener agents should have human-like attributes (e.g. identity, personality) which can be freely customized by users, this limits their realism. In this paper, we propose a user-friendly framework called CustomListener to realize the free-form text prior guided listener generation. To achieve speaker-listener coordination, we design a Static to Dynamic Portrait module (SDP), which interacts with speaker information to transform static text into dynamic portrait token with completion rhythm and amplitude information. To achieve coherence between segments, we design a Past Guided Generation Module (PGG) to maintain the consistency of customized listener attributes through the motion prior, and utilize a diffusion-based structure conditioned on the portrait token and the motion prior to realize the controllable generation. To train and evaluate our model, we have constructed two text-annotated listening head datasets based on ViCo and RealTalk, which provide text-video paired labels. Extensive experiments have verified the effectiveness of our model.</li>
</ul>

<h3>Title: Nonlinear Sheaf Diffusion in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Olga Zaghen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00337">https://arxiv.org/abs/2403.00337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00337">https://arxiv.org/pdf/2403.00337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00337]] Nonlinear Sheaf Diffusion in Graph Neural Networks(https://arxiv.org/abs/2403.00337)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work focuses on exploring the potential benefits of introducing a nonlinear Laplacian in Sheaf Neural Networks for graph-related tasks. The primary aim is to understand the impact of such nonlinearity on diffusion dynamics, signal propagation, and performance of neural network architectures in discrete-time settings. The study primarily emphasizes experimental analysis, using real-world and synthetic datasets to validate the practical effectiveness of different versions of the model. This approach shifts the focus from an initial theoretical exploration to demonstrating the practical utility of the proposed model.</li>
</ul>

<h3>Title: Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity  for Abstract Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ruiqian Nai, Zixin Wen, Ji Li, Yuanzhi Li, Yang Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00352">https://arxiv.org/abs/2403.00352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00352">https://arxiv.org/pdf/2403.00352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00352]] Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity  for Abstract Visual Reasoning(https://arxiv.org/abs/2403.00352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In representation learning, a disentangled representation is highly desirable as it encodes generative factors of data in a separable and compact pattern. Researchers have advocated leveraging disentangled representations to complete downstream tasks with encouraging empirical evidence. This paper further investigates the necessity of disentangled representation in downstream applications. Specifically, we show that dimension-wise disentangled representations are unnecessary on a fundamental downstream task, abstract visual reasoning. We provide extensive empirical evidence against the necessity of disentanglement, covering multiple datasets, representation learning methods, and downstream network architectures. Furthermore, our findings suggest that the informativeness of representations is a better indicator of downstream performance than disentanglement. Finally, the positive correlation between informativeness and disentanglement explains the claimed usefulness of disentangled representations in previous works. The source code is available at https://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git.</li>
</ul>

<h3>Title: Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with  Extract-Then-Assign Strategy</h3>
<ul>
<li><strong>Authors: </strong>Jieyong Kim, Ryang Heo, Yongsik Seo, SeongKu Kang, Jinyoung Yeo, Dongha Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00354">https://arxiv.org/abs/2403.00354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00354">https://arxiv.org/pdf/2403.00354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00354]] Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with  Extract-Then-Assign Strategy(https://arxiv.org/abs/2403.00354)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the task of aspect sentiment quad prediction (ASQP), generative methods for predicting sentiment quads have shown promising results. However, they still suffer from imprecise predictions and limited interpretability, caused by data scarcity and inadequate modeling of the quadruplet composition process. In this paper, we propose Self-Consistent Reasoning-based Aspect-sentiment quadruple Prediction (SCRAP), optimizing its model to generate reasonings and the corresponding sentiment quadruplets in sequence. SCRAP adopts the Extract-Then-Assign reasoning strategy, which closely mimics human cognition. In the end, SCRAP significantly improves the model's ability to handle complex reasoning tasks and correctly predict quadruplets through consistency voting, resulting in enhanced interpretability and accuracy in ASQP.</li>
</ul>

<h3>Title: HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry  for Enhanced 3D Text2Shape Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhiying Leng, Tolga Birdal, Xiaohui Liang, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00372">https://arxiv.org/abs/2403.00372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00372">https://arxiv.org/pdf/2403.00372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00372]] HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry  for Enhanced 3D Text2Shape Generation(https://arxiv.org/abs/2403.00372)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D shape generation from text is a fundamental task in 3D representation learning. The text-shape pairs exhibit a hierarchical structure, where a general text like "chair" covers all 3D shapes of the chair, while more detailed prompts refer to more specific shapes. Furthermore, both text and 3D shapes are inherently hierarchical structures. However, existing Text2Shape methods, such as SDFusion, do not exploit that. In this work, we propose HyperSDFusion, a dual-branch diffusion model that generates 3D shapes from a given text. Since hyperbolic space is suitable for handling hierarchical data, we propose to learn the hierarchical representations of text and 3D shapes in hyperbolic space. First, we introduce a hyperbolic text-image encoder to learn the sequential and multi-modal hierarchical features of text in hyperbolic space. In addition, we design a hyperbolic text-graph convolution module to learn the hierarchical features of text in hyperbolic space. In order to fully utilize these text features, we introduce a dual-branch structure to embed text features in 3D feature space. At last, to endow the generated 3D shapes with a hierarchical structure, we devise a hyperbolic hierarchical loss. Our method is the first to explore the hyperbolic hierarchical representation for text-to-shape generation. Experimental results on the existing text-to-shape paired dataset, Text2Shape, achieved state-of-the-art results.</li>
</ul>

<h3>Title: Invariant Test-Time Adaptation for Vision-Language Model Generalization</h3>
<ul>
<li><strong>Authors: </strong>Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00376">https://arxiv.org/abs/2403.00376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00376">https://arxiv.org/pdf/2403.00376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00376]] Invariant Test-Time Adaptation for Vision-Language Model Generalization(https://arxiv.org/abs/2403.00376)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while disregarding decision shortcuts during the inference phase. The proposed method effectively alleviates excessive dependence on potentially misleading, task-irrelevant contextual information, while concurrently emphasizing critical, task-related visual cues. We conduct comparative analysis of the proposed method against various approaches which validates its effectiveness.</li>
</ul>

<h3>Title: Provably Robust DPO: Aligning Language Models with Noisy Feedback</h3>
<ul>
<li><strong>Authors: </strong>Sayak Ray Chowdhury, Anush Kini, Nagarajan Natarajan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00409">https://arxiv.org/abs/2403.00409</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00409">https://arxiv.org/pdf/2403.00409</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00409]] Provably Robust DPO: Aligning Language Models with Noisy Feedback(https://arxiv.org/abs/2403.00409)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive. In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising concerns about the impact of noisy data on the learned policy. We design a novel loss function, which de-bias the effect of noise on average, making a policy trained by minimizing that loss robust to the noise. Under log-linear parameterization of the policy class and assuming good feature coverage of the SFT policy, we prove that the sub-optimality gap of the proposed robust DPO (rDPO) policy compared to the optimal policy is of the order $O(\frac{1}{1-2\epsilon}\sqrt{\frac{d}{n}})$, where $\epsilon < 1/2$ is flip rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset. Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless dataset show that rDPO is robust to noise in preference labels compared to vanilla DPO and other heuristics proposed by practitioners.</li>
</ul>

<h3>Title: Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with  Fact-Checking in Turkish</h3>
<ul>
<li><strong>Authors: </strong>Recep Firat Cekinel, Pinar Karagoz, Cagri Coltekin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00411">https://arxiv.org/abs/2403.00411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00411">https://arxiv.org/pdf/2403.00411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00411]] Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with  Fact-Checking in Turkish(https://arxiv.org/abs/2403.00411)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.</li>
</ul>

<h3>Title: Data-efficient Event Camera Pre-training via Disentangled Masked  Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zhenpeng Huang, Chao Li, Hao Chen, Yongjian Deng, Yifeng Geng, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00416">https://arxiv.org/abs/2403.00416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00416">https://arxiv.org/pdf/2403.00416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00416]] Data-efficient Event Camera Pre-training via Disentangled Masked  Modeling(https://arxiv.org/abs/2403.00416)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In this paper, we present a new data-efficient voxel-based self-supervised learning method for event cameras. Our pre-training overcomes the limitations of previous methods, which either sacrifice temporal information by converting event sequences into 2D images for utilizing pre-trained image models or directly employ paired image data for knowledge distillation to enhance the learning of event streams. In order to make our pre-training data-efficient, we first design a semantic-uniform masking method to address the learning imbalance caused by the varying reconstruction difficulties of different regions in non-uniform data when using random masking. In addition, we ease the traditional hybrid masked modeling process by explicitly decomposing it into two branches, namely local spatio-temporal reconstruction and global semantic reconstruction to encourage the encoder to capture local correlations and global semantics, respectively. This decomposition allows our selfsupervised learning method to converge faster with minimal pre-training data. Compared to previous approaches, our self-supervised learning method does not rely on paired RGB images, yet enables simultaneous exploration of spatial and temporal cues in multiple scales. It exhibits excellent generalization performance and demonstrates significant improvements across various tasks with fewer parameters and lower computational costs.</li>
</ul>

<h3>Title: LLMs for Targeted Sentiment in News Headlines: Exploring Different  Levels of Prompt Prescriptiveness</h3>
<ul>
<li><strong>Authors: </strong>Jana Juroš, Laura Majer, Jan Šnajder</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00418">https://arxiv.org/abs/2403.00418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00418">https://arxiv.org/pdf/2403.00418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00418]] LLMs for Targeted Sentiment in News Headlines: Exploring Different  Levels of Prompt Prescriptiveness(https://arxiv.org/abs/2403.00418)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>News headlines often evoke sentiment by intentionally portraying entities in particular ways, making targeted sentiment analysis (TSA) of headlines a worthwhile but difficult task. Fine-tuned encoder models show satisfactory TSA performance, but their background knowledge is limited, and they require a labeled dataset. LLMs offer a potentially universal solution for TSA due to their broad linguistic and world knowledge along with in-context learning abilities, yet their performance is heavily influenced by prompt design. Drawing parallels with annotation paradigms for subjective tasks, we explore the influence of prompt design on the performance of LLMs for TSA of news headlines. We evaluate the predictive accuracy of state-of-the-art LLMs using prompts with different levels of prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts matching annotation guidelines. Recognizing the subjective nature of TSA, we evaluate the ability of LLMs to quantify predictive uncertainty via calibration error and correlation to human inter-annotator agreement. We find that, except for few-shot prompting, calibration and F1-score improve with increased prescriptiveness, but the optimal level depends on the model.</li>
</ul>

<h3>Title: Abductive Ego-View Accident Video Understanding for Safe Driving  Perception</h3>
<ul>
<li><strong>Authors: </strong>Jianwu Fang, Lei-lei Li, Junfei Zhou, Junbin Xiao, Hongkai Yu, Chen Lv, Jianru Xue, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00436">https://arxiv.org/abs/2403.00436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00436">https://arxiv.org/pdf/2403.00436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00436]] Abductive Ego-View Accident Video Understanding for Safe Driving  Perception(https://arxiv.org/abs/2403.00436)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each with temporally aligned text descriptions. We annotate over 2.23 million object boxes and 58,650 pairs of video-based accident reasons, covering 58 accident categories. MM-AU supports various accident understanding tasks, particularly multimodal video diffusion to understand accident cause-effect chains for safe driving. With MM-AU, we present an Abductive accident Video understanding framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven by an abductive CLIP model. This model involves a contrastive interaction loss to learn the pair co-occurrence of normal, near-accident, accident frames with the corresponding text descriptions, such as accident reasons, prevention advice, and accident categories. OAVD enforces the causal region learning while fixing the content of the original frame background in video generation, to find the dominant cause-effect chain for certain accidents. Extensive experiments verify the abductive ability of AdVersa-SD and the superiority of OAVD against the state-of-the-art diffusion models. Additionally, we provide careful benchmark evaluations for object detection and accident reason answering since AdVersa-SD relies on precise object and accident reason information.</li>
</ul>

<h3>Title: LoMOE: Localized Multi-Object Editing via Multi-Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Goirik Chakrabarty, Aditya Chandrasekar, Ramya Hebbalaguppe, Prathosh AP</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00437">https://arxiv.org/abs/2403.00437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00437">https://arxiv.org/pdf/2403.00437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00437]] LoMOE: Localized Multi-Object Editing via Multi-Diffusion(https://arxiv.org/abs/2403.00437)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent developments in the field of diffusion models have demonstrated an exceptional capacity to generate high-quality prompt-conditioned image edits. Nevertheless, previous approaches have primarily relied on textual prompts for image editing, which tend to be less effective when making precise edits to specific objects or fine-grained regions within a scene containing single/multiple objects. We introduce a novel framework for zero-shot localized multi-object editing through a multi-diffusion process to overcome this challenge. This framework empowers users to perform various operations on objects within an image, such as adding, replacing, or editing $\textbf{many}$ objects in a complex scene $\textbf{in one pass}$. Our approach leverages foreground masks and corresponding simple text prompts that exert localized influences on the target regions resulting in high-fidelity image editing. A combination of cross-attention and background preservation losses within the latent space ensures that the characteristics of the object being edited are preserved while simultaneously achieving a high-quality, seamless reconstruction of the background with fewer artifacts compared to the current methods. We also curate and release a dataset dedicated to multi-object editing, named $\texttt{LoMOE}$-Bench. Our experiments against existing state-of-the-art methods demonstrate the improved effectiveness of our approach in terms of both image editing quality and inference speed.</li>
</ul>

<h3>Title: An Ordinal Diffusion Model for Generating Medical Images with Different  Severity Levels</h3>
<ul>
<li><strong>Authors: </strong>Shumpei Takezaki, Seiichi Uchida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00452">https://arxiv.org/abs/2403.00452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00452">https://arxiv.org/pdf/2403.00452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00452]] An Ordinal Diffusion Model for Generating Medical Images with Different  Severity Levels(https://arxiv.org/abs/2403.00452)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently been used for medical image generation because of their high image quality. In this study, we focus on generating medical images with ordinal classes, which have ordinal relationships, such as severity levels. We propose an Ordinal Diffusion Model (ODM) that controls the ordinal relationships of the estimated noise images among the classes. Our model was evaluated experimentally by generating retinal and endoscopic images of multiple severity classes. ODM achieved higher performance than conventional generative models by generating realistic images, especially in high-severity classes with fewer training samples.</li>
</ul>

<h3>Title: Deformable One-shot Face Stylization via DINO Semantic Guidance</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Zichong Chen, Hui Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00459">https://arxiv.org/abs/2403.00459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00459">https://arxiv.org/pdf/2403.00459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00459]] Deformable One-shot Face Stylization via DINO Semantic Guidance(https://arxiv.org/abs/2403.00459)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper addresses the complex issue of one-shot face stylization, focusing on the simultaneous consideration of appearance and structure, where previous methods have fallen short. We explore deformation-aware face stylization that diverges from traditional single-image style reference, opting for a real-style image pair instead. The cornerstone of our method is the utilization of a self-supervised vision transformer, specifically DINO-ViT, to establish a robust and consistent facial structure representation across both real and style domains. Our stylization process begins by adapting the StyleGAN generator to be deformation-aware through the integration of spatial transformers (STN). We then introduce two innovative constraints for generator fine-tuning under the guidance of DINO semantics: i) a directional deformation loss that regulates directional vectors in DINO space, and ii) a relative structural consistency constraint based on DINO token self-similarities, ensuring diverse generation. Additionally, style-mixing is employed to align the color generation with the reference, minimizing inconsistent correspondences. This framework delivers enhanced deformability for general one-shot face stylization, achieving notable efficiency with a fine-tuning duration of approximately 10 minutes. Extensive qualitative and quantitative comparisons demonstrate our superiority over state-of-the-art one-shot face stylization methods. Code is available at \url{https://github.com/zichongc/DoesFS}.</li>
</ul>

<h3>Title: Learning and Leveraging World Models in Visual Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes, Laurent Najman, Yann LeCun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00504">https://arxiv.org/abs/2403.00504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00504">https://arxiv.org/pdf/2403.00504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00504]] Learning and Leveraging World Models in Visual Representation Learning(https://arxiv.org/abs/2403.00504)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.</li>
</ul>

<h3>Title: Large Language Models for Simultaneous Named Entity Extraction and  Spelling Correction</h3>
<ul>
<li><strong>Authors: </strong>Edward Whittaker, Ikuo Kitagishi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00528">https://arxiv.org/abs/2403.00528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00528">https://arxiv.org/pdf/2403.00528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00528]] Large Language Models for Simultaneous Named Entity Extraction and  Spelling Correction(https://arxiv.org/abs/2403.00528)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) such as BERT, have been shown to perform well on the task of identifying Named Entities (NE) in text. A BERT LM is typically used as a classifier to classify individual tokens in the input text, or to classify spans of tokens, as belonging to one of a set of possible NE categories. In this paper, we hypothesise that decoder-only Large Language Models (LLMs) can also be used generatively to extract both the NE, as well as potentially recover the correct surface form of the NE, where any spelling errors that were present in the input text get automatically corrected. We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on the task of producing NEs from text that was obtained by applying Optical Character Recognition (OCR) to images of Japanese shop receipts; in this work, we do not attempt to find or evaluate the location of NEs in the text. We show that the best fine-tuned LLM performs as well as, or slightly better than, the best fine-tuned BERT LM, although the differences are not significant. However, the best LLM is also shown to correct OCR errors in some cases, as initially hypothesised.</li>
</ul>

<h3>Title: Standardizing the Measurement of Text Diversity: A Tool and a  Comparative Analysis of Scores</h3>
<ul>
<li><strong>Authors: </strong>Chantal Shaib, Joe Barrow, Jiuding Sun, Alexa F. Siu, Byron C. Wallace, Ani Nenkova</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00553">https://arxiv.org/abs/2403.00553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00553">https://arxiv.org/pdf/2403.00553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00553]] Standardizing the Measurement of Text Diversity: A Tool and a  Comparative Analysis of Scores(https://arxiv.org/abs/2403.00553)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The diversity across outputs generated by large language models shapes the perception of their quality and utility. Prompt leaks, templated answer structure, and canned responses across different interactions are readily noticed by people, but there is no standard score to measure this aspect of model behavior. In this work we empirically investigate diversity scores on English texts. We find that computationally efficient compression algorithms capture information similar to what is measured by slow to compute $n$-gram overlap homogeneity scores. Further, a combination of measures -- compression ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are sufficient to report, as they have low mutual correlation with each other. The applicability of scores extends beyond analysis of generative models; for example, we highlight applications on instruction-tuning datasets and human-produced texts. We release a diversity score package to facilitate research and invite consistency across reports.</li>
</ul>

<h3>Title: Rethinking cluster-conditioned diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Nikolas Adaloglou, Tim Kaiser, Felix Michels, Markus Kollmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00570">https://arxiv.org/abs/2403.00570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00570">https://arxiv.org/pdf/2403.00570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00570]] Rethinking cluster-conditioned diffusion models(https://arxiv.org/abs/2403.00570)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.</li>
</ul>

<h3>Title: Improving Explicit Spatial Relationships in Text-to-Image Generation  through an Automatically Derived Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa, Eneko Agirre, Frank Keller</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00587">https://arxiv.org/abs/2403.00587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00587">https://arxiv.org/pdf/2403.00587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00587]] Improving Explicit Spatial Relationships in Text-to-Image Generation  through an Automatically Derived Dataset(https://arxiv.org/abs/2403.00587)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as 'left of' or 'below'. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an 'unseen' split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially fine-tune text-to-image systems. We show that fine-tuning two different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9 points improvements in the VISOR metric. The improvement holds in the 'unseen' split, showing that SD$_{SR4G}$ is able to generalize to unseen objects. SD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids complex architectures. Our analysis shows that improvement is consistent for all relations. The dataset and the code will be publicly available.</li>
</ul>

<h3>Title: Rethinking The Uniformity Metric in Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Xianghong Fang, Jian Li, Qiang Sun, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00642">https://arxiv.org/abs/2403.00642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00642">https://arxiv.org/pdf/2403.00642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00642]] Rethinking The Uniformity Metric in Self-Supervised Learning(https://arxiv.org/abs/2403.00642)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various established self-supervised methods, our proposed uniformity metric consistently enhances their performance in downstream tasks.Our code was released at https://github.com/sunset-clouds/WassersteinUniformityMetric.</li>
</ul>

<h3>Title: Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Liu, Fang Liu, Zhanghan Ke, Nanxuan Zhao, Rynson W.H. Lau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00644">https://arxiv.org/abs/2403.00644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00644">https://arxiv.org/pdf/2403.00644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00644]] Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks(https://arxiv.org/abs/2403.00644)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models trained on large-scale datasets have achieved remarkable progress in image synthesis. However, due to the randomness in the diffusion process, they often struggle with handling diverse low-level tasks that require details preservation. To overcome this limitation, we present a new Diff-Plugin framework to enable a single pre-trained diffusion model to generate high-fidelity results across a variety of low-level tasks. Specifically, we first propose a lightweight Task-Plugin module with a dual branch design to provide task-specific priors, guiding the diffusion process in preserving image content. We then propose a Plugin-Selector that can automatically select different Task-Plugins based on the text instruction, allowing users to edit images by indicating multiple low-level tasks with natural language. We conduct extensive experiments on 8 low-level vision tasks. The results demonstrate the superiority of Diff-Plugin over existing methods, particularly in real-world scenarios. Our ablations further validate that Diff-Plugin is stable, schedulable, and supports robust training across different dataset sizes.</li>
</ul>

<h3>Title: Advancing Additive Manufacturing through Deep Learning: A Comprehensive  Review of Current Progress and Future Challenges</h3>
<ul>
<li><strong>Authors: </strong>Amirul Islam Saimon, Emmanuel Yangue, Xiaowei Yue, Zhenyu (James)Kong, Chenang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.00669">https://arxiv.org/abs/2403.00669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.00669">https://arxiv.org/pdf/2403.00669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.00669]] Advancing Additive Manufacturing through Deep Learning: A Comprehensive  Review of Current Progress and Future Challenges(https://arxiv.org/abs/2403.00669)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Additive manufacturing (AM) has already proved itself to be the potential alternative to widely-used subtractive manufacturing due to its extraordinary capacity of manufacturing highly customized products with minimum material wastage. Nevertheless, it is still not being considered as the primary choice for the industry due to some of its major inherent challenges, including complex and dynamic process interactions, which are sometimes difficult to fully understand even with traditional machine learning because of the involvement of high-dimensional data such as images, point clouds, and voxels. However, the recent emergence of deep learning (DL) is showing great promise in overcoming many of these challenges as DL can automatically capture complex relationships from high-dimensional data without hand-crafted feature extraction. Therefore, the volume of research in the intersection of AM and DL is exponentially growing each year which makes it difficult for the researchers to keep track of the trend and future potential directions. Furthermore, to the best of our knowledge, there is no comprehensive review paper in this research track summarizing the recent studies. Therefore, this paper reviews the recent studies that apply DL for making the AM process better with a high-level summary of their contributions and limitations. Finally, it summarizes the current challenges and recommends some of the promising opportunities in this domain for further investigation with a special focus on generalizing DL models for wide-range of geometry types, managing uncertainties both in AM data and DL models, overcoming limited and noisy AM data issues by incorporating generative models, and unveiling the potential of interpretable DL for AM.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
