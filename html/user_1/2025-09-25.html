<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-25</h1>
<h3>Title: Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yifu Luo, Yongzhe Chang, Xueqian Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19305">https://arxiv.org/abs/2509.19305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19305">https://arxiv.org/pdf/2509.19305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19305]] Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning(https://arxiv.org/abs/2509.19305)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion probability models have shown significant promise in offline reinforcement learning by directly modeling trajectory sequences. However, existing approaches primarily focus on time-domain features while overlooking frequency-domain features, leading to frequency shift and degraded performance according to our observation. In this paper, we investigate the RL problem from a new perspective of the frequency domain. We first observe that time-domain-only approaches inadvertently introduce shifts in the low-frequency components of the frequency domain, which results in trajectory instability and degraded performance. To address this issue, we propose Wavelet Fourier Diffuser (WFDiffuser), a novel diffusion-based RL framework that integrates Discrete Wavelet Transform to decompose trajectories into low- and high-frequency components. To further enhance diffusion modeling for each component, WFDiffuser employs Short-Time Fourier Transform and cross attention mechanisms to extract frequency-domain features and facilitate cross-frequency interaction. Extensive experiment results on the D4RL benchmark demonstrate that WFDiffuser effectively mitigates frequency shift, leading to smoother, more stable trajectories and improved decision-making performance over existing methods.</li>
</ul>

<h3>Title: Quantifying Compositionality of Classic and State-of-the-Art Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Zhijin Guo (1 and 2), Chenhao Xue (1), Zhaozhen Xu (2), Hongbo Bo (2), Yuxuan Ye (2), Janet B. Pierrehumbert (1), Martha Lewis (3) ((1) University of Oxford, (2) University of Bristol, (3) University of Amsterdam)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19332">https://arxiv.org/abs/2509.19332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19332">https://arxiv.org/pdf/2509.19332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19332]] Quantifying Compositionality of Classic and State-of-the-Art Embeddings(https://arxiv.org/abs/2509.19332)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>For language models to generalize correctly to novel expressions, it is critical that they exploit access compositional meanings when this is justified. Even if we don't know what a "pelp" is, we can use our knowledge of numbers to understand that "ten pelps" makes more pelps than "two pelps". Static word embeddings such as Word2vec made strong, indeed excessive, claims about compositionality. The SOTA generative, transformer models and graph models, however, go too far in the other direction by providing no real limits on shifts in meaning due to context. To quantify the additive compositionality, we formalize a two-step, generalized evaluation that (i) measures the linearity between known entity attributes and their embeddings via canonical correlation analysis, and (ii) evaluates additive generalization by reconstructing embeddings for unseen attribute combinations and checking reconstruction metrics such as L2 loss, cosine similarity, and retrieval accuracy. These metrics also capture failure cases where linear composition breaks down. Sentences, knowledge graphs, and word embeddings are evaluated and tracked the compositionality across all layers and training stages. Stronger compositional signals are observed in later training stages across data modalities, and in deeper layers of the transformer-based model before a decline at the top layer. Code is available at this https URL.</li>
</ul>

<h3>Title: SCORE: A Semantic Evaluation Framework for Generative Document Parsing</h3>
<ul>
<li><strong>Authors: </strong>Renyu Li, Antonio Jimeno Yepes, Yao You, Kamil Pluci≈Ñski, Maximilian Operlejn, Crag Wolfe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19345">https://arxiv.org/abs/2509.19345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19345">https://arxiv.org/pdf/2509.19345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19345]] SCORE: A Semantic Evaluation Framework for Generative Document Parsing(https://arxiv.org/abs/2509.19345)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multi-modal generative document parsing systems challenge traditional evaluation: unlike deterministic OCR or layout models, they often produce semantically correct yet structurally divergent outputs. Conventional metrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing valid interpretations and obscuring system behavior. We introduce SCORE (Structural and COntent Robust Evaluation), an interpretation-agnostic framework that integrates (i) adjusted edit distance for robust content fidelity, (ii) token-level diagnostics to distinguish hallucinations from omissions, (iii) table evaluation with spatial tolerance and semantic alignment, and (iv) hierarchy-aware consistency checks. Together, these dimensions enable evaluation that embraces representational diversity while enforcing semantic rigor. Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE consistently revealed cross-dataset performance patterns missed by standard metrics. In 2-5% of pages with ambiguous table structures, traditional metrics penalized systems by 12-25% on average, leading to distorted rankings. SCORE corrected these cases, recovering equivalence between alternative but valid interpretations. Moreover, by normalizing generative outputs into a format-agnostic representation, SCORE reproduces traditional scores (e.g., table F1 up to 0.93) without requiring object-detection pipelines, demonstrating that generative parsing alone suffices for comprehensive evaluation. By exposing how interpretive diversity impacts evaluation outcomes and providing multi-dimensional, interpretable diagnostics, SCORE establishes foundational principles for semantically grounded, fair, and practical benchmarking of modern document parsing systems.</li>
</ul>

<h3>Title: RoadMind: Towards a Geospatial AI Expert for Disaster Response</h3>
<ul>
<li><strong>Authors: </strong>Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19354">https://arxiv.org/abs/2509.19354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19354">https://arxiv.org/pdf/2509.19354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19354]] RoadMind: Towards a Geospatial AI Expert for Disaster Response(https://arxiv.org/abs/2509.19354)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive performance across a range of natural language tasks, but remain limited in their ability to reason about geospatial data, particularly road networks, distances, and directions. This gap poses challenges in disaster scenarios, where spatial understanding is critical for tasks such as evacuation planning and resource allocation. In this work, we present RoadMind, a self-supervised framework that enhances the geospatial reasoning capabilities of LLMs using structured data from OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data for a given city and converts it into multiple supervision formats tailored to key spatial tasks. We pretrain and fine-tune LLMs on these representations using QLoRA adapters and 4-bit quantized models. We evaluate our approach on three disaster-prone cities with varying global representation, Los Angeles, Christchurch, and Manila, across tasks such as road segment identification, nearest road retrieval, and distance/direction estimation. Our results show that models trained via RoadMind significantly outperform strong baselines, including state-of-the-art LLMs equipped with advanced prompt engineering. This demonstrates the potential of structured geospatial data to enhance language models with robust spatial reasoning, enabling more effective offline AI systems for disaster response.</li>
</ul>

<h3>Title: Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data</h3>
<ul>
<li><strong>Authors: </strong>Buhe Li, Berkay Kaplan, Maksym Lazirko, Aleksandr Kogan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19366">https://arxiv.org/abs/2509.19366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19366">https://arxiv.org/pdf/2509.19366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19366]] Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data(https://arxiv.org/abs/2509.19366)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This study investigates the effectiveness of unsupervised outlier detection methods in audit analytics, utilizing USA spending data from the U.S. Department of Health and Human Services (DHHS) as a case example. We employ and compare multiple outlier detection algorithms, including Histogram-based Outlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum Covariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify anomalies in federal spending patterns. The research addresses the growing need for efficient and accurate anomaly detection in large-scale governmental datasets, where traditional auditing methods may fall short. Our methodology involves data preparation, algorithm implementation, and performance evaluation using precision, recall, and F1 scores. Results indicate that a hybrid approach, combining multiple detection strategies, enhances the robustness and accuracy of outlier identification in complex financial data. This study contributes to the field of audit analytics by providing insights into the comparative effectiveness of various outlier detection models and demonstrating the potential of unsupervised learning techniques in improving audit quality and efficiency. The findings have implications for auditors, policymakers, and researchers seeking to leverage advanced analytics in governmental financial oversight and risk management.</li>
</ul>

<h3>Title: Frame-based Equivariant Diffusion Models for 3D Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohan Guo (Faculty of Science, University of Amsterdam), Cong Liu (AMLab, University of Amsterdam), Patrick Forr√© (AMLab, University of Amsterdam)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19506">https://arxiv.org/abs/2509.19506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19506">https://arxiv.org/pdf/2509.19506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19506]] Frame-based Equivariant Diffusion Models for 3D Molecular Generation(https://arxiv.org/abs/2509.19506)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent methods for molecular generation face a trade-off: they either enforce strict equivariance with costly architectures or relax it to gain scalability and flexibility. We propose a frame-based diffusion paradigm that achieves deterministic E(3)-equivariance while decoupling symmetry handling from the backbone. Building on this paradigm, we investigate three variants: Global Frame Diffusion (GFD), which assigns a shared molecular frame; Local Frame Diffusion (LFD), which constructs node-specific frames and benefits from additional alignment constraints; and Invariant Frame Diffusion (IFD), which relies on pre-canonicalized invariant representations. To enhance expressivity, we further utilize EdgeDiT, a Diffusion Transformer with edge-aware attention. On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance, with a test NLL of -137.97 at standard scale and -141.85 at double scale, alongside atom stability of 98.98%, and molecular stability of 90.51%. These results surpass all equivariant baselines while maintaining high validity and uniqueness and nearly 2x faster sampling compared to EDM. Altogether, our study establishes frame-based diffusion as a scalable, flexible, and physically grounded paradigm for molecular generation, highlighting the critical role of global structure preservation.</li>
</ul>

<h3>Title: DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions</h3>
<ul>
<li><strong>Authors: </strong>Zongyue Li, Xiao Han, Yusong Li, Niklas Strauss, Matthias Schubert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19538">https://arxiv.org/abs/2509.19538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19538">https://arxiv.org/pdf/2509.19538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19538]] DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions(https://arxiv.org/abs/2509.19538)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based world models have demonstrated strong capabilities in synthesizing realistic long-horizon trajectories for offline reinforcement learning (RL). However, many existing methods do not directly generate actions alongside states and rewards, limiting their compatibility with standard value-based offline RL algorithms that rely on one-step temporal difference (TD) learning. While prior work has explored joint modeling of states, rewards, and actions to address this issue, such formulations often lead to increased training complexity and reduced performance in practice. We propose \textbf{DAWM}, a diffusion-based world model that generates future state-reward trajectories conditioned on the current state, action, and return-to-go, paired with an inverse dynamics model (IDM) for efficient action inference. This modular design produces complete synthetic transitions suitable for one-step TD-based offline RL, enabling effective and computationally efficient training. Empirically, we show that conservative offline RL algorithms such as TD3BC and IQL benefit significantly from training on these augmented trajectories, consistently outperforming prior diffusion-based baselines across multiple tasks in the D4RL benchmark.</li>
</ul>

<h3>Title: ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Aleksis Datseris, Sylvia Vassileva, Ivan Koychev, Svetla Boytcheva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19569">https://arxiv.org/abs/2509.19569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19569">https://arxiv.org/pdf/2509.19569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19569]] ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities(https://arxiv.org/abs/2509.19569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to position embeddings in transformer models, named "Exact Positional Embeddings" (ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training.</li>
</ul>

<h3>Title: LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines</h3>
<ul>
<li><strong>Authors: </strong>Yanfang (Fanny)Ye, Zheyuan Zhang, Tianyi Ma, Zehong Wang, Yiyang Li, Shifu Hou, Weixiang Sun, Kaiwen Shi, Yijun Ma, Wei Song, Ahmed Abbasi, Ying Cheng, Jane Cleland-Huang, Steven Corcelli, Patricia Culligan, Robert Goulding, Ming Hu, Ting Hua, John Lalor, Fang Liu, Tengfei Luo, Ed Maginn, Nuno Moniz, Jason Rohr, Brett Savoie, Daniel Slate, Tom Stapleford, Matthew Webber, Olaf Wiest, Johnny Zhang, Nitesh Chawla</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19580">https://arxiv.org/abs/2509.19580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19580">https://arxiv.org/pdf/2509.19580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19580]] LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines(https://arxiv.org/abs/2509.19580)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view of the world. For example, Large Language Models (LLMs) based applications such as ChatGPT have shown the capability of generating human-like conversation on extensive topics. Due to the impressive performance on a variety of language-related tasks (e.g., open-domain question answering, translation, and document summarization), one can envision the far-reaching impacts that can be brought by the LLMs with broader real-world applications (e.g., customer service, education and accessibility, and scientific discovery). Inspired by their success, this paper will offer an overview of state-of-the-art LLMs and their integration into a wide range of academic disciplines, including: (1) arts, letters, and law (e.g., history, philosophy, political science, arts and architecture, law), (2) economics and business (e.g., finance, economics, accounting, marketing), and (3) science and engineering (e.g., mathematics, physics and mechanical engineering, chemistry and chemical engineering, life sciences and bioengineering, earth sciences and civil engineering, computer science and electrical engineering). Integrating humanity and technology, in this paper, we will explore how LLMs are shaping research and practice in these fields, while also discussing key limitations, open challenges, and future directions in the era of generative AI. The review of how LLMs are engaged across disciplines-along with key observations and insights-can help researchers and practitioners interested in exploiting LLMs to advance their works in diverse real-world applications.</li>
</ul>

<h3>Title: A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery</h3>
<ul>
<li><strong>Authors: </strong>Alexander Ho, Sukyeong Lee, Francis T.F. Tsai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19586">https://arxiv.org/abs/2509.19586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19586">https://arxiv.org/pdf/2509.19586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19586]] A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery(https://arxiv.org/abs/2509.19586)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce FragAtlas-62M, a specialized foundation model trained on the largest fragment dataset to date. Built on the complete ZINC-22 fragment subset comprising over 62 million molecules, it achieves unprecedented coverage of fragment chemical space. Our GPT-2 based model (42.7M parameters) generates 99.90% chemically valid fragments. Validation across 12 descriptors and three fingerprint methods shows generated fragments closely match the training distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC fragments while producing 22% novel structures with practical relevance. We release FragAtlas-62M with training code, preprocessed data, documentation, and model weights to accelerate adoption.</li>
</ul>

<h3>Title: Synthesizing Artifact Dataset for Pixel-level Detection</h3>
<ul>
<li><strong>Authors: </strong>Dennis Menn, Feng Liang, Diana Marculescu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19589">https://arxiv.org/abs/2509.19589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19589">https://arxiv.org/pdf/2509.19589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19589]] Synthesizing Artifact Dataset for Pixel-level Detection(https://arxiv.org/abs/2509.19589)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artifact detectors have been shown to enhance the performance of image-generative models by serving as reward models during fine-tuning. These detectors enable the generative model to improve overall output fidelity and aesthetics. However, training the artifact detector requires expensive pixel-level human annotations that specify the artifact regions. The lack of annotated data limits the performance of the artifact detector. A naive pseudo-labeling approach-training a weak detector and using it to annotate unlabeled images-suffers from noisy labels, resulting in poor performance. To address this, we propose an artifact corruption pipeline that automatically injects artifacts into clean, high-quality synthetic images on a predetermined region, thereby producing pixel-level annotations without manual labeling. The proposed method enables training of an artifact detector that achieves performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified on human-labeled data, compared to baseline approaches. This work represents an initial step toward scalable pixel-level artifact annotation datasets that integrate world knowledge into artifact detection.</li>
</ul>

<h3>Title: TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19638">https://arxiv.org/abs/2509.19638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19638">https://arxiv.org/pdf/2509.19638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19638]] TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation(https://arxiv.org/abs/2509.19638)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, anomaly</a></li>
<li><strong>Abstract: </strong>Generating high-quality synthetic time series is a fundamental yet challenging task across domains such as forecasting and anomaly detection, where real data can be scarce, noisy, or costly to collect. Unlike static data generation, synthesizing time series requires modeling both the marginal distribution of observations and the conditional temporal dependencies that govern sequential dynamics. We propose TIMED, a unified generative framework that integrates a denoising diffusion probabilistic model (DDPM) to capture global structure via a forward-reverse diffusion process, a supervisor network trained with teacher forcing to learn autoregressive dependencies through next-step prediction, and a Wasserstein critic that provides adversarial feedback to ensure temporal smoothness and fidelity. To further align the real and synthetic distributions in feature space, TIMED incorporates a Maximum Mean Discrepancy (MMD) loss, promoting both diversity and sample quality. All components are built using masked attention architectures optimized for sequence modeling and are trained jointly to effectively capture both unconditional and conditional aspects of time series data. Experimental results across diverse multivariate time series benchmarks demonstrate that TIMED generates more realistic and temporally coherent sequences than state-of-the-art generative models.</li>
</ul>

<h3>Title: Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Kevin Garcia, Cassandra Garza, Brooklyn Berry, Yifeng Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19654">https://arxiv.org/abs/2509.19654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19654">https://arxiv.org/pdf/2509.19654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19654]] Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification(https://arxiv.org/abs/2509.19654)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The surge in the significance of time series in digital health domains necessitates advanced methodologies for extracting meaningful patterns and representations. Self-supervised contrastive learning has emerged as a promising approach for learning directly from raw data. However, time series data in digital health is known to be highly noisy, inherently involves concept drifting, and poses a challenge for training a generalizable deep learning model. In this paper, we specifically focus on data distribution shift caused by different human behaviors and propose a self-supervised learning framework that is aware of the bag-of-symbol representation. The bag-of-symbol representation is known for its insensitivity to data warping, location shifts, and noise existed in time series data, making it potentially pivotal in guiding deep learning to acquire a representation resistant to such data shifting. We demonstrate that the proposed method can achieve significantly better performance where significant data shifting exists.</li>
</ul>

<h3>Title: From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition</h3>
<ul>
<li><strong>Authors: </strong>Ling Lo, Kelvin C.K. Chan, Wen-Huang Cheng, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19690">https://arxiv.org/abs/2509.19690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19690">https://arxiv.org/pdf/2509.19690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19690]] From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition(https://arxiv.org/abs/2509.19690)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing models often struggle with complex temporal changes, particularly when generating videos with gradual attribute transitions. The most common prompt interpolation approach for motion transitions often fails to handle gradual attribute transitions, where inconsistencies tend to become more pronounced. In this work, we propose a simple yet effective method to extend existing models for smooth and consistent attribute transitions, through introducing frame-wise guidance during the denoising process. Our approach constructs a data-specific transitional direction for each noisy latent, guiding the gradual shift from initial to final attributes frame by frame while preserving the motion dynamics of the video. Moreover, we present the Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both attribute and motion dynamics, to comprehensively evaluate the performance of different models. We further propose two metrics to assess the accuracy and smoothness of attribute transitions. Experimental results demonstrate that our approach performs favorably against existing baselines, achieving visual fidelity, maintaining alignment with text prompts, and delivering seamless attribute transitions. Code and CATBench are released: this https URL.</li>
</ul>

<h3>Title: Anatomically Constrained Transformers for Cardiac Amyloidosis Classification</h3>
<ul>
<li><strong>Authors: </strong>Alexander Thorley, Agis Chartsias, Jordan Strom, Roberto Lang, Jeremy Slivnick, Jamie O'Driscoll, Rajan Sharma, Dipak Kotecha, Jinming Duan, Alberto Gomez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19691">https://arxiv.org/abs/2509.19691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19691">https://arxiv.org/pdf/2509.19691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19691]] Anatomically Constrained Transformers for Cardiac Amyloidosis Classification(https://arxiv.org/abs/2509.19691)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities in clinical measurements from echocardiograms such as reduced global longitudinal strain of the myocardium. An alternative approach for detecting CA is via neural networks, using video classification models such as convolutional neural networks. These models process entire video clips, but provide no assurance that classification is based on clinically relevant features known to be associated with CA. An alternative paradigm for disease classification is to apply models to quantitative features such as strain, ensuring that the classification relates to clinically relevant features. Drawing inspiration from this approach, we explicitly constrain a transformer model to the anatomical region where many known CA abnormalities occur -- the myocardium, which we embed as a set of deforming points and corresponding sampled image patches into input tokens. We show that our anatomical constraint can also be applied to the popular self-supervised learning masked autoencoder pre-training, where we propose to mask and reconstruct only anatomical patches. We show that by constraining both the transformer and pre-training task to the myocardium where CA imaging features are localized, we achieve increased performance on a CA classification task compared to full video transformers. Our model provides an explicit guarantee that the classification is focused on only anatomical regions of the echo, and enables us to visualize transformer attention scores over the deforming myocardium.</li>
</ul>

<h3>Title: Linear Transformers Implicitly Discover Unified Numerical Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19702">https://arxiv.org/abs/2509.19702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19702">https://arxiv.org/pdf/2509.19702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19702]] Linear Transformers Implicitly Discover Unified Numerical Algorithms(https://arxiv.org/abs/2509.19702)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nystr√∂m extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nystr√∂m extrapolation, highlighting a powerful capability of in-context learning.</li>
</ul>

<h3>Title: Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jiesi Hu, Yanwu Yang, Zhiyu Ye, Chenfei Ye, Hanyang Peng, Jianfeng Cao, Ting Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19711">https://arxiv.org/abs/2509.19711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19711">https://arxiv.org/pdf/2509.19711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19711]] Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis(https://arxiv.org/abs/2509.19711)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The rise of In-Context Learning (ICL) for universal medical image segmentation has introduced an unprecedented demand for large-scale, diverse datasets for training, exacerbating the long-standing problem of data scarcity. While data synthesis offers a promising solution, existing methods often fail to simultaneously achieve both high data diversity and a domain distribution suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a novel data synthesis framework built upon domain randomization. SynthICL ensures realism by leveraging anatomical priors from real-world datasets, generates diverse anatomical structures to cover a broad data distribution, and explicitly models inter-subject variations to create data cohorts suitable for ICL. Extensive experiments on four held-out datasets validate our framework's effectiveness, showing that models trained with our data achieve performance gains of up to 63\% in average Dice and substantially enhanced generalization to unseen anatomical domains. Our work helps mitigate the data bottleneck for ICL-based segmentation, paving the way for robust models. Our code and the generated dataset are publicly available at this https URL.</li>
</ul>

<h3>Title: Talking Head Generation via AU-Guided Landmark Prediction</h3>
<ul>
<li><strong>Authors: </strong>Shao-Yu Chang, Jingyi Xu, Hieu Le, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19749">https://arxiv.org/abs/2509.19749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19749">https://arxiv.org/pdf/2509.19749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19749]] Talking Head Generation via AU-Guided Landmark Prediction(https://arxiv.org/abs/2509.19749)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a two-stage framework for audio-driven talking head generation with fine-grained expression control via facial Action Units (AUs). Unlike prior methods relying on emotion labels or implicit AU conditioning, our model explicitly maps AUs to 2D facial landmarks, enabling physically grounded, per-frame expression control. In the first stage, a variational motion generator predicts temporally coherent landmark sequences from audio and AU intensities. In the second stage, a diffusion-based synthesizer generates realistic, lip-synced videos conditioned on these landmarks and a reference image. This separation of motion and appearance improves expression accuracy, temporal stability, and visual realism. Experiments on the MEAD dataset show that our method outperforms state-of-the-art baselines across multiple metrics, demonstrating the effectiveness of explicit AU-to-landmark modeling for expressive talking head generation.</li>
</ul>

<h3>Title: PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaocheng Fang, Jiarui Jin, Haoyu Wang, Che Liu, Jieyi Cai, Guangkun Nie, Jun Li, Hongyan Li, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19774">https://arxiv.org/abs/2509.19774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19774">https://arxiv.org/pdf/2509.19774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19774]] PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection(https://arxiv.org/abs/2509.19774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In clinical practice, electrocardiography (ECG) remains the gold standard for cardiac monitoring, providing crucial insights for diagnosing a wide range of cardiovascular diseases (CVDs). However, its reliance on specialized equipment and trained personnel limits feasibility for continuous routine monitoring. Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks definitive electrophysiological information, preventing conclusive diagnosis. Generative models present a promising approach to translate PPG into clinically valuable ECG signals, yet current methods face substantial challenges, including the misalignment of physiological semantics in generative models and the complexity of modeling in high-dimensional signals. To this end, we propose PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent space via the CardioAlign Encoder and employs latent rectified flow to generate ECGs with high fidelity and interpretability. To the best of our knowledge, this is the first study to experiment on MCMED, a newly released clinical-grade dataset comprising over 10 million paired PPG-ECG samples from more than 118,000 emergency department visits with expert-labeled cardiovascular disease annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG translation and cardiovascular disease detection. Moreover, cardiologist-led evaluations confirm that the synthesized ECGs achieve high fidelity and improve diagnostic reliability, underscoring our method's potential for real-world cardiovascular screening.</li>
</ul>

<h3>Title: StrCGAN: A Generative Framework for Stellar Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Shantanusinh Parmar</a></li>
<li><strong>Subjects: </strong>cs.CV, astro-ph.IM, astro-ph.SR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19805">https://arxiv.org/abs/2509.19805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19805">https://arxiv.org/pdf/2509.19805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19805]] StrCGAN: A Generative Framework for Stellar Image Restoration(https://arxiv.org/abs/2509.19805)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to enhance low-resolution astrophotography images. Our goal is to reconstruct high-fidelity ground truth-like representations of celestial objects, a task that is challenging due to the limited resolution and quality of small-telescope observations such as the MobilTelesco dataset. Traditional models such as CycleGAN provide a foundation for image-to-image translation but are restricted to 2D mappings and often distort the morphology of stars and galaxies. To overcome these limitations, we extend the CycleGAN framework with three key innovations: 3D convolutional layers to capture volumetric spatial correlations, multi-spectral fusion to align optical and near-infrared (NIR) domains, and astrophysical regularization modules to preserve stellar morphology. Ground-truth references from multi-mission all-sky surveys spanning optical to NIR guide the training process, ensuring that reconstructions remain consistent across spectral bands. Together, these components allow StrCGAN to generate reconstructions that are not only visually sharper but also physically consistent, outperforming standard GAN models in the task of astrophysical image enhancement.</li>
</ul>

<h3>Title: An Efficient Conditional Score-based Filter for High Dimensional Nonlinear Filtering Problems</h3>
<ul>
<li><strong>Authors: </strong>Zhijun Zeng, Weiye Gan, Junqing Chen, Zuoqiang Shi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19816">https://arxiv.org/abs/2509.19816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19816">https://arxiv.org/pdf/2509.19816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19816]] An Efficient Conditional Score-based Filter for High Dimensional Nonlinear Filtering Problems(https://arxiv.org/abs/2509.19816)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In many engineering and applied science domains, high-dimensional nonlinear filtering is still a challenging problem. Recent advances in score-based diffusion models offer a promising alternative for posterior sampling but require repeated retraining to track evolving priors, which is impractical in high dimensions. In this work, we propose the Conditional Score-based Filter (CSF), a novel algorithm that leverages a set-transformer encoder and a conditional diffusion model to achieve efficient and accurate posterior sampling without retraining. By decoupling prior modeling and posterior sampling into offline and online stages, CSF enables scalable score-based filtering across diverse nonlinear systems. Extensive experiments on benchmark problems show that CSF achieves superior accuracy, robustness, and efficiency across diverse nonlinear filtering scenarios.</li>
</ul>

<h3>Title: Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach</h3>
<ul>
<li><strong>Authors: </strong>Samir Brahim Belhaouari, Yunis Carreon Kahalan, Humaira Shaffique, Ismael Belhaouari, Ashhadul Islam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19856">https://arxiv.org/abs/2509.19856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19856">https://arxiv.org/pdf/2509.19856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19856]] Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach(https://arxiv.org/abs/2509.19856)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The effectiveness of machine learning models, particularly in unbalanced classification tasks, is often hindered by the failure to differentiate between critical instances near the decision boundary and redundant samples concentrated in the core of the data distribution. In this paper, we propose a method to systematically identify and differentiate between these two types of data. Through extensive experiments on multiple benchmark datasets, we show that the boundary data oversampling method improves the F1 score by up to 10\% on 96\% of the datasets, whereas our core-aware reduction method compresses datasets up to 90\% while preserving their accuracy, making it 10 times more powerful than the original dataset. Beyond imbalanced classification, our method has broader implications for efficient model training, particularly in computationally expensive domains such as Large Language Model (LLM) training. By prioritizing high-quality, decision-relevant data, our approach can be extended to text, multimodal, and self-supervised learning scenarios, offering a pathway to faster convergence, improved generalization, and significant computational savings. This work paves the way for future research in data-efficient learning, where intelligent sampling replaces brute-force expansion, driving the next generation of AI advancements. Our code is available as a Python package at this https URL .</li>
</ul>

<h3>Title: Towards Self-Supervised Foundation Models for Critical Care Time Series</h3>
<ul>
<li><strong>Authors: </strong>Katja Naasunnguaq Jagd, Rachael DeVries, Ole Winther</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19885">https://arxiv.org/abs/2509.19885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19885">https://arxiv.org/pdf/2509.19885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19885]] Towards Self-Supervised Foundation Models for Critical Care Time Series(https://arxiv.org/abs/2509.19885)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Domain-specific foundation models for healthcare have expanded rapidly in recent years, yet foundation models for critical care time series remain relatively underexplored due to the limited size and availability of datasets. In this work, we introduce an early-stage pre-trained foundation model for critical care time-series based on the Bi-Axial Transformer (BAT), trained on pooled electronic health record datasets. We demonstrate effective transfer learning by fine-tuning the model on a dataset distinct from the training sources for mortality prediction, where it outperforms supervised baselines, particularly for small datasets ($<5,000$). These contributions highlight the potential of self-supervised foundation models for critical care times series to support generalizable and robust clinical applications in resource-limited settings.</li>
</ul>

<h3>Title: Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network</h3>
<ul>
<li><strong>Authors: </strong>Pin-Jui Huang, Yu-Hsuan Liao, SooHeon Kim, NoSeong Park, JongBae Park, DongMyung Shin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19896">https://arxiv.org/abs/2509.19896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19896">https://arxiv.org/pdf/2509.19896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19896]] Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network(https://arxiv.org/abs/2509.19896)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Computational models that predict cellular phenotypic responses to chemical and genetic perturbations can accelerate drug discovery by prioritizing therapeutic hypotheses and reducing costly wet-lab iteration. However, extracting biologically meaningful and batch-robust cell painting representations remains challenging. Conventional self-supervised and contrastive learning approaches often require a large-scale model and/or a huge amount of carefully curated data, still struggling with batch effects. We present Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel representation learning framework that aligns embeddings of cells subjected to the same perturbation across different wells, enforcing semantic consistency despite batch effects. Integrated into a masked siamese architecture, this alignment yields features that capture fine-grained morphology while remaining data- and parameter-efficient. For instance, in a gene-gene relationship retrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly available self-supervised (OpenPhenom) and contrastive learning (CellCLIP) methods, improving the benchmark scores by +29\% and +9\%, respectively, while training on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M images for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN vs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that CWA-MSN is a simple and effective way to learn cell image representation, enabling efficient phenotype modeling even under limited data and parameter budgets.</li>
</ul>

<h3>Title: Latent Iterative Refinement Flow: A Geometric-Constrained Approach for Few-Shot Generation</h3>
<ul>
<li><strong>Authors: </strong>Songtao Li, Zhenyu Liao, Tianqi Hou, Ting Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19903">https://arxiv.org/abs/2509.19903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19903">https://arxiv.org/pdf/2509.19903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19903]] Latent Iterative Refinement Flow: A Geometric-Constrained Approach for Few-Shot Generation(https://arxiv.org/abs/2509.19903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Few-shot generation, the synthesis of high-quality and diverse samples from limited training data, remains a significant challenge in generative modeling. Existing methods trained from scratch often fail to overcome overfitting and mode collapse, and fine-tuning large models can inherit biases while neglecting the crucial geometric structure of the latent space. To address these limitations, we introduce Latent Iterative Refinement Flow (LIRF), a novel approach that reframes few-shot generation as the progressive densification of geometrically structured manifold. LIRF establishes a stable latent space using an autoencoder trained with our novel \textbf{manifold-preservation loss} $L_{\text{manifold}}$. This loss ensures that the latent space maintains the geometric and semantic correspondence of the input data. Building on this, we propose an iterative generate-correct-augment cycle. Within this cycle, candidate samples are refined by a geometric \textbf{correction operator}, a provably contractive mapping that pulls samples toward the data manifold while preserving diversity. We also provide the \textbf{Convergence Theorem} demonstrating a predictable decrease in Hausdorff distance between generated and true data manifold. We also demonstrate the framework's scalability by generating coherent, high-resolution images on AFHQ-Cat. Ablation studies confirm that both the manifold-preserving latent space and the contractive correction mechanism are critical components of this success. Ultimately, LIRF provides a solution for data-scarce generative modeling that is not only theoretically grounded but also highly effective in practice.</li>
</ul>

<h3>Title: Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches</h3>
<ul>
<li><strong>Authors: </strong>Remo Sasso, Michelangelo Conserva, Dominik Jeurissen, Paulo Rauber</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19924">https://arxiv.org/abs/2509.19924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19924">https://arxiv.org/pdf/2509.19924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19924]] Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches(https://arxiv.org/abs/2509.19924)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Exploration in reinforcement learning (RL) remains challenging, particularly in sparse-reward settings. While foundation models possess strong semantic priors, their capabilities as zero-shot exploration agents in classic RL benchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed bandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our investigation reveals a key limitation: while VLMs can infer high-level objectives from visual input, they consistently fail at precise low-level control: the "knowing-doing gap". To analyze a potential bridge for this gap, we investigate a simple on-policy hybrid framework in a controlled, best-case scenario. Our results in this idealized setting show that VLM guidance can significantly improve early-stage sample efficiency, providing a clear analysis of the potential and constraints of using foundation models to guide exploration rather than for end-to-end control.</li>
</ul>

<h3>Title: TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees</h3>
<ul>
<li><strong>Authors: </strong>Emmanouil Panagiotou, Beno√Æt Ronval, Arjun Roy, Ludwig Bothmann, Bernd Bischl, Siegfried Nijssen, Eirini Ntoutsi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19927">https://arxiv.org/abs/2509.19927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19927">https://arxiv.org/pdf/2509.19927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19927]] TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees(https://arxiv.org/abs/2509.19927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Ensuring fairness in machine learning remains a significant challenge, as models often inherit biases from their training data. Generative models have recently emerged as a promising approach to mitigate bias at the data level while preserving utility. However, many rely on deep architectures, despite evidence that simpler models can be highly effective for tabular data. In this work, we introduce TABFAIRGDT, a novel method for generating fair synthetic tabular data using autoregressive decision trees. To enforce fairness, we propose a soft leaf resampling technique that adjusts decision tree outputs to reduce bias while preserving predictive performance. Our approach is non-parametric, effectively capturing complex relationships between mixed feature types, without relying on assumptions about the underlying data distributions. We evaluate TABFAIRGDT on benchmark fairness datasets and demonstrate that it outperforms state-of-the-art (SOTA) deep generative models, achieving better fairness-utility trade-off for downstream tasks, as well as higher synthetic data quality. Moreover, our method is lightweight, highly efficient, and CPU-compatible, requiring no data pre-processing. Remarkably, TABFAIRGDT achieves a 72% average speedup over the fastest SOTA baseline across various dataset sizes, and can generate fair synthetic data for medium-sized datasets (10 features, 10K samples) in just one second on a standard CPU, making it an ideal solution for real-world fairness-sensitive applications.</li>
</ul>

<h3>Title: GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes</h3>
<ul>
<li><strong>Authors: </strong>Guo Chen, Jiarun Liu, Sicong Du, Chenming Wu, Deqi Li, Shi-Sheng Huang, Guofeng Zhang, Sheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19937">https://arxiv.org/abs/2509.19937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19937">https://arxiv.org/pdf/2509.19937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19937]] GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes(https://arxiv.org/abs/2509.19937)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper presents GS-RoadPatching, an inpainting method for driving scene completion by referring to completely reconstructed regions, which are represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians. Our key insight is that the highly repetitive patterns in driving scenes often share multi-modal similarities within the implicit 3DGS feature space and are particularly suitable for structural matching to enable effective 3DGS-based substitutional inpainting. Practically, we construct feature-embedded 3DGS scenes to incorporate a patch measurement method for abstracting local context at different scales and, subsequently, propose a structural search method to find candidate patches in 3D space effectively. Finally, we propose a simple yet effective substitution-and-fusion optimization for better visual harmony. We conduct extensive experiments on multiple publicly available datasets to demonstrate the effectiveness and efficiency of our proposed method in driving scenes, and the results validate that our method achieves state-of-the-art performance compared to the baseline methods in terms of both quality and interoperability. Additional experiments in general scenes also demonstrate the applicability of the proposed 3D inpainting strategy. The project page and code are available at: this https URL</li>
</ul>

<h3>Title: Learnable Sampler Distillation for Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Feiyang Fu, Tongxian Guo, Zhaoqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19962">https://arxiv.org/abs/2509.19962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19962">https://arxiv.org/pdf/2509.19962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19962]] Learnable Sampler Distillation for Discrete Diffusion Models(https://arxiv.org/abs/2509.19962)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps. Our code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Ji, Chaohui Yu, Junyao Gao, Fan Wang, Cairong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19979">https://arxiv.org/abs/2509.19979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19979">https://arxiv.org/pdf/2509.19979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19979]] CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion(https://arxiv.org/abs/2509.19979)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, camera-controlled video generation has seen rapid development, offering more precise control over video generation. However, existing methods predominantly focus on camera control in perspective projection video generation, while geometrically consistent panoramic video generation remains challenging. This limitation is primarily due to the inherent complexities in panoramic pose representation and spherical projection. To address this issue, we propose CamPVG, the first diffusion-based framework for panoramic video generation guided by precise camera poses. We achieve camera position encoding for panoramic images and cross-view feature aggregation based on spherical projection. Specifically, we propose a panoramic Pl√ºcker embedding that encodes camera extrinsic parameters through spherical coordinate transformation. This pose encoder effectively captures panoramic geometry, overcoming the limitations of traditional methods when applied to equirectangular projections. Additionally, we introduce a spherical epipolar module that enforces geometric constraints through adaptive attention masking along epipolar lines. This module enables fine-grained cross-view feature aggregation, substantially enhancing the quality and consistency of generated panoramic videos. Extensive experiments demonstrate that our method generates high-quality panoramic videos consistent with camera trajectories, far surpassing existing methods in panoramic video generation.</li>
</ul>

<h3>Title: Pi-Transformer: A Physics-informed Attention Mechanism for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Sepehr Maleki, Negar Pourmoazemi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19985">https://arxiv.org/abs/2509.19985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19985">https://arxiv.org/pdf/2509.19985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19985]] Pi-Transformer: A Physics-informed Attention Mechanism for Time Series Anomaly Detection(https://arxiv.org/abs/2509.19985)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomalies in multivariate time series often arise from temporal context and cross-channel coordination rather than isolated outliers. We present Pi-Transformer, a physics-informed transformer with two attention pathways: a data-driven series attention and a smoothly evolving prior attention that encodes temporal invariants such as scale-related self-similarity and phase synchrony. The prior acts as a stable reference that calibrates reconstruction error. During training, we pair a reconstruction objective with a divergence term that encourages agreement between the two attentions while keeping them meaningfully distinct; the prior is regularised to evolve smoothly and is lightly distilled towards dataset-level statistics. At inference, the model combines an alignment-weighted reconstruction signal (Energy) with a mismatch signal that highlights timing and phase disruptions, and fuses them into a single score for detection. Across five benchmarks (SMD, MSL, SMAP, SWaT, and PSM), Pi-Transformer achieves state-of-the-art or highly competitive F1, with particular strength on timing and phase-breaking anomalies. Case analyses show complementary behaviour of the two streams and interpretable detections around regime changes. Embedding physics-informed priors into attention yields a calibrated and robust approach to anomaly detection in complex multivariate systems. Code is publicly available at this GitHub repository\footnote{this https URL}.</li>
</ul>

<h3>Title: Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Zhifang Zhang, Jiahan Zhang, Shengjie Zhou, Qi Wei, Shuo He, Feng Liu, Lei Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19994">https://arxiv.org/abs/2509.19994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19994">https://arxiv.org/pdf/2509.19994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19994]] Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models(https://arxiv.org/abs/2509.19994)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Multimodal pre-trained models (e.g., ImageBind), which align distinct data modalities into a shared embedding space, have shown remarkable success across downstream tasks. However, their increasing adoption raises serious security concerns, especially regarding targeted adversarial attacks. In this paper, we show that existing targeted adversarial attacks on multimodal pre-trained models still have limitations in two aspects: generalizability and undetectability. Specifically, the crafted targeted adversarial examples (AEs) exhibit limited generalization to partially known or semantically similar targets in cross-modal alignment tasks (i.e., limited generalizability) and can be easily detected by simple anomaly detection methods (i.e., limited undetectability). To address these limitations, we propose a novel method called Proxy Targeted Attack (PTA), which leverages multiple source-modal and target-modal proxies to optimize targeted AEs, ensuring they remain evasive to defenses while aligning with multiple potential targets. We also provide theoretical analyses to highlight the relationship between generalizability and undetectability and to ensure optimal generalizability while meeting the specified requirements for undetectability. Furthermore, experimental results demonstrate that our PTA can achieve a high success rate across various related targets and remain undetectable against multiple anomaly detection methods.</li>
</ul>

<h3>Title: Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture</h3>
<ul>
<li><strong>Authors: </strong>Nico Schulthess, Ender Konukoglu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19997">https://arxiv.org/abs/2509.19997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19997">https://arxiv.org/pdf/2509.19997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19997]] Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture(https://arxiv.org/abs/2509.19997)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In this work, we leverage informative embeddings from foundational models for unsupervised anomaly detection in medical imaging. For small datasets, a memory-bank of normative features can directly be used for anomaly detection which has been demonstrated recently. However, this is unsuitable for large medical datasets as the computational burden increases substantially. Therefore, we propose to model the distribution of normative DINOv2 embeddings with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model that automatically adjusts the number of mixture components to the data at hand. Rather than using a memory bank, we use the similarity between the component centers and the embeddings as anomaly score function to create a coarse anomaly segmentation mask. Our experiments show that through DPMM embeddings of DINOv2, despite being trained on natural images, achieve very competitive anomaly detection performance on medical imaging benchmarks and can do this while at least halving the computation time at inference. Our analysis further indicates that normalized DINOv2 embeddings are generally more aligned with anatomical structures than unnormalized features, even in the presence of anomalies, making them great representations for anomaly detection. The code is available at this https URL.</li>
</ul>

<h3>Title: Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification</h3>
<ul>
<li><strong>Authors: </strong>Lubos Mjachky, Ivan Homoliak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20024">https://arxiv.org/abs/2509.20024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20024">https://arxiv.org/pdf/2509.20024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20024]] Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification(https://arxiv.org/abs/2509.20024)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Biometric-based authentication systems are getting broadly adopted in many areas. However, these systems do not allow participating users to influence the way their data is used. Furthermore, the data may leak and can be misused without the users' knowledge. In this paper, we propose a new authentication method that preserves the privacy of individuals and is based on a generative adversarial network (GAN). Concretely, we suggest using the GAN for translating images of faces to a visually private domain (e.g., flowers or shoes). Classifiers, which are used for authentication purposes, are then trained on the images from the visually private domain. Based on our experiments, the method is robust against attacks and still provides meaningful utility.</li>
</ul>

<h3>Title: Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations</h3>
<ul>
<li><strong>Authors: </strong>Rami Zewail</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20048">https://arxiv.org/abs/2509.20048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20048">https://arxiv.org/pdf/2509.20048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20048]] Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations(https://arxiv.org/abs/2509.20048)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Learning robust representations for biosignals is often hampered by the challenge of designing effective data this http URL methods can fail to capture the complex variations inherent in physiological data. Within this context, we propose a novel hybrid framework, Diffusion-Augmented Contrastive Learning (DACL), that fuses concepts from diffusion models and supervised contrastive learning. The DACL framework operates on a latent space created by a lightweight Variational Autoencoder (VAE) trained on our novel Scattering Transformer (ST) features [12]. It utilizes the diffusion forward process as a principled data augmentation technique to generate multiple noisy views of these latent embeddings. A U-Net style encoder is then trained with a supervised contrastive objective to learn a representation that balances class discrimination with robustness to noise across various diffusion time steps. We evaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset, achieving a competitive AUROC of 0.7815. This work establishes a new paradigm for representation learning by using the diffusion process itself to drive the contrastive objective, creating noise-invariant embeddings that demonstrate a strong foundation for class separability.</li>
</ul>

<h3>Title: One Filters All: A Generalist Filter for State Estimation</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Liu, Wenhan Cao, Chang Liu, Zeyu He, Tianyi Zhang, Shengbo Eben Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20051">https://arxiv.org/abs/2509.20051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20051">https://arxiv.org/pdf/2509.20051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20051]] One Filters All: A Generalist Filter for State Estimation(https://arxiv.org/abs/2509.20051)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Estimating hidden states in dynamical systems, also known as optimal filtering, is a long-standing problem in various fields of science and engineering. In this paper, we introduce a general filtering framework, \textbf{LLM-Filter}, which leverages large language models (LLMs) for state estimation by embedding noisy observations with text prototypes. In various experiments for classical dynamical systems, we find that first, state estimation can significantly benefit from the reasoning knowledge embedded in pre-trained LLMs. By achieving proper modality alignment with the frozen LLM, LLM-Filter outperforms the state-of-the-art learning-based approaches. Second, we carefully design the prompt structure, System-as-Prompt (SaP), incorporating task instructions that enable the LLM to understand the estimation tasks. Guided by these prompts, LLM-Filter exhibits exceptional generalization, capable of performing filtering tasks accurately in changed or even unseen environments. We further observe a scaling-law behavior in LLM-Filter, where accuracy improves with larger model sizes and longer training times. These findings make LLM-Filter a promising foundation model of filtering.</li>
</ul>

<h3>Title: From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training</h3>
<ul>
<li><strong>Authors: </strong>Tianqiao Liu, Xueyi Li, Hao Wang, Haoxuan Li, Zhichao Chen, Weiqi Luo, Zitao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20072">https://arxiv.org/abs/2509.20072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20072">https://arxiv.org/pdf/2509.20072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20072]] From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training(https://arxiv.org/abs/2509.20072)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-in speech-out conversational systems. However, existing multimodal models handling interleaved audio and text, such as MOSHI require complex multi stage training pipelines, incurring substantial computational costs. Moreover, these models uniformly apply autoregressive generation to both text and audio tokens, overlooking a fundamental asymmetry in their dependency structures: while text tokens exhibit strong target target dependencies requiring causal ordering, audio tokens are predominantly driven by source target dependencies, where audio outputs primarily condition on source text rather than preceding audio tokens. In this work, we propose TtT, a unified audio-text modeling framework that integrates AR text generation with non-autoregressive audio diffusion within a single Transformer architecture initialized from a pretrained LLM.</li>
</ul>

<h3>Title: Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Zizheng Yang, Hu Yu, Bing Li, Jinghao Zhang, Jie Huang, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20091">https://arxiv.org/abs/2509.20091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20091">https://arxiv.org/pdf/2509.20091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20091]] Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing(https://arxiv.org/abs/2509.20091)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently been investigated as powerful generative solvers for image dehazing, owing to their remarkable capability to model the data distribution. However, the massive computational burden imposed by the retraining of diffusion models, coupled with the extensive sampling steps during the inference, limit the broader application of diffusion models in image dehazing. To address these issues, we explore the properties of hazy images in the semantic latent space of frozen pre-trained diffusion models, and propose a Diffusion Latent Inspired network for Image Dehazing, dubbed DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of pre-trained diffusion models can represent the content and haze characteristics of hazy images, as the diffusion time-step changes. Building upon this insight, we integrate the diffusion latent representations at different time-steps into a delicately designed dehazing network to provide instructions for image dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative sampling process by effectively utilizing the informative representations derived from the pre-trained diffusion models, which also offers a novel perspective for introducing diffusion models to image dehazing. Extensive experiments on multiple datasets demonstrate that the proposed method achieves superior performance to existing image dehazing methods. Code is available at this https URL.</li>
</ul>

<h3>Title: Incomplete Data, Complete Dynamics: A Diffusion Approach</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Chenguang Wang, Hongyi Ye, Yongtao Guan, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20098">https://arxiv.org/abs/2509.20098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20098">https://arxiv.org/pdf/2509.20098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20098]] Incomplete Data, Complete Dynamics: A Diffusion Approach(https://arxiv.org/abs/2509.20098)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning physical dynamics from data is a fundamental challenge in machine learning and scientific modeling. Real-world observational data are inherently incomplete and irregularly sampled, posing significant challenges for existing data-driven approaches. In this work, we propose a principled diffusion-based framework for learning physical systems from incomplete training samples. To this end, our method strategically partitions each such sample into observed context and unobserved query components through a carefully designed splitting strategy, then trains a conditional diffusion model to reconstruct the missing query portions given available contexts. This formulation enables accurate imputation across arbitrary observation patterns without requiring complete data supervision. Specifically, we provide theoretical analysis demonstrating that our diffusion training paradigm on incomplete data achieves asymptotic convergence to the true complete generative process under mild regularity conditions. Empirically, we show that our method significantly outperforms existing baselines on synthetic and real-world physical dynamics benchmarks, including fluid flows and weather systems, with particularly strong performance in limited and irregular observation regimes. These results demonstrate the effectiveness of our theoretically principled approach for learning and imputing partially observed dynamics.</li>
</ul>

<h3>Title: Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>JuanaJuana Valeria Hurtado, Rohit Mohan, Abhinav Valada</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20107">https://arxiv.org/abs/2509.20107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20107">https://arxiv.org/pdf/2509.20107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20107]] Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models(https://arxiv.org/abs/2509.20107)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) captures spatial information along with dense spectral measurements across numerous narrow wavelength bands. This rich spectral content has the potential to facilitate robust robotic perception, particularly in environments with complex material compositions, varying illumination, or other visually challenging conditions. However, current HSI semantic segmentation methods underperform due to their reliance on architectures and learning frameworks optimized for RGB inputs. In this work, we propose a novel hyperspectral adapter that leverages pretrained vision foundation models to effectively learn from hyperspectral data. Our architecture incorporates a spectral transformer and a spectrum-aware spatial prior module to extract rich spatial-spectral features. Additionally, we introduce a modality-aware interaction block that facilitates effective integration of hyperspectral representations and frozen vision Transformer features through dedicated extraction and injection mechanisms. Extensive evaluations on three benchmark autonomous driving datasets demonstrate that our architecture achieves state-of-the-art semantic segmentation performance while directly using HSI inputs, outperforming both vision-based and hyperspectral segmentation methods. We make the code available at this https URL.</li>
</ul>

<h3>Title: Discovering Association Rules in High-Dimensional Small Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Erkan Karabulut, Daniel Daza, Paul Groth, Victoria Degeler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20113">https://arxiv.org/abs/2509.20113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20113">https://arxiv.org/pdf/2509.20113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20113]] Discovering Association Rules in High-Dimensional Small Tabular Data(https://arxiv.org/abs/2509.20113)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Association Rule Mining (ARM) aims to discover patterns between features in datasets in the form of propositional rules, supporting both knowledge discovery and interpretable machine learning in high-stakes decision-making. However, in high-dimensional settings, rule explosion and computational overhead render popular algorithmic approaches impractical without effective search space reduction, challenges that propagate to downstream tasks. Neurosymbolic methods, such as Aerial+, have recently been proposed to address the rule explosion in ARM. While they tackle the high dimensionality of the data, they also inherit limitations of neural networks, particularly reduced performance in low-data regimes. This paper makes three key contributions to association rule discovery in high-dimensional tabular data. First, we empirically show that Aerial+ scales one to two orders of magnitude better than state-of-the-art algorithmic and neurosymbolic baselines across five real-world datasets. Second, we introduce the novel problem of ARM in high-dimensional, low-data settings, such as gene expression data from the biomedicine domain with around 18k features and 50 samples. Third, we propose two fine-tuning approaches to Aerial+ using tabular foundation models. Our proposed approaches are shown to significantly improve rule quality on five real-world datasets, demonstrating their effectiveness in low-data, high-dimensional scenarios.</li>
</ul>

<h3>Title: U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT</h3>
<ul>
<li><strong>Authors: </strong>Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20154">https://arxiv.org/abs/2509.20154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20154">https://arxiv.org/pdf/2509.20154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20154]] U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT(https://arxiv.org/abs/2509.20154)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography (CBCT) is vital for clinical applications like treatment planning and diagnosis. However, this process requires extensive expertise and is exceptionally time-consuming, highlighting the critical need for automated algorithms that can effectively utilize unlabeled data. In this paper, we propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on the U-Mamba2 model and employs a multi-stage training strategy. The framework first pre-trains U-Mamba2 in a self-supervised manner using a disruptive autoencoder. It then leverages unlabeled data through consistency regularization, where we introduce input and feature perturbations to ensure stable model outputs. Finally, a pseudo-labeling strategy is implemented with a reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL achieved an average score of 0.872 and a DSC of 0.969 on the validation dataset, demonstrating the superior performance of our approach. The code is available at this https URL.</li>
</ul>

<h3>Title: Generative Model Inversion Through the Lens of the Manifold Hypothesis</h3>
<ul>
<li><strong>Authors: </strong>Xiong Peng, Bo Han, Fengfei Yu, Tongliang Liu, Feng Liu, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20177">https://arxiv.org/abs/2509.20177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20177">https://arxiv.org/pdf/2509.20177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20177]] Generative Model Inversion Through the Lens of the Manifold Hypothesis(https://arxiv.org/abs/2509.20177)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Model inversion attacks (MIAs) aim to reconstruct class-representative samples from trained models. Recent generative MIAs utilize generative adversarial networks to learn image priors that guide the inversion process, yielding reconstructions with high visual quality and strong fidelity to the private training data. To explore the reason behind their effectiveness, we begin by examining the gradients of inversion loss with respect to synthetic inputs, and find that these gradients are surprisingly noisy. Further analysis reveals that generative inversion implicitly denoises these gradients by projecting them onto the tangent space of the generator manifold, filtering out off-manifold components while preserving informative directions aligned with the manifold. Our empirical measurements show that, in models trained with standard supervision, loss gradients often exhibit large angular deviations from the data manifold, indicating poor alignment with class-relevant directions. This observation motivates our central hypothesis: models become more vulnerable to MIAs when their loss gradients align more closely with the generator manifold. We validate this hypothesis by designing a novel training objective that explicitly promotes such alignment. Building on this insight, we further introduce a training-free approach to enhance gradient-manifold alignment during inversion, leading to consistent improvements over state-of-the-art generative MIAs.</li>
</ul>

<h3>Title: An Improved Time Series Anomaly Detection by Applying Structural Similarity</h3>
<ul>
<li><strong>Authors: </strong>Tiejun Wang, Rui Wang, Xudong Mou, Mengyuan Ma, Tianyu Wo, Renyu Yang, Xudong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20184">https://arxiv.org/abs/2509.20184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20184">https://arxiv.org/pdf/2509.20184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20184]] An Improved Time Series Anomaly Detection by Applying Structural Similarity(https://arxiv.org/abs/2509.20184)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Effective anomaly detection in time series is pivotal for modern industrial applications and financial systems. Due to the scarcity of anomaly labels and the high cost of manual labeling, reconstruction-based unsupervised approaches have garnered considerable attention. However, accurate anomaly detection remains an unsettled challenge, since the optimization objectives of reconstruction-based methods merely rely on point-by-point distance measures, ignoring the potential structural characteristics of time series and thus failing to tackle complex pattern-wise anomalies. In this paper, we propose StrAD, a novel structure-enhanced anomaly detection approach to enrich the optimization objective by incorporating structural information hidden in the time series and steering the data reconstruction procedure to better capture such structural features. StrAD accommodates the trend, seasonality, and shape in the optimization objective of the reconstruction model to learn latent structural characteristics and capture the intrinsic pattern variation of time series. The proposed structure-aware optimization objective mechanism can assure the alignment between the original data and the reconstructed data in terms of structural features, thereby keeping consistency in global fluctuation and local characteristics. The mechanism is pluggable and applicable to any reconstruction-based methods, enhancing the model sensitivity to both point-wise anomalies and pattern-wise anomalies. Experimental results show that StrAD improves the performance of state-of-the-art reconstruction-based models across five real-world anomaly detection datasets.</li>
</ul>

<h3>Title: 4D Driving Scene Generation With Stereo Forcing</h3>
<ul>
<li><strong>Authors: </strong>Hao Lu, Zhuang Ma, Guangfeng Jiang, Wenhang Ge, Bohan Li, Yuzhan Cai, Wenzhao Zheng, Yunpeng Zhang, Yingcong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20251">https://arxiv.org/abs/2509.20251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20251">https://arxiv.org/pdf/2509.20251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20251]] 4D Driving Scene Generation With Stereo Forcing(https://arxiv.org/abs/2509.20251)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{this https URL}{PhiGensis}.</li>
</ul>

<h3>Title: A Versatile Foundation Model for AI-enabled Mammogram Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Fuxiang Huang, Jiayi Zhu, Yunfang Yu, Yu Xie, Yuan Guo, Qingcong Kong, Mingxiang Wu, Xinrui Jiang, Shu Yang, Jiabo Ma, Ziyi Liu, Zhe Xu, Zhixuan Chen, Yujie Tan, Zifan He, Luhui Mao, Xi Wang, Junlin Hou, Lei Zhang, Qiong Luo, Zhenhui Li, Herui Yao, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20271">https://arxiv.org/abs/2509.20271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20271">https://arxiv.org/pdf/2509.20271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20271]] A Versatile Foundation Model for AI-enabled Mammogram Interpretation(https://arxiv.org/abs/2509.20271)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Breast cancer is the most commonly diagnosed cancer and the leading cause of cancer-related mortality in women globally. Mammography is essential for the early detection and diagnosis of breast lesions. Despite recent progress in foundation models (FMs) for mammogram analysis, their clinical translation remains constrained by several fundamental limitations, including insufficient diversity in training data, limited model generalizability, and a lack of comprehensive evaluation across clinically relevant tasks. Here, we introduce VersaMammo, a versatile foundation model for mammograms, designed to overcome these limitations. We curated the largest multi-institutional mammogram dataset to date, comprising 706,239 images from 21 sources. To improve generalization, we propose a two-stage pre-training strategy to develop VersaMammo, a mammogram foundation model. First, a teacher model is trained via self-supervised learning to extract transferable features from unlabeled mammograms. Then, supervised learning combined with knowledge distillation transfers both features and clinical knowledge into VersaMammo. To ensure a comprehensive evaluation, we established a benchmark comprising 92 specific tasks, including 68 internal tasks and 24 external validation tasks, spanning 5 major clinical task categories: lesion detection, segmentation, classification, image retrieval, and visual question answering. VersaMammo achieves state-of-the-art performance, ranking first in 50 out of 68 specific internal tasks and 20 out of 24 external validation tasks, with average ranks of 1.5 and 1.2, respectively. These results demonstrate its superior generalization and clinical utility, offering a substantial advancement toward reliable and scalable breast cancer screening and diagnosis.</li>
</ul>

<h3>Title: FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xichen Xu, Yanshu Wang, Jinbao Wang, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20295">https://arxiv.org/abs/2509.20295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20295">https://arxiv.org/pdf/2509.20295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20295]] FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis(https://arxiv.org/abs/2509.20295)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: this https URL.</li>
</ul>

<h3>Title: Video models are zero-shot learners and reasoners</h3>
<ul>
<li><strong>Authors: </strong>Thadd√§us Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20328">https://arxiv.org/abs/2509.20328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20328">https://arxiv.org/pdf/2509.20328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20328]] Video models are zero-shot learners and reasoners(https://arxiv.org/abs/2509.20328)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.</li>
</ul>

<h3>Title: PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chen Wang, Chuhao Chen, Yiming Huang, Zhiyang Dou, Yuan Liu, Jiatao Gu, Lingjie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20358">https://arxiv.org/abs/2509.20358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20358">https://arxiv.org/pdf/2509.20358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20358]] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation(https://arxiv.org/abs/2509.20358)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing video generation models excel at producing photo-realistic videos from text or images, but often lack physical plausibility and 3D controllability. To overcome these limitations, we introduce PhysCtrl, a novel framework for physics-grounded image-to-video generation with physical parameters and force control. At its core is a generative physics network that learns the distribution of physical dynamics across four materials (elastic, sand, plasticine, and rigid) via a diffusion model conditioned on physics parameters and applied forces. We represent physical dynamics as 3D point trajectories and train on a large-scale synthetic dataset of 550K animations generated by physics simulators. We enhance the diffusion model with a novel spatiotemporal attention block that emulates particle interactions and incorporates physics-based constraints during training to enforce physical plausibility. Experiments show that PhysCtrl generates realistic, physics-grounded motion trajectories which, when used to drive image-to-video models, yield high-fidelity, controllable videos that outperform existing methods in both visual quality and physical plausibility. Project Page: this https URL</li>
</ul>

<h3>Title: EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20360">https://arxiv.org/abs/2509.20360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20360">https://arxiv.org/pdf/2509.20360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20360]] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning(https://arxiv.org/abs/2509.20360)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
