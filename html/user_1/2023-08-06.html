<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01472">http://arxiv.org/abs/2308.01472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01472]] Reverse Stable Diffusion: What prompt was used to generate this image?(http://arxiv.org/abs/2308.01472)</code></li>
<li>Summary: <p>Text-to-image diffusion models such as Stable Diffusion have recently
attracted the interest of many researchers, and inverting the diffusion process
can play an important role in better understanding the generative process and
how to engineer prompts in order to obtain the desired images. To this end, we
introduce the new task of predicting the text prompt given an image generated
by a generative diffusion model. We combine a series of white-box and black-box
models (with and without access to the weights of the diffusion network) to
deal with the proposed task. We propose a novel learning framework comprising
of a joint prompt regression and multi-label vocabulary classification
objective that generates improved prompts. To further improve our method, we
employ a curriculum learning procedure that promotes the learning of
image-prompt pairs with lower labeling noise (i.e. that are better aligned),
and an unsupervised domain-adaptive kernel learning method that uses the
similarities between samples in the source and target domains as extra
features. We conduct experiments on the DiffusionDB data set, predicting text
prompts from images generated by Stable Diffusion. Our novel learning framework
produces excellent results on the aforementioned task, yielding the highest
gains when applied on the white-box model. In addition, we make an interesting
discovery: training a diffusion model on the prompt generation task can make
the model generate images that are much better aligned with the input prompts,
when the model is directly reused for text-to-image generation.
</p></li>
</ul>

<h3>Title: Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models. (arXiv:2308.01594v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01594">http://arxiv.org/abs/2308.01594</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01594]] Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models(http://arxiv.org/abs/2308.01594)</code></li>
<li>Summary: <p>Electron microscopy (EM) images exhibit anisotropic axial resolution due to
the characteristics inherent to the imaging modality, presenting challenges in
analysis and downstream tasks.In this paper, we propose a diffusion-model-based
framework that overcomes the limitations of requiring reference data or prior
knowledge about the degradation process. Our approach utilizes 2D diffusion
models to consistently reconstruct 3D volumes and is well-suited for highly
downsampled data. Extensive experiments conducted on two public datasets
demonstrate the robustness and superiority of leveraging the generative prior
compared to supervised learning methods. Additionally, we demonstrate our
method's feasibility for self-supervised reconstruction, which can restore a
single anisotropic volume without any training data.
</p></li>
</ul>

<h3>Title: DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models. (arXiv:2308.01655v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01655">http://arxiv.org/abs/2308.01655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01655]] DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models(http://arxiv.org/abs/2308.01655)</code></li>
<li>Summary: <p>Recent data-driven image colorization methods have enabled automatic or
reference-based colorization, while still suffering from unsatisfactory and
inaccurate object-level color control. To address these issues, we propose a
new method called DiffColor that leverages the power of pre-trained diffusion
models to recover vivid colors conditioned on a prompt text, without any
additional inputs. DiffColor mainly contains two stages: colorization with
generative color prior and in-context controllable colorization. Specifically,
we first fine-tune a pre-trained text-to-image model to generate colorized
images using a CLIP-based contrastive loss. Then we try to obtain an optimized
text embedding aligning the colorized image and the text prompt, and a
fine-tuned diffusion model enabling high-quality image reconstruction. Our
method can produce vivid and diverse colors with a few iterations, and keep the
structure and background intact while having colors well-aligned with the
target language guidance. Moreover, our method allows for in-context
colorization, i.e., producing different colorization results by modifying
prompt texts without any fine-tuning, and can achieve object-level controllable
colorization results. Extensive experiments and user studies demonstrate that
DiffColor outperforms previous works in terms of visual quality, color
fidelity, and diversity of colorization options.
</p></li>
</ul>

<h3>Title: Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling. (arXiv:2308.01850v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01850">http://arxiv.org/abs/2308.01850</a></li>
<li>Code URL: https://github.com/yangzhao1230/pcmdm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01850]] Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling(http://arxiv.org/abs/2308.01850)</code></li>
<li>Summary: <p>Text-to-motion generation has gained increasing attention, but most existing
methods are limited to generating short-term motions that correspond to a
single sentence describing a single action. However, when a text stream
describes a sequence of continuous motions, the generated motions corresponding
to each sentence may not be coherently linked. Existing long-term motion
generation methods face two main issues. Firstly, they cannot directly generate
coherent motions and require additional operations such as interpolation to
process the generated actions. Secondly, they generate subsequent actions in an
autoregressive manner without considering the influence of future actions on
previous ones. To address these issues, we propose a novel approach that
utilizes a past-conditioned diffusion model with two optional coherent sampling
methods: Past Inpainting Sampling and Compositional Transition Sampling. Past
Inpainting Sampling completes subsequent motions by treating previous motions
as conditions, while Compositional Transition Sampling models the distribution
of the transition as the composition of two adjacent motions guided by
different text prompts. Our experimental results demonstrate that our proposed
method is capable of generating compositional and coherent long-term 3D human
motions controlled by a user-instructed long text stream. The code is available
at
\href{https://github.com/yangzhao1230/PCMDM}{https://github.com/yangzhao1230/PCMDM}.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Multimodal Neurons in Pretrained Text-Only Transformers. (arXiv:2308.01544v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01544">http://arxiv.org/abs/2308.01544</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01544]] Multimodal Neurons in Pretrained Text-Only Transformers(http://arxiv.org/abs/2308.01544)</code></li>
<li>Summary: <p>Language models demonstrate remarkable capacity to generalize representations
learned in one modality to downstream tasks in other modalities. Can we trace
this ability to individual neurons? We study the case where a frozen text
transformer is augmented with vision using a self-supervised visual encoder and
a single linear projection learned on an image-to-text task. Outputs of the
projection layer are not immediately decodable into language describing image
content; instead, we find that translation between modalities occurs deeper
within the transformer. We introduce a procedure for identifying "multimodal
neurons" that convert visual representations into corresponding text, and
decoding the concepts they inject into the model's residual stream. In a series
of experiments, we show that multimodal neurons operate on specific visual
concepts across inputs, and have a systematic causal effect on image
captioning.
</p></li>
</ul>

<h3>Title: ReIDTrack: Multi-Object Track and Segmentation Without Motion. (arXiv:2308.01622v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01622">http://arxiv.org/abs/2308.01622</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01622]] ReIDTrack: Multi-Object Track and Segmentation Without Motion(http://arxiv.org/abs/2308.01622)</code></li>
<li>Summary: <p>In recent years, dominant Multi-object tracking (MOT) and segmentation (MOTS)
methods mainly follow the tracking-by-detection paradigm. Transformer-based
end-to-end (E2E) solutions bring some ideas to MOT and MOTS, but they cannot
achieve a new state-of-the-art (SOTA) performance in major MOT and MOTS
benchmarks. Detection and association are two main modules of the
tracking-by-detection paradigm. Association techniques mainly depend on the
combination of motion and appearance information. As deep learning has been
recently developed, the performance of the detection and appearance model is
rapidly improved. These trends made us consider whether we can achieve SOTA
based on only high-performance detection and appearance model. Our paper mainly
focuses on exploring this direction based on CBNetV2 with Swin-B as a detection
model and MoCo-v2 as a self-supervised appearance model. Motion information and
IoU mapping were removed during the association. Our method wins 1st place on
the MOTS track and wins 2nd on the MOT track in the CVPR2023 WAD workshop. We
hope our simple and effective method can give some insights to the MOT and MOTS
research community. Source code will be released under this git repository
</p></li>
</ul>

<h3>Title: Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01399">http://arxiv.org/abs/2308.01399</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01399]] Learning to Model the World with Language(http://arxiv.org/abs/2308.01399)</code></li>
<li>Summary: <p>To interact with humans in the world, agents need to understand the diverse
types of language that people use, relate them to the visual world, and act
based on them. While current agents learn to execute simple language
instructions from task rewards, we aim to build agents that leverage diverse
language that conveys general knowledge, describes the state of the world,
provides interactive feedback, and more. Our key idea is that language helps
agents predict the future: what will be observed, how the world will behave,
and which situations will be rewarded. This perspective unifies language
understanding with future prediction as a powerful self-supervised learning
objective. We present Dynalang, an agent that learns a multimodal world model
that predicts future text and image representations and learns to act from
imagined model rollouts. Unlike traditional agents that use language only to
predict actions, Dynalang acquires rich language understanding by using past
language also to predict future language, video, and rewards. In addition to
learning from online interaction in an environment, Dynalang can be pretrained
on datasets of text, video, or both without actions or rewards. From using
language hints in grid worlds to navigating photorealistic scans of homes,
Dynalang utilizes diverse types of language to improve task performance,
including environment descriptions, game rules, and instructions.
</p></li>
</ul>

<h3>Title: Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation. (arXiv:2308.01831v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01831">http://arxiv.org/abs/2308.01831</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01831]] Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation(http://arxiv.org/abs/2308.01831)</code></li>
<li>Summary: <p>In this paper, we propose a method to learn unified representations of
multilingual speech and text with a single model, especially focusing on the
purpose of speech synthesis. We represent multilingual speech audio with speech
units, the quantized representations of speech features encoded from a
self-supervised speech model. Therefore, we can focus on their linguistic
content by treating the audio as pseudo text and can build a unified
representation of speech and text. Then, we propose to train an encoder-decoder
structured model with a Unit-to-Unit Translation (UTUT) objective on
multilingual data. Specifically, by conditioning the encoder with the source
language token and the decoder with the target language token, the model is
optimized to translate the spoken language into that of the target language, in
a many-to-many language translation setting. Therefore, the model can build the
knowledge of how spoken languages are comprehended and how to relate them to
different languages. A single pre-trained model with UTUT can be employed for
diverse multilingual speech- and text-related tasks, such as Speech-to-Speech
Translation (STS), multilingual Text-to-Speech Synthesis (TTS), and
Text-to-Speech Translation (TTST). By conducting comprehensive experiments
encompassing various languages, we validate the efficacy of the proposed method
across diverse multilingual tasks. Moreover, we show UTUT can perform
many-to-many language STS, which has not been previously explored in the
literature. Samples are available on https://choijeongsoo.github.io/utut.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Supply chain emission estimation using large language models. (arXiv:2308.01741v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01741">http://arxiv.org/abs/2308.01741</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01741]] Supply chain emission estimation using large language models(http://arxiv.org/abs/2308.01741)</code></li>
<li>Summary: <p>Large enterprises face a crucial imperative to achieve the Sustainable
Development Goals (SDGs), especially goal 13, which focuses on combating
climate change and its impacts. To mitigate the effects of climate change,
reducing enterprise Scope 3 (supply chain emissions) is vital, as it accounts
for more than 90\% of total emission inventories. However, tracking Scope 3
emissions proves challenging, as data must be collected from thousands of
upstream and downstream suppliers.To address the above mentioned challenges, we
propose a first-of-a-kind framework that uses domain-adapted NLP foundation
models to estimate Scope 3 emissions, by utilizing financial transactions as a
proxy for purchased goods and services. We compared the performance of the
proposed framework with the state-of-art text classification models such as
TF-IDF, word2Vec, and Zero shot learning. Our results show that the
domain-adapted foundation model outperforms state-of-the-art text mining
techniques and performs as well as a subject matter expert (SME). The proposed
framework could accelerate the Scope 3 estimation at Enterprise scale and will
help to take appropriate climate actions to achieve SDG 13.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Circumventing Concept Erasure Methods For Text-to-Image Generative Models. (arXiv:2308.01508v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01508">http://arxiv.org/abs/2308.01508</a></li>
<li>Code URL: https://github.com/nyu-dice-lab/circumventing-concept-erasure</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01508]] Circumventing Concept Erasure Methods For Text-to-Image Generative Models(http://arxiv.org/abs/2308.01508)</code></li>
<li>Summary: <p>Text-to-image generative models can produce photo-realistic images for an
extremely broad range of concepts, and their usage has proliferated widely
among the general public. On the flip side, these models have numerous
drawbacks, including their potential to generate images featuring sexually
explicit content, mirror artistic styles without permission, or even
hallucinate (or deepfake) the likenesses of celebrities. Consequently, various
methods have been proposed in order to "erase" sensitive concepts from
text-to-image models. In this work, we examine five recently proposed concept
erasure methods, and show that targeted concepts are not fully excised from any
of these methods. Specifically, we leverage the existence of special learned
word embeddings that can retrieve "erased" concepts from the sanitized models
with no alterations to their weights. Our results highlight the brittleness of
post hoc concept erasure methods, and call into question their use in the
algorithmic toolkit for AI safety.
</p></li>
</ul>

<h3>Title: Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01626">http://arxiv.org/abs/2308.01626</a></li>
<li>Code URL: https://github.com/alexmotogna/generatorapi</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01626]] Interleaving GANs with knowledge graphs to support design creativity for book covers(http://arxiv.org/abs/2308.01626)</code></li>
<li>Summary: <p>An attractive book cover is important for the success of a book. In this
paper, we apply Generative Adversarial Networks (GANs) to the book covers
domain, using different methods for training in order to obtain better
generated images. We interleave GANs with knowledge graphs to alter the input
title to obtain multiple possible options for any given title, which are then
used as an augmented input to the generator. Finally, we use the discriminator
obtained during the training phase to select the best images generated with new
titles. Our method performed better at generating book covers than previous
attempts, and the knowledge graph gives better options to the book author or
editor compared to using GANs alone.
</p></li>
</ul>

<h3>Title: BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout. (arXiv:2308.01661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01661">http://arxiv.org/abs/2308.01661</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01661]] BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout(http://arxiv.org/abs/2308.01661)</code></li>
<li>Summary: <p>Using synthesized images to boost the performance of perception models is a
long-standing research challenge in computer vision. It becomes more eminent in
visual-centric autonomous driving systems with multi-view cameras as some
long-tail scenarios can never be collected. Guided by the BEV segmentation
layouts, the existing generative networks seem to synthesize photo-realistic
street-view images when evaluated solely on scene-level metrics. However, once
zoom-in, they usually fail to produce accurate foreground and background
details such as heading. To this end, we propose a two-stage generative method,
dubbed BEVControl, that can generate accurate foreground and background
contents. In contrast to segmentation-like input, it also supports sketch style
input, which is more flexible for humans to edit. In addition, we propose a
comprehensive multi-level evaluation protocol to fairly compare the quality of
the generated scene, foreground object, and background geometry. Our extensive
experiments show that our BEVControl surpasses the state-of-the-art method,
BEVGen, by a significant margin, from 5.89 to 26.80 on foreground segmentation
mIoU. In addition, we show that using images generated by BEVControl to train
the downstream perception model, it achieves on average 1.29 improvement in NDS
score.
</p></li>
</ul>

<h3>Title: Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment. (arXiv:2308.01771v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01771">http://arxiv.org/abs/2308.01771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01771]] Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment(http://arxiv.org/abs/2308.01771)</code></li>
<li>Summary: <p>This study investigated the potential of end-to-end deep learning tools as a
more effective substitute for FEM in predicting stress-strain fields within 2D
cross sections of arterial wall. We first proposed a U-Net based fully
convolutional neural network (CNN) to predict the von Mises stress and strain
distribution based on the spatial arrangement of calcification within arterial
wall cross-sections. Further, we developed a conditional generative adversarial
network (cGAN) to enhance, particularly from the perceptual perspective, the
prediction accuracy of stress and strain field maps for arterial walls with
various calcification quantities and spatial configurations. On top of U-Net
and cGAN, we also proposed their ensemble approaches, respectively, to further
improve the prediction accuracy of field maps. Our dataset, consisting of input
and output images, was generated by implementing boundary conditions and
extracting stress-strain field maps. The trained U-Net models can accurately
predict von Mises stress and strain fields, with structural similarity index
scores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 for
stress and strain, respectively, on a reserved test set. Meanwhile, the cGAN
models in a combination of ensemble and transfer learning techniques
demonstrate high accuracy in predicting von Mises stress and strain fields, as
evidenced by SSIM scores of 0.890 for stress and 0.803 for strain.
Additionally, mean squared errors of 0.008 for stress and 0.017 for strain
further support the model's performance on a designated test set. Overall, this
study developed a surrogate model for finite element analysis, which can
accurately and efficiently predict stress-strain fields of arterial walls
regardless of complex geometries and boundary conditions.
</p></li>
</ul>

<h3>Title: An End-to-end Food Portion Estimation Framework Based on Shape Reconstruction from Monocular Image. (arXiv:2308.01810v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01810">http://arxiv.org/abs/2308.01810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01810]] An End-to-end Food Portion Estimation Framework Based on Shape Reconstruction from Monocular Image(http://arxiv.org/abs/2308.01810)</code></li>
<li>Summary: <p>Dietary assessment is a key contributor to monitoring health status. Existing
self-report methods are tedious and time-consuming with substantial biases and
errors. Image-based food portion estimation aims to estimate food energy values
directly from food images, showing great potential for automated dietary
assessment solutions. Existing image-based methods either use a single-view
image or incorporate multi-view images and depth information to estimate the
food energy, which either has limited performance or creates user burdens. In
this paper, we propose an end-to-end deep learning framework for food energy
estimation from a monocular image through 3D shape reconstruction. We leverage
a generative model to reconstruct the voxel representation of the food object
from the input image to recover the missing 3D information. Our method is
evaluated on a publicly available food image dataset Nutrition5k, resulting a
Mean Absolute Error (MAE) of 40.05 kCal and Mean Absolute Percentage Error
(MAPE) of 11.47% for food energy estimation. Our method uses RGB image as the
only input at the inference stage and achieves competitive results compared to
the existing method requiring both RGB and depth information.
</p></li>
</ul>

<h3>Title: LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01413">http://arxiv.org/abs/2308.01413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01413]] LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning(http://arxiv.org/abs/2308.01413)</code></li>
<li>Summary: <p>Transformer-based models have revolutionized the performance of a wide range
of language tasks. Intuitively, one might expect text classification, which
does not necessitate as many high-level representations as generative tasks, to
be comprehensively addressed with the powerful representation capabilities of
Transformers. However, in reality, there remains significant potential for
enhancement, particularly in the areas of multi-class and multi-label
classification of lengthy textual documents and other large files. The
performance of Transformer-based models is mainly hindered by a major
limitation: a restricted input length, e.g., 512 tokens for BERT. While an
increase in GPU memory can marginally extend this limit, practical real-world
applications often operate under constrained GPU resources. In this work, we
tackle the input limit problem from the perspective of correlated multiple
instance learning. The proposed approach, LaFiCMIL, serves as a versatile
framework applicable to various large file classification tasks covering
binary, multi-class, and multi-label classification tasks, spanning various
domains including Natural Language Processing, Programming Language Processing,
and Android Analysis. To evaluate its effectiveness, we employ eight benchmark
datasets pertaining to Long Document Classification, Code Defect Detection, and
Android Malware Detection. Leveraging BERT-family models as feature extractors,
our experimental results demonstrate that LaFiCMIL achieves new
state-of-the-art performance across all benchmark datasets. This is largely
attributable to its capability of scaling BERT up to nearly 20K tokens, running
on a single Tesla V-100 GPU with 32G of memory.
</p></li>
</ul>

<h3>Title: Local Large Language Models for Complex Structured Medical Tasks. (arXiv:2308.01727v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01727">http://arxiv.org/abs/2308.01727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01727]] Local Large Language Models for Complex Structured Medical Tasks(http://arxiv.org/abs/2308.01727)</code></li>
<li>Summary: <p>This paper introduces an approach that combines the language reasoning
capabilities of large language models (LLMs) with the benefits of local
training to tackle complex, domain-specific tasks. Specifically, the authors
demonstrate their approach by extracting structured condition codes from
pathology reports. The proposed approach utilizes local LLMs, which can be
fine-tuned to respond to specific generative instructions and provide
structured outputs. The authors collected a dataset of over 150k uncurated
surgical pathology reports, containing gross descriptions, final diagnoses, and
condition codes. They trained different model architectures, including LLaMA,
BERT and LongFormer and evaluated their performance. The results show that the
LLaMA-based models significantly outperform BERT-style models across all
evaluated metrics, even with extremely reduced precision. The LLaMA models
performed especially well with large datasets, demonstrating their ability to
handle complex, multi-label tasks. Overall, this work presents an effective
approach for utilizing LLMs to perform domain-specific tasks using accessible
hardware, with potential applications in the medical domain, where complex data
extraction and classification are required.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Harder synthetic anomalies to improve OoD detection in Medical Images. (arXiv:2308.01412v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01412">http://arxiv.org/abs/2308.01412</a></li>
<li>Code URL: https://github.com/snavalm/mood22</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01412]] Harder synthetic anomalies to improve OoD detection in Medical Images(http://arxiv.org/abs/2308.01412)</code></li>
<li>Summary: <p>Our method builds upon previous Medical Out-of-Distribution (MOOD) challenge
winners that empirically show that synthetic local anomalies generated copying
/ interpolating foreign patches are useful to train segmentation networks able
to generalize to unseen types of anomalies. In terms of the synthetic anomaly
generation process, our contributions makes synthetic anomalies more
heterogeneous and challenging by 1) using random shapes instead of squares and
2) smoothing the interpolation edge of anomalies so networks cannot rely on the
high gradient between image - foreign patch to identify anomalies. Our
experiments using the validation set of 2020 MOOD winners show that both
contributions improved substantially the method performance. We used a standard
3D U-Net architecture as segmentation network, trained patch-wise in both brain
and abdominal datasets. Our final challenge submission consisted of 10 U-Nets
trained across 5 data folds with different configurations of the anomaly
generation process. Our method achieved first position in both sample-wise and
pixel-wise tasks in the 2022 edition of the Medical Out-of-Distribution held at
MICCAI.
</p></li>
</ul>

<h3>Title: Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection. (arXiv:2308.01639v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01639">http://arxiv.org/abs/2308.01639</a></li>
<li>Code URL: https://github.com/mediabrain-sjtu/ecgad</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01639]] Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection(http://arxiv.org/abs/2308.01639)</code></li>
<li>Summary: <p>Electrocardiogram (ECG) is a widely used diagnostic tool for detecting heart
conditions. Rare cardiac diseases may be underdiagnosed using traditional ECG
analysis, considering that no training dataset can exhaust all possible cardiac
disorders. This paper proposes using anomaly detection to identify any
unhealthy status, with normal ECGs solely for training. However, detecting
anomalies in ECG can be challenging due to significant inter-individual
differences and anomalies present in both global rhythm and local morphology.
To address this challenge, this paper introduces a novel multi-scale
cross-restoration framework for ECG anomaly detection and localization that
considers both local and global ECG characteristics. The proposed framework
employs a two-branch autoencoder to facilitate multi-scale feature learning
through a masking and restoration process, with one branch focusing on global
features from the entire ECG and the other on local features from
heartbeat-level details, mimicking the diagnostic process of cardiologists.
Anomalies are identified by their high restoration errors. To evaluate the
performance on a large number of individuals, this paper introduces a new
challenging benchmark with signal point-level ground truths annotated by
experienced cardiologists. The proposed method demonstrates state-of-the-art
performance on this benchmark and two other well-known ECG datasets. The
benchmark dataset and source code are available at:
\url{https://github.com/MediaBrain-SJTU/ECGAD}
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. (arXiv:2308.01684v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.01684">http://arxiv.org/abs/2308.01684</a></li>
<li>Code URL: https://github.com/oooranz/baby-cothought</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.01684]] Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models(http://arxiv.org/abs/2308.01684)</code></li>
<li>Summary: <p>Large Language Models (LLMs) demonstrate remarkable performance on a variety
of Natural Language Understanding (NLU) tasks, primarily due to their
in-context learning ability. This ability is utilized in our proposed
"CoThought" pipeline, which efficiently trains smaller "baby" language models
(BabyLMs) by leveraging the Chain of Thought (CoT) prompting of LLMs. Our
pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo,
transforming it into task-oriented, human-readable texts that are comparable to
the school texts for language learners. The BabyLM is then pretrained on this
restructured dataset in a RoBERTa (Liu et al., 2019) fashion. In evaluations
across 4 benchmarks, our BabyLM outperforms the RoBERTa-base in 10 linguistic,
NLU, and question answering tasks by more than 3 points, showing superior
ability to extract contextual information. These results suggest that compact
LMs pretrained on small, LLM-restructured data can better understand tasks and
achieve improved performance. The code for data processing and model training
is available at: https://github.com/oooranz/Baby-CoThought.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
