<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-19</h1>
<h3>Title: ss-Mamba: Semantic-Spline Selective State-Space Model</h3>
<ul>
<li><strong>Authors: </strong>Zuochen Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14802">https://arxiv.org/abs/2506.14802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14802">https://arxiv.org/pdf/2506.14802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14802]] ss-Mamba: Semantic-Spline Selective State-Space Model(https://arxiv.org/abs/2506.14802)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We propose ss-Mamba, a novel foundation model that enhances time series forecasting by integrating semantic-aware embeddings and adaptive spline-based temporal encoding within a selective state-space modeling framework. Building upon the recent success of Transformer architectures, ss-Mamba adopts the Mamba selective state space model as an efficient alternative that achieves comparable performance while significantly reducing computational complexity from quadratic to linear time. Semantic index embeddings, initialized from pretrained language models, allow effective generalization to previously unseen series through meaningful semantic priors. Additionally, spline-based Kolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex seasonalities and non-stationary temporal effects, providing a powerful enhancement over conventional temporal feature encodings. Extensive experimental evaluations confirm that ss-Mamba delivers superior accuracy, robustness, and interpretability, demonstrating its capability as a versatile and computationally efficient alternative to traditional Transformer-based models in time-series forecasting.</li>
</ul>

<h3>Title: ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes</h3>
<ul>
<li><strong>Authors: </strong>Jun Yin, Jing Zhong, Pengyu Zeng, Peilin Li, Zixuan Dai, Miao Zhang, Shuai Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14832">https://arxiv.org/abs/2506.14832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14832">https://arxiv.org/pdf/2506.14832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14832]] ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes(https://arxiv.org/abs/2506.14832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In contemporary architectural design, the growing complexity and diversity of design demands have made generative plugin tools essential for quickly producing initial concepts and exploring novel 3D forms. However, objectively analyzing the differences between human-designed and machine-generated 3D forms remains a challenge, limiting our understanding of their respective strengths and hindering the advancement of generative tools. To address this, we built ArchForms-4000, a dataset containing 2,000 architect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet, a 3D convolutional neural network tailored for classifying and analyzing architectural forms, incorporating a saliency module to highlight key spatial features aligned with architectural reasoning; And conducted comparative experiments showing our model outperforms human experts in distinguishing form origins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall. This study not only highlights the distinctive advantages of human-designed forms in spatial organization, proportional harmony, and detail refinement but also provides valuable insights for enhancing generative design tools in the future.</li>
</ul>

<h3>Title: PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Lukas Schiesser, Cornelius Wolff, Sophie Haas, Simon Pukrop</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14842">https://arxiv.org/abs/2506.14842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14842">https://arxiv.org/pdf/2506.14842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14842]] PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers(https://arxiv.org/abs/2506.14842)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Building image classification models remains cumbersome in data-scarce domains, where collecting large labeled datasets is impractical. In-context learning (ICL) has emerged as a promising paradigm for few-shot image classification (FSIC), enabling models to generalize across domains without gradient-based adaptation. However, prior work has largely overlooked a critical component of ICL-based FSIC pipelines: the role of image embeddings. In this work, we present PictSure, an ICL framework that places the embedding model -- its architecture, pretraining, and training dynamics -- at the center of analysis. We systematically examine the effects of different visual encoder types, pretraining objectives, and fine-tuning strategies on downstream FSIC performance. Our experiments show that the training success and the out-of-domain performance are highly dependent on how the embedding models are pretrained. Consequently, PictSure manages to outperform existing ICL-based FSIC models on out-of-domain benchmarks that differ significantly from the training distribution, while maintaining comparable results on in-domain tasks. Code can be found at this https URL.</li>
</ul>

<h3>Title: DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Renjith Prasad, Abhilekh Borah, Hasnat Md Abdullah, Chathurangi Shyalika, Gurpreet Singh, Ritvik Garimella, Rajarshi Roy, Harshul Surana, Nasrin Imanpour, Suranjana Trivedy, Amit Sheth, Amitava Das</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14903">https://arxiv.org/abs/2506.14903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14903">https://arxiv.org/pdf/2506.14903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14903]] DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization(https://arxiv.org/abs/2506.14903)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Alignment is crucial for text-to-image (T2I) models to ensure that generated images faithfully capture user intent while maintaining safety and fairness. Direct Preference Optimization (DPO), prominent in large language models (LLMs), is extending its influence to T2I systems. This paper introduces DPO-Kernels for T2I models, a novel extension enhancing alignment across three dimensions: (i) Hybrid Loss, integrating embedding-based objectives with traditional probability-based loss for improved optimization; (ii) Kernelized Representations, employing Radial Basis Function (RBF), Polynomial, and Wavelet kernels for richer feature transformations and better separation between safe and unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's default Kullback-Leibler (KL) regularizer by incorporating Wasserstein and R'enyi divergences for enhanced stability and robustness. We introduce DETONATE, the first large-scale benchmark of its kind, comprising approximately 100K curated image pairs categorized as chosen and rejected. DETONATE encapsulates three axes of social bias and discrimination: Race, Gender, and Disability. Prompts are sourced from hate speech datasets, with images generated by leading T2I models including Stable Diffusion 3.5 Large, Stable Diffusion XL, and Midjourney. Additionally, we propose the Alignment Quality Index (AQI), a novel geometric measure quantifying latent-space separability of safe/unsafe image activations, revealing hidden vulnerabilities. Empirically, we demonstrate that DPO-Kernels maintain strong generalization bounds via Heavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are publicly released.</li>
</ul>

<h3>Title: Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xinkai Zhao, Yuta Tokuoka, Junichiro Iwasawa, Keita Oda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14919">https://arxiv.org/abs/2506.14919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14919">https://arxiv.org/pdf/2506.14919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14919]] Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models(https://arxiv.org/abs/2506.14919)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The increasing use of diffusion models for image generation, especially in sensitive areas like medical imaging, has raised significant privacy concerns. Membership Inference Attack (MIA) has emerged as a potential approach to determine if a specific image was used to train a diffusion model, thus quantifying privacy risks. Existing MIA methods often rely on diffusion reconstruction errors, where member images are expected to have lower reconstruction errors than non-member images. However, applying these methods directly to medical images faces challenges. Reconstruction error is influenced by inherent image difficulty, and diffusion models struggle with high-frequency detail reconstruction. To address these issues, we propose a Frequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical image diffusion models. By focusing on reconstruction errors within a specific mid-frequency range and excluding both high-frequency (difficult to reconstruct) and low-frequency (less informative) regions, our frequency-selective approach mitigates the confounding factor of inherent image difficulty. Specifically, we analyze the reverse diffusion process, obtain the mid-frequency reconstruction error, and compute the structural similarity index score between the reconstructed and original images. Membership is determined by comparing this score to a threshold. Experiments on several medical image datasets demonstrate that our FCRE method outperforms existing MIA methods.</li>
</ul>

<h3>Title: Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Luan Gonçalves Miranda, Pedro Ivo da Cruz, Murilo Bellezoni Loiola</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.NI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14937">https://arxiv.org/abs/2506.14937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14937">https://arxiv.org/pdf/2506.14937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14937]] Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders(https://arxiv.org/abs/2506.14937)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Currently, digital security mechanisms like Anomaly Detection Systems using Autoencoders (AE) show great potential for bypassing problems intrinsic to the data, such as data imbalance. Because AE use a non-trivial and nonstandardized separation threshold to classify the extracted reconstruction error, the definition of this threshold directly impacts the performance of the detection process. Thus, this work proposes the automatic definition of this threshold using some machine learning algorithms. For this, three algorithms were evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.</li>
</ul>

<h3>Title: Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Maxime Usdin, Lito Kriara, Licinio Craveiro</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.14986">https://arxiv.org/abs/2506.14986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.14986">https://arxiv.org/pdf/2506.14986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.14986]] Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks(https://arxiv.org/abs/2506.14986)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Early multiple sclerosis (MS) disability progression prediction is challenging due to disease heterogeneity. This work predicts 48- and 72-week disability using sparse baseline clinical data and 12 weeks of daily digital Floodlight data from the CONSONANCE clinical trial. We employed state-of-the-art tabular and time-series foundation models (FMs), a custom multimodal attention-based transformer, and machine learning methods. Despite the difficulty of early prediction (AUROC 0.63), integrating digital data via advanced models improved performance over clinical data alone. A transformer model using unimodal embeddings from the Moment FM yielded the best result, but our multimodal transformer consistently outperformed its unimodal counterpart, confirming the advantages of combining clinical with digital data. Our findings demonstrate the promise of FMs and multimodal approaches to extract predictive signals from complex and diverse clinical and digital life sciences data (e.g., imaging, omics), enabling more accurate prognostics for MS and potentially other complex diseases.</li>
</ul>

<h3>Title: Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?</h3>
<ul>
<li><strong>Authors: </strong>Gary Song Yan, Yusen Zhang, Jinyu Zhao, Hao Zhang, Zhangping Yang, Guanye Xiong, Yanfei Liu, Tao Zhang, Yujie He, Siyuan Tian, Yao Gou, Min Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15033">https://arxiv.org/abs/2506.15033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15033">https://arxiv.org/pdf/2506.15033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15033]] Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?(https://arxiv.org/abs/2506.15033)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this pioneering study, we introduce StyleWallfacer, a groundbreaking unified training and inference framework, which not only addresses various issues encountered in the style transfer process of traditional methods but also unifies the framework for different tasks. This framework is designed to revolutionize the field by enabling artist level style transfer and text driven stylization. First, we propose a semantic-based style injection method that uses BLIP to generate text descriptions strictly aligned with the semantics of the style image in CLIP space. By leveraging a large language model to remove style-related descriptions from these descriptions, we create a semantic gap. This gap is then used to fine-tune the model, enabling efficient and drift-free injection of style knowledge. Second, we propose a data augmentation strategy based on human feedback, incorporating high-quality samples generated early in the fine-tuning process into the training set to facilitate progressive learning and significantly reduce its overfitting. Finally, we design a training-free triple diffusion process using the fine-tuned model, which manipulates the features of self-attention layers in a manner similar to the cross-attention mechanism. Specifically, in the generation process, the key and value of the content-related process are replaced with those of the style-related process to inject style while maintaining text control over the model. We also introduce query preservation to mitigate disruptions to the original content. Under such a design, we have achieved high-quality image-driven style transfer and text-driven stylization, delivering artist-level style transfer results while preserving the original image content. Moreover, we achieve image color editing during the style transfer process for the first time.</li>
</ul>

<h3>Title: CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets</h3>
<ul>
<li><strong>Authors: </strong>Samhita Kuili, Mohammadreza Amini, Burak Kantarci</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15075">https://arxiv.org/abs/2506.15075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15075">https://arxiv.org/pdf/2506.15075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15075]] CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets(https://arxiv.org/abs/2506.15075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the ever-expanding domain of 5G-NR wireless cellular networks, over-the-air jamming attacks are prevalent as security attacks, compromising the quality of the received signal. We simulate a jamming environment by incorporating additive white Gaussian noise (AWGN) into the real-world In-phase and Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is exploited to implement a jamming detection over various characteristics such as heterogenous I/Q datasets; extracting relevant information on Synchronization Signal Blocks (SSBs), and fewer SSB observations with notable class imbalance. Given the characteristics of datasets, balanced datasets are acquired by employing a Conv1D conditional Wasserstein Generative Adversarial Network-Gradient Penalty(CWGAN-GP) on both majority and minority SSB observations. Additionally, we compare the performance and detection ability of the proposed CAE model on augmented datasets with benchmark models: Convolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder (CSAE). Despite the complexity of data heterogeneity involved across all datasets, CAE depicts the robustness in detection performance of jammed signal by achieving average values of 97.33% precision, 91.33% recall, 94.08% F1-score, and 94.35% accuracy over CDAE and CSAE.</li>
</ul>

<h3>Title: Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jing Yang Lee, Kong-Aik Lee, Woon-Seng Gan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15131">https://arxiv.org/abs/2506.15131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15131">https://arxiv.org/pdf/2506.15131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15131]] Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs(https://arxiv.org/abs/2506.15131)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.</li>
</ul>

<h3>Title: Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gyeongje Cho, Yeonkyoun So, Chanwoo Park, Sangmin Lee, Sungmok Jung, Jaejin Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15138">https://arxiv.org/abs/2506.15138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15138">https://arxiv.org/pdf/2506.15138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15138]] Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models(https://arxiv.org/abs/2506.15138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce token fertility without compromising model performance. Our approach uses a rule-based pre-tokenization method that aligns with the linguistic structure of the Korean language. We also create a seed vocabulary containing tokens that resemble linguistic units and employ a branching entropy-based selection algorithm. These techniques increase the average token length, thus lowering fertility while preserving linguistic information. Experimental results indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces the number of tokens by 10%, improving the inference speed by 10%) compared to BPE without compromising performance across various downstream tasks. These findings demonstrate that our linguistically informed approach is effective and practical for designing efficient tokenizers for language models.</li>
</ul>

<h3>Title: Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography</h3>
<ul>
<li><strong>Authors: </strong>Abdur Rahman, Keerthiveena Balraj, Manojkumar Ramteke, Anurag Singh Rathore</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15166">https://arxiv.org/abs/2506.15166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15166">https://arxiv.org/pdf/2506.15166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15166]] Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography(https://arxiv.org/abs/2506.15166)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion probabilistic models (DPMs) have revolutionized image processing, demonstrating significant potential in medical applications. Accurate segmentation of the left ventricle (LV) in echocardiograms is crucial for diagnostic procedures and necessary treatments. However, ultrasound images are notoriously noisy with low contrast and ambiguous LV boundaries, thereby complicating the segmentation process. To address these challenges, this paper introduces Echo-DND, a novel dual-noise diffusion model specifically designed for this task. Echo-DND leverages a unique combination of Gaussian and Bernoulli noises. It also incorporates a multi-scale fusion conditioning module to improve segmentation precision. Furthermore, it utilizes spatial coherence calibration to maintain spatial integrity in segmentation masks. The model's performance was rigorously validated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations demonstrate that the proposed framework outperforms existing SOTA models. It achieves high Dice scores of 0.962 and 0.939 on these datasets, respectively. The proposed Echo-DND model establishes a new standard in echocardiogram segmentation, and its architecture holds promise for broader applicability in other medical imaging tasks, potentially improving diagnostic accuracy across various medical domains. Project page: this https URL</li>
</ul>

<h3>Title: Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors</h3>
<ul>
<li><strong>Authors: </strong>Jiyi Wang, Jingyang Ke, Bo Dai, Anqi Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15190">https://arxiv.org/abs/2506.15190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15190">https://arxiv.org/pdf/2506.15190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15190]] Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors(https://arxiv.org/abs/2506.15190)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Animals flexibly recombine a finite set of core motor primitives to meet diverse task demands, but existing behavior-segmentation methods oversimplify this process by imposing discrete syllables under restrictive generative assumptions. To reflect the animal behavior generation procedure, we introduce skill-based imitation learning (SKIL) for behavior understanding, a reinforcement learning-based imitation framework that (1) infers interpretable skill sets, i.e., latent basis functions of behavior, by leveraging representation learning on transition probabilities, and (2) parameterizes policies as dynamic mixtures of these skills. We validate our approach on a simple grid world, a discrete labyrinth, and unconstrained videos of freely moving animals. Across tasks, it identifies reusable skill components, learns continuously evolving compositional policies, and generates realistic trajectories beyond the capabilities of traditional discrete models. By exploiting generative behavior modeling with compositional representations, our method offers a concise, principled account of how complex animal behaviors emerge from dynamic combinations of fundamental motor primitives.</li>
</ul>

<h3>Title: Conquering the Retina: Bringing Visual in-Context Learning to OCT</h3>
<ul>
<li><strong>Authors: </strong>Alessio Negrini, Simon Reiß</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15200">https://arxiv.org/abs/2506.15200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15200">https://arxiv.org/pdf/2506.15200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15200]] Conquering the Retina: Bringing Visual in-Context Learning to OCT(https://arxiv.org/abs/2506.15200)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advancements in medical image analysis have led to the development of highly specialized models tailored to specific clinical tasks. These models have demonstrated exceptional performance and remain a crucial research direction. Yet, their applicability is limited to predefined tasks, requiring expertise and extensive resources for development and adaptation. In contrast, generalist models offer a different form of utility: allowing medical practitioners to define tasks on the fly without the need for task-specific model development. In this work, we explore how to train generalist models for the domain of retinal optical coherence tomography using visual in-context learning (VICL), i.e., training models to generalize across tasks based on a few examples provided at inference time. To facilitate rigorous assessment, we propose a broad evaluation protocol tailored to VICL in OCT. We extensively evaluate a state-of-the-art medical VICL approach on multiple retinal OCT datasets, establishing a first baseline to highlight the potential and current limitations of in-context learning for OCT. To foster further research and practical adoption, we openly release our code.</li>
</ul>

<h3>Title: A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals</h3>
<ul>
<li><strong>Authors: </strong>Andrea Cadeddu, Alessandro Chessa, Vincenzo De Leo, Gianni Fenu, Enrico Motta, Francesco Osborne, Diego Reforgiato Recupero, Angelo Salatino, Luca Secchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15208">https://arxiv.org/abs/2506.15208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15208">https://arxiv.org/pdf/2506.15208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15208]] A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals(https://arxiv.org/abs/2506.15208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>In 2012, the United Nations introduced 17 Sustainable Development Goals (SDGs) aimed at creating a more sustainable and improved future by 2030. However, tracking progress toward these goals is difficult because of the extensive scale and complexity of the data involved. Text classification models have become vital tools in this area, automating the analysis of vast amounts of text from a variety of sources. Additionally, large language models (LLMs) have recently proven indispensable for many natural language processing tasks, including text classification, thanks to their ability to recognize complex linguistic patterns and semantics. This study analyzes various proprietary and open-source LLMs for a single-label, multi-class text classification task focused on the SDGs. Then, it also evaluates the effectiveness of task adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot and Few-Shot Learning, as well as Fine-Tuning within this domain. The results reveal that smaller models, when optimized through prompt engineering, can perform on par with larger models like OpenAI's GPT (Generative Pre-trained Transformer).</li>
</ul>

<h3>Title: DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder</h3>
<ul>
<li><strong>Authors: </strong>Dan He, Weisheng Li, Guofen Wang, Yuping Huang, Shiqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15218">https://arxiv.org/abs/2506.15218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15218">https://arxiv.org/pdf/2506.15218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15218]] DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder(https://arxiv.org/abs/2506.15218)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multimodal medical image fusion (MMIF) extracts the most meaningful information from multiple source images, enabling a more comprehensive and accurate diagnosis. Achieving high-quality fusion results requires a careful balance of brightness, color, contrast, and detail; this ensures that the fused images effectively display relevant anatomical structures and reflect the functional status of the tissues. However, existing MMIF methods have limited capacity to capture detailed features during conventional training and suffer from insufficient cross-modal feature interaction, leading to suboptimal fused image quality. To address these issues, this study proposes a two-stage diffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In Stage I, a diffusion process trains UNet for image reconstruction. UNet captures detailed information through progressive denoising and represents multilevel data, providing a rich set of feature representations for the subsequent fusion network. In Stage II, noisy images at various steps are input into the fusion network to enhance the model's feature recognition capability. Three key fusion modules are also integrated to process medical images from different modalities adaptively. Ultimately, the robust network structure and a hybrid loss function are integrated to harmonize the fused image's brightness, color, contrast, and detail, enhancing its quality and information density. The experimental results across various medical image types demonstrate that the proposed method performs exceptionally well regarding objective evaluation metrics. The fused image preserves appropriate brightness, a comprehensive distribution of radioactive tracers, rich textures, and clear edges. The code is available at this https URL.</li>
</ul>

<h3>Title: Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants</h3>
<ul>
<li><strong>Authors: </strong>Jaione Bengoetxea, Itziar Gonzalez-Dios, Rodrigo Agerri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15239">https://arxiv.org/abs/2506.15239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15239">https://arxiv.org/pdf/2506.15239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15239]] Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants(https://arxiv.org/abs/2506.15239)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In this paper, we evaluate the capacity of current language technologies to understand Basque and Spanish language varieties. We use Natural Language Inference (NLI) as a pivot task and introduce a novel, manually-curated parallel dataset in Basque and Spanish, along with their respective variants. Our empirical analysis of crosslingual and in-context learning experiments using encoder-only and decoder-based Large Language Models (LLMs) shows a performance drop when handling linguistic variation, especially in Basque. Error analysis suggests that this decline is not due to lexical overlap, but rather to the linguistic variation itself. Further ablation experiments indicate that encoder-only models particularly struggle with Western Basque, which aligns with linguistic theory that identifies peripheral dialects (e.g., Western) as more distant from the standard. All data and code are publicly available.</li>
</ul>

<h3>Title: Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance</h3>
<ul>
<li><strong>Authors: </strong>Qingkai Zhang, L. Jeff Hong, Houmin Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.RM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15305">https://arxiv.org/abs/2506.15305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15305">https://arxiv.org/pdf/2506.15305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15305]] Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance(https://arxiv.org/abs/2506.15305)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid expansion of cross-border e-commerce (CBEC) has created significant opportunities for small and medium-sized enterprises (SMEs), yet financing remains a critical challenge due to SMEs' limited credit histories. Third-party logistics (3PL)-led supply chain finance (SCF) has emerged as a promising solution, leveraging in-transit inventory as collateral. We propose an advanced credit risk management framework tailored for 3PL-led SCF, addressing the dual challenges of credit risk assessment and loan size determination. Specifically, we leverage conditional generative modeling of sales distributions through Quantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for risk estimation. We propose a unified framework that enables flexible estimation of multiple risk measures while introducing a functional risk measure formulation that systematically captures the relationship between these risk measures and varying loan levels, supported by theoretical guarantees. To capture complex covariate interactions in e-commerce sales data, we integrate QRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on synthetic and real-world data validate the efficacy of our model for credit risk assessment and loan size determination. This study represents a pioneering application of generative AI in CBEC SCF risk management, offering a solid foundation for enhanced credit practices and improved SME access to capital.</li>
</ul>

<h3>Title: MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning</h3>
<ul>
<li><strong>Authors: </strong>Leonid Ivanov, Vasily Yuryev, Dmitry Yudin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15313">https://arxiv.org/abs/2506.15313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15313">https://arxiv.org/pdf/2506.15313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15313]] MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning(https://arxiv.org/abs/2506.15313)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In autonomous driving, high-definition (HD) maps and semantic maps in bird's-eye view (BEV) are essential for accurate localization, planning, and decision-making. This paper introduces an enhanced End-to-End model named MapFM for online vectorized HD map generation. We show significantly boost feature representation quality by incorporating powerful foundation model for encoding camera images. To further enrich the model's understanding of the environment and improve prediction quality, we integrate auxiliary prediction heads for semantic segmentation in the BEV representation. This multi-task learning approach provides richer contextual supervision, leading to a more comprehensive scene representation and ultimately resulting in higher accuracy and improved quality of the predicted vectorized HD maps. The source code is available at this https URL.</li>
</ul>

<h3>Title: When and How Unlabeled Data Provably Improve In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Yingcong Li, Xiangyu Chang, Muti Kara, Xiaofeng Liu, Amit Roy-Chowdhury, Samet Oymak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15329">https://arxiv.org/abs/2506.15329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15329">https://arxiv.org/pdf/2506.15329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15329]] When and How Unlabeled Data Provably Improve In-Context Learning(https://arxiv.org/abs/2506.15329)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Recent research shows that in-context learning (ICL) can be effective even when demonstrations have missing or incorrect labels. To shed light on this capability, we examine a canonical setting where the demonstrations are drawn according to a binary Gaussian mixture model (GMM) and a certain fraction of the demonstrations have missing labels. We provide a comprehensive theoretical study to show that: (1) The loss landscape of one-layer linear attention models recover the optimal fully-supervised estimator but completely fail to exploit unlabeled data; (2) In contrast, multilayer or looped transformers can effectively leverage unlabeled data by implicitly constructing estimators of the form $\sum_{i\ge 0} a_i (X^\top X)^iX^\top y$ with $X$ and $y$ denoting features and partially-observed labels (with missing entries set to zero). We characterize the class of polynomials that can be expressed as a function of depth and draw connections to Expectation Maximization, an iterative pseudo-labeling algorithm commonly used in semi-supervised learning. Importantly, the leading polynomial power is exponential in depth, so mild amount of depth/looping suffices. As an application of theory, we propose looping off-the-shelf tabular foundation models to enhance their semi-supervision capabilities. Extensive evaluations on real-world datasets show that our method significantly improves the semisupervised tabular learning performance over the standard single pass inference.</li>
</ul>

<h3>Title: Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges</h3>
<ul>
<li><strong>Authors: </strong>A.S. Stankevich, I.B. Petrov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15346">https://arxiv.org/abs/2506.15346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15346">https://arxiv.org/pdf/2506.15346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15346]] Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges(https://arxiv.org/abs/2506.15346)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent developments in application of deep learning models to acoustic Full Waveform Inversion (FWI) are marked by the use of diffusion models as prior distributions for Bayesian-like inference procedures. The advantage of these methods is the ability to generate high-resolution samples, which are otherwise unattainable with classical inversion methods or other deep learning-based solutions. However, the iterative and stochastic nature of sampling from diffusion models along with heuristic nature of output control remain limiting factors for their applicability. For instance, an optimal way to include the approximate velocity model into diffusion-based inversion scheme remains unclear, even though it is considered an essential part of FWI pipeline. We address the issue by employing a Schrödinger Bridge that interpolates between the distributions of ground truth and smoothed velocity models. To facilitate the learning of nonlinear drifts that transfer samples between distributions we extend the concept of Image-to-Image Schrödinger Bridge ($\text{I}^2\text{SB}$) to conditional sampling, resulting in a conditional Image-to-Image Schrödinger Bridge (c$\text{I}^2\text{SB}$) framework. To validate our method, we assess its effectiveness in reconstructing the reference velocity model from its smoothed approximation, coupled with the observed seismic signal of fixed shape. Our experiments demonstrate that the proposed solution outperforms our reimplementation of conditional diffusion model suggested in earlier works, while requiring only a few neural function evaluations (NFEs) to achieve sample fidelity superior to that attained with supervised learning-based approach. The supplementary code implementing the algorithms described in this paper can be found in the repository this https URL.</li>
</ul>

<h3>Title: Unsupervised Pelage Pattern Unwrapping for Animal Re-identification</h3>
<ul>
<li><strong>Authors: </strong>Aleksandr Algasov, Ekaterina Nepovinnykh, Fedor Zolotarev, Tuomas Eerola, Heikki Kälviäinen, Pavel Zemčík, Charles V. Stewart</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15369">https://arxiv.org/abs/2506.15369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15369">https://arxiv.org/pdf/2506.15369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15369]] Unsupervised Pelage Pattern Unwrapping for Animal Re-identification(https://arxiv.org/abs/2506.15369)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Existing individual re-identification methods often struggle with the deformable nature of animal fur or skin patterns which undergo geometric distortions due to body movement and posture changes. In this paper, we propose a geometry-aware texture mapping approach that unwarps pelage patterns, the unique markings found on an animal's skin or fur, into a canonical UV space, enabling more robust feature matching. Our method uses surface normal estimation to guide the unwrapping process while preserving the geometric consistency between the 3D surface and the 2D texture space. We focus on two challenging species: Saimaa ringed seals (Pusa hispida saimensis) and leopards (Panthera pardus). Both species have distinctive yet highly deformable fur patterns. By integrating our pattern-preserving UV mapping with existing re-identification techniques, we demonstrate improved accuracy across diverse poses and viewing angles. Our framework does not require ground truth UV annotations and can be trained in a self-supervised manner. Experiments on seal and leopard datasets show up to a 5.4% improvement in re-identification accuracy.</li>
</ul>

<h3>Title: Sampling 3D Molecular Conformers with Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>J. Thorben Frank, Winfried Ripken, Gregor Lied, Klaus-Robert Müller, Oliver T. Unke, Stefan Chmiela</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15378">https://arxiv.org/abs/2506.15378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15378">https://arxiv.org/pdf/2506.15378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15378]] Sampling 3D Molecular Conformers with Diffusion Transformers(https://arxiv.org/abs/2506.15378)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have demonstrated strong performance in generative modeling, particularly in image synthesis, making them a compelling choice for molecular conformer generation. However, applying DiTs to molecules introduces novel challenges, such as integrating discrete molecular graph information with continuous 3D geometry, handling Euclidean symmetries, and designing conditioning mechanisms that generalize across molecules of varying sizes and structures. We propose DiTMC, a framework that adapts DiTs to address these challenges through a modular architecture that separates the processing of 3D coordinates from conditioning on atomic connectivity. To this end, we introduce two complementary graph-based conditioning strategies that integrate seamlessly with the DiT architecture. These are combined with different attention mechanisms, including both standard non-equivariant and SO(3)-equivariant formulations, enabling flexible control over the trade-off between between accuracy and computational efficiency. Experiments on standard conformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC achieves state-of-the-art precision and physical validity. Our results highlight how architectural choices and symmetry priors affect sample quality and efficiency, suggesting promising directions for large-scale generative modeling of molecular structures. Code available at this https URL.</li>
</ul>

<h3>Title: When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class</h3>
<ul>
<li><strong>Authors: </strong>Yujin Kim, Hyunsoo Kim, Hyunwoo J.Kim, Suhyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15381">https://arxiv.org/abs/2506.15381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15381">https://arxiv.org/pdf/2506.15381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15381]] When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class(https://arxiv.org/abs/2506.15381)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Open-source pre-trained models hold great potential for diverse applications, but their utility declines when their training data is unavailable. Data-Free Image Synthesis (DFIS) aims to generate images that approximate the learned data distribution of a pre-trained model without accessing the original data. However, existing DFIS meth ods produce samples that deviate from the training data distribution due to the lack of prior knowl edge about natural images. To overcome this limitation, we propose DDIS, the first Diffusion-assisted Data-free Image Synthesis method that leverages a text-to-image diffusion model as a powerful image prior, improving synthetic image quality. DDIS extracts knowledge about the learned distribution from the given model and uses it to guide the diffusion model, enabling the generation of images that accurately align with the training data distribution. To achieve this, we introduce Domain Alignment Guidance (DAG) that aligns the synthetic data domain with the training data domain during the diffusion sampling process. Furthermore, we optimize a single Class Alignment Token (CAT) embedding to effectively capture class-specific attributes in the training dataset. Experiments on PACS and Ima geNet demonstrate that DDIS outperforms prior DFIS methods by generating samples that better reflect the training data distribution, achieving SOTA performance in data-free applications.</li>
</ul>

<h3>Title: Provable Maximum Entropy Manifold Exploration via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Riccardo De Santi, Marin Vlastelica, Ya-Ping Hsieh, Zebang Shen, Niao He, Andreas Krause</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15385">https://arxiv.org/abs/2506.15385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15385">https://arxiv.org/pdf/2506.15385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15385]] Provable Maximum Entropy Manifold Exploration via Diffusion Models(https://arxiv.org/abs/2506.15385)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Exploration is critical for solving real-world decision-making problems such as scientific discovery, where the objective is to generate truly novel designs rather than mimic existing data distributions. In this work, we address the challenge of leveraging the representational power of generative models for exploration without relying on explicit uncertainty quantification. We introduce a novel framework that casts exploration as entropy maximization over the approximate data manifold implicitly defined by a pre-trained diffusion model. Then, we present a novel principle for exploration based on density estimation, a problem well-known to be challenging in practice. To overcome this issue and render this method truly scalable, we leverage a fundamental connection between the entropy of the density induced by a diffusion model and its score function. Building on this, we develop an algorithm based on mirror descent that solves the exploration problem as sequential fine-tuning of a pre-trained diffusion model. We prove its convergence to the optimal exploratory diffusion model under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we empirically evaluate our approach on both synthetic and high-dimensional text-to-image diffusion, demonstrating promising results.</li>
</ul>

<h3>Title: Evaluation Pipeline for systematically searching for Anomaly Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Florian Rokohl, Alexander Lehnert, Marc Reichenbach</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15388">https://arxiv.org/abs/2506.15388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15388">https://arxiv.org/pdf/2506.15388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15388]] Evaluation Pipeline for systematically searching for Anomaly Detection Systems(https://arxiv.org/abs/2506.15388)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Digitalization in the medical world provides major benefits while making it a target for attackers and thus hard to secure. To deal with network intruders we propose an anomaly detection system on hardware to detect malicious clients in real-time. We meet real-time and power restrictions using FPGAs. Overall system performance is achieved via the presented holistic system evaluation.</li>
</ul>

<h3>Title: Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material</h3>
<ul>
<li><strong>Authors: </strong>Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Xianghui Yang, Huiwen Shi, Zibo Zhao, Bowen Zhang, Hongyu Yan, Lifu Wang, Sicong Liu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Dongyuan Guo, Junlin Yu, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Shida Wei, Chao Zhang, Yonghao Tan, Yifu Sun, Lin Niu, Shirui Huang, Bojian Zheng, Shu Liu, Shilin Chen, Xiang Yuan, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Jingwei Huang, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15442">https://arxiv.org/abs/2506.15442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15442">https://arxiv.org/pdf/2506.15442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15442]] Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material(https://arxiv.org/abs/2506.15442)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.</li>
</ul>

<h3>Title: Semi-supervised Graph Anomaly Detection via Robust Homophily Learning</h3>
<ul>
<li><strong>Authors: </strong>Guoguo Ai, Hezhe Qiao, Hui Yan, Guansong Pang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15448">https://arxiv.org/abs/2506.15448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15448">https://arxiv.org/pdf/2506.15448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15448]] Semi-supervised Graph Anomaly Detection via Robust Homophily Learning(https://arxiv.org/abs/2506.15448)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled normal nodes to identify abnormal nodes from a large set of unlabeled nodes in a graph. Current methods in this line posit that 1) normal nodes share a similar level of homophily and 2) the labeled normal nodes can well represent the homophily patterns in the normal class. However, this assumption often does not hold well since normal nodes in a graph can exhibit diverse homophily in real-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily Learning, to adaptively learn such homophily patterns. RHO consists of two novel modules, adaptive frequency response filters (AdaFreq) and graph normality alignment (GNA). AdaFreq learns a set of adaptive spectral filters that capture different frequency components of the labeled normal nodes with varying homophily in the channel-wise and cross-channel views of node attributes. GNA is introduced to enforce consistency between the channel-wise and cross-channel homophily representations to robustify the normality learned by the filters in the two views. Experiments on eight real-world GAD datasets show that RHO can effectively learn varying, often under-represented, homophily in the small normal node set and substantially outperforms state-of-the-art competing methods. Code is available at this https URL.</li>
</ul>

<h3>Title: GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects</h3>
<ul>
<li><strong>Authors: </strong>Shujia Li, Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Yutong Ban</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15483">https://arxiv.org/abs/2506.15483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15483">https://arxiv.org/pdf/2506.15483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15483]] GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects(https://arxiv.org/abs/2506.15483)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While diffusion models and large-scale motion datasets have advanced text-driven human motion synthesis, extending these advances to 4D human-object interaction (HOI) remains challenging, mainly due to the limited availability of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel two-stage framework aimed at achieving two key objectives: 1) generalization to unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the initial stage of our framework, we employ an Object-AnchorNet to reconstruct sparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI datasets, thereby mitigating the dependence on large-scale 4D HOI datasets. Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the second stage to seamlessly interpolate sparse 3D HOI keyframes into densely temporally coherent 4D HOI sequences. To enhance the quality of generated 4D HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to extract human-object contact patterns and a novel Contact-Aware HOI Attention to effectively integrate the contact signals into diffusion models. Experimental results show that we achieve state-of-the-art results on the publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong generalization abilities to unseen objects, while enabling high-fidelity 4D HOI generation.</li>
</ul>

<h3>Title: Lessons from Training Grounded LLMs with Verifiable Rewards</h3>
<ul>
<li><strong>Authors: </strong>Shang Hong Sim, Tej Deep Pala, Vernon Toh, Hai Leong Chieu, Amir Zadeh, Chuan Li, Navonil Majumder, Soujanya Poria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15522">https://arxiv.org/abs/2506.15522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15522">https://arxiv.org/pdf/2506.15522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15522]] Lessons from Training Grounded LLMs with Verifiable Rewards(https://arxiv.org/abs/2506.15522)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating grounded and trustworthy responses remains a key challenge for large language models (LLMs). While retrieval-augmented generation (RAG) with citation-based grounding holds promise, instruction-tuned models frequently fail even in straightforward scenarios: missing explicitly stated answers, citing incorrectly, or refusing when evidence is available. In this work, we explore how reinforcement learning (RL) and internal reasoning can enhance grounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method to train models using verifiable outcome-based rewards targeting answer correctness, citation sufficiency, and refusal quality, without requiring gold reasoning traces or expensive annotations. Through comprehensive experiments across ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented models significantly outperform instruction-only variants, especially in handling unanswerable queries and generating well-cited responses. A two-stage training setup, first optimizing answer and citation behavior and then refusal, further improves grounding by stabilizing the learning signal. Additionally, we revisit instruction tuning via GPT-4 distillation and find that combining it with GRPO enhances performance on long-form, generative QA tasks. Overall, our findings highlight the value of reasoning, stage-wise optimization, and outcome-driven RL for building more verifiable and reliable LLMs.</li>
</ul>

<h3>Title: CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Shahnaz Jamil-Copley, Richard H. Clayton, Chen (Cherise)Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15549">https://arxiv.org/abs/2506.15549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15549">https://arxiv.org/pdf/2506.15549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15549]] CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation(https://arxiv.org/abs/2506.15549)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep learning-based myocardial scar segmentation from late gadolinium enhancement (LGE) cardiac MRI has shown great potential for accurate and timely diagnosis and treatment planning for structural cardiac diseases. However, the limited availability and variability of LGE images with high-quality scar labels restrict the development of robust segmentation models. To address this, we introduce CLAIM: \textbf{C}linically-Guided \textbf{L}GE \textbf{A}ugmentation for Real\textbf{i}stic and Diverse \textbf{M}yocardial Scar Synthesis and Segmentation framework, a framework for anatomically grounded scar generation and segmentation. At its core is the SMILE module (Scar Mask generation guided by cLinical knowledgE), which conditions a diffusion-based generator on the clinically adopted AHA 17-segment model to synthesize images with anatomically consistent and spatially diverse scar patterns. In addition, CLAIM employs a joint training strategy in which the scar segmentation network is optimized alongside the generator, aiming to enhance both the realism of synthesized scars and the accuracy of the scar segmentation performance. Experimental results show that CLAIM produces anatomically coherent scar patterns and achieves higher Dice similarity with real scar distributions compared to baseline models. Our approach enables controllable and realistic myocardial scar synthesis and has demonstrated utility for downstream medical imaging task.</li>
</ul>

<h3>Title: Control and Realism: Best of Both Worlds in Layout-to-Image without Training</h3>
<ul>
<li><strong>Authors: </strong>Bonan Li, Yinhan Hu, Songhua Liu, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15563">https://arxiv.org/abs/2506.15563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15563">https://arxiv.org/pdf/2506.15563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15563]] Control and Realism: Best of Both Worlds in Layout-to-Image without Training(https://arxiv.org/abs/2506.15563)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Layout-to-Image generation aims to create complex scenes with precise control over the placement and arrangement of subjects. Existing works have demonstrated that pre-trained Text-to-Image diffusion models can achieve this goal without training on any specific data; however, they often face challenges with imprecise localization and unrealistic artifacts. Focusing on these drawbacks, we propose a novel training-free method, WinWinLay. At its core, WinWinLay presents two key strategies, Non-local Attention Energy Function and Adaptive Update, that collaboratively enhance control precision and realism. On one hand, we theoretically demonstrate that the commonly used attention energy function introduces inherent spatial distribution biases, hindering objects from being uniformly aligned with layout instructions. To overcome this issue, non-local attention prior is explored to redistribute attention scores, facilitating objects to better conform to the specified spatial conditions. On the other hand, we identify that the vanilla backpropagation update rule can cause deviations from the pre-trained domain, leading to out-of-distribution artifacts. We accordingly introduce a Langevin dynamics-based adaptive update scheme as a remedy that promotes in-domain updating while respecting layout constraints. Extensive experiments demonstrate that WinWinLay excels in controlling element placement and achieving photorealistic visual fidelity, outperforming the current state-of-the-art methods.</li>
</ul>

<h3>Title: Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengyang Shan, Emily Ruth Diana, Jiawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15568">https://arxiv.org/abs/2506.15568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15568">https://arxiv.org/pdf/2506.15568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15568]] Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models(https://arxiv.org/abs/2506.15568)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a comprehensive evaluation of gender fairness in large language models (LLMs), focusing on their ability to handle both binary and non-binary genders. While previous studies primarily focus on binary gender distinctions, we introduce the Gender Inclusivity Fairness Index (GIFI), a novel and comprehensive metric that quantifies the diverse gender inclusivity of LLMs. GIFI consists of a wide range of evaluations at different levels, from simply probing the model with respect to provided gender pronouns to testing various aspects of model generation and cognitive behaviors under different gender assumptions, revealing biases associated with varying gender identifiers. We conduct extensive evaluations with GIFI on 22 prominent open-source and proprietary LLMs of varying sizes and capabilities, discovering significant variations in LLMs' gender inclusivity. Our study highlights the importance of improving LLMs' inclusivity, providing a critical benchmark for future advancements in gender fairness in generative models.</li>
</ul>

<h3>Title: SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification</h3>
<ul>
<li><strong>Authors: </strong>Chengye Wang, Yifei Shen, Zexi Kuang, Arman Cohan, Yilun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15569">https://arxiv.org/abs/2506.15569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15569">https://arxiv.org/pdf/2506.15569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15569]] SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification(https://arxiv.org/abs/2506.15569)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.</li>
</ul>

<h3>Title: One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yujing Sun, Lingchen Sun, Shuaizheng Liu, Rongyuan Wu, Zhengqiang Zhang, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15591">https://arxiv.org/abs/2506.15591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15591">https://arxiv.org/pdf/2506.15591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15591]] One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution(https://arxiv.org/abs/2506.15591)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>It is a challenging problem to reproduce rich spatial details while maintaining temporal consistency in real-world video super-resolution (Real-VSR), especially when we leverage pre-trained generative models such as stable diffusion (SD) for realistic details synthesis. Existing SD-based Real-VSR methods often compromise spatial details for temporal coherence, resulting in suboptimal visual quality. We argue that the key lies in how to effectively extract the degradation-robust temporal consistency priors from the low-quality (LQ) input video and enhance the video details while maintaining the extracted consistency priors. To achieve this, we propose a Dual LoRA Learning (DLoRAL) paradigm to train an effective SD-based one-step diffusion model, achieving realistic frame details and temporal consistency simultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module to aggregate complementary information across frames, and train a Consistency-LoRA (C-LoRA) to learn robust temporal representations from degraded inputs. After consistency learning, we fix the CFR and C-LoRA modules and train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with the temporal space defined by C-LoRA to keep temporal coherence. The two phases alternate iteratively for optimization, collaboratively delivering consistent and detail-rich outputs. During inference, the two LoRA branches are merged into the SD model, allowing efficient and high-quality video restoration in a single diffusion step. Experiments show that DLoRAL achieves strong performance in both accuracy and speed. Code and models are available at this https URL.</li>
</ul>

<h3>Title: From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns</h3>
<ul>
<li><strong>Authors: </strong>Bernardo Leite, Henrique Lopes Cardoso, Pedro Pinto, Abel Ferreira, Luís Abreu, Isabel Rangel, Sandra Monteiro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15598">https://arxiv.org/abs/2506.15598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15598">https://arxiv.org/pdf/2506.15598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15598]] From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns(https://arxiv.org/abs/2506.15598)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While MCQs are valuable for learning and evaluation, manually creating them with varying difficulty levels and targeted reading skills remains a time-consuming and costly task. Recent advances in generative AI provide an opportunity to automate MCQ generation efficiently. However, assessing the actual quality and reliability of generated MCQs has received limited attention -- particularly regarding cases where generation fails. This aspect becomes particularly important when the generated MCQs are meant to be applied in real-world settings. Additionally, most MCQ generation studies focus on English, leaving other languages underexplored. This paper investigates the capabilities of current generative models in producing MCQs for reading comprehension in Portuguese, a morphologically rich language. Our study focuses on generating MCQs that align with curriculum-relevant narrative elements and span different difficulty levels. We evaluate these MCQs through expert review and by analyzing the psychometric properties extracted from student responses to assess their suitability for elementary school students. Our results show that current models can generate MCQs of comparable quality to human-authored ones. However, we identify issues related to semantic clarity and answerability. Also, challenges remain in generating distractors that engage students and meet established criteria for high-quality MCQ option design.</li>
</ul>

<h3>Title: BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yuqing Lan, Chenyang Zhu, Zhirui Gao, Jiazhao Zhang, Yihan Cao, Renjiao Yi, Yijie Wang, Kai Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15610">https://arxiv.org/abs/2506.15610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15610">https://arxiv.org/pdf/2506.15610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15610]] BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion(https://arxiv.org/abs/2506.15610)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Open-vocabulary 3D object detection has gained significant interest due to its critical applications in autonomous driving and embodied AI. Existing detection methods, whether offline or online, typically rely on dense point cloud reconstruction, which imposes substantial computational overhead and memory constraints, hindering real-time deployment in downstream tasks. To address this, we propose a novel reconstruction-free online framework tailored for memory-efficient and real-time 3D detection. Specifically, given streaming posed RGB-D video input, we leverage Cubify Anything as a pre-trained visual foundation model (VFM) for single-view 3D object detection by bounding boxes, coupled with CLIP to capture open-vocabulary semantics of detected objects. To fuse all detected bounding boxes across different views into a unified one, we employ an association module for correspondences of multi-views and an optimization module to fuse the 3D bounding boxes of the same instance predicted in multi-views. The association module utilizes 3D Non-Maximum Suppression (NMS) and a box correspondence matching module, while the optimization module uses an IoU-guided efficient random optimization technique based on particle filtering to enforce multi-view consistency of the 3D bounding boxes while minimizing computational complexity. Extensive experiments on ScanNetV2 and CA-1M datasets demonstrate that our method achieves state-of-the-art performance among online methods. Benefiting from this novel reconstruction-free paradigm for 3D object detection, our method exhibits great generalization abilities in various scenarios, enabling real-time perception even in environments exceeding 1000 square meters.</li>
</ul>

<h3>Title: HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization</h3>
<ul>
<li><strong>Authors: </strong>Roey Ron, Guy Tevet, Haim Sawdayee, Amit H. Bermano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15625">https://arxiv.org/abs/2506.15625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15625">https://arxiv.org/pdf/2506.15625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15625]] HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization(https://arxiv.org/abs/2506.15625)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present HOIDiNi, a text-driven diffusion framework for synthesizing realistic and plausible human-object interaction (HOI). HOI generation is extremely challenging since it induces strict contact accuracies alongside a diverse motion manifold. While current literature trades off between realism and physical correctness, HOIDiNi optimizes directly in the noise space of a pretrained diffusion model using Diffusion Noise Optimization (DNO), achieving both. This is made feasible thanks to our observation that the problem can be separated into two phases: an object-centric phase, primarily making discrete choices of hand-object contact locations, and a human-centric phase that refines the full-body motion to realize this blueprint. This structured approach allows for precise hand-object contact without compromising motion naturalness. Quantitative, qualitative, and subjective evaluations on the GRAB dataset alone clearly indicate HOIDiNi outperforms prior works and baselines in contact accuracy, physical validity, and overall quality. Our results demonstrate the ability to generate complex, controllable interactions, including grasping, placing, and full-body coordination, driven solely by textual prompts. this https URL.</li>
</ul>

<h3>Title: Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability</h3>
<ul>
<li><strong>Authors: </strong>Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15629">https://arxiv.org/abs/2506.15629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15629">https://arxiv.org/pdf/2506.15629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15629]] Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability(https://arxiv.org/abs/2506.15629)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In generative commonsense reasoning tasks such as CommonGen, generative large language models (LLMs) compose sentences that include all given concepts. However, when focusing on instruction-following capabilities, if a prompt specifies a concept order, LLMs must generate sentences that adhere to the specified order. To address this, we propose Ordered CommonGen, a benchmark designed to evaluate the compositional generalization and instruction-following abilities of LLMs. This benchmark measures ordered coverage to assess whether concepts are generated in the specified order, enabling a simultaneous evaluation of both abilities. We conducted a comprehensive analysis using 36 LLMs and found that, while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs or identical results even when the concept order is altered. Moreover, even the most instruction-compliant LLM achieved only about 75% ordered coverage, highlighting the need for improvements in both instruction-following and compositional generalization capabilities.</li>
</ul>

<h3>Title: UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting</h3>
<ul>
<li><strong>Authors: </strong>Kai He, Ruofan Liang, Jacob Munkberg, Jon Hasselgren, Nandita Vijaykumar, Alexander Keller, Sanja Fidler, Igor Gilitschenski, Zan Gojcic, Zian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15673">https://arxiv.org/abs/2506.15673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15673">https://arxiv.org/pdf/2506.15673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15673]] UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting(https://arxiv.org/abs/2506.15673)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We address the challenge of relighting a single image or video, a task that demands precise scene intrinsic understanding and high-quality light transport synthesis. Existing end-to-end relighting models are often limited by the scarcity of paired multi-illumination data, restricting their ability to generalize across diverse scenes. Conversely, two-stage pipelines that combine inverse and forward rendering can mitigate data requirements but are susceptible to error accumulation and often fail to produce realistic outputs under complex lighting conditions or with sophisticated materials. In this work, we introduce a general-purpose approach that jointly estimates albedo and synthesizes relit outputs in a single pass, harnessing the generative capabilities of video diffusion models. This joint formulation enhances implicit scene comprehension and facilitates the creation of realistic lighting effects and intricate material interactions, such as shadows, reflections, and transparency. Trained on synthetic multi-illumination data and extensive automatically labeled real-world videos, our model demonstrates strong generalization across diverse domains and surpasses previous methods in both visual fidelity and temporal consistency.</li>
</ul>

<h3>Title: Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15682">https://arxiv.org/abs/2506.15682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15682">https://arxiv.org/pdf/2506.15682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15682]] Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model(https://arxiv.org/abs/2506.15682)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and this http URL using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at this https URL and our code is available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
