<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-03</h1>
<h3>Title: Science Written by Generative AI is Perceived as Less Intelligent, but  More Credible and Trustworthy than Science Written by Humans</h3>
<ul>
<li><strong>Authors: </strong>David M. Markowitz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00706">https://arxiv.org/abs/2405.00706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00706">https://arxiv.org/pdf/2405.00706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00706]] Science Written by Generative AI is Perceived as Less Intelligent, but  More Credible and Trustworthy than Science Written by Humans(https://arxiv.org/abs/2405.00706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper evaluated the effectiveness of using generative AI to simplify science communication and enhance public trust in science. By comparing lay summaries of journal articles from PNAS, yoked to those generated by AI, this work assessed linguistic simplicity across such summaries and public perceptions. Study 1a analyzed simplicity features of PNAS abstracts (scientific summaries) and significance statements (lay summaries), observing that lay summaries were indeed linguistically simpler, but effect size differences were small. Study 1b used GPT-4 to create significance statements based on paper abstracts and this more than doubled the average effect size without fine-tuning. Finally, Study 2 experimentally demonstrated that simply-written GPT summaries facilitated more favorable public perceptions of scientists (their credibility, trustworthiness) than more complexly-written human PNAS summaries. AI has the potential to engage scientific communities and the public via a simple language heuristic, advocating for its integration into scientific dissemination for a more informed society.</li>
</ul>

<h3>Title: Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of  Theories, Detection Methods, and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Xiaomin Yu, Yezhaohui Wang, Yanfang Chen, Zhen Tao, Dinghao Xi, Shichao Song, Simin Niu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00711">https://arxiv.org/abs/2405.00711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00711">https://arxiv.org/pdf/2405.00711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00711]] Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of  Theories, Detection Methods, and Opportunities(https://arxiv.org/abs/2405.00711)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, generative artificial intelligence models, represented by Large Language Models (LLMs) and Diffusion Models (DMs), have revolutionized content production methods. These artificial intelligence-generated content (AIGC) have become deeply embedded in various aspects of daily life and work, spanning texts, images, videos, and audio. The authenticity of AI-generated content is progressively enhancing, approaching human-level creative standards. However, these technologies have also led to the emergence of Fake Artificial Intelligence Generated Content (FAIGC), posing new challenges in distinguishing genuine information. It is crucial to recognize that AIGC technology is akin to a double-edged sword; its potent generative capabilities, while beneficial, also pose risks for the creation and dissemination of FAIGC. In this survey, We propose a new taxonomy that provides a more comprehensive breakdown of the space of FAIGC methods today. Next, we explore the modalities and generative technologies of FAIGC, categorized under AI-generated disinformation and AI-generated misinformation. From various perspectives, we then introduce FAIGC detection methods, including Deceptive FAIGC Detection, Deepfake Detection, and Hallucination-based FAIGC Detection. Finally, we discuss outstanding challenges and promising areas for future research.</li>
</ul>

<h3>Title: Can't say cant? Measuring and Reasoning of Dark Jargons in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xu Ji, Jianyi Zhang, Ziyin Zhou, Zhangchi Zhao, Qianqian Qiao, Kaiying Han, Md Imran Hossen, Xiali Hei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00718">https://arxiv.org/abs/2405.00718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00718">https://arxiv.org/pdf/2405.00718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00718]] Can't say cant? Measuring and Reasoning of Dark Jargons in Large  Language Models(https://arxiv.org/abs/2405.00718)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Ensuring the resilience of Large Language Models (LLMs) against malicious exploitation is paramount, with recent focus on mitigating offensive responses. Yet, the understanding of cant or dark jargon remains unexplored. This paper introduces a domain-specific Cant dataset and CantCounter evaluation framework, employing Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis stages. Experiments reveal LLMs, including ChatGPT, are susceptible to cant bypassing filters, with varying recognition accuracy influenced by question types, setups, and prompt clues. Updated models exhibit higher acceptance rates for cant queries. Moreover, LLM reactions differ across domains, e.g., reluctance to engage in racism versus LGBT topics. These findings underscore LLMs' understanding of cant and reflect training data characteristics and vendor approaches to sensitive topics. Additionally, we assess LLMs' ability to demonstrate reasoning capabilities. Access to our datasets and code is available at https://github.com/cistineup/CantCounter.</li>
</ul>

<h3>Title: Soft Preference Optimization: Aligning Language Models to Expert  Distributions</h3>
<ul>
<li><strong>Authors: </strong>Arsalan Sharifnassab, Sina Ghiassian, Saber Salehkaleybar, Surya Kanoria, Dale Schuurmans</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00747">https://arxiv.org/abs/2405.00747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00747">https://arxiv.org/pdf/2405.00747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00747]] Soft Preference Optimization: Aligning Language Models to Expert  Distributions(https://arxiv.org/abs/2405.00747)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose Soft Preference Optimization (SPO), a method for aligning generative models, such as Large Language Models (LLMs), with human preferences, without the need for a reward model. SPO optimizes model outputs directly over a preference dataset through a natural loss function that integrates preference loss with a regularization term across the model's entire output distribution rather than limiting it to the preference dataset. Although SPO does not require the assumption of an existing underlying reward model, we demonstrate that, under the Bradley-Terry (BT) model assumption, it converges to a softmax of scaled rewards, with the distribution's "softness" adjustable via the softmax exponent, an algorithm parameter. We showcase SPO's methodology, its theoretical foundation, and its comparative advantages in simplicity, computational efficiency, and alignment precision.</li>
</ul>

<h3>Title: Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00760">https://arxiv.org/abs/2405.00760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00760">https://arxiv.org/pdf/2405.00760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00760]] Deep Reward Supervisions for Tuning Text-to-Image Diffusion Models(https://arxiv.org/abs/2405.00760)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Optimizing a text-to-image diffusion model with a given reward function is an important but underexplored research area. In this study, we propose Deep Reward Tuning (DRTune), an algorithm that directly supervises the final output image of a text-to-image diffusion model and back-propagates through the iterative sampling process to the input noise. We find that training earlier steps in the sampling process is crucial for low-level rewards, and deep supervision can be achieved efficiently and effectively by stopping the gradient of the denoising network input. DRTune is extensively evaluated on various reward models. It consistently outperforms other algorithms, particularly for low-level control signals, where all shallow supervision methods fail. Additionally, we fine-tune Stable Diffusion XL 1.0 (SDXL 1.0) model via DRTune to optimize Human Preference Score v2.1, resulting in the Favorable Diffusion XL 1.0 (FDXL 1.0) model. FDXL 1.0 significantly enhances image quality compared to SDXL 1.0 and reaches comparable quality compared with Midjourney v5.2.</li>
</ul>

<h3>Title: Obtaining Favorable Layouts for Multiple Object Generation</h3>
<ul>
<li><strong>Authors: </strong>Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00791">https://arxiv.org/abs/2405.00791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00791">https://arxiv.org/pdf/2405.00791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00791]] Obtaining Favorable Layouts for Multiple Object Generation(https://arxiv.org/abs/2405.00791)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts.</li>
</ul>

<h3>Title: Guided Conditional Diffusion Classifier (ConDiff) for Enhanced  Prediction of Infection in Diabetic Foot Ulcers</h3>
<ul>
<li><strong>Authors: </strong>Palawat Busaranuvong, Emmanuel Agu, Deepak Kumar, Shefalika Gautam, Reza Saadati Fard, Bengisu Tulu, Diane Strong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00858">https://arxiv.org/abs/2405.00858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00858">https://arxiv.org/pdf/2405.00858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00858]] Guided Conditional Diffusion Classifier (ConDiff) for Enhanced  Prediction of Infection in Diabetic Foot Ulcers(https://arxiv.org/abs/2405.00858)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>To detect infected wounds in Diabetic Foot Ulcers (DFUs) from photographs, preventing severe complications and amputations. Methods: This paper proposes the Guided Conditional Diffusion Classifier (ConDiff), a novel deep-learning infection detection model that combines guided image synthesis with a denoising diffusion model and distance-based classification. The process involves (1) generating guided conditional synthetic images by injecting Gaussian noise to a guide image, followed by denoising the noise-perturbed image through a reverse diffusion process, conditioned on infection status and (2) classifying infections based on the minimum Euclidean distance between synthesized images and the original guide image in embedding space. Results: ConDiff demonstrated superior performance with an accuracy of 83% and an F1-score of 0.858, outperforming state-of-the-art models by at least 3%. The use of a triplet loss function reduces overfitting in the distance-based classifier. Conclusions: ConDiff not only enhances diagnostic accuracy for DFU infections but also pioneers the use of generative discriminative models for detailed medical image analysis, offering a promising approach for improving patient outcomes.</li>
</ul>

<h3>Title: Beyond Human Vision: The Role of Large Vision Language Models in  Microscope Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Prateek Verma, Minh-Hao Van, Xintao Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00876">https://arxiv.org/abs/2405.00876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00876">https://arxiv.org/pdf/2405.00876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00876]] Beyond Human Vision: The Role of Large Vision Language Models in  Microscope Image Analysis(https://arxiv.org/abs/2405.00876)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision language models (VLMs) have recently emerged and gained the spotlight for their ability to comprehend the dual modality of image and textual data. VLMs such as LLaVA, ChatGPT-4, and Gemini have recently shown impressive performance on tasks such as natural image captioning, visual question answering (VQA), and spatial reasoning. Additionally, a universal segmentation model by Meta AI, Segment Anything Model (SAM) shows unprecedented performance at isolating objects from unforeseen images. Since medical experts, biologists, and materials scientists routinely examine microscopy or medical images in conjunction with textual information in the form of captions, literature, or reports, and draw conclusions of great importance and merit, it is indubitably essential to test the performance of VLMs and foundation models such as SAM, on these images. In this study, we charge ChatGPT, LLaVA, Gemini, and SAM with classification, segmentation, counting, and VQA tasks on a variety of microscopy images. We observe that ChatGPT and Gemini are impressively able to comprehend the visual features in microscopy images, while SAM is quite capable at isolating artefacts in a general sense. However, the performance is not close to that of a domain expert - the models are readily encumbered by the introduction of impurities, defects, artefact overlaps and diversity present in the images.</li>
</ul>

<h3>Title: SonicDiffusion: Audio-Driven Image Generation and Editing with  Pretrained Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Burak Can Biner, Farrin Marouf Sofian, Umur Berkay Karakaş, Duygu Ceylan, Erkut Erdem, Aykut Erdem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00878">https://arxiv.org/abs/2405.00878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00878">https://arxiv.org/pdf/2405.00878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00878]] SonicDiffusion: Audio-Driven Image Generation and Editing with  Pretrained Diffusion Models(https://arxiv.org/abs/2405.00878)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We are witnessing a revolution in conditional image synthesis with the recent success of large scale text-to-image generation methods. This success also opens up new opportunities in controlling the generation and editing process using multi-modal input. While spatial control using cues such as depth, sketch, and other images has attracted a lot of research, we argue that another equally effective modality is audio since sound and sight are two main components of human perception. Hence, we propose a method to enable audio-conditioning in large scale image diffusion models. Our method first maps features obtained from audio clips to tokens that can be injected into the diffusion model in a fashion similar to text tokens. We introduce additional audio-image cross attention layers which we finetune while freezing the weights of the original layers of the diffusion model. In addition to audio conditioned image generation, our method can also be utilized in conjuction with diffusion based editing methods to enable audio conditioned image editing. We demonstrate our method on a wide range of audio and image datasets. We perform extensive comparisons with recent methods and show favorable performance.</li>
</ul>

<h3>Title: Transformer-Based Self-Supervised Learning for Histopathological  Classification of Ischemic Stroke Clot Origin</h3>
<ul>
<li><strong>Authors: </strong>K. Yeh, M. S. Jabal, V. Gupta, D. F. Kallmes, W. Brinjikji, B. S. Erdal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00908">https://arxiv.org/abs/2405.00908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00908">https://arxiv.org/pdf/2405.00908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00908]] Transformer-Based Self-Supervised Learning for Histopathological  Classification of Ischemic Stroke Clot Origin(https://arxiv.org/abs/2405.00908)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Background and Purpose: Identifying the thromboembolism source in ischemic stroke is crucial for treatment and secondary prevention yet is often undetermined. This study describes a self-supervised deep learning approach in digital pathology of emboli for classifying ischemic stroke clot origin from histopathological images. Methods: The dataset included whole slide images (WSI) from the STRIP AI Kaggle challenge, consisting of retrieved clots from ischemic stroke patients following mechanical thrombectomy. Transformer-based deep learning models were developed using transfer learning and self-supervised pretraining for classifying WSI. Customizations included an attention pooling layer, weighted loss function, and threshold optimization. Various model architectures were tested and compared, and model performances were primarily evaluated using weighted logarithmic loss. Results: The model achieved a logloss score of 0.662 in cross-validation and 0.659 on the test set. Different model backbones were compared, with the swin_large_patch4_window12_384 showed higher performance. Thresholding techniques for clot origin classification were employed to balance false positives and negatives. Conclusion: The study demonstrates the extent of efficacy of transformer-based deep learning models in identifying ischemic stroke clot origins from histopathological images and emphasizes the need for refined modeling techniques specifically adapted to thrombi WSI. Further research is needed to improve model performance, interpretability, validate its effectiveness. Future enhancement could include integrating larger patient cohorts, advanced preprocessing strategies, and exploring ensemble multimodal methods for enhanced diagnostic accuracy.</li>
</ul>

<h3>Title: EchoScene: Indoor Scene Generation via Information Echo over Scene Graph  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Guangyao Zhai, Evin Pınar Örnek, Dave Zhenyu Chen, Ruotong Liao, Yan Di, Nassir Navab, Federico Tombari, Benjamin Busam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00915">https://arxiv.org/abs/2405.00915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00915">https://arxiv.org/pdf/2405.00915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00915]] EchoScene: Indoor Scene Generation via Information Echo over Scene Graph  Diffusion(https://arxiv.org/abs/2405.00915)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present EchoScene, an interactive and controllable generative model that generates 3D indoor scenes on scene graphs. EchoScene leverages a dual-branch diffusion model that dynamically adapts to scene graphs. Existing methods struggle to handle scene graphs due to varying numbers of nodes, multiple edge combinations, and manipulator-induced node-edge operations. EchoScene overcomes this by associating each node with a denoising process and enables collaborative information exchange, enhancing controllable and consistent generation aware of global constraints. This is achieved through an information echo scheme in both shape and layout branches. At every denoising step, all processes share their denoising data with an information exchange unit that combines these updates using graph convolution. The scheme ensures that the denoising processes are influenced by a holistic understanding of the scene graph, facilitating the generation of globally coherent scenes. The resulting scenes can be manipulated during inference by editing the input scene graph and sampling the noise in the diffusion model. Extensive experiments validate our approach, which maintains scene controllability and surpasses previous methods in generation fidelity. Moreover, the generated scenes are of high quality and thus directly compatible with off-the-shelf texture generation. Code and trained models are open-sourced.</li>
</ul>

<h3>Title: Generative manufacturing systems using diffusion models and ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Li, Fei Tao, Wei Ye, Aydin Nassehi, John W. Sutherland</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.HC, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00958">https://arxiv.org/abs/2405.00958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00958">https://arxiv.org/pdf/2405.00958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00958]] Generative manufacturing systems using diffusion models and ChatGPT(https://arxiv.org/abs/2405.00958)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this study, we introduce Generative Manufacturing Systems (GMS) as a novel approach to effectively manage and coordinate autonomous manufacturing assets, thereby enhancing their responsiveness and flexibility to address a wide array of production objectives and human preferences. Deviating from traditional explicit modeling, GMS employs generative AI, including diffusion models and ChatGPT, for implicit learning from envisioned futures, marking a shift from a model-optimum to a training-sampling decision-making. Through the integration of generative AI, GMS enables complex decision-making through interactive dialogue with humans, allowing manufacturing assets to generate multiple high-quality global decisions that can be iteratively refined based on human feedback. Empirical findings showcase GMS's substantial improvement in system resilience and responsiveness to uncertainties, with decision times reduced from seconds to milliseconds. The study underscores the inherent creativity and diversity in the generated solutions, facilitating human-centric decision-making through seamless and continuous human-machine interactions.</li>
</ul>

<h3>Title: Context-Aware Clustering using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sindhu Tipirneni, Ravinarayana Adkathimar, Nurendra Choudhary, Gaurush Hiranandani, Rana Ali Amjad, Vassilis N. Ioannidis, Changhe Yuan, Chandan K. Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00988">https://arxiv.org/abs/2405.00988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00988">https://arxiv.org/pdf/2405.00988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00988]] Context-Aware Clustering using Large Language Models(https://arxiv.org/abs/2405.00988)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of Large Language Models (LLMs) in text understanding and generation, their potential for text clustering tasks remains underexplored. We observed that powerful closed-source LLMs provide good quality clusterings of entity sets but are not scalable due to the massive compute power required and the associated costs. Thus, we propose CACTUS (Context-Aware ClusTering with aUgmented triplet losS), a systematic approach that leverages open-source LLMs for efficient and effective supervised clustering of entity subsets, particularly focusing on text-based entities. Existing text clustering methods fail to effectively capture the context provided by the entity subset. Moreover, though there are several language modeling based approaches for clustering, very few are designed for the task of supervised clustering. This paper introduces a novel approach towards clustering entity subsets using LLMs by capturing context via a scalable inter-entity attention mechanism. We propose a novel augmented triplet loss function tailored for supervised clustering, which addresses the inherent challenges of directly applying the triplet loss to this problem. Furthermore, we introduce a self-supervised clustering task based on text augmentation techniques to improve the generalization of our model. For evaluation, we collect ground truth clusterings from a closed-source LLM and transfer this knowledge to an open-source LLM under the supervised clustering framework, allowing a faster and cheaper open-source model to perform the same task. Experiments on various e-commerce query and product clustering datasets demonstrate that our proposed approach significantly outperforms existing unsupervised and supervised baselines under various external clustering evaluation metrics.</li>
</ul>

<h3>Title: Part-aware Shape Generation with Latent 3D Diffusion of Neural Voxel  Fields</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Huang, SHilong Zou, Xinwang Liu, Kai Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00998">https://arxiv.org/abs/2405.00998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00998">https://arxiv.org/pdf/2405.00998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00998]] Part-aware Shape Generation with Latent 3D Diffusion of Neural Voxel  Fields(https://arxiv.org/abs/2405.00998)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper presents a novel latent 3D diffusion model for the generation of neural voxel fields, aiming to achieve accurate part-aware structures. Compared to existing methods, there are two key designs to ensure high-quality and accurate part-aware generation. On one hand, we introduce a latent 3D diffusion process for neural voxel fields, enabling generation at significantly higher resolutions that can accurately capture rich textural and geometric details. On the other hand, a part-aware shape decoder is introduced to integrate the part codes into the neural voxel fields, guiding the accurate part decomposition and producing high-quality rendering results. Through extensive experimentation and comparisons with state-of-the-art methods, we evaluate our approach across four different classes of data. The results demonstrate the superior generative capabilities of our proposed method in part-aware shape generation, outperforming existing state-of-the-art methods.</li>
</ul>

<h3>Title: On Mechanistic Knowledge Localization in Text-to-Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Samyadeep Basu, Keivan Rezaei, Ryan Rossi, Cherry Zhao, Vlad Morariu, Varun Manjunatha, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01008">https://arxiv.org/abs/2405.01008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01008">https://arxiv.org/pdf/2405.01008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01008]] On Mechanistic Knowledge Localization in Text-to-Image Generative Models(https://arxiv.org/abs/2405.01008)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Identifying layers within text-to-image models which control visual attributes can facilitate efficient model editing through closed-form updates. Recent work, leveraging causal tracing show that early Stable-Diffusion variants confine knowledge primarily to the first layer of the CLIP text-encoder, while it diffuses throughout the UNet.Extending this framework, we observe that for recent models (e.g., SD-XL, DeepFloyd), causal tracing fails in pinpointing localized knowledge, highlighting challenges in model editing. To address this issue, we introduce the concept of Mechanistic Localization in text-to-image models, where knowledge about various visual attributes (e.g., ``style", ``objects", ``facts") can be mechanistically localized to a small fraction of layers in the UNet, thus facilitating efficient model editing. We localize knowledge using our method LocoGen which measures the direct effect of intermediate layers to output generation by performing interventions in the cross-attention layers of the UNet. We then employ LocoEdit, a fast closed-form editing method across popular open-source text-to-image models (including the latest SD-XL)and explore the possibilities of neuron-level model editing. Using Mechanistic Localization, our work offers a better view of successes and failures in localization-based text-to-image model editing. Code will be available at \href{https://github.com/samyadeepbasu/LocoGen}{https://github.com/samyadeepbasu/LocoGen}.</li>
</ul>

<h3>Title: Explicitly Modeling Generality into Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Jingyao Wang, Wenwen Qiang, Changwen Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01053">https://arxiv.org/abs/2405.01053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01053">https://arxiv.org/pdf/2405.01053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01053]] Explicitly Modeling Generality into Self-Supervised Learning(https://arxiv.org/abs/2405.01053)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The goal of generality in machine learning is to achieve excellent performance on various unseen tasks and domains. Recently, self-supervised learning (SSL) has been regarded as an effective method to achieve this goal. It can learn high-quality representations from unlabeled data and achieve promising empirical performance on multiple downstream tasks. Existing SSL methods mainly constrain generality from two aspects: (i) large-scale training data, and (ii) learning task-level shared knowledge. However, these methods lack explicit modeling of the SSL generality in the learning objective, and the theoretical understanding of SSL's generality remains limited. This may cause SSL models to overfit in data-scarce situations and generalize poorly in the real world, making it difficult to achieve true generality. To address these issues, we provide a theoretical definition of generality in SSL and define a $\sigma$-measurement to help quantify it. Based on this insight, we explicitly model generality into self-supervised learning and further propose a novel SSL framework, called GeSSL. It introduces a self-motivated target based on $\sigma$-measurement, which enables the model to find the optimal update direction towards generality. Extensive theoretical and empirical evaluations demonstrate the superior performance of the proposed GeSSL.</li>
</ul>

<h3>Title: A text-based, generative deep learning model for soil reflectance  spectrum simulation in the VIS-NIR (400-2499 nm) bands</h3>
<ul>
<li><strong>Authors: </strong>Tong Lei, Brian N. Bailey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01060">https://arxiv.org/abs/2405.01060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01060">https://arxiv.org/pdf/2405.01060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01060]] A text-based, generative deep learning model for soil reflectance  spectrum simulation in the VIS-NIR (400-2499 nm) bands(https://arxiv.org/abs/2405.01060)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Simulating soil reflectance spectra is invaluable for soil-plant radiative modeling and training machine learning models, yet it is difficult as the intricate relationships between soil structure and its constituents. To address this, a fully data-driven soil optics generative model (SOGM) for simulation of soil reflectance spectra based on soil property inputs was developed. The model is trained on an extensive dataset comprising nearly 180,000 soil spectra-property pairs from 17 datasets. It generates soil reflectance spectra from text-based inputs describing soil properties and their values rather than only numerical values and labels in binary vector format. The generative model can simulate output spectra based on an incomplete set of input properties. SOGM is based on the denoising diffusion probabilistic model (DDPM). Two additional sub-models were also built to complement the SOGM: a spectral padding model that can fill in the gaps for spectra shorter than the full visible-near-infrared range (VIS-NIR; 400 to 2499 nm), and a wet soil spectra model that can estimate the effects of water content on soil reflectance spectra given the dry spectrum predicted by the SOGM. The SOGM was up-scaled by coupling with the Helios 3D plant modeling software, which allowed for generation of synthetic aerial images of simulated soil and plant scenes. It can also be easily integrated with soil-plant radiation model used for remote sensin research like PROSAIL. The testing results of the SOGM on new datasets that not included in model training proved that the model can generate reasonable soil reflectance spectra based on available property inputs. The presented models are openly accessible on: https://github.com/GEMINI-Breeding/SOGM_soil_spectra_simulation.</li>
</ul>

<h3>Title: Automated Virtual Product Placement and Assessment in Images using  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Mahmudul Alam, Negin Sokhandan, Emmett Goodman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01130">https://arxiv.org/abs/2405.01130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01130">https://arxiv.org/pdf/2405.01130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01130]] Automated Virtual Product Placement and Assessment in Images using  Diffusion Models(https://arxiv.org/abs/2405.01130)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In Virtual Product Placement (VPP) applications, the discrete integration of specific brand products into images or videos has emerged as a challenging yet important task. This paper introduces a novel three-stage fully automated VPP system. In the first stage, a language-guided image segmentation model identifies optimal regions within images for product inpainting. In the second stage, Stable Diffusion (SD), fine-tuned with a few example product images, is used to inpaint the product into the previously identified candidate regions. The final stage introduces an "Alignment Module", which is designed to effectively sieve out low-quality images. Comprehensive experiments demonstrate that the Alignment Module ensures the presence of the intended product in every generated image and enhances the average quality of images by 35%. The results presented in this paper demonstrate the effectiveness of the proposed VPP system, which holds significant potential for transforming the landscape of virtual advertising and marketing strategies.</li>
</ul>

<h3>Title: Why Tabular Foundation Models Should Be a Research Priority</h3>
<ul>
<li><strong>Authors: </strong>Boris van Breugel, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01147">https://arxiv.org/abs/2405.01147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01147">https://arxiv.org/pdf/2405.01147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01147]] Why Tabular Foundation Models Should Be a Research Priority(https://arxiv.org/abs/2405.01147)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent text and image foundation models are incredibly impressive, and these models are attracting an ever-increasing portion of research resources. In this position piece we aim to shift the ML research community's priorities ever so slightly to a different modality: tabular data. Tabular data is the dominant modality in many fields, yet it is given hardly any research attention and significantly lags behind in terms of scale and power. We believe the time is now to start developing tabular foundation models, or what we coin a Large Tabular Model (LTM). LTMs could revolutionise the way science and ML use tabular data: not as single datasets that are analyzed in a vacuum, but contextualized with respect to related datasets. The potential impact is far-reaching: from few-shot tabular models to automating data science; from out-of-distribution synthetic data to empowering multidisciplinary scientific discovery. We intend to excite reflections on the modalities we study, and convince some researchers to study large tabular models.</li>
</ul>

<h3>Title: SynFlowNet: Towards Molecule Design with Guaranteed Synthesis Pathways</h3>
<ul>
<li><strong>Authors: </strong>Miruna Cretu, Charles Harris, Julien Roy, Emmanuel Bengio, Pietro Liò</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01155">https://arxiv.org/abs/2405.01155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01155">https://arxiv.org/pdf/2405.01155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01155]] SynFlowNet: Towards Molecule Design with Guaranteed Synthesis Pathways(https://arxiv.org/abs/2405.01155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent breakthroughs in generative modelling have led to a number of works proposing molecular generation models for drug discovery. While these models perform well at capturing drug-like motifs, they are known to often produce synthetically inaccessible molecules. This is because they are trained to compose atoms or fragments in a way that approximates the training distribution, but they are not explicitly aware of the synthesis constraints that come with making molecules in the lab. To address this issue, we introduce SynFlowNet, a GFlowNet model whose action space uses chemically validated reactions and reactants to sequentially build new molecules. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool. SynFlowNet consistently samples synthetically feasible molecules, while still being able to find diverse and high-utility candidates. Furthermore, we compare molecules designed with SynFlowNet to experimentally validated actives, and find that they show comparable properties of interest, such as molecular weight, SA score and predicted protein binding affinity.</li>
</ul>

<h3>Title: Self-Supervised Learning for Interventional Image Analytics: Towards  Robust Device Trackers</h3>
<ul>
<li><strong>Authors: </strong>Saahil Islam, Venkatesh N. Murthy, Dominik Neumann, Badhan Kumar Das, Puneet Sharma, Andreas Maier, Dorin Comaniciu, Florin C. Ghesu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01156">https://arxiv.org/abs/2405.01156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01156">https://arxiv.org/pdf/2405.01156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01156]] Self-Supervised Learning for Interventional Image Analytics: Towards  Robust Device Trackers(https://arxiv.org/abs/2405.01156)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>An accurate detection and tracking of devices such as guiding catheters in live X-ray image acquisitions is an essential prerequisite for endovascular cardiac interventions. This information is leveraged for procedural guidance, e.g., directing stent placements. To ensure procedural safety and efficacy, there is a need for high robustness no failures during tracking. To achieve that, one needs to efficiently tackle challenges, such as: device obscuration by contrast agent or other external devices or wires, changes in field-of-view or acquisition angle, as well as the continuous movement due to cardiac and respiratory motion. To overcome the aforementioned challenges, we propose a novel approach to learn spatio-temporal features from a very large data cohort of over 16 million interventional X-ray frames using self-supervision for image sequence data. Our approach is based on a masked image modeling technique that leverages frame interpolation based reconstruction to learn fine inter-frame temporal correspondences. The features encoded in the resulting model are fine-tuned downstream. Our approach achieves state-of-the-art performance and in particular robustness compared to ultra optimized reference solutions (that use multi-stage feature fusion, multi-task and flow regularization). The experiments show that our method achieves 66.31% reduction in maximum tracking error against reference solutions (23.20% when flow regularization is used); achieving a success score of 97.95% at a 3x faster inference speed of 42 frames-per-second (on GPU). The results encourage the use of our approach in various other tasks within interventional image analytics that require effective understanding of spatio-temporal semantics.</li>
</ul>

<h3>Title: Interpretable Data-driven Anomaly Detection in Industrial Processes with  ExIFFI</h3>
<ul>
<li><strong>Authors: </strong>Davide Frizzo, Francesco Borsatti, Alessio Arcudi, Antonio De Moliner, Roberto Oboe, Gian Antonio Susto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01158">https://arxiv.org/abs/2405.01158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01158">https://arxiv.org/pdf/2405.01158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01158]] Interpretable Data-driven Anomaly Detection in Industrial Processes with  ExIFFI(https://arxiv.org/abs/2405.01158)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) is a crucial process often required in industrial settings. Anomalies can signal underlying issues within a system, prompting further investigation. Industrial processes aim to streamline operations as much as possible, encompassing the production of the final product, making AD an essential mean to reach this goal.Conventional anomaly detection methodologies typically classify observations as either normal or anomalous without providing insight into the reasons behind these classifications.Consequently, in light of the emergence of Industry 5.0, a more desirable approach involves providing interpretable outcomes, enabling users to understand the rationale behind the results.This paper presents the first industrial application of ExIFFI, a recently developed approach focused on the production of fast and efficient explanations for the Extended Isolation Forest (EIF) Anomaly detection method. ExIFFI is tested on two publicly available industrial datasets demonstrating superior effectiveness in explanations and computational efficiency with the respect to other state-of-the-art explainable AD models.</li>
</ul>

<h3>Title: Towards Inclusive Face Recognition Through Synthetic Ethnicity  Alteration</h3>
<ul>
<li><strong>Authors: </strong>Praveen Kumar Chandaliya, Kiran Raja, Raghavendra Ramachandra, Zahid Akhtar, Christoph Busch</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01273">https://arxiv.org/abs/2405.01273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01273">https://arxiv.org/pdf/2405.01273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01273]] Towards Inclusive Face Recognition Through Synthetic Ethnicity  Alteration(https://arxiv.org/abs/2405.01273)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Numerous studies have shown that existing Face Recognition Systems (FRS), including commercial ones, often exhibit biases toward certain ethnicities due to under-represented data. In this work, we explore ethnicity alteration and skin tone modification using synthetic face image generation methods to increase the diversity of datasets. We conduct a detailed analysis by first constructing a balanced face image dataset representing three ethnicities: Asian, Black, and Indian. We then make use of existing Generative Adversarial Network-based (GAN) image-to-image translation and manifold learning models to alter the ethnicity from one to another. A systematic analysis is further conducted to assess the suitability of such datasets for FRS by studying the realistic skin-tone representation using Individual Typology Angle (ITA). Further, we also analyze the quality characteristics using existing Face image quality assessment (FIQA) approaches. We then provide a holistic FRS performance analysis using four different systems. Our findings pave the way for future research works in (i) developing both specific ethnicity and general (any to any) ethnicity alteration models, (ii) expanding such approaches to create databases with diverse skin tones, (iii) creating datasets representing various ethnicities which further can help in mitigating bias while addressing privacy concerns.</li>
</ul>

<h3>Title: StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video  Generation</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, Qibin Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01434">https://arxiv.org/abs/2405.01434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01434">https://arxiv.org/pdf/2405.01434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01434]] StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video  Generation(https://arxiv.org/abs/2405.01434)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>For recent diffusion-based generative models, maintaining consistent content across a series of generated images, especially those containing subjects and complex details, presents a significant challenge. In this paper, we propose a new way of self-attention calculation, termed Consistent Self-Attention, that significantly boosts the consistency between the generated images and augments prevalent pretrained diffusion-based text-to-image models in a zero-shot manner. To extend our method to long-range video generation, we further introduce a novel semantic space temporal motion prediction module, named Semantic Motion Predictor. It is trained to estimate the motion conditions between two provided images in the semantic spaces. This module converts the generated sequence of images into videos with smooth transitions and consistent subjects that are significantly more stable than the modules based on latent spaces only, especially in the context of long video generation. By merging these two novel components, our framework, referred to as StoryDiffusion, can describe a text-based story with consistent images or videos encompassing a rich variety of contents. The proposed StoryDiffusion encompasses pioneering explorations in visual story generation with the presentation of images and videos, which we hope could inspire more research from the aspect of architectural modifications. Our code is made publicly available at https://github.com/HVision-NKU/StoryDiffusion.</li>
</ul>

<h3>Title: Advancing human-centric AI for robust X-ray analysis through holistic  self-supervised learning</h3>
<ul>
<li><strong>Authors: </strong>Théo Moutakanni, Piotr Bojanowski, Guillaume Chassagnon, Céline Hudelot, Armand Joulin, Yann LeCun, Matthew Muckley, Maxime Oquab, Marie-Pierre Revel, Maria Vakalopoulou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01469">https://arxiv.org/abs/2405.01469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01469">https://arxiv.org/pdf/2405.01469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01469]] Advancing human-centric AI for robust X-ray analysis through holistic  self-supervised learning(https://arxiv.org/abs/2405.01469)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>AI Foundation models are gaining traction in various applications, including medical fields like radiology. However, medical foundation models are often tested on limited tasks, leaving their generalisability and biases unexplored. We present RayDINO, a large visual encoder trained by self-supervision on 873k chest X-rays. We compare RayDINO to previous state-of-the-art models across nine radiology tasks, from classification and dense segmentation to text generation, and provide an in depth analysis of population, age and sex biases of our model. Our findings suggest that self-supervision allows patient-centric AI proving useful in clinical workflows and interpreting X-rays holistically. With RayDINO and small task-specific adapters, we reach state-of-the-art results and improve generalization to unseen populations while mitigating bias, illustrating the true promise of foundation models: versatility and robustness.</li>
</ul>

<h3>Title: Digital Twin Generators for Disease Modeling</h3>
<ul>
<li><strong>Authors: </strong>Nameyeh Alam, Jake Basilico, Daniele Bertolini, Satish Casie Chetty, Heather D'Angelo, Ryan Douglas, Charles K. Fisher, Franklin Fuller, Melissa Gomes, Rishabh Gupta, Alex Lang, Anton Loukianov, Rachel Mak-McCully, Cary Murray, Hanalei Pham, Susanna Qiao, Elena Ryapolova-Webb, Aaron Smith, Dimitri Theoharatos, Anil Tolwani, Eric W. Tramel, Anna Vidovszky, Judy Viduya, Jonathan R. Walsh</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01488">https://arxiv.org/abs/2405.01488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01488">https://arxiv.org/pdf/2405.01488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01488]] Digital Twin Generators for Disease Modeling(https://arxiv.org/abs/2405.01488)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A patient's digital twin is a computational model that describes the evolution of their health over time. Digital twins have the potential to revolutionize medicine by enabling individual-level computer simulations of human health, which can be used to conduct more efficient clinical trials or to recommend personalized treatment options. Due to the overwhelming complexity of human biology, machine learning approaches that leverage large datasets of historical patients' longitudinal health records to generate patients' digital twins are more tractable than potential mechanistic models. In this manuscript, we describe a neural network architecture that can learn conditional generative models of clinical trajectories, which we call Digital Twin Generators (DTGs), that can create digital twins of individual patients. We show that the same neural network architecture can be trained to generate accurate digital twins for patients across 13 different indications simply by changing the training set and tuning hyperparameters. By introducing a general purpose architecture, we aim to unlock the ability to scale machine learning approaches to larger datasets and across more indications so that a digital twin could be created for any patient in the world.</li>
</ul>

<h3>Title: Controllable Text Generation in the Instruction-Tuning Era</h3>
<ul>
<li><strong>Authors: </strong>Dhananjay Ashok, Barnabas Poczos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01490">https://arxiv.org/abs/2405.01490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01490">https://arxiv.org/pdf/2405.01490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01490]] Controllable Text Generation in the Instruction-Tuning Era(https://arxiv.org/abs/2405.01490)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>While most research on controllable text generation has focused on steering base Language Models, the emerging instruction-tuning and prompting paradigm offers an alternate approach to controllability. We compile and release ConGenBench, a testbed of 17 different controllable generation tasks, using a subset of it to benchmark the performance of 9 different baselines and methods on Instruction-tuned Language Models. To our surprise, we find that prompting-based approaches outperform controllable text generation methods on most datasets and tasks, highlighting a need for research on controllable text generation with Instruction-tuned Language Models in specific. Prompt-based approaches match human performance on most stylistic tasks while lagging on structural tasks, foregrounding a need to study more varied constraints and more challenging stylistic tasks. To facilitate such research, we provide an algorithm that uses only a task dataset and a Large Language Model with in-context capabilities to automatically generate a constraint dataset. This method eliminates the fields dependence on pre-curated constraint datasets, hence vastly expanding the range of constraints that can be studied in the future.</li>
</ul>

<h3>Title: Navigating Heterogeneity and Privacy in One-Shot Federated Learning with  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Matias Mendieta, Guangyu Sun, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01494">https://arxiv.org/abs/2405.01494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01494">https://arxiv.org/pdf/2405.01494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01494]] Navigating Heterogeneity and Privacy in One-Shot Federated Learning with  Diffusion Models(https://arxiv.org/abs/2405.01494)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables multiple clients to train models collectively while preserving data privacy. However, FL faces challenges in terms of communication cost and data heterogeneity. One-shot federated learning has emerged as a solution by reducing communication rounds, improving efficiency, and providing better security against eavesdropping attacks. Nevertheless, data heterogeneity remains a significant challenge, impacting performance. This work explores the effectiveness of diffusion models in one-shot FL, demonstrating their applicability in addressing data heterogeneity and improving FL performance. Additionally, we investigate the utility of our diffusion model approach, FedDiff, compared to other one-shot FL methods under differential privacy (DP). Furthermore, to improve generated sample quality under DP settings, we propose a pragmatic Fourier Magnitude Filtering (FMF) method, enhancing the effectiveness of generated data for global model training.</li>
</ul>

<h3>Title: LocInv: Localization-aware Inversion for Text-Guided Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Chuanming Tang, Kai Wang, Fei Yang, Joost van de Weijer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01496">https://arxiv.org/abs/2405.01496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01496">https://arxiv.org/pdf/2405.01496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01496]] LocInv: Localization-aware Inversion for Text-Guided Image Editing(https://arxiv.org/abs/2405.01496)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale Text-to-Image (T2I) diffusion models demonstrate significant generation capabilities based on textual prompts. Based on the T2I diffusion models, text-guided image editing research aims to empower users to manipulate generated images by altering the text prompts. However, existing image editing techniques are prone to editing over unintentional regions that are beyond the intended target area, primarily due to inaccuracies in cross-attention maps. To address this problem, we propose Localization-aware Inversion (LocInv), which exploits segmentation maps or bounding boxes as extra localization priors to refine the cross-attention maps in the denoising phases of the diffusion process. Through the dynamic updating of tokens corresponding to noun words in the textual input, we are compelling the cross-attention maps to closely align with the correct noun and adjective words in the text prompt. Based on this technique, we achieve fine-grained image editing over particular objects while preventing undesired changes to other regions. Our method LocInv, based on the publicly available Stable Diffusion, is extensively evaluated on a subset of the COCO dataset, and consistently obtains superior results both quantitatively and qualitatively.The code will be released at https://github.com/wangkai930418/DPL</li>
</ul>

<h3>Title: A separability-based approach to quantifying generalization: which layer  is best?</h3>
<ul>
<li><strong>Authors: </strong>Luciano Dyballa, Evan Gerritz, Steven W. Zucker</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01524">https://arxiv.org/abs/2405.01524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01524">https://arxiv.org/pdf/2405.01524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01524]] A separability-based approach to quantifying generalization: which layer  is best?(https://arxiv.org/abs/2405.01524)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Generalization to unseen data remains poorly understood for deep learning classification and foundation models. How can one assess the ability of networks to adapt to new or extended versions of their input space in the spirit of few-shot learning, out-of-distribution generalization, and domain adaptation? Which layers of a network are likely to generalize best? We provide a new method for evaluating the capacity of networks to represent a sampled domain, regardless of whether the network has been trained on all classes in the domain. Our approach is the following: after fine-tuning state-of-the-art pre-trained models for visual classification on a particular domain, we assess their performance on data from related but distinct variations in that domain. Generalization power is quantified as a function of the latent embeddings of unseen data from intermediate layers for both unsupervised and supervised settings. Working throughout all stages of the network, we find that (i) high classification accuracy does not imply high generalizability; and (ii) deeper layers in a model do not always generalize the best, which has implications for pruning. Since the trends observed across datasets are largely consistent, we conclude that our approach reveals (a function of) the intrinsic capacity of the different layers of a model to generalize.</li>
</ul>

<h3>Title: Customizing Text-to-Image Models with a Single Image Pair</h3>
<ul>
<li><strong>Authors: </strong>Maxwell Jones, Sheng-Yu Wang, Nupur Kumari, David Bau, Jun-Yan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01536">https://arxiv.org/abs/2405.01536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01536">https://arxiv.org/pdf/2405.01536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01536]] Customizing Text-to-Image Models with a Single Image Pair(https://arxiv.org/abs/2405.01536)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Art reinterpretation is the practice of creating a variation of a reference work, making a paired artwork that exhibits a distinct artistic style. We ask if such an image pair can be used to customize a generative model to capture the demonstrated stylistic difference. We propose Pair Customization, a new customization method that learns stylistic difference from a single image pair and then applies the acquired style to the generation process. Unlike existing methods that learn to mimic a single concept from a collection of images, our method captures the stylistic difference between paired images. This allows us to apply a stylistic change without overfitting to the specific image content in the examples. To address this new task, we employ a joint optimization method that explicitly separates the style and content into distinct LoRA weight spaces. We optimize these style and content weights to reproduce the style and content images while encouraging their orthogonality. During inference, we modify the diffusion process via a new style guidance based on our learned weights. Both qualitative and quantitative experiments show that our method can effectively learn style while avoiding overfitting to image content, highlighting the potential of modeling such stylistic differences from a single image pair.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
