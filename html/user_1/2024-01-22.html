<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-22</h1>
<h3>Title: Resolution Chromatography of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Juno Hwang, Yong-Hyun Park, Junghyo Jo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10247">https://arxiv.org/abs/2401.10247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10247">https://arxiv.org/pdf/2401.10247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10247]] Resolution Chromatography of Diffusion Models(https://arxiv.org/abs/2401.10247)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models generate high-resolution images through iterative stochastic processes. In particular, the denoising method is one of the most popular approaches that predicts the noise in samples and denoises it at each time step. It has been commonly observed that the resolution of generated samples changes over time, starting off blurry and coarse, and becoming sharper and finer. In this paper, we introduce "resolution chromatography" that indicates the signal generation rate of each resolution, which is very helpful concept to mathematically explain this coarse-to-fine behavior in generation process, to understand the role of noise schedule, and to design time-dependent modulation. Using resolution chromatography, we determine which resolution level becomes dominant at a specific time step, and experimentally verify our theory with text-to-image diffusion models. We also propose some direct applications utilizing the concept: upscaling pre-trained models to higher resolutions and time-dependent prompt composing. Our theory not only enables a better understanding of numerous pre-existing techniques for manipulating image generation, but also suggests the potential for designing better noise schedules.</li>
</ul>

<h3>Title: An attempt to generate new bridge types from latent space of generative  flow</h3>
<ul>
<li><strong>Authors: </strong>Hongjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10299">https://arxiv.org/abs/2401.10299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10299">https://arxiv.org/pdf/2401.10299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10299]] An attempt to generate new bridge types from latent space of generative  flow(https://arxiv.org/abs/2401.10299)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Through examples of coordinate and probability transformation between different distributions, the basic principle of normalizing flow is introduced in a simple and concise manner. From the perspective of the distribution of random variable function, the essence of probability transformation is explained, and the scaling factor Jacobian determinant of probability transformation is introduced. Treating the dataset as a sample from the population, obtaining normalizing flow is essentially through sampling surveys to statistically infer the numerical features of the population, and then the loss function is established by using the maximum likelihood estimation method. This article introduces how normalizing flow cleverly solves the two major application challenges of high-dimensional matrix determinant calculation and neural network reversible transformation. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge, constructing and training normalizing flow based on the Glow API in the TensorFlow Probability library. The model can smoothly transform the complex distribution of the bridge dataset into a standard normal distribution, and from the obtained latent space sampling, it can generate new bridge types that are different from the training dataset.</li>
</ul>

<h3>Title: MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online  Anomaly Detection with Multivariate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Jingchao Ni, Gauthier Guinet, Peihong Jiang, Laurent Callot, Andrey Kan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10338">https://arxiv.org/abs/2401.10338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10338">https://arxiv.org/pdf/2401.10338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10338]] MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online  Anomaly Detection with Multivariate Time Series(https://arxiv.org/abs/2401.10338)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In large IT systems, software deployment is a crucial process in online services as their code is regularly updated. However, a faulty code change may degrade the target service's performance and cause cascading outages in downstream services. Thus, software deployments should be comprehensively monitored, and their anomalies should be detected timely. In this paper, we study the problem of anomaly detection for deployments. We begin by identifying the challenges unique to this anomaly detection problem, which is at entity-level (e.g., deployments), relative to the more typical problem of anomaly detection in multivariate time series (MTS). The unique challenges include the heterogeneity of deployments, the low latency tolerance, the ambiguous anomaly definition, and the limited supervision. To address them, we propose a novel framework, semi-supervised hybrid Model for Entity-Level Online Detection of anomalY (MELODY). MELODY first transforms the MTS of different entities to the same feature space by an online feature extractor, then uses a newly proposed semi-supervised deep one-class model for detecting anomalous entities. We evaluated MELODY on real data of cloud services with 1.2M+ time series. The relative F1 score improvement of MELODY over the state-of-the-art methods ranges from 7.6% to 56.5%. The user evaluation suggests MELODY is suitable for monitoring deployments in large online systems.</li>
</ul>

<h3>Title: Vulnerabilities of Foundation Model Integrated Federated Learning Under  Adversarial Threats</h3>
<ul>
<li><strong>Authors: </strong>Chen Wu, Xi Li, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10375">https://arxiv.org/abs/2401.10375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10375">https://arxiv.org/pdf/2401.10375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10375]] Vulnerabilities of Foundation Model Integrated Federated Learning Under  Adversarial Threats(https://arxiv.org/abs/2401.10375)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) addresses critical issues in machine learning related to data privacy and security, yet suffering from data insufficiency and imbalance under certain circumstances. The emergence of foundation models (FMs) offers potential solutions to the limitations of existing FL frameworks, e.g., by generating synthetic data for model initialization. However, due to the inherent safety concerns of FMs, integrating FMs into FL could introduce new risks, which remains largely unexplored. To address this gap, we conduct the first investigation on the vulnerability of FM integrated FL (FM-FL) under adversarial threats. Based on a unified framework of FM-FL, we introduce a novel attack strategy that exploits safety issues of FM to compromise FL client models. Through extensive experiments with well-known models and benchmark datasets in both image and text domains, we reveal the high susceptibility of the FM-FL to this new threat under various FL configurations. Furthermore, we find that existing FL defense strategies offer limited protection against this novel attack approach. This research highlights the critical need for enhanced security measures in FL in the era of FMs.</li>
</ul>

<h3>Title: Catastrophic Interference is Mitigated in Naturalistic Power-Law  Learning Environments</h3>
<ul>
<li><strong>Authors: </strong>Atith Gandhi, Raj Sanjay Shah, Vijay Marupudi, Sashank Varma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10393">https://arxiv.org/abs/2401.10393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10393">https://arxiv.org/pdf/2401.10393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10393]] Catastrophic Interference is Mitigated in Naturalistic Power-Law  Learning Environments(https://arxiv.org/abs/2401.10393)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based approach for a domain-incremental task: learning permutations in the MNIST task. We compare our rehearsal environment with other baselines to show its efficacy in promoting continual learning. Additionally, we investigate whether this environment shows forward facilitation, i.e., faster learning of later tasks. Next, we explore the robustness of our learning environment to the number of tasks, model size, and amount of data rehearsed after each task. Notably, our results show that the performance is comparable or superior to that of models trained using popular regularization methods and also to rehearsals in non-power-law environments. The benefits of this training paradigm include simplicity and the lack of a need for extra neural circuitry. In addition, because our method is orthogonal to other methods, future research can combine training in power-law environments with other continual learning mechanisms.</li>
</ul>

<h3>Title: Inflation with Diffusion: Efficient Temporal Adaptation for  Text-to-Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Xin Yuan, Jinoo Baek, Keyang Xu, Omer Tov, Hongliang Fei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10404">https://arxiv.org/abs/2401.10404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10404">https://arxiv.org/pdf/2401.10404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10404]] Inflation with Diffusion: Efficient Temporal Adaptation for  Text-to-Video Super-Resolution(https://arxiv.org/abs/2401.10404)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose an efficient diffusion-based text-to-video super-resolution (SR) tuning approach that leverages the readily learned capacity of pixel level image diffusion model to capture spatial information for video generation. To accomplish this goal, we design an efficient architecture by inflating the weightings of the text-to-image SR model into our video generation framework. Additionally, we incorporate a temporal adapter to ensure temporal coherence across video frames. We investigate different tuning approaches based on our inflated architecture and report trade-offs between computational costs and super-resolution quality. Empirical evaluation, both quantitative and qualitative, on the Shutterstock video dataset, demonstrates that our approach is able to perform text-to-video SR generation with good visual quality and temporal consistency. To evaluate temporal coherence, we also present visualizations in video format in https://drive.google.com/drive/folders/1YVc-KMSJqOrEUdQWVaI-Yfu8Vsfu_1aO?usp=sharing .</li>
</ul>

<h3>Title: Large Language Models are Efficient Learners of Noise-Robust Speech  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, EnSiong Chng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10446">https://arxiv.org/abs/2401.10446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10446">https://arxiv.org/pdf/2401.10446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10446]] Large Language Models are Efficient Learners of Noise-Robust Speech  Recognition(https://arxiv.org/abs/2401.10446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, which can promote the denoising process in GER. Furthermore, in order to enhance its representation ability of audio noise, we design a knowledge distillation (KD) approach via mutual information estimation to distill the real noise information in audio embeddings to our language embedding. Experiments on various latest LLMs demonstrate our approach achieves a new breakthrough with up to 53.9% correction improvement in terms of word error rate while with limited training data. Analysis shows that our language-space noise embedding can well represent the noise conditions of source speech, under which off-the-shelf LLMs show strong ability of language-space denoising.</li>
</ul>

<h3>Title: Data-driven grapheme-to-phoneme representations for a lexicon-free  text-to-speech</h3>
<ul>
<li><strong>Authors: </strong>Abhinav Garg, Jiyeon Kim, Sushil Khyalia, Chanwoo Kim, Dhananjaya Gowda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10465">https://arxiv.org/abs/2401.10465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10465">https://arxiv.org/pdf/2401.10465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10465]] Data-driven grapheme-to-phoneme representations for a lexicon-free  text-to-speech(https://arxiv.org/abs/2401.10465)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Grapheme-to-Phoneme (G2P) is an essential first step in any modern, high-quality Text-to-Speech (TTS) system. Most of the current G2P systems rely on carefully hand-crafted lexicons developed by experts. This poses a two-fold problem. Firstly, the lexicons are generated using a fixed phoneme set, usually, ARPABET or IPA, which might not be the most optimal way to represent phonemes for all languages. Secondly, the man-hours required to produce such an expert lexicon are very high. In this paper, we eliminate both of these issues by using recent advances in self-supervised learning to obtain data-driven phoneme representations instead of fixed representations. We compare our lexicon-free approach against strong baselines that utilize a well-crafted lexicon. Furthermore, we show that our data-driven lexicon-free method performs as good or even marginally better than the conventional rule-based or lexicon-based neural G2Ps in terms of Mean Opinion Score (MOS) while using no prior language lexicon or phoneme set, i.e. no linguistic expertise.</li>
</ul>

<h3>Title: LDReg: Local Dimensionality Regularized Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Hanxun Huang, Ricardo J. G. B. Campello, Sarah Monazam Erfani, Xingjun Ma, Michael E. Houle, James Bailey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10474">https://arxiv.org/abs/2401.10474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10474">https://arxiv.org/pdf/2401.10474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10474]] LDReg: Local Dimensionality Regularized Self-Supervised Learning(https://arxiv.org/abs/2401.10474)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the representation quality of SSL. The results also show that LDReg can regularize dimensionality at both local and global levels.</li>
</ul>

<h3>Title: Knowledge Fusion of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10491">https://arxiv.org/abs/2401.10491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10491">https://arxiv.org/pdf/2401.10491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10491]] Knowledge Fusion of Large Language Models(https://arxiv.org/abs/2401.10491)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \url{https://github.com/fanqiwan/FuseLLM}.</li>
</ul>

<h3>Title: On mitigating stability-plasticity dilemma in CLIP-guided image morphing  via geodesic distillation loss</h3>
<ul>
<li><strong>Authors: </strong>Yeongtak Oh, Saehyung Lee, Uiwon Hwang, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10526">https://arxiv.org/abs/2401.10526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10526">https://arxiv.org/pdf/2401.10526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10526]] On mitigating stability-plasticity dilemma in CLIP-guided image morphing  via geodesic distillation loss(https://arxiv.org/abs/2401.10526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large-scale language-vision pre-training models, such as CLIP, have achieved remarkable text-guided image morphing results by leveraging several unconditional generative models. However, existing CLIP-guided image morphing methods encounter difficulties when morphing photorealistic images. Specifically, existing guidance fails to provide detailed explanations of the morphing regions within the image, leading to misguidance. In this paper, we observed that such misguidance could be effectively mitigated by simply using a proper regularization loss. Our approach comprises two key components: 1) a geodesic cosine similarity loss that minimizes inter-modality features (i.e., image and text) on a projected subspace of CLIP space, and 2) a latent regularization loss that minimizes intra-modality features (i.e., image and image) on the image manifold. By replacing the na\"ive directional CLIP loss in a drop-in replacement manner, our method achieves superior morphing results on both images and videos for various benchmarks, including CLIP-inversion.</li>
</ul>

<h3>Title: PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology  Optimization</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Yuan, Haoyi Zhou, Tianyu Chen, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10547">https://arxiv.org/abs/2401.10547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10547">https://arxiv.org/pdf/2401.10547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10547]] PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology  Optimization(https://arxiv.org/abs/2401.10547)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>A multitude of toxic online behaviors, ranging from network attacks to anonymous traffic and spam, have severely disrupted the smooth operation of networks. Due to the inherent sender-receiver nature of network behaviors, graph-based frameworks are commonly used for detecting anomalous behaviors. However, in real-world scenarios, the boundary between normal and anomalous behaviors tends to be ambiguous. The local heterophily of graphs interferes with the detection, and existing methods based on nodes or edges introduce unwanted noise into representation results, thereby impacting the effectiveness of detection. To address these issues, we propose PhoGAD, a graph-based anomaly detection framework. PhoGAD leverages persistent homology optimization to clarify behavioral boundaries. Building upon this, the weights of adjacent edges are designed to mitigate the effects of local heterophily. Subsequently, to tackle the noise problem, we conduct a formal analysis and propose a disentangled representation-based explicit embedding method, ultimately achieving anomaly behavior detection. Experiments on intrusion, traffic, and spam datasets verify that PhoGAD has surpassed the performance of state-of-the-art (SOTA) frameworks in detection efficacy. Notably, PhoGAD demonstrates robust detection even with diminished anomaly proportions, highlighting its applicability to real-world scenarios. The analysis of persistent homology demonstrates its effectiveness in capturing the topological structure formed by normal edge features. Additionally, ablation experiments validate the effectiveness of the innovative mechanisms integrated within PhoGAD.</li>
</ul>

<h3>Title: 3D Shape Completion on Unseen Categories:A Weakly-supervised Approach</h3>
<ul>
<li><strong>Authors: </strong>Lintai Wu, Junhui Hou, Linqi Song, Yong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10578">https://arxiv.org/abs/2401.10578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10578">https://arxiv.org/pdf/2401.10578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10578]] 3D Shape Completion on Unseen Categories:A Weakly-supervised Approach(https://arxiv.org/abs/2401.10578)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>3D shapes captured by scanning devices are often incomplete due to occlusion. 3D shape completion methods have been explored to tackle this limitation. However, most of these methods are only trained and tested on a subset of categories, resulting in poor generalization to unseen categories. In this paper, we introduce a novel weakly-supervised framework to reconstruct the complete shapes from unseen categories. We first propose an end-to-end prior-assisted shape learning network that leverages data from the seen categories to infer a coarse shape. Specifically, we construct a prior bank consisting of representative shapes from the seen categories. Then, we design a multi-scale pattern correlation module for learning the complete shape of the input by analyzing the correlation between local patterns within the input and the priors at various scales. In addition, we propose a self-supervised shape refinement model to further refine the coarse shape. Considering the shape variability of 3D objects across categories, we construct a category-specific prior bank to facilitate shape refinement. Then, we devise a voxel-based partial matching loss and leverage the partial scans to drive the refinement process. Extensive experimental results show that our approach is superior to state-of-the-art methods by a large margin.</li>
</ul>

<h3>Title: Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion  Model</h3>
<ul>
<li><strong>Authors: </strong>Yinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10700">https://arxiv.org/abs/2401.10700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10700">https://arxiv.org/pdf/2401.10700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10700]] Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion  Model(https://arxiv.org/abs/2401.10700)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability. In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning. Thus, we propose a novel energy-guided diffusion model that does not require training a complicated time-dependent classifier to extract the policy, greatly simplifying the training. We compare FISOR against baselines on DSRL benchmark for safe offline RL. Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks.</li>
</ul>

<h3>Title: Sat2Scene: 3D Urban Scene Generation from Satellite Images with  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zuoyue Li, Zhenqiang Li, Zhaopeng Cui, Marc Pollefeys, Martin R. Oswald</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10786">https://arxiv.org/abs/2401.10786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10786">https://arxiv.org/pdf/2401.10786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10786]] Sat2Scene: 3D Urban Scene Generation from Satellite Images with  Diffusion(https://arxiv.org/abs/2401.10786)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Directly generating scenes from satellite imagery offers exciting possibilities for integration into applications like games and map services. However, challenges arise from significant view changes and scene scale. Previous efforts mainly focused on image or video generation, lacking exploration into the adaptability of scene generation for arbitrary views. Existing 3D generation works either operate at the object level or are difficult to utilize the geometry obtained from satellite imagery. To overcome these limitations, we propose a novel architecture for direct 3D scene generation by introducing diffusion models into 3D sparse representations and combining them with neural rendering techniques. Specifically, our approach generates texture colors at the point level for a given geometry using a 3D diffusion model first, which is then transformed into a scene representation in a feed-forward manner. The representation can be utilized to render arbitrary views which would excel in both single-frame quality and inter-frame consistency. Experiments in two city-scale datasets show that our model demonstrates proficiency in generating photo-realistic street-view image sequences and cross-view urban scenes from satellite imagery.</li>
</ul>

<h3>Title: ActAnywhere: Subject-Aware Video Background Generation</h3>
<ul>
<li><strong>Authors: </strong>Boxiao Pan, Zhan Xu, Chun-Hao Paul Huang, Krishna Kumar Singh, Yang Zhou, Leonidas J. Guibas, Jimei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10822">https://arxiv.org/abs/2401.10822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10822">https://arxiv.org/pdf/2401.10822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10822]] ActAnywhere: Subject-Aware Video Background Generation(https://arxiv.org/abs/2401.10822)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating video background that tailors to foreground subject motion is an important problem for the movie industry and visual effects community. This task involves synthesizing background that aligns with the motion and appearance of the foreground subject, while also complies with the artist's creative intention. We introduce ActAnywhere, a generative model that automates this process which traditionally requires tedious manual efforts. Our model leverages the power of large-scale video diffusion models, and is specifically tailored for this task. ActAnywhere takes a sequence of foreground subject segmentation as input and an image that describes the desired scene as condition, to produce a coherent video with realistic foreground-background interactions while adhering to the condition frame. We train our model on a large-scale dataset of human-scene interaction videos. Extensive evaluations demonstrate the superior performance of our model, significantly outperforming baselines. Moreover, we show that ActAnywhere generalizes to diverse out-of-distribution samples, including non-human subjects. Please visit our project webpage at https://actanywhere.github.io.</li>
</ul>

<h3>Title: Understanding Video Transformers via Universal Concept Discovery</h3>
<ul>
<li><strong>Authors: </strong>Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos G. Derpanis, Pavel Tokmakov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10831">https://arxiv.org/abs/2401.10831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10831">https://arxiv.org/pdf/2401.10831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10831]] Understanding Video Transformers via Universal Concept Discovery(https://arxiv.org/abs/2401.10831)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video models. Performing this analysis jointly over a diverse set of supervised and self-supervised representations, we discover that some of these mechanism are universal in video transformers. Finally, we demonstrate that VTCDcan be used to improve model performance for fine-grained tasks.</li>
</ul>

<h3>Title: Source-Free and Image-Only Unsupervised Domain Adaptation for Category  Level Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Prakhar Kaushik, Aayush Mishra, Adam Kortylewski, Alan Yuille</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10848">https://arxiv.org/abs/2401.10848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10848">https://arxiv.org/pdf/2401.10848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10848]] Source-Free and Image-Only Unsupervised Domain Adaptation for Category  Level Object Pose Estimation(https://arxiv.org/abs/2401.10848)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We consider the problem of source-free unsupervised category-level pose estimation from only RGB images to a target domain without any access to source domain data or 3D annotations during adaptation. Collecting and annotating real-world 3D data and corresponding images is laborious, expensive, yet unavoidable process, since even 3D pose domain adaptation methods require 3D data in the target domain. We introduce 3DUDA, a method capable of adapting to a nuisance-ridden target domain without 3D or depth data. Our key insight stems from the observation that specific object subparts remain stable across out-of-domain (OOD) scenarios, enabling strategic utilization of these invariant subcomponents for effective model updates. We represent object categories as simple cuboid meshes, and harness a generative model of neural feature activations modeled at each mesh vertex learnt using differential rendering. We focus on individual locally robust mesh vertex features and iteratively update them based on their proximity to corresponding features in the target domain even when the global pose is not correct. Our model is then trained in an EM fashion, alternating between updating the vertex features and the feature extractor. We show that our method simulates fine-tuning on a global pseudo-labeled dataset under mild assumptions, which converges to the target domain asymptotically. Through extensive empirical validation, including a complex extreme UDA setup which combines real nuisances, synthetic noise, and occlusion, we demonstrate the potency of our simple approach in addressing the domain shift challenge and significantly improving pose estimation accuracy.</li>
</ul>

<h3>Title: Synthesizing Moving People with 3D Control</h3>
<ul>
<li><strong>Authors: </strong>Boyi Li, Jathushan Rajasegaran, Yossi Gandelsman, Alexei A. Efros, Jitendra Malik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10889">https://arxiv.org/abs/2401.10889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10889">https://arxiv.org/pdf/2401.10889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10889]] Synthesizing Moving People with 3D Control(https://arxiv.org/abs/2401.10889)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to that, the 3D control allows various synthetic camera trajectories to render a person. Our experiments show that our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods. Please check our website for more details: https://boyiliee.github.io/3DHM.github.io/.</li>
</ul>

<h3>Title: Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</h3>
<ul>
<li><strong>Authors: </strong>Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10891">https://arxiv.org/abs/2401.10891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10891">https://arxiv.org/pdf/2401.10891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10891]] Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data(https://arxiv.org/abs/2401.10891)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This work presents Depth Anything, a highly practical solution for robust monocular depth estimation. Without pursuing novel technical modules, we aim to build a simple yet powerful foundation model dealing with any images under any circumstances. To this end, we scale up the dataset by designing a data engine to collect and automatically annotate large-scale unlabeled data (~62M), which significantly enlarges the data coverage and thus is able to reduce the generalization error. We investigate two simple yet effective strategies that make data scaling-up promising. First, a more challenging optimization target is created by leveraging data augmentation tools. It compels the model to actively seek extra visual knowledge and acquire robust representations. Second, an auxiliary supervision is developed to enforce the model to inherit rich semantic priors from pre-trained encoders. We evaluate its zero-shot capabilities extensively, including six public datasets and randomly captured photos. It demonstrates impressive generalization ability. Further, through fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs are set. Our better depth model also results in a better depth-conditioned ControlNet. Our models are released at https://github.com/LiheYoung/Depth-Anything.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
