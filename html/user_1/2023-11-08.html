<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: 3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion. (arXiv:2311.03742v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03742">http://arxiv.org/abs/2311.03742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03742]] 3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion(http://arxiv.org/abs/2311.03742)</code></li>
<li>Summary: <p>Good 3D object detection performance from LiDAR-Camera sensors demands
seamless feature alignment and fusion strategies. We propose the 3DifFusionDet
framework in this paper, which structures 3D object detection as a denoising
diffusion process from noisy 3D boxes to target boxes. In this framework,
ground truth boxes diffuse in a random distribution for training, and the model
learns to reverse the noising process. During inference, the model gradually
refines a set of boxes that were generated at random to the outcomes. Under the
feature align strategy, the progressive refinement method could make a
significant contribution to robust LiDAR-Camera fusion. The iterative
refinement process could also demonstrate great adaptability by applying the
framework to various detecting circumstances where varying levels of accuracy
and speed are required. Extensive experiments on KITTI, a benchmark for
real-world traffic object identification, revealed that 3DifFusionDet is able
to perform favorably in comparison to earlier, well-respected detectors.
</p></li>
</ul>

<h3>Title: Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models. (arXiv:2311.03830v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03830">http://arxiv.org/abs/2311.03830</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03830]] Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models(http://arxiv.org/abs/2311.03830)</code></li>
<li>Summary: <p>Denoising Diffusion models have exhibited remarkable capabilities in image
generation. However, generating high-quality samples requires a large number of
iterations. Knowledge distillation for diffusion models is an effective method
to address this limitation with a shortened sampling process but causes
degraded generative quality. Based on our analysis with bias-variance
decomposition and experimental observations, we attribute the degradation to
the spatial fitting error occurring in the training of both the teacher and
student model. Accordingly, we propose $\textbf{S}$patial
$\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction
$\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD utilizes attention
guidance from the teacher model and a designed semantic gradient predictor to
reduce the student's fitting error. Empirically, our proposed model facilitates
high-quality sample generation in a few function evaluations. We achieve an FID
of 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\times$64 with only one step,
outperforming existing diffusion methods. Our study provides a new perspective
on diffusion distillation by highlighting the intrinsic denoising ability of
models.
</p></li>
</ul>

<h3>Title: RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments. (arXiv:2311.03904v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03904">http://arxiv.org/abs/2311.03904</a></li>
<li>Code URL: https://github.com/ai-it-avs/robustmat</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03904]] RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments(http://arxiv.org/abs/2311.03904)</code></li>
<li>Summary: <p>For autonomous vehicles (AVs), visual perception techniques based on sensors
like cameras play crucial roles in information acquisition and processing. In
various computer perception tasks for AVs, it may be helpful to match landmark
patches taken by an onboard camera with other landmark patches captured at a
different time or saved in a street scene image database. To perform matching
under challenging driving environments caused by changing seasons, weather, and
illumination, we utilize the spatial neighborhood information of each patch. We
propose an approach, named RobustMat, which derives its robustness to
perturbations from neural differential equations. A convolutional neural ODE
diffusion module is used to learn the feature representation for the landmark
patches. A graph neural PDE diffusion module then aggregates information from
neighboring landmark patches in the street scene. Finally, feature similarity
learning outputs the final matching score. Our approach is evaluated on several
street scene datasets and demonstrated to achieve state-of-the-art matching
results under environmental perturbations.
</p></li>
</ul>

<h3>Title: Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning. (arXiv:2311.03756v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03756">http://arxiv.org/abs/2311.03756</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03756]] Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning(http://arxiv.org/abs/2311.03756)</code></li>
<li>Summary: <p>This paper considers optimal traffic signal control in smart cities, which
has been taken as a complex networked system control problem. Given the
interacting dynamics among traffic lights and road networks, attaining
controller adaptivity and scalability stands out as a primary challenge.
Capturing the spatial-temporal correlation among traffic lights under the
framework of Multi-Agent Reinforcement Learning (MARL) is a promising solution.
Nevertheless, existing MARL algorithms ignore effective information aggregation
which is fundamental for improving the learning capacity of decentralized
agents. In this paper, we design a new decentralized control architecture with
improved environmental observability to capture the spatial-temporal
correlation. Specifically, we first develop a topology-aware information
aggregation strategy to extract correlation-related information from
unstructured data gathered in the road network. Particularly, we transfer the
road network topology into a graph shift operator by forming a diffusion
process on the topology, which subsequently facilitates the construction of
graph signals. A diffusion convolution module is developed, forming a new MARL
algorithm, which endows agents with the capabilities of graph learning.
Extensive experiments based on both synthetic and real-world datasets verify
that our proposal outperforms existing decentralized algorithms.
</p></li>
</ul>

<h3>Title: Formulating Discrete Probability Flow Through Optimal Transport. (arXiv:2311.03886v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03886">http://arxiv.org/abs/2311.03886</a></li>
<li>Code URL: https://github.com/pangzecheung/discrete-probability-flow</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03886]] Formulating Discrete Probability Flow Through Optimal Transport(http://arxiv.org/abs/2311.03886)</code></li>
<li>Summary: <p>Continuous diffusion models are commonly acknowledged to display a
deterministic probability flow, whereas discrete diffusion models do not. In
this paper, we aim to establish the fundamental theory for the probability flow
of discrete diffusion models. Specifically, we first prove that the continuous
probability flow is the Monge optimal transport map under certain conditions,
and also present an equivalent evidence for discrete cases. In view of these
findings, we are then able to define the discrete probability flow in line with
the principles of optimal transport. Finally, drawing upon our newly
established definitions, we propose a novel sampling method that surpasses
previous discrete diffusion models in its ability to generate more certain
outcomes. Extensive experiments on the synthetic toy dataset and the CIFAR-10
dataset have validated the effectiveness of our proposed discrete probability
flow. Code is released at:
https://github.com/PangzeCheung/Discrete-Probability-Flow.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: A Simple and Efficient Baseline for Data Attribution on Images. (arXiv:2311.03386v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03386">http://arxiv.org/abs/2311.03386</a></li>
<li>Code URL: https://github.com/vasusingla/simple-data-attribution</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03386]] A Simple and Efficient Baseline for Data Attribution on Images(http://arxiv.org/abs/2311.03386)</code></li>
<li>Summary: <p>Data attribution methods play a crucial role in understanding machine
learning models, providing insight into which training data points are most
responsible for model outputs during deployment. However, current
state-of-the-art approaches require a large ensemble of as many as 300,000
models to accurately attribute model predictions. These approaches therefore
come at a high computational cost, are memory intensive, and are hard to scale
to large models or datasets. In this work, we focus on a minimalist baseline,
utilizing the feature space of a backbone pretrained via self-supervised
learning to perform data attribution. Our method is model-agnostic and scales
easily to large datasets. We show results on CIFAR-10 and ImageNet, achieving
strong performance that rivals or outperforms state-of-the-art approaches at a
fraction of the compute or memory cost. Contrary to prior work, our results
reinforce the intuition that a model's prediction on one image is most impacted
by visually similar training samples. Our approach serves as a simple and
efficient baseline for data attribution on images.
</p></li>
</ul>

<h3>Title: CycleCL: Self-supervised Learning for Periodic Videos. (arXiv:2311.03402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03402">http://arxiv.org/abs/2311.03402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03402]] CycleCL: Self-supervised Learning for Periodic Videos(http://arxiv.org/abs/2311.03402)</code></li>
<li>Summary: <p>Analyzing periodic video sequences is a key topic in applications such as
automatic production systems, remote sensing, medical applications, or physical
training. An example is counting repetitions of a physical exercise. Due to the
distinct characteristics of periodic data, self-supervised methods designed for
standard image datasets do not capture changes relevant to the progression of
the cycle and fail to ignore unrelated noise. They thus do not work well on
periodic data. In this paper, we propose CycleCL, a self-supervised learning
method specifically designed to work with periodic data. We start from the
insight that a good visual representation for periodic data should be sensitive
to the phase of a cycle, but be invariant to the exact repetition, i.e. it
should generate identical representations for a specific phase throughout all
repetitions. We exploit the repetitions in videos to design a novel contrastive
learning method based on a triplet loss that optimizes for these desired
properties. Our method uses pre-trained features to sample pairs of frames from
approximately the same phase and negative pairs of frames from different
phases. Then, we iterate between optimizing a feature encoder and resampling
triplets, until convergence. By optimizing a model this way, we are able to
learn features that have the mentioned desired properties. We evaluate CycleCL
on an industrial and multiple human actions datasets, where it significantly
outperforms previous video-based self-supervised learning methods on all tasks.
</p></li>
</ul>

<h3>Title: Random Field Augmentations for Self-Supervised Representation Learning. (arXiv:2311.03629v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03629">http://arxiv.org/abs/2311.03629</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03629]] Random Field Augmentations for Self-Supervised Representation Learning(http://arxiv.org/abs/2311.03629)</code></li>
<li>Summary: <p>Self-supervised representation learning is heavily dependent on data
augmentations to specify the invariances encoded in representations. Previous
work has shown that applying diverse data augmentations is crucial to
downstream performance, but augmentation techniques remain under-explored. In
this work, we propose a new family of local transformations based on Gaussian
random fields to generate image augmentations for self-supervised
representation learning. These transformations generalize the well-established
affine and color transformations (translation, rotation, color jitter, etc.)
and greatly increase the space of augmentations by allowing transformation
parameter values to vary from pixel to pixel. The parameters are treated as
continuous functions of spatial coordinates, and modeled as independent
Gaussian random fields. Empirical results show the effectiveness of the new
transformations for self-supervised representation learning. Specifically, we
achieve a 1.7% top-1 accuracy improvement over baseline on ImageNet downstream
classification, and a 3.6% improvement on out-of-distribution iNaturalist
downstream classification. However, due to the flexibility of the new
transformations, learned representations are sensitive to hyperparameters.
While mild transformations improve representations, we observe that strong
transformations can degrade the structure of an image, indicating that
balancing the diversity and strength of augmentations is important for
improving generalization of learned representations.
</p></li>
</ul>

<h3>Title: Image Generation and Learning Strategy for Deep Document Forgery Detection. (arXiv:2311.03650v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03650">http://arxiv.org/abs/2311.03650</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03650]] Image Generation and Learning Strategy for Deep Document Forgery Detection(http://arxiv.org/abs/2311.03650)</code></li>
<li>Summary: <p>In recent years, document processing has flourished and brought numerous
benefits. However, there has been a significant rise in reported cases of
forged document images. Specifically, recent advancements in deep neural
network (DNN) methods for generative tasks may amplify the threat of document
forgery. Traditional approaches for forged document images created by prevalent
copy-move methods are unsuitable against those created by DNN-based methods, as
we have verified. To address this issue, we construct a training dataset of
document forgery images, named FD-VIED, by emulating possible attacks, such as
text addition, removal, and replacement with recent DNN-methods. Additionally,
we introduce an effective pre-training approach through self-supervised
learning with both natural images and document images. In our experiments, we
demonstrate that our approach enhances detection performance.
</p></li>
</ul>

<h3>Title: Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization. (arXiv:2311.03785v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03785">http://arxiv.org/abs/2311.03785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03785]] Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization(http://arxiv.org/abs/2311.03785)</code></li>
<li>Summary: <p>Multimodal representation learning poses significant challenges in capturing
informative and distinct features from multiple modalities. Existing methods
often struggle to exploit the unique characteristics of each modality due to
unified multimodal annotations. In this study, we propose Self-MI in the
self-supervised learning fashion, which also leverage Contrastive Predictive
Coding (CPC) as an auxiliary technique to maximize the Mutual Information (MI)
between unimodal input pairs and the multimodal fusion result with unimodal
inputs. Moreover, we design a label generation module, $ULG_{MI}$ for short,
that enables us to create meaningful and informative labels for each modality
in a self-supervised manner. By maximizing the Mutual Information, we encourage
better alignment between the multimodal fusion and the individual modalities,
facilitating improved multimodal fusion. Extensive experiments on three
benchmark datasets including CMU-MOSI, CMU-MOSEI, and SIMS, demonstrate the
effectiveness of Self-MI in enhancing the multimodal fusion task.
</p></li>
</ul>

<h3>Title: SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations. (arXiv:2311.03651v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03651">http://arxiv.org/abs/2311.03651</a></li>
<li>Code URL: https://github.com/snuchankim/sero</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03651]] SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations(http://arxiv.org/abs/2311.03651)</code></li>
<li>Summary: <p>Robotic agents trained using reinforcement learning have the problem of
taking unreliable actions in an out-of-distribution (OOD) state. Agents can
easily become OOD in real-world environments because it is almost impossible
for them to visit and learn the entire state space during training.
Unfortunately, unreliable actions do not ensure that agents perform their
original tasks successfully. Therefore, agents should be able to recognize
whether they are in OOD states and learn how to return to the learned state
distribution rather than continue to take unreliable actions. In this study, we
propose a novel method for retraining agents to recover from OOD situations in
a self-supervised manner when they fall into OOD states. Our in-depth
experimental results demonstrate that our method substantially improves the
agent's ability to recover from OOD situations in terms of sample efficiency
and restoration of the performance for the original tasks. Moreover, we show
that our method can retrain the agent to recover from OOD situations even when
in-distribution states are difficult to visit through exploration.
</p></li>
</ul>

<h3>Title: PT-Tuning: Bridging the Gap between Time Series Masked Reconstruction and Forecasting via Prompt Token Tuning. (arXiv:2311.03768v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03768">http://arxiv.org/abs/2311.03768</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03768]] PT-Tuning: Bridging the Gap between Time Series Masked Reconstruction and Forecasting via Prompt Token Tuning(http://arxiv.org/abs/2311.03768)</code></li>
<li>Summary: <p>Self-supervised learning has been actively studied in time series domain
recently, especially for masked reconstruction. Most of these methods follow
the "Pre-training + Fine-tuning" paradigm in which a new decoder replaces the
pre-trained decoder to fit for a specific downstream task, leading to
inconsistency of upstream and downstream tasks. In this paper, we first point
out that the unification of task objectives and adaptation for task difficulty
are critical for bridging the gap between time series masked reconstruction and
forecasting. By reserving the pre-trained mask token during fine-tuning stage,
the forecasting task can be taken as a special case of masked reconstruction,
where the future values are masked and reconstructed based on history values.
It guarantees the consistency of task objectives but there is still a gap in
task difficulty. Because masked reconstruction can utilize contextual
information while forecasting can only use historical information to
reconstruct. To further mitigate the existed gap, we propose a simple yet
effective prompt token tuning (PT-Tuning) paradigm, in which all pre-trained
parameters are frozen and only a few trainable prompt tokens are added to
extended mask tokens in element-wise manner. Extensive experiments on
real-world datasets demonstrate the superiority of our proposed paradigm with
state-of-the-art performance compared to representation learning and end-to-end
supervised forecasting methods.
</p></li>
</ul>

<h3>Title: Learned Causal Method Prediction. (arXiv:2311.03989v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03989">http://arxiv.org/abs/2311.03989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03989]] Learned Causal Method Prediction(http://arxiv.org/abs/2311.03989)</code></li>
<li>Summary: <p>For a given causal question, it is important to efficiently decide which
causal inference method to use for a given dataset. This is challenging because
causal methods typically rely on complex and difficult-to-verify assumptions,
and cross-validation is not applicable since ground truth causal quantities are
unobserved.In this work, we propose CAusal Method Predictor (CAMP), a framework
for predicting the best method for a given dataset. To this end, we generate
datasets from a diverse set of synthetic causal models, score the candidate
methods, and train a model to directly predict the highest-scoring method for
that dataset. Next, by formulating a self-supervised pre-training objective
centered on dataset assumptions relevant for causal inference, we significantly
reduce the need for costly labeled data and enhance training efficiency. Our
strategy learns to map implicit dataset properties to the best method in a
data-driven manner. In our experiments, we focus on method prediction for
causal discovery. CAMP outperforms selecting any individual candidate method
and demonstrates promising generalization to unseen semi-synthetic and
real-world benchmarks.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models. (arXiv:2311.03799v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03799">http://arxiv.org/abs/2311.03799</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03799]] Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models(http://arxiv.org/abs/2311.03799)</code></li>
<li>Summary: <p>Human-object interaction (HOI) detection aims to comprehend the intricate
relationships between humans and objects, predicting $&lt;human, action, object&gt;$
triplets, and serving as the foundation for numerous computer vision tasks. The
complexity and diversity of human-object interactions in the real world,
however, pose significant challenges for both annotation and recognition,
particularly in recognizing interactions within an open world context. This
study explores the universal interaction recognition in an open-world setting
through the use of Vision-Language (VL) foundation models and large language
models (LLMs). The proposed method is dubbed as \emph{\textbf{UniHOI}}. We
conduct a deep analysis of the three hierarchical features inherent in visual
HOI detectors and propose a method for high-level relation extraction aimed at
VL foundation models, which we call HO prompt-based learning. Our design
includes an HO Prompt-guided Decoder (HOPD), facilitates the association of
high-level relation representations in the foundation model with various HO
pairs within the image. Furthermore, we utilize a LLM (\emph{i.e.} GPT) for
interaction interpretation, generating a richer linguistic understanding for
complex HOIs. For open-category interaction recognition, our method supports
either of two input types: interaction phrase or interpretive sentence. Our
efficient architecture design and learning methods effectively unleash the
potential of the VL foundation models and LLMs, allowing UniHOI to surpass all
existing methods with a substantial margin, under both supervised and zero-shot
settings. The code and pre-trained weights are available at:
\url{https://github.com/Caoyichao/UniHOI}.
</p></li>
</ul>

<h3>Title: Quantifying Uncertainty in Natural Language Explanations of Large Language Models. (arXiv:2311.03533v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03533">http://arxiv.org/abs/2311.03533</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03533]] Quantifying Uncertainty in Natural Language Explanations of Large Language Models(http://arxiv.org/abs/2311.03533)</code></li>
<li>Summary: <p>Large Language Models (LLMs) are increasingly used as powerful tools for
several high-stakes natural language processing (NLP) applications. Recent
prompting works claim to elicit intermediate reasoning steps and key tokens
that serve as proxy explanations for LLM predictions. However, there is no
certainty whether these explanations are reliable and reflect the LLMs
behavior. In this work, we make one of the first attempts at quantifying the
uncertainty in explanations of LLMs. To this end, we propose two novel metrics
-- $\textit{Verbalized Uncertainty}$ and $\textit{Probing Uncertainty}$ -- to
quantify the uncertainty of generated explanations. While verbalized
uncertainty involves prompting the LLM to express its confidence in its
explanations, probing uncertainty leverages sample and model perturbations as a
means to quantify the uncertainty. Our empirical analysis of benchmark datasets
reveals that verbalized uncertainty is not a reliable estimate of explanation
confidence. Further, we show that the probing uncertainty estimates are
correlated with the faithfulness of an explanation, with lower uncertainty
corresponding to explanations with higher faithfulness. Our study provides
insights into the challenges and opportunities of quantifying uncertainty in
LLM explanations, contributing to the broader discussion of the trustworthiness
of foundation models.
</p></li>
</ul>

<h3>Title: Neuro-GPT: Developing A Foundation Model for EEG. (arXiv:2311.03764v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03764">http://arxiv.org/abs/2311.03764</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03764]] Neuro-GPT: Developing A Foundation Model for EEG(http://arxiv.org/abs/2311.03764)</code></li>
<li>Summary: <p>To handle the scarcity and heterogeneity of electroencephalography (EEG) data
in Brain-Computer Interface (BCI) tasks, and to harness the vast public data,
we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT
model. The foundation model is pre-trained on a large-scale public EEG dataset,
using a self-supervised task which learns how to reconstruct the masked chunk
in EEG. We then fine-tune the foundation model on a Motor Imagery
Classification task where only 9 subjects are available. Experiments
demonstrated that applying foundation model can significantly improve
classification performance compared to the model trained from scratch, which
provides evidence for the advanced generalizability of foundation model and the
ability to address the challenges of data scarcity and heterogeneity.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: An attempt to generate new bridge types from latent space of variational autoencoder. (arXiv:2311.03380v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03380">http://arxiv.org/abs/2311.03380</a></li>
<li>Code URL: https://github.com/QQ583304953/Bridge-VAE</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03380]] An attempt to generate new bridge types from latent space of variational autoencoder(http://arxiv.org/abs/2311.03380)</code></li>
<li>Summary: <p>Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot.
</p></li>
</ul>

<h3>Title: DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries. (arXiv:2311.03725v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03725">http://arxiv.org/abs/2311.03725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03725]] DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries(http://arxiv.org/abs/2311.03725)</code></li>
<li>Summary: <p>Utilizing Convolutional Neural Networks (CNNs), Recurrent Neural Networks
(RNNs), and Generative Adversarial Networks (GANs), our system introduces an
innovative approach to defect detection in manufacturing. This technology
excels in precisely identifying faults by extracting intricate details from
product photographs, utilizing RNNs to detect evolving errors and generating
synthetic defect data to bolster the model's robustness and adaptability across
various defect scenarios. The project leverages a deep learning framework to
automate real-time flaw detection in the manufacturing process. It harnesses
extensive datasets of annotated images to discern complex defect patterns. This
integrated system seamlessly fits into production workflows, thereby boosting
efficiency and elevating product quality. As a result, it reduces waste and
operational costs, ultimately enhancing market competitiveness.
</p></li>
</ul>

<h3>Title: Unsupervised Video Summarization. (arXiv:2311.03745v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03745">http://arxiv.org/abs/2311.03745</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03745]] Unsupervised Video Summarization(http://arxiv.org/abs/2311.03745)</code></li>
<li>Summary: <p>This paper introduces a new, unsupervised method for automatic video
summarization using ideas from generative adversarial networks but eliminating
the discriminator, having a simple loss function, and separating training of
different parts of the model. An iterative training strategy is also applied by
alternately training the reconstructor and the frame selector for multiple
iterations. Furthermore, a trainable mask vector is added to the model in
summary generation during training and evaluation. The method also includes an
unsupervised model selection algorithm. Results from experiments on two public
datasets (SumMe and TVSum) and four datasets we created (Soccer, LoL, MLB, and
ShortMLB) demonstrate the effectiveness of each component on the model
performance, particularly the iterative training strategy. Evaluations and
comparisons with the state-of-the-art methods highlight the advantages of the
proposed method in performance, stability, and training efficiency.
</p></li>
</ul>

<h3>Title: SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation. (arXiv:2311.03866v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03866">http://arxiv.org/abs/2311.03866</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03866]] SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation(http://arxiv.org/abs/2311.03866)</code></li>
<li>Summary: <p>SCONE-GAN presents an end-to-end image translation, which is shown to be
effective for learning to generate realistic and diverse scenery images. Most
current image-to-image translation approaches are devised as two mappings: a
translation from the source to target domain and another to represent its
inverse. While successful in many applications, these approaches may suffer
from generating trivial solutions with limited diversity. That is because these
methods learn more frequent associations rather than the scene structures. To
mitigate the problem, we propose SCONE-GAN that utilises graph convolutional
networks to learn the objects dependencies, maintain the image structure and
preserve its semantics while transferring images into the target domain. For
more realistic and diverse image generation we introduce style reference image.
We enforce the model to maximize the mutual information between the style image
and output. The proposed method explicitly maximizes the mutual information
between the related patches, thus encouraging the generator to produce more
diverse images. We validate the proposed algorithm for image-to-image
translation and stylizing outdoor images. Both qualitative and quantitative
results demonstrate the effectiveness of our approach on four dataset.
</p></li>
</ul>

<h3>Title: Improving the Effectiveness of Deep Generative Data. (arXiv:2311.03959v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03959">http://arxiv.org/abs/2311.03959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03959]] Improving the Effectiveness of Deep Generative Data(http://arxiv.org/abs/2311.03959)</code></li>
<li>Summary: <p>Recent deep generative models (DGMs) such as generative adversarial networks
(GANs) and diffusion probabilistic models (DPMs) have shown their impressive
ability in generating high-fidelity photorealistic images. Although looking
appealing to human eyes, training a model on purely synthetic images for
downstream image processing tasks like image classification often results in an
undesired performance drop compared to training on real data. Previous works
have demonstrated that enhancing a real dataset with synthetic images from DGMs
can be beneficial. However, the improvements were subjected to certain
circumstances and yet were not comparable to adding the same number of real
images. In this work, we propose a new taxonomy to describe factors
contributing to this commonly observed phenomenon and investigate it on the
popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a
large portion of the performance drop when using synthetic images from DGM and
propose strategies to better utilize them in downstream tasks. Extensive
experiments on multiple datasets showcase that our method outperforms baselines
on downstream classification tasks both in case of training on synthetic only
(Synthetic-to-Real) and training on a mix of real and synthetic data (Data
Augmentation), particularly in the data-scarce scenario.
</p></li>
</ul>

<h3>Title: Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining. (arXiv:2311.03964v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03964">http://arxiv.org/abs/2311.03964</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03964]] Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining(http://arxiv.org/abs/2311.03964)</code></li>
<li>Summary: <p>Contemporary large-scale visual language models (VLMs) exhibit strong
representation capacities, making them ubiquitous for enhancing image and text
understanding tasks. They are often trained in a contrastive manner on a large
and diverse corpus of images and corresponding text captions scraped from the
internet. Despite this, VLMs often struggle with compositional reasoning tasks
which require a fine-grained understanding of the complex interactions of
objects and their attributes. This failure can be attributed to two main
factors: 1) Contrastive approaches have traditionally focused on mining
negative examples from existing datasets. However, the mined negative examples
might not be difficult for the model to discriminate from the positive. An
alternative to mining would be negative sample generation 2) But existing
generative approaches primarily focus on generating hard negative texts
associated with a given image. Mining in the other direction, i.e., generating
negative image samples associated with a given text has been ignored. To
overcome both these limitations, we propose a framework that not only mines in
both directions but also generates challenging negative samples in both
modalities, i.e., images and texts. Leveraging these generative hard negative
samples, we significantly enhance VLMs' performance in tasks involving
multimodal compositional reasoning. Our code and dataset are released at
https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.
</p></li>
</ul>

<h3>Title: Bias and Diversity in Synthetic-based Face Recognition. (arXiv:2311.03970v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03970">http://arxiv.org/abs/2311.03970</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03970]] Bias and Diversity in Synthetic-based Face Recognition(http://arxiv.org/abs/2311.03970)</code></li>
<li>Summary: <p>Synthetic data is emerging as a substitute for authentic data to solve
ethical and legal challenges in handling authentic face data. The current
models can create real-looking face images of people who do not exist. However,
it is a known and sensitive problem that face recognition systems are
susceptible to bias, i.e. performance differences between different demographic
and non-demographics attributes, which can lead to unfair decisions. In this
work, we investigate how the diversity of synthetic face recognition datasets
compares to authentic datasets, and how the distribution of the training data
of the generative models affects the distribution of the synthetic data. To do
this, we looked at the distribution of gender, ethnicity, age, and head
position. Furthermore, we investigated the concrete bias of three recent
synthetic-based face recognition models on the studied attributes in comparison
to a baseline model trained on authentic data. Our results show that the
generator generate a similar distribution as the used training data in terms of
the different attributes. With regard to bias, it can be seen that the
synthetic-based models share a similar bias behavior with the authentic-based
models. However, with the uncovered lower intra-identity attribute consistency
seems to be beneficial in reducing bias.
</p></li>
</ul>

<h3>Title: A Survey of Large Language Models Attribution. (arXiv:2311.03731v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03731">http://arxiv.org/abs/2311.03731</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03731]] A Survey of Large Language Models Attribution(http://arxiv.org/abs/2311.03731)</code></li>
<li>Summary: <p>Open-domain generative systems have gained significant attention in the field
of conversational AI (e.g., generative search engines). This paper presents a
comprehensive review of the attribution mechanisms employed by these systems,
particularly large language models. Though attribution or citation improve the
factuality and verifiability, issues like ambiguous knowledge reservoirs,
inherent biases, and the drawbacks of excessive attribution can hinder the
effectiveness of these systems. The aim of this survey is to provide valuable
insights for researchers, aiding in the refinement of attribution methodologies
to enhance the reliability and veracity of responses generated by open-domain
generative systems. We believe that this field is still in its early stages;
hence, we maintain a repository to keep track of ongoing studies at
https://github.com/HITsz-TMG/awesome-llm-attributions.
</p></li>
</ul>

<h3>Title: Training Multi-layer Neural Networks on Ising Machine. (arXiv:2311.03408v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03408">http://arxiv.org/abs/2311.03408</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03408]] Training Multi-layer Neural Networks on Ising Machine(http://arxiv.org/abs/2311.03408)</code></li>
<li>Summary: <p>As a dedicated quantum device, Ising machines could solve large-scale binary
optimization problems in milliseconds. There is emerging interest in utilizing
Ising machines to train feedforward neural networks due to the prosperity of
generative artificial intelligence. However, existing methods can only train
single-layer feedforward networks because of the complex nonlinear network
topology. This paper proposes an Ising learning algorithm to train quantized
neural network (QNN), by incorporating two essential techinques, namely binary
representation of topological network and order reduction of loss function. As
far as we know, this is the first algorithm to train multi-layer feedforward
networks on Ising machines, providing an alternative to gradient-based
backpropagation. Firstly, training QNN is formulated as a quadratic constrained
binary optimization (QCBO) problem by representing neuron connection and
activation function as equality constraints. All quantized variables are
encoded by binary bits based on binary encoding protocol. Secondly, QCBO is
converted to a quadratic unconstrained binary optimization (QUBO) problem, that
can be efficiently solved on Ising machines. The conversion leverages both
penalty function and Rosenberg order reduction, who together eliminate equality
constraints and reduce high-order loss function into a quadratic one. With some
assumptions, theoretical analysis shows the space complexity of our algorithm
is $\mathcal{O}(H^2L + HLN\log H)$, quantifying the required number of Ising
spins. Finally, the algorithm effectiveness is validated with a simulated Ising
machine on MNIST dataset. After annealing 700 ms, the classification accuracy
achieves 98.3%. Among 100 runs, the success probability of finding the optimal
solution is 72%. Along with the increasing number of spins on Ising machine,
our algorithm has the potential to train deeper neural networks.
</p></li>
</ul>

<h3>Title: A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle. (arXiv:2311.03414v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03414">http://arxiv.org/abs/2311.03414</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03414]] A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle(http://arxiv.org/abs/2311.03414)</code></li>
<li>Summary: <p>One of the most promising developments in computer vision in recent years is
the use of generative neural networks for functionality condition-based 3D
design reconstruction and generation. Here, neural networks learn dependencies
between functionalities and a geometry in a very effective way. For a neural
network the functionalities are translated in conditions to a certain geometry.
But the more conditions the design generation needs to reflect, the more
difficult it is to learn clear dependencies. This leads to a multi criteria
design problem due various conditions, which are not considered in the neural
network structure so far.
</p>
<p>In this paper, we address this multi-criteria challenge for a 3D design use
case related to an unmanned aerial vehicle (UAV) motor mount. We generate
10,000 abstract 3D designs and subject them all to simulations for three
physical disciplines: mechanics, thermodynamics, and aerodynamics. Then, we
train a Conditional Variational Autoencoder (CVAE) using the geometry and
corresponding multicriteria functional constraints as input. We use our trained
CVAE as well as the Marching cubes algorithm to generate meshes for simulation
based evaluation. The results are then evaluated with the generated UAV
designs. Subsequently, we demonstrate the ability to generate optimized designs
under self-defined functionality conditions using the trained neural network.
</p></li>
</ul>

<h2>anomaly</h2>
<h2>in-context</h2>
<h3>Title: Instruct Me More! Random Prompting for Visual In-Context Learning. (arXiv:2311.03648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03648">http://arxiv.org/abs/2311.03648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03648]] Instruct Me More! Random Prompting for Visual In-Context Learning(http://arxiv.org/abs/2311.03648)</code></li>
<li>Summary: <p>Large-scale models trained on extensive datasets, have emerged as the
preferred approach due to their high generalizability across various tasks.
In-context learning (ICL), a popular strategy in natural language processing,
uses such models for different tasks by providing instructive prompts but
without updating model parameters. This idea is now being explored in computer
vision, where an input-output image pair (called an in-context pair) is
supplied to the model with a query image as a prompt to exemplify the desired
output. The efficacy of visual ICL often depends on the quality of the prompts.
We thus introduce a method coined Instruct Me More (InMeMo), which augments
in-context pairs with a learnable perturbation (prompt), to explore its
potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the
current state-of-the-art performance. Specifically, compared to the baseline
without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for
foreground segmentation and single object detection tasks, respectively. Our
findings suggest that InMeMo offers a versatile and efficient way to enhance
the performance of visual ICL with lightweight training. Code is available at
https://github.com/Jackieam/InMeMo.
</p></li>
</ul>

<h3>Title: In-Context Exemplars as Clues to Retrieving from Large Associative Memory. (arXiv:2311.03498v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03498">http://arxiv.org/abs/2311.03498</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03498]] In-Context Exemplars as Clues to Retrieving from Large Associative Memory(http://arxiv.org/abs/2311.03498)</code></li>
<li>Summary: <p>Recently, large language models (LLMs) have made remarkable progress in
natural language processing. The most representative ability of LLMs is
in-context learning (ICL), which enables LLMs to learn patterns from in-context
exemplars without training. The performance of ICL greatly depends on the
exemplars used. However, how to choose exemplars remains unclear due to the
lack of understanding of how in-context learning works. In this paper, we
present a novel perspective on ICL by conceptualizing it as contextual
retrieval from a model of associative memory. We establish a theoretical
framework of ICL based on Hopfield Networks. Based on our framework, we look
into how in-context exemplars influence the performance of ICL and propose more
efficient active exemplar selection. Our study sheds new light on the mechanism
of ICL by connecting it to memory retrieval, with potential implications for
advancing the understanding of LLMs.
</p></li>
</ul>

<h3>Title: Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning. (arXiv:2311.03748v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03748">http://arxiv.org/abs/2311.03748</a></li>
<li>Code URL: https://github.com/psunlpgroup/fish-dip</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03748]] Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning(http://arxiv.org/abs/2311.03748)</code></li>
<li>Summary: <p>Unified Sequence Labeling that articulates different sequence labeling
problems such as Named Entity Recognition, Relation Extraction, Semantic Role
Labeling, etc. in a generalized sequence-to-sequence format opens up the
opportunity to make the maximum utilization of large language model knowledge
toward structured prediction. Unfortunately, this requires formatting them into
specialized augmented format unknown to the base pretrained language model
(PLMs) necessitating finetuning to the target format. This significantly bounds
its usefulness in data-limited settings where finetuning large models cannot
properly generalize to the target format. To address this challenge and
leverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic
sparse finetuning strategy that selectively focuses on a fraction of
parameters, informed by feedback from highly regressing examples, during the
fine-tuning process. By leveraging the dynamism of sparsity, our approach
mitigates the impact of well-learned samples and prioritizes underperforming
instances for improvement in generalization. Across five tasks of sequence
labeling, we demonstrate that FISH-DIP can smoothly optimize the model in low
resource settings offering upto 40% performance improvements over full
fine-tuning depending on target evaluation settings. Also, compared to
in-context learning and other parameter-efficient fine-tuning approaches,
FISH-DIP performs comparably or better, notably in extreme low-resource
settings.
</p></li>
</ul>

<h3>Title: Which is better? Exploring Prompting Strategy For LLM-based Metrics. (arXiv:2311.03754v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.03754">http://arxiv.org/abs/2311.03754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.03754]] Which is better? Exploring Prompting Strategy For LLM-based Metrics(http://arxiv.org/abs/2311.03754)</code></li>
<li>Summary: <p>This paper describes the DSBA submissions to the Prompting Large Language
Models as Explainable Metrics shared task, where systems were submitted to two
tracks: small and large summarization tracks. With advanced Large Language
Models (LLMs) such as GPT-4, evaluating the quality of Natural Language
Generation (NLG) has become increasingly paramount. Traditional
similarity-based metrics such as BLEU and ROUGE have shown to misalign with
human evaluation and are ill-suited for open-ended generation tasks. To address
this issue, we explore the potential capability of LLM-based metrics,
especially leveraging open-source LLMs. In this study, wide range of prompts
and prompting techniques are systematically analyzed with three approaches:
prompting strategy, score aggregation, and explainability. Our research focuses
on formulating effective prompt templates, determining the granularity of NLG
quality scores and assessing the impact of in-context examples on LLM-based
evaluation. Furthermore, three aggregation strategies are compared to identify
the most reliable method for aggregating NLG quality scores. To examine
explainability, we devise a strategy that generates rationales for the scores
and analyzes the characteristics of the explanation produced by the open-source
LLMs. Extensive experiments provide insights regarding evaluation capabilities
of open-source LLMs and suggest effective prompting strategies.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
