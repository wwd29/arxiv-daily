<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement. (arXiv:2309.11125v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11125">http://arxiv.org/abs/2309.11125</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11125]] PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement(http://arxiv.org/abs/2309.11125)</code></li>
<li>Summary: <p>Dominant Person Search methods aim to localize and recognize query persons in
a unified network, which jointly optimizes two sub-tasks, \ie, detection and
Re-IDentification (ReID). Despite significant progress, two major challenges
remain: 1) Detection-prior modules in previous methods are suboptimal for the
ReID task. 2) The collaboration between two sub-tasks is ignored. To alleviate
these issues, we present a novel Person Search framework based on the Diffusion
model, PSDiff. PSDiff formulates the person search as a dual denoising process
from noisy boxes and ReID embeddings to ground truths. Unlike existing methods
that follow the Detection-to-ReID paradigm, our denoising paradigm eliminates
detection-prior modules to avoid the local-optimum of the ReID task. Following
the new paradigm, we further design a new Collaborative Denoising Layer (CDL)
to optimize detection and ReID sub-tasks in an iterative and collaborative way,
which makes two sub-tasks mutually beneficial. Extensive experiments on the
standard benchmarks show that PSDiff achieves state-of-the-art performance with
fewer parameters and elastic computing overhead.
</p></li>
</ul>

<h3>Title: Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates. (arXiv:2309.11281v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11281">http://arxiv.org/abs/2309.11281</a></li>
<li>Code URL: https://github.com/kcshum/pose-conditioned-NeRF-object-fusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11281]] Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates(http://arxiv.org/abs/2309.11281)</code></li>
<li>Summary: <p>Neural radiance field is an emerging rendering method that generates
high-quality multi-view consistent images from a neural scene representation
and volume rendering. Although neural radiance field-based techniques are
robust for scene reconstruction, their ability to add or remove objects remains
limited. This paper proposes a new language-driven approach for object
manipulation with neural radiance fields through dataset updates. Specifically,
to insert a new foreground object represented by a set of multi-view images
into a background radiance field, we use a text-to-image diffusion model to
learn and generate combined images that fuse the object of interest into the
given background across views. These combined images are then used for refining
the background radiance field so that we can render view-consistent images
containing both the object and the background. To ensure view consistency, we
propose a dataset updates strategy that prioritizes radiance field training
with camera views close to the already-trained views prior to propagating the
training to remaining views. We show that under the same dataset updates
strategy, we can easily adapt our method for object insertion using data from
text-to-3D models as well as object removal. Experimental results show that our
method generates photorealistic images of the edited scenes, and outperforms
state-of-the-art methods in 3D reconstruction and neural radiance field
blending.
</p></li>
</ul>

<h3>Title: FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion. (arXiv:2309.11306v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11306">http://arxiv.org/abs/2309.11306</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11306]] FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion(http://arxiv.org/abs/2309.11306)</code></li>
<li>Summary: <p>Speech-driven 3D facial animation synthesis has been a challenging task both
in industry and research. Recent methods mostly focus on deterministic deep
learning methods meaning that given a speech input, the output is always the
same. However, in reality, the non-verbal facial cues that reside throughout
the face are non-deterministic in nature. In addition, majority of the
approaches focus on 3D vertex based datasets and methods that are compatible
with existing facial animation pipelines with rigged characters is scarce. To
eliminate these issues, we present FaceDiffuser, a non-deterministic deep
learning model to generate speech-driven facial animations that is trained with
both 3D vertex and blendshape based datasets. Our method is based on the
diffusion technique and uses the pre-trained large speech representation model
HuBERT to encode the audio input. To the best of our knowledge, we are the
first to employ the diffusion method for the task of speech-driven 3D facial
animation synthesis. We have run extensive objective and subjective analyses
and show that our approach achieves better or comparable results in comparison
to the state-of-the-art methods. We also introduce a new in-house dataset that
is based on a blendshape based rigged character. We recommend watching the
accompanying supplementary video. The code and the dataset will be publicly
available.
</p></li>
</ul>

<h3>Title: Face Aging via Diffusion-based Editing. (arXiv:2309.11321v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11321">http://arxiv.org/abs/2309.11321</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11321]] Face Aging via Diffusion-based Editing(http://arxiv.org/abs/2309.11321)</code></li>
<li>Summary: <p>In this paper, we address the problem of face aging: generating past or
future facial images by incorporating age-related changes to the given face.
Previous aging methods rely solely on human facial image datasets and are thus
constrained by their inherent scale and bias. This restricts their application
to a limited generatable age range and the inability to handle large age gaps.
We propose FADING, a novel approach to address Face Aging via DIffusion-based
editiNG. We go beyond existing methods by leveraging the rich prior of
large-scale language-image diffusion models. First, we specialize a pre-trained
diffusion model for the task of face age editing by using an age-aware
fine-tuning scheme. Next, we invert the input image to latent noise and obtain
optimized null text embeddings. Finally, we perform text-guided local age
editing via attention control. The quantitative and qualitative analyses
demonstrate that our method outperforms existing approaches with respect to
aging accuracy, attribute preservation, and aging quality.
</p></li>
</ul>

<h3>Title: FreeU: Free Lunch in Diffusion U-Net. (arXiv:2309.11497v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11497">http://arxiv.org/abs/2309.11497</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11497]] FreeU: Free Lunch in Diffusion U-Net(http://arxiv.org/abs/2309.11497)</code></li>
<li>Summary: <p>In this paper, we uncover the untapped potential of diffusion U-Net, which
serves as a "free lunch" that substantially improves the generation quality on
the fly. We initially investigate the key contributions of the U-Net
architecture to the denoising process and identify that its main backbone
primarily contributes to denoising, whereas its skip connections mainly
introduce high-frequency features into the decoder module, causing the network
to overlook the backbone semantics. Capitalizing on this discovery, we propose
a simple yet effective method-termed "FreeU" - that enhances generation quality
without additional training or finetuning. Our key insight is to strategically
re-weight the contributions sourced from the U-Net's skip connections and
backbone feature maps, to leverage the strengths of both components of the
U-Net architecture. Promising results on image and video generation tasks
demonstrate that our FreeU can be readily integrated to existing diffusion
models, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion,
to improve the generation quality with only a few lines of code. All you need
is to adjust two scaling factors during inference. Project page:
https://chenyangsi.top/FreeU/.
</p></li>
</ul>

<h3>Title: Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models. (arXiv:2309.11420v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11420">http://arxiv.org/abs/2309.11420</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11420]] Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models(http://arxiv.org/abs/2309.11420)</code></li>
<li>Summary: <p>We investigate the approximation efficiency of score functions by deep neural
networks in diffusion-based generative modeling. While existing approximation
theories utilize the smoothness of score functions, they suffer from the curse
of dimensionality for intrinsically high-dimensional data. This limitation is
pronounced in graphical models such as Markov random fields, common for image
distributions, where the approximation efficiency of score functions remains
unestablished.
</p>
<p>To address this, we observe score functions can often be well-approximated in
graphical models through variational inference denoising algorithms.
Furthermore, these algorithms are amenable to efficient neural network
representation. We demonstrate this in examples of graphical models, including
Ising models, conditional Ising models, restricted Boltzmann machines, and
sparse encoding models. Combined with off-the-shelf discretization error bounds
for diffusion-based sampling, we provide an efficient sample complexity bound
for diffusion-based generative modeling when the score function is learned by
deep neural networks.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics. (arXiv:2309.10972v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.10972">http://arxiv.org/abs/2309.10972</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.10972]] SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics(http://arxiv.org/abs/2309.10972)</code></li>
<li>Summary: <p>Accurately determining salient regions of an image is challenging when
labeled data is scarce. DINO-based self-supervised approaches have recently
leveraged meaningful image semantics captured by patch-wise features for
locating foreground objects. Recent methods have also incorporated intuitive
priors and demonstrated value in unsupervised methods for object partitioning.
In this paper, we propose SEMPART, which jointly infers coarse and fine
bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART
preserves fine boundary details using graph-driven regularization and
successfully distills the coarse mask semantics into the fine mask. Our salient
object detection and single object localization findings suggest that SEMPART
produces high-quality masks rapidly without additional post-processing and
benefits from co-optimizing the coarse and fine branches.
</p></li>
</ul>

<h3>Title: Weak Supervision for Label Efficient Visual Bug Detection. (arXiv:2309.11077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11077">http://arxiv.org/abs/2309.11077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11077]] Weak Supervision for Label Efficient Visual Bug Detection(http://arxiv.org/abs/2309.11077)</code></li>
<li>Summary: <p>As video games evolve into expansive, detailed worlds, visual quality becomes
essential, yet increasingly challenging. Traditional testing methods, limited
by resources, face difficulties in addressing the plethora of potential bugs.
Machine learning offers scalable solutions; however, heavy reliance on large
labeled datasets remains a constraint. Addressing this challenge, we propose a
novel method, utilizing unlabeled gameplay and domain-specific augmentations to
generate datasets &amp; self-supervised objectives used during pre-training or
multi-task settings for downstream visual bug detection. Our methodology uses
weak-supervision to scale datasets for the crafted objectives and facilitates
both autonomous and interactive weak-supervision, incorporating unsupervised
clustering and/or an interactive approach based on text and geometric prompts.
We demonstrate on first-person player clipping/collision bugs (FPPC) within the
expansive Giantmap game world, that our approach is very effective, improving
over a strong supervised baseline in a practical, very low-prevalence, low data
regime (0.336 $\rightarrow$ 0.550 F1 score). With just 5 labeled "good"
exemplars (i.e., 0 bugs), our self-supervised objective alone captures enough
signal to outperform the low-labeled supervised settings. Building on
large-pretrained vision models, our approach is adaptable across various visual
bugs. Our results suggest applicability in curating datasets for broader image
and video tasks within video games beyond visual bugs.
</p></li>
</ul>

<h3>Title: Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval. (arXiv:2309.11091v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11091">http://arxiv.org/abs/2309.11091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11091]] Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval(http://arxiv.org/abs/2309.11091)</code></li>
<li>Summary: <p>With the explosive growth of web videos in recent years, large-scale
Content-Based Video Retrieval (CBVR) becomes increasingly essential in video
filtering, recommendation, and copyright protection. Segment-level CBVR
(S-CBVR) locates the start and end time of similar segments in finer
granularity, which is beneficial for user browsing efficiency and infringement
detection especially in long video scenarios. The challenge of S-CBVR task is
how to achieve high temporal alignment accuracy with efficient computation and
low storage consumption. In this paper, we propose a Segment Similarity and
Alignment Network (SSAN) in dealing with the challenge which is firstly trained
end-to-end in S-CBVR. SSAN is based on two newly proposed modules in video
retrieval: (1) An efficient Self-supervised Keyframe Extraction (SKE) module to
reduce redundant frame features, (2) A robust Similarity Pattern Detection
(SPD) module for temporal alignment. In comparison with uniform frame
extraction, SKE not only saves feature storage and search time, but also
introduces comparable accuracy and limited extra computation time. In terms of
temporal alignment, SPD localizes similar segments with higher accuracy and
efficiency than existing deep learning methods. Furthermore, we jointly train
SSAN with SKE and SPD and achieve an end-to-end improvement. Meanwhile, the two
key modules SKE and SPD can also be effectively inserted into other video
retrieval pipelines and gain considerable performance improvements.
Experimental results on public datasets show that SSAN can obtain higher
alignment accuracy while saving storage and online query computational cost
compared to existing methods.
</p></li>
</ul>

<h3>Title: Self-supervised Domain-agnostic Domain Adaptation for Satellite Images. (arXiv:2309.11109v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11109">http://arxiv.org/abs/2309.11109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11109]] Self-supervised Domain-agnostic Domain Adaptation for Satellite Images(http://arxiv.org/abs/2309.11109)</code></li>
<li>Summary: <p>Domain shift caused by, e.g., different geographical regions or acquisition
conditions is a common issue in machine learning for global scale satellite
image processing. A promising method to address this problem is domain
adaptation, where the training and the testing datasets are split into two or
multiple domains according to their distributions, and an adaptation method is
applied to improve the generalizability of the model on the testing dataset.
However, defining the domain to which each satellite image belongs is not
trivial, especially under large-scale multi-temporal and multi-sensory
scenarios, where a single image mosaic could be generated from multiple data
sources. In this paper, we propose an self-supervised domain-agnostic domain
adaptation (SS(DA)2) method to perform domain adaptation without such a domain
definition. To achieve this, we first design a contrastive generative
adversarial loss to train a generative network to perform image-to-image
translation between any two satellite image patches. Then, we improve the
generalizability of the downstream models by augmenting the training data with
different testing spectral characteristics. The experimental results on public
benchmarks verify the effectiveness of SS(DA)2.
</p></li>
</ul>

<h3>Title: Self-supervised learning unveils change in urban housing from street-level images. (arXiv:2309.11354v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11354">http://arxiv.org/abs/2309.11354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11354]] Self-supervised learning unveils change in urban housing from street-level images(http://arxiv.org/abs/2309.11354)</code></li>
<li>Summary: <p>Cities around the world face a critical shortage of affordable and decent
housing. Despite its critical importance for policy, our ability to effectively
monitor and track progress in urban housing is limited. Deep learning-based
computer vision methods applied to street-level images have been successful in
the measurement of socioeconomic and environmental inequalities but did not
fully utilize temporal images to track urban change as time-varying labels are
often unavailable. We used self-supervised methods to measure change in London
using 15 million street images taken between 2008 and 2021. Our novel
adaptation of Barlow Twins, Street2Vec, embeds urban structure while being
invariant to seasonal and daily changes without manual annotations. It
outperformed generic embeddings, successfully identified point-level change in
London's housing supply from street-level images, and distinguished between
major and minor change. This capability can provide timely information for
urban planning and policy decisions toward more liveable, equitable, and
sustainable cities.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt. (arXiv:2309.11065v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11065">http://arxiv.org/abs/2309.11065</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11065]] UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt(http://arxiv.org/abs/2309.11065)</code></li>
<li>Summary: <p>Recent research has shown that multi-task pre-training greatly improves the
model's robustness and transfer ability, which is crucial for building a
high-quality dialog system. However, most previous works on multi-task
pre-training rely heavily on human-defined input format or prompt, which is not
optimal in quality and quantity. In this work, we propose to use Task-based
Automatic Prompt generation (TAP) to automatically generate high-quality
prompts. Using the high-quality prompts generated, we scale the corpus of the
pre-trained conversation model to 122 datasets from 15 dialog-related tasks,
resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful
foundation model for various conversational tasks and different dialog systems.
Extensive experiments have shown that UniPCM is robust to input prompts and
capable of various dialog-related tasks. Moreover, UniPCM has strong transfer
ability and excels at low resource scenarios, achieving SOTA results on 9
different datasets ranging from task-oriented dialog to open-domain
conversation. Furthermore, we are amazed to find that TAP can generate prompts
on par with those collected with crowdsourcing. The code is released with the
paper.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Score Mismatching for Generative Modeling. (arXiv:2309.11043v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11043">http://arxiv.org/abs/2309.11043</a></li>
<li>Code URL: https://github.com/senmaoy/Score-Mismatching</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11043]] Score Mismatching for Generative Modeling(http://arxiv.org/abs/2309.11043)</code></li>
<li>Summary: <p>We propose a new score-based model with one-step sampling. Previously,
score-based models were burdened with heavy computations due to iterative
sampling. For substituting the iterative process, we train a standalone
generator to compress all the time steps with the gradient backpropagated from
the score network. In order to produce meaningful gradients for the generator,
the score network is trained to simultaneously match the real data distribution
and mismatch the fake data distribution. This model has the following
advantages: 1) For sampling, it generates a fake image with only one step
forward. 2) For training, it only needs 10 diffusion steps.3) Compared with
consistency model, it is free of the ill-posed problem caused by consistency
loss. On the popular CIFAR-10 dataset, our model outperforms Consistency Model
and Denoising Score Matching, which demonstrates the potential of the
framework. We further provide more examples on the MINIST and LSUN datasets.
The code is available on GitHub.
</p></li>
</ul>

<h3>Title: Contrastive Pseudo Learning for Open-World DeepFake Attribution. (arXiv:2309.11132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11132">http://arxiv.org/abs/2309.11132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11132]] Contrastive Pseudo Learning for Open-World DeepFake Attribution(http://arxiv.org/abs/2309.11132)</code></li>
<li>Summary: <p>The challenge in sourcing attribution for forgery faces has gained widespread
attention due to the rapid development of generative techniques. While many
recent works have taken essential steps on GAN-generated faces, more
threatening attacks related to identity swapping or expression transferring are
still overlooked. And the forgery traces hidden in unknown attacks from the
open-world unlabeled faces still remain under-explored. To push the related
frontier research, we introduce a new benchmark called Open-World DeepFake
Attribution (OW-DFA), which aims to evaluate attribution performance against
various types of fake faces under open-world scenarios. Meanwhile, we propose a
novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task
through 1) introducing a Global-Local Voting module to guide the feature
alignment of forged faces with different manipulated regions, 2) designing a
Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused
by similar methods in unlabeled set. In addition, we extend the CPL framework
with a multi-stage paradigm that leverages pre-train technique and iterative
learning to further enhance traceability performance. Extensive experiments
verify the superiority of our proposed method on the OW-DFA and also
demonstrate the interpretability of deepfake attribution task and its impact on
improving the security of deepfake detection area.
</p></li>
</ul>

<h3>Title: DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11499">http://arxiv.org/abs/2309.11499</a></li>
<li>Code URL: https://github.com/RunpeiDong/DreamLLM</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11499]] DreamLLM: Synergistic Multimodal Comprehension and Creation(http://arxiv.org/abs/2309.11499)</code></li>
<li>Summary: <p>This paper presents DreamLLM, a learning framework that first achieves
versatile Multimodal Large Language Models (MLLMs) empowered with frequently
overlooked synergy between multimodal comprehension and creation. DreamLLM
operates on two fundamental principles. The first focuses on the generative
modeling of both language and image posteriors by direct sampling in the raw
multimodal space. This approach circumvents the limitations and information
loss inherent to external feature extractors like CLIP, and a more thorough
multimodal understanding is obtained. Second, DreamLLM fosters the generation
of raw, interleaved documents, modeling both text and image contents, along
with unstructured layouts. This allows DreamLLM to learn all conditional,
marginal, and joint multimodal distributions effectively. As a result, DreamLLM
is the first MLLM capable of generating free-form interleaved content.
Comprehensive experiments highlight DreamLLM's superior performance as a
zero-shot multimodal generalist, reaping from the enhanced learning synergy.
</p></li>
</ul>

<h3>Title: Benchmarks for Pir\'a 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change. (arXiv:2309.10945v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.10945">http://arxiv.org/abs/2309.10945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.10945]] Benchmarks for Pir\'a 2(http://arxiv.org/abs/2309.10945)</code></li>
<li>Summary: <p>Pir\'a is a reading comprehension dataset focused on the ocean, the Brazilian
coast, and climate change, built from a collection of scientific abstracts and
reports on these topics. This dataset represents a versatile language resource,
particularly useful for testing the ability of current machine learning models
to acquire expert scientific knowledge. Despite its potential, a detailed set
of baselines has not yet been developed for Pir\'a. By creating these
baselines, researchers can more easily utilize Pir\'a as a resource for testing
machine learning models across a wide range of question answering tasks. In
this paper, we define six benchmarks over the Pir\'a dataset, covering closed
generative question answering, machine reading comprehension, information
retrieval, open question answering, answer triggering, and multiple choice
question answering. As part of this effort, we have also produced a curated
version of the original dataset, where we fixed a number of grammar issues,
repetitions, and other shortcomings. Furthermore, the dataset has been extended
in several new directions, so as to face the aforementioned benchmarks:
translation of supporting texts from English into Portuguese, classification
labels for answerability, automatic paraphrases of questions and answers, and
multiple choice candidates. The results described in this paper provide several
points of reference for researchers interested in exploring the challenges
provided by the Pir\'a dataset.
</p></li>
</ul>

<h3>Title: Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters. (arXiv:2309.11042v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11042">http://arxiv.org/abs/2309.11042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11042]] Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters(http://arxiv.org/abs/2309.11042)</code></li>
<li>Summary: <p>Recently, Large Language Models (LLMs) have achieved amazing zero-shot
learning performance over a variety of Natural Language Processing (NLP) tasks,
especially for text generative tasks. Yet, the large size of LLMs often leads
to the high computational cost of model training and online deployment. In our
work, we present ALTER, a system that effectively builds the multi-tAsk
Learners with mixTure-of-task-adaptERs upon small language models (with &lt;1B
parameters) to address multiple NLP tasks simultaneously, capturing the
commonalities and differences between tasks, in order to support
domain-specific applications. Specifically, in ALTER, we propose the
Mixture-of-Task-Adapters (MTA) module as an extension to the transformer
architecture for the underlying model to capture the intra-task and inter-task
knowledge. A two-stage training method is further proposed to optimize the
collaboration between adapters at a small computational cost. Experimental
results over a mixture of NLP tasks show that our proposed MTA architecture and
the two-stage training method achieve good performance. Based on ALTER, we have
also produced MTA-equipped language models for various domains.
</p></li>
</ul>

<h3>Title: Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables. (arXiv:2309.11049v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11049">http://arxiv.org/abs/2309.11049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11049]] Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables(http://arxiv.org/abs/2309.11049)</code></li>
<li>Summary: <p>Question answering on tabular data (TableQA), which aims at generating
answers to questions grounded on a given table, has attracted increasing
attention in recent years. Existing work tends to generate factual short-form
answers by extracting information from one or a few table cells without
reasoning over selected table cells. However, the free-form TableQA, requiring
a more complex relevant table cell selection strategy and the complex
integration and inference of separate pieces of information, has been
under-explored. To this end, this paper proposes a generalized three-stage
approach: Table-to-Graph conversion and cell localizing, external knowledge
retrieval and table-text fusion (called TAG-QA), addressing the challenge of
inferring long free-form answer for generative TableQA. In particular, TAG-QA
(1) locates relevant table cells using a graph neural network to gather
intersecting cells between relevant rows and columns; (2) leverages external
knowledge from Wikipedia and (3) generates answers by integrating both tabular
data and natural linguistic information. Experiments with a human evaluation
demonstrate that TAG-QA is capable of generating more faithful and coherent
sentence when compared with several state-of-the-art baselines. Especially,
TAG-QA outperforms the strong pipeline-based baseline TAPAS by 17% and 14%, in
terms of BLEU-4 and PARENT F-score, respectively. Moreover, TAG-QA outperforms
end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score.
</p></li>
</ul>

<h3>Title: Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11259">http://arxiv.org/abs/2309.11259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11259]] Sequence-to-Sequence Spanish Pre-trained Language Models(http://arxiv.org/abs/2309.11259)</code></li>
<li>Summary: <p>In recent years, substantial advancements in pre-trained language models have
paved the way for the development of numerous non-English language versions,
with a particular focus on encoder-only and decoder-only architectures. While
Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited
prowess in natural language understanding and generation, there remains a
scarcity of encoder-decoder models designed for sequence-to-sequence tasks
involving input-output pairs. This paper breaks new ground by introducing the
implementation and evaluation of renowned encoder-decoder architectures,
exclusively pre-trained on Spanish corpora. Specifically, we present Spanish
versions of BART, T5, and BERT2BERT-style models and subject them to a
comprehensive assessment across a diverse range of sequence-to-sequence tasks,
spanning summarization, rephrasing, and generative question answering. Our
findings underscore the competitive performance of all models, with BART and T5
emerging as top performers across all evaluated tasks. As an additional
contribution, we have made all models publicly available to the research
community, fostering future exploration and development in Spanish language
processing.
</p></li>
</ul>

<h3>Title: Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11044">http://arxiv.org/abs/2309.11044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11044]] Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion(http://arxiv.org/abs/2309.11044)</code></li>
<li>Summary: <p>Federated Learning (FL) is currently one of the most popular technologies in
the field of Artificial Intelligence (AI) due to its collaborative learning and
ability to preserve client privacy. However, it faces challenges such as
non-identically and non-independently distributed (non-IID) and data with
imbalanced labels among local clients. To address these limitations, the
research community has explored various approaches such as using local model
parameters, federated generative adversarial learning, and federated
representation learning. In our study, we propose a novel Clustered FedStack
framework based on the previously published Stacked Federated Learning
(FedStack) framework. The local clients send their model predictions and output
layer weights to a server, which then builds a robust global model. This global
model clusters the local clients based on their output layer weights using a
clustering mechanism. We adopt three clustering mechanisms, namely K-Means,
Agglomerative, and Gaussian Mixture Models, into the framework and evaluate
their performance. We use Bayesian Information Criterion (BIC) with the maximum
likelihood function to determine the number of clusters. The Clustered FedStack
models outperform baseline models with clustering mechanisms. To estimate the
convergence of our proposed framework, we use Cyclical learning rates.
</p></li>
</ul>

<h3>Title: Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.11427">http://arxiv.org/abs/2309.11427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.11427]] Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing(http://arxiv.org/abs/2309.11427)</code></li>
<li>Summary: <p>This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.10923">http://arxiv.org/abs/2309.10923</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.10923]] Semi-automatic staging area for high-quality structured data extraction from scientific literature(http://arxiv.org/abs/2309.10923)</code></li>
<li>Summary: <p>In this study, we propose a staging area for ingesting new superconductors'
experimental data in SuperCon that is machine-collected from scientific
articles. Our objective is to enhance the efficiency of updating SuperCon while
maintaining or enhancing the data quality. We present a semi-automatic staging
area driven by a workflow combining automatic and manual processes on the
extracted database. An anomaly detection automatic process aims to pre-screen
the collected data. Users can then manually correct any errors through a user
interface tailored to simplify the data verification on the original PDF
documents. Additionally, when a record is corrected, its raw data is collected
and utilised to improve machine learning models as training data. Evaluation
experiments demonstrate that our staging area significantly improves curation
quality. We compare the interface with the traditional manual approach of
reading PDF documents and recording information in an Excel document. Using the
interface boosts the precision and recall by 6% and 50%, respectively to an
average increase of 40% in F1-score.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.10954">http://arxiv.org/abs/2309.10954</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.10954]] In-Context Learning for Text Classification with Many Labels(http://arxiv.org/abs/2309.10954)</code></li>
<li>Summary: <p>In-context learning (ICL) using large language models for tasks with many
labels is challenging due to the limited context window, which makes it
difficult to fit a sufficient number of examples in the prompt. In this paper,
we use a pre-trained dense retrieval model to bypass this limitation, giving
the model only a partial view of the full label space for each inference call.
Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art
performance in few-shot settings for three common intent classification
datasets, with no finetuning. We also surpass fine-tuned performance on
fine-grained sentiment classification in certain cases. We analyze the
performance across number of in-context examples and different model scales,
showing that larger models are necessary to effectively and consistently make
use of larger context lengths for ICL. By running several ablations, we analyze
the model's use of: a) the similarity of the in-context examples to the current
input, b) the semantic content of the class names, and c) the correct
correspondence between examples and labels. We demonstrate that all three are
needed to varying degrees depending on the domain, contrary to certain recent
works.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
