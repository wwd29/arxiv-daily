<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Zero-Shot Object Counting with Language-Vision Models. (arXiv:2309.13097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13097">http://arxiv.org/abs/2309.13097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13097]] Zero-Shot Object Counting with Language-Vision Models(http://arxiv.org/abs/2309.13097)</code></li>
<li>Summary: <p>Class-agnostic object counting aims to count object instances of an arbitrary
class at test time. It is challenging but also enables many potential
applications. Current methods require human-annotated exemplars as inputs which
are often unavailable for novel categories, especially for autonomous systems.
Thus, we propose zero-shot object counting (ZSC), a new setting where only the
class name is available during test time. This obviates the need for human
annotators and enables automated operation. To perform ZSC, we propose finding
a few object crops from the input image and use them as counting exemplars. The
goal is to identify patches containing the objects of interest while also being
visually representative for all instances in the image. To do this, we first
construct class prototypes using large language-vision models, including CLIP
and Stable Diffusion, to select the patches containing the target objects.
Furthermore, we propose a ranking model that estimates the counting error of
each patch to select the most suitable exemplars for counting. Experimental
results on a recent class-agnostic counting dataset, FSC-147, validate the
effectiveness of our method.
</p></li>
</ul>

<h3>Title: GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER. (arXiv:2309.13274v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13274">http://arxiv.org/abs/2309.13274</a></li>
<li>Code URL: https://github.com/iva-mzsun/glober</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13274]] GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER(http://arxiv.org/abs/2309.13274)</code></li>
<li>Summary: <p>Video generation necessitates both global coherence and local realism. This
work presents a novel non-autoregressive method GLOBER, which first generates
global features to obtain comprehensive global guidance and then synthesizes
video frames based on the global features to generate coherent videos.
Specifically, we propose a video auto-encoder, where a video encoder encodes
videos into global features, and a video decoder, built on a diffusion model,
decodes the global features and synthesizes video frames in a
non-autoregressive manner. To achieve maximum flexibility, our video decoder
perceives temporal information through normalized frame indexes, which enables
it to synthesize arbitrary sub video clips with predetermined starting and
ending frame indexes. Moreover, a novel adversarial loss is introduced to
improve the global coherence and local realism between the synthesized video
frames. Finally, we employ a diffusion-based video generator to fit the global
features outputted by the video encoder for video generation. Extensive
experimental results demonstrate the effectiveness and efficiency of our
proposed method, and new state-of-the-art results have been achieved on
multiple benchmarks.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Understanding Calibration of Deep Neural Networks for Medical Image Classification. (arXiv:2309.13132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13132">http://arxiv.org/abs/2309.13132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13132]] Understanding Calibration of Deep Neural Networks for Medical Image Classification(http://arxiv.org/abs/2309.13132)</code></li>
<li>Summary: <p>In the field of medical image analysis, achieving high accuracy is not
enough; ensuring well-calibrated predictions is also crucial. Confidence scores
of a deep neural network play a pivotal role in explainability by providing
insights into the model's certainty, identifying cases that require attention,
and establishing trust in its predictions. Consequently, the significance of a
well-calibrated model becomes paramount in the medical imaging domain, where
accurate and reliable predictions are of utmost importance. While there has
been a significant effort towards training modern deep neural networks to
achieve high accuracy on medical imaging tasks, model calibration and factors
that affect it remain under-explored. To address this, we conducted a
comprehensive empirical study that explores model performance and calibration
under different training regimes. We considered fully supervised training,
which is the prevailing approach in the community, as well as rotation-based
self-supervised method with and without transfer learning, across various
datasets and architecture sizes. Multiple calibration metrics were employed to
gain a holistic understanding of model calibration. Our study reveals that
factors such as weight distributions and the similarity of learned
representations correlate with the calibration trends observed in the models.
Notably, models trained using rotation-based self-supervised pretrained regime
exhibit significantly better calibration while achieving comparable or even
superior performance compared to fully supervised models across different
medical imaging datasets. These findings shed light on the importance of model
calibration in medical image analysis and highlight the benefits of
incorporating self-supervised learning approach to improve both performance and
calibration.
</p></li>
</ul>

<h3>Title: Poster: Self-Supervised Quantization-Aware Knowledge Distillation. (arXiv:2309.13220v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13220">http://arxiv.org/abs/2309.13220</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13220]] Poster: Self-Supervised Quantization-Aware Knowledge Distillation(http://arxiv.org/abs/2309.13220)</code></li>
<li>Summary: <p>Quantization-aware training (QAT) starts with a pre-trained full-precision
model and performs quantization during retraining. However, existing QAT works
require supervision from the labels and they suffer from accuracy loss due to
reduced precision. To address these limitations, this paper proposes a novel
Self-Supervised Quantization-Aware Knowledge Distillation framework (SQAKD).
SQAKD first unifies the forward and backward dynamics of various quantization
functions and then reframes QAT as a co-optimization problem that
simultaneously minimizes the KL-Loss and the discretization error, in a
self-supervised manner. The evaluation shows that SQAKD significantly improves
the performance of various state-of-the-art QAT works. SQAKD establishes
stronger baselines and does not require extensive labeled training data,
potentially making state-of-the-art QAT research more accessible.
</p></li>
</ul>

<h3>Title: M$^3$CS: Multi-Target Masked Point Modeling with Learnable Codebook and Siamese Decoders. (arXiv:2309.13235v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13235">http://arxiv.org/abs/2309.13235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13235]] M$^3$CS: Multi-Target Masked Point Modeling with Learnable Codebook and Siamese Decoders(http://arxiv.org/abs/2309.13235)</code></li>
<li>Summary: <p>Masked point modeling has become a promising scheme of self-supervised
pre-training for point clouds. Existing methods reconstruct either the original
points or related features as the objective of pre-training. However,
considering the diversity of downstream tasks, it is necessary for the model to
have both low- and high-level representation modeling capabilities to capture
geometric details and semantic contexts during pre-training. To this end,
M$^3$CS is proposed to enable the model with the above abilities. Specifically,
with masked point cloud as input, M$^3$CS introduces two decoders to predict
masked representations and the original points simultaneously. While an extra
decoder doubles parameters for the decoding process and may lead to
overfitting, we propose siamese decoders to keep the amount of learnable
parameters unchanged. Further, we propose an online codebook projecting
continuous tokens into discrete ones before reconstructing masked points. In
such way, we can enforce the decoder to take effect through the combinations of
tokens rather than remembering each token. Comprehensive experiments show that
M$^3$CS achieves superior performance at both classification and segmentation
tasks, outperforming existing methods.
</p></li>
</ul>

<h3>Title: Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation. (arXiv:2309.13248v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13248">http://arxiv.org/abs/2309.13248</a></li>
<li>Code URL: https://github.com/kfan21/eoras</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13248]] Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation(http://arxiv.org/abs/2309.13248)</code></li>
<li>Summary: <p>Video amodal segmentation is a particularly challenging task in computer
vision, which requires to deduce the full shape of an object from the visible
parts of it. Recently, some studies have achieved promising performance by
using motion flow to integrate information across frames under a
self-supervised setting. However, motion flow has a clear limitation by the two
factors of moving cameras and object deformation. This paper presents a
rethinking to previous works. We particularly leverage the supervised signals
with object-centric representation in \textit{real-world scenarios}. The
underlying idea is the supervision signal of the specific object and the
features from different views can mutually benefit the deduction of the full
mask in any specific frame. We thus propose an Efficient object-centric
Representation amodal Segmentation (EoRaS). Specially, beyond solely relying on
supervision signals, we design a translation module to project image features
into the Bird's-Eye View (BEV), which introduces 3D information to improve
current feature quality. Furthermore, we propose a multi-view fusion layer
based temporal module which is equipped with a set of object slots and
interacts with features from different views by attention mechanism to fulfill
sufficient object representation completion. As a result, the full mask of the
object can be decoded from image features updated by object slots. Extensive
experiments on both real-world and synthetic benchmarks demonstrate the
superiority of our proposed method, achieving state-of-the-art performance. Our
code will be released at \url{https://github.com/kfan21/EoRaS}.
</p></li>
</ul>

<h3>Title: C$^2$VAE: Gaussian Copula-based VAE Differing Disentangled from Coupled Representations with Contrastive Posterior. (arXiv:2309.13303v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13303">http://arxiv.org/abs/2309.13303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13303]] C$^2$VAE: Gaussian Copula-based VAE Differing Disentangled from Coupled Representations with Contrastive Posterior(http://arxiv.org/abs/2309.13303)</code></li>
<li>Summary: <p>We present a self-supervised variational autoencoder (VAE) to jointly learn
disentangled and dependent hidden factors and then enhance disentangled
representation learning by a self-supervised classifier to eliminate coupled
representations in a contrastive manner. To this end, a Contrastive Copula VAE
(C$^2$VAE) is introduced without relying on prior knowledge about data in the
probabilistic principle and involving strong modeling assumptions on the
posterior in the neural architecture. C$^2$VAE simultaneously factorizes the
posterior (evidence lower bound, ELBO) with total correlation (TC)-driven
decomposition for learning factorized disentangled representations and extracts
the dependencies between hidden features by a neural Gaussian copula for copula
coupled representations. Then, a self-supervised contrastive classifier
differentiates the disentangled representations from the coupled
representations, where a contrastive loss regularizes this contrastive
classification together with the TC loss for eliminating entangled factors and
strengthening disentangled representations. C$^2$VAE demonstrates a strong
effect in enhancing disentangled representation learning. C$^2$VAE further
contributes to improved optimization addressing the TC-based VAE instability
and the trade-off between reconstruction and representation.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13160">http://arxiv.org/abs/2309.13160</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13160]] GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior(http://arxiv.org/abs/2309.13160)</code></li>
<li>Summary: <p>Variational Autoencoders (VAEs) have become a cornerstone in generative
modeling and representation learning within machine learning. This paper
explores a nuanced aspect of VAEs, focusing on interpreting the Kullback
Leibler (KL) Divergence, a critical component within the Evidence Lower Bound
(ELBO) that governs the trade-off between reconstruction accuracy and
regularization. While the KL Divergence enforces alignment between latent
variable distributions and a prior imposing a structure on the overall latent
space but leaves individual variable distributions unconstrained. The proposed
method redefines the ELBO with a mixture of Gaussians for the posterior
probability, introduces a regularization term to prevent variance collapse, and
employs a PatchGAN discriminator to enhance texture realism. Implementation
details involve ResNetV2 architectures for both the Encoder and Decoder. The
experiments demonstrate the ability to generate realistic faces, offering a
promising solution for enhancing VAE based generative models.
</p></li>
</ul>

<h3>Title: Flow Factorized Representation Learning. (arXiv:2309.13167v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13167">http://arxiv.org/abs/2309.13167</a></li>
<li>Code URL: https://github.com/kingjamessong/latent-flow</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13167]] Flow Factorized Representation Learning(http://arxiv.org/abs/2309.13167)</code></li>
<li>Summary: <p>A prominent goal of representation learning research is to achieve
representations which are factorized in a useful manner with respect to the
ground truth factors of variation. The fields of disentangled and equivariant
representation learning have approached this ideal from a range of
complimentary perspectives; however, to date, most approaches have proven to
either be ill-specified or insufficiently flexible to effectively separate all
realistic factors of interest in a learned latent space. In this work, we
propose an alternative viewpoint on such structured representation learning
which we call Flow Factorized Representation Learning, and demonstrate it to
learn both more efficient and more usefully structured representations than
existing frameworks. Specifically, we introduce a generative model which
specifies a distinct set of latent probability paths that define different
input transformations. Each latent flow is generated by the gradient field of a
learned potential following dynamic optimal transport. Our novel setup brings
new understandings to both \textit{disentanglement} and \textit{equivariance}.
We show that our model achieves higher likelihoods on standard representation
learning benchmarks while simultaneously being closer to approximately
equivariant models. Furthermore, we demonstrate that the transformations
learned by our model are flexibly composable and can also extrapolate to new
data, implying a degree of robustness and generalizability approaching the
ultimate goal of usefully factorized representation learning.
</p></li>
</ul>

<h3>Title: MISFIT-V: Misaligned Image Synthesis and Fusion using Information from Thermal and Visual. (arXiv:2309.13216v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13216">http://arxiv.org/abs/2309.13216</a></li>
<li>Code URL: https://github.com/Aadharc/Visual_Thermal_Image_Fusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13216]] MISFIT-V: Misaligned Image Synthesis and Fusion using Information from Thermal and Visual(http://arxiv.org/abs/2309.13216)</code></li>
<li>Summary: <p>Detecting humans from airborne visual and thermal imagery is a fundamental
challenge for Wilderness Search-and-Rescue (WiSAR) teams, who must perform this
function accurately in the face of immense pressure. The ability to fuse these
two sensor modalities can potentially reduce the cognitive load on human
operators and/or improve the effectiveness of computer vision object detection
models. However, the fusion task is particularly challenging in the context of
WiSAR due to hardware limitations and extreme environmental factors. This work
presents Misaligned Image Synthesis and Fusion using Information from Thermal
and Visual (MISFIT-V), a novel two-pronged unsupervised deep learning approach
that utilizes a Generative Adversarial Network (GAN) and a cross-attention
mechanism to capture the most relevant features from each modality.
Experimental results show MISFIT-V offers enhanced robustness against
misalignment and poor lighting/thermal environmental conditions compared to
existing visual-thermal image fusion methods.
</p></li>
</ul>

<h3>Title: ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education. (arXiv:2309.13243v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13243">http://arxiv.org/abs/2309.13243</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13243]] ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education(http://arxiv.org/abs/2309.13243)</code></li>
<li>Summary: <p>The integration of generative AI in education is expanding, yet empirical
analyses of large-scale, real-world interactions between students and AI
systems still remain limited. In this study, we present ChEDDAR, ChatGPT &amp; EFL
Learner's Dialogue Dataset As Revising an essay, which is collected from a
semester-long longitudinal experiment involving 212 college students enrolled
in English as Foreign Langauge (EFL) writing courses. The students were asked
to revise their essays through dialogues with ChatGPT. ChEDDAR includes a
conversation log, utterance-level essay edit history, self-rated satisfaction,
and students' intent, in addition to session-level pre-and-post surveys
documenting their objectives and overall experiences. We analyze students'
usage patterns and perceptions regarding generative AI with respect to their
intent and satisfaction. As a foundational step, we establish baseline results
for two pivotal tasks in task-oriented dialogue systems within educational
contexts: intent detection and satisfaction estimation. We finally suggest
further research to refine the integration of generative AI into education
settings, outlining potential scenarios utilizing ChEDDAR. ChEDDAR is publicly
available at https://github.com/zeunie/ChEDDAR.
</p></li>
</ul>

<h3>Title: Beyond Fairness: Age-Harmless Parkinson's Detection via Voice. (arXiv:2309.13292v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13292">http://arxiv.org/abs/2309.13292</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13292]] Beyond Fairness: Age-Harmless Parkinson's Detection via Voice(http://arxiv.org/abs/2309.13292)</code></li>
<li>Summary: <p>Parkinson's disease (PD), a neurodegenerative disorder, often manifests as
speech and voice dysfunction. While utilizing voice data for PD detection has
great potential in clinical applications, the widely used deep learning models
currently have fairness issues regarding different ages of onset. These deep
models perform well for the elderly group (age $&gt;$ 55) but are less accurate
for the young group (age $\leq$ 55). Through our investigation, the discrepancy
between the elderly and the young arises due to 1) an imbalanced dataset and 2)
the milder symptoms often seen in early-onset patients. However, traditional
debiasing methods are impractical as they typically impair the prediction
accuracy for the majority group while minimizing the discrepancy. To address
this issue, we present a new debiasing method using GradCAM-based feature
masking combined with ensemble models, ensuring that neither fairness nor
accuracy is compromised. Specifically, the GradCAM-based feature masking
selectively obscures age-related features in the input voice data while
preserving essential information for PD detection. The ensemble models further
improve the prediction accuracy for the minority (young group). Our approach
effectively improves detection accuracy for early-onset patients without
sacrificing performance for the elderly group. Additionally, we propose a
two-step detection strategy for the young group, offering a practical risk
assessment for potential early-onset PD patients.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Real3D-AD: A Dataset of Point Cloud Anomaly Detection. (arXiv:2309.13226v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13226">http://arxiv.org/abs/2309.13226</a></li>
<li>Code URL: https://github.com/m-3lab/real3d-ad</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13226]] Real3D-AD: A Dataset of Point Cloud Anomaly Detection(http://arxiv.org/abs/2309.13226)</code></li>
<li>Summary: <p>High-precision point cloud anomaly detection is the gold standard for
identifying the defects of advancing machining and precision manufacturing.
Despite some methodological advances in this area, the scarcity of datasets and
the lack of a systematic benchmark hinder its development. We introduce
Real3D-AD, a challenging high-precision point cloud anomaly detection dataset,
addressing the limitations in the field. With 1,254 high-resolution 3D items
(\xgy{from forty thousand to millions of points for each item}), Real3D-AD is
the largest dataset for high-precision 3D industrial anomaly detection to date.
Real3D-AD surpasses existing 3D anomaly detection datasets available regarding
point cloud resolution (0.0010mm-0.0015mm), $360^{\circ}$ degree coverage and
perfect prototype. Additionally, we present a comprehensive benchmark for
Real3D-AD, revealing the absence of baseline methods for high-precision point
cloud anomaly detection. To address this, we propose Reg3D-AD, a
registration-based 3D anomaly detection method incorporating a novel feature
memory bank that preserves local and global representations. Extensive
experiments on the Real3D-AD dataset highlight the effectiveness of Reg3D-AD.
For reproducibility and accessibility, we provide the Real3D-AD dataset,
benchmark source code, and Reg3D-AD on our
website:https://github.com/M-3LAB/Real3D-AD.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: A Practical Survey on Zero-shot Prompt Design for In-context Learning. (arXiv:2309.13205v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13205">http://arxiv.org/abs/2309.13205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13205]] A Practical Survey on Zero-shot Prompt Design for In-context Learning(http://arxiv.org/abs/2309.13205)</code></li>
<li>Summary: <p>The remarkable advancements in large language models (LLMs) have brought
about significant improvements in Natural Language Processing(NLP) tasks. This
paper presents a comprehensive review of in-context learning techniques,
focusing on different types of prompts, including discrete, continuous,
few-shot, and zero-shot, and their impact on LLM performance. We explore
various approaches to prompt design, such as manual design, optimization
algorithms, and evaluation methods, to optimize LLM performance across diverse
tasks. Our review covers key research studies in prompt engineering, discussing
their methodologies and contributions to the field. We also delve into the
challenges faced in evaluating prompt performance, given the absence of a
single "best" prompt and the importance of considering multiple metrics. In
conclusion, the paper highlights the critical role of prompt design in
harnessing the full potential of LLMs and provides insights into the
combination of manual design, optimization techniques, and rigorous evaluation
for more effective and efficient use of LLMs in various NLP tasks.
</p></li>
</ul>

<h3>Title: User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue. (arXiv:2309.13233v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13233">http://arxiv.org/abs/2309.13233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13233]] User Simulation with Large Language Models for Evaluating Task-Oriented Dialogue(http://arxiv.org/abs/2309.13233)</code></li>
<li>Summary: <p>One of the major impediments to the development of new task-oriented dialogue
(TOD) systems is the need for human evaluation at multiple stages and
iterations of the development process. In an effort to move toward automated
evaluation of TOD, we propose a novel user simulator built using recently
developed large pretrained language models (LLMs). In order to increase the
linguistic diversity of our system relative to the related previous work, we do
not fine-tune the LLMs used by our system on existing TOD datasets; rather we
use in-context learning to prompt the LLMs to generate robust and
linguistically diverse output with the goal of simulating the behavior of human
interlocutors. Unlike previous work, which sought to maximize goal success rate
(GSR) as the primary metric of simulator performance, our goal is a system
which achieves a GSR similar to that observed in human interactions with TOD
systems. Using this approach, our current simulator is effectively able to
interact with several TOD systems, especially on single-intent conversational
goals, while generating lexically and syntactically diverse output relative to
previous simulators that rely upon fine-tuned models. Finally, we collect a
Human2Bot dataset of humans interacting with the same TOD systems with which we
experimented in order to better quantify these achievements.
</p></li>
</ul>

<h3>Title: Calibrating LLM-Based Evaluator. (arXiv:2309.13308v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.13308">http://arxiv.org/abs/2309.13308</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.13308]] Calibrating LLM-Based Evaluator(http://arxiv.org/abs/2309.13308)</code></li>
<li>Summary: <p>Recent advancements in large language models (LLMs) on language modeling and
emergent capabilities make them a promising reference-free evaluator of natural
language generation quality, and a competent alternative to human evaluation.
However, hindered by the closed-source or high computational demand to host and
tune, there is a lack of practice to further calibrate an off-the-shelf
LLM-based evaluator towards better human alignment. In this work, we propose
AutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate
and align an LLM-based evaluator toward human preference. Instead of explicitly
modeling human preferences, we first implicitly encompass them within a set of
human labels. Then, an initial set of scoring criteria is drafted by the
language model itself, leveraging in-context learning on different few-shot
examples. To further calibrate this set of criteria, we select the best
performers and re-draft them with self-refinement. Our experiments on multiple
text quality evaluation datasets illustrate a significant improvement in
correlation with expert evaluation through calibration. Our comprehensive
qualitative analysis conveys insightful intuitions and observations on the
essence of effective scoring criteria.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
