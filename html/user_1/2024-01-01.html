<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-01</h1>
<h2>diffusion</h2>
<h3>Title: Leveraging Open-Vocabulary Diffusion to Camouflaged Instance Segmentation. (arXiv:2312.17505v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17505">http://arxiv.org/abs/2312.17505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17505]] Leveraging Open-Vocabulary Diffusion to Camouflaged Instance Segmentation(http://arxiv.org/abs/2312.17505)</code></li>
<li>Summary: <p>Text-to-image diffusion techniques have shown exceptional capability of
producing high-quality images from text descriptions. This indicates that there
exists a strong correlation between the visual and textual domains. In
addition, text-image discriminative models such as CLIP excel in image
labelling from text prompts, thanks to the rich and diverse information
available from open concepts. In this paper, we leverage these technical
advances to solve a challenging problem in computer vision: camouflaged
instance segmentation. Specifically, we propose a method built upon a
state-of-the-art diffusion model, empowered by open-vocabulary to learn
multi-scale textual-visual features for camouflaged object representations.
Such cross-domain representations are desirable in segmenting camouflaged
objects where visual cues are subtle to distinguish the objects from the
background, especially in segmenting novel objects which are not seen in
training. We also develop technically supportive components to effectively fuse
cross-domain features and engage relevant features towards respective
foreground objects. We validate our method and compare it with existing ones on
several benchmark datasets of camouflaged instance segmentation and generic
open-vocabulary instance segmentation. Experimental results confirm the
advances of our method over existing ones. We will publish our code and
pre-trained models to support future research.
</p></li>
</ul>

<h3>Title: FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis. (arXiv:2312.17681v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17681">http://arxiv.org/abs/2312.17681</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17681]] FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis(http://arxiv.org/abs/2312.17681)</code></li>
<li>Summary: <p>Diffusion models have transformed the image-to-image (I2I) synthesis and are
now permeating into videos. However, the advancement of video-to-video (V2V)
synthesis has been hampered by the challenge of maintaining temporal
consistency across video frames. This paper proposes a consistent V2V synthesis
framework by jointly leveraging spatial conditions and temporal optical flow
clues within the source video. Contrary to prior methods that strictly adhere
to optical flow, our approach harnesses its benefits while handling the
imperfection in flow estimation. We encode the optical flow via warping from
the first frame and serve it as a supplementary reference in the diffusion
model. This enables our model for video synthesis by editing the first frame
with any prevalent I2I models and then propagating edits to successive frames.
Our V2V model, FlowVid, demonstrates remarkable properties: (1) Flexibility:
FlowVid works seamlessly with existing I2I models, facilitating various
modifications, including stylization, object swaps, and local edits. (2)
Efficiency: Generation of a 4-second video with 30 FPS and 512x512 resolution
takes only 1.5 minutes, which is 3.1x, 7.2x, and 10.5x faster than CoDeF,
Rerender, and TokenFlow, respectively. (3) High-quality: In user studies, our
FlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender
(10.2%), and TokenFlow (40.4%).
</p></li>
</ul>

<h3>Title: PINN surrogate of Li-ion battery models for parameter inference. Part II: Regularization and application of the pseudo-2D model. (arXiv:2312.17336v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17336">http://arxiv.org/abs/2312.17336</a></li>
<li>Code URL: <a href="https://github.com/nrel/pinnstripes">https://github.com/nrel/pinnstripes</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17336]] PINN surrogate of Li-ion battery models for parameter inference(http://arxiv.org/abs/2312.17336)</code></li>
<li>Summary: <p>Bayesian parameter inference is useful to improve Li-ion battery diagnostics
and can help formulate battery aging models. However, it is computationally
intensive and cannot be easily repeated for multiple cycles, multiple operating
conditions, or multiple replicate cells. To reduce the computational cost of
Bayesian calibration, numerical solvers for physics-based models can be
replaced with faster surrogates. A physics-informed neural network (PINN) is
developed as a surrogate for the pseudo-2D (P2D) battery model calibration. For
the P2D surrogate, additional training regularization was needed as compared to
the PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and
P2D surrogate models are exercised for parameter inference and compared to data
obtained from a direct numerical solution of the governing equations. A
parameter inference study highlights the ability to use these PINNs to
calibrate scaling parameters for the cathode Li diffusion and the anode
exchange current density. By realizing computational speed-ups of 2250x for the
P2D model, as compared to using standard integrating methods, the PINN
surrogates enable rapid state-of-health diagnostics. In the low-data
availability scenario, the testing error was estimated to 2mV for the SPM
surrogate and 10mV for the P2D surrogate which could be mitigated with
additional data.
</p></li>
</ul>

<h3>Title: Classifier-free graph diffusion for molecular property targeting. (arXiv:2312.17397v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17397">http://arxiv.org/abs/2312.17397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17397]] Classifier-free graph diffusion for molecular property targeting(http://arxiv.org/abs/2312.17397)</code></li>
<li>Summary: <p>This work focuses on the task of property targeting: that is, generating
molecules conditioned on target chemical properties to expedite candidate
screening for novel drug and materials development. DiGress is a recent
diffusion model for molecular graphs whose distinctive feature is allowing
property targeting through classifier-based (CB) guidance. While CB guidance
may work to generate molecular-like graphs, we hint at the fact that its
assumptions apply poorly to the chemical domain. Based on this insight we
propose a classifier-free DiGress (FreeGress), which works by directly
injecting the conditioning information into the training process. CF guidance
is convenient given its less stringent assumptions and since it does not
require to train an auxiliary property regressor, thus halving the number of
trainable parameters in the model. We empirically show that our model yields up
to 79% improvement in Mean Absolute Error with respect to DiGress on property
targeting tasks on QM9 and ZINC-250k benchmarks. As an additional contribution,
we propose a simple yet powerful approach to improve chemical validity of
generated samples, based on the observation that certain chemical properties
such as molecular weight correlate with the number of atoms in molecules.
</p></li>
</ul>

<h3>Title: Data Augmentation for Supervised Graph Outlier Detection with Latent Diffusion Models. (arXiv:2312.17679v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17679">http://arxiv.org/abs/2312.17679</a></li>
<li>Code URL: <a href="https://github.com/kayzliu/godm">https://github.com/kayzliu/godm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17679]] Data Augmentation for Supervised Graph Outlier Detection with Latent Diffusion Models(http://arxiv.org/abs/2312.17679)</code></li>
<li>Summary: <p>Graph outlier detection is a prominent task of research and application in
the realm of graph neural networks. It identifies the outlier nodes that
exhibit deviation from the majority in the graph. One of the fundamental
challenges confronting supervised graph outlier detection algorithms is the
prevalent issue of class imbalance, where the scarcity of outlier instances
compared to normal instances often results in suboptimal performance.
Conventional methods mitigate the imbalance by reweighting instances in the
estimation of the loss function, assigning higher weights to outliers and lower
weights to inliers. Nonetheless, these strategies are prone to overfitting and
underfitting, respectively. Recently, generative models, especially diffusion
models, have demonstrated their efficacy in synthesizing high-fidelity images.
Despite their extraordinary generation quality, their potential in data
augmentation for supervised graph outlier detection remains largely
underexplored.
</p>
<p>To bridge this gap, we introduce GODM, a novel data augmentation for
mitigating class imbalance in supervised Graph Outlier detection with latent
Diffusion Models. Specifically, our proposed method consists of three key
components: (1) Variantioanl Encoder maps the heterogeneous information
inherent within the graph data into a unified latent space. (2) Graph Generator
synthesizes graph data that are statistically similar to real outliers from
latent space, and (3) Latent Diffusion Model learns the latent space
distribution of real organic data by iterative denoising. Extensive experiments
conducted on multiple datasets substantiate the effectiveness and efficiency of
GODM. The case study further demonstrated the generation quality of our
synthetic data. To foster accessibility and reproducibility, we encapsulate
GODM into a plug-and-play package and release it at the Python Package Index
(PyPI).
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: HEAP: Unsupervised Object Discovery and Localization with Contrastive Grouping. (arXiv:2312.17492v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17492">http://arxiv.org/abs/2312.17492</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17492]] HEAP: Unsupervised Object Discovery and Localization with Contrastive Grouping(http://arxiv.org/abs/2312.17492)</code></li>
<li>Summary: <p>Unsupervised object discovery and localization aims to detect or segment
objects in an image without any supervision. Recent efforts have demonstrated a
notable potential to identify salient foreground objects by utilizing
self-supervised transformer features. However, their scopes only build upon
patch-level features within an image, neglecting region/image-level and
cross-image relationships at a broader scale. Moreover, these methods cannot
differentiate various semantics from multiple instances. To address these
problems, we introduce Hierarchical mErging framework via contrAstive grouPing
(HEAP). Specifically, a novel lightweight head with cross-attention mechanism
is designed to adaptively group intra-image patches into semantically coherent
regions based on correlation among self-supervised features. Further, to ensure
the distinguishability among various regions, we introduce a region-level
contrastive clustering loss to pull closer similar regions across images. Also,
an image-level contrastive loss is present to push foreground and background
representations apart, with which foreground objects and background are
accordingly discovered. HEAP facilitates efficient hierarchical image
decomposition, which contributes to more accurate object discovery while also
enabling differentiation among objects of various classes. Extensive
experimental results on semantic segmentation retrieval, unsupervised object
discovery, and saliency detection tasks demonstrate that HEAP achieves
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: QGFace: Quality-Guided Joint Training For Mixed-Quality Face Recognition. (arXiv:2312.17494v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17494">http://arxiv.org/abs/2312.17494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17494]] QGFace: Quality-Guided Joint Training For Mixed-Quality Face Recognition(http://arxiv.org/abs/2312.17494)</code></li>
<li>Summary: <p>The quality of a face crop in an image is decided by many factors such as
camera resolution, distance, and illumination condition. This makes the
discrimination of face images with different qualities a challenging problem in
realistic applications. However, most existing approaches are designed
specifically for high-quality (HQ) or low-quality (LQ) images, and the
performances would degrade for the mixed-quality images. Besides, many methods
ask for pre-trained feature extractors or other auxiliary structures to support
the training and the evaluation. In this paper, we point out that the key to
better understand both the HQ and the LQ images simultaneously is to apply
different learning methods according to their qualities. We propose a novel
quality-guided joint training approach for mixed-quality face recognition,
which could simultaneously learn the images of different qualities with a
single encoder. Based on quality partition, classification-based method is
employed for HQ data learning. Meanwhile, for the LQ images which lack identity
information, we learn them with self-supervised image-image contrastive
learning. To effectively catch up the model update and improve the
discriminability of contrastive learning in our joint training scenario, we
further propose a proxy-updated real-time queue to compose the contrastive
pairs with features from the genuine encoder. Experiments on the low-quality
datasets SCface and Tinyface, the mixed-quality dataset IJB-B, and five
high-quality datasets demonstrate the effectiveness of our proposed approach in
recognizing face images of different qualities.
</p></li>
</ul>

<h3>Title: Learning Vision from Models Rivals Learning Vision from Data. (arXiv:2312.17742v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17742">http://arxiv.org/abs/2312.17742</a></li>
<li>Code URL: <a href="https://github.com/google-research/syn-rep-learn">https://github.com/google-research/syn-rep-learn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17742]] Learning Vision from Models Rivals Learning Vision from Data(http://arxiv.org/abs/2312.17742)</code></li>
<li>Summary: <p>We introduce SynCLR, a novel approach for learning visual representations
exclusively from synthetic images and synthetic captions, without any real
data. We synthesize a large dataset of image captions using LLMs, then use an
off-the-shelf text-to-image model to generate multiple images corresponding to
each synthetic caption. We perform visual representation learning on these
synthetic images via contrastive learning, treating images sharing the same
caption as positive pairs. The resulting representations transfer well to many
downstream tasks, competing favorably with other general-purpose visual
representation learners such as CLIP and DINO v2 in image classification tasks.
Furthermore, in dense prediction tasks such as semantic segmentation, SynCLR
outperforms previous self-supervised methods by a significant margin, e.g.,
improving over MAE and iBOT by 6.2 and 4.3 mIoU on ADE20k for ViT-B/16.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: MVPatch: More Vivid Patch for Adversarial Camouflaged Attacks on Object Detectors in the Physical World. (arXiv:2312.17431v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17431">http://arxiv.org/abs/2312.17431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17431]] MVPatch: More Vivid Patch for Adversarial Camouflaged Attacks on Object Detectors in the Physical World(http://arxiv.org/abs/2312.17431)</code></li>
<li>Summary: <p>Recent research has shown that adversarial patches can manipulate outputs
from object detection models. However, the conspicuous patterns on these
patches may draw more attention and raise suspicions among humans. Moreover,
existing works have primarily focused on the attack performance of individual
models and have neglected the generation of adversarial patches for ensemble
attacks on multiple object detection models. To tackle these concerns, we
propose a novel approach referred to as the More Vivid Patch (MVPatch), which
aims to improve the transferability and stealthiness of adversarial patches
while considering the limitations observed in prior paradigms, such as easy
identification and poor transferability. Our approach incorporates an attack
algorithm that decreases object confidence scores of multiple object detectors
by using the ensemble attack loss function, thereby enhancing the
transferability of adversarial patches. Additionally, we propose a lightweight
visual similarity measurement algorithm realized by the Compared Specified
Image Similarity (CSS) loss function, which allows for the generation of
natural and stealthy adversarial patches without the reliance on additional
generative models. Extensive experiments demonstrate that the proposed MVPatch
algorithm achieves superior attack transferability compared to similar
algorithms in both digital and physical domains, while also exhibiting a more
natural appearance. These findings emphasize the remarkable stealthiness and
transferability of the proposed MVPatch attack algorithm.
</p></li>
</ul>

<h3>Title: Distance Guided Generative Adversarial Network for Explainable Binary Classifications. (arXiv:2312.17538v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17538">http://arxiv.org/abs/2312.17538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17538]] Distance Guided Generative Adversarial Network for Explainable Binary Classifications(http://arxiv.org/abs/2312.17538)</code></li>
<li>Summary: <p>Despite the potential benefits of data augmentation for mitigating the data
insufficiency, traditional augmentation methods primarily rely on the prior
intra-domain knowledge. On the other hand, advanced generative adversarial
networks (GANs) generate inter-domain samples with limited variety. These
previous methods make limited contributions to describing the decision
boundaries for binary classification. In this paper, we propose a distance
guided GAN (DisGAN) which controls the variation degrees of generated samples
in the hyperplane space. Specifically, we instantiate the idea of DisGAN by
combining two ways. The first way is vertical distance GAN (VerDisGAN) where
the inter-domain generation is conditioned on the vertical distances. The
second way is horizontal distance GAN (HorDisGAN) where the intra-domain
generation is conditioned on the horizontal distances. Furthermore, VerDisGAN
can produce the class-specific regions by mapping the source images to the
hyperplane. Experimental results show that DisGAN consistently outperforms the
GAN-based augmentation methods with explainable binary classification. The
proposed method can apply to different classification architectures and has
potential to extend to multi-class classification.
</p></li>
</ul>

<h3>Title: P2M2-Net: Part-Aware Prompt-Guided Multimodal Point Cloud Completion. (arXiv:2312.17611v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17611">http://arxiv.org/abs/2312.17611</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17611]] P2M2-Net: Part-Aware Prompt-Guided Multimodal Point Cloud Completion(http://arxiv.org/abs/2312.17611)</code></li>
<li>Summary: <p>Inferring missing regions from severely occluded point clouds is highly
challenging. Especially for 3D shapes with rich geometry and structure details,
inherent ambiguities of the unknown parts are existing. Existing approaches
either learn a one-to-one mapping in a supervised manner or train a generative
model to synthesize the missing points for the completion of 3D point cloud
shapes. These methods, however, lack the controllability for the completion
process and the results are either deterministic or exhibiting uncontrolled
diversity. Inspired by the prompt-driven data generation and editing, we
propose a novel prompt-guided point cloud completion framework, coined
P2M2-Net, to enable more controllable and more diverse shape completion. Given
an input partial point cloud and a text prompt describing the part-aware
information such as semantics and structure of the missing region, our
Transformer-based completion network can efficiently fuse the multimodal
features and generate diverse results following the prompt guidance. We train
the P2M2-Net on a new large-scale PartNet-Prompt dataset and conduct extensive
experiments on two challenging shape completion benchmarks. Quantitative and
qualitative results show the efficacy of incorporating prompts for more
controllable part-aware point cloud completion and generation. Code and data
are available at https://github.com/JLU-ICL/P2M2-Net.
</p></li>
</ul>

<h3>Title: From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models. (arXiv:2312.17256v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17256">http://arxiv.org/abs/2312.17256</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17256]] From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models(http://arxiv.org/abs/2312.17256)</code></li>
<li>Summary: <p>Large language models (LLMs) are able to engage in natural-sounding
conversations with humans, showcasing unprecedented capabilities for
information retrieval and automated decision support. They have disrupted
human-technology interaction and the way businesses operate. However,
technologies based on generative artificial intelligence (GenAI) are known to
hallucinate, misinform, and display biases introduced by the massive datasets
on which they are trained. Existing research indicates that humans may
unconsciously internalize these biases, which can persist even after they stop
using the programs. This study explores the cultural self-perception of LLMs by
prompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from
the GLOBE project. The findings reveal that their cultural self-perception is
most closely aligned with the values of English-speaking countries and
countries characterized by sustained economic competitiveness. Recognizing the
cultural biases of LLMs and understanding how they work is crucial for all
members of society because one does not want the black box of artificial
intelligence to perpetuate bias in humans, who might, in turn, inadvertently
create and train even more biased algorithms.
</p></li>
</ul>

<h3>Title: PanGu-$\pi$: Enhancing Language Model Architectures via Nonlinearity Compensation. (arXiv:2312.17276v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17276">http://arxiv.org/abs/2312.17276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17276]] PanGu-$\pi$: Enhancing Language Model Architectures via Nonlinearity Compensation(http://arxiv.org/abs/2312.17276)</code></li>
<li>Summary: <p>The recent trend of large language models (LLMs) is to increase the scale of
both model size (\aka the number of parameters) and dataset to achieve better
generative ability, which is definitely proved by a lot of work such as the
famous GPT and Llama. However, large models often involve massive computational
costs, and practical applications cannot afford such high prices. However, the
method of constructing a strong model architecture for LLMs is rarely
discussed. We first analyze the state-of-the-art language model architectures
and observe the feature collapse problem. Based on the theoretical analysis, we
propose that the nonlinearity is also very important for language models, which
is usually studied in convolutional neural networks for vision tasks. The
series informed activation function is then introduced with tiny calculations
that can be ignored, and an augmented shortcut is further used to enhance the
model nonlinearity. We then demonstrate that the proposed approach is
significantly effective for enhancing the model nonlinearity through carefully
designed ablations; thus, we present a new efficient model architecture for
establishing modern, namely, PanGu-$\pi$. Experiments are then conducted using
the same dataset and training strategy to compare PanGu-$\pi$ with
state-of-the-art LLMs. The results show that PanGu-$\pi$-7B can achieve a
comparable performance to that of benchmarks with about 10\% inference
speed-up, and PanGu-$\pi$-1B can achieve state-of-the-art performance in terms
of accuracy and efficiency. In addition, we have deployed PanGu-$\pi$-7B in the
high-value domains of finance and law, developing an LLM named YunShan for
practical application. The results show that YunShan can surpass other models
with similar scales on benchmarks.
</p></li>
</ul>

<h3>Title: AI Content Self-Detection for Transformer-based Large Language Models. (arXiv:2312.17289v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17289">http://arxiv.org/abs/2312.17289</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17289]] AI Content Self-Detection for Transformer-based Large Language Models(http://arxiv.org/abs/2312.17289)</code></li>
<li>Summary: <p>$ $The usage of generative artificial intelligence (AI) tools based on large
language models, including ChatGPT, Bard, and Claude, for text generation has
many exciting applications with the potential for phenomenal productivity
gains. One issue is authorship attribution when using AI tools. This is
especially important in an academic setting where the inappropriate use of
generative AI tools may hinder student learning or stifle research by creating
a large amount of automatically generated derivative work. Existing plagiarism
detection systems can trace the source of submitted text but are not yet
equipped with methods to accurately detect AI-generated text. This paper
introduces the idea of direct origin detection and evaluates whether generative
AI systems can recognize their output and distinguish it from human-written
texts. We argue why current transformer-based models may be able to self-detect
their own generated text and perform a small empirical study using zero-shot
learning to investigate if that is the case. Results reveal varying
capabilities of AI systems to identify their generated text. Google's Bard
model exhibits the largest capability of self-detection with an accuracy of
94\%, followed by OpenAI's ChatGPT with 83\%. On the other hand, Anthropic's
Claude model seems to be not able to self-detect.
</p></li>
</ul>

<h3>Title: Optimizing watermarks for large language models. (arXiv:2312.17295v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17295">http://arxiv.org/abs/2312.17295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17295]] Optimizing watermarks for large language models(http://arxiv.org/abs/2312.17295)</code></li>
<li>Summary: <p>With the rise of large language models (LLMs) and concerns about potential
misuse, watermarks for generative LLMs have recently attracted much attention.
An important aspect of such watermarks is the trade-off between their
identifiability and their impact on the quality of the generated text. This
paper introduces a systematic approach to this trade-off in terms of a
multi-objective optimization problem. For a large class of robust, efficient
watermarks, the associated Pareto optimal solutions are identified and shown to
outperform the currently default watermark.
</p></li>
</ul>

<h3>Title: EHR Interaction Between Patients and AI: NoteAid EHR Interaction. (arXiv:2312.17475v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17475">http://arxiv.org/abs/2312.17475</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17475]] EHR Interaction Between Patients and AI: NoteAid EHR Interaction(http://arxiv.org/abs/2312.17475)</code></li>
<li>Summary: <p>With the rapid advancement of Large Language Models (LLMs) and their
outstanding performance in semantic and contextual comprehension, the potential
of LLMs in specialized domains warrants exploration. This paper introduces the
NoteAid EHR Interaction Pipeline, an innovative approach developed using
generative LLMs to assist in patient education, a task stemming from the need
to aid patients in understanding Electronic Health Records (EHRs). Building
upon the NoteAid work, we designed two novel tasks from the patient's
perspective: providing explanations for EHR content that patients may not
understand and answering questions posed by patients after reading their EHRs.
We extracted datasets containing 10,000 instances from MIMIC Discharge
Summaries and 876 instances from the MADE medical notes collection,
respectively, executing the two tasks through the NoteAid EHR Interaction
Pipeline with these data. Performance data of LLMs on these tasks were
collected and constructed as the corresponding NoteAid EHR Interaction Dataset.
Through a comprehensive evaluation of the entire dataset using LLM assessment
and a rigorous manual evaluation of 64 instances, we showcase the potential of
LLMs in patient education. Besides, the results provide valuable data support
for future exploration and applications in this domain while also supplying
high-quality synthetic datasets for in-house system training.
</p></li>
</ul>

<h3>Title: Building Efficient Universal Classifiers with Natural Language Inference. (arXiv:2312.17543v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17543">http://arxiv.org/abs/2312.17543</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17543]] Building Efficient Universal Classifiers with Natural Language Inference(http://arxiv.org/abs/2312.17543)</code></li>
<li>Summary: <p>Generative Large Language Models (LLMs) have become the mainstream choice for
fewshot and zeroshot learning thanks to the universality of text generation.
Many users, however, do not need the broad capabilities of generative LLMs when
they only want to automate a classification task. Smaller BERT-like models can
also learn universal tasks, which allow them to do any text classification task
without requiring fine-tuning (zeroshot classification) or to learn new tasks
with only a few examples (fewshot), while being significantly more efficient
than generative LLMs. This paper (1) explains how Natural Language Inference
(NLI) can be used as a universal classification task that follows similar
principles as instruction fine-tuning of generative LLMs, (2) provides a
step-by-step guide with reusable Jupyter notebooks for building a universal
classifier, and (3) shares the resulting universal classifier that is trained
on 33 datasets with 389 diverse classes. Parts of the code we share has been
used to train our older zeroshot classifiers that have been downloaded more
than 55 million times via the Hugging Face Hub as of December 2023. Our new
classifier improves zeroshot performance by 9.4%.
</p></li>
</ul>

<h3>Title: Large Language Models for Generative Information Extraction: A Survey. (arXiv:2312.17617v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17617">http://arxiv.org/abs/2312.17617</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17617]] Large Language Models for Generative Information Extraction: A Survey(http://arxiv.org/abs/2312.17617)</code></li>
<li>Summary: <p>Information extraction (IE) aims to extract structural knowledge (such as
entities, relations, and events) from plain natural language texts. Recently,
generative Large Language Models (LLMs) have demonstrated remarkable
capabilities in text understanding and generation, allowing for generalization
across various domains and tasks. As a result, numerous works have been
proposed to harness abilities of LLMs and offer viable solutions for IE tasks
based on a generative paradigm. To conduct a comprehensive systematic review
and exploration of LLM efforts for IE tasks, in this study, we survey the most
recent advancements in this field. We first present an extensive overview by
categorizing these works in terms of various IE subtasks and learning
paradigms, then we empirically analyze the most advanced methods and discover
the emerging trend of IE tasks with LLMs. Based on thorough review conducted,
we identify several insights in technique and promising research directions
that deserve further exploration in future studies. We maintain a public
repository and consistently update related resources at:
\url{https://github.com/quqxui/Awesome-LLM4IE-Papers}.
</p></li>
</ul>

<h3>Title: Generative Posterior Networks for Approximately Bayesian Epistemic Uncertainty Estimation. (arXiv:2312.17411v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17411">http://arxiv.org/abs/2312.17411</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17411]] Generative Posterior Networks for Approximately Bayesian Epistemic Uncertainty Estimation(http://arxiv.org/abs/2312.17411)</code></li>
<li>Summary: <p>In many real-world problems, there is a limited set of training data, but an
abundance of unlabeled data. We propose a new method, Generative Posterior
Networks (GPNs), that uses unlabeled data to estimate epistemic uncertainty in
high-dimensional problems. A GPN is a generative model that, given a prior
distribution over functions, approximates the posterior distribution directly
by regularizing the network towards samples from the prior. We prove
theoretically that our method indeed approximates the Bayesian posterior and
show empirically that it improves epistemic uncertainty estimation and
scalability over competing methods.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Anticipated Network Surveillance -- An extrapolated study to predict cyber-attacks using Machine Learning and Data Analytics. (arXiv:2312.17270v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17270">http://arxiv.org/abs/2312.17270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17270]] Anticipated Network Surveillance -- An extrapolated study to predict cyber-attacks using Machine Learning and Data Analytics(http://arxiv.org/abs/2312.17270)</code></li>
<li>Summary: <p>Machine learning and data mining techniques are utiized for enhancement of
the security of any network. Researchers used machine learning for pattern
detection, anomaly detection, dynamic policy setting, etc. The methods allow
the program to learn from data and make decisions without human intervention,
consuming a huge training period and computation power. This paper discusses a
novel technique to predict an upcoming attack in a network based on several
data parameters. The dataset is continuous in real-time implementation. The
proposed model comprises dataset pre-processing, and training, followed by the
testing phase. Based on the results of the testing phase, the best model is
selected using which, event class which may lead to an attack is extracted. The
event statistics are used for attack
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Overview of the PromptCBLUE Shared Task in CHIP2023. (arXiv:2312.17522v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.17522">http://arxiv.org/abs/2312.17522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.17522]] Overview of the PromptCBLUE Shared Task in CHIP2023(http://arxiv.org/abs/2312.17522)</code></li>
<li>Summary: <p>This paper presents an overview of the PromptCBLUE shared task
(<a href="http://cips-chip.org.cn/2023/eval1">this http URL</a>) held in the CHIP-2023 Conference. This
shared task reformualtes the CBLUE benchmark, and provide a good testbed for
Chinese open-domain or medical-domain large language models (LLMs) in general
medical natural language processing. Two different tracks are held: (a) prompt
tuning track, investigating the multitask prompt tuning of LLMs, (b) probing
the in-context learning capabilities of open-sourced LLMs. Many teams from both
the industry and academia participated in the shared tasks, and the top teams
achieved amazing test results. This paper describes the tasks, the datasets,
evaluation metrics, and the top systems for both tasks. Finally, the paper
summarizes the techniques and results of the evaluation of the various
approaches explored by the participating teams.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
