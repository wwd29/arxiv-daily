<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Boosting Diffusion Models with an Adaptive Momentum Sampler. (arXiv:2308.11941v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11941">http://arxiv.org/abs/2308.11941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11941]] Boosting Diffusion Models with an Adaptive Momentum Sampler(http://arxiv.org/abs/2308.11941)</code></li>
<li>Summary: <p>Diffusion probabilistic models (DPMs) have been shown to generate
high-quality images without the need for delicate adversarial training.
However, the current sampling process in DPMs is prone to violent shaking. In
this paper, we present a novel reverse sampler for DPMs inspired by the
widely-used Adam optimizer. Our proposed sampler can be readily applied to a
pre-trained diffusion model, utilizing momentum mechanisms and adaptive
updating to smooth the reverse sampling process and ensure stable generation,
resulting in outputs of enhanced quality. By implicitly reusing update
directions from early steps, our proposed sampler achieves a better balance
between high-level semantics and low-level details. Additionally, this sampler
is flexible and can be easily integrated into pre-trained DPMs regardless of
the sampler used during training. Our experimental results on multiple
benchmarks demonstrate that our proposed reverse sampler yields remarkable
improvements over different baselines. We will make the source code available.
</p></li>
</ul>

<h3>Title: LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model. (arXiv:2308.11945v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11945">http://arxiv.org/abs/2308.11945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11945]] LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model(http://arxiv.org/abs/2308.11945)</code></li>
<li>Summary: <p>Dancing with music is always an essential human art form to express emotion.
Due to the high temporal-spacial complexity, long-term 3D realist dance
generation synchronized with music is challenging. Existing methods suffer from
the freezing problem when generating long-term dances due to error accumulation
and training-inference discrepancy. To address this, we design a conditional
diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance
generation, addressing the challenges of temporal coherency and spatial
constraint. LongDanceDiff contains a transformer-based diffusion model, where
the input is a concatenation of music, past motions, and noised future motions.
This partial noising strategy leverages the full-attention mechanism and learns
the dependencies among music and past motions. To enhance the diversity of
generated dance motions and mitigate the freezing problem, we introduce a
mutual information minimization objective that regularizes the dependency
between past and future motions. We also address common visual quality issues
in dance generation, such as foot sliding and unsmooth motion, by incorporating
spatial constraints through a Global-Trajectory Modulation (GTM) layer and
motion perceptual losses, thereby improving the smoothness and naturalness of
motion generation. Extensive experiments demonstrate a significant improvement
in our approach over the existing state-of-the-art methods. We plan to release
our codes and models soon.
</p></li>
</ul>

<h3>Title: Efficient Transfer Learning in Diffusion Models via Adversarial Noise. (arXiv:2308.11948v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11948">http://arxiv.org/abs/2308.11948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11948]] Efficient Transfer Learning in Diffusion Models via Adversarial Noise(http://arxiv.org/abs/2308.11948)</code></li>
<li>Summary: <p>Diffusion Probabilistic Models (DPMs) have demonstrated substantial promise
in image generation tasks but heavily rely on the availability of large amounts
of training data. Previous works, like GANs, have tackled the limited data
problem by transferring pre-trained models learned with sufficient data.
However, those methods are hard to be utilized in DPMs since the distinct
differences between DPM-based and GAN-based methods, showing in the unique
iterative denoising process integral and the need for many timesteps with
no-targeted noise in DPMs. In this paper, we propose a novel DPMs-based
transfer learning method, TAN, to address the limited data problem. It includes
two strategies: similarity-guided training, which boosts transfer with a
classifier, and adversarial noise selection which adaptive chooses targeted
noise based on the input image. Extensive experiments in the context of
few-shot image generation tasks demonstrate that our method is not only
efficient but also excels in terms of image quality and diversity when compared
to existing GAN-based and DDPM-based methods.
</p></li>
</ul>

<h3>Title: High-quality Image Dehazing with Diffusion Model. (arXiv:2308.11949v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11949">http://arxiv.org/abs/2308.11949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11949]] High-quality Image Dehazing with Diffusion Model(http://arxiv.org/abs/2308.11949)</code></li>
<li>Summary: <p>Image dehazing is quite challenging in dense-haze scenarios, where quite less
original information remains in the hazy image. Though previous methods have
made marvelous progress, they still suffer from information loss in content and
color in dense-haze scenarios. The recently emerged Denoising Diffusion
Probabilistic Model (DDPM) exhibits strong generation ability, showing
potential for solving this problem. However, DDPM fails to consider the physics
property of dehazing task, limiting its information completion capacity. In
this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing
framework that applies to complex hazy scenarios. Specifically, DehazeDDPM
works in two stages. The former stage physically models the dehazing task with
the Atmospheric Scattering Model (ASM), pulling the distribution closer to the
clear data and endowing DehazeDDPM with fog-aware ability. The latter stage
exploits the strong generation ability of DDPM to compensate for the
haze-induced huge information loss, by working in conjunction with the physical
modelling. Extensive experiments demonstrate that our method attains
state-of-the-art performance on both synthetic and real-world hazy datasets.
</p></li>
</ul>

<h3>Title: Manipulating Embeddings of Stable Diffusion Prompts. (arXiv:2308.12059v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12059">http://arxiv.org/abs/2308.12059</a></li>
<li>Code URL: https://github.com/webis-de/arxiv23-prompt-embedding-manipulation</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12059]] Manipulating Embeddings of Stable Diffusion Prompts(http://arxiv.org/abs/2308.12059)</code></li>
<li>Summary: <p>Generative text-to-image models such as Stable Diffusion allow users to
generate images based on a textual description, the prompt. Changing the prompt
is still the primary means for the user to change a generated image as desired.
However, changing the image by reformulating the prompt remains a difficult
process of trial and error, which has led to the emergence of prompt
engineering as a new field of research. We propose and analyze methods to
change the embedding of a prompt directly instead of the prompt text. It allows
for more fine-grained and targeted control that takes into account user
intentions. Our approach treats the generative text-to-image model as a
continuous function and passes gradients between the image space and the prompt
embedding space. By addressing different user interaction problems, we can
apply this idea in three scenarios: (1) Optimization of a metric defined in
image space that could measure, for example, image style. (2) Assistance of
users in creative tasks by enabling them to navigate the image space along a
selection of directions of "near" prompt embeddings. (3) Changing the embedding
of the prompt to include information that the user has seen in a particular
seed but finds difficult to describe in the prompt. Our experiments demonstrate
the feasibility of the described methods.
</p></li>
</ul>

<h3>Title: Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12219">http://arxiv.org/abs/2308.12219</a></li>
<li>Code URL: https://github.com/yegcjs/diffusionllm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12219]] Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning(http://arxiv.org/abs/2308.12219)</code></li>
<li>Summary: <p>The recent surge of generative AI has been fueled by the generative power of
diffusion probabilistic models and the scalable capabilities of large language
models. Despite their potential, it remains elusive whether diffusion language
models can solve general language tasks comparable to their autoregressive
counterparts. This paper demonstrates that scaling diffusion models w.r.t.
data, sizes, and tasks can effectively make them strong language learners. We
build competent diffusion language models at scale by first acquiring knowledge
from massive data via masked language modeling pretraining thanks to their
intrinsic connections. We then reprogram pretrained masked language models into
diffusion language models via diffusive adaptation, wherein task-specific
finetuning and instruction finetuning are explored to unlock their versatility
in solving general language tasks. Experiments show that scaling diffusion
language models consistently improves performance across downstream language
tasks. We further discover that instruction finetuning can elicit zero-shot and
few-shot in-context learning abilities that help tackle many unseen tasks by
following natural language instructions, and show promise in advanced and
challenging abilities such as reasoning
</p></li>
</ul>

<h3>Title: Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11624">http://arxiv.org/abs/2308.11624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11624]] Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks(http://arxiv.org/abs/2308.11624)</code></li>
<li>Summary: <p>An innovative methodology that leverages artificial intelligence (AI) and
graph representation for semiconductor device encoding in TCAD device
simulation is proposed. A graph-based universal encoding scheme is presented
that not only considers material-level and device-level embeddings, but also
introduces a novel spatial relationship embedding inspired by interpolation
operations typically used in finite element meshing. Universal physical laws
from device simulations are leveraged for comprehensive data-driven modeling,
which encompasses surrogate Poisson emulation and current-voltage (IV)
prediction based on drift-diffusion model. Both are achieved using a novel
graph attention network, referred to as RelGAT. Comprehensive technical details
based on the device simulator Sentaurus TCAD are presented, empowering
researchers to adopt the proposed AI-driven Electronic Design Automation (EDA)
solution at the device level.
</p></li>
</ul>

<h3>Title: Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models. (arXiv:2308.11890v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11890">http://arxiv.org/abs/2308.11890</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11890]] Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models(http://arxiv.org/abs/2308.11890)</code></li>
<li>Summary: <p>Ligand-based drug design aims to identify novel drug candidates of similar
shapes with known active molecules. In this paper, we formulated an in silico
shape-conditioned molecule generation problem to generate 3D molecule
structures conditioned on the shape of a given molecule. To address this
problem, we developed a translation- and rotation-equivariant shape-guided
generative model ShapeMol. ShapeMol consists of an equivariant shape encoder
that maps molecular surface shapes into latent embeddings, and an equivariant
diffusion model that generates 3D molecules based on these embeddings.
Experimental results show that ShapeMol can generate novel, diverse, drug-like
molecules that retain 3D molecular shapes similar to the given shape condition.
These results demonstrate the potential of ShapeMol in designing drug
candidates of desired 3D shapes binding to protein target pockets.
</p></li>
</ul>

<h3>Title: On-Manifold Projected Gradient Descent. (arXiv:2308.12279v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12279">http://arxiv.org/abs/2308.12279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12279]] On-Manifold Projected Gradient Descent(http://arxiv.org/abs/2308.12279)</code></li>
<li>Summary: <p>This work provides a computable, direct, and mathematically rigorous
approximation to the differential geometry of class manifolds for
high-dimensional data, along with nonlinear projections from input space onto
these class manifolds. The tools are applied to the setting of neural network
image classifiers, where we generate novel, on-manifold data samples, and
implement a projected gradient descent algorithm for on-manifold adversarial
training. The susceptibility of neural networks (NNs) to adversarial attack
highlights the brittle nature of NN decision boundaries in input space.
Introducing adversarial examples during training has been shown to reduce the
susceptibility of NNs to adversarial attack; however, it has also been shown to
reduce the accuracy of the classifier if the examples are not valid examples
for that class. Realistic "on-manifold" examples have been previously generated
from class manifolds in the latent of an autoencoder. Our work explores these
phenomena in a geometric and computational setting that is much closer to the
raw, high-dimensional input space than can be provided by VAE or other black
box dimensionality reductions. We employ conformally invariant diffusion maps
(CIDM) to approximate class manifolds in diffusion coordinates, and develop the
Nystr\"{o}m projection to project novel points onto class manifolds in this
setting. On top of the manifold approximation, we leverage the spectral
exterior calculus (SEC) to determine geometric quantities such as tangent
vectors of the manifold. We use these tools to obtain adversarial examples that
reside on a class manifold, yet fool a classifier. These misclassifications
then become explainable in terms of human-understandable manipulations within
the data, by expressing the on-manifold adversary in the semantic basis on the
manifold.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning. (arXiv:2308.11677v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11677">http://arxiv.org/abs/2308.11677</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11677]] An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning(http://arxiv.org/abs/2308.11677)</code></li>
<li>Summary: <p>Class-Incremental Learning (CIL) aims to build classification models from
data streams. At each step of the CIL process, new classes must be integrated
into the model. Due to catastrophic forgetting, CIL is particularly challenging
when examples from past classes cannot be stored, the case on which we focus
here. To date, most approaches are based exclusively on the target dataset of
the CIL process. However, the use of models pre-trained in a self-supervised
way on large amounts of data has recently gained momentum. The initial model of
the CIL process may only use the first batch of the target dataset, or also use
pre-trained weights obtained on an auxiliary dataset. The choice between these
two initial learning strategies can significantly influence the performance of
the incremental learning model, but has not yet been studied in depth.
Performance is also influenced by the choice of the CIL algorithm, the neural
architecture, the nature of the target task, the distribution of classes in the
stream and the number of examples available for learning. We conduct a
comprehensive experimental study to assess the roles of these factors. We
present a statistical analysis framework that quantifies the relative
contribution of each factor to incremental performance. Our main finding is
that the initial training strategy is the dominant factor influencing the
average incremental accuracy, but that the choice of CIL algorithm is more
important in preventing forgetting. Based on this analysis, we propose
practical recommendations for choosing the right initial training strategy for
a given incremental learning use case. These recommendations are intended to
facilitate the practical deployment of incremental learning.
</p></li>
</ul>

<h3>Title: WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters. (arXiv:2308.11776v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11776]] WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters(http://arxiv.org/abs/2308.11776)</code></li>
<li>Summary: <p>Depth estimation in surgical video plays a crucial role in many image-guided
surgery procedures. However, it is difficult and time consuming to create depth
map ground truth datasets in surgical videos due in part to inconsistent
brightness and noise in the surgical scene. Therefore, building an accurate and
robust self-supervised depth and camera ego-motion estimation system is gaining
more attention from the computer vision community. Although several
self-supervision methods alleviate the need for ground truth depth maps and
poses, they still need known camera intrinsic parameters, which are often
missing or not recorded. Moreover, the camera intrinsic prediction methods in
existing works depend heavily on the quality of datasets. In this work, we
aimed to build a self-supervised depth and ego-motion estimation system which
can predict not only accurate depth maps and camera pose, but also camera
intrinsic parameters. We proposed a cost-volume-based supervision manner to
give the system auxiliary supervision for camera parameters prediction. The
experimental results showed that the proposed method improved the accuracy of
estimated camera parameters, ego-motion, and depth estimation.
</p></li>
</ul>

<h3>Title: Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations. (arXiv:2308.11796v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11796">http://arxiv.org/abs/2308.11796</a></li>
<li>Code URL: https://github.com/smsd75/timetuning</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11796]] Time Does Tell: Self-Supervised Time-Tuning of Dense Image Representations(http://arxiv.org/abs/2308.11796)</code></li>
<li>Summary: <p>Spatially dense self-supervised learning is a rapidly growing problem domain
with promising applications for unsupervised segmentation and pretraining for
dense downstream tasks. Despite the abundance of temporal data in the form of
videos, this information-rich source has been largely overlooked. Our paper
aims to address this gap by proposing a novel approach that incorporates
temporal consistency in dense self-supervised learning. While methods designed
solely for images face difficulties in achieving even the same performance on
videos, our method improves not only the representation quality for videos-but
also images. Our approach, which we call time-tuning, starts from
image-pretrained models and fine-tunes them with a novel self-supervised
temporal-alignment clustering loss on unlabeled videos. This effectively
facilitates the transfer of high-level information from videos to image
representations. Time-tuning improves the state-of-the-art by 8-10% for
unsupervised semantic segmentation on videos and matches it for images. We
believe this method paves the way for further self-supervised scaling by
leveraging the abundant availability of videos. The implementation can be found
here : https://github.com/SMSD75/Timetuning
</p></li>
</ul>

<h3>Title: Semantic-Aware Implicit Template Learning via Part Deformation Consistency. (arXiv:2308.11916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11916">http://arxiv.org/abs/2308.11916</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11916]] Semantic-Aware Implicit Template Learning via Part Deformation Consistency(http://arxiv.org/abs/2308.11916)</code></li>
<li>Summary: <p>Learning implicit templates as neural fields has recently shown impressive
performance in unsupervised shape correspondence. Despite the success, we
observe current approaches, which solely rely on geometric information, often
learn suboptimal deformation across generic object shapes, which have high
structural variability. In this paper, we highlight the importance of part
deformation consistency and propose a semantic-aware implicit template learning
framework to enable semantically plausible deformation. By leveraging semantic
prior from a self-supervised feature extractor, we suggest local conditioning
with novel semantic-aware deformation code and deformation consistency
regularizations regarding part deformation, global deformation, and global
scaling. Our extensive experiments demonstrate the superiority of the proposed
method over baselines in various tasks: keypoint transfer, part label transfer,
and texture transfer. More interestingly, our framework shows a larger
performance gain under more challenging settings. We also provide qualitative
analyses to validate the effectiveness of semantic-aware deformation. The code
is available at https://github.com/mlvlab/PDC.
</p></li>
</ul>

<h3>Title: Head-Tail Cooperative Learning Network for Unbiased Scene Graph Generation. (arXiv:2308.12048v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12048">http://arxiv.org/abs/2308.12048</a></li>
<li>Code URL: https://github.com/wanglei0618/htcl</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12048]] Head-Tail Cooperative Learning Network for Unbiased Scene Graph Generation(http://arxiv.org/abs/2308.12048)</code></li>
<li>Summary: <p>Scene Graph Generation (SGG) as a critical task in image understanding,
facing the challenge of head-biased prediction caused by the long-tail
distribution of predicates. However, current unbiased SGG methods can easily
prioritize improving the prediction of tail predicates while ignoring the
substantial sacrifice in the prediction of head predicates, leading to a shift
from head bias to tail bias. To address this issue, we propose a model-agnostic
Head-Tail Collaborative Learning (HTCL) network that includes head-prefer and
tail-prefer feature representation branches that collaborate to achieve
accurate recognition of both head and tail predicates. We also propose a
self-supervised learning approach to enhance the prediction ability of the
tail-prefer feature representation branch by constraining tail-prefer predicate
features. Specifically, self-supervised learning converges head predicate
features to their class centers while dispersing tail predicate features as
much as possible through contrast learning and head center loss. We demonstrate
the effectiveness of our HTCL by applying it to various SGG models on VG150,
Open Images V6 and GQA200 datasets. The results show that our method achieves
higher mean Recall with a minimal sacrifice in Recall and achieves a new
state-of-the-art overall performance. Our code is available at
https://github.com/wanglei0618/HTCL.
</p></li>
</ul>

<h3>Title: CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images. (arXiv:2308.12288v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12288">http://arxiv.org/abs/2308.12288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12288]] CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images(http://arxiv.org/abs/2308.12288)</code></li>
<li>Summary: <p>We present a method for teaching machines to understand and model the
underlying spatial common sense of diverse human-object interactions in 3D in a
self-supervised way. This is a challenging task, as there exist specific
manifolds of the interactions that can be considered human-like and natural,
but the human pose and the geometry of objects can vary even for similar
interactions. Such diversity makes the annotating task of 3D interactions
difficult and hard to scale, which limits the potential to reason about that in
a supervised way. One way of learning the 3D spatial relationship between
humans and objects during interaction is by showing multiple 2D images captured
from different viewpoints when humans interact with the same type of objects.
The core idea of our method is to leverage a generative model that produces
high-quality 2D images from an arbitrary text prompt input as an "unbounded"
data generator with effective controllability and view diversity. Despite its
imperfection of the image quality over real images, we demonstrate that the
synthesized images are sufficient to learn the 3D human-object spatial
relations. We present multiple strategies to leverage the synthesized images,
including (1) the first method to leverage a generative image model for 3D
human-object spatial relation learning; (2) a framework to reason about the 3D
spatial relations from inconsistent 2D cues in a self-supervised manner via 3D
occupancy reasoning with pose canonicalization; (3) semantic clustering to
disambiguate different types of interactions with the same object types; and
(4) a novel metric to assess the quality of 3D spatial learning of interaction.
Project Page: https://jellyheadandrew.github.io/projects/chorus
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11971]] EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE(http://arxiv.org/abs/2308.11971)</code></li>
<li>Summary: <p>Building scalable vision-language models to learn from diverse, multimodal
data remains an open challenge. In this paper, we introduce an Efficient
Vision-languagE foundation model, namely EVE, which is one unified multimodal
Transformer pre-trained solely by one unified pre-training task. Specifically,
EVE encodes both vision and language within a shared Transformer network
integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which
capture modality-specific information by selectively switching to different
experts. To unify pre-training tasks of vision and language, EVE performs
masked signal modeling on image-text pairs to reconstruct masked signals, i.e.,
image pixels and text tokens, given visible signals. This simple yet effective
pre-training objective accelerates training by 3.5x compared to the model
pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing
to the combination of the unified architecture and pre-training task, EVE is
easy to scale up, enabling better downstream performance with fewer resources
and faster training speed. Despite its simplicity, EVE achieves
state-of-the-art performance on various vision-language downstream tasks,
including visual question answering, visual reasoning, and image-text
retrieval.
</p></li>
</ul>

<h3>Title: Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment. (arXiv:2308.12001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12001">http://arxiv.org/abs/2308.12001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12001]] Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment(http://arxiv.org/abs/2308.12001)</code></li>
<li>Summary: <p>Image Quality Assessment (IQA) constitutes a fundamental task within the
field of computer vision, yet it remains an unresolved challenge, owing to the
intricate distortion conditions, diverse image contents, and limited
availability of data. Recently, the community has witnessed the emergence of
numerous large-scale pretrained foundation models, which greatly benefit from
dramatically increased data and parameter capacities. However, it remains an
open problem whether the scaling law in high-level tasks is also applicable to
IQA task which is closely related to low-level clues. In this paper, we
demonstrate that with proper injection of local distortion features, a larger
pretrained and fixed foundation model performs better in IQA tasks.
Specifically, for the lack of local distortion structure and inductive bias of
vision transformer (ViT), alongside the large-scale pretrained ViT, we use
another pretrained convolution neural network (CNN), which is well known for
capturing the local structure, to extract multi-scale image features. Further,
we propose a local distortion extractor to obtain local distortion features
from the pretrained CNN and a local distortion injector to inject the local
distortion features into ViT. By only training the extractor and injector, our
method can benefit from the rich knowledge in the powerful foundation models
and achieve state-of-the-art performance on popular IQA datasets, indicating
that IQA is not only a low-level problem but also benefits from stronger
high-level features drawn from large-scale pretrained models.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Weakly Supervised Face and Whole Body Recognition in Turbulent Environments. (arXiv:2308.11757v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11757">http://arxiv.org/abs/2308.11757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11757]] Weakly Supervised Face and Whole Body Recognition in Turbulent Environments(http://arxiv.org/abs/2308.11757)</code></li>
<li>Summary: <p>Face and person recognition have recently achieved remarkable success under
challenging scenarios, such as off-pose and cross-spectrum matching. However,
long-range recognition systems are often hindered by atmospheric turbulence,
leading to spatially and temporally varying distortions in the image. Current
solutions rely on generative models to reconstruct a turbulent-free image, but
often preserve photo-realism instead of discriminative features that are
essential for recognition. This can be attributed to the lack of large-scale
datasets of turbulent and pristine paired images, necessary for optimal
reconstruction. To address this issue, we propose a new weakly supervised
framework that employs a parameter-efficient self-attention module to generate
domain agnostic representations, aligning turbulent and pristine images into a
common subspace. Additionally, we introduce a new tilt map estimator that
predicts geometric distortions observed in turbulent images. This estimate is
used to re-rank gallery matches, resulting in up to 13.86\% improvement in
rank-1 accuracy. Our method does not require synthesizing turbulent-free images
or ground-truth paired images, and requires significantly fewer annotated
samples, enabling more practical and rapid utility of increasingly large
datasets. We analyze our framework using two datasets -- Long-Range Face
Identification Dataset (LRFID) and BRIAR Government Collection 1 (BGC1) --
achieving enhanced discriminability under varying turbulence and standoff
distance.
</p></li>
</ul>

<h3>Title: CoC-GAN: Employing Context Cluster for Unveiling a New Pathway in Image Generation. (arXiv:2308.11857v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11857">http://arxiv.org/abs/2308.11857</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11857]] CoC-GAN: Employing Context Cluster for Unveiling a New Pathway in Image Generation(http://arxiv.org/abs/2308.11857)</code></li>
<li>Summary: <p>Image generation tasks are traditionally undertaken using Convolutional
Neural Networks (CNN) or Transformer architectures for feature aggregating and
dispatching. Despite the frequent application of convolution and attention
structures, these structures are not fundamentally required to solve the
problem of instability and the lack of interpretability in image generation. In
this paper, we propose a unique image generation process premised on the
perspective of converting images into a set of point clouds. In other words, we
interpret an image as a set of points. As such, our methodology leverages
simple clustering methods named Context Clustering (CoC) to generate images
from unordered point sets, which defies the convention of using convolution or
attention mechanisms. Hence, we exclusively depend on this clustering
technique, combined with the multi-layer perceptron (MLP) in a generative
model. Furthermore, we implement the integration of a module termed the 'Point
Increaser' for the model. This module is just an MLP tasked with generating
additional points for clustering, which are subsequently integrated within the
paradigm of the Generative Adversarial Network (GAN). We introduce this model
with the novel structure as the Context Clustering Generative Adversarial
Network (CoC-GAN), which offers a distinctive viewpoint in the domain of
feature aggregating and dispatching. Empirical evaluations affirm that our
CoC-GAN, devoid of convolution and attention mechanisms, exhibits outstanding
performance. Its interpretability, endowed by the CoC module, also allows for
visualization in our experiments. The promising results underscore the
feasibility of our method and thus warrant future investigations of applying
Context Clustering to more novel and interpretable image generation.
</p></li>
</ul>

<h3>Title: LFS-GAN: Lifelong Few-Shot Image Generation. (arXiv:2308.11917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11917">http://arxiv.org/abs/2308.11917</a></li>
<li>Code URL: https://github.com/jjuon/lfs-gan</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11917]] LFS-GAN: Lifelong Few-Shot Image Generation(http://arxiv.org/abs/2308.11917)</code></li>
<li>Summary: <p>We address a challenging lifelong few-shot image generation task for the
first time. In this situation, a generative model learns a sequence of tasks
using only a few samples per task. Consequently, the learned model encounters
both catastrophic forgetting and overfitting problems at a time. Existing
studies on lifelong GANs have proposed modulation-based methods to prevent
catastrophic forgetting. However, they require considerable additional
parameters and cannot generate high-fidelity and diverse images from limited
data. On the other hand, the existing few-shot GANs suffer from severe
catastrophic forgetting when learning multiple tasks. To alleviate these
issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can
generate high-quality and diverse images in lifelong few-shot image generation
task. Our proposed framework learns each task using an efficient task-specific
modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and
has a rich representation ability due to its unique reconstruction technique.
Furthermore, we propose a novel mode seeking loss to improve the diversity of
our model in low-data circumstances. Extensive experiments demonstrate that the
proposed LFS-GAN can generate high-fidelity and diverse images without any
forgetting and mode collapse in various domains, achieving state-of-the-art in
lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN
even outperforms the existing few-shot GANs in the few-shot image generation
task. The code is available at Github.
</p></li>
</ul>

<h3>Title: A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12143]] A Probabilistic Fluctuation based Membership Inference Attack for Generative Models(http://arxiv.org/abs/2308.12143)</code></li>
<li>Summary: <p>Membership Inference Attack (MIA) identifies whether a record exists in a
machine learning model's training set by querying the model. MIAs on the
classic classification models have been well-studied, and recent works have
started to explore how to transplant MIA onto generative models. Our
investigation indicates that existing MIAs designed for generative models
mainly depend on the overfitting in target models. However, overfitting can be
avoided by employing various regularization techniques, whereas existing MIAs
demonstrate poor performance in practice. Unlike overfitting, memorization is
essential for deep learning models to attain optimal performance, making it a
more prevalent phenomenon. Memorization in generative models leads to an
increasing trend in the probability distribution of generating records around
the member record. Therefore, we propose a Probabilistic Fluctuation Assessing
Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by
detecting these trends via analyzing the overall probabilistic fluctuations
around given records. We conduct extensive experiments across multiple
generative models and datasets, which demonstrate PFAMI can improve the attack
success rate (ASR) by about 27.9% when compared with the best baseline.
</p></li>
</ul>

<h3>Title: A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces. (arXiv:2308.12271v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12271">http://arxiv.org/abs/2308.12271</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12271]] A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces(http://arxiv.org/abs/2308.12271)</code></li>
<li>Summary: <p>Since thermal imagery offers a unique modality to investigate pain, the U.S.
National Institutes of Health (NIH) has collected a large and diverse set of
cancer patient facial thermograms for AI-based pain research. However,
differing angles from camera capture between thermal and visible sensors has
led to misalignment between Visible-Thermal (VT) images. We modernize the
classic computer vision task of image registration by applying and modifying a
generative alignment algorithm to register VT cancer faces, without the need
for a reference or alignment parameters. By registering VT faces, we
demonstrate that the quality of thermal images produced in the generative AI
downstream task of Visible-to-Thermal (V2T) image translation significantly
improves up to 52.5\%, than without registration. Images in this paper have
been approved by the NIH NCI for public dissemination.
</p></li>
</ul>

<h3>Title: Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test. (arXiv:2308.11827v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11827]] Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test(http://arxiv.org/abs/2308.11827)</code></li>
<li>Summary: <p>Large language models such as Open AI's Generative Pre-trained Transformer
(GPT) models are proficient at answering questions, but their knowledge is
confined to the information present in their training data. This limitation
renders them ineffective when confronted with questions about recent
developments or non-public documents. Our research proposes a method that
enables GPT models to answer questions by employing context from an information
source not previously included in their training data. The methodology includes
preprocessing of contextual information, the embedding of contexts and queries,
constructing prompt through the integration of context embeddings, and
generating answers using GPT models. We applied this method in a controlled
test scenario using the California Driver's Handbook as the information source.
The GPT-3 model achieved a 96% passing score on a set of 50 sample driving
knowledge test questions. In contrast, without context, the model's passing
score fell to 82%. However, the model still fails to answer some questions
correctly even with providing library of context, highlighting room for
improvement. The research also examined the impact of prompt length and context
format, on the model's performance. Overall, the study provides insights into
the limitations and potential improvements for GPT models in question-answering
tasks.
</p></li>
</ul>

<h3>Title: How to Protect Copyright Data in Optimization of Large Language Models?. (arXiv:2308.12247v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12247">http://arxiv.org/abs/2308.12247</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12247]] How to Protect Copyright Data in Optimization of Large Language Models?(http://arxiv.org/abs/2308.12247)</code></li>
<li>Summary: <p>Large language models (LLMs) and generative AI have played a transformative
role in computer research and applications. Controversy has arisen as to
whether these models output copyrighted data, which can occur if the data the
models are trained on is copyrighted. LLMs are built on the transformer neural
network architecture, which in turn relies on a mathematical computation called
Attention that uses the softmax function.
</p>
<p>In this paper, we show that large language model training and optimization
can be seen as a softmax regression problem. We then establish a method of
efficiently performing softmax regression, in a way that prevents the
regression function from generating copyright data. This establishes a
theoretical method of training large language models in a way that avoids
generating copyright data.
</p></li>
</ul>

<h3>Title: Maintaining Plasticity via Regenerative Regularization. (arXiv:2308.11958v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11958]] Maintaining Plasticity via Regenerative Regularization(http://arxiv.org/abs/2308.11958)</code></li>
<li>Summary: <p>In continual learning, plasticity refers to the ability of an agent to
quickly adapt to new information. Neural networks are known to lose plasticity
when processing non-stationary data streams. In this paper, we propose L2 Init,
a very simple approach for maintaining plasticity by incorporating in the loss
function L2 regularization toward initial parameters. This is very similar to
standard L2 regularization (L2), the only difference being that L2 regularizes
toward the origin. L2 Init is simple to implement and requires selecting only a
single hyper-parameter. The motivation for this method is the same as that of
methods that reset neurons or parameter values. Intuitively, when recent losses
are insensitive to particular parameters, these parameters drift toward their
initial values. This prepares parameters to adapt quickly to new tasks. On
simple problems representative of different types of nonstationarity in
continual learning, we demonstrate that L2 Init consistently mitigates
plasticity loss. We additionally find that our regularization term reduces
parameter magnitudes and maintains a high effective feature rank.
</p></li>
</ul>

<h3>Title: Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11978]] Will More Expressive Graph Neural Networks do Better on Generative Tasks?(http://arxiv.org/abs/2308.11978)</code></li>
<li>Summary: <p>Graph generation poses a significant challenge as it involves predicting a
complete graph with multiple nodes and edges based on simply a given label.
This task also carries fundamental importance to numerous real-world
applications, including de-novo drug and molecular design. In recent years,
several successful methods have emerged in the field of graph generation.
However, these approaches suffer from two significant shortcomings: (1) the
underlying Graph Neural Network (GNN) architectures used in these methods are
often underexplored; and (2) these methods are often evaluated on only a
limited number of metrics. To fill this gap, we investigate the expressiveness
of GNNs under the context of the molecular graph generation task, by replacing
the underlying GNNs of graph generative models with more expressive GNNs.
Specifically, we analyse the performance of six GNNs in two different
generative frameworks (GCPN and GraphAF), on six different molecular generative
objectives on the ZINC-250k dataset. Through our extensive experiments, we
demonstrate that advanced GNNs can indeed improve the performance of GCPN and
GraphAF on molecular generation tasks, but GNN expressiveness is not a
necessary condition for a good GNN-based generative model. Moreover, we show
that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results
across 17 other non-GNN-based graph generative approaches, such as variational
autoencoders and Bayesian optimisation models, on the proposed molecular
generative objectives (DRD2, Median1, Median2), which are important metrics for
de-novo molecular design.
</p></li>
</ul>

<h3>Title: How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12252">http://arxiv.org/abs/2308.12252</a></li>
<li>Code URL: https://github.com/maozj6/hsai-predictor</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12252]] How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy(http://arxiv.org/abs/2308.12252)</code></li>
<li>Summary: <p>End-to-end learning has emerged as a major paradigm for developing autonomous
systems. Unfortunately, with its performance and convenience comes an even
greater challenge of safety assurance. A key factor of this challenge is the
absence of the notion of a low-dimensional and interpretable dynamical state,
around which traditional assurance methods revolve. Focusing on the online
safety prediction problem, this paper proposes a configurable family of
learning pipelines based on generative world models, which do not require
low-dimensional states. To implement these pipelines, we overcome the
challenges of learning safety-informed latent representations and missing
safety labels under prediction-induced distribution shift. These pipelines come
with statistical calibration guarantees on their safety chance predictions
based on conformal prediction. We perform an extensive evaluation of the
proposed learning pipelines on two case studies of image-controlled systems: a
racing car and a cartpole.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection. (arXiv:2308.11681v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11681">http://arxiv.org/abs/2308.11681</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11681]] VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection(http://arxiv.org/abs/2308.11681)</code></li>
<li>Summary: <p>The recent contrastive language-image pre-training (CLIP) model has shown
great success in a wide range of image-level tasks, revealing remarkable
ability for learning powerful visual representations with rich semantics. An
open and worthwhile problem is efficiently adapting such a strong model to the
video domain and designing a robust video anomaly detector. In this work, we
propose VadCLIP, a new paradigm for weakly supervised video anomaly detection
(WSVAD) by leveraging the frozen CLIP model directly without any pre-training
and fine-tuning process. Unlike current works that directly feed extracted
features into the weakly supervised classifier for frame-level binary
classification, VadCLIP makes full use of fine-grained associations between
vision and language on the strength of CLIP and involves dual branch. One
branch simply utilizes visual features for coarse-grained binary
classification, while the other fully leverages the fine-grained language-image
alignment. With the benefit of dual branch, VadCLIP achieves both
coarse-grained and fine-grained video anomaly detection by transferring
pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments
on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best
performance on both coarse-grained and fine-grained WSVAD, surpassing the
state-of-the-art methods by a large margin. Specifically, VadCLIP achieves
84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and
features will be released to facilitate future VAD research.
</p></li>
</ul>

<h3>Title: Exploring the Optimization Objective of One-Class Classification for Anomaly Detection. (arXiv:2308.11898v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11898">http://arxiv.org/abs/2308.11898</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11898]] Exploring the Optimization Objective of One-Class Classification for Anomaly Detection(http://arxiv.org/abs/2308.11898)</code></li>
<li>Summary: <p>One-class classification (OCC) is a longstanding method for anomaly
detection. With the powerful representation capability of the pre-trained
backbone, OCC methods have witnessed significant performance improvements.
Typically, most of these OCC methods employ transfer learning to enhance the
discriminative nature of the pre-trained backbone's features, thus achieving
remarkable efficacy. While most current approaches emphasize feature transfer
strategies, we argue that the optimization objective space within OCC methods
could also be an underlying critical factor influencing performance. In this
work, we conducted a thorough investigation into the optimization objective of
OCC. Through rigorous theoretical analysis and derivation, we unveil a key
insights: any space with the suitable norm can serve as an equivalent
substitute for the hypersphere center, without relying on the distribution
assumption of training samples. Further, we provide guidelines for determining
the feasible domain of norms for the OCC optimization objective. This novel
insight sparks a simple and data-agnostic deep one-class classification method.
Our method is straightforward, with a single 1x1 convolutional layer as a
trainable projector and any space with suitable norm as the optimization
objective. Extensive experiments validate the reliability and efficacy of our
findings and the corresponding methodology, resulting in state-of-the-art
performance in both one-class classification and industrial vision anomaly
detection and segmentation tasks.
</p></li>
</ul>

<h3>Title: Few-shot Anomaly Detection in Text with Deviation Learning. (arXiv:2308.11780v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11780">http://arxiv.org/abs/2308.11780</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11780]] Few-shot Anomaly Detection in Text with Deviation Learning(http://arxiv.org/abs/2308.11780)</code></li>
<li>Summary: <p>Most current methods for detecting anomalies in text concentrate on
constructing models solely relying on unlabeled data. These models operate on
the presumption that no labeled anomalous examples are available, which
prevents them from utilizing prior knowledge of anomalies that are typically
present in small numbers in many real-world applications. Furthermore, these
models prioritize learning feature embeddings rather than optimizing anomaly
scores directly, which could lead to suboptimal anomaly scoring and inefficient
use of data during the learning process. In this paper, we introduce FATE, a
deep few-shot learning-based framework that leverages limited anomaly examples
and learns anomaly scores explicitly in an end-to-end method using deviation
learning. In this approach, the anomaly scores of normal examples are adjusted
to closely resemble reference scores obtained from a prior distribution.
Conversely, anomaly samples are forced to have anomalous scores that
considerably deviate from the reference score in the upper tail of the prior.
Additionally, our model is optimized to learn the distinct behavior of
anomalies by utilizing a multi-head self-attention layer and multiple instance
learning approaches. Comprehensive experiments on several benchmark datasets
demonstrate that our proposed approach attains a new level of state-of-the-art
performance.
</p></li>
</ul>

<h3>Title: Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection. (arXiv:2308.11834v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11834">http://arxiv.org/abs/2308.11834</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11834]] Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection(http://arxiv.org/abs/2308.11834)</code></li>
<li>Summary: <p>Bayesian classifiers perform well when each of the features is completely
independent of the other which is not always valid in real world application.
The aim of this study is to implement and compare the performances of each
variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on
anomaly detection in network intrusion, and to investigate whether there is any
association between each variant assumption and their performance. Our
investigation showed that each variant of Bayesian algorithm blindly follows
its assumption regardless of feature property, and that the assumption is the
single most important factor that influences their accuracy. Experimental
results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial
has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69%
test (82.84% train). Going deeper, we investigated and found that each Naive
Bayes variants performances and accuracy is largely due to each classifier
assumption, Gaussian classifier performed best on anomaly detection due to its
assumption that features follow normal distributions which are continuous,
while multinomial classifier have a dismal performance as it simply assumes
discreet and multinomial distribution.
</p></li>
</ul>

<h3>Title: Class Label-aware Graph Anomaly Detection. (arXiv:2308.11669v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.11669">http://arxiv.org/abs/2308.11669</a></li>
<li>Code URL: https://github.com/jhkim611/clad</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.11669]] Class Label-aware Graph Anomaly Detection(http://arxiv.org/abs/2308.11669)</code></li>
<li>Summary: <p>Unsupervised GAD methods assume the lack of anomaly labels, i.e., whether a
node is anomalous or not. One common observation we made from previous
unsupervised methods is that they not only assume the absence of such anomaly
labels, but also the absence of class labels (the class a node belongs to used
in a general node classification task). In this work, we study the utility of
class labels for unsupervised GAD; in particular, how they enhance the
detection of structural anomalies. To this end, we propose a Class Label-aware
Graph Anomaly Detection framework (CLAD) that utilizes a limited amount of
labeled nodes to enhance the performance of unsupervised GAD. Extensive
experiments on ten datasets demonstrate the superior performance of CLAD in
comparison to existing unsupervised GAD methods, even in the absence of
ground-truth class label information. The source code for CLAD is available at
\url{https://github.com/jhkim611/CLAD}.
</p></li>
</ul>

<h2>in-context</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
