<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-11-21</h1>
<h3>Title: CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization</h3>
<ul>
<li><strong>Authors: </strong>Nay Myat Min, Long H. Pham, Yige Li, Jun Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12768">https://arxiv.org/abs/2411.12768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12768">https://arxiv.org/pdf/2411.12768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12768]] CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization(https://arxiv.org/abs/2411.12768)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent studies reveal that Large Language Models (LLMs) are susceptible to backdoor attacks, where adversaries embed hidden triggers that manipulate model responses. Existing backdoor defense methods are primarily designed for vision or classification tasks, and are thus ineffective for text generation tasks, leaving LLMs vulnerable. We introduce Internal Consistency Regularization (CROW), a novel defense using consistency regularization finetuning to address layer-wise inconsistencies caused by backdoor triggers. CROW leverages the intuition that clean models exhibit smooth, consistent transitions in hidden representations across layers, whereas backdoored models show noticeable fluctuation when triggered. By enforcing internal consistency through adversarial perturbations and regularization, CROW neutralizes backdoor effects without requiring clean reference models or prior trigger knowledge, relying only on a small set of clean data. This makes it practical for deployment across various LLM architectures. Experimental results demonstrate that CROW consistently achieves a significant reductions in attack success rates across diverse backdoor strategies and tasks, including negative sentiment, targeted refusal, and code injection, on models such as Llama-2 (7B, 13B), CodeLlama (7B, 13B) and Mistral-7B, while preserving the model's generative capabilities.</li>
</ul>

<h3>Title: Decoupling Training-Free Guided Diffusion by ADMM</h3>
<ul>
<li><strong>Authors: </strong>Youyuan Zhang, Zehua Liu, Zenan Li, Zhaoyu Li, James J. Clark, Xujie Si</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12773">https://arxiv.org/abs/2411.12773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12773">https://arxiv.org/pdf/2411.12773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12773]] Decoupling Training-Free Guided Diffusion by ADMM(https://arxiv.org/abs/2411.12773)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we consider the conditional generation problem by guiding off-the-shelf unconditional diffusion models with differentiable loss functions in a plug-and-play fashion. While previous research has primarily focused on balancing the unconditional diffusion model and the guided loss through a tuned weight hyperparameter, we propose a novel framework that distinctly decouples these two components. Specifically, we introduce two variables ${x}$ and ${z}$, to represent the generated samples governed by the unconditional generation model and the guidance function, respectively. This decoupling reformulates conditional generation into two manageable subproblems, unified by the constraint ${x} = {z}$. Leveraging this setup, we develop a new algorithm based on the Alternating Direction Method of Multipliers (ADMM) to adaptively balance these components. Additionally, we establish the equivalence between the diffusion reverse step and the proximal operator of ADMM and provide a detailed convergence analysis of our algorithm under certain mild assumptions. Our experiments demonstrate that our proposed method ADMMDiff consistently generates high-quality samples while ensuring strong adherence to the conditioning criteria. It outperforms existing methods across a range of conditional generation tasks, including image generation with various guidance and controllable motion synthesis.</li>
</ul>

<h3>Title: Stylecodes: Encoding Stylistic Information For Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ciara Rowles</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12811">https://arxiv.org/abs/2411.12811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12811">https://arxiv.org/pdf/2411.12811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12811]] Stylecodes: Encoding Stylistic Information For Image Generation(https://arxiv.org/abs/2411.12811)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel in image generation, but controlling them remains a challenge. We focus on the problem of style-conditioned image generation. Although example images work, they are cumbersome: srefs (style-reference codes) from MidJourney solve this issue by expressing a specific image style in a short numeric code. These have seen widespread adoption throughout social media due to both their ease of sharing and the fact they allow using an image for style control, without having to post the source images themselves. However, users are not able to generate srefs from their own images, nor is the underlying training procedure public. We propose StyleCodes: an open-source and open-research style encoder architecture and training procedure to express image style as a 20-symbol base64 code. Our experiments show that our encoding results in minimal loss in quality compared to traditional image-to-style techniques.</li>
</ul>

<h3>Title: Generalized Prompt Tuning: Adapting Frozen Univariate Time Series Foundation Models for Multivariate Healthcare Time Series</h3>
<ul>
<li><strong>Authors: </strong>Mingzhu Liu, Angela H. Chen, George H. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12824">https://arxiv.org/abs/2411.12824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12824">https://arxiv.org/pdf/2411.12824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12824]] Generalized Prompt Tuning: Adapting Frozen Univariate Time Series Foundation Models for Multivariate Healthcare Time Series(https://arxiv.org/abs/2411.12824)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series foundation models are pre-trained on large datasets and are able to achieve state-of-the-art performance in diverse tasks. However, to date, there has been limited work demonstrating how well these models perform in medical applications, where labeled data can be scarce. Further, we observe that currently, the majority of time series foundation models either are univariate in nature, or assume channel independence, meaning that they handle multivariate time series but do not model how the different variables relate. In this paper, we propose a prompt-tuning-inspired fine-tuning technique, Generalized Prompt Tuning (Gen-P-Tuning), that enables us to adapt an existing univariate time series foundation model (treated as frozen) to handle multivariate time series prediction. Our approach provides a way to combine information across channels (variables) of multivariate time series. We demonstrate the effectiveness of our fine-tuning approach against various baselines on two MIMIC classification tasks, and on influenza-like illness forecasting.</li>
</ul>

<h3>Title: Towards motion from video diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Paul Janson, Tiberiu Popa, Eugene Belilovsky</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12831">https://arxiv.org/abs/2411.12831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12831">https://arxiv.org/pdf/2411.12831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12831]] Towards motion from video diffusion models(https://arxiv.org/abs/2411.12831)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-conditioned video diffusion models have emerged as a powerful tool in the realm of video generation and editing. But their ability to capture the nuances of human movement remains under-explored. Indeed the ability of these models to faithfully model an array of text prompts can lead to a wide host of applications in human and character animation. In this work, we take initial steps to investigate whether these models can effectively guide the synthesis of realistic human body animations. Specifically we propose to synthesize human motion by deforming an SMPL-X body representation guided by Score distillation sampling (SDS) calculated using a video diffusion model. By analyzing the fidelity of the resulting animations, we gain insights into the extent to which we can obtain motion using publicly available text-to-video diffusion models using SDS. Our findings shed light on the potential and limitations of these models for generating diverse and plausible human motions, paving the way for further research in this exciting area.</li>
</ul>

<h3>Title: HyperGAN-CLIP: A Unified Framework for Domain Adaptation, Image Synthesis and Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Abdul Basit Anees, Ahmet Canberk Baykal, Muhammed Burak Kizil, Duygu Ceylan, Erkut Erdem, Aykut Erdem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12832">https://arxiv.org/abs/2411.12832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12832">https://arxiv.org/pdf/2411.12832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12832]] HyperGAN-CLIP: A Unified Framework for Domain Adaptation, Image Synthesis and Manipulation(https://arxiv.org/abs/2411.12832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs), particularly StyleGAN and its variants, have demonstrated remarkable capabilities in generating highly realistic images. Despite their success, adapting these models to diverse tasks such as domain adaptation, reference-guided synthesis, and text-guided manipulation with limited training data remains challenging. Towards this end, in this study, we present a novel framework that significantly extends the capabilities of a pre-trained StyleGAN by integrating CLIP space via hypernetworks. This integration allows dynamic adaptation of StyleGAN to new domains defined by reference images or textual descriptions. Additionally, we introduce a CLIP-guided discriminator that enhances the alignment between generated images and target domains, ensuring superior image quality. Our approach demonstrates unprecedented flexibility, enabling text-guided image manipulation without the need for text-specific training data and facilitating seamless style transfer. Comprehensive qualitative and quantitative evaluations confirm the robustness and superior performance of our framework compared to existing methods.</li>
</ul>

<h3>Title: Data-to-Model Distillation: Data-Efficient Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Sajedi, Samir Khaki, Lucy Z. Liu, Ehsan Amjadian, Yuri A. Lawryshyn, Konstantinos N. Plataniotis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12841">https://arxiv.org/abs/2411.12841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12841">https://arxiv.org/pdf/2411.12841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12841]] Data-to-Model Distillation: Data-Efficient Learning Framework(https://arxiv.org/abs/2411.12841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to distill the knowledge of a large-scale real dataset into small yet informative synthetic data such that a model trained on it performs as well as a model trained on the full dataset. Despite recent progress, existing dataset distillation methods often struggle with computational efficiency, scalability to complex high-resolution datasets, and generalizability to deep architectures. These approaches typically require retraining when the distillation ratio changes, as knowledge is embedded in raw pixels. In this paper, we propose a novel framework called Data-to-Model Distillation (D2M) to distill the real dataset's knowledge into the learnable parameters of a pre-trained generative model by aligning rich representations extracted from real and generated images. The learned generative model can then produce informative training images for different distillation ratios and deep architectures. Extensive experiments on 15 datasets of varying resolutions show D2M's superior performance, re-distillation efficiency, and cross-architecture generalizability. Our method effectively scales up to high-resolution 128x128 ImageNet-1K. Furthermore, we verify D2M's practical benefits for downstream applications in neural architecture search.</li>
</ul>

<h3>Title: CDI: Copyrighted Data Identification in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jan Dubiński, Antoni Kowalczuk, Franziska Boenisch, Adam Dziedzic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12858">https://arxiv.org/abs/2411.12858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12858">https://arxiv.org/pdf/2411.12858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12858]] CDI: Copyrighted Data Identification in Diffusion Models(https://arxiv.org/abs/2411.12858)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Models (DMs) benefit from large and diverse datasets for their training. Since this data is often scraped from the Internet without permission from the data owners, this raises concerns about copyright and intellectual property protections. While (illicit) use of data is easily detected for training samples perfectly re-created by a DM at inference time, it is much harder for data owners to verify if their data was used for training when the outputs from the suspect DM are not close replicas. Conceptually, membership inference attacks (MIAs), which detect if a given data point was used during training, present themselves as a suitable tool to address this challenge. However, we demonstrate that existing MIAs are not strong enough to reliably determine the membership of individual images in large, state-of-the-art DMs. To overcome this limitation, we propose CDI, a framework for data owners to identify whether their dataset was used to train a given DM. CDI relies on dataset inference techniques, i.e., instead of using the membership signal from a single data point, CDI leverages the fact that most data owners, such as providers of stock photography, visual media companies, or even individual artists, own datasets with multiple publicly exposed data points which might all be included in the training of a given DM. By selectively aggregating signals from existing MIAs and using new handcrafted methods to extract features for these datasets, feeding them to a scoring model, and applying rigorous statistical testing, CDI allows data owners with as little as 70 data points to identify with a confidence of more than 99% whether their data was used to train a given DM. Thereby, CDI represents a valuable tool for data owners to claim illegitimate use of their copyrighted data.</li>
</ul>

<h3>Title: From Text to Pose to Image: Improving Diffusion Model Control and Quality</h3>
<ul>
<li><strong>Authors: </strong>Clément Bonnett, Ariel N. Lee, Franck Wertel, Antoine Tamano, Tanguy Cizain, Pablo Ducru</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12872">https://arxiv.org/abs/2411.12872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12872">https://arxiv.org/pdf/2411.12872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12872]] From Text to Pose to Image: Improving Diffusion Model Control and Quality(https://arxiv.org/abs/2411.12872)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In the last two years, text-to-image diffusion models have become extremely popular. As their quality and usage increase, a major concern has been the need for better output control. In addition to prompt engineering, one effective method to improve the controllability of diffusion models has been to condition them on additional modalities such as image style, depth map, or keypoints. This forms the basis of ControlNets or Adapters. When attempting to apply these methods to control human poses in outputs of text-to-image diffusion models, two main challenges have arisen. The first challenge is generating poses following a wide range of semantic text descriptions, for which previous methods involved searching for a pose within a dataset of (caption, pose) pairs. The second challenge is conditioning image generation on a specified pose while keeping both high aesthetic and high pose fidelity. In this article, we fix these two main issues by introducing a text-to-pose (T2P) generative model alongside a new sampling algorithm, and a new pose adapter that incorporates more pose keypoints for higher pose fidelity. Together, these two new state-of-the-art models enable, for the first time, a generative text-to-pose-to-image framework for higher pose control in diffusion models. We release all models and the code used for the experiments at this https URL.</li>
</ul>

<h3>Title: A Foundation Model for Unified Urban Spatio-Temporal Flow Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yuan Yuan, Jingtao Ding, Chonghua Han, Depeng Jin, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.12972">https://arxiv.org/abs/2411.12972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.12972">https://arxiv.org/pdf/2411.12972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.12972]] A Foundation Model for Unified Urban Spatio-Temporal Flow Prediction(https://arxiv.org/abs/2411.12972)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Urban spatio-temporal flow prediction, encompassing traffic flows and crowd flows, is crucial for optimizing city infrastructure and managing traffic and emergency responses. Traditional approaches have relied on separate models tailored to either grid-based data, representing cities as uniform cells, or graph-based data, modeling cities as networks of nodes and edges. In this paper, we build UniFlow, a foundational model for general urban flow prediction that unifies both grid-based and graphbased data. We first design a multi-view spatio-temporal patching mechanism to standardize different data into a consistent sequential format and then introduce a spatio-temporal transformer architecture to capture complex correlations and dynamics. To leverage shared spatio-temporal patterns across different data types and facilitate effective cross-learning, we propose SpatioTemporal Memory Retrieval Augmentation (ST-MRA). By creating structured memory modules to store shared spatio-temporal patterns, ST-MRA enhances predictions through adaptive memory retrieval. Extensive experiments demonstrate that UniFlow outperforms existing models in both grid-based and graph-based flow prediction, excelling particularly in scenarios with limited data availability, showcasing its superior performance and broad applicability. The datasets and code implementation have been released on this https URL.</li>
</ul>

<h3>Title: Evaluating LLMs Capabilities Towards Understanding Social Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Anique Tahir, Lu Cheng, Manuel Sandoval, Yasin N. Silva, Deborah L. Hall, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13008">https://arxiv.org/abs/2411.13008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13008">https://arxiv.org/pdf/2411.13008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13008]] Evaluating LLMs Capabilities Towards Understanding Social Dynamics(https://arxiv.org/abs/2411.13008)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Social media discourse involves people from different backgrounds, beliefs, and motives. Thus, often such discourse can devolve into toxic interactions. Generative Models, such as Llama and ChatGPT, have recently exploded in popularity due to their capabilities in zero-shot question-answering. Because these models are increasingly being used to ask questions of social significance, a crucial research question is whether they can understand social media dynamics. This work provides a critical analysis regarding generative LLM's ability to understand language and dynamics in social contexts, particularly considering cyberbullying and anti-cyberbullying (posts aimed at reducing cyberbullying) interactions. Specifically, we compare and contrast the capabilities of different large language models (LLMs) to understand three key aspects of social dynamics: language, directionality, and the occurrence of bullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit promising results in some social media understanding tasks (understanding directionality), they presented mixed results in others (proper paraphrasing and bullying/anti-bullying detection). We also found that fine-tuning and prompt engineering mechanisms can have positive effects in some tasks. We believe that a understanding of LLM's capabilities is crucial to design future models that can be effectively used in social applications.</li>
</ul>

<h3>Title: Improving OOD Generalization of Pre-trained Encoders via Aligned Embedding-Space Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Shuman Peng, Arash Khoeini, Sharan Vaswani, Martin Ester</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13073">https://arxiv.org/abs/2411.13073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13073">https://arxiv.org/pdf/2411.13073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13073]] Improving OOD Generalization of Pre-trained Encoders via Aligned Embedding-Space Ensembles(https://arxiv.org/abs/2411.13073)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The quality of self-supervised pre-trained embeddings on out-of-distribution (OOD) data is poor without fine-tuning. A straightforward and simple approach to improving the generalization of pre-trained representation to OOD data is the use of deep ensembles. However, obtaining an effective ensemble in the embedding space with only unlabeled data remains an unsolved problem. We first perform a theoretical analysis that reveals the relationship between individual hyperspherical embedding spaces in an ensemble. We then design a principled method to align these embedding spaces in an unsupervised manner. Experimental results on the MNIST dataset show that our embedding-space ensemble method improves pre-trained embedding quality on in-distribution and OOD data compared to single encoders.</li>
</ul>

<h3>Title: Practical Compact Deep Compressed Sensing</h3>
<ul>
<li><strong>Authors: </strong>Bin Chen, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13081">https://arxiv.org/abs/2411.13081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13081">https://arxiv.org/pdf/2411.13081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13081]] Practical Compact Deep Compressed Sensing(https://arxiv.org/abs/2411.13081)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed the success of deep networks in compressed sensing (CS), which allows for a significant reduction in sampling cost and has gained growing attention since its inception. In this paper, we propose a new practical and compact network dubbed PCNet for general image CS. Specifically, in PCNet, a novel collaborative sampling operator is designed, which consists of a deep conditional filtering step and a dual-branch fast sampling step. The former learns an implicit representation of a linear transformation matrix into a few convolutions and first performs adaptive local filtering on the input image, while the latter then uses a discrete cosine transform and a scrambled block-diagonal Gaussian matrix to generate under-sampled measurements. Our PCNet is equipped with an enhanced proximal gradient descent algorithm-unrolled network for reconstruction. It offers flexibility, interpretability, and strong recovery performance for arbitrary sampling rates once trained. Additionally, we provide a deployment-oriented extraction scheme for single-pixel CS imaging systems, which allows for the convenient conversion of any linear sampling operator to its matrix form to be loaded onto hardware like digital micro-mirror devices. Extensive experiments on natural image CS, quantized CS, and self-supervised CS demonstrate the superior reconstruction accuracy and generalization ability of PCNet compared to existing state-of-the-art methods, particularly for high-resolution images. Code is available at this https URL.</li>
</ul>

<h3>Title: Virtual Staining of Label-Free Tissue in Imaging Mass Spectrometry</h3>
<ul>
<li><strong>Authors: </strong>Yijie Zhang, Luzhe Huang, Nir Pillar, Yuzhu Li, Lukasz G. Migas, Raf Van de Plas, Jeffrey M. Spraggins, Aydogan Ozcan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, physics.med-ph, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13120">https://arxiv.org/abs/2411.13120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13120">https://arxiv.org/pdf/2411.13120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13120]] Virtual Staining of Label-Free Tissue in Imaging Mass Spectrometry(https://arxiv.org/abs/2411.13120)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Imaging mass spectrometry (IMS) is a powerful tool for untargeted, highly multiplexed molecular mapping of tissue in biomedical research. IMS offers a means of mapping the spatial distributions of molecular species in biological tissue with unparalleled chemical specificity and sensitivity. However, most IMS platforms are not able to achieve microscopy-level spatial resolution and lack cellular morphological contrast, necessitating subsequent histochemical staining, microscopic imaging and advanced image registration steps to enable molecular distributions to be linked to specific tissue features and cell types. Here, we present a virtual histological staining approach that enhances spatial resolution and digitally introduces cellular morphological contrast into mass spectrometry images of label-free human tissue using a diffusion model. Blind testing on human kidney tissue demonstrated that the virtually stained images of label-free samples closely match their histochemically stained counterparts (with Periodic Acid-Schiff staining), showing high concordance in identifying key renal pathology structures despite utilizing IMS data with 10-fold larger pixel size. Additionally, our approach employs an optimized noise sampling technique during the diffusion model's inference process to reduce variance in the generated images, yielding reliable and repeatable virtual staining. We believe this virtual staining method will significantly expand the applicability of IMS in life sciences and open new avenues for mass spectrometry-based biomedical research.</li>
</ul>

<h3>Title: Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Junliang Xing, Lei Jin, Congyan Lang, Pin Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13127">https://arxiv.org/abs/2411.13127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13127">https://arxiv.org/pdf/2411.13127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13127]] Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images(https://arxiv.org/abs/2411.13127)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Cloud segmentation is a critical challenge in remote sensing image interpretation, as its accuracy directly impacts the effectiveness of subsequent data processing and analysis. Recently, vision foundation models (VFM) have demonstrated powerful generalization capabilities across various visual tasks. In this paper, we present a parameter-efficient adaptive approach, termed Cloud-Adapter, designed to enhance the accuracy and robustness of cloud segmentation. Our method leverages a VFM pretrained on general domain data, which remains frozen, eliminating the need for additional training. Cloud-Adapter incorporates a lightweight spatial perception module that initially utilizes a convolutional neural network (ConvNet) to extract dense spatial representations. These multi-scale features are then aggregated and serve as contextual inputs to an adapting module, which modulates the frozen transformer layers within the VFM. Experimental results demonstrate that the Cloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the frozen backbone, achieves substantial performance gains. Cloud-Adapter consistently attains state-of-the-art (SOTA) performance across a wide variety of cloud segmentation datasets from multiple satellite sources, sensor series, data processing levels, land cover scenarios, and annotation granularities. We have released the source code and pretrained models at this https URL to support further research.</li>
</ul>

<h3>Title: CopyrightMeter: Revisiting Copyright Protection in Text-to-image Models</h3>
<ul>
<li><strong>Authors: </strong>Naen Xu, Changjiang Li, Tianyu Du, Minxi Li, Wenjie Luo, Jiacheng Liang, Yuyuan Li, Xuhong Zhang, Meng Han, Jianwei Yin, Ting Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13144">https://arxiv.org/abs/2411.13144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13144">https://arxiv.org/pdf/2411.13144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13144]] CopyrightMeter: Revisiting Copyright Protection in Text-to-image Models(https://arxiv.org/abs/2411.13144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have emerged as powerful tools for generating high-quality images from textual descriptions. However, their increasing popularity has raised significant copyright concerns, as these models can be misused to reproduce copyrighted content without authorization. In response, recent studies have proposed various copyright protection methods, including adversarial perturbation, concept erasure, and watermarking techniques. However, their effectiveness and robustness against advanced attacks remain largely unexplored. Moreover, the lack of unified evaluation frameworks has hindered systematic comparison and fair assessment of different approaches. To bridge this gap, we systematize existing copyright protection methods and attacks, providing a unified taxonomy of their design spaces. We then develop CopyrightMeter, a unified evaluation framework that incorporates 17 state-of-the-art protections and 16 representative attacks. Leveraging CopyrightMeter, we comprehensively evaluate protection methods across multiple dimensions, thereby uncovering how different design choices impact fidelity, efficacy, and resilience under attacks. Our analysis reveals several key findings: (i) most protections (16/17) are not resilient against attacks; (ii) the "best" protection varies depending on the target priority; (iii) more advanced attacks significantly promote the upgrading of protections. These insights provide concrete guidance for developing more robust protection methods, while its unified evaluation protocol establishes a standard benchmark for future copyright protection research in text-to-image generation.</li>
</ul>

<h3>Title: RAW-Diffusion: RGB-Guided Diffusion Models for High-Fidelity RAW Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Christoph Reinders, Radu Berdan, Beril Besbinar, Junji Otsuka, Daisuke Iso</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13150">https://arxiv.org/abs/2411.13150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13150">https://arxiv.org/pdf/2411.13150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13150]] RAW-Diffusion: RGB-Guided Diffusion Models for High-Fidelity RAW Image Generation(https://arxiv.org/abs/2411.13150)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current deep learning approaches in computer vision primarily focus on RGB data sacrificing information. In contrast, RAW images offer richer representation, which is crucial for precise recognition, particularly in challenging conditions like low-light environments. The resultant demand for comprehensive RAW image datasets contrasts with the labor-intensive process of creating specific datasets for individual sensors. To address this, we propose a novel diffusion-based method for generating RAW images guided by RGB images. Our approach integrates an RGB-guidance module for feature extraction from RGB inputs, then incorporates these features into the reverse diffusion process with RGB-guided residual blocks across various resolutions. This approach yields high-fidelity RAW images, enabling the creation of camera-specific RAW datasets. Our RGB2RAW experiments on four DSLR datasets demonstrate state-of-the-art performance. Moreover, RAW-Diffusion demonstrates exceptional data efficiency, achieving remarkable performance with as few as 25 training samples or even fewer. We extend our method to create BDD100K-RAW and Cityscapes-RAW datasets, revealing its effectiveness for object detection in RAW imagery, significantly reducing the amount of required RAW images.</li>
</ul>

<h3>Title: Long-term Detection System for Six Kinds of Abnormal Behavior of the Elderly Living Alone</h3>
<ul>
<li><strong>Authors: </strong>Kai Tanaka, Mineichi Kudo, Keigo Kimura, Atsuyoshi Nakamura</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13153">https://arxiv.org/abs/2411.13153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13153">https://arxiv.org/pdf/2411.13153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13153]] Long-term Detection System for Six Kinds of Abnormal Behavior of the Elderly Living Alone(https://arxiv.org/abs/2411.13153)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The proportion of elderly people is increasing worldwide, particularly those living alone in Japan. As elderly people get older, their risks of physical disabilities and health issues increase. To automatically discover these issues at a low cost in daily life, sensor-based detection in a smart home is promising. As part of the effort towards early detection of abnormal behaviors, we propose a simulator-based detection systems for six typical anomalies: being semi-bedridden, being housebound, forgetting, wandering, fall while walking and fall while standing. Our detection system can be customized for various room layout, sensor arrangement and resident's characteristics by training detection classifiers using the simulator with the parameters fitted to individual cases. Considering that the six anomalies that our system detects have various occurrence durations, such as being housebound for weeks or lying still for seconds after a fall, the detection classifiers of our system produce anomaly labels depending on each anomaly's occurrence duration, e.g., housebound per day and falls per second. We propose a method that standardizes the processing of sensor data, and uses a simple detection approach. Although the validity depends on the realism of the simulation, numerical evaluations using sensor data that includes a variety of resident behavior patterns over nine years as test data show that (1) the methods for detecting wandering and falls are comparable to previous methods, and (2) the methods for detecting being semi-bedridden, being housebound, and forgetting achieve a sensitivity of over 0.9 with fewer than one false alarm every 50 days.</li>
</ul>

<h3>Title: BIPro: Zero-shot Chinese Poem Generation via Block Inverse Prompting Constrained Generation Framework</h3>
<ul>
<li><strong>Authors: </strong>Xu Zou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13237">https://arxiv.org/abs/2411.13237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13237">https://arxiv.org/pdf/2411.13237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13237]] BIPro: Zero-shot Chinese Poem Generation via Block Inverse Prompting Constrained Generation Framework(https://arxiv.org/abs/2411.13237)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, generative pre-trained models have made significant strides, particularly highlighted by the release of ChatGPT and GPT-4, which exhibit superior cross-domain capabilities. However, these models still face challenges on constrained writing tasks like poem generation under open-domain titles. In response to this challenge, we introduce Block Inverse Prompting (BIPro) constrained generation framework. BIPro leverages two block inverse prompting methods, revise and rewrite, that mimic the process of human text writing using block generative models. It significantly improves the zero-shot generation quality on the formidable constrained generation task of open-domain traditional-form Chinese poem generation. Based on a less powerful block generative model GLM-10B-Chinese, poems composed via BIPro without priming or additional training outperform both most advanced direct generative systems like GPT-4 or GLM-4 and best domain-specific systems such as Yusheng, Shisanbai, or Baidu Poetry Helper in human evaluation by proficient poets. Finally, BIPro considerably narrows the gap between AI-generated works and short-listed human literary arts in another human evaluation, unveiling the promising potential of block generative models in improving the quality of constrained generation.</li>
</ul>

<h3>Title: XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Wang, Yanbo Wang, Xumin Yu, Jie Zhou, Jiwen Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13243">https://arxiv.org/abs/2411.13243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13243">https://arxiv.org/pdf/2411.13243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13243]] XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation(https://arxiv.org/abs/2411.13243)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing methodologies in open vocabulary 3D semantic segmentation primarily concentrate on establishing a unified feature space encompassing 3D, 2D, and textual modalities. Nevertheless, traditional techniques such as global feature alignment or vision-language model distillation tend to impose only approximate correspondence, struggling notably with delineating fine-grained segmentation boundaries. To address this gap, we propose a more meticulous mask-level alignment between 3D features and the 2D-text embedding space through a cross-modal mask reasoning framework, XMask3D. In our approach, we developed a mask generator based on the denoising UNet from a pre-trained diffusion model, leveraging its capability for precise textual control over dense pixel representations and enhancing the open-world adaptability of the generated masks. We further integrate 3D global features as implicit conditions into the pre-trained 2D denoising UNet, enabling the generation of segmentation masks with additional 3D geometry awareness. Subsequently, the generated 2D masks are employed to align mask-level 3D representations with the vision-language feature space, thereby augmenting the open vocabulary capability of 3D geometry embeddings. Finally, we fuse complementary 2D and 3D mask features, resulting in competitive performance across multiple benchmarks for 3D open vocabulary semantic segmentation. Code is available at this https URL.</li>
</ul>

<h3>Title: Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Zhibo Chu, Zichong Wang, Qitao Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13244">https://arxiv.org/abs/2411.13244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13244">https://arxiv.org/pdf/2411.13244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13244]] Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for Text-to-SQL(https://arxiv.org/abs/2411.13244)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive problem-solving skills across many tasks, but they still underperform compared to humans in various downstream applications, such as text-to-SQL. On the BIRD benchmark leaderboard, human performance achieves an accuracy of 92.96\%, whereas the top-performing method reaches only 72.39\%. Notably, these state-of-the-art (SoTA) methods predominantly rely on in-context learning to simulate human-like reasoning. However, they overlook a critical human skill: continual learning. Inspired by the educational practice of maintaining mistake notebooks during our formative years, we propose LPE-SQL (Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework designed to augment LLMs by enabling continual learning without requiring parameter fine-tuning. LPE-SQL consists of four modules that \textbf{i)} retrieve relevant entries, \textbf{ii)} efficient sql generation, \textbf{iii)} generate the final result through a cross-consistency mechanism and \textbf{iv)} log successful and failed tasks along with their reasoning processes or reflection-generated tips. Importantly, the core module of LPE-SQL is the fourth one, while the other modules employ foundational methods, allowing LPE-SQL to be easily integrated with SoTA technologies to further enhance performance. Our experimental results demonstrate that this continual learning approach yields substantial performance gains, with the smaller Llama-3.1-70B model with surpassing the performance of the larger Llama-3.1-405B model using SoTA methods.</li>
</ul>

<h3>Title: Teaching VLMs to Localize Specific Objects from In-context Examples</h3>
<ul>
<li><strong>Authors: </strong>Sivan Doveh, Nimrod Shabtay, Wei Lin, Eli Schwartz, Hilde Kuehne, Raja Giryes, Rogerio Feris, Leonid Karlinsky, James Glass, Assaf Arbelle, Shimon Ullman, M. Jehanzeb Mirza</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13317">https://arxiv.org/abs/2411.13317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13317">https://arxiv.org/pdf/2411.13317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13317]] Teaching VLMs to Localize Specific Objects from In-context Examples(https://arxiv.org/abs/2411.13317)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have shown remarkable capabilities across diverse visual tasks, including image recognition, video understanding, and Visual Question Answering (VQA) when explicitly trained for these tasks. Despite these advances, we find that current VLMs lack a fundamental cognitive ability: learning to localize objects in a scene by taking into account the context. In this work, we focus on the task of few-shot personalized localization, where a model is given a small set of annotated images (in-context examples) -- each with a category label and bounding box -- and is tasked with localizing the same object type in a query image. To provoke personalized localization abilities in models, we present a data-centric solution that fine-tunes them using carefully curated data from video object tracking datasets. By leveraging sequences of frames tracking the same object across multiple shots, we simulate instruction-tuning dialogues that promote context awareness. To reinforce this, we introduce a novel regularization technique that replaces object labels with pseudo-names, ensuring the model relies on visual context rather than prior knowledge. Our method significantly enhances few-shot localization performance without sacrificing generalization, as demonstrated on several benchmarks tailored to personalized localization. This work is the first to explore and benchmark personalized few-shot localization for VLMs, laying a foundation for future research in context-driven vision-language applications. The code for our project is available at this https URL</li>
</ul>

<h3>Title: Vertical Validation: Evaluating Implicit Generative Models for Graphs on Thin Support Regions</h3>
<ul>
<li><strong>Authors: </strong>Mai Elkady, Thu Bui, Bruno Ribeiro, David I. Inouye</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13358">https://arxiv.org/abs/2411.13358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13358">https://arxiv.org/pdf/2411.13358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13358]] Vertical Validation: Evaluating Implicit Generative Models for Graphs on Thin Support Regions(https://arxiv.org/abs/2411.13358)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>There has been a growing excitement that implicit graph generative models could be used to design or discover new molecules for medicine or material design. Because these molecules have not been discovered, they naturally lie in unexplored or scarcely supported regions of the distribution of known molecules. However, prior evaluation methods for implicit graph generative models have focused on validating statistics computed from the thick support (e.g., mean and variance of a graph property). Therefore, there is a mismatch between the goal of generating novel graphs and the evaluation methods. To address this evaluation gap, we design a novel evaluation method called Vertical Validation (VV) that systematically creates thin support regions during the train-test splitting procedure and then reweights generated samples so that they can be compared to the held-out test data. This procedure can be seen as a generalization of the standard train-test procedure except that the splits are dependent on sample features. We demonstrate that our method can be used to perform model selection if performance on thin support regions is the desired goal. As a side benefit, we also show that our approach can better detect overfitting as exemplified by memorization.</li>
</ul>

<h3>Title: WaterPark: A Robustness Assessment of Language Model Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liang, Zian Wang, Lauren Hong, Shouling Ji, Ting Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13425">https://arxiv.org/abs/2411.13425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13425">https://arxiv.org/pdf/2411.13425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13425]] WaterPark: A Robustness Assessment of Language Model Watermarking(https://arxiv.org/abs/2411.13425)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To mitigate the misuse of large language models (LLMs), such as disinformation, automated phishing, and academic cheating, there is a pressing need for the capability of identifying LLM-generated texts. Watermarking emerges as one promising solution: it plants statistical signals into LLMs' generative processes and subsequently verifies whether LLMs produce given texts. Various watermarking methods (``watermarkers'') have been proposed; yet, due to the lack of unified evaluation platforms, many critical questions remain under-explored: i) What are the strengths/limitations of various watermarkers, especially their attack robustness? ii) How do various design choices impact their robustness? iii) How to optimally operate watermarkers in adversarial environments? To fill this gap, we systematize existing LLM watermarkers and watermark removal attacks, mapping out their design spaces. We then develop WaterPark, a unified platform that integrates 10 state-of-the-art watermarkers and 12 representative attacks. More importantly, leveraging WaterPark, we conduct a comprehensive assessment of existing watermarkers, unveiling the impact of various design choices on their attack robustness. For instance, a watermarker's resilience to increasingly intensive attacks hinges on its context dependency. We further explore the best practices to operate watermarkers in adversarial environments. For instance, using a generic detector alongside a watermark-specific detector improves the security of vulnerable watermarkers. We believe our study sheds light on current LLM watermarking techniques while WaterPark serves as a valuable testbed to facilitate future research.</li>
</ul>

<h3>Title: LIMBA: An Open-Source Framework for the Preservation and Valorization of Low-Resource Languages using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Salvatore Mario Carta, Stefano Chessa, Giulia Contu, Andrea Corriga, Andrea Deidda, Gianni Fenu, Luca Frigau, Alessandro Giuliani, Luca Grassi, Marco Manolo Manca, Mirko Marras, Francesco Mola, Bastianino Mossa, Piergiorgio Mura, Marco Ortu, Leonardo Piano, Simone Pisano, Alessia Pisu, Alessandro Sebastian Podda, Livio Pompianu, Simone Seu, Sandro Gabriele Tiddia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13453">https://arxiv.org/abs/2411.13453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13453">https://arxiv.org/pdf/2411.13453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13453]] LIMBA: An Open-Source Framework for the Preservation and Valorization of Low-Resource Languages using Generative Models(https://arxiv.org/abs/2411.13453)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Minority languages are vital to preserving cultural heritage, yet they face growing risks of extinction due to limited digital resources and the dominance of artificial intelligence models trained on high-resource languages. This white paper proposes a framework to generate linguistic tools for low-resource languages, focusing on data creation to support the development of language models that can aid in preservation efforts. Sardinian, an endangered language, serves as the case study to demonstrate the framework's effectiveness. By addressing the data scarcity that hinders intelligent applications for such languages, we contribute to promoting linguistic diversity and support ongoing efforts in language standardization and revitalization through modern technologies.</li>
</ul>

<h3>Title: VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13503">https://arxiv.org/abs/2411.13503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13503">https://arxiv.org/pdf/2411.13503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13503]] VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models(https://arxiv.org/abs/2411.13503)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects "video generation quality" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has several appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). The evaluation metrics with fine-grained levels reveal individual models' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models' ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. 4) Versatile Benchmarking: VBench++ supports evaluating text-to-video and image-to-video. We introduce a high-quality Image Suite with an adaptive aspect ratio to enable fair evaluations across different image-to-video generation settings. Beyond assessing technical quality, VBench++ evaluates the trustworthiness of video generative models, providing a more holistic view of model performance. 5) Full Open-Sourcing: We fully open-source VBench++ and continually add new video generation models to our leaderboard to drive forward the field of video generation.</li>
</ul>

<h3>Title: Identity Preserving 3D Head Stylization with Multiview Score Distillation</h3>
<ul>
<li><strong>Authors: </strong>Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Furkan Guzelant, Aysegul Dundar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13536">https://arxiv.org/abs/2411.13536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13536">https://arxiv.org/pdf/2411.13536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13536]] Identity Preserving 3D Head Stylization with Multiview Score Distillation(https://arxiv.org/abs/2411.13536)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the this https URL for more visuals.</li>
</ul>

<h3>Title: Promoting User Data Autonomy During the Dissolution of a Monopolistic Firm</h3>
<ul>
<li><strong>Authors: </strong>Rushabh Solanki, Elliot Creager</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13546">https://arxiv.org/abs/2411.13546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13546">https://arxiv.org/pdf/2411.13546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13546]] Promoting User Data Autonomy During the Dissolution of a Monopolistic Firm(https://arxiv.org/abs/2411.13546)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The deployment of AI in consumer products is currently focused on the use of so-called foundation models, large neural networks pre-trained on massive corpora of digital records. This emphasis on scaling up datasets and pre-training computation raises the risk of further consolidating the industry, and enabling monopolistic (or oligopolistic) behavior. Judges and regulators seeking to improve market competition may employ various remedies. This paper explores dissolution -- the breaking up of a monopolistic entity into smaller firms -- as one such remedy, focusing in particular on the technical challenges and opportunities involved in the breaking up of large models and datasets. We show how the framework of Conscious Data Contribution can enable user autonomy during under dissolution. Through a simulation study, we explore how fine-tuning and the phenomenon of "catastrophic forgetting" could actually prove beneficial as a type of machine unlearning that allows users to specify which data they want used for what purposes.</li>
</ul>

<h3>Title: HF-Diff: High-Frequency Perceptual Loss and Distribution Matching for One-Step Diffusion-Based Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Shoaib Meraj Sami, Md Mahedi Hasan, Jeremy Dawson, Nasser Nasrabadi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13548">https://arxiv.org/abs/2411.13548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13548">https://arxiv.org/pdf/2411.13548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13548]] HF-Diff: High-Frequency Perceptual Loss and Distribution Matching for One-Step Diffusion-Based Image Super-Resolution(https://arxiv.org/abs/2411.13548)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although recent diffusion-based single-step super-resolution methods achieve better performance as compared to SinSR, they are computationally complex. To improve the performance of SinSR, we investigate preserving the high-frequency detail features during super-resolution (SR) because the downgraded images lack detailed information. For this purpose, we introduce a high-frequency perceptual loss by utilizing an invertible neural network (INN) pretrained on the ImageNet dataset. Different feature maps of pretrained INN produce different high-frequency aspects of an image. During the training phase, we impose to preserve the high-frequency features of super-resolved and ground truth (GT) images that improve the SR image quality during inference. Furthermore, we also utilize the Jenson-Shannon divergence between GT and SR images in the pretrained DINO-v2 embedding space to match their distribution. By introducing the $\textbf{h}igh$- $\textbf{f}requency$ preserving loss and distribution matching constraint in the single-step $\textbf{diff}usion-based$ SR ($\textbf{HF-Diff}$), we achieve a state-of-the-art CLIPIQA score in the benchmark RealSR, RealSet65, DIV2K-Val, and ImageNet datasets. Furthermore, the experimental results in several datasets demonstrate that our high-frequency perceptual loss yields better SR image quality than LPIPS and VGG-based perceptual losses. Our code will be released at this https URL.</li>
</ul>

<h3>Title: Generating 3D-Consistent Videos from Unposed Internet Photos</h3>
<ul>
<li><strong>Authors: </strong>Gene Chou, Kai Zhang, Sai Bi, Hao Tan, Zexiang Xu, Fujun Luan, Bharath Hariharan, Noah Snavely</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13549">https://arxiv.org/abs/2411.13549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13549">https://arxiv.org/pdf/2411.13549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13549]] Generating 3D-Consistent Videos from Unposed Internet Photos(https://arxiv.org/abs/2411.13549)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We address the problem of generating videos from unposed internet photos. A handful of input images serve as keyframes, and our model interpolates between them to simulate a path moving between the cameras. Given random images, a model's ability to capture underlying geometry, recognize scene identity, and relate frames in terms of camera position and orientation reflects a fundamental understanding of 3D structure and scene layout. However, existing video models such as Luma Dream Machine fail at this task. We design a self-supervised method that takes advantage of the consistency of videos and variability of multiview internet photos to train a scalable, 3D-aware video model without any 3D annotations such as camera parameters. We validate that our method outperforms all baselines in terms of geometric and appearance consistency. We also show our model benefits applications that enable camera control, such as 3D Gaussian Splatting. Our results suggest that we can scale up scene-level 3D learning using only 2D data such as videos and multiview internet photos.</li>
</ul>

<h3>Title: Find Any Part in 3D</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Ma, Yisong Yue, Georgia Gkioxari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13550">https://arxiv.org/abs/2411.13550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13550">https://arxiv.org/pdf/2411.13550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13550]] Find Any Part in 3D(https://arxiv.org/abs/2411.13550)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We study open-world part segmentation in 3D: segmenting any part in any object based on any text query. Prior methods are limited in object categories and part vocabularies. Recent advances in AI have demonstrated effective open-world recognition capabilities in 2D. Inspired by this progress, we propose an open-world, direct-prediction model for 3D part segmentation that can be applied zero-shot to any object. Our approach, called Find3D, trains a general-category point embedding model on large-scale 3D assets from the internet without any human annotation. It combines a data engine, powered by foundation models for annotating data, with a contrastive training method. We achieve strong performance and generalization across multiple datasets, with up to a 3x improvement in mIoU over the next best method. Our model is 6x to over 300x faster than existing baselines. To encourage research in general-category open-world 3D part segmentation, we also release a benchmark for general objects and parts. Project website: this https URL</li>
</ul>

<h3>Title: REDUCIO! Generating 1024$\times$1024 Video within 16 Seconds using Extremely Compressed Motion Latents</h3>
<ul>
<li><strong>Authors: </strong>Rui Tian, Qi Dai, Jianmin Bao, Kai Qiu, Yifan Yang, Chong Luo, Zuxuan Wu, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.13552">https://arxiv.org/abs/2411.13552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.13552">https://arxiv.org/pdf/2411.13552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.13552]] REDUCIO! Generating 1024$\times$1024 Video within 16 Seconds using Extremely Compressed Motion Latents(https://arxiv.org/abs/2411.13552)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Commercial video generation models have exhibited realistic, high-fidelity results but are still restricted to limited access. One crucial obstacle for large-scale applications is the expensive training and inference cost. In this paper, we argue that videos contain much more redundant information than images, thus can be encoded by very few motion latents based on a content image. Towards this goal, we design an image-conditioned VAE to encode a video to an extremely compressed motion latent space. This magic Reducio charm enables 64x reduction of latents compared to a common 2D VAE, without sacrificing the quality. Training diffusion models on such a compact representation easily allows for generating 1K resolution videos. We then adopt a two-stage video generation paradigm, which performs text-to-image and text-image-to-video sequentially. Extensive experiments show that our Reducio-DiT achieves strong performance in evaluation, though trained with limited GPU resources. More importantly, our method significantly boost the efficiency of video LDMs both in training and inference. We train Reducio-DiT in around 3.2K training hours in total and generate a 16-frame 1024*1024 video clip within 15.5 seconds on a single A100 GPU. Code released at this https URL .</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
