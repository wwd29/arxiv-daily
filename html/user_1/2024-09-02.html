<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-02</h1>
<h3>Title: Generative AI in Ship Design</h3>
<ul>
<li><strong>Authors: </strong>Sahil Thakur, Navneet V Saxena, Prof Sitikantha Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.16798">https://arxiv.org/abs/2408.16798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.16798">https://arxiv.org/pdf/2408.16798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.16798]] Generative AI in Ship Design(https://arxiv.org/abs/2408.16798)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The process of ship design is intricate, heavily influenced by the hull form which accounts for approximately 70% of the total cost. Traditional methods rely on human-driven iterative processes based on naval architecture principles and engineering analysis. In contrast, generative AI presents a novel approach, utilizing computational algorithms rooted in machine learning and artificial intelligence to optimize ship hull design. This report outlines the systematic creation of a generative AI for this purpose, involving steps such as dataset collection, model architecture selection, training, and validation. Utilizing the "SHIP-D" dataset, consisting of 30,000 hull forms, the report adopts the Gaussian Mixture Model (GMM) as the generative model architecture. GMMs offer a statistical framework to analyze data distribution, crucial for generating innovative ship designs efficiently. Overall, this approach holds promise in revolutionizing ship design by exploring a broader design space and integrating multidisciplinary optimization objectives effectively.</li>
</ul>

<h3>Title: HLogformer: A Hierarchical Transformer for Representing Log Data</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Hou, Mina Ghashami, Mikhail Kuznetsov, MohamadAli Torkamani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.16803">https://arxiv.org/abs/2408.16803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.16803">https://arxiv.org/pdf/2408.16803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.16803]] HLogformer: A Hierarchical Transformer for Representing Log Data(https://arxiv.org/abs/2408.16803)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Transformers have gained widespread acclaim for their versatility in handling diverse data structures, yet their application to log data remains underexplored. Log data, characterized by its hierarchical, dictionary-like structure, poses unique challenges when processed using conventional transformer models. Traditional methods often rely on manually crafted templates for parsing logs, a process that is labor-intensive and lacks generalizability. Additionally, the linear treatment of log sequences by standard transformers neglects the rich, nested relationships within log entries, leading to suboptimal representations and excessive memory usage. To address these issues, we introduce HLogformer, a novel hierarchical transformer framework specifically designed for log data. HLogformer leverages the hierarchical structure of log entries to significantly reduce memory costs and enhance representation learning. Unlike traditional models that treat log data as flat sequences, our framework processes log entries in a manner that respects their inherent hierarchical organization. This approach ensures comprehensive encoding of both fine-grained details and broader contextual relationships. Our contributions are threefold: First, HLogformer is the first framework to design a dynamic hierarchical transformer tailored for dictionary-like log data. Second, it dramatically reduces memory costs associated with processing extensive log sequences. Third, comprehensive experiments demonstrate that HLogformer more effectively encodes hierarchical contextual information, proving to be highly effective for downstream tasks such as synthetic anomaly detection and product recommendation.</li>
</ul>

<h3>Title: See or Guess: Counterfactually Regularized Image Captioning</h3>
<ul>
<li><strong>Authors: </strong>Qian Cao, Xu Chen, Ruihua Song, Xiting Wang, Xinting Huang, Yuchen Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.16809">https://arxiv.org/abs/2408.16809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.16809">https://arxiv.org/pdf/2408.16809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.16809]] See or Guess: Counterfactually Regularized Image Captioning(https://arxiv.org/abs/2408.16809)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image captioning, which generates natural language descriptions of the visual information in an image, is a crucial task in vision-language research. Previous models have typically addressed this task by aligning the generative capabilities of machines with human intelligence through statistical fitting of existing datasets. While effective for normal images, they may struggle to accurately describe those where certain parts of the image are obscured or edited, unlike humans who excel in such cases. These weaknesses they exhibit, including hallucinations and limited interpretability, often hinder performance in scenarios with shifted association patterns. In this paper, we present a generic image captioning framework that employs causal inference to make existing models more capable of interventional tasks, and counterfactually explainable. Our approach includes two variants leveraging either total effect or natural direct effect. Integrating them into the training process enables models to handle counterfactual scenarios, increasing their generalizability. Extensive experiments on various datasets show that our method effectively reduces hallucinations and improves the model's faithfulness to images, demonstrating high portability across both small-scale and large-scale image-to-text models. The code is available at this https URL.</li>
</ul>

<h3>Title: Enabling Local Editing in Diffusion Models by Joint and Individual Component Analysis</h3>
<ul>
<li><strong>Authors: </strong>Theodoros Kouzelis, Manos Plitsis, Mihalis A. Nikolaou, Yannis Panagakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.16845">https://arxiv.org/abs/2408.16845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.16845">https://arxiv.org/pdf/2408.16845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.16845]] Enabling Local Editing in Diffusion Models by Joint and Individual Component Analysis(https://arxiv.org/abs/2408.16845)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in Diffusion Models (DMs) have led to significant progress in visual synthesis and editing tasks, establishing them as a strong competitor to Generative Adversarial Networks (GANs). However, the latent space of DMs is not as well understood as that of GANs. Recent research has focused on unsupervised semantic discovery in the latent space of DMs by leveraging the bottleneck layer of the denoising network, which has been shown to exhibit properties of a semantic latent space. However, these approaches are limited to discovering global attributes. In this paper we address, the challenge of local image manipulation in DMs and introduce an unsupervised method to factorize the latent semantics learned by the denoising network of pre-trained DMs. Given an arbitrary image and defined regions of interest, we utilize the Jacobian of the denoising network to establish a relation between the regions of interest and their corresponding subspaces in the latent space. Furthermore, we disentangle the joint and individual components of these subspaces to identify latent directions that enable local image manipulation. Once discovered, these directions can be applied to different images to produce semantically consistent edits, making our method suitable for practical applications. Experimental results on various datasets demonstrate that our method can produce semantic edits that are more localized and have better fidelity compared to the state-of-the-art.</li>
</ul>

<h3>Title: Revising Multimodal VAEs with Diffusion Decoders</h3>
<ul>
<li><strong>Authors: </strong>Daniel Wesego, Amirmohammad Rooshenas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.16883">https://arxiv.org/abs/2408.16883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.16883">https://arxiv.org/pdf/2408.16883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.16883]] Revising Multimodal VAEs with Diffusion Decoders(https://arxiv.org/abs/2408.16883)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multimodal VAEs often struggle with generating high-quality outputs, a challenge that extends beyond the inherent limitations of the VAE framework. The core issue lies in the restricted joint representation of the latent space, particularly when complex modalities like images are involved. Feedforward decoders, commonly used for these intricate modalities, inadvertently constrain the joint latent space, leading to a degradation in the quality of the other modalities as well. Although recent studies have shown improvement by introducing modality-specific representations, the issue remains significant. In this work, we demonstrate that incorporating a flexible diffusion decoder specifically for the image modality not only enhances the generation quality of the images but also positively impacts the performance of the other modalities that rely on feedforward decoders. This approach addresses the limitations imposed by conventional joint representations and opens up new possibilities for improving multimodal generation tasks using the multimodal VAE framework. Our model provides state-of-the-art results compared to other multimodal VAEs in different datasets with higher coherence and superior quality in the generated modalities</li>
</ul>

<h3>Title: LLaVA-Chef: A Multi-modal Generative Model for Food Recipes</h3>
<ul>
<li><strong>Authors: </strong>Fnu Mohbat, Mohammed J. Zaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.16889">https://arxiv.org/abs/2408.16889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.16889">https://arxiv.org/pdf/2408.16889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.16889]] LLaVA-Chef: A Multi-modal Generative Model for Food Recipes(https://arxiv.org/abs/2408.16889)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of online recipe sharing within a globalized context, there has been a notable surge in research towards comprehending and generating food recipes. Recent advancements in large language models (LLMs) like GPT-2 and LLaVA have paved the way for Natural Language Processing (NLP) approaches to delve deeper into various facets of food-related tasks, encompassing ingredient recognition and comprehensive recipe generation. Despite impressive performance and multi-modal adaptability of LLMs, domain-specific training remains paramount for their effective application. This work evaluates existing LLMs for recipe generation and proposes LLaVA-Chef, a novel model trained on a curated dataset of diverse recipe prompts in a multi-stage approach. First, we refine the mapping of visual food image embeddings to the language space. Second, we adapt LLaVA to the food domain by fine-tuning it on relevant recipe data. Third, we utilize diverse prompts to enhance the model's recipe comprehension. Finally, we improve the linguistic quality of generated recipes by penalizing the model with a custom loss function. LLaVA-Chef demonstrates impressive improvements over pretrained LLMs and prior works. A detailed qualitative analysis reveals that LLaVA-Chef generates more detailed recipes with precise ingredient mentions, compared to existing approaches.</li>
</ul>

<h3>Title: Ig3D: Integrating 3D Face Representations in Facial Expression Inference</h3>
<ul>
<li><strong>Authors: </strong>Lu Dong, Xiao Wang, Srirangaraj Setlur, Venu Govindaraju, Ifeoma Nwogu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.16907">https://arxiv.org/abs/2408.16907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.16907">https://arxiv.org/pdf/2408.16907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.16907]] Ig3D: Integrating 3D Face Representations in Facial Expression Inference(https://arxiv.org/abs/2408.16907)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reconstructing 3D faces with facial geometry from single images has allowed for major advances in animation, generative models, and virtual reality. However, this ability to represent faces with their 3D features is not as fully explored by the facial expression inference (FEI) community. This study therefore aims to investigate the impacts of integrating such 3D representations into the FEI task, specifically for facial expression classification and face-based valence-arousal (VA) estimation. To accomplish this, we first assess the performance of two 3D face representations (both based on the 3D morphable model, FLAME) for the FEI tasks. We further explore two fusion architectures, intermediate fusion and late fusion, for integrating the 3D face representations with existing 2D inference frameworks. To evaluate our proposed architecture, we extract the corresponding 3D representations and perform extensive tests on the AffectNet and RAF-DB datasets. Our experimental results demonstrate that our proposed method outperforms the state-of-the-art AffectNet VA estimation and RAF-DB classification tasks. Moreover, our method can act as a complement to other existing methods to boost performance in many emotion inference tasks.</li>
</ul>

<h3>Title: Contrastive Learning with Synthetic Positives</h3>
<ul>
<li><strong>Authors: </strong>Dewen Zeng, Yawen Wu, Xinrong Hu, Xiaowei Xu, Yiyu Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.16965">https://arxiv.org/abs/2408.16965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.16965">https://arxiv.org/pdf/2408.16965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.16965]] Contrastive Learning with Synthetic Positives(https://arxiv.org/abs/2408.16965)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Contrastive learning with the nearest neighbor has proved to be one of the most efficient self-supervised learning (SSL) techniques by utilizing the similarity of multiple instances within the same class. However, its efficacy is constrained as the nearest neighbor algorithm primarily identifies ``easy'' positive pairs, where the representations are already closely located in the embedding space. In this paper, we introduce a novel approach called Contrastive Learning with Synthetic Positives (CLSP) that utilizes synthetic images, generated by an unconditional diffusion model, as the additional positives to help the model learn from diverse positives. Through feature interpolation in the diffusion model sampling process, we generate images with distinct backgrounds yet similar semantic content to the anchor image. These images are considered ``hard'' positives for the anchor image, and when included as supplementary positives in the contrastive loss, they contribute to a performance improvement of over 2\% and 1\% in linear evaluation compared to the previous NNCLR and All4One methods across multiple benchmark datasets such as CIFAR10, achieving state-of-the-art methods. On transfer learning benchmarks, CLSP outperforms existing SSL frameworks on 6 out of 8 downstream datasets. We believe CLSP establishes a valuable baseline for future SSL studies incorporating synthetic data in the training process.</li>
</ul>

<h3>Title: ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features from Multi-View Images</h3>
<ul>
<li><strong>Authors: </strong>Xiaoshuai Zhang, Zhicheng Wang, Howard Zhou, Soham Ghosh, Danushen Gnanapragasam, Varun Jampani, Hao Su, Leonidas Guibas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17027">https://arxiv.org/abs/2408.17027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17027">https://arxiv.org/pdf/2408.17027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17027]] ConDense: Consistent 2D/3D Pre-training for Dense and Sparse Features from Multi-View Images(https://arxiv.org/abs/2408.17027)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>To advance the state of the art in the creation of 3D foundation models, this paper introduces the ConDense framework for 3D pre-training utilizing existing pre-trained 2D networks and large-scale multi-view datasets. We propose a novel 2D-3D joint training scheme to extract co-embedded 2D and 3D features in an end-to-end pipeline, where 2D-3D feature consistency is enforced through a volume rendering NeRF-like ray marching process. Using dense per pixel features we are able to 1) directly distill the learned priors from 2D models to 3D models and create useful 3D backbones, 2) extract more consistent and less noisy 2D features, 3) formulate a consistent embedding space where 2D, 3D, and other modalities of data (e.g., natural language prompts) can be jointly queried. Furthermore, besides dense features, ConDense can be trained to extract sparse features (e.g., key points), also with 2D-3D consistency -- condensing 3D NeRF representations into compact sets of decorated key points. We demonstrate that our pre-trained model provides good initialization for various 3D tasks including 3D classification and segmentation, outperforming other 3D pre-training methods by a significant margin. It also enables, by exploiting our sparse features, additional useful downstream tasks, such as matching 2D images to 3D scenes, detecting duplicate 3D scenes, and querying a repository of 3D scenes through natural language -- all quite efficiently and without any per-scene fine-tuning.</li>
</ul>

<h3>Title: Meta-UAD: A Meta-Learning Scheme for User-level Network Traffic Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Tongtong Feng, Qi Qi, Lingqi Guo, Jingyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17031">https://arxiv.org/abs/2408.17031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17031">https://arxiv.org/pdf/2408.17031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17031]] Meta-UAD: A Meta-Learning Scheme for User-level Network Traffic Anomaly Detection(https://arxiv.org/abs/2408.17031)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Accuracy anomaly detection in user-level network traffic is crucial for network security. Compared with existing models that passively detect specific anomaly classes with large labeled training samples, user-level network traffic contains sizeable new anomaly classes with few labeled samples and has an imbalance, self-similar, and data-hungry nature. Motivation on those limitations, in this paper, we propose \textit{Meta-UAD}, a Meta-learning scheme for User-level network traffic Anomaly Detection. Meta-UAD uses the CICFlowMeter to extract 81 flow-level statistical features and remove some invalid ones using cumulative importance ranking. Meta-UAD adopts a meta-learning training structure and learns from the collection of K-way-M-shot classification tasks, which can use a pre-trained model to adapt any new class with few samples by few iteration steps. We evaluate our scheme on two public datasets. Compared with existing models, the results further demonstrate the superiority of Meta-UAD with 15{\%} - 43{\%} gains in F1-score.</li>
</ul>

<h3>Title: Text-to-Image Generation Via Energy-Based CLIP</h3>
<ul>
<li><strong>Authors: </strong>Roy Ganz, Michael Elad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17046">https://arxiv.org/abs/2408.17046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17046">https://arxiv.org/pdf/2408.17046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17046]] Text-to-Image Generation Via Energy-Based CLIP(https://arxiv.org/abs/2408.17046)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Joint Energy Models (JEMs), while drawing significant research attention, have not been successfully scaled to real-world, high-resolution datasets. We present EB-CLIP, a novel approach extending JEMs to the multimodal vision-language domain using CLIP, integrating both generative and discriminative objectives. For the generative objective, we introduce an image-text joint-energy function based on Cosine similarity in the CLIP space, training CLIP to assign low energy to real image-caption pairs and high energy otherwise. For the discriminative objective, we employ contrastive adversarial loss, extending the adversarial training objective to the multimodal domain. EB-CLIP not only generates realistic images from text but also achieves competitive results on the compositionality benchmark, outperforming leading methods with fewer parameters. Additionally, we demonstrate the superior guidance capability of EB-CLIP by enhancing CLIP-based generative frameworks and converting unconditional diffusion models to text-based ones. Lastly, we show that EB-CLIP can serve as a more robust evaluation metric for text-to-image generative tasks than CLIP.</li>
</ul>

<h3>Title: Can We Leave Deepfake Data Behind in Training Deepfake Detector?</h3>
<ul>
<li><strong>Authors: </strong>Jikang Cheng, Zhiyuan Yan, Ying Zhang, Yuhao Luo, Zhongyuan Wang, Chen Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17052">https://arxiv.org/abs/2408.17052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17052">https://arxiv.org/pdf/2408.17052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17052]] Can We Leave Deepfake Data Behind in Training Deepfake Detector?(https://arxiv.org/abs/2408.17052)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generalization ability of deepfake detectors is vital for their applications in real-world scenarios. One effective solution to enhance this ability is to train the models with manually-blended data, which we termed "blendfake", encouraging models to learn generic forgery artifacts like blending boundary. Interestingly, current SoTA methods utilize blendfake without incorporating any deepfake data in their training process. This is likely because previous empirical observations suggest that vanilla hybrid training (VHT), which combines deepfake and blendfake data, results in inferior performance to methods using only blendfake data (so-called "1+1<2"). Therefore, a critical question arises: Can we leave deepfake behind and rely solely on blendfake data to train an effective deepfake detector? Intuitively, as deepfakes also contain additional informative forgery clues (e.g., deep generative artifacts), excluding all deepfake data in training deepfake detectors seems counter-intuitive. In this paper, we rethink the role of blendfake in detecting deepfakes and formulate the process from "real to blendfake to deepfake" to be a progressive transition. Specifically, blendfake and deepfake can be explicitly delineated as the oriented pivot anchors between "real-to-fake" transitions. The accumulation of forgery information should be oriented and progressively increasing during this transition process. To this end, we propose an Oriented Progressive Regularizor (OPR) to establish the constraints that compel the distribution of anchors to be discretely arranged. Furthermore, we introduce feature bridging to facilitate the smooth transition between adjacent anchors. Extensive experiments confirm that our design allows leveraging forgery information from both blendfake and deepfake effectively and comprehensively.</li>
</ul>

<h3>Title: Efficient Image Restoration through Low-Rank Adaptation and Stable Diffusion XL</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17060">https://arxiv.org/abs/2408.17060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17060">https://arxiv.org/pdf/2408.17060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17060]] Efficient Image Restoration through Low-Rank Adaptation and Stable Diffusion XL(https://arxiv.org/abs/2408.17060)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this study, we propose an enhanced image restoration model, SUPIR, based on the integration of two low-rank adaptive (LoRA) modules with the Stable Diffusion XL (SDXL) framework. Our method leverages the advantages of LoRA to fine-tune SDXL models, thereby significantly improving image restoration quality and efficiency. We collect 2600 high-quality real-world images, each with detailed descriptive text, for training the model. The proposed method is evaluated on standard benchmarks and achieves excellent performance, demonstrated by higher peak signal-to-noise ratio (PSNR), lower learned perceptual image patch similarity (LPIPS), and higher structural similarity index measurement (SSIM) scores. These results underscore the effectiveness of combining LoRA with SDXL for advanced image restoration tasks, highlighting the potential of our approach in generating high-fidelity restored images.</li>
</ul>

<h3>Title: Instant Adversarial Purification with Adversarial Consistency Distillation</h3>
<ul>
<li><strong>Authors: </strong>Chun Tong Lei, Hon Ming Yam, Zhongliang Guo, Chun Pong Lau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17064">https://arxiv.org/abs/2408.17064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17064">https://arxiv.org/pdf/2408.17064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17064]] Instant Adversarial Purification with Adversarial Consistency Distillation(https://arxiv.org/abs/2408.17064)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Neural networks, despite their remarkable performance in widespread applications, including image classification, are also known to be vulnerable to subtle adversarial noise. Although some diffusion-based purification methods have been proposed, for example, DiffPure, those methods are time-consuming. In this paper, we propose One Step Control Purification (OSCP), a diffusion-based purification model that can purify the adversarial image in one Neural Function Evaluation (NFE) in diffusion models. We use Latent Consistency Model (LCM) and ControlNet for our one-step purification. OSCP is computationally friendly and time efficient compared to other diffusion-based purification methods; we achieve defense success rate of 74.19\% on ImageNet, only requiring 0.1s for each purification. Moreover, there is a fundamental incongruence between consistency distillation and adversarial perturbation. To address this ontological dissonance, we propose Gaussian Adversarial Noise Distillation (GAND), a novel consistency distillation framework that facilitates a more nuanced reconciliation of the latent space dynamics, effectively bridging the natural and adversarial manifolds. Our experiments show that the GAND does not need a Full Fine Tune (FFT); PEFT, e.g., LoRA is sufficient.</li>
</ul>

<h3>Title: FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Chen Hu, Jingjing Deng, Xianghua Xie, Xiaoke Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17090">https://arxiv.org/abs/2408.17090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17090">https://arxiv.org/pdf/2408.17090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17090]] FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition(https://arxiv.org/abs/2408.17090)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Federated learning is a machine learning paradigm that enables decentralized clients to collaboratively learn a shared model while keeping all the training data local. While considerable research has focused on federated image generation, particularly Generative Adversarial Networks, Variational Autoencoders have received less attention. In this paper, we address the challenges of non-IID (independently and identically distributed) data environments featuring multiple groups of images of different types. Specifically, heterogeneous data distributions can lead to difficulties in maintaining a consistent latent space and can also result in local generators with disparate texture features being blended during aggregation. We introduce a novel approach, FissionVAE, which decomposes the latent space and constructs decoder branches tailored to individual client groups. This method allows for customized learning that aligns with the unique data distributions of each group. Additionally, we investigate the incorporation of hierarchical VAE architectures and demonstrate the use of heterogeneous decoder architectures within our model. We also explore strategies for setting the latent prior distributions to enhance the decomposition process. To evaluate our approach, we assemble two composite datasets: the first combines MNIST and FashionMNIST; the second comprises RGB datasets of cartoon and human faces, wild animals, marine vessels, and remote sensing images of Earth. Our experiments demonstrate that FissionVAE greatly improves generation quality on these datasets compared to baseline federated VAE models.</li>
</ul>

<h3>Title: RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation and Retrieval-Guidance</h3>
<ul>
<li><strong>Authors: </strong>Avideep Mukherjee, Soumya Banerjee, Vinay P. Namboodiri, Piyush Rai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17095">https://arxiv.org/abs/2408.17095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17095">https://arxiv.org/pdf/2408.17095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17095]] RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation and Retrieval-Guidance(https://arxiv.org/abs/2408.17095)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based models demonstrate impressive generation capabilities. However, they also have a massive number of parameters, resulting in enormous model sizes, thus making them unsuitable for deployment on resource-constraint devices. Block-wise generation can be a promising alternative for designing compact-sized (parameter-efficient) deep generative models since the model can generate one block at a time instead of generating the whole image at once. However, block-wise generation is also considerably challenging because ensuring coherence across generated blocks can be non-trivial. To this end, we design a retrieval-augmented generation (RAG) approach and leverage the corresponding blocks of the images retrieved by the RAG module to condition the training and generation stages of a block-wise denoising diffusion model. Our conditioning schemes ensure coherence across the different blocks during training and, consequently, during generation. While we showcase our approach using the latent diffusion model (LDM) as the base model, it can be used with other variants of denoising diffusion models. We validate the solution of the coherence problem through the proposed approach by reporting substantive experiments to demonstrate our approach's effectiveness in compact model size and excellent generation quality.</li>
</ul>

<h3>Title: VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Juncan Deng, Shuaiting Li, Zeyu Wang, Hong Gu, Kedong Xu, Kejie Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17131">https://arxiv.org/abs/2408.17131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17131">https://arxiv.org/pdf/2408.17131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17131]] VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers(https://arxiv.org/abs/2408.17131)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.</li>
</ul>

<h3>Title: Flow Matching for Optimal Reaction Coordinates of Biomolecular System</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Zhang, Zhicheng Zhang, Yong Wang, Hao Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.bio-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17139">https://arxiv.org/abs/2408.17139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17139">https://arxiv.org/pdf/2408.17139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17139]] Flow Matching for Optimal Reaction Coordinates of Biomolecular System(https://arxiv.org/abs/2408.17139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Flow Matching for Reaction Coordinates (FMRC), a novel deep learning algorithm designed to identify optimal reaction coordinates (RC) in biomolecular reversible dynamics. FMRC is based on the mathematical principles of lumpability and decomposability, which we reformulate into a conditional probability framework for efficient data-driven optimization using deep generative models. While FMRC does not explicitly learn the well-established transfer operator or its eigenfunctions, it can effectively encode the dynamics of leading eigenfunctions of the system transfer operator into its low-dimensional RC space. We further quantitatively compare its performance with several state-of-the-art algorithms by evaluating the quality of Markov State Models (MSM) constructed in their respective RC spaces, demonstrating the superiority of FMRC in three increasingly complex biomolecular systems. Finally, we discuss its potential applications in downstream applications such as enhanced sampling methods and MSM construction.</li>
</ul>

<h3>Title: RenDetNet: Weakly-supervised Shadow Detection with Shadow Caster Verification</h3>
<ul>
<li><strong>Authors: </strong>Nikolina Kubiak, Elliot Wortman, Armin Mustafa, Graeme Phillipson, Stephen Jolly, Simon Hadfield</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17143">https://arxiv.org/abs/2408.17143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17143">https://arxiv.org/pdf/2408.17143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17143]] RenDetNet: Weakly-supervised Shadow Detection with Shadow Caster Verification(https://arxiv.org/abs/2408.17143)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Existing shadow detection models struggle to differentiate dark image areas from shadows. In this paper, we tackle this issue by verifying that all detected shadows are real, i.e. they have paired shadow casters. We perform this step in a physically-accurate manner by differentiably re-rendering the scene and observing the changes stemming from carving out estimated shadow casters. Thanks to this approach, the RenDetNet proposed in this paper is the first learning-based shadow detection model whose supervisory signals can be computed in a self-supervised manner. The developed system compares favourably against recent models trained on our data. As part of this publication, we release our code on github.</li>
</ul>

<h3>Title: Self-supervised Anomaly Detection Pretraining Enhances Long-tail ECG Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Aofan Jiang, Chaoqin Huang, Qing Cao, Yuchen Xu, Zi Zeng, Kang Chen, Ya Zhang, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17154">https://arxiv.org/abs/2408.17154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17154">https://arxiv.org/pdf/2408.17154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17154]] Self-supervised Anomaly Detection Pretraining Enhances Long-tail ECG Diagnosis(https://arxiv.org/abs/2408.17154)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Current computer-aided ECG diagnostic systems struggle with the underdetection of rare but critical cardiac anomalies due to the imbalanced nature of ECG datasets. This study introduces a novel approach using self-supervised anomaly detection pretraining to address this limitation. The anomaly detection model is specifically designed to detect and localize subtle deviations from normal cardiac patterns, capturing the nuanced details essential for accurate ECG interpretation. Validated on an extensive dataset of over one million ECG records from clinical practice, characterized by a long-tail distribution across 116 distinct categories, the anomaly detection-pretrained ECG diagnostic model has demonstrated a significant improvement in overall accuracy. Notably, our approach yielded a 94.7% AUROC, 92.2% sensitivity, and 92.5\% specificity for rare ECG types, significantly outperforming traditional methods and narrowing the performance gap with common ECG types. The integration of anomaly detection pretraining into ECG analysis represents a substantial contribution to the field, addressing the long-standing challenge of long-tail data distributions in clinical diagnostics. Furthermore, prospective validation in real-world clinical settings revealed that our AI-driven approach enhances diagnostic efficiency, precision, and completeness by 32%, 6.7%, and 11.8% respectively, when compared to standard practices. This advancement marks a pivotal step forward in the integration of AI within clinical cardiology, with particularly profound implications for emergency care, where rapid and accurate ECG interpretation is crucial. The contributions of this study not only push the boundaries of current ECG diagnostic capabilities but also lay the groundwork for more reliable and accessible cardiovascular care.</li>
</ul>

<h3>Title: How Could Generative AI Support Compliance with the EU AI Act? A Review for Safe Automated Driving Perception</h3>
<ul>
<li><strong>Authors: </strong>Mert Keser, Youssef Shoeb, Alois Knoll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17222">https://arxiv.org/abs/2408.17222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17222">https://arxiv.org/pdf/2408.17222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17222]] How Could Generative AI Support Compliance with the EU AI Act? A Review for Safe Automated Driving Perception(https://arxiv.org/abs/2408.17222)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep Neural Networks (DNNs) have become central for the perception functions of autonomous vehicles, substantially enhancing their ability to understand and interpret the environment. However, these systems exhibit inherent limitations such as brittleness, opacity, and unpredictable behavior in out-of-distribution scenarios. The European Union (EU) Artificial Intelligence (AI) Act, as a pioneering legislative framework, aims to address these challenges by establishing stringent norms and standards for AI systems, including those used in autonomous driving (AD), which are categorized as high-risk AI. In this work, we explore how the newly available generative AI models can potentially support addressing upcoming regulatory requirements in AD perception, particularly with respect to safety. This short review paper summarizes the requirements arising from the EU AI Act regarding DNN-based perception systems and systematically categorizes existing generative AI applications in AD. While generative AI models show promise in addressing some of the EU AI Acts requirements, such as transparency and robustness, this review examines their potential benefits and discusses how developers could leverage these methods to enhance compliance with the Act. The paper also highlights areas where further research is needed to ensure reliable and safe integration of these technologies.</li>
</ul>

<h3>Title: Abstracted Gaussian Prototypes for One-Shot Concept Learning</h3>
<ul>
<li><strong>Authors: </strong>Chelsea Zou, Kenneth J. Kurtz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17251">https://arxiv.org/abs/2408.17251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17251">https://arxiv.org/pdf/2408.17251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17251]] Abstracted Gaussian Prototypes for One-Shot Concept Learning(https://arxiv.org/abs/2408.17251)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a cluster-based generative image segmentation framework to encode higher-level representations of visual concepts based on one-shot learning inspired by the Omniglot Challenge. The inferred parameters of each component of a Gaussian Mixture Model (GMM) represent a distinct topological subpart of a visual concept. Sampling new data from these parameters generates augmented subparts to build a more robust prototype for each concept, i.e., the Abstracted Gaussian Prototype (AGP). This framework addresses one-shot classification tasks using a cognitively-inspired similarity metric and addresses one-shot generative tasks through a novel AGP-VAE pipeline employing variational autoencoders (VAEs) to generate new class variants. Results from human judges reveal that the generative pipeline produces novel examples and classes of visual concepts that are broadly indistinguishable from those made by humans. The proposed framework leads to impressive but not state-of-the-art classification accuracy; thus, the contribution is two-fold: 1) the system is uniquely low in theoretical and computational complexity and operates in a completely standalone manner compared while existing approaches draw heavily on pre-training or knowledge engineering; and 2) in contrast with competing neural network models, the AGP approach addresses the importance of breadth of task capability emphasized in the Omniglot challenge (i.e., successful performance on generative tasks). These two points are critical as we advance toward an understanding of how learning/reasoning systems can produce viable, robust, and flexible concepts based on literally nothing more than a single example.</li>
</ul>

<h3>Title: VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters</h3>
<ul>
<li><strong>Authors: </strong>Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Joy Wang, Jianling Sun, Chenghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17253">https://arxiv.org/abs/2408.17253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17253">https://arxiv.org/pdf/2408.17253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17253]] VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters(https://arxiv.org/abs/2408.17253)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either fine-tune large language models (LLMs) or build large-scale time-series datasets to develop TSF foundation models. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. In this paper, we explore a new road to building a TSF foundation model from rich and high-quality natural images, based on the intrinsic similarities between images and time series. To bridge the gap between the two domains, we reformulate the TSF task as an image reconstruction task, which is further processed by a visual masked autoencoder (MAE) self-supervised pre-trained on the ImageNet dataset. Surprisingly, without further adaptation in the time-series domain, the proposed VisionTS could achieve superior zero-shot forecasting performance compared to existing TSF foundation models. With minimal fine-tuning, VisionTS could further improve the forecasting and achieve state-of-the-art performance in most cases. These findings suggest that visual models could be a free lunch for TSF and highlight the potential for future cross-domain research between computer vision and TSF. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Self-supervised learning for crystal property prediction via denoising</h3>
<ul>
<li><strong>Authors: </strong>Alexander New, Nam Q. Le, Michael J. Pekala, Christopher D. Stiles</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17255">https://arxiv.org/abs/2408.17255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17255">https://arxiv.org/pdf/2408.17255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17255]] Self-supervised learning for crystal property prediction via denoising(https://arxiv.org/abs/2408.17255)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate prediction of the properties of crystalline materials is crucial for targeted discovery, and this prediction is increasingly done with data-driven models. However, for many properties of interest, the number of materials for which a specific property has been determined is much smaller than the number of known materials. To overcome this disparity, we propose a novel self-supervised learning (SSL) strategy for material property prediction. Our approach, crystal denoising self-supervised learning (CDSSL), pretrains predictive models (e.g., graph networks) with a pretext task based on recovering valid material structures when given perturbed versions of these structures. We demonstrate that CDSSL models out-perform models trained without SSL, across material types, properties, and dataset sizes.</li>
</ul>

<h3>Title: Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution</h3>
<ul>
<li><strong>Authors: </strong>Yixin Wu, Yun Shen, Michael Backes, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17285">https://arxiv.org/abs/2408.17285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17285">https://arxiv.org/pdf/2408.17285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17285]] Image-Perfect Imperfections: Safety, Bias, and Authenticity in the Shadow of Text-To-Image Model Evolution(https://arxiv.org/abs/2408.17285)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image models, such as Stable Diffusion (SD), undergo iterative updates to improve image quality and address concerns such as safety. Improvements in image quality are straightforward to assess. However, how model updates resolve existing concerns and whether they raise new questions remain unexplored. This study takes an initial step in investigating the evolution of text-to-image models from the perspectives of safety, bias, and authenticity. Our findings, centered on Stable Diffusion, indicate that model updates paint a mixed picture. While updates progressively reduce the generation of unsafe images, the bias issue, particularly in gender, intensifies. We also find that negative stereotypes either persist within the same Non-White race group or shift towards other Non-White race groups through SD updates, yet with minimal association of these traits with the White race group. Additionally, our evaluation reveals a new concern stemming from SD updates: State-of-the-art fake image detectors, initially trained for earlier SD versions, struggle to identify fake images generated by updated versions. We show that fine-tuning these detectors on fake images generated by updated versions achieves at least 96.6\% accuracy across various SD versions, addressing this issue. Our insights highlight the importance of continued efforts to mitigate biases and vulnerabilities in evolving text-to-image models.</li>
</ul>

<h3>Title: Assessing Generative Language Models in Classification Tasks: Performance and Self-Evaluation Capabilities in the Environmental and Climate Change Domain</h3>
<ul>
<li><strong>Authors: </strong>Francesca Grasso, Stefano Locci</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17362">https://arxiv.org/abs/2408.17362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17362">https://arxiv.org/pdf/2408.17362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17362]] Assessing Generative Language Models in Classification Tasks: Performance and Self-Evaluation Capabilities in the Environmental and Climate Change Domain(https://arxiv.org/abs/2408.17362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper examines the performance of two Large Language Models (LLMs), GPT3.5 and Llama2 and one Small Language Model (SLM) Gemma, across three different classification tasks within the climate change (CC) and environmental domain. Employing BERT-based models as a baseline, we compare their efficacy against these transformer-based models. Additionally, we assess the models' self-evaluation capabilities by analyzing the calibration of verbalized confidence scores in these text classification tasks. Our findings reveal that while BERT-based models generally outperform both the LLMs and SLM, the performance of the large generative models is still noteworthy. Furthermore, our calibration analysis reveals that although Gemma is well-calibrated in initial tasks, it thereafter produces inconsistent results; Llama is reasonably calibrated, and GPT consistently exhibits strong calibration. Through this research, we aim to contribute to the ongoing discussion on the utility and effectiveness of generative LMs in addressing some of the planet's most urgent issues, highlighting their strengths and limitations in the context of ecology and CC.</li>
</ul>

<h3>Title: CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yiran Chen, Anyi Rao, Xuekun Jiang, Shishi Xiao, Ruiqing Ma, Zeyu Wang, Hui Xiong, Bo Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17424">https://arxiv.org/abs/2408.17424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17424">https://arxiv.org/pdf/2408.17424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17424]] CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion(https://arxiv.org/abs/2408.17424)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>With advancements in video generative AI models (e.g., SORA), creators are increasingly using these techniques to enhance video previsualization. However, they face challenges with incomplete and mismatched AI workflows. Existing methods mainly rely on text descriptions and struggle with camera placement, a key component of previsualization. To address these issues, we introduce CinePreGen, a visual previsualization system enhanced with engine-powered diffusion. It features a novel camera and storyboard interface that offers dynamic control, from global to local camera adjustments. This is combined with a user-friendly AI rendering workflow, which aims to achieve consistent results through multi-masked IP-Adapter and engine simulation guidelines. In our comprehensive evaluation study, we demonstrate that our system reduces development viscosity (i.e., the complexity and challenges in the development process), meets users' needs for extensive control and iteration in the design process, and outperforms other AI video production workflows in cinematic camera movement, as shown by our experiments and a within-subjects user study. With its intuitive camera controls and realistic rendering of camera motion, CinePreGen shows great potential for improving video production for both individual creators and industry professionals.</li>
</ul>

<h3>Title: DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Mona Sheikh Zeinoddin, Chiara Lena, Jiongqi Qu, Luca Carlini, Mattia Magro, Seunghoi Kim, Elena De Momi, Sophia Bano, Matthew Grech-Sollars, Evangelos Mazomenos, Daniel C. Alexander, Danail Stoyanov, Matthew J. Clarkson, Mobarakol Islam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.17433">https://arxiv.org/abs/2408.17433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.17433">https://arxiv.org/pdf/2408.17433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.17433]] DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model(https://arxiv.org/abs/2408.17433)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D reconstruction and visualization. While foundation models like Depth Anything Models (DAM) show promise, directly applying them to surgery often yields suboptimal results. Fully fine-tuning on limited surgical data can cause overfitting and catastrophic forgetting, compromising model robustness and generalization. Although Low-Rank Adaptation (LoRA) addresses some adaptation issues, its uniform parameter distribution neglects the inherent feature hierarchy, where earlier layers, learning more general features, require more parameters than later ones. To tackle this issue, we introduce Depth Anything in Robotic Endoscopic Surgery (DARES), a novel approach that employs a new adaptation technique, Vector Low-Rank Adaptation (Vector-LoRA) on the DAM V2 to perform self-supervised monocular depth estimation in RAS scenes. To enhance learning efficiency, we introduce Vector-LoRA by integrating more parameters in earlier layers and gradually decreasing parameters in later layers. We also design a reprojection loss based on the multi-scale SSIM error to enhance depth perception by better tailoring the foundation model to the specific requirements of the surgical environment. The proposed method is validated on the SCARED dataset and demonstrates superior performance over recent state-of-the-art self-supervised monocular depth estimation techniques, achieving an improvement of 13.3% in the absolute relative error metric. The code and pre-trained weights are available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
