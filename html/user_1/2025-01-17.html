<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-01-17</h1>
<h3>Title: Synthetic Data and Health Privacy</h3>
<ul>
<li><strong>Authors: </strong>Gwénolé Abgrall, Xavier Monnet, Anmol Arora</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09031">https://arxiv.org/abs/2501.09031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09031">https://arxiv.org/pdf/2501.09031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09031]] Synthetic Data and Health Privacy(https://arxiv.org/abs/2501.09031)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This Viewpoint discusses generative artificial intelligence and safeguarding privacy by using synthetic data as a substitute for private health data.</li>
</ul>

<h3>Title: Do generative video models learn physical principles from watching videos?</h3>
<ul>
<li><strong>Authors: </strong>Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, Robert Geirhos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09038">https://arxiv.org/abs/2501.09038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09038">https://arxiv.org/pdf/2501.09038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09038]] Do generative video models learn physical principles from watching videos?(https://arxiv.org/abs/2501.09038)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>AI video generation is undergoing a revolution, with quality and realism advancing rapidly. These advances have led to a passionate scientific debate: Do video models learn ``world models'' that discover laws of physics -- or, alternatively, are they merely sophisticated pixel predictors that achieve visual realism without understanding the physical principles of reality? We address this question by developing Physics-IQ, a comprehensive benchmark dataset that can only be solved by acquiring a deep understanding of various physical principles, like fluid dynamics, optics, solid mechanics, magnetism and thermodynamics. We find that across a range of current models (Sora, Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical understanding is severely limited, and unrelated to visual realism. At the same time, some test cases can already be successfully solved. This indicates that acquiring certain physical principles from observation alone may be possible, but significant challenges remain. While we expect rapid advances ahead, our work demonstrates that visual realism does not imply physical understanding. Our project page is at this https URL; code at this https URL.</li>
</ul>

<h3>Title: Pseudolabel guided pixels contrast for domain adaptive semantic segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jianzi Xiang, Cailu Wan, Zhu Cao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09040">https://arxiv.org/abs/2501.09040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09040">https://arxiv.org/pdf/2501.09040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09040]] Pseudolabel guided pixels contrast for domain adaptive semantic segmentation(https://arxiv.org/abs/2501.09040)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is essential for comprehending images, but the process necessitates a substantial amount of detailed annotations at the pixel level. Acquiring such annotations can be costly in the real-world. Unsupervised domain adaptation (UDA) for semantic segmentation is a technique that uses virtual data with labels to train a model and adapts it to real data without labels. Some recent works use contrastive learning, which is a powerful method for self-supervised learning, to help with this technique. However, these works do not take into account the diversity of features within each class when using contrastive learning, which leads to errors in class prediction. We analyze the limitations of these works and propose a novel framework called Pseudo-label Guided Pixel Contrast (PGPC), which overcomes the disadvantages of previous methods. We also investigate how to use more information from target images without adding noise from pseudo-labels. We test our method on two standard UDA benchmarks and show that it outperforms existing methods. Specifically, we achieve relative improvements of 5.1% mIoU and 4.6% mIoU on the Grand Theft Auto V (GTA5) to Cityscapes and SYNTHIA to Cityscapes tasks based on DAFormer, respectively. Furthermore, our approach can enhance the performance of other UDA approaches without increasing model complexity. Code is available at this https URL</li>
</ul>

<h3>Title: Generative Visual Commonsense Answering and Explaining with Generative Scene Graph Constructing</h3>
<ul>
<li><strong>Authors: </strong>Fan Yuan, Xiaoyuan Fang, Rong Quan, Jing Li, Wei Bi, Xiaogang Xu, Piji Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09041">https://arxiv.org/abs/2501.09041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09041">https://arxiv.org/pdf/2501.09041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09041]] Generative Visual Commonsense Answering and Explaining with Generative Scene Graph Constructing(https://arxiv.org/abs/2501.09041)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual Commonsense Reasoning, which is regarded as one challenging task to pursue advanced visual scene comprehension, has been used to diagnose the reasoning ability of AI systems. However, reliable reasoning requires a good grasp of the scene's details. Existing work fails to effectively exploit the real-world object relationship information present within the scene, and instead overly relies on knowledge from training memory. Based on these observations, we propose a novel scene-graph-enhanced visual commonsense reasoning generation method named \textit{\textbf{G2}}, which first utilizes the image patches and LLMs to construct a location-free scene graph, and then answer and explain based on the scene graph's information. We also propose automatic scene graph filtering and selection strategies to absorb valuable scene graph information during training. Extensive experiments are conducted on the tasks and datasets of scene graph constructing and visual commonsense answering and explaining, respectively. Experimental results and ablation analysis demonstrate the effectiveness of our proposed framework.</li>
</ul>

<h3>Title: CookingDiffusion: Cooking Procedural Image Generation with Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yuan Wang, Bin Xhu, Yanbin Hao, Chong-Wah Ngo, Yi Tan, Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09042">https://arxiv.org/abs/2501.09042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09042">https://arxiv.org/pdf/2501.09042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09042]] CookingDiffusion: Cooking Procedural Image Generation with Stable Diffusion(https://arxiv.org/abs/2501.09042)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image generation models have excelled in creating diverse and realistic images. This success extends to food imagery, where various conditional inputs like cooking styles, ingredients, and recipes are utilized. However, a yet-unexplored challenge is generating a sequence of procedural images based on cooking steps from a recipe. This could enhance the cooking experience with visual guidance and possibly lead to an intelligent cooking simulation system. To fill this gap, we introduce a novel task called \textbf{cooking procedural image generation}. This task is inherently demanding, as it strives to create photo-realistic images that align with cooking steps while preserving sequential consistency. To collectively tackle these challenges, we present \textbf{CookingDiffusion}, a novel approach that leverages Stable Diffusion and three innovative Memory Nets to model procedural prompts. These prompts encompass text prompts (representing cooking steps), image prompts (corresponding to cooking images), and multi-modal prompts (mixing cooking steps and images), ensuring the consistent generation of cooking procedural images. To validate the effectiveness of our approach, we preprocess the YouCookII dataset, establishing a new benchmark. Our experimental results demonstrate that our model excels at generating high-quality cooking procedural images with remarkable consistency across sequential cooking steps, as measured by both the FID and the proposed Average Procedure Consistency metrics. Furthermore, CookingDiffusion demonstrates the ability to manipulate ingredients and cooking methods in a recipe. We will make our code, models, and dataset publicly accessible.</li>
</ul>

<h3>Title: Spatio-Temporal Foundation Models: Vision, Challenges, and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Adam Goodge, Wee Siong Ng, Bryan Hooi, See Kiong Ng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09045">https://arxiv.org/abs/2501.09045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09045">https://arxiv.org/pdf/2501.09045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09045]] Spatio-Temporal Foundation Models: Vision, Challenges, and Opportunities(https://arxiv.org/abs/2501.09045)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have revolutionized artificial intelligence, setting new benchmarks in performance and enabling transformative capabilities across a wide range of vision and language tasks. However, despite the prevalence of spatio-temporal data in critical domains such as transportation, public health, and environmental monitoring, spatio-temporal foundation models (STFMs) have not yet achieved comparable success. In this paper, we articulate a vision for the future of STFMs, outlining their essential characteristics and the generalization capabilities necessary for broad applicability. We critically assess the current state of research, identifying gaps relative to these ideal traits, and highlight key challenges that impede their progress. Finally, we explore potential opportunities and directions to advance research towards the aim of effective and broadly applicable STFMs.</li>
</ul>

<h3>Title: Generating Realistic Synthetic Head Rotation Data for Extended Reality using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Jakob Struye, Filip Lemic, Jeroen Famaey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09050">https://arxiv.org/abs/2501.09050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09050">https://arxiv.org/pdf/2501.09050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09050]] Generating Realistic Synthetic Head Rotation Data for Extended Reality using Deep Learning(https://arxiv.org/abs/2501.09050)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Extended Reality is a revolutionary method of delivering multimedia content to users. A large contributor to its popularity is the sense of immersion and interactivity enabled by having real-world motion reflected in the virtual experience accurately and immediately. This user motion, mainly caused by head rotations, induces several technical challenges. For instance, which content is generated and transmitted depends heavily on where the user is looking. Seamless systems, taking user motion into account proactively, will therefore require accurate predictions of upcoming rotations. Training and evaluating such predictors requires vast amounts of orientational input data, which is expensive to gather, as it requires human test subjects. A more feasible approach is to gather a modest dataset through test subjects, and then extend it to a more sizeable set using synthetic data generation methods. In this work, we present a head rotation time series generator based on TimeGAN, an extension of the well-known Generative Adversarial Network, designed specifically for generating time series. This approach is able to extend a dataset of head rotations with new samples closely matching the distribution of the measured time series.</li>
</ul>

<h3>Title: SHYI: Action Support for Contrastive Learning in High-Fidelity Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianxiang Xia, Lin Xiao, Yannick Montorfani, Francesco Pavia, Enis Simsar, Thomas Hofmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09055">https://arxiv.org/abs/2501.09055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09055">https://arxiv.org/pdf/2501.09055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09055]] SHYI: Action Support for Contrastive Learning in High-Fidelity Text-to-Image Generation(https://arxiv.org/abs/2501.09055)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this project, we address the issue of infidelity in text-to-image generation, particularly for actions involving multiple objects. For this we build on top of the CONFORM framework which uses Contrastive Learning to improve the accuracy of the generated image for multiple objects. However the depiction of actions which involves multiple different object has still large room for improvement. To improve, we employ semantically hypergraphic contrastive adjacency learning, a comprehension of enhanced contrastive structure and "contrast but link" technique. We further amend Stable Diffusion's understanding of actions by InteractDiffusion. As evaluation metrics we use image-text similarity CLIP and TIFA. In addition, we conducted a user study. Our method shows promising results even with verbs that Stable Diffusion understands mediocrely. We then provide future directions by analyzing the results. Our codebase can be found on polybox under the link: this https URL</li>
</ul>

<h3>Title: Generative Medical Image Anonymization Based on Latent Code Projection and Optimization</h3>
<ul>
<li><strong>Authors: </strong>Huiyu Li, Nicholas Ayache, Hervé Delingette</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09114">https://arxiv.org/abs/2501.09114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09114">https://arxiv.org/pdf/2501.09114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09114]] Generative Medical Image Anonymization Based on Latent Code Projection and Optimization(https://arxiv.org/abs/2501.09114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Medical image anonymization aims to protect patient privacy by removing identifying information, while preserving the data utility to solve downstream tasks. In this paper, we address the medical image anonymization problem with a two-stage solution: latent code projection and optimization. In the projection stage, we design a streamlined encoder to project input images into a latent space and propose a co-training scheme to enhance the projection process. In the optimization stage, we refine the latent code using two deep loss functions designed to address the trade-off between identity protection and data utility dedicated to medical images. Through a comprehensive set of qualitative and quantitative experiments, we showcase the effectiveness of our approach on the MIMIC-CXR chest X-ray dataset by generating anonymized synthetic images that can serve as training set for detecting lung pathologies. Source codes are available at this https URL.</li>
</ul>

<h3>Title: Deep Self-Supervised Disturbance Mapping with the OPERA Sentinel-1 Radiometric Terrain Corrected SAR Backscatter Product</h3>
<ul>
<li><strong>Authors: </strong>Harris Hardiman-Mostow, Charles Marshak, Alexander L. Handwerger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09129">https://arxiv.org/abs/2501.09129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09129">https://arxiv.org/pdf/2501.09129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09129]] Deep Self-Supervised Disturbance Mapping with the OPERA Sentinel-1 Radiometric Terrain Corrected SAR Backscatter Product(https://arxiv.org/abs/2501.09129)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Mapping land surface disturbances supports disaster response, resource and ecosystem management, and climate adaptation efforts. Synthetic aperture radar (SAR) is an invaluable tool for disturbance mapping, providing consistent time-series images of the ground regardless of weather or illumination conditions. Despite SAR's potential for disturbance mapping, processing SAR data to an analysis-ready format requires expertise and significant compute resources, particularly for large-scale global analysis. In October 2023, NASA's Observational Products for End-Users from Remote Sensing Analysis (OPERA) project released the near-global Radiometric Terrain Corrected SAR backscatter from Sentinel-1 (RTC-S1) dataset, providing publicly available, analysis-ready SAR imagery. In this work, we utilize this new dataset to systematically analyze land surface disturbances. As labeling SAR data is often prohibitively time-consuming, we train a self-supervised vision transformer - which requires no labels to train - on OPERA RTC-S1 data to estimate a per-pixel distribution from the set of baseline imagery and assess disturbances when there is significant deviation from the modeled distribution. To test our model's capability and generality, we evaluate three different natural disasters - which represent high-intensity, abrupt disturbances - from three different regions of the world. Across events, our approach yields high quality delineations: F1 scores exceeding 0.6 and Areas Under the Precision-Recall Curve exceeding 0.65, consistently outperforming existing SAR disturbance methods. Our findings suggest that a self-supervised vision transformer is well-suited for global disturbance mapping and can be a valuable tool for operational, near-global disturbance monitoring, particularly when labeled data does not exist.</li>
</ul>

<h3>Title: Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xingxin He, Yifan Hu, Zhaoye Zhou, Mohamed Jarraya, Fang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09138">https://arxiv.org/abs/2501.09138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09138">https://arxiv.org/pdf/2501.09138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09138]] Few-Shot Adaptation of Training-Free Foundation Model for 3D Medical Image Segmentation(https://arxiv.org/abs/2501.09138)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision foundation models have achieved remarkable progress across various image analysis tasks. In the image segmentation task, foundation models like the Segment Anything Model (SAM) enable generalizable zero-shot segmentation through user-provided prompts. However, SAM primarily trained on natural images, lacks the domain-specific expertise of medical imaging. This limitation poses challenges when applying SAM to medical image segmentation, including the need for extensive fine-tuning on specialized medical datasets and a dependency on manual prompts, which are both labor-intensive and require intervention from medical experts. This work introduces the Few-shot Adaptation of Training-frEe SAM (FATE-SAM), a novel method designed to adapt the advanced Segment Anything Model 2 (SAM2) for 3D medical image segmentation. FATE-SAM reassembles pre-trained modules of SAM2 to enable few-shot adaptation, leveraging a small number of support examples to capture anatomical knowledge and perform prompt-free segmentation, without requiring model fine-tuning. To handle the volumetric nature of medical images, we incorporate a Volumetric Consistency mechanism that enhances spatial coherence across 3D slices. We evaluate FATE-SAM on multiple medical imaging datasets and compare it with supervised learning methods, zero-shot SAM approaches, and fine-tuned medical SAM methods. Results show that FATE-SAM delivers robust and accurate segmentation while eliminating the need for large annotated datasets and expert intervention. FATE-SAM provides a practical, efficient solution for medical image segmentation, making it more accessible for clinical applications.</li>
</ul>

<h3>Title: Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy and Consistency for Enhanced Readability</h3>
<ul>
<li><strong>Authors: </strong>Stephanie L. Day, Jacapo Cirica, Steven R. Clapp, Veronika Penkova, Amy E. Giroux, Abbey Banta, Catherine Bordeau, Poojitha Mutteneni, Ben D. Sawyer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09158">https://arxiv.org/abs/2501.09158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09158">https://arxiv.org/pdf/2501.09158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09158]] Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy and Consistency for Enhanced Readability(https://arxiv.org/abs/2501.09158)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (GenAI) holds great promise as a tool to support personalized learning. Teachers need tools to efficiently and effectively enhance content readability of educational texts so that they are matched to individual students reading levels, while retaining key details. Large Language Models (LLMs) show potential to fill this need, but previous research notes multiple shortcomings in current approaches. In this study, we introduced a generalized approach and metrics for the systematic evaluation of the accuracy and consistency in which LLMs, prompting techniques, and a novel multi-agent architecture to simplify sixty informational reading passages, reducing each from the twelfth grade level down to the eighth, sixth, and fourth grade levels. We calculated the degree to which each LLM and prompting technique accurately achieved the targeted grade level for each passage, percentage change in word count, and consistency in maintaining keywords and key phrases (semantic similarity). One-sample t-tests and multiple regression models revealed significant differences in the best performing LLM and prompt technique for each of the four metrics. Both LLMs and prompting techniques demonstrated variable utility in grade level accuracy and consistency of keywords and key phrases when attempting to level content down to the fourth grade reading level. These results demonstrate the promise of the application of LLMs for efficient and precise automated text simplification, the shortcomings of current models and prompting methods in attaining an ideal balance across various evaluation criteria, and a generalizable method to evaluate future systems.</li>
</ul>

<h3>Title: Attention is All You Need Until You Need Retention</h3>
<ul>
<li><strong>Authors: </strong>M. Murat Yaslioglu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09166">https://arxiv.org/abs/2501.09166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09166">https://arxiv.org/pdf/2501.09166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09166]] Attention is All You Need Until You Need Retention(https://arxiv.org/abs/2501.09166)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This work introduces a novel Retention Layer mechanism for Transformer based architectures, addressing their inherent lack of intrinsic retention capabilities. Unlike human cognition, which can encode and dynamically recall symbolic templates, Generative Pretrained Transformers rely solely on fixed pretrained weights and ephemeral context windows, limiting their adaptability. The proposed Retention Layer incorporates a persistent memory module capable of real time data population, dynamic recall, and guided output generation. This enhancement allows models to store, update, and reuse observed patterns across sessions, enabling incremental learning and bridging the gap between static pretraining and dynamic, context sensitive adaptation. The Retention Layer design parallels social learning processes, encompassing attention, retention, reproduction, and motivation stages. Technically, it integrates a memory attention mechanism and episodic buffers to manage memory scalability, mitigate overfitting, and ensure efficient recall. Applications span adaptive personal assistants, real time fraud detection, autonomous robotics, content moderation, and healthcare diagnostics. In each domain, the retention mechanism enables systems to learn incrementally, personalize outputs, and respond to evolving real world challenges effectively. By emulating key aspects of human learning, this retention enhanced architecture fosters a more fluid and responsive AI paradigm, paving the way for dynamic, session aware models that extend the capabilities of traditional Transformers into domains requiring continual adaptation.</li>
</ul>

<h3>Title: Grounding Text-To-Image Diffusion Models For Controlled High-Quality Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Süleyman, Göksel Biricik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09194">https://arxiv.org/abs/2501.09194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09194">https://arxiv.org/pdf/2501.09194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09194]] Grounding Text-To-Image Diffusion Models For Controlled High-Quality Image Generation(https://arxiv.org/abs/2501.09194)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-image (T2I) diffusion models have demonstrated an outstanding performance in synthesizing diverse high-quality visuals from natural language text captions. Multiple layout-to-image models have been developed to control the generation process by utilizing a broad array of layouts such as segmentation maps, edges, and human keypoints. In this work, we present ObjectDiffusion, a model that takes inspirations from the top cutting-edge image generative frameworks to seamlessly condition T2I models with new bounding boxes capabilities. Specifically, we make substantial modifications to the network architecture introduced in ContorlNet to integrate it with the condition processing and injection techniques proposed in GLIGEN. ObjectDiffusion is initialized with pretraining parameters to leverage the generation knowledge obtained from training on large-scale datasets. We fine-tune ObjectDiffusion on the COCO2017 training dataset and evaluate it on the COCO2017 validation dataset. Our model achieves an AP$_{50}$ of 46.6, an AR of 44.5, and a FID of 19.8 outperforming the current SOTA model trained on open-source datasets in all of the three metrics. ObjectDiffusion demonstrates a distinctive capability in synthesizing diverse, high-quality, high-fidelity images that seamlessly conform to the semantic and spatial control layout. Evaluated in qualitative and quantitative tests, ObjectDiffusion exhibits remarkable grounding abilities on closed-set and open-set settings across a wide variety of contexts. The qualitative assessment verifies the ability of ObjectDiffusion to generate multiple objects of different sizes and locations.</li>
</ul>

<h3>Title: Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures</h3>
<ul>
<li><strong>Authors: </strong>Pengru Deng, Jiapeng Yao, Chun Li, Su Wang, Xinrun Li, Varun Ojha, Xuhui He, Takashi Matsumoto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09203">https://arxiv.org/abs/2501.09203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09203">https://arxiv.org/pdf/2501.09203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09203]] Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures(https://arxiv.org/abs/2501.09203)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Visual-Spatial Systems has become increasingly essential in concrete crack inspection. However, existing methods often lacks adaptability to diverse scenarios, exhibits limited robustness in image-based approaches, and struggles with curved or complex geometries. To address these limitations, an innovative framework for two-dimensional (2D) crack detection, three-dimensional (3D) reconstruction, and 3D automatic crack measurement was proposed by integrating computer vision technologies and multi-modal Simultaneous localization and mapping (SLAM) in this study. Firstly, building on a base DeepLabv3+ segmentation model, and incorporating specific refinements utilizing foundation model Segment Anything Model (SAM), we developed a crack segmentation method with strong generalization across unfamiliar scenarios, enabling the generation of precise 2D crack masks. To enhance the accuracy and robustness of 3D reconstruction, Light Detection and Ranging (LiDAR) point clouds were utilized together with image data and segmentation masks. By leveraging both image- and LiDAR-SLAM, we developed a multi-frame and multi-modal fusion framework that produces dense, colorized point clouds, effectively capturing crack semantics at a 3D real-world scale. Furthermore, the crack geometric attributions were measured automatically and directly within 3D dense point cloud space, surpassing the limitations of conventional 2D image-based measurements. This advancement makes the method suitable for structural components with curved and complex 3D geometries. Experimental results across various concrete structures highlight the significant improvements and unique advantages of the proposed method, demonstrating its effectiveness, accuracy, and robustness in real-world applications.</li>
</ul>

<h3>Title: Leveraging Scale-aware Representations for improved Concept-Representation Alignment in ViTs</h3>
<ul>
<li><strong>Authors: </strong>Sanchit Sinha, Guangzhi Xiong, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09221">https://arxiv.org/abs/2501.09221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09221">https://arxiv.org/pdf/2501.09221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09221]] Leveraging Scale-aware Representations for improved Concept-Representation Alignment in ViTs(https://arxiv.org/abs/2501.09221)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) are increasingly being adopted in various sensitive vision applications - like medical diagnosis, facial recognition, etc. To improve the interpretability of such models, many approaches attempt to forward-align them with carefully annotated abstract, human-understandable semantic entities - concepts. Concepts provide global rationales to the model predictions and can be quickly understood/intervened on by domain experts. Most current research focuses on designing model-agnostic, plug-and-play generic concept-based explainability modules that do not incorporate the inner workings of foundation models (e.g., inductive biases, scale invariance, etc.) during training. To alleviate this issue for ViTs, in this paper, we propose a novel Concept Representation Alignment Module (CRAM) which learns both scale and position-aware representations from multi-scale feature pyramids and patch representations respectively. CRAM further aligns these representations with concept annotations through an attention matrix. The proposed CRAM module improves the predictive performance of ViT architectures and also provides accurate and robust concept explanations as demonstrated on five datasets - including three widely used benchmarks (CUB, Pascal APY, Concept-MNIST) and 2 real-world datasets (AWA2, KITS).</li>
</ul>

<h3>Title: Foundations of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tong Xiao, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09223">https://arxiv.org/abs/2501.09223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09223">https://arxiv.org/pdf/2501.09223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09223]] Foundations of Large Language Models(https://arxiv.org/abs/2501.09223)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into four main chapters, each exploring a key area: pre-training, generative models, prompting techniques, and alignment methods. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.</li>
</ul>

<h3>Title: Task Vectors in In-Context Learning: Emergence, Formation, and Benefit</h3>
<ul>
<li><strong>Authors: </strong>Liu Yang, Ziqian Lin, Kangwook Lee, Dimitris Papailiopoulos, Robert Nowak</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09240">https://arxiv.org/abs/2501.09240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09240">https://arxiv.org/pdf/2501.09240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09240]] Task Vectors in In-Context Learning: Emergence, Formation, and Benefit(https://arxiv.org/abs/2501.09240)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning is a remarkable capability of transformers, referring to their ability to adapt to specific tasks based on a short history or context. Previous research has found that task-specific information is locally encoded within models, though their emergence and functionality remain unclear due to opaque pre-training processes. In this work, we investigate the formation of task vectors in a controlled setting, using models trained from scratch on synthetic datasets. Our findings confirm that task vectors naturally emerge under certain conditions, but the tasks may be relatively weakly and/or non-locally encoded within the model. To promote strong task vectors encoded at a prescribed location within the model, we propose an auxiliary training mechanism based on a task vector prompting loss (TVP-loss). This method eliminates the need to search for task-correlated encodings within the trained model and demonstrably improves robustness and generalization.</li>
</ul>

<h3>Title: Perspective Transition of Large Language Models for Solving Subjective Tasks</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09265">https://arxiv.org/abs/2501.09265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09265">https://arxiv.org/pdf/2501.09265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09265]] Perspective Transition of Large Language Models for Solving Subjective Tasks(https://arxiv.org/abs/2501.09265)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMs may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.</li>
</ul>

<h3>Title: Text-guided Synthetic Geometric Augmentation for Zero-shot 3D Understanding</h3>
<ul>
<li><strong>Authors: </strong>Kohei Torimi, Ryosuke Yamada, Daichi Otsuka, Kensho Hara, Yuki M. Asano, Hirokatsu Kataoka, Yoshimitsu Aoki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09278">https://arxiv.org/abs/2501.09278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09278">https://arxiv.org/pdf/2501.09278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09278]] Text-guided Synthetic Geometric Augmentation for Zero-shot 3D Understanding(https://arxiv.org/abs/2501.09278)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Zero-shot recognition models require extensive training data for generalization. However, in zero-shot 3D classification, collecting 3D data and captions is costly and laborintensive, posing a significant barrier compared to 2D vision. Recent advances in generative models have achieved unprecedented realism in synthetic data production, and recent research shows the potential for using generated data as training data. Here, naturally raising the question: Can synthetic 3D data generated by generative models be used as expanding limited 3D datasets? In response, we present a synthetic 3D dataset expansion method, Textguided Geometric Augmentation (TeGA). TeGA is tailored for language-image-3D pretraining, which achieves SoTA in zero-shot 3D classification, and uses a generative textto-3D model to enhance and extend limited 3D datasets. Specifically, we automatically generate text-guided synthetic 3D data and introduce a consistency filtering strategy to discard noisy samples where semantics and geometric shapes do not match with text. In the experiment to double the original dataset size using TeGA, our approach demonstrates improvements over the baselines, achieving zeroshot performance gains of 3.0% on Objaverse-LVIS, 4.6% on ScanObjectNN, and 8.7% on ModelNet40. These results demonstrate that TeGA effectively bridges the 3D data gap, enabling robust zero-shot 3D classification even with limited real training data and paving the way for zero-shot 3D vision application.</li>
</ul>

<h3>Title: A Study of In-Context-Learning-Based Text-to-SQL Errors</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Shen, Chengcheng Wan, Ruoyi Qiao, Jiazhen Zou, Hang Xu, Yuchen Shao, Yueling Zhang, Weikai Miao, Geguang Pu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09310">https://arxiv.org/abs/2501.09310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09310">https://arxiv.org/pdf/2501.09310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09310]] A Study of In-Context-Learning-Based Text-to-SQL Errors(https://arxiv.org/abs/2501.09310)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been adopted to perform text-to-SQL tasks, utilizing their in-context learning (ICL) capability to translate natural language questions into structured query language (SQL). However, such a technique faces correctness problems and requires efficient repairing solutions. In this paper, we conduct the first comprehensive study of text-to-SQL errors. Our study covers four representative ICL-based techniques, five basic repairing methods, two benchmarks, and two LLM settings. We find that text-to-SQL errors are widespread and summarize 29 error types of 7 categories. We also find that existing repairing attempts have limited correctness improvement at the cost of high computational overhead with many mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL error detection and repairing framework. The evaluation demonstrates that MapleRepair outperforms existing solutions by repairing 13.8% more queries with neglectable mis-repairs and 67.4% less overhead.</li>
</ul>

<h3>Title: UVRM: A Scalable 3D Reconstruction Model from Unposed Videos</h3>
<ul>
<li><strong>Authors: </strong>Shiu-hong Kao, Xiao Li, Jinglu Wang, Chi-Keung Tang, Yu-Wing Tai, Yan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09347">https://arxiv.org/abs/2501.09347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09347">https://arxiv.org/pdf/2501.09347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09347]] UVRM: A Scalable 3D Reconstruction Model from Unposed Videos(https://arxiv.org/abs/2501.09347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large Reconstruction Models (LRMs) have recently become a popular method for creating 3D foundational models. Training 3D reconstruction models with 2D visual data traditionally requires prior knowledge of camera poses for the training samples, a process that is both time-consuming and prone to errors. Consequently, 3D reconstruction training has been confined to either synthetic 3D datasets or small-scale datasets with annotated poses. In this study, we investigate the feasibility of 3D reconstruction using unposed video data of various objects. We introduce UVRM, a novel 3D reconstruction model capable of being trained and evaluated on monocular videos without requiring any information about the pose. UVRM uses a transformer network to implicitly aggregate video frames into a pose-invariant latent feature space, which is then decoded into a tri-plane 3D representation. To obviate the need for ground-truth pose annotations during training, UVRM employs a combination of the score distillation sampling (SDS) method and an analysis-by-synthesis approach, progressively synthesizing pseudo novel-views using a pre-trained diffusion model. We qualitatively and quantitatively evaluate UVRM's performance on the G-Objaverse and CO3D datasets without relying on pose information. Extensive experiments show that UVRM is capable of effectively and efficiently reconstructing a wide range of 3D objects from unposed videos.</li>
</ul>

<h3>Title: Strategic Base Representation Learning via Feature Augmentations for Few-Shot Class Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Parinita Nema, Vinod K Kurmi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09361">https://arxiv.org/abs/2501.09361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09361">https://arxiv.org/pdf/2501.09361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09361]] Strategic Base Representation Learning via Feature Augmentations for Few-Shot Class Incremental Learning(https://arxiv.org/abs/2501.09361)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Few-shot class incremental learning implies the model to learn new classes while retaining knowledge of previously learned classes with a small number of training instances. Existing frameworks typically freeze the parameters of the previously learned classes during the incorporation of new classes. However, this approach often results in suboptimal class separation of previously learned classes, leading to overlap between old and new classes. Consequently, the performance of old classes degrades on new classes. To address these challenges, we propose a novel feature augmentation driven contrastive learning framework designed to enhance the separation of previously learned classes to accommodate new classes. Our approach involves augmenting feature vectors and assigning proxy labels to these vectors. This strategy expands the feature space, ensuring seamless integration of new classes within the expanded space. Additionally, we employ a self-supervised contrastive loss to improve the separation between previous classes. We validate our framework through experiments on three FSCIL benchmark datasets: CIFAR100, miniImageNet, and CUB200. The results demonstrate that our Feature Augmentation driven Contrastive Learning framework significantly outperforms other approaches, achieving state-of-the-art performance.</li>
</ul>

<h3>Title: Evaluating LLM Abilities to Understand Tabular Electronic Health Records: A Comprehensive Study of Patient Data Extraction and Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Jesus Lovon (IRIT-IRIS), Martin Mouysset (IRIT-IRIS), Jo Oleiwan (IRIT-IRIS), Jose G. Moreno (IRIT-IRIS), Christine Damase-Michel, Lynda Tamine (IRIT-IRIS)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09384">https://arxiv.org/abs/2501.09384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09384">https://arxiv.org/pdf/2501.09384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09384]] Evaluating LLM Abilities to Understand Tabular Electronic Health Records: A Comprehensive Study of Patient Data Extraction and Retrieval(https://arxiv.org/abs/2501.09384)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Electronic Health Record (EHR) tables pose unique challenges among which is the presence of hidden contextual dependencies between medical features with a high level of data dimensionality and sparsity. This study presents the first investigation into the abilities of LLMs to comprehend EHRs for patient data extraction and retrieval. We conduct extensive experiments using the MIMICSQL dataset to explore the impact of the prompt structure, instruction, context, and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task performance. Through quantitative and qualitative analyses, our findings show that optimal feature selection and serialization methods can enhance task performance by up to 26.79% compared to naive approaches. Similarly, in-context learning setups with relevant example selection improve data extraction performance by 5.95%. Based on our study findings, we propose guidelines that we believe would help the design of LLM-based models to support health search.</li>
</ul>

<h3>Title: Towards Robust and Realistic Human Pose Estimation via WiFi Signals</h3>
<ul>
<li><strong>Authors: </strong>Yang Chen, Jingcai Guo, Song Guo, Jingren Zhou, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09411">https://arxiv.org/abs/2501.09411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09411">https://arxiv.org/pdf/2501.09411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09411]] Towards Robust and Realistic Human Pose Estimation via WiFi Signals(https://arxiv.org/abs/2501.09411)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Robust WiFi-based human pose estimation is a challenging task that bridges discrete and subtle WiFi signals to human skeletons. This paper revisits this problem and reveals two critical yet overlooked issues: 1) cross-domain gap, i.e., due to significant variations between source-target domain pose distributions; and 2) structural fidelity gap, i.e., predicted skeletal poses manifest distorted topology, usually with misplaced joints and disproportionate bone lengths. This paper fills these gaps by reformulating the task into a novel two-phase framework dubbed DT-Pose: Domain-consistent representation learning and Topology-constrained Pose decoding. Concretely, we first propose a temporal-consistent contrastive learning strategy with uniformity regularization, coupled with self-supervised masking-reconstruction operations, to enable robust learning of domain-consistent and motion-discriminative WiFi-specific representations. Beyond this, we introduce a simple yet effective pose decoder with task prompts, which integrates Graph Convolution Network (GCN) and Transformer layers to constrain the topology structure of the generated skeleton by exploring the adjacent-overarching relationships among human joints. Extensive experiments conducted on various benchmark datasets highlight the superior performance of our method in tackling these fundamental challenges in both 2D/3D human pose estimation tasks.</li>
</ul>

<h3>Title: AugRefer: Advancing 3D Visual Grounding via Cross-Modal Augmentation and Spatial Relation-based Referring</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Wang, Na Zhao, Zhiyuan Han, Dan Guo, Xun Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09428">https://arxiv.org/abs/2501.09428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09428">https://arxiv.org/pdf/2501.09428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09428]] AugRefer: Advancing 3D Visual Grounding via Cross-Modal Augmentation and Spatial Relation-based Referring(https://arxiv.org/abs/2501.09428)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>3D visual grounding (3DVG), which aims to correlate a natural language description with the target object within a 3D scene, is a significant yet challenging task. Despite recent advancements in this domain, existing approaches commonly encounter a shortage: a limited amount and diversity of text3D pairs available for training. Moreover, they fall short in effectively leveraging different contextual clues (e.g., rich spatial relations within the 3D visual space) for grounding. To address these limitations, we propose AugRefer, a novel approach for advancing 3D visual grounding. AugRefer introduces cross-modal augmentation designed to extensively generate diverse text-3D pairs by placing objects into 3D scenes and creating accurate and semantically rich descriptions using foundation models. Notably, the resulting pairs can be utilized by any existing 3DVG methods for enriching their training data. Additionally, AugRefer presents a language-spatial adaptive decoder that effectively adapts the potential referring objects based on the language description and various 3D spatial relations. Extensive experiments on three benchmark datasets clearly validate the effectiveness of AugRefer.</li>
</ul>

<h3>Title: CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation</h3>
<ul>
<li><strong>Authors: </strong>Hwan Heo, Jangyeong Kim, Seongyeong Lee, Jeong A Wi, Junyoung Choi, Sangjun Ahn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09433">https://arxiv.org/abs/2501.09433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09433">https://arxiv.org/pdf/2501.09433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09433]] CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation(https://arxiv.org/abs/2501.09433)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The synthesis of high-quality 3D assets from textual or visual inputs has become a central objective in modern generative modeling. Despite the proliferation of 3D generation algorithms, they frequently grapple with challenges such as multi-view inconsistency, slow generation times, low fidelity, and surface reconstruction problems. While some studies have addressed some of these issues, a comprehensive solution remains elusive. In this paper, we introduce \textbf{CaPa}, a carve-and-paint framework that generates high-fidelity 3D assets efficiently. CaPa employs a two-stage process, decoupling geometry generation from texture synthesis. Initially, a 3D latent diffusion model generates geometry guided by multi-view inputs, ensuring structural consistency across perspectives. Subsequently, leveraging a novel, model-agnostic Spatially Decoupled Attention, the framework synthesizes high-resolution textures (up to 4K) for a given geometry. Furthermore, we propose a 3D-aware occlusion inpainting algorithm that fills untextured regions, resulting in cohesive results across the entire model. This pipeline generates high-quality 3D assets in less than 30 seconds, providing ready-to-use outputs for commercial applications. Experimental results demonstrate that CaPa excels in both texture fidelity and geometric stability, establishing a new standard for practical, scalable 3D asset generation.</li>
</ul>

<h3>Title: Scaling up self-supervised learning for improved surgical foundation models</h3>
<ul>
<li><strong>Authors: </strong>Tim J.M. Jaspers, Ronald L.P.D. de Jong, Yiping Li, Carolus H.J. Kusters, Franciscus H.A. Bakker, Romy C. van Jaarsveld, Gino M. Kuiper, Richard van Hillegersberg, Jelle P. Ruurda, Willem M. Brinkman, Josien P.W. Pluim, Peter H.N. de With, Marcel Breeuwer, Yasmina Al Khalil, Fons van der Sommen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09436">https://arxiv.org/abs/2501.09436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09436">https://arxiv.org/pdf/2501.09436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09436]] Scaling up self-supervised learning for improved surgical foundation models(https://arxiv.org/abs/2501.09436)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have revolutionized computer vision by achieving vastly superior performance across diverse tasks through large-scale pretraining on extensive datasets. However, their application in surgical computer vision has been limited. This study addresses this gap by introducing SurgeNetXL, a novel surgical foundation model that sets a new benchmark in surgical computer vision. Trained on the largest reported surgical dataset to date, comprising over 4.7 million video frames, SurgeNetXL achieves consistent top-tier performance across six datasets spanning four surgical procedures and three tasks, including semantic segmentation, phase recognition, and critical view of safety (CVS) classification. Compared with the best-performing surgical foundation models, SurgeNetXL shows mean improvements of 2.4, 9.0, and 12.6 percent for semantic segmentation, phase recognition, and CVS classification, respectively. Additionally, SurgeNetXL outperforms the best-performing ImageNet-based variants by 14.4, 4.0, and 1.6 percent in the respective tasks. In addition to advancing model performance, this study provides key insights into scaling pretraining datasets, extending training durations, and optimizing model architectures specifically for surgical computer vision. These findings pave the way for improved generalizability and robustness in data-scarce scenarios, offering a comprehensive framework for future research in this domain. All models and a subset of the SurgeNetXL dataset, including over 2 million video frames, are publicly available at: this https URL.</li>
</ul>

<h3>Title: Teaching Wav2Vec2 the Language of the Brain</h3>
<ul>
<li><strong>Authors: </strong>Tobias Fiedler, Leon Hermann, Florian Müller, Sarel Cohen, Peter Chin, Tobias Friedrich, Eilon Vaadia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09459">https://arxiv.org/abs/2501.09459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09459">https://arxiv.org/pdf/2501.09459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09459]] Teaching Wav2Vec2 the Language of the Brain(https://arxiv.org/abs/2501.09459)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The decoding of continuously spoken speech from neuronal activity has the potential to become an important clinical solution for paralyzed patients. Deep Learning Brain Computer Interfaces (BCIs) have recently successfully mapped neuronal activity to text contents in subjects who attempted to formulate speech. However, only small BCI datasets are available. In contrast, labeled data and pre-trained models for the closely related task of speech recognition from audio are widely available. One such model is Wav2Vec2 which has been trained in a self-supervised fashion to create meaningful representations of speech audio data. In this study, we show that patterns learned by Wav2Vec2 are transferable to brain data. Specifically, we replace its audio feature extractor with an untrained Brain Feature Extractor (BFE) model. We then execute full fine-tuning with pre-trained weights for Wav2Vec2, training ''from scratch'' without pre-trained weights as well as freezing a pre-trained Wav2Vec2 and training only the BFE each for 45 different BFE architectures. Across these experiments, the best run is from full fine-tuning with pre-trained weights, achieving a Character Error Rate (CER) of 18.54\%, outperforming the best training from scratch run by 20.46\% and that of frozen Wav2Vec2 training by 15.92\% percentage points. These results indicate that knowledge transfer from audio speech recognition to brain decoding is possible and significantly improves brain decoding performance for the same architectures. Related source code is available at this https URL.</li>
</ul>

<h3>Title: Pruning for Sparse Diffusion Models based on Gradient Flow</h3>
<ul>
<li><strong>Authors: </strong>Ben Wan, Tianyi Zheng, Zhaoyu Chen, Yuxiao Wang, Jia Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09464">https://arxiv.org/abs/2501.09464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09464">https://arxiv.org/pdf/2501.09464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09464]] Pruning for Sparse Diffusion Models based on Gradient Flow(https://arxiv.org/abs/2501.09464)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Models (DMs) have impressive capabilities among generation models, but are limited to slower inference speeds and higher computational costs. Previous works utilize one-shot structure pruning to derive lightweight DMs from pre-trained ones, but this approach often leads to a significant drop in generation quality and may result in the removal of crucial weights. Thus we propose a iterative pruning method based on gradient flow, including the gradient flow pruning process and the gradient flow pruning criterion. We employ a progressive soft pruning strategy to maintain the continuity of the mask matrix and guide it along the gradient flow of the energy function based on the pruning criterion in sparse space, thereby avoiding the sudden information loss typically caused by one-shot pruning. Gradient-flow based criterion prune parameters whose removal increases the gradient norm of loss function and can enable fast convergence for a pruned model in iterative pruning stage. Our extensive experiments on widely used datasets demonstrate that our method achieves superior performance in efficiency and consistency with pre-trained models.</li>
</ul>

<h3>Title: DEFOM-Stereo: Depth Foundation Model Based Stereo Matching</h3>
<ul>
<li><strong>Authors: </strong>Hualie Jiang, Zhiqiang Lou, Laiyan Ding, Rui Xu, Minglang Tan, Wenjie Jiang, Rui Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09466">https://arxiv.org/abs/2501.09466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09466">https://arxiv.org/pdf/2501.09466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09466]] DEFOM-Stereo: Depth Foundation Model Based Stereo Matching(https://arxiv.org/abs/2501.09466)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Stereo matching is a key technique for metric depth estimation in computer vision and robotics. Real-world challenges like occlusion and non-texture hinder accurate disparity estimation from binocular matching cues. Recently, monocular relative depth estimation has shown remarkable generalization using vision foundation models. Thus, to facilitate robust stereo matching with monocular depth cues, we incorporate a robust monocular relative depth model into the recurrent stereo-matching framework, building a new framework for depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature extraction stage, we construct the combined context and matching feature encoder by integrating features from conventional CNNs and DEFOM. In the update stage, we use the depth predicted by DEFOM to initialize the recurrent disparity and introduce a scale update module to refine the disparity at the correct scale. DEFOM-Stereo is verified to have comparable performance on the Scene Flow dataset with state-of-the-art (SOTA) methods and notably shows much stronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks, ranking 1st on many metrics. In the joint evaluation under the robust vision challenge, our model simultaneously outperforms previous models on the individual benchmarks. Both results demonstrate the outstanding capabilities of the proposed model.</li>
</ul>

<h3>Title: VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization</h3>
<ul>
<li><strong>Authors: </strong>Zixun Fang, Zhiheng Liu, Kai Zhu, Yu Liu, Ka Leong Cheng, Wei Zhai, Yang Cao, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09499">https://arxiv.org/abs/2501.09499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09499">https://arxiv.org/pdf/2501.09499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09499]] VanGogh: A Unified Multimodal Diffusion-based Framework for Video Colorization(https://arxiv.org/abs/2501.09499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video colorization aims to transform grayscale videos into vivid color representations while maintaining temporal consistency and structural integrity. Existing video colorization methods often suffer from color bleeding and lack comprehensive control, particularly under complex motion or diverse semantic cues. To this end, we introduce VanGogh, a unified multimodal diffusion-based framework for video colorization. VanGogh tackles these challenges using a Dual Qformer to align and fuse features from multiple modalities, complemented by a depth-guided generation process and an optical flow loss, which help reduce color overflow. Additionally, a color injection strategy and luma channel replacement are implemented to improve generalization and mitigate flickering artifacts. Thanks to this design, users can exercise both global and local control over the generation process, resulting in higher-quality colorized videos. Extensive qualitative and quantitative evaluations, and user studies, demonstrate that VanGogh achieves superior temporal consistency and color this http URL page: this https URL.</li>
</ul>

<h3>Title: AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09503">https://arxiv.org/abs/2501.09503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09503">https://arxiv.org/pdf/2501.09503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09503]] AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation(https://arxiv.org/abs/2501.09503)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an "encode-then-route" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at this https URL .</li>
</ul>

<h3>Title: Confidence Estimation for Error Detection in Text-to-SQL Systems</h3>
<ul>
<li><strong>Authors: </strong>Oleg Somov, Elena Tutubalina</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09527">https://arxiv.org/abs/2501.09527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09527">https://arxiv.org/pdf/2501.09527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09527]] Confidence Estimation for Error Detection in Text-to-SQL Systems(https://arxiv.org/abs/2501.09527)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Text-to-SQL enables users to interact with databases through natural language, simplifying the retrieval and synthesis of information. Despite the success of large language models (LLMs) in converting natural language questions into SQL queries, their broader adoption is limited by two main challenges: achieving robust generalization across diverse queries and ensuring interpretative confidence in their predictions. To tackle these issues, our research investigates the integration of selective classifiers into Text-to-SQL systems. We analyse the trade-off between coverage and risk using entropy based confidence estimation with selective classifiers and assess its impact on the overall performance of Text-to-SQL models. Additionally, we explore the models' initial calibration and improve it with calibration techniques for better model alignment between confidence and accuracy. Our experimental results show that encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and decoder-only Llama 3, thus the designated external entropy-based selective classifier has better performance. The study also reveal that, in terms of error detection, selective classifier with a higher probability detects errors associated with irrelevant questions rather than incorrect query generations.</li>
</ul>

<h3>Title: Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis</h3>
<ul>
<li><strong>Authors: </strong>Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09555">https://arxiv.org/abs/2501.09555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09555">https://arxiv.org/pdf/2501.09555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09555]] Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis(https://arxiv.org/abs/2501.09555)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety. However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data. Methods: Our approach has two key components. First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap. Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data. This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs. Results: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition). Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks. Conclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets. The code and dataset will be released in this https URL.</li>
</ul>

<h3>Title: Sequential PatchCore: Anomaly Detection for Surface Inspection using Synthetic Impurities</h3>
<ul>
<li><strong>Authors: </strong>Runzhou Mao, Juraj Fulir, Christoph Garth, Petra Gospodnetić</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09579">https://arxiv.org/abs/2501.09579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09579">https://arxiv.org/pdf/2501.09579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09579]] Sequential PatchCore: Anomaly Detection for Surface Inspection using Synthetic Impurities(https://arxiv.org/abs/2501.09579)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The appearance of surface impurities (e.g., water stains, fingerprints, stickers) is an often-mentioned issue that causes degradation of automated visual inspection systems. At the same time, synthetic data generation techniques for visual surface inspection have focused primarily on generating perfect examples and defects, disregarding impurities. This study highlights the importance of considering impurities when generating synthetic data. We introduce a procedural method to include photorealistic water stains in synthetic data. The synthetic datasets are generated to correspond to real datasets and are further used to train an anomaly detection model and investigate the influence of water stains. The high-resolution images used for surface inspection lead to memory bottlenecks during anomaly detection training. To address this, we introduce Sequential PatchCore - a method to build coresets sequentially and make training on large images using consumer-grade hardware tractable. This allows us to perform transfer learning using coresets pre-trained on different dataset versions. Our results show the benefits of using synthetic data for pre-training an explicit coreset anomaly model and the extended performance benefits of finetuning the coreset using real data. We observed how the impurities and labelling ambiguity lower the model performance and have additionally reported the defect-wise recall to provide an industrially relevant perspective on model performance.</li>
</ul>

<h3>Title: Cueless EEG imagined speech for subject identification: dataset and benchmarks</h3>
<ul>
<li><strong>Authors: </strong>Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09700">https://arxiv.org/abs/2501.09700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09700">https://arxiv.org/pdf/2501.09700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09700]] Cueless EEG imagined speech for subject identification: dataset and benchmarks(https://arxiv.org/abs/2501.09700)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Electroencephalogram (EEG) signals have emerged as a promising modality for biometric identification. While previous studies have explored the use of imagined speech with semantically meaningful words for subject identification, most have relied on additional visual or auditory cues. In this study, we introduce a cueless EEG-based imagined speech paradigm, where subjects imagine the pronunciation of semantically meaningful words without any external cues. This innovative approach addresses the limitations of prior methods by requiring subjects to select and imagine words from a predefined list naturally. The dataset comprises over 4,350 trials from 11 subjects across five sessions. We assess a variety of classification methods, including traditional machine learning techniques such as Support Vector Machines (SVM) and XGBoost, as well as time-series foundation models and deep learning architectures specifically designed for EEG classification, such as EEG Conformer and Shallow ConvNet. A session-based hold-out validation strategy was employed to ensure reliable evaluation and prevent data leakage. Our results demonstrate outstanding classification accuracy, reaching 97.93%. These findings highlight the potential of cueless EEG paradigms for secure and reliable subject identification in real-world applications, such as brain-computer interfaces (BCIs).</li>
</ul>

<h3>Title: Domain Adaptation of Foundation LLMs for e-Commerce</h3>
<ul>
<li><strong>Authors: </strong>Christian Herold, Michael Kozielski, Tala Bazazo, Pavel Petrushkov, Hadi Hashemi, Patrycja Cieplicka, Dominika Basaj, Shahram Khadivi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09706">https://arxiv.org/abs/2501.09706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09706">https://arxiv.org/pdf/2501.09706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09706]] Domain Adaptation of Foundation LLMs for e-Commerce(https://arxiv.org/abs/2501.09706)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present the e-Llama models: 8 billion and 70 billion parameter large language models that are adapted towards the e-commerce domain. These models are meant as foundation models with deep knowledge about e-commerce, that form a base for instruction- and fine-tuning. The e-Llama models are obtained by continuously pretraining the Llama 3.1 base models on 1 trillion tokens of domain-specific data. We discuss our approach and motivate our choice of hyperparameters with a series of ablation studies. To quantify how well the models have been adapted to the e-commerce domain, we define and implement a set of multilingual, e-commerce specific evaluation tasks. We show that, when carefully choosing the training setup, the Llama 3.1 models can be adapted towards the new domain without sacrificing significant performance on general domain tasks. We also explore the possibility of merging the adapted model and the base model for a better control of the performance trade-off between domains.</li>
</ul>

<h3>Title: Comparative Insights from 12 Machine Learning Models in Extracting Economic Ideology from Political Text</h3>
<ul>
<li><strong>Authors: </strong>Jihed Ncib</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09719">https://arxiv.org/abs/2501.09719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09719">https://arxiv.org/pdf/2501.09719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09719]] Comparative Insights from 12 Machine Learning Models in Extracting Economic Ideology from Political Text(https://arxiv.org/abs/2501.09719)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study conducts a systematic assessment of the capabilities of 12 machine learning models and model variations in detecting economic ideology. As an evaluation benchmark, I use manifesto data spanning six elections in the United Kingdom and pre-annotated by expert and crowd coders. The analysis assesses the performance of several generative, fine-tuned, and zero-shot models at the granular and aggregate levels. The results show that generative models such as GPT-4o and Gemini 1.5 Flash consistently outperform other models against all benchmarks. However, they pose issues of accessibility and resource availability. Fine-tuning yielded competitive performance and offers a reliable alternative through domain-specific optimization. But its dependency on training data severely limits scalability. Zero-shot models consistently face difficulties with identifying signals of economic ideology, often resulting in negative associations with human coding. Using general knowledge for the domain-specific task of ideology scaling proved to be unreliable. Other key findings include considerable within-party variation, fine-tuning benefiting from larger training data, and zero-shot's sensitivity to prompt content. The assessments include the strengths and limitations of each model and derive best-practices for automated analyses of political content.</li>
</ul>

<h3>Title: A Simple Aerial Detection Baseline of Multimodal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09720">https://arxiv.org/abs/2501.09720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09720">https://arxiv.org/pdf/2501.09720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09720]] A Simple Aerial Detection Baseline of Multimodal Language Models(https://arxiv.org/abs/2501.09720)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>The multimodal language models (MLMs) based on generative pre-trained Transformer are considered powerful candidates for unifying various domains and tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding performance in multiple tasks, such as visual question answering and visual grounding. In addition to visual grounding that detects specific objects corresponded to given instruction, aerial detection, which detects all objects of multiple categories, is also a valuable and challenging task for RS foundation models. However, aerial detection has not been explored by existing RS MLMs because the autoregressive prediction mechanism of MLMs differs significantly from the detection outputs. In this paper, we present a simple baseline for applying MLMs to aerial detection for the first time, named LMMRotate. Specifically, we first introduce a normalization method to transform detection outputs into textual outputs to be compatible with the MLM framework. Then, we propose a evaluation method, which ensures a fair comparison between MLMs and conventional object detection models. We construct the baseline by fine-tuning open-source general-purpose MLMs and achieve impressive detection performance comparable to conventional detector. We hope that this baseline will serve as a reference for future MLM development, enabling more comprehensive capabilities for understanding RS images. Code is available at this https URL.</li>
</ul>

<h3>Title: Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps</h3>
<ul>
<li><strong>Authors: </strong>Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09732">https://arxiv.org/abs/2501.09732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09732">https://arxiv.org/pdf/2501.09732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09732]] Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps(https://arxiv.org/abs/2501.09732)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.</li>
</ul>

<h3>Title: ComplexVAD: Detecting Interaction Anomalies in Video</h3>
<ul>
<li><strong>Authors: </strong>Furkan Mumcu, Michael J. Jones, Yasin Yilmaz, Anoop Cherian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09733">https://arxiv.org/abs/2501.09733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09733">https://arxiv.org/pdf/2501.09733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09733]] ComplexVAD: Detecting Interaction Anomalies in Video(https://arxiv.org/abs/2501.09733)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Existing video anomaly detection datasets are inadequate for representing complex anomalies that occur due to the interactions between objects. The absence of complex anomalies in previous video anomaly detection datasets affects research by shifting the focus onto simple anomalies. To address this problem, we introduce a new large-scale dataset: ComplexVAD. In addition, we propose a novel method to detect complex anomalies via modeling the interactions between objects using a scene graph with spatio-temporal attributes. With our proposed method and two other state-of-the-art video anomaly detection methods, we obtain baseline scores on ComplexVAD and demonstrate that our new method outperforms existing works.</li>
</ul>

<h3>Title: Learnings from Scaling Visual Tokenizers for Reconstruction and Generation</h3>
<ul>
<li><strong>Authors: </strong>Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09755">https://arxiv.org/abs/2501.09755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09755">https://arxiv.org/pdf/2501.09755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09755]] Learnings from Scaling Visual Tokenizers for Reconstruction and Generation(https://arxiv.org/abs/2501.09755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.</li>
</ul>

<h3>Title: SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces</h3>
<ul>
<li><strong>Authors: </strong>Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy, Jingyuan Liu, Julie Dorsey, Zhixin Shu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.09756">https://arxiv.org/abs/2501.09756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.09756">https://arxiv.org/pdf/2501.09756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.09756]] SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces(https://arxiv.org/abs/2501.09756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject's identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: \url{this https URL}</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
