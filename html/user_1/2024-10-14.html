<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-14</h1>
<h3>Title: Self-Attention Mechanism in Multimodal Context for Banking Transaction Flow</h3>
<ul>
<li><strong>Authors: </strong>Cyrile Delestre, Yoann Sola</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08243">https://arxiv.org/abs/2410.08243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08243">https://arxiv.org/pdf/2410.08243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08243]] Self-Attention Mechanism in Multimodal Context for Banking Transaction Flow(https://arxiv.org/abs/2410.08243)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Banking Transaction Flow (BTF) is a sequential data found in a number of banking activities such as marketing, credit risk or banking fraud. It is a multimodal data composed of three modalities: a date, a numerical value and a wording. We propose in this work an application of self-attention mechanism to the processing of BTFs. We trained two general models on a large amount of BTFs in a self-supervised way: one RNN-based model and one Transformer-based model. We proposed a specific tokenization in order to be able to process BTFs. The performance of these two models was evaluated on two banking downstream tasks: a transaction categorization task and a credit risk task. The results show that fine-tuning these two pre-trained models allowed to perform better than the state-of-the-art approaches for both tasks.</li>
</ul>

<h3>Title: Generalization from Starvation: Hints of Universality in LLM Knowledge Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>David D. Baek, Yuxiao Li, Max Tegmark</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08255">https://arxiv.org/abs/2410.08255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08255">https://arxiv.org/pdf/2410.08255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08255]] Generalization from Starvation: Hints of Universality in LLM Knowledge Graph Learning(https://arxiv.org/abs/2410.08255)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Motivated by interpretability and reliability, we investigate how neural networks represent knowledge during graph learning, We find hints of universality, where equivalent representations are learned across a range of model sizes (from $10^2$ to $10^9$ parameters) and contexts (MLP toy models, LLM in-context learning and LLM training). We show that these attractor representations optimize generalization to unseen examples by exploiting properties of knowledge graph relations (e.g. symmetry and meta-transitivity). We find experimental support for such universality by showing that LLMs and simpler neural networks can be stitched, i.e., by stitching the first part of one model to the last part of another, mediated only by an affine or almost affine transformation. We hypothesize that this dynamic toward simplicity and generalization is driven by "intelligence from starvation": where overfitting is minimized by pressure to minimize the use of resources that are either scarce or competed for against other tasks.</li>
</ul>

<h3>Title: In Search of Forgotten Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Prasanna Mayilvahanan, Roland S. Zimmermann, Thadd√§us Wiedemer, Evgenia Rusak, Attila Juhos, Matthias Bethge, Wieland Brendel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08258">https://arxiv.org/abs/2410.08258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08258">https://arxiv.org/pdf/2410.08258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08258]] In Search of Forgotten Domain Generalization(https://arxiv.org/abs/2410.08258)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION -- LAION-Natural and LAION-Rendition -- that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale -- a crucial prerequisite for improving model robustness.</li>
</ul>

<h3>Title: Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08261">https://arxiv.org/abs/2410.08261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08261">https://arxiv.org/pdf/2410.08261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08261]] Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis(https://arxiv.org/abs/2410.08261)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing $1024 \times 1024$ resolution images.</li>
</ul>

<h3>Title: Towards Foundation Models for Mixed Integer Linear Programming</h3>
<ul>
<li><strong>Authors: </strong>Sirui Li, Janardhan Kulkarni, Ishai Menache, Cathy Wu, Beibin Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08288">https://arxiv.org/abs/2410.08288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08288">https://arxiv.org/pdf/2410.08288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08288]] Towards Foundation Models for Mixed Integer Linear Programming(https://arxiv.org/abs/2410.08288)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Mixed Integer Linear Programming (MILP) is essential for modeling complex decision-making problems but faces challenges in computational tractability and requires expert formulation. Current deep learning approaches for MILP focus on specific problem classes and do not generalize to unseen classes. To address this shortcoming, we take a foundation model training approach, where we train a single deep learning model on a diverse set of MILP problems to generalize across problem classes. As existing datasets for MILP lack diversity and volume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that is capable of generating a large set of diverse MILP classes with an unlimited amount of instances. We study our methodology on three key learning tasks that capture diverse aspects of MILP: (1) integrality gap prediction, (2) learning to branch, and (3) a new task of aligning MILP instances with natural language descriptions. Our empirical results show that models trained on the data generated by MILP-Evolve achieve significant improvements on unseen problems, including MIPLIB benchmarks. Our work highlights the potential of moving towards a foundation model approach for MILP that can generalize to a broad range of MILP applications. We are committed to fully open-sourcing our work to advance further research.</li>
</ul>

<h3>Title: Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?</h3>
<ul>
<li><strong>Authors: </strong>Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, Sanjiv Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08292">https://arxiv.org/abs/2410.08292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08292">https://arxiv.org/pdf/2410.08292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08292]] Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?(https://arxiv.org/abs/2410.08292)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The remarkable capability of Transformers to do reasoning and few-shot learning, without any fine-tuning, is widely conjectured to stem from their ability to implicitly simulate a multi-step algorithms -- such as gradient descent -- with their weights in a single forward pass. Recently, there has been progress in understanding this complex phenomenon from an expressivity point of view, by demonstrating that Transformers can express such multi-step algorithms. However, our knowledge about the more fundamental aspect of its learnability, beyond single layer models, is very limited. In particular, can training Transformers enable convergence to algorithmic solutions? In this work we resolve this for in-context linear regression with linear looped Transformers -- a multi-layer model with weight sharing that is conjectured to have an inductive bias to learn fix-point iterative algorithms. More specifically, for this setting we show that the global minimizer of the population training loss implements multi-step preconditioned gradient descent, with a preconditioner that adapts to the data distribution. Furthermore, we show a fast convergence for gradient flow on the regression loss, despite the non-convexity of the landscape, by proving a novel gradient dominance condition. To our knowledge, this is the first theoretical analysis for multi-layer Transformer in this setting. We further validate our theoretical findings through synthetic experiments.</li>
</ul>

<h3>Title: Dynamics of Concept Learning and Compositional Generalization</h3>
<ul>
<li><strong>Authors: </strong>Yongyi Yang, Core Francisco Park, Ekdeep Singh Lubana, Maya Okawa, Wei Hu, Hidenori Tanaka</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08309">https://arxiv.org/abs/2410.08309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08309">https://arxiv.org/pdf/2410.08309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08309]] Dynamics of Concept Learning and Compositional Generalization(https://arxiv.org/abs/2410.08309)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Prior work has shown that text-conditioned diffusion models can learn to identify and manipulate primitive concepts underlying a compositional data-generating process, enabling generalization to entirely novel, out-of-distribution compositions. Beyond performance evaluations, these studies develop a rich empirical phenomenology of learning dynamics, showing that models generalize sequentially, respecting the compositional hierarchy of the data-generating process. Moreover, concept-centric structures within the data significantly influence a model's speed of learning the ability to manipulate a concept. In this paper, we aim to better characterize these empirical results from a theoretical standpoint. Specifically, we propose an abstraction of prior work's compositional generalization problem by introducing a structured identity mapping (SIM) task, where a model is trained to learn the identity mapping on a Gaussian mixture with structurally organized centroids. We mathematically analyze the learning dynamics of neural networks trained on this SIM task and show that, despite its simplicity, SIM's learning dynamics capture and help explain key empirical observations on compositional generalization with diffusion models identified in prior work. Our theory also offers several new insights -- e.g., we find a novel mechanism for non-monotonic learning dynamics of test loss in early phases of training. We validate our new predictions by training a text-conditioned diffusion model, bridging our simplified framework and complex generative models. Overall, this work establishes the SIM task as a meaningful theoretical abstraction of concept learning dynamics in modern generative models.</li>
</ul>

<h3>Title: Level of agreement between emotions generated by Artificial Intelligence and human evaluation: a methodological proposal</h3>
<ul>
<li><strong>Authors: </strong>Miguel Carrasco, Cesar Gonzalez-Martin, Sonia Navajas-Torrente, Raul Dastres</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08332">https://arxiv.org/abs/2410.08332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08332">https://arxiv.org/pdf/2410.08332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08332]] Level of agreement between emotions generated by Artificial Intelligence and human evaluation: a methodological proposal(https://arxiv.org/abs/2410.08332)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Images are capable of conveying emotions, but emotional experience is highly subjective. Advances in artificial intelligence have enabled the generation of images based on emotional descriptions. However, the level of agreement between the generative images and human emotional responses has not yet been evaluated. To address this, 20 artistic landscapes were generated using StyleGAN2-ADA. Four variants evoking positive emotions (contentment, amusement) and negative emotions (fear, sadness) were created for each image, resulting in 80 pictures. An online questionnaire was designed using this material, in which 61 observers classified the generated images. Statistical analyses were performed on the collected data to determine the level of agreement among participants, between the observer's responses, and the AI-generated emotions. A generally good level of agreement was found, with better results for negative emotions. However, the study confirms the subjectivity inherent in emotional evaluation.</li>
</ul>

<h3>Title: Metalic: Meta-Learning In-Context with Protein Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jacob Beck, Shikha Surana, Manus McAuliffe, Oliver Bent, Thomas D. Barrett, Juan Jose Garau Luis, Paul Duckworth</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08355">https://arxiv.org/abs/2410.08355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08355">https://arxiv.org/pdf/2410.08355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08355]] Metalic: Meta-Learning In-Context with Protein Language Models(https://arxiv.org/abs/2410.08355)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Predicting the biophysical and functional properties of proteins is essential for in silico protein design. Machine learning has emerged as a promising technique for such prediction tasks. However, the relative scarcity of in vitro annotations means that these models often have little, or no, specific data on the desired fitness prediction task. As a result of limited data, protein language models (PLMs) are typically trained on general protein sequence modeling tasks, and then fine-tuned, or applied zero-shot, to protein fitness prediction. When no task data is available, the models make strong assumptions about the correlation between the protein sequence likelihood and fitness scores. In contrast, we propose meta-learning over a distribution of standard fitness prediction tasks, and demonstrate positive transfer to unseen fitness prediction tasks. Our method, called Metalic (Meta-Learning In-Context), uses in-context learning and fine-tuning, when data is available, to adapt to new tasks. Crucially, fine-tuning enables considerable generalization, even though it is not accounted for during meta-training. Our fine-tuned models achieve strong results with 18 times fewer parameters than state-of-the-art models. Moreover, our method sets a new state-of-the-art in low-data settings on ProteinGym, an established fitness-prediction benchmark. Due to data scarcity, we believe meta-learning will play a pivotal role in advancing protein engineering.</li>
</ul>

<h3>Title: GUS-Net: Social Bias Classification in Text with Generalizations, Unfairness, and Stereotypes</h3>
<ul>
<li><strong>Authors: </strong>Maximus Powers, Hua Wei, Umang Mavani, Harshitha Reddy Jonala, Ansh Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08388">https://arxiv.org/abs/2410.08388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08388">https://arxiv.org/pdf/2410.08388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08388]] GUS-Net: Social Bias Classification in Text with Generalizations, Unfairness, and Stereotypes(https://arxiv.org/abs/2410.08388)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The detection of bias in natural language processing (NLP) is a critical challenge, particularly with the increasing use of large language models (LLMs) in various domains. This paper introduces GUS-Net, an innovative approach to bias detection that focuses on three key types of biases: (G)eneralizations, (U)nfairness, and (S)tereotypes. GUS-Net leverages generative AI and automated agents to create a comprehensive synthetic dataset, enabling robust multi-label token classification. Our methodology enhances traditional bias detection methods by incorporating the contextual encodings of pre-trained models, resulting in improved accuracy and depth in identifying biased entities. Through extensive experiments, we demonstrate that GUS-Net outperforms state-of-the-art techniques, achieving superior performance in terms of accuracy, F1-score, and Hamming Loss. The findings highlight GUS-Net's effectiveness in capturing a wide range of biases across diverse contexts, making it a valuable tool for social bias detection in text. This study contributes to the ongoing efforts in NLP to address implicit bias, providing a pathway for future research and applications in various fields. The Jupyter notebooks used to create the dataset and model are available at: this https URL. Warning: This paper contains examples of harmful language, and reader discretion is recommended.</li>
</ul>

<h3>Title: Heating Up Quasi-Monte Carlo Graph Random Features: A Diffusion Kernel Perspective</h3>
<ul>
<li><strong>Authors: </strong>Brooke Feinberg, Aiwen Li</a></li>
<li><strong>Subjects: </strong>cs.LG, math.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08389">https://arxiv.org/abs/2410.08389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08389">https://arxiv.org/pdf/2410.08389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08389]] Heating Up Quasi-Monte Carlo Graph Random Features: A Diffusion Kernel Perspective(https://arxiv.org/abs/2410.08389)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We build upon a recently introduced class of quasi-graph random features (q-GRFs), which have demonstrated the ability to yield lower variance estimators of the 2-regularized Laplacian kernel (Choromanski 2023). Our research investigates whether similar results can be achieved with alternative kernel functions, specifically the Diffusion (or Heat), Mat√©rn, and Inverse Cosine kernels. We find that the Diffusion kernel performs most similarly to the 2-regularized Laplacian, and we further explore graph types that benefit from the previously established antithetic termination procedure. Specifically, we explore Erd≈ës-R√©nyi and Barab√°si-Albert random graph models, Binary Trees, and Ladder graphs, with the goal of identifying combinations of specific kernel and graph type that benefit from antithetic termination. We assert that q-GRFs achieve lower variance estimators of the Diffusion (or Heat) kernel on Ladder graphs. However, the number of rungs on the Ladder graphs impacts the algorithm's performance; further theoretical results supporting our experimentation are forthcoming. This work builds upon some of the earliest Quasi-Monte Carlo methods for kernels defined on combinatorial objects, paving the way for kernel-based learning algorithms and future real-world applications in various domains.</li>
</ul>

<h3>Title: KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data</h3>
<ul>
<li><strong>Authors: </strong>Andy Zhou, Xiaojun Xu, Ramesh Raghunathan, Alok Lal, Xinze Guan, Bin Yu, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08390">https://arxiv.org/abs/2410.08390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08390">https://arxiv.org/pdf/2410.08390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08390]] KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data(https://arxiv.org/abs/2410.08390)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Graph-based anomaly detection is pivotal in diverse security applications, such as fraud detection in transaction networks and intrusion detection for network traffic. Standard approaches, including Graph Neural Networks (GNNs), often struggle to generalize across shifting data distributions. Meanwhile, real-world domain knowledge is more stable and a common existing component of real-world detection strategies. To explicitly integrate such knowledge into data-driven models such as GCNs, we propose KnowGraph, which integrates domain knowledge with data-driven learning for enhanced graph-based anomaly detection. KnowGraph comprises two principal components: (1) a statistical learning component that utilizes a main model for the overarching detection task, augmented by multiple specialized knowledge models that predict domain-specific semantic entities; (2) a reasoning component that employs probabilistic graphical models to execute logical inferences based on model outputs, encoding domain knowledge through weighted first-order logic formulas. Extensive experiments on these large-scale real-world datasets show that KnowGraph consistently outperforms state-of-the-art baselines in both transductive and inductive settings, achieving substantial gains in average precision when generalizing to completely unseen test graphs. Further ablation studies demonstrate the effectiveness of the proposed reasoning component in improving detection performance, especially under extreme class imbalance. These results highlight the potential of integrating domain knowledge into data-driven models for high-stakes, graph-based security applications.</li>
</ul>

<h3>Title: The Effects of Hallucinations in Synthetic Training Data for Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Steven Rogulsky, Nicholas Popovic, Michael F√§rber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08393">https://arxiv.org/abs/2410.08393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08393">https://arxiv.org/pdf/2410.08393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08393]] The Effects of Hallucinations in Synthetic Training Data for Relation Extraction(https://arxiv.org/abs/2410.08393)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Relation extraction is crucial for constructing knowledge graphs, with large high-quality datasets serving as the foundation for training, fine-tuning, and evaluating models. Generative data augmentation (GDA) is a common approach to expand such datasets. However, this approach often introduces hallucinations, such as spurious facts, whose impact on relation extraction remains underexplored. In this paper, we examine the effects of hallucinations on the performance of relation extraction on the document and sentence levels. Our empirical study reveals that hallucinations considerably compromise the ability of models to extract relations from text, with recall reductions between 19.1% and 39.2%. We identify that relevant hallucinations impair the model's performance, while irrelevant hallucinations have a minimal impact. Additionally, we develop methods for the detection of hallucinations to improve data quality and model performance. Our approaches successfully classify texts as either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and 92.2%. These methods not only assist in removing hallucinations but also help in estimating their prevalence within datasets, which is crucial for selecting high-quality data. Overall, our work confirms the profound impact of relevant hallucinations on the effectiveness of relation extraction models.</li>
</ul>

<h3>Title: Generalizable autoregressive modeling of time series through functional narratives</h3>
<ul>
<li><strong>Authors: </strong>Ran Liu, Wenrui Ma, Ellen Zippi, Hadi Pouransari, Jingyun Xiao, Chris Sandino, Behrooz Mahasseni, Juri Minxha, Erdrin Azemi, Eva L. Dyer, Ali Moin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08421">https://arxiv.org/abs/2410.08421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08421">https://arxiv.org/pdf/2410.08421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08421]] Generalizable autoregressive modeling of time series through functional narratives(https://arxiv.org/abs/2410.08421)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series data are inherently functions of time, yet current transformers often learn time series by modeling them as mere concatenations of time periods, overlooking their functional properties. In this work, we propose a novel objective for transformers that learn time series by re-interpreting them as temporal functions. We build an alternative sequence of time series by constructing degradation operators of different intensity in the functional space, creating augmented variants of the original sample that are abstracted or simplified to different degrees. Based on the new set of generated sequence, we train an autoregressive transformer that progressively recovers the original sample from the most simplified variant. Analogous to the next word prediction task in languages that learns narratives by connecting different words, our autoregressive transformer aims to learn the Narratives of Time Series (NoTS) by connecting different functions in time. Theoretically, we justify the construction of the alternative sequence through its advantages in approximating functions. When learning time series data with transformers, constructing sequences of temporal functions allows for a broader class of approximable functions (e.g., differentiation) compared to sequences of time periods, leading to a 26\% performance improvement in synthetic feature regression experiments. Experimentally, we validate NoTS in 3 different tasks across 22 real-world datasets, where we show that NoTS significantly outperforms other pre-training methods by up to 6\%. Additionally, combining NoTS on top of existing transformer architectures can consistently boost the performance. Our results demonstrate the potential of NoTS as a general-purpose dynamic learner, offering a viable alternative for developing foundation models for time series analysis.</li>
</ul>

<h3>Title: Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zi'ou Zheng, Christopher Malon, Martin Renqiang Min, Xiaodan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08436">https://arxiv.org/abs/2410.08436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08436">https://arxiv.org/pdf/2410.08436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08436]] Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models(https://arxiv.org/abs/2410.08436)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>When performing complex multi-step reasoning tasks, the ability of Large Language Models (LLMs) to derive structured intermediate proof steps is important for ensuring that the models truly perform the desired reasoning and for improving models' explainability. This paper is centred around a focused study: whether the current state-of-the-art generalist LLMs can leverage the structures in a few examples to better construct the proof structures with \textit{in-context learning}. Our study specifically focuses on structure-aware demonstration and structure-aware pruning. We demonstrate that they both help improve performance. A detailed analysis is provided to help understand the results.</li>
</ul>

<h3>Title: AdvDiffuser: Generating Adversarial Safety-Critical Driving Scenarios via Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yuting Xie, Xianda Guo, Cong Wang, Kunhua Liu, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08453">https://arxiv.org/abs/2410.08453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08453">https://arxiv.org/pdf/2410.08453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08453]] AdvDiffuser: Generating Adversarial Safety-Critical Driving Scenarios via Guided Diffusion(https://arxiv.org/abs/2410.08453)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Safety-critical scenarios are infrequent in natural driving environments but hold significant importance for the training and testing of autonomous driving systems. The prevailing approach involves generating safety-critical scenarios automatically in simulation by introducing adversarial adjustments to natural environments. These adjustments are often tailored to specific tested systems, thereby disregarding their transferability across different systems. In this paper, we propose AdvDiffuser, an adversarial framework for generating safety-critical driving scenarios through guided diffusion. By incorporating a diffusion model to capture plausible collective behaviors of background vehicles and a lightweight guide model to effectively handle adversarial scenarios, AdvDiffuser facilitates transferability. Experimental results on the nuScenes dataset demonstrate that AdvDiffuser, trained on offline driving logs, can be applied to various tested systems with minimal warm-up episode data and outperform other existing methods in terms of realism, diversity, and adversarial performance.</li>
</ul>

<h3>Title: Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both</h3>
<ul>
<li><strong>Authors: </strong>Abhijnan Nath, Changsoo Jung, Ethan Seefried, Nikhil Krishnaswamy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08458">https://arxiv.org/abs/2410.08458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08458">https://arxiv.org/pdf/2410.08458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08458]] Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both(https://arxiv.org/abs/2410.08458)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reward modeling of human preferences is one of the cornerstones of building usable generative large language models (LLMs). While traditional RLHF-based alignment methods explicitly maximize the expected rewards from a separate reward model, more recent supervised alignment methods like Direct Preference Optimization (DPO) circumvent this phase to avoid problems including model drift and reward overfitting. Although popular due to its simplicity, DPO and similar direct alignment methods can still lead to degenerate policies, and rely heavily on the Bradley-Terry-based preference formulation to model reward differences between pairs of candidate outputs. This formulation is challenged by non-deterministic or noisy preference labels, for example human scoring of two candidate outputs is of low confidence. In this paper, we introduce DRDO (Direct Reward Distillation and policy-Optimization), a supervised knowledge distillation-based preference alignment method that simultaneously models rewards and preferences to avoid such degeneracy. DRDO directly mimics rewards assigned by an oracle while learning human preferences from a novel preference likelihood formulation. Our experimental results on the Ultrafeedback and TL;DR datasets demonstrate that policies trained using DRDO surpass previous methods such as DPO and e-DPO in terms of expected rewards and are more robust, on average, to noisy preference signals as well as out-of-distribution (OOD) settings.</li>
</ul>

<h3>Title: Distributionally robust self-supervised learning for tabular data</h3>
<ul>
<li><strong>Authors: </strong>Shantanu Ghosh, Tiankang Xie, Mikhail Kuznetsov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08511">https://arxiv.org/abs/2410.08511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08511">https://arxiv.org/pdf/2410.08511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08511]] Distributionally robust self-supervised learning for tabular data(https://arxiv.org/abs/2410.08511)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) models trained using Empirical Risk Minimization (ERM) often exhibit systematic errors on specific subpopulations of tabular data, known as error slices. Learning robust representation in presence of error slices is challenging, especially in self-supervised settings during the feature reconstruction phase, due to high cardinality features and the complexity of constructing error sets. Traditional robust representation learning methods are largely focused on improving worst group performance in supervised setting in computer vision, leaving a gap in approaches tailored for tabular data. We address this gap by developing a framework to learn robust representation in tabular data during self-supervised pre-training. Our approach utilizes an encoder-decoder model trained with Masked Language Modeling (MLM) loss to learn robust latent representations. This paper applies the Just Train Twice (JTT) and Deep Feature Reweighting (DFR) methods during the pre-training phase for tabular data. These methods fine-tune the ERM pre-trained model by up-weighting error-prone samples or creating balanced datasets for specific categorical features. This results in specialized models for each feature, which are then used in an ensemble approach to enhance downstream classification performance. This methodology improves robustness across slices, thus enhancing overall generalization performance. Extensive experiments across various datasets demonstrate the efficacy of our approach.</li>
</ul>

<h3>Title: VOVTrack: Exploring the Potentiality in Videos for Open-Vocabulary Object Tracking</h3>
<ul>
<li><strong>Authors: </strong>Zekun Qian, Ruize Han, Junhui Hou, Linqi Song, Wei Feng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08529">https://arxiv.org/abs/2410.08529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08529">https://arxiv.org/pdf/2410.08529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08529]] VOVTrack: Exploring the Potentiality in Videos for Open-Vocabulary Object Tracking(https://arxiv.org/abs/2410.08529)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Open-vocabulary multi-object tracking (OVMOT) represents a critical new challenge involving the detection and tracking of diverse object categories in videos, encompassing both seen categories (base classes) and unseen categories (novel classes). This issue amalgamates the complexities of open-vocabulary object detection (OVD) and multi-object tracking (MOT). Existing approaches to OVMOT often merge OVD and MOT methodologies as separate modules, predominantly focusing on the problem through an image-centric lens. In this paper, we propose VOVTrack, a novel method that integrates object states relevant to MOT and video-centric training to address this challenge from a video object tracking standpoint. First, we consider the tracking-related state of the objects during tracking and propose a new prompt-guided attention mechanism for more accurate localization and classification (detection) of the time-varying objects. Subsequently, we leverage raw video data without annotations for training by formulating a self-supervised object similarity learning technique to facilitate temporal object association (tracking). Experimental results underscore that VOVTrack outperforms existing methods, establishing itself as a state-of-the-art solution for open-vocabulary tracking task.</li>
</ul>

<h3>Title: Diffusion Models Need Visual Priors for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Yue, Zidong Wang, Zeyu Lu, Shuyang Sun, Meng Wei, Wanli Ouyang, Lei Bai, Luping Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08531">https://arxiv.org/abs/2410.08531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08531">https://arxiv.org/pdf/2410.08531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08531]] Diffusion Models Need Visual Priors for Image Generation(https://arxiv.org/abs/2410.08531)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Conventional class-guided diffusion models generally succeed in generating images with correct semantic content, but often struggle with texture details. This limitation stems from the usage of class priors, which only provide coarse and limited conditional information. To address this issue, we propose Diffusion on Diffusion (DoD), an innovative multi-stage generation framework that first extracts visual priors from previously generated samples, then provides rich guidance for the diffusion model leveraging visual priors from the early stages of diffusion sampling. Specifically, we introduce a latent embedding module that employs a compression-reconstruction approach to discard redundant detail information from the conditional samples in each stage, retaining only the semantic information for guidance. We evaluate DoD on the popular ImageNet-$256 \times 256$ dataset, reducing 7$\times$ training cost compared to SiT and DiT with even better performance in terms of the FID-50K score. Our largest model DoD-XL achieves an FID-50K score of 1.83 with only 1 million training steps, which surpasses other state-of-the-art methods without bells and whistles during inference.</li>
</ul>

<h3>Title: Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Abhijay Ghildyal, Yuanhan Chen, Saman Zadtootaghaj, Nabajeet Barman, Alan C. Bovik</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08534">https://arxiv.org/abs/2410.08534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08534">https://arxiv.org/pdf/2410.08534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08534]] Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities(https://arxiv.org/abs/2410.08534)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The advent of AI has influenced many aspects of human life, from self-driving cars and intelligent chatbots to text-based image and video generation models capable of creating realistic images and videos based on user prompts (text-to-image, image-to-image, and image-to-video). AI-based methods for image and video super resolution, video frame interpolation, denoising, and compression have already gathered significant attention and interest in the industry and some solutions are already being implemented in real-world products and services. However, to achieve widespread integration and acceptance, AI-generated and enhanced content must be visually accurate, adhere to intended use, and maintain high visual quality to avoid degrading the end user's quality of experience (QoE). One way to monitor and control the visual "quality" of AI-generated and -enhanced content is by deploying Image Quality Assessment (IQA) and Video Quality Assessment (VQA) models. However, most existing IQA and VQA models measure visual fidelity in terms of "reconstruction" quality against a pristine reference content and were not designed to assess the quality of "generative" artifacts. To address this, newer metrics and models have recently been proposed, but their performance evaluation and overall efficacy have been limited by datasets that were too small or otherwise lack representative content and/or distortion capacity; and by performance measures that can accurately report the success of an IQA/VQA model for "GenAI". This paper examines the current shortcomings and possibilities presented by AI-generated and enhanced image and video content, with a particular focus on end-user perceived quality. Finally, we discuss open questions and make recommendations for future work on the "GenAI" quality assessment problems, towards further progressing on this interesting and relevant field of research.</li>
</ul>

<h3>Title: Score Neural Operator: A Generative Model for Learning and Generalizing Across Multiple Probability Distributions</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Liao, Aoyang Qin, Jacob Seidman, Junqi Wang, Wei Wang, Paris Perdikaris</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08549">https://arxiv.org/abs/2410.08549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08549">https://arxiv.org/pdf/2410.08549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08549]] Score Neural Operator: A Generative Model for Learning and Generalizing Across Multiple Probability Distributions(https://arxiv.org/abs/2410.08549)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Most existing generative models are limited to learning a single probability distribution from the training data and cannot generalize to novel distributions for unseen data. An architecture that can generate samples from both trained datasets and unseen probability distributions would mark a significant breakthrough. Recently, score-based generative models have gained considerable attention for their comprehensive mode coverage and high-quality image synthesis, as they effectively learn an operator that maps a probability distribution to its corresponding score function. In this work, we introduce the $\emph{Score Neural Operator}$, which learns the mapping from multiple probability distributions to their score functions within a unified framework. We employ latent space techniques to facilitate the training of score matching, which tends to over-fit in the original image pixel space, thereby enhancing sample generation quality. Our trained Score Neural Operator demonstrates the ability to predict score functions of probability measures beyond the training space and exhibits strong generalization performance in both 2-dimensional Gaussian Mixture Models and 1024-dimensional MNIST double-digit datasets. Importantly, our approach offers significant potential for few-shot learning applications, where a single image from a new distribution can be leveraged to generate multiple distinct images from that distribution.</li>
</ul>

<h3>Title: Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pascl Zwick, Kevin Roesch, Marvin Klemp, Oliver Bringmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08551">https://arxiv.org/abs/2410.08551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08551">https://arxiv.org/pdf/2410.08551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08551]] Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models(https://arxiv.org/abs/2410.08551)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Anonymization plays a key role in protecting sensible information of individuals in real world datasets. Self-driving cars for example need high resolution facial features to track people and their viewing direction to predict future behaviour and react accordingly. In order to protect people's privacy whilst keeping important features in the dataset, it is important to replace the full body of a person with a highly detailed anonymized one. In contrast to doing face anonymization, full body replacement decreases the ability of recognizing people by their hairstyle or clothes. In this paper, we propose a workflow for full body person anonymization utilizing Stable Diffusion as a generative backend. Text-to-image diffusion models, like Stable Diffusion, OpenAI's DALL-E or Midjourney, have become very popular in recent time, being able to create photorealistic images from a single text prompt. We show that our method outperforms state-of-the art anonymization pipelines with respect to image quality, resolution, Inception Score (IS) and Frechet Inception Distance (FID). Additionally, our method is invariant with respect to the image generator and thus able to be used with the latest models available.</li>
</ul>

<h3>Title: Learning General Representation of 12-Lead Electrocardiogram with a Joint-Embedding Predictive architecture</h3>
<ul>
<li><strong>Authors: </strong>Sehun Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08559">https://arxiv.org/abs/2410.08559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08559">https://arxiv.org/pdf/2410.08559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08559]] Learning General Representation of 12-Lead Electrocardiogram with a Joint-Embedding Predictive architecture(https://arxiv.org/abs/2410.08559)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We propose a self-supervised learning method for 12-lead Electrocardiogram (ECG) analysis, named ECG Joint Embedding Predictive Architecture (ECG-JEPA). ECG-JEPA employs a masking strategy to learn semantic representations of ECG data. Unlike existing methods, ECG-JEPA predicts at the hidden representation level rather than reconstructing raw data. This approach offers several advantages in the ECG domain: (1) it avoids producing unnecessary details, such as noise, which is common in standard ECG; and (2) it addresses the limitations of na√Øve L2 loss between raw signals. Another key contribution is the introduction of a special masked attention tailored for 12-lead ECG data, Cross-Pattern Attention (CroPA). CroPA enables the model to effectively capture inter-patch relationships. Additionally, ECG-JEPA is highly scalable, allowing efficient training on large datasets. Our code is openly available this https URL.</li>
</ul>

<h3>Title: Diffusion-Based Depth Inpainting for Transparent and Reflective Objects</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Sun, Dingchang Hu, Yixiang Dai, Guijin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08567">https://arxiv.org/abs/2410.08567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08567">https://arxiv.org/pdf/2410.08567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08567]] Diffusion-Based Depth Inpainting for Transparent and Reflective Objects(https://arxiv.org/abs/2410.08567)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Transparent and reflective objects, which are common in our everyday lives, present a significant challenge to 3D imaging techniques due to their unique visual and optical properties. Faced with these types of objects, RGB-D cameras fail to capture the real depth value with their accurate spatial information. To address this issue, we propose DITR, a diffusion-based Depth Inpainting framework specifically designed for Transparent and Reflective objects. This network consists of two stages, including a Region Proposal stage and a Depth Inpainting stage. DITR dynamically analyzes the optical and geometric depth loss and inpaints them automatically. Furthermore, comprehensive experimental results demonstrate that DITR is highly effective in depth inpainting tasks of transparent and reflective objects with robust adaptability.</li>
</ul>

<h3>Title: VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Houlun Chen, Xin Wang, Hong Chen, Zeyang Zhang, Wei Feng, Bin Huang, Jia Jia, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08593">https://arxiv.org/abs/2410.08593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08593">https://arxiv.org/pdf/2410.08593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08593]] VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding(https://arxiv.org/abs/2410.08593)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding, which hinders precise video moment localization when given fine-grained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic \underline{V}id\underline{E}o-text annotation pipeline to generate captions with \underline{R}el\underline{I}able \underline{FI}n\underline{E}-grained statics and \underline{D}ynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR. Code and Datasets are in \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: StraGo: Harnessing Strategic Guidance for Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming Ding, Linjun Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08601">https://arxiv.org/abs/2410.08601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08601">https://arxiv.org/pdf/2410.08601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08601]] StraGo: Harnessing Strategic Guidance for Prompt Optimization(https://arxiv.org/abs/2410.08601)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Prompt engineering is pivotal for harnessing the capabilities of large language models (LLMs) across diverse applications. While existing prompt optimization methods improve prompt effectiveness, they often lead to prompt drifting, where newly generated prompts can adversely impact previously successful cases while addressing failures. Furthermore, these methods tend to rely heavily on LLMs' intrinsic capabilities for prompt optimization tasks. In this paper, we introduce StraGo (Strategic-Guided Optimization), a novel approach designed to mitigate prompt drifting by leveraging insights from both successful and failed cases to identify critical factors for achieving optimization objectives. StraGo employs a how-to-do methodology, integrating in-context learning to formulate specific, actionable strategies that provide detailed, step-by-step guidance for prompt optimization. Extensive experiments conducted across a range of tasks, including reasoning, natural language understanding, domain-specific knowledge, and industrial applications, demonstrate StraGo's superior performance. It establishes a new state-of-the-art in prompt optimization, showcasing its ability to deliver stable and effective prompt improvements.</li>
</ul>

<h3>Title: Text-To-Image with Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Mehrshad Momen-Tayefeh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08608">https://arxiv.org/abs/2410.08608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08608">https://arxiv.org/pdf/2410.08608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08608]] Text-To-Image with Generative Adversarial Networks(https://arxiv.org/abs/2410.08608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating realistic images from human texts is one of the most challenging problems in the field of computer vision (CV). The meaning of descriptions given can be roughly reflected by existing text-to-image approaches. In this paper, our main purpose is to propose a brief comparison between five different methods base on the Generative Adversarial Networks (GAN) to make image from the text. In addition, each model architectures synthesis images with different resolution. Furthermore, the best and worst obtained resolutions is 64*64, 256*256 respectively. However, we checked and compared some metrics that introduce the accuracy of each model. Also, by doing this study, we found out the best model for this problem by comparing these different approaches essential metrics.</li>
</ul>

<h3>Title: Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting</h3>
<ul>
<li><strong>Authors: </strong>Purushothaman Natarajan, Kamal Basha, Athira Nambiar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08612">https://arxiv.org/abs/2410.08612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08612">https://arxiv.org/pdf/2410.08612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08612]] Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting(https://arxiv.org/abs/2410.08612)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.</li>
</ul>

<h3>Title: Towards Cross-domain Few-shot Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiazhen Chen, Sichao Fu, Zhibin Zhang, Zheng Ma, Mingbin Feng, Tony S. Wirjanto, Qinmu Peng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08629">https://arxiv.org/abs/2410.08629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08629">https://arxiv.org/pdf/2410.08629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08629]] Towards Cross-domain Few-shot Graph Anomaly Detection(https://arxiv.org/abs/2410.08629)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Few-shot graph anomaly detection (GAD) has recently garnered increasing attention, which aims to discern anomalous patterns among abundant unlabeled test nodes under the guidance of a limited number of labeled training nodes. Existing few-shot GAD approaches typically adopt meta-training methods trained on richly labeled auxiliary networks to facilitate rapid adaptation to target networks that possess sparse labels. However, these proposed methods often assume that the auxiliary and target networks exist in the same data distributions-an assumption rarely holds in practical settings. This paper explores a more prevalent and complex scenario of cross-domain few-shot GAD, where the goal is to identify anomalies within sparsely labeled target graphs using auxiliary graphs from a related, yet distinct domain. The challenge here is nontrivial owing to inherent data distribution discrepancies between the source and target domains, compounded by the uncertainties of sparse labeling in the target domain. In this paper, we propose a simple and effective framework, termed CDFS-GAD, specifically designed to tackle the aforementioned challenges. CDFS-GAD first introduces a domain-adaptive graph contrastive learning module, which is aimed at enhancing cross-domain feature alignment. Then, a prompt tuning module is further designed to extract domain-specific features tailored to each domain. Moreover, a domain-adaptive hypersphere classification loss is proposed to enhance the discrimination between normal and anomalous instances under minimal supervision, utilizing domain-sensitive norms. Lastly, a self-training strategy is introduced to further refine the predicted scores, enhancing its reliability in few-shot settings. Extensive experiments on twelve real-world cross-domain data pairs demonstrate the effectiveness of the proposed CDFS-GAD framework in comparison to various existing GAD methods.</li>
</ul>

<h3>Title: GAI-Enabled Explainable Personalized Federated Semi-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Yubo Peng, Feibo Jiang, Li Dong, Kezhi Wang, Kun Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08634">https://arxiv.org/abs/2410.08634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08634">https://arxiv.org/pdf/2410.08634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08634]] GAI-Enabled Explainable Personalized Federated Semi-Supervised Learning(https://arxiv.org/abs/2410.08634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a commonly distributed algorithm for mobile users (MUs) training artificial intelligence (AI) models, however, several challenges arise when applying FL to real-world scenarios, such as label scarcity, non-IID data, and unexplainability. As a result, we propose an explainable personalized FL framework, called XPFL. First, we introduce a generative AI (GAI) assisted personalized federated semi-supervised learning, called GFed. Particularly, in local training, we utilize a GAI model to learn from large unlabeled data and apply knowledge distillation-based semi-supervised learning to train the local FL model using the knowledge acquired from the GAI model. In global aggregation, we obtain the new local FL model by fusing the local and global FL models in specific proportions, allowing each local model to incorporate knowledge from others while preserving its personalized characteristics. Second, we propose an explainable AI mechanism for FL, named XFed. Specifically, in local training, we apply a decision tree to match the input and output of the local FL model. In global aggregation, we utilize t-distributed stochastic neighbor embedding (t-SNE) to visualize the local models before and after aggregation. Finally, simulation results validate the effectiveness of the proposed XPFL framework.</li>
</ul>

<h3>Title: E-Motion: Future Motion Simulation via Event Sequence Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Song Wu, Zhiyu Zhu, Junhui Hou, Guangming Shi, Jinjian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08649">https://arxiv.org/abs/2410.08649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08649">https://arxiv.org/pdf/2410.08649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08649]] E-Motion: Future Motion Simulation via Event Sequence Diffusion(https://arxiv.org/abs/2410.08649)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems.</li>
</ul>

<h3>Title: SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08669">https://arxiv.org/abs/2410.08669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08669">https://arxiv.org/pdf/2410.08669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08669]] SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction(https://arxiv.org/abs/2410.08669)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. However, the scarcity of large-scale driving datasets has hindered the development of robust and generalizable motion prediction models, limiting their ability to capture complex interactions and road geometries. Inspired by recent advances in natural language processing (NLP) and computer vision (CV), self-supervised learning (SSL) has gained significant attention in the motion prediction community for learning rich and transferable scene representations. Nonetheless, existing pre-training methods for motion prediction have largely focused on specific model architectures and single dataset, limiting their scalability and generalizability. To address these challenges, we propose SmartPretrain, a general and scalable SSL framework for motion prediction that is both model-agnostic and dataset-agnostic. Our approach integrates contrastive and reconstructive SSL, leveraging the strengths of both generative and discriminative paradigms to effectively represent spatiotemporal evolution and interactions without imposing architectural constraints. Additionally, SmartPretrain employs a dataset-agnostic scenario sampling strategy that integrates multiple datasets, enhancing data volume, diversity, and robustness. Extensive experiments on multiple datasets demonstrate that SmartPretrain consistently improves the performance of state-of-the-art prediction models across datasets, data splits and main metrics. For instance, SmartPretrain significantly reduces the MissRate of Forecast-MAE by 10.6%. These results highlight SmartPretrain's effectiveness as a unified, scalable solution for motion prediction, breaking free from the limitations of the small-data regime. Codes are available at this https URL</li>
</ul>

<h3>Title: Gait Sequence Upsampling using Diffusion Models for single LiDAR sensors</h3>
<ul>
<li><strong>Authors: </strong>Jeongho Ahn, Kazuto Nakashima, Koki Yoshino, Yumi Iwashita, Ryo Kurazume</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08680">https://arxiv.org/abs/2410.08680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08680">https://arxiv.org/pdf/2410.08680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08680]] Gait Sequence Upsampling using Diffusion Models for single LiDAR sensors(https://arxiv.org/abs/2410.08680)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recently, 3D LiDAR has emerged as a promising technique in the field of gait-based person identification, serving as an alternative to traditional RGB cameras, due to its robustness under varying lighting conditions and its ability to capture 3D geometric information. However, long capture distances or the use of low-cost LiDAR sensors often result in sparse human point clouds, leading to a decline in identification performance. To address these challenges, we propose a sparse-to-dense upsampling model for pedestrian point clouds in LiDAR-based gait recognition, named LidarGSU, which is designed to improve the generalization capability of existing identification models. Our method utilizes diffusion probabilistic models (DPMs), which have shown high fidelity in generative tasks such as image completion. In this work, we leverage DPMs on sparse sequential pedestrian point clouds as conditional masks in a video-to-video translation approach, applied in an inpainting manner. We conducted extensive experiments on the SUSTeck1K dataset to evaluate the generative quality and recognition performance of the proposed method. Furthermore, we demonstrate the applicability of our upsampling model using a real-world dataset, captured with a low-resolution sensor across varying measurement distances.</li>
</ul>

<h3>Title: Distillation of Discrete Diffusion through Dimensional Correlations</h3>
<ul>
<li><strong>Authors: </strong>Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08709">https://arxiv.org/abs/2410.08709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08709">https://arxiv.org/pdf/2410.08709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08709]] Distillation of Discrete Diffusion through Dimensional Correlations(https://arxiv.org/abs/2410.08709)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated exceptional performances in various fields of generative modeling. While they often outperform competitors including VAEs and GANs in sample quality and diversity, they suffer from slow sampling speed due to their iterative nature. Recently, distillation techniques and consistency models are mitigating this issue in continuous domains, but discrete diffusion models have some specific challenges towards faster generation. Most notably, in the current literature, correlations between different dimensions (pixels, locations) are ignored, both by its modeling and loss functions, due to computational limitations. In this paper, we propose "mixture" models in discrete diffusion that are capable of treating dimensional correlations while remaining scalable, and we provide a set of loss functions for distilling the iterations of existing models. Two primary theoretical insights underpin our approach: first, that dimensionally independent models can well approximate the data distribution if they are allowed to conduct many sampling steps, and second, that our loss functions enables mixture models to distill such many-step conventional models into just a few steps by learning the dimensional correlations. We empirically demonstrate that our proposed method for discrete diffusions work in practice, by distilling a continuous-time discrete diffusion model pretrained on the CIFAR-10 dataset.</li>
</ul>

<h3>Title: Measuring the Groundedness of Legal Question-Answering Systems</h3>
<ul>
<li><strong>Authors: </strong>Dietrich Trautmann, Natalia Ostapuk, Quentin Grail, Adrian Alan Pol, Guglielmo Bonifazi, Shang Gao, Martin Gajek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08764">https://arxiv.org/abs/2410.08764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08764">https://arxiv.org/pdf/2410.08764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08764]] Measuring the Groundedness of Legal Question-Answering Systems(https://arxiv.org/abs/2410.08764)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In high-stakes domains like legal question-answering, the accuracy and trustworthiness of generative AI systems are of paramount importance. This work presents a comprehensive benchmark of various methods to assess the groundedness of AI-generated responses, aiming to significantly enhance their reliability. Our experiments include similarity-based metrics and natural language inference models to evaluate whether responses are well-founded in the given contexts. We also explore different prompting strategies for large language models to improve the detection of ungrounded responses. We validated the effectiveness of these methods using a newly created grounding classification corpus, designed specifically for legal queries and corresponding responses from retrieval-augmented prompting, focusing on their alignment with source material. Our results indicate potential in groundedness classification of generated responses, with the best method achieving a macro-F1 score of 0.8. Additionally, we evaluated the methods in terms of their latency to determine their suitability for real-world applications, as this step typically follows the generation process. This capability is essential for processes that may trigger additional manual verification or automated response regeneration. In summary, this study demonstrates the potential of various detection methods to improve the trustworthiness of generative AI in legal settings.</li>
</ul>

<h3>Title: One-shot Generative Domain Adaptation in 3D GANs</h3>
<ul>
<li><strong>Authors: </strong>Ziqiang Li, Yi Wu, Chaoyue Wang, Xue Rui, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08824">https://arxiv.org/abs/2410.08824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08824">https://arxiv.org/pdf/2410.08824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08824]] One-shot Generative Domain Adaptation in 3D GANs(https://arxiv.org/abs/2410.08824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D-aware image generation necessitates extensive training data to ensure stable training and mitigate the risk of overfitting. This paper first considers a novel task known as One-shot 3D Generative Domain Adaptation (GDA), aimed at transferring a pre-trained 3D generator from one domain to a new one, relying solely on a single reference image. One-shot 3D GDA is characterized by the pursuit of specific attributes, namely, high fidelity, large diversity, cross-domain consistency, and multi-view consistency. Within this paper, we introduce 3D-Adapter, the first one-shot 3D GDA method, for diverse and faithful generation. Our approach begins by judiciously selecting a restricted weight set for fine-tuning, and subsequently leverages four advanced loss functions to facilitate adaptation. An efficient progressive fine-tuning strategy is also implemented to enhance the adaptation process. The synergy of these three technological components empowers 3D-Adapter to achieve remarkable performance, substantiated both quantitatively and qualitatively, across all desired properties of 3D GDA. Furthermore, 3D-Adapter seamlessly extends its capabilities to zero-shot scenarios, and preserves the potential for crucial tasks such as interpolation, reconstruction, and editing within the latent space of the pre-trained generator. Code will be available at this https URL.</li>
</ul>

<h3>Title: Audio Description Generation in the Era of LLMs and VLMs: A Review of Transferable Generative AI Technologies</h3>
<ul>
<li><strong>Authors: </strong>Yingqiang Gao, Lukas Fischer, Alexa Lintner, Sarah Ebling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08860">https://arxiv.org/abs/2410.08860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08860">https://arxiv.org/pdf/2410.08860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08860]] Audio Description Generation in the Era of LLMs and VLMs: A Review of Transferable Generative AI Technologies(https://arxiv.org/abs/2410.08860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Audio descriptions (ADs) function as acoustic commentaries designed to assist blind persons and persons with visual impairments in accessing digital media content on television and in movies, among other settings. As an accessibility service typically provided by trained AD professionals, the generation of ADs demands significant human effort, making the process both time-consuming and costly. Recent advancements in natural language processing (NLP) and computer vision (CV), particularly in large language models (LLMs) and vision-language models (VLMs), have allowed for getting a step closer to automatic AD generation. This paper reviews the technologies pertinent to AD generation in the era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV technologies can be applied to generate ADs and identify essential research directions for the future.</li>
</ul>

<h3>Title: Interdependency Matters: Graph Alignment for Multivariate Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuanyi Wang, Haifeng Sun, Chengsen Wang, Mengde Zhu, Jingyu Wang, Wei Tang, Qi Qi, Zirui Zhuang, Jianxin Liao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08877">https://arxiv.org/abs/2410.08877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08877">https://arxiv.org/pdf/2410.08877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08877]] Interdependency Matters: Graph Alignment for Multivariate Time Series Anomaly Detection(https://arxiv.org/abs/2410.08877)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection in multivariate time series (MTS) is crucial for various applications in data mining and industry. Current industrial methods typically approach anomaly detection as an unsupervised learning task, aiming to identify deviations by estimating the normal distribution in noisy, label-free datasets. These methods increasingly incorporate interdependencies between channels through graph structures to enhance accuracy. However, the role of interdependencies is more critical than previously understood, as shifts in interdependencies between MTS channels from normal to anomalous data are significant. This observation suggests that \textit{anomalies could be detected by changes in these interdependency graph series}. To capitalize on this insight, we introduce MADGA (MTS Anomaly Detection via Graph Alignment), which redefines anomaly detection as a graph alignment (GA) problem that explicitly utilizes interdependencies for anomaly detection. MADGA dynamically transforms subsequences into graphs to capture the evolving interdependencies, and Graph alignment is performed between these graphs, optimizing an alignment plan that minimizes cost, effectively minimizing the distance for normal data and maximizing it for anomalous data. Uniquely, our GA approach involves explicit alignment of both nodes and edges, employing Wasserstein distance for nodes and Gromov-Wasserstein distance for edges. To our knowledge, this is the first application of GA to MTS anomaly detection that explicitly leverages interdependency for this purpose. Extensive experiments on diverse real-world datasets validate the effectiveness of MADGA, demonstrating its capability to detect anomalies and differentiate interdependencies, consistently achieving state-of-the-art across various scenarios.</li>
</ul>

<h3>Title: Can GPTs Evaluate Graphic Design Based on Design Principles?</h3>
<ul>
<li><strong>Authors: </strong>Daichi Haraguchi, Naoto Inoue, Wataru Shimoda, Hayato Mitani, Seiichi Uchida, Kota Yamaguchi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08885">https://arxiv.org/abs/2410.08885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08885">https://arxiv.org/pdf/2410.08885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08885]] Can GPTs Evaluate Graphic Design Based on Design Principles?(https://arxiv.org/abs/2410.08885)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent advancements in foundation models show promising capability in graphic design generation. Several studies have started employing Large Multimodal Models (LMMs) to evaluate graphic designs, assuming that LMMs can properly assess their quality, but it is unclear if the evaluation is reliable. One way to evaluate the quality of graphic design is to assess whether the design adheres to fundamental graphic design principles, which are the designer's common practice. In this paper, we compare the behavior of GPT-based evaluation and heuristic evaluation based on design principles using human annotations collected from 60 subjects. Our experiments reveal that, while GPTs cannot distinguish small details, they have a reasonably good correlation with human annotation and exhibit a similar tendency to heuristic metrics based on design principles, suggesting that they are indeed capable of assessing the quality of graphic design. Our dataset is available at this https URL .</li>
</ul>

<h3>Title: Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Kun Ding, Qiang Yu, Haojian Zhang, Gaofeng Meng, Shiming Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08895">https://arxiv.org/abs/2410.08895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08895">https://arxiv.org/pdf/2410.08895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08895]] Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation(https://arxiv.org/abs/2410.08895)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Cache-based approaches stand out as both effective and efficient for adapting vision-language models (VLMs). Nonetheless, the existing cache model overlooks three crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text similarity, neglecting the importance of image-image similarity, leading to a gap between pre-training and adaptation. 2) The current cache model is based on the Nadaraya-Watson (N-W) estimator, which disregards the intricate relationships among training samples while constructing weight function. 3) Under the condition of limited samples, the logits generated by cache model are of high uncertainty, directly using these logits without accounting for the confidence could be problematic. This work presents three calibration modules aimed at addressing the above challenges. Similarity Calibration refines the image-image similarity by using unlabeled images. We add a learnable projection layer with residual connection on top of the pre-trained image encoder of CLIP and optimize the parameters by minimizing self-supervised contrastive loss. Weight Calibration introduces a precision matrix into the weight function to adequately model the relation between training samples, transforming the existing cache model to a Gaussian Process (GP) regressor, which could be more accurate than N-W estimator. Confidence Calibration leverages the predictive variances computed by GP Regression to dynamically re-scale the logits of cache model, ensuring that the cache model's outputs are appropriately adjusted based on their confidence levels. Besides, to reduce the high complexity of GPs, we further propose a group-based learning strategy. Integrating the above designs, we propose both training-free and training-required variants. Extensive experiments on 11 few-shot classification datasets validate that the proposed methods can achieve state-of-the-art performance.</li>
</ul>

<h3>Title: DiffPO: A causal diffusion model for learning distributions of potential outcomes</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Ma, Valentyn Melnychuk, Jonas Schweisthal, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08924">https://arxiv.org/abs/2410.08924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08924">https://arxiv.org/pdf/2410.08924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08924]] DiffPO: A causal diffusion model for learning distributions of potential outcomes(https://arxiv.org/abs/2410.08924)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Predicting potential outcomes of interventions from observational data is crucial for decision-making in medicine, but the task is challenging due to the fundamental problem of causal inference. Existing methods are largely limited to point estimates of potential outcomes with no uncertain quantification; thus, the full information about the distributions of potential outcomes is typically ignored. In this paper, we propose a novel causal diffusion model called DiffPO, which is carefully designed for reliable inferences in medicine by learning the distribution of potential outcomes. In our DiffPO, we leverage a tailored conditional denoising diffusion model to learn complex distributions, where we address the selection bias through a novel orthogonal diffusion loss. Another strength of our DiffPO method is that it is highly flexible (e.g., it can also be used to estimate different causal quantities such as CATE). Across a wide range of experiments, we show that our method achieves state-of-the-art performance.</li>
</ul>

<h3>Title: HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Xiling Li, Korbinian Franz Rudolf, Nils Blank, Rudolf Lioutikov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08925">https://arxiv.org/abs/2410.08925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08925">https://arxiv.org/pdf/2410.08925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08925]] HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable Deep Learning(https://arxiv.org/abs/2410.08925)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Prototype Learning methods provide an interpretable alternative to black-box deep learning models. Approaches such as ProtoPNet learn, which part of a test image "look like" known prototypical parts from training images, combining predictive power with the inherent interpretability of case-based reasoning. However, existing approaches have two main drawbacks: A) They rely solely on deterministic similarity scores without statistical confidence. B) The prototypes are learned in a black-box manner without human input. This work introduces HyperPg, a new prototype representation leveraging Gaussian distributions on a hypersphere in latent space, with learnable mean and variance. HyperPg prototypes adapt to the spread of clusters in the latent space and output likelihood scores. The new architecture, HyperPgNet, leverages HyperPg to learn prototypes aligned with human concepts from pixel-level annotations. Consequently, each prototype represents a specific concept such as color, image texture, or part of the image subject. A concept extraction pipeline built on foundation models provides pixel-level annotations, significantly reducing human labeling effort. Experiments on CUB-200-2011 and Stanford Cars datasets demonstrate that HyperPgNet outperforms other prototype learning architectures while using fewer parameters and training steps. Additionally, the concept-aligned HyperPg prototypes are learned transparently, enhancing model interpretability.</li>
</ul>

<h3>Title: Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images</h3>
<ul>
<li><strong>Authors: </strong>Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster, Marco Carminati, Enkelejda Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08926">https://arxiv.org/abs/2410.08926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08926">https://arxiv.org/pdf/2410.08926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08926]] Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images(https://arxiv.org/abs/2410.08926)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We explore the transformative potential of SAM 2, a vision foundation model, in advancing gaze estimation and eye tracking technologies. By significantly reducing annotation time, lowering technical barriers through its ease of deployment, and enhancing segmentation accuracy, SAM 2 addresses critical challenges faced by researchers and practitioners. Utilizing its zero-shot segmentation capabilities with minimal user input-a single click per video-we tested SAM 2 on over 14 million eye images from diverse datasets, including virtual reality setups and the world's largest unified dataset recorded using wearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches the performance of domain-specific models trained solely on eye images, achieving competitive mean Intersection over Union (mIoU) scores of up to 93% without fine-tuning. Additionally, we provide our code and segmentation masks for these widely used datasets to promote further research.</li>
</ul>

<h3>Title: Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory</h3>
<ul>
<li><strong>Authors: </strong>Aymane El Firdoussi, Mohamed El Amine Seddik, Soufiane Hayou, Reda Alami, Ahmed Alzubaidi, Hakim Hacid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08942">https://arxiv.org/abs/2410.08942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08942">https://arxiv.org/pdf/2410.08942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08942]] Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory(https://arxiv.org/abs/2410.08942)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic data has gained attention for training large language models, but poor-quality data can harm performance (see, e.g., Shumailov et al. (2023); Seddik et al. (2024)). A potential solution is data pruning, which retains only high-quality data based on a score function (human or machine feedback). Previous work Feng et al. (2024) analyzed models trained on synthetic data as sample size increases. We extend this by using random matrix theory to derive the performance of a binary classifier trained on a mix of real and pruned synthetic data in a high dimensional setting. Our findings identify conditions where synthetic data could improve performance, focusing on the quality of the generative model and verification strategy. We also show a smooth phase transition in synthetic label noise, contrasting with prior sharp behavior in infinite sample limits. Experiments with toy models and large language models validate our theoretical results.</li>
</ul>

<h3>Title: Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Zhang, Ahmed Elgohary, Ahmed Magooda, Daniel Khashabi, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08968">https://arxiv.org/abs/2410.08968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08968">https://arxiv.org/pdf/2410.08968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08968]] Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements(https://arxiv.org/abs/2410.08968)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned. We propose Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, we align models to follow safety configs -- free-form natural language descriptions of the desired safety behaviors -- that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, we propose CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, we devise a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts. We show that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. Our framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality.</li>
</ul>

<h3>Title: Semantic Score Distillation Sampling for Compositional Text-to-3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Ling Yang, Zixiang Zhang, Junlin Han, Bohan Zeng, Runjia Li, Philip Torr, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09009">https://arxiv.org/abs/2410.09009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09009">https://arxiv.org/pdf/2410.09009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09009]] Semantic Score Distillation Sampling for Compositional Text-to-3D Generation(https://arxiv.org/abs/2410.09009)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: this https URL</li>
</ul>

<h3>Title: SceneCraft: Layout-Guided 3D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiuyu Yang, Yunze Man, Jun-Kun Chen, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09049">https://arxiv.org/abs/2410.09049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09049">https://arxiv.org/pdf/2410.09049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09049]] SceneCraft: Layout-Guided 3D Scene Generation(https://arxiv.org/abs/2410.09049)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools. Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps. Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation. Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality. Code and more results are available at: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
