<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-21</h1>
<h3>Title: The (R)Evolution of Multimodal Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12451">https://arxiv.org/abs/2402.12451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12451">https://arxiv.org/pdf/2402.12451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12451]] The (R)Evolution of Multimodal Large Language Models: A Survey(https://arxiv.org/abs/2402.12451)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in terms of performance and computational requirements. Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs.</li>
</ul>

<h3>Title: Integrating kNN with Foundation Models for Adaptable and Privacy-Aware  Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Doerrich, Tobias Archut, Francesco Di Salvo, Christian Ledig</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12500">https://arxiv.org/abs/2402.12500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12500">https://arxiv.org/pdf/2402.12500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12500]] Integrating kNN with Foundation Models for Adaptable and Privacy-Aware  Image Classification(https://arxiv.org/abs/2402.12500)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes. Yet, this adaptability is vital for addressing user data privacy concerns. We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining. Specifically, our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a vision-based foundation model, pre-trained self-supervised on natural images, enhancing interpretability and adaptability. We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions. Quantitative experiments confirm improved classification across established benchmark datasets and the method's applicability to distinct medical image classification tasks. Additionally, we assess the method's robustness in continual learning and data removal scenarios. The approach exhibits great promise for bridging the gap between foundation models' performance and challenges tied to data privacy. The source code is available at https://github.com/TobArc/privacy-aware-image-classification-with-kNN.</li>
</ul>

<h3>Title: PARCv2: Physics-aware Recurrent Convolutional Neural Networks for  Spatiotemporal Dynamics Modeling</h3>
<ul>
<li><strong>Authors: </strong>Phong C.H. Nguyen, Xinlun Cheng, Shahab Arfaza, Pradeep Seshadri, Yen T. Nguyen, Munho Kim, Sanghun Choi, H.S. Udaykumar, Stephen Baek</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12503">https://arxiv.org/abs/2402.12503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12503">https://arxiv.org/pdf/2402.12503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12503]] PARCv2: Physics-aware Recurrent Convolutional Neural Networks for  Spatiotemporal Dynamics Modeling(https://arxiv.org/abs/2402.12503)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model advection-reaction-diffusion equations, as well as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested on both standard benchmark problems in fluid dynamics, namely Burgers and Navier-Stokes equations, and then applied to more complex shock-induced reaction problems in energetic materials. We evaluate the behavior of PARCv2 in comparison to other physics-informed and learning bias models and demonstrate its potential to model unsteady and advection-dominant dynamics regimes.</li>
</ul>

<h3>Title: System Identification of Neural Systems: Going Beyond Images to  Modelling Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Mai Gamal, Mohamed Rashad, Eman Ehab, Seif Eldawlatly, Mennatullah Siam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12519">https://arxiv.org/abs/2402.12519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12519">https://arxiv.org/pdf/2402.12519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12519]] System Identification of Neural Systems: Going Beyond Images to  Modelling Dynamics(https://arxiv.org/abs/2402.12519)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Vast literature has compared the recordings of biological neurons in the brain to deep neural networks. The ultimate goal is to interpret deep networks or to better understand and encode biological neural systems. Recently, there has been a debate on whether system identification is possible and how much it can tell us about the brain computation. System identification recognizes whether one model is more valid to represent the brain computation over another. Nonetheless, previous work did not consider the time aspect and how video and dynamics (e.g., motion) modelling in deep networks relate to these biological neural systems within a large-scale comparison. Towards this end, we propose a system identification study focused on comparing single image vs. video understanding models with respect to the visual cortex recordings. Our study encompasses two sets of experiments; a real environment setup and a simulated environment setup. The study also encompasses more than 30 models and, unlike prior works, we focus on convolutional vs. transformer-based, single vs. two-stream, and fully vs. self-supervised video understanding models. The goal is to capture a greater variety of architectures that model dynamics. As such, this signifies the first large-scale study of video understanding models from a neuroscience perspective. Our results in the simulated experiments, show that system identification can be attained to a certain level in differentiating image vs. video understanding models. Moreover, we provide key insights on how video understanding models predict visual cortex responses; showing video understanding better than image understanding models, convolutional models are better in the early-mid regions than transformer based except for multiscale transformers that are still good in predicting these regions, and that two-stream models are better than single stream.</li>
</ul>

<h3>Title: Parallel Structures in Pre-training Data Yield In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, He He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12530">https://arxiv.org/abs/2402.12530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12530">https://arxiv.org/pdf/2402.12530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12530]] Parallel Structures in Pre-training Data Yield In-Context Learning(https://arxiv.org/abs/2402.12530)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures. A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data.</li>
</ul>

<h3>Title: Improving Deep Generative Models on Many-To-One Image-to-Image  Translation</h3>
<ul>
<li><strong>Authors: </strong>Sagar Saxena, Mohammad Nayeem Teli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12531">https://arxiv.org/abs/2402.12531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12531">https://arxiv.org/pdf/2402.12531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12531]] Improving Deep Generative Models on Many-To-One Image-to-Image  Translation(https://arxiv.org/abs/2402.12531)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models have been applied to multiple applications in image- to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized MNIST dataset and a Color-Recall score that can provide a simple benchmark for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep generative models on many-to-one image-to- image translation. We apply this framework to StarGAN V2 and show that in both unsupervised and semi-supervised settings, the performance of this new model improves on many-to-one image-to-image translation.</li>
</ul>

<h3>Title: Hierarchical Bayes Approach to Personalized Federated Unsupervised  Learning</h3>
<ul>
<li><strong>Authors: </strong>Kaan Ozkara, Bruce Huang, Ruida Zhou, Suhas Diggavi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12537">https://arxiv.org/abs/2402.12537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12537">https://arxiv.org/pdf/2402.12537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12537]] Hierarchical Bayes Approach to Personalized Federated Unsupervised  Learning(https://arxiv.org/abs/2402.12537)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics. Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored. We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models. We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity, local sample size). We also develop a theoretical framework for personalized diffusion models, which shows the benefits of collaboration even under heterogeneity. We finally evaluate our proposed algorithms using synthetic and real data, demonstrating the effective sample amplification for personalized tasks, induced through collaboration, despite data heterogeneity.</li>
</ul>

<h3>Title: Multilinear Mixture of Experts: Scalable Expert Specialization through  Factorization</h3>
<ul>
<li><strong>Authors: </strong>James Oldfield, Markos Georgopoulos, Grigorios G. Chrysos, Christos Tzelepis, Yannis Panagakis, Mihalis A. Nicolaou, Jiankang Deng, Ioannis Patras</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12550">https://arxiv.org/abs/2402.12550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12550">https://arxiv.org/pdf/2402.12550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12550]] Multilinear Mixture of Experts: Scalable Expert Specialization through  Factorization(https://arxiv.org/abs/2402.12550)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers when fine-tuning foundation models for vision tasks leads to more specialized experts at the class-level whilst remaining competitive with the performance of parameter-matched linear layer counterparts. Finally, we show that learned expert specialism further facilitates manual correction of demographic bias in CelebA attribute classification. Our MMoE model code is available at https://github.com/james-oldfield/MMoE.</li>
</ul>

<h3>Title: Standardize: Aligning Language Models with Expert-Defined Standards for  Content Generation</h3>
<ul>
<li><strong>Authors: </strong>Joseph Marvin Imperial, Gail Forey, Harish Tayyar Madabushi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12593">https://arxiv.org/abs/2402.12593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12593">https://arxiv.org/pdf/2402.12593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12593]] Standardize: Aligning Language Models with Expert-Defined Standards for  Content Generation(https://arxiv.org/abs/2402.12593)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children's reading materials. However, current works in controllable text generation have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style in-context learning-based framework to guide large language models to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 40% to 100% increase in precise accuracy for Llama2 and GPT-4, respectively, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the generation process can effectively guide models to produce better standard-aligned content.</li>
</ul>

<h3>Title: Analysis of Using Sigmoid Loss for Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Chungpa Lee, Joonhwan Chang, Jy-yong Sohn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12613">https://arxiv.org/abs/2402.12613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12613">https://arxiv.org/pdf/2402.12613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12613]] Analysis of Using Sigmoid Loss for Contrastive Learning(https://arxiv.org/abs/2402.12613)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Contrastive learning has emerged as a prominent branch of self-supervised learning for several years. Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain the optimal embedding with respect to the sigmoid loss. Second, we mathematically analyze the optimal embedding minimizing the sigmoid loss for contrastive learning. The optimal embedding ranges from simplex equiangular-tight-frame to antipodal structure, depending on the temperature parameter used in the sigmoid loss. Third, our experimental results on synthetic datasets coincide with the theoretical results on the optimal embedding structures.</li>
</ul>

<h3>Title: Generative AI Security: Challenges and Countermeasures</h3>
<ul>
<li><strong>Authors: </strong>Banghua Zhu, Norman Mu, Jiantao Jiao, David Wagner</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12617">https://arxiv.org/abs/2402.12617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12617">https://arxiv.org/pdf/2402.12617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12617]] Generative AI Security: Challenges and Countermeasures(https://arxiv.org/abs/2402.12617)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.</li>
</ul>

<h3>Title: Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Lu, Matthew Y.R. Yang, Gautam Kamath, Yaoliang Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12626">https://arxiv.org/abs/2402.12626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12626">https://arxiv.org/pdf/2402.12626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12626]] Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors(https://arxiv.org/abs/2402.12626)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-trained feature extractors. Specifically, we propose two types of attacks: (1) the input space attacks, where we modify existing attacks to directly craft poisoned data in the input space. However, due to the difficulty of optimization under constraints, we further propose (2) the feature targeted attacks, where we mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space. Our experiments examine such attacks in popular downstream tasks of fine-tuning on the same dataset and transfer learning that considers domain adaptation. Empirical results reveal that transfer learning is more vulnerable to our attacks. Additionally, input space attacks are a strong threat if no countermeasures are posed, but are otherwise weaker than feature targeted attacks.</li>
</ul>

<h3>Title: DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal  Category-level Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Takuya Ikeda, Sergey Zakharov, Tianyi Ko, Muhammad Zubair Irshad, Robert Lee, Katherine Liu, Rares Ambrus, Koichi Nishiwaki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12647">https://arxiv.org/abs/2402.12647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12647">https://arxiv.org/pdf/2402.12647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12647]] DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal  Category-level Pose Estimation(https://arxiv.org/abs/2402.12647)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain.</li>
</ul>

<h3>Title: OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech  Recognition, Translation, and Language Identification</h3>
<ul>
<li><strong>Authors: </strong>Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12654">https://arxiv.org/abs/2402.12654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12654">https://arxiv.org/pdf/2402.12654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12654]] OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech  Recognition, Translation, and Language Identification(https://arxiv.org/abs/2402.12654)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>There has been an increasing interest in large speech models that can perform multiple speech processing tasks in a single model. Such models usually adopt the encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up to 25% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form ASR result with 20x speed-up. We will publicly release our codebase, pre-trained model, and training logs to promote open science in speech foundation models.</li>
</ul>

<h3>Title: Learning Domain-Invariant Temporal Dynamics for Few-Shot Action  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yuke Li, Guangyi Chen, Ben Abramowitz, Stefano Anzellott, Donglai Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12706">https://arxiv.org/abs/2402.12706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12706">https://arxiv.org/pdf/2402.12706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12706]] Learning Domain-Invariant Temporal Dynamics for Few-Shot Action  Recognition(https://arxiv.org/abs/2402.12706)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. Our central hypothesis is that temporal invariance in the dynamic system between latent variables lends itself to transferability (domain-invariance). We therefore propose DITeD, or Domain-Invariant Temporal Dynamics for knowledge transfer. To detect the temporal invariance part, we propose a generative framework with a two-stage training strategy during pre-training. Specifically, we explicitly model invariant dynamics including temporal dynamic generation and transitions, and the variant visual and domain encoders. Then we pre-train the model with the self-supervised signals to learn the representation. After that, we fix the whole representation model and tune the classifier. During adaptation, we fix the transferable temporal dynamics and update the image encoder. The efficacy of our approach is revealed by the superior accuracy of DITeD over leading alternatives across standard few-shot action recognition datasets. Moreover, we validate that the learned temporal dynamic transition and temporal dynamic generation modules possess transferable qualities.</li>
</ul>

<h3>Title: MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for  Single or Sparse-view 3D Object Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, Rakesh Ranjan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12712">https://arxiv.org/abs/2402.12712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12712">https://arxiv.org/pdf/2402.12712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12712]] MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for  Single or Sparse-view 3D Object Reconstruction(https://arxiv.org/abs/2402.12712)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model.</li>
</ul>

<h3>Title: Diffusion Posterior Sampling is Computationally Intractable</h3>
<ul>
<li><strong>Authors: </strong>Shivam Gupta, Ajil Jalal, Aditya Parulekar, Eric Price, Zhiyang Xun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12727">https://arxiv.org/abs/2402.12727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12727">https://arxiv.org/pdf/2402.12727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12727]] Diffusion Posterior Sampling is Computationally Intractable(https://arxiv.org/abs/2402.12727)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x \mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time. In this paper we show that posterior sampling is \emph{computationally intractable}: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which \emph{every} algorithm takes superpolynomial time, even though \emph{unconditional} sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert.</li>
</ul>

<h3>Title: MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12741">https://arxiv.org/abs/2402.12741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12741">https://arxiv.org/pdf/2402.12741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12741]] MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion(https://arxiv.org/abs/2402.12741)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on https://github.com/measure-infinity/mulan-code.</li>
</ul>

<h3>Title: Me LLaMA: Foundation Large Language Models for Medical Applications</h3>
<ul>
<li><strong>Authors: </strong>Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Huan He, Lucila Ohno-Machido, Yonghui Wu, Hua Xu, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12749">https://arxiv.org/abs/2402.12749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12749">https://arxiv.org/pdf/2402.12749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12749]] Me LLaMA: Foundation Large Language Models for Medical Applications(https://arxiv.org/abs/2402.12749)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 6 out of 8 datasets and GPT-4 in 3 out of 8 datasets. In addition, we empirically investigated the catastrophic forgetting problem, and our results show that Me LLaMA models outperform other medical LLMs. Me LLaMA is one of the first and largest open-source foundational LLMs designed for the medical domain, using both biomedical and clinical data. It exhibits superior performance across both general and medical tasks compared to other medical LLMs, rendering it an attractive choice for medical AI applications. All resources are available at: https://github.com/BIDS-Xu-Lab/Me-LLaMA.</li>
</ul>

<h3>Title: FGAD: Self-boosted Knowledge Distillation for An Effective Federated  Graph Anomaly Detection Framework</h3>
<ul>
<li><strong>Authors: </strong>Jinyu Cai, Yunhe Zhang, Zhoumin Lu, Wenzhong Guo, See-kiong Ng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12761">https://arxiv.org/abs/2402.12761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12761">https://arxiv.org/pdf/2402.12761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12761]] FGAD: Self-boosted Knowledge Distillation for An Effective Federated  Graph Anomaly Detection Framework(https://arxiv.org/abs/2402.12761)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Graph anomaly detection (GAD) aims to identify anomalous graphs that significantly deviate from other ones, which has raised growing attention due to the broad existence and complexity of graph-structured data in many real-world scenarios. However, existing GAD methods usually execute with centralized training, which may lead to privacy leakage risk in some sensitive cases, thereby impeding collaboration among organizations seeking to collectively develop robust GAD models. Although federated learning offers a promising solution, the prevalent non-IID problems and high communication costs present significant challenges, particularly pronounced in collaborations with graph data distributed among different participants. To tackle these challenges, we propose an effective federated graph anomaly detection framework (FGAD). We first introduce an anomaly generator to perturb the normal graphs to be anomalous, and train a powerful anomaly detector by distinguishing generated anomalous graphs from normal ones. Then, we leverage a student model to distill knowledge from the trained anomaly detector (teacher model), which aims to maintain the personality of local models and alleviate the adverse impact of non-IID problems. Moreover, we design an effective collaborative learning mechanism that facilitates the personalization preservation of local models and significantly reduces communication costs among clients. Empirical results of the GAD tasks on non-IID graphs compared with state-of-the-art baselines demonstrate the superiority and efficiency of the proposed FGAD method.</li>
</ul>

<h3>Title: Two-stage Rainfall-Forecasting Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>XuDong Ling, ChaoRong Li, FengQing Qin, LiHong Zhu, Yuanyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12779">https://arxiv.org/abs/2402.12779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12779">https://arxiv.org/pdf/2402.12779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12779]] Two-stage Rainfall-Forecasting Diffusion Model(https://arxiv.org/abs/2402.12779)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}.</li>
</ul>

<h3>Title: OccFlowNet: Towards Self-supervised Occupancy Estimation via  Differentiable Rendering and Occupancy Flow</h3>
<ul>
<li><strong>Authors: </strong>Simon Boeder, Fabian Gigengack, Benjamin Risse</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12792">https://arxiv.org/abs/2402.12792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12792">https://arxiv.org/pdf/2402.12792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12792]] OccFlowNet: Towards Self-supervised Occupancy Estimation via  Differentiable Rendering and Occupancy Flow(https://arxiv.org/abs/2402.12792)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain.</li>
</ul>

<h3>Title: On Sensitivity of Learning with Limited Labelled Data to the Effects of  Randomness: Impact of Interactions and Systematic Choices</h3>
<ul>
<li><strong>Authors: </strong>Branislav Pecher, Ivan Srba, Maria Bielikova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12817">https://arxiv.org/abs/2402.12817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12817">https://arxiv.org/pdf/2402.12817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12817]] On Sensitivity of Learning with Limited Labelled Data to the Effects of  Randomness: Impact of Interactions and Systematic Choices(https://arxiv.org/abs/2402.12817)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format.</li>
</ul>

<h3>Title: Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How  Many Labelled Samples Do We Need?</h3>
<ul>
<li><strong>Authors: </strong>Branislav Pecher, Ivan Srba, Maria Bielikova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12819">https://arxiv.org/abs/2402.12819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12819">https://arxiv.org/pdf/2402.12819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12819]] Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How  Many Labelled Samples Do We Need?(https://arxiv.org/abs/2402.12819)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results variance.</li>
</ul>

<h3>Title: PromptKD: Distilling Student-Friendly Knowledge for Generative Language  Models via Prompt Tuning</h3>
<ul>
<li><strong>Authors: </strong>Gyeongman Kim, Doohyuk Jang, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12842">https://arxiv.org/abs/2402.12842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12842">https://arxiv.org/pdf/2402.12842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12842]] PromptKD: Distilling Student-Friendly Knowledge for Generative Language  Models via Prompt Tuning(https://arxiv.org/abs/2402.12842)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets using the GPT-2 model family show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements.</li>
</ul>

<h3>Title: SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets</h3>
<ul>
<li><strong>Authors: </strong>Sankarshanaa Sagaram, Aditya Kasliwal, Krish Didwania, Laven Srivastava, Pallavi Kailas, Ujjwal Verma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12843">https://arxiv.org/abs/2402.12843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12843">https://arxiv.org/pdf/2402.12843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12843]] SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets(https://arxiv.org/abs/2402.12843)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.</li>
</ul>

<h3>Title: RealCompo: Dynamic Equilibrium between Realism and Compositionality  Improves Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12908">https://arxiv.org/abs/2402.12908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12908">https://arxiv.org/pdf/2402.12908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12908]] RealCompo: Dynamic Equilibrium between Realism and Compositionality  Improves Text-to-Image Diffusion Models(https://arxiv.org/abs/2402.12908)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at https://github.com/YangLing0818/RealCompo</li>
</ul>

<h3>Title: CLIPping the Deception: Adapting Vision-Language Models for Universal  Deepfake Detection</h3>
<ul>
<li><strong>Authors: </strong>Sohail Ahmed Khan, Duc-Tien Dang-Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12927">https://arxiv.org/abs/2402.12927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12927">https://arxiv.org/pdf/2402.12927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12927]] CLIPping the Deception: Adapting Vision-Language Models for Universal  Deepfake Detection(https://arxiv.org/abs/2402.12927)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools.</li>
</ul>

<h3>Title: Prompt Stealing Attacks Against Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zeyang Sha, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12959">https://arxiv.org/abs/2402.12959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12959">https://arxiv.org/pdf/2402.12959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12959]] Prompt Stealing Attacks Against Large Language Models(https://arxiv.org/abs/2402.12959)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.</li>
</ul>

<h3>Title: GlrIA - A Generative and Open Large Language Model for Portuguese</h3>
<ul>
<li><strong>Authors: </strong>Ricardo Lopes, Joo Magalhes, David Semedo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12969">https://arxiv.org/abs/2402.12969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12969">https://arxiv.org/pdf/2402.12969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12969]] GlrIA - A Generative and Open Large Language Model for Portuguese(https://arxiv.org/abs/2402.12969)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs). These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese. We introduce Gl\'orIA, a robust European Portuguese decoder LLM. To pre-train Gl\'orIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks. Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark. Evaluation shows that Gl\'orIA significantly outperforms existing open PT decoder models in language modeling and that it can generate sound, knowledge-rich, and coherent PT-PT text. The model also exhibits strong potential for various downstream tasks.</li>
</ul>

<h3>Title: Visual Style Prompting with Swapping Self-Attention</h3>
<ul>
<li><strong>Authors: </strong>Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12974">https://arxiv.org/abs/2402.12974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12974">https://arxiv.org/pdf/2402.12974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12974]] Visual Style Prompting with Swapping Self-Attention(https://arxiv.org/abs/2402.12974)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately. Our project page is available \href{https://curryjung.github.io/VisualStylePrompt/}{here}.</li>
</ul>

<h3>Title: The Impact of Demonstrations on Multilingual In-Context Learning: A  Multidimensional Analysis</h3>
<ul>
<li><strong>Authors: </strong>Miaoran Zhang, Vagrant Gautam, Mingyang Wang, Jesujoba O. Alabi, Xiaoyu Shen, Dietrich Klakow, Marius Mosbach</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.12976">https://arxiv.org/abs/2402.12976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.12976">https://arxiv.org/pdf/2402.12976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.12976]] The Impact of Demonstrations on Multilingual In-Context Learning: A  Multidimensional Analysis(https://arxiv.org/abs/2402.12976)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.</li>
</ul>

<h3>Title: Text-Guided Molecule Generation with Diffusion Language Model</h3>
<ul>
<li><strong>Authors: </strong>Haisong Gong, Qiang Liu, Shu Wu, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.CL, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13040">https://arxiv.org/abs/2402.13040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13040">https://arxiv.org/pdf/2402.13040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13040]] Text-Guided Molecule Generation with Diffusion Language Model(https://arxiv.org/abs/2402.13040)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating coherent and precise molecules with specific properties, opening new avenues in drug discovery and related scientific domains. Code will be released at: https://github.com/Deno-V/tgm-dlm.</li>
</ul>

<h3>Title: Effective and Efficient Conversation Retrieval for Dialogue State  Tracking with Implicit Text Summaries</h3>
<ul>
<li><strong>Authors: </strong>Seanie Lee, Jianpeng Chen, Joris Driesen, Alexandru Coca, Anders Johannsen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13043">https://arxiv.org/abs/2402.13043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13043">https://arxiv.org/pdf/2402.13043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13043]] Effective and Efficient Conversation Retrieval for Dialogue State  Tracking with Implicit Text Summaries(https://arxiv.org/abs/2402.13043)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We validate our retrieval approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.</li>
</ul>

<h3>Title: Identifying Semantic Induction Heads to Understand In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Xipeng Qiu, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13055">https://arxiv.org/abs/2402.13055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13055">https://arxiv.org/pdf/2402.13055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13055]] Identifying Semantic Induction Heads to Understand In-Context Learning(https://arxiv.org/abs/2402.13055)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs.</li>
</ul>

<h3>Title: VGMShield: Mitigating Misuse of Video Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yan Pang, Yang Zhang, Tianhao Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13126">https://arxiv.org/abs/2402.13126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13126">https://arxiv.org/pdf/2402.13126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13126]] VGMShield: Mitigating Misuse of Video Generative Models(https://arxiv.org/abs/2402.13126)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information. In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot perfectly handle spatial-temporal relationships, and thus, we can accomplish detection and tracing with nearly perfect accuracy. Furthermore, anticipating future generative model improvements, we propose a {\it prevention} method that adds invisible perturbations to images to make the generated videos look unreal. Together with fake video detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models.</li>
</ul>

<h3>Title: Neural Network Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13144">https://arxiv.org/abs/2402.13144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13144">https://arxiv.org/pdf/2402.13144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13144]] Neural Network Diffusion(https://arxiv.org/abs/2402.13144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models.</li>
</ul>

<h3>Title: CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for  Boosting Metaphor Generation</h3>
<ul>
<li><strong>Authors: </strong>Yujie Shao, Xinrong Yao, Xingwei Qu, Chenghua Lin, Shi Wang, Stephen W. Huang, Ge Zhang, Jie Fu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13145">https://arxiv.org/abs/2402.13145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13145">https://arxiv.org/pdf/2402.13145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13145]] CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for  Boosting Metaphor Generation(https://arxiv.org/abs/2402.13145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that resonate more with real-world intuition. We test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using our annotated corpus. These models are able to generate creative and fluent metaphor sentences more frequently induced by selected samples from our dataset, demonstrating the value of our corpus for Chinese metaphor research. The code is available in the https://anonymous.4open.science/r/Chinese_Metaphor_Explanation-63F2.</li>
</ul>

<h3>Title: Defending Jailbreak Prompts via In-Context Adversarial Game</h3>
<ul>
<li><strong>Authors: </strong>Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13148">https://arxiv.org/abs/2402.13148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13148">https://arxiv.org/pdf/2402.13148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13148]] Defending Jailbreak Prompts via In-Context Adversarial Game(https://arxiv.org/abs/2402.13148)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism.</li>
</ul>

<h3>Title: AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech  Technologies</h3>
<ul>
<li><strong>Authors: </strong>Jos-M. Acosta-Triana, David Gimeno-Gmez, Carlos-D. Martnez-Hinarejos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13152">https://arxiv.org/abs/2402.13152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13152">https://arxiv.org/pdf/2402.13152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13152]] AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech  Technologies(https://arxiv.org/abs/2402.13152)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub.</li>
</ul>

<h3>Title: CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset  for Advancing Graph Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Ulrik Friis-Jensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam, Kirsten M. . Jensen, Raghavendra Selvan</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13221">https://arxiv.org/abs/2402.13221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13221">https://arxiv.org/pdf/2402.13221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13221]] CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset  for Advancing Graph Machine Learning(https://arxiv.org/abs/2402.13221)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images or text. We invite the graph ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets: A medium-scale dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K) and a large-scale dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K). We define 11 property prediction tasks and 6 structure prediction tasks, which are of special interest for nanomaterial research. We benchmark the performance of a wide array of baseline methods and use these benchmarking results to highlight areas which need future work. To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale -- both on the individual graph level and of the dataset as a whole -- and the only nanomaterials datasets with high structural and elemental diversity.</li>
</ul>

<h3>Title: A Touch, Vision, and Language Dataset for Multimodal Alignment</h3>
<ul>
<li><strong>Authors: </strong>Letian Fu, Gaurav Datta, Huang Huang, William Chung-Ho Panitch, Jaimyn Drake, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta, Roberto Calandra, Ken Goldberg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13232">https://arxiv.org/abs/2402.13232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13232">https://arxiv.org/pdf/2402.13232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13232]] A Touch, Vision, and Language Dataset for Multimodal Alignment(https://arxiv.org/abs/2402.13232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code and data: https://tactile-vlm.github.io.</li>
</ul>

<h3>Title: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic  Compositional Reasoning via Counterfactual Examples</h3>
<ul>
<li><strong>Authors: </strong>Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13254">https://arxiv.org/abs/2402.13254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13254">https://arxiv.org/pdf/2402.13254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13254]] CounterCurate: Enhancing Physical and Semantic Visio-Linguistic  Compositional Reasoning via Counterfactual Examples(https://arxiv.org/abs/2402.13254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
