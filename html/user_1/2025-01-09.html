<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-01-09</h1>
<h3>Title: More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Shuo Shang, Xiuying Chen, Rui Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04070">https://arxiv.org/abs/2501.04070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04070">https://arxiv.org/pdf/2501.04070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04070]] More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives(https://arxiv.org/abs/2501.04070)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as the number of ICL demonstrations increases from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce DR-ICL, a novel optimization method that enhances model performance through Differentiated Learning and advantage-based Reweighting objectives. Globally, DR-ICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby improving generalization. This approach allows the model to handle varying numbers of shots effectively, mitigating the impact of noisy data. Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the Many-Shot ICL Benchmark (MICLB)-a large-scale benchmark covering shot numbers from 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes. MICLB facilitates the evaluation of many-shot ICL strategies across seven prominent NLP tasks and 50 distinct datasets. Experimental results demonstrate that LLMs enhanced with DR-ICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios. We release the code and benchmark dataset hoping to facilitate further research in many-shot ICL.</li>
</ul>

<h3>Title: TrojanDec: Data-free Detection of Trojan Inputs in Self-supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Yupei Liu, Yanting Wang, Jinyuan Jia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04108">https://arxiv.org/abs/2501.04108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04108">https://arxiv.org/pdf/2501.04108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04108]] TrojanDec: Data-free Detection of Trojan Inputs in Self-supervised Learning(https://arxiv.org/abs/2501.04108)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>An image encoder pre-trained by self-supervised learning can be used as a general-purpose feature extractor to build downstream classifiers for various downstream tasks. However, many studies showed that an attacker can embed a trojan into an encoder such that multiple downstream classifiers built based on the trojaned encoder simultaneously inherit the trojan behavior. In this work, we propose TrojanDec, the first data-free method to identify and recover a test input embedded with a trigger. Given a (trojaned or clean) encoder and a test input, TrojanDec first predicts whether the test input is trojaned. If not, the test input is processed in a normal way to maintain the utility. Otherwise, the test input will be further restored to remove the trigger. Our extensive evaluation shows that TrojanDec can effectively identify the trojan (if any) from a given test input and recover it under state-of-the-art trojan attacks. We further demonstrate by experiments that our TrojanDec outperforms the state-of-the-art defenses.</li>
</ul>

<h3>Title: BiasGuard: Guardrailing Fairness in Machine Learning Production Systems</h3>
<ul>
<li><strong>Authors: </strong>Nurit Cohen-Inger, Seffi Cohen, Neomi Rabaev, Lior Rokach, Bracha Shapira</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04142">https://arxiv.org/abs/2501.04142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04142">https://arxiv.org/pdf/2501.04142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04142]] BiasGuard: Guardrailing Fairness in Machine Learning Production Systems(https://arxiv.org/abs/2501.04142)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As machine learning (ML) systems increasingly impact critical sectors such as hiring, financial risk assessments, and criminal justice, the imperative to ensure fairness has intensified due to potential negative implications. While much ML fairness research has focused on enhancing training data and processes, addressing the outputs of already deployed systems has received less attention. This paper introduces 'BiasGuard', a novel approach designed to act as a fairness guardrail in production ML systems. BiasGuard leverages Test-Time Augmentation (TTA) powered by Conditional Generative Adversarial Network (CTGAN), a cutting-edge generative AI model, to synthesize data samples conditioned on inverted protected attribute values, thereby promoting equitable outcomes across diverse groups. This method aims to provide equal opportunities for both privileged and unprivileged groups while significantly enhancing the fairness metrics of deployed systems without the need for retraining. Our comprehensive experimental analysis across diverse datasets reveals that BiasGuard enhances fairness by 31% while only reducing accuracy by 0.09% compared to non-mitigated benchmarks. Additionally, BiasGuard outperforms existing post-processing methods in improving fairness, positioning it as an effective tool to safeguard against biases when retraining the model is impractical.</li>
</ul>

<h3>Title: Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation</h3>
<ul>
<li><strong>Authors: </strong>Kam Woh Ng, Jing Yang, Jia Wei Sii, Jiankang Deng, Chee Seng Chan, Yi-Zhe Song, Tao Xiang, Xiatian Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04144">https://arxiv.org/abs/2501.04144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04144">https://arxiv.org/pdf/2501.04144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04144]] Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation(https://arxiv.org/abs/2501.04144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>In this paper, we push the boundaries of fine-grained 3D generation into truly creative territory. Current methods either lack intricate details or simply mimic existing objects -- we enable both. By lifting 2D fine-grained understanding into 3D through multi-view diffusion and modeling part latents as continuous distributions, we unlock the ability to generate entirely new, yet plausible parts through interpolation and sampling. A self-supervised feature consistency loss further ensures stable generation of these unseen parts. The result is the first system capable of creating novel 3D objects with species-specific details that transcend existing examples. While we demonstrate our approach on birds, the underlying framework extends beyond things that can chirp! Code will be released at this https URL.</li>
</ul>

<h3>Title: Generative Dataset Distillation Based on Self-knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Longzhen Li, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04202">https://arxiv.org/abs/2501.04202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04202">https://arxiv.org/pdf/2501.04202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04202]] Generative Dataset Distillation Based on Self-knowledge Distillation(https://arxiv.org/abs/2501.04202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dataset distillation is an effective technique for reducing the cost and complexity of model training while maintaining performance by compressing large datasets into smaller, more efficient versions. In this paper, we present a novel generative dataset distillation method that can improve the accuracy of aligning prediction logits. Our approach integrates self-knowledge distillation to achieve more precise distribution matching between the synthetic and original data, thereby capturing the overall structure and relationships within the data. To further improve the accuracy of alignment, we introduce a standardization step on the logits before performing distribution matching, ensuring consistency in the range of logits. Through extensive experiments, we demonstrate that our method outperforms existing state-of-the-art methods, resulting in superior distillation performance.</li>
</ul>

<h3>Title: Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images</h3>
<ul>
<li><strong>Authors: </strong>Ren Tasai, Guang Li, Ren Togo, Minghui Tang, Takaaki Yoshimura, Hiroyuki Sugimori, Kenji Hirata, Takahiro Ogawa, Kohsuke Kudo, Miki Haseyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04217">https://arxiv.org/abs/2501.04217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04217">https://arxiv.org/pdf/2501.04217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04217]] Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images(https://arxiv.org/abs/2501.04217)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We propose a novel continual self-supervised learning method (CSSL) considering medical domain knowledge in chest CT images. Our approach addresses the challenge of sequential learning by effectively capturing the relationship between previously learned knowledge and new information at different stages. By incorporating an enhanced DER into CSSL and maintaining both diversity and representativeness within the rehearsal buffer of DER, the risk of data interference during pretraining is reduced, enabling the model to learn more richer and robust feature representations. In addition, we incorporate a mixup strategy and feature distillation to further enhance the model's ability to learn meaningful representations. We validate our method using chest CT images obtained under two different imaging conditions, demonstrating superior performance compared to state-of-the-art methods.</li>
</ul>

<h3>Title: ContextMRI: Enhancing Compressed Sensing MRI through Metadata Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Hyungjin Chung, Dohun Lee, Zihui Wu, Byung-Hoon Kim, Katherine L. Bouman, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04284">https://arxiv.org/abs/2501.04284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04284">https://arxiv.org/pdf/2501.04284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04284]] ContextMRI: Enhancing Compressed Sensing MRI through Metadata Conditioning(https://arxiv.org/abs/2501.04284)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Compressed sensing MRI seeks to accelerate MRI acquisition processes by sampling fewer k-space measurements and then reconstructing the missing data algorithmically. The success of these approaches often relies on strong priors or learned statistical models. While recent diffusion model-based priors have shown great potential, previous methods typically ignore clinically available metadata (e.g. patient demographics, imaging parameters, slice-specific information). In practice, metadata contains meaningful cues about the anatomy and acquisition protocol, suggesting it could further constrain the reconstruction problem. In this work, we propose ContextMRI, a text-conditioned diffusion model for MRI that integrates granular metadata into the reconstruction process. We train a pixel-space diffusion model directly on minimally processed, complex-valued MRI images. During inference, metadata is converted into a structured text prompt and fed to the model via CLIP text embeddings. By conditioning the prior on metadata, we unlock more accurate reconstructions and show consistent gains across multiple datasets, acceleration factors, and undersampling patterns. Our experiments demonstrate that increasing the fidelity of metadata, ranging from slice location and contrast to patient age, sex, and pathology, systematically boosts reconstruction performance. This work highlights the untapped potential of leveraging clinical context for inverse problems and opens a new direction for metadata-driven MRI reconstruction.</li>
</ul>

<h3>Title: An Analysis of Model Robustness across Concurrent Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Myeongho Jeon, Suhwan Choi, Hyoje Lee, Teresa Yeo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04288">https://arxiv.org/abs/2501.04288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04288">https://arxiv.org/pdf/2501.04288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04288]] An Analysis of Model Robustness across Concurrent Distribution Shifts(https://arxiv.org/abs/2501.04288)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Machine learning models, meticulously optimized for source data, often fail to predict target data when faced with distribution shifts (DSs). Previous benchmarking studies, though extensive, have mainly focused on simple DSs. Recognizing that DSs often occur in more complex forms in real-world scenarios, we broadened our study to include multiple concurrent shifts, such as unseen domain shifts combined with spurious correlations. We evaluated 26 algorithms that range from simple heuristic augmentations to zero-shot inference using foundation models, across 168 source-target pairs from eight datasets. Our analysis of over 100K models reveals that (i) concurrent DSs typically worsen performance compared to a single shift, with certain exceptions, (ii) if a model improves generalization for one distribution shift, it tends to be effective for others, and (iii) heuristic data augmentations achieve the best overall performance on both synthetic and real-world datasets.</li>
</ul>

<h3>Title: DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hyogon Ryu, NaHyeon Park, Hyunjung Shim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04304">https://arxiv.org/abs/2501.04304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04304">https://arxiv.org/pdf/2501.04304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04304]] DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models(https://arxiv.org/abs/2501.04304)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the widespread use of text-to-image diffusion models across various tasks, their computational and memory demands limit practical applications. To mitigate this issue, quantization of diffusion models has been explored. It reduces memory usage and computational costs by compressing weights and activations into lower-bit formats. However, existing methods often struggle to preserve both image quality and text-image alignment, particularly in lower-bit($<$ 8bits) quantization. In this paper, we analyze the challenges associated with quantizing text-to-image diffusion models from a distributional perspective. Our analysis reveals that activation outliers play a crucial role in determining image quality. Additionally, we identify distinctive patterns in cross-attention scores, which significantly affects text-image alignment. To address these challenges, we propose Distribution-aware Group Quantization (DGQ), a method that identifies and adaptively handles pixel-wise and channel-wise outliers to preserve image quality. Furthermore, DGQ applies prompt-specific logarithmic quantization scales to maintain text-image alignment. Our method demonstrates remarkable performance on datasets such as MS-COCO and PartiPrompts. We are the first to successfully achieve low-bit quantization of text-to-image diffusion models without requiring additional fine-tuning of weight quantization parameters.</li>
</ul>

<h3>Title: Physics-Informed Super-Resolution Diffusion for 6D Phase Space Diagnostics</h3>
<ul>
<li><strong>Authors: </strong>Alexander Scheinker</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, physics.acc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04305">https://arxiv.org/abs/2501.04305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04305">https://arxiv.org/pdf/2501.04305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04305]] Physics-Informed Super-Resolution Diffusion for 6D Phase Space Diagnostics(https://arxiv.org/abs/2501.04305)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Adaptive physics-informed super-resolution diffusion is developed for non-invasive virtual diagnostics of the 6D phase space density of charged particle beams. An adaptive variational autoencoder (VAE) embeds initial beam condition images and scalar measurements to a low-dimensional latent space from which a 326 pixel 6D tensor representation of the beam's 6D phase space density is generated. Projecting from a 6D tensor generates physically consistent 2D projections. Physics-guided super-resolution diffusion transforms low-resolution images of the 6D density to high resolution 256x256 pixel images. Un-supervised adaptive latent space tuning enables tracking of time-varying beams without knowledge of time-varying initial conditions. The method is demonstrated with experimental data and multi-particle simulations at the HiRES UED. The general approach is applicable to a wide range of complex dynamic systems evolving in high-dimensional phase space. The method is shown to be robust to distribution shift without re-training.</li>
</ul>

<h3>Title: Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring Contexts</h3>
<ul>
<li><strong>Authors: </strong>Preethi Seshadri, Seraphina Goldfarb-Tarrant</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04316">https://arxiv.org/abs/2501.04316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04316">https://arxiv.org/pdf/2501.04316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04316]] Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring Contexts(https://arxiv.org/abs/2501.04316)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly being deployed in high-stakes applications like hiring, yet their potential for unfair decision-making and outcomes remains understudied, particularly in generative settings. In this work, we examine the fairness of LLM-based hiring systems through two real-world tasks: resume summarization and retrieval. By constructing a synthetic resume dataset and curating job postings, we investigate whether model behavior differs across demographic groups and is sensitive to demographic perturbations. Our findings reveal that race-based differences appear in approximately 10% of generated summaries, while gender-based differences occur in only 1%. In the retrieval setting, all evaluated models display non-uniform selection patterns across demographic groups and exhibit high sensitivity to both gender and race-based perturbations. Surprisingly, retrieval models demonstrate comparable sensitivity to non-demographic changes, suggesting that fairness issues may stem, in part, from general brittleness issues. Overall, our results indicate that LLM-based hiring systems, especially at the retrieval stage, can exhibit notable biases that lead to discriminatory outcomes in real-world contexts.</li>
</ul>

<h3>Title: Edit as You See: Image-guided Video Editing via Masked Motion Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zhi-Lin Huang, Yixuan Liu, Chujun Qin, Zhongdao Wang, Dong Zhou, Dong Li, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04325">https://arxiv.org/abs/2501.04325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04325">https://arxiv.org/pdf/2501.04325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04325]] Edit as You See: Image-guided Video Editing via Masked Motion Modeling(https://arxiv.org/abs/2501.04325)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have significantly facilitated text-guided video editing. However, there is a relative scarcity of research on image-guided video editing, a method that empowers users to edit videos by merely indicating a target object in the initial frame and providing an RGB image as reference, without relying on the text prompts. In this paper, we propose a novel Image-guided Video Editing Diffusion model, termed IVEDiff for the image-guided video editing. IVEDiff is built on top of image editing models, and is equipped with learnable motion modules to maintain the temporal consistency of edited video. Inspired by self-supervised learning concepts, we introduce a masked motion modeling fine-tuning strategy that empowers the motion module's capabilities for capturing inter-frame motion dynamics, while preserving the capabilities for intra-frame semantic correlations modeling of the base image editing model. Moreover, an optical-flow-guided motion reference network is proposed to ensure the accurate propagation of information between edited video frames, alleviating the misleading effects of invalid information. We also construct a benchmark to facilitate further research. The comprehensive experiments demonstrate that our method is able to generate temporally smooth edited videos while robustly dealing with various editing objects with high quality.</li>
</ul>

<h3>Title: Instructive3D: Editing Large Reconstruction Models with Text Instructions</h3>
<ul>
<li><strong>Authors: </strong>Kunal Kathare, Ankit Dhiman, K Vikas Gowda, Siddharth Aravindan, Shubham Monga, Basavaraja Shanthappa Vandrotti, Lokesh R Boregowda</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04374">https://arxiv.org/abs/2501.04374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04374">https://arxiv.org/pdf/2501.04374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04374]] Instructive3D: Editing Large Reconstruction Models with Text Instructions(https://arxiv.org/abs/2501.04374)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Transformer based methods have enabled users to create, modify, and comprehend text and image data. Recently proposed Large Reconstruction Models (LRMs) further extend this by providing the ability to generate high-quality 3D models with the help of a single object image. These models, however, lack the ability to manipulate or edit the finer details, such as adding standard design patterns or changing the color and reflectance of the generated objects, thus lacking fine-grained control that may be very helpful in domains such as augmented reality, animation and gaming. Naively training LRMs for this purpose would require generating precisely edited images and 3D object pairs, which is computationally expensive. In this paper, we propose Instructive3D, a novel LRM based model that integrates generation and fine-grained editing, through user text prompts, of 3D objects into a single model. We accomplish this by adding an adapter that performs a diffusion process conditioned on a text prompt specifying edits in the triplane latent space representation of 3D object models. Our method does not require the generation of edited 3D objects. Additionally, Instructive3D allows us to perform geometrically consistent modifications, as the edits done through user-defined text prompts are applied to the triplane latent representation thus enhancing the versatility and precision of 3D objects generated. We compare the objects generated by Instructive3D and a baseline that first generates the 3D object meshes using a standard LRM model and then edits these 3D objects using text prompts when images are provided from the Objaverse LVIS dataset. We find that Instructive3D produces qualitatively superior 3D objects with the properties specified by the edit prompts.</li>
</ul>

<h3>Title: iFADIT: Invertible Face Anonymization via Disentangled Identity Transform</h3>
<ul>
<li><strong>Authors: </strong>Lin Yuan, Kai Liang, Xiong Li, Tao Wu, Nannan Wang, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04390">https://arxiv.org/abs/2501.04390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04390">https://arxiv.org/pdf/2501.04390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04390]] iFADIT: Invertible Face Anonymization via Disentangled Identity Transform(https://arxiv.org/abs/2501.04390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face anonymization aims to conceal the visual identity of a face to safeguard the individual's privacy. Traditional methods like blurring and pixelation can largely remove identifying features, but these techniques significantly degrade image quality and are vulnerable to deep reconstruction attacks. Generative models have emerged as a promising solution for anonymizing faces while preserving a natural this http URL, many still face limitations in visual quality and often overlook the potential to recover the original face from the anonymized version, which can be valuable in specific contexts such as image forensics. This paper proposes a novel framework named iFADIT, an acronym for Invertible Face Anonymization via Disentangled Identity this http URL framework features a disentanglement architecture coupled with a secure flow-based model: the former decouples identity information from non-identifying attributes, while the latter transforms the decoupled identity into an anonymized version in an invertible manner controlled by a secret key. The anonymized face can then be reconstructed based on a pre-trained StyleGAN that ensures high image quality and realistic facial details. Recovery of the original face (aka de-anonymization) is possible upon the availability of the matching secret, by inverting the anonymization process based on the same set of model parameters. Furthermore, a dedicated secret-key mechanism along with a dual-phase training strategy is devised to ensure the desired properties of face anonymization. Qualitative and quantitative experiments demonstrate the superiority of the proposed approach in anonymity, reversibility, security, diversity, and interpretability over competing methods.</li>
</ul>

<h3>Title: Improving Image Captioning by Mimicking Human Reformulation Feedback at Inference-time</h3>
<ul>
<li><strong>Authors: </strong>Uri Berger, Omri Abend, Lea Frermann, Gabriel Stanovsky</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04513">https://arxiv.org/abs/2501.04513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04513">https://arxiv.org/pdf/2501.04513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04513]] Improving Image Captioning by Mimicking Human Reformulation Feedback at Inference-time(https://arxiv.org/abs/2501.04513)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Incorporating automatically predicted human feedback into the process of training generative models has attracted substantial recent interest, while feedback at inference time has received less attention. The typical feedback at training time, i.e., preferences of choice given two samples, does not naturally transfer to the inference phase. We introduce a novel type of feedback -- caption reformulations -- and train models to mimic reformulation feedback based on human annotations. Our method does not require training the image captioning model itself, thereby demanding substantially less computational effort. We experiment with two types of reformulation feedback: first, we collect a dataset of human reformulations that correct errors in the generated captions. We find that incorporating reformulation models trained on this data into the inference phase of existing image captioning models results in improved captions, especially when the original captions are of low quality. We apply our method to non-English image captioning, a domain where robust models are less prevalent, and gain substantial improvement. Second, we apply reformulations to style transfer. Quantitative evaluations reveal state-of-the-art performance on German image captioning and English style transfer, while human validation with a detailed comparative framework exposes the specific axes of improvement.</li>
</ul>

<h3>Title: Learnable Scaled Gradient Descent for Guaranteed Robust Tensor PCA</h3>
<ul>
<li><strong>Authors: </strong>Lanlan Feng, Ce Zhu, Yipeng Liu, Saiprasad Ravishankar, Longxiu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04565">https://arxiv.org/abs/2501.04565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04565">https://arxiv.org/pdf/2501.04565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04565]] Learnable Scaled Gradient Descent for Guaranteed Robust Tensor PCA(https://arxiv.org/abs/2501.04565)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Robust tensor principal component analysis (RTPCA) aims to separate the low-rank and sparse components from multi-dimensional data, making it an essential technique in the signal processing and computer vision fields. Recently emerging tensor singular value decomposition (t-SVD) has gained considerable attention for its ability to better capture the low-rank structure of tensors compared to traditional matrix SVD. However, existing methods often rely on the computationally expensive tensor nuclear norm (TNN), which limits their scalability for real-world tensors. To address this issue, we explore an efficient scaled gradient descent (SGD) approach within the t-SVD framework for the first time, and propose the RTPCA-SGD method. Theoretically, we rigorously establish the recovery guarantees of RTPCA-SGD under mild assumptions, demonstrating that with appropriate parameter selection, it achieves linear convergence to the true low-rank tensor at a constant rate, independent of the condition number. To enhance its practical applicability, we further propose a learnable self-supervised deep unfolding model, which enables effective parameter learning. Numerical experiments on both synthetic and real-world datasets demonstrate the superior performance of the proposed methods while maintaining competitive computational efficiency, especially consuming less time than RTPCA-TNN.</li>
</ul>

<h3>Title: Unified Coding for Both Human Perception and Generalized Machine Analytics with CLIP Supervision</h3>
<ul>
<li><strong>Authors: </strong>Kangsheng Yin, Quan Liu, Xuelin Shen, Yulin He, Wenhan Yang, Shiqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04579">https://arxiv.org/abs/2501.04579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04579">https://arxiv.org/pdf/2501.04579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04579]] Unified Coding for Both Human Perception and Generalized Machine Analytics with CLIP Supervision(https://arxiv.org/abs/2501.04579)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The image compression model has long struggled with adaptability and generalization, as the decoded bitstream typically serves only human or machine needs and fails to preserve information for unseen visual tasks. Therefore, this paper innovatively introduces supervision obtained from multimodal pre-training models and incorporates adaptive multi-objective optimization tailored to support both human visual perception and machine vision simultaneously with a single bitstream, denoted as Unified and Generalized Image Coding for Machine (UG-ICM). Specifically, to get rid of the reliance between compression models with downstream task supervision, we introduce Contrastive Language-Image Pre-training (CLIP) models into the training constraint for improved generalization. Global-to-instance-wise CLIP supervision is applied to help obtain hierarchical semantics that make models more generalizable for the tasks relying on the information of different granularity. Furthermore, for supporting both human and machine visions with only a unifying bitstream, we incorporate a conditional decoding strategy that takes as conditions human or machine preferences, enabling the bitstream to be decoded into different versions for corresponding preferences. As such, our proposed UG-ICM is fully trained in a self-supervised manner, i.e., without awareness of any specific downstream models and tasks. The extensive experiments have shown that the proposed UG-ICM is capable of achieving remarkable improvements in various unseen machine analytics tasks, while simultaneously providing perceptually satisfying images.</li>
</ul>

<h3>Title: Boosting Salient Object Detection with Knowledge Distillated from Large Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Miaoyang He, Shuyong Gao, Tsui Qin Mok, Weifeng Ge, Wengqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04582">https://arxiv.org/abs/2501.04582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04582">https://arxiv.org/pdf/2501.04582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04582]] Boosting Salient Object Detection with Knowledge Distillated from Large Foundation Models(https://arxiv.org/abs/2501.04582)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Salient Object Detection (SOD) aims to identify and segment prominent regions within a scene. Traditional models rely on manually annotated pseudo labels with precise pixel-level accuracy, which is time-consuming. We developed a low-cost, high-precision annotation method by leveraging large foundation models to address the challenges. Specifically, we use a weakly supervised approach to guide large models in generating pseudo-labels through textual prompts. Since large models do not effectively focus on the salient regions of images, we manually annotate a subset of text to fine-tune the model. Based on this approach, which enables precise and rapid generation of pseudo-labels, we introduce a new dataset, BDS-TR. Compared to the previous DUTS-TR dataset, BDS-TR is more prominent in scale and encompasses a wider variety of categories and scenes. This expansion will enhance our model's applicability across a broader range of scenarios and provide a more comprehensive foundational dataset for future SOD research. Additionally, we present an edge decoder based on dynamic upsampling, which focuses on object edges while gradually recovering image feature resolution. Comprehensive experiments on five benchmark datasets demonstrate that our method significantly outperforms state-of-the-art approaches and also surpasses several existing fully-supervised SOD methods. The code and results will be made available.</li>
</ul>

<h3>Title: Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion</h3>
<ul>
<li><strong>Authors: </strong>Yangfan He, Sida Li, Kun Li, Jianhui Wang, Binxu Li, Tianyu Shi, Jun Yin, Miao Zhang, Xueqian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04606">https://arxiv.org/abs/2501.04606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04606">https://arxiv.org/pdf/2501.04606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04606]] Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion(https://arxiv.org/abs/2501.04606)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image (T2I) generation using diffusion models have enabled cost-effective video-editing applications by leveraging pre-trained models, eliminating the need for resource-intensive training. However, the frame-independence of T2I generation often results in poor temporal consistency. Existing methods address this issue through temporal layer fine-tuning or inference-based temporal propagation, but these approaches suffer from high training costs or limited temporal coherence. To address these challenges, we propose a General and Efficient Adapter (GE-Adapter) that integrates temporal-spatial and semantic consistency with Baliteral DDIM inversion. This framework introduces three key components: (1) Frame-based Temporal Consistency Blocks (FTC Blocks) to capture frame-specific features and enforce smooth inter-frame transitions via temporally-aware loss functions; (2) Channel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral filters to enhance spatial coherence by reducing noise and artifacts; and (3) Token-based Semantic Consistency Module (TSC Module) to maintain semantic alignment using shared prompt tokens and frame-specific tokens. Our method significantly improves perceptual quality, text-image alignment, and temporal coherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves enhanced fidelity and frame-to-frame coherence, offering a practical solution for T2V editing.</li>
</ul>

<h3>Title: Disentangled Clothed Avatar Generation with Layered Representation</h3>
<ul>
<li><strong>Authors: </strong>Weitian Zhang, Sijing Wu, Manwen Liao, Yichao Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04631">https://arxiv.org/abs/2501.04631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04631">https://arxiv.org/pdf/2501.04631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04631]] Disentangled Clothed Avatar Generation with Layered Representation(https://arxiv.org/abs/2501.04631)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Clothed avatar generation has wide applications in virtual and augmented reality, filmmaking, and more. Previous methods have achieved success in generating diverse digital avatars, however, generating avatars with disentangled components (\eg, body, hair, and clothes) has long been a challenge. In this paper, we propose LayerAvatar, the first feed-forward diffusion-based method for generating component-disentangled clothed avatars. To achieve this, we first propose a layered UV feature plane representation, where components are distributed in different layers of the Gaussian-based UV feature plane with corresponding semantic labels. This representation supports high-resolution and real-time rendering, as well as expressive animation including controllable gestures and facial expressions. Based on the well-designed representation, we train a single-stage diffusion model and introduce constrain terms to address the severe occlusion problem of the innermost human body layer. Extensive experiments demonstrate the impressive performances of our method in generating disentangled clothed avatars, and we further explore its applications in component transfer. The project page is available at: this https URL</li>
</ul>

<h3>Title: A Statistical Theory of Contrastive Pre-training and Multimodal Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Kazusato Oko, Licong Lin, Yuhang Cai, Song Mei</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04641">https://arxiv.org/abs/2501.04641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04641">https://arxiv.org/pdf/2501.04641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04641]] A Statistical Theory of Contrastive Pre-training and Multimodal Generative AI(https://arxiv.org/abs/2501.04641)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Multi-modal generative AI systems, such as those combining vision and language, rely on contrastive pre-training to learn representations across different modalities. While their practical benefits are widely acknowledged, a rigorous theoretical understanding of the contrastive pre-training framework remains limited. This paper develops a theoretical framework to explain the success of contrastive pre-training in downstream tasks, such as zero-shot classification, conditional diffusion models, and vision-language models. We introduce the concept of approximate sufficient statistics, a generalization of the classical sufficient statistics, and show that near-minimizers of the contrastive pre-training loss are approximately sufficient, making them adaptable to diverse downstream tasks. We further propose the Joint Generative Hierarchical Model for the joint distribution of images and text, showing that transformers can efficiently approximate relevant functions within this model via belief propagation. Building on this framework, we derive sample complexity guarantees for multi-modal learning based on contrastive pre-trained representations. Numerical simulations validate these theoretical findings, demonstrating the strong generalization performance of contrastively pre-trained transformers in various multi-modal tasks.</li>
</ul>

<h3>Title: SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M. Rehg, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04689">https://arxiv.org/abs/2501.04689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04689">https://arxiv.org/pdf/2501.04689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04689]] SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images(https://arxiv.org/abs/2501.04689)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We study the problem of single-image 3D object reconstruction. Recent works have diverged into two directions: regression-based modeling and generative modeling. Regression methods efficiently infer visible surfaces, but struggle with occluded regions. Generative methods handle uncertain regions better by modeling distributions, but are computationally expensive and the generation is often misaligned with visible surfaces. In this paper, we present SPAR3D, a novel two-stage approach aiming to take the best of both directions. The first stage of SPAR3D generates sparse 3D point clouds using a lightweight point diffusion model, which has a fast sampling speed. The second stage uses both the sampled point cloud and the input image to create highly detailed meshes. Our two-stage design enables probabilistic modeling of the ill-posed single-image 3D task while maintaining high computational efficiency and great output fidelity. Using point clouds as an intermediate representation further allows for interactive user edits. Evaluated on diverse datasets, SPAR3D demonstrates superior performance over previous state-of-the-art methods, at an inference speed of 0.7 seconds. Project page with code and model: this https URL</li>
</ul>

<h3>Title: Test-Time Optimization for Domain Adaptive Open Vocabulary Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ulindu De Silva, Didula Samaraweera, Sasini Wanigathunga, Kavindu Kariyawasam, Kanchana Ranasinghe, Muzammal Naseer, Ranga Rodrigo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04696">https://arxiv.org/abs/2501.04696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04696">https://arxiv.org/pdf/2501.04696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04696]] Test-Time Optimization for Domain Adaptive Open Vocabulary Segmentation(https://arxiv.org/abs/2501.04696)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We present Seg-TTO, a novel framework for zero-shot, open-vocabulary semantic segmentation (OVSS), designed to excel in specialized domain tasks. While current open vocabulary approaches show impressive performance on standard segmentation benchmarks under zero-shot settings, they fall short of supervised counterparts on highly domain-specific datasets. We focus on segmentation-specific test-time optimization to address this gap. Segmentation requires an understanding of multiple concepts within a single image while retaining the locality and spatial structure of representations. We propose a novel self-supervised objective adhering to these requirements and use it to align the model parameters with input images at test time. In the textual modality, we learn multiple embeddings for each category to capture diverse concepts within an image, while in the visual modality, we calculate pixel-level losses followed by embedding aggregation operations specific to preserving spatial structure. Our resulting framework termed Seg-TTO is a plug-in-play module. We integrate Seg-TTO with three state-of-the-art OVSS approaches and evaluate across 22 challenging OVSS tasks covering a range of specialized domains. Our Seg-TTO demonstrates clear performance improvements across these establishing new state-of-the-art. Code: this https URL.</li>
</ul>

<h3>Title: ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04698">https://arxiv.org/abs/2501.04698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04698">https://arxiv.org/pdf/2501.04698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04698]] ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning(https://arxiv.org/abs/2501.04698)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges in this task: 1) the identity decoupling problem, where directly adopting existing customization methods inevitably mix attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training such a model that represents and decouples various concepts well. To address these challenges, we introduce ConceptMaster, an innovative framework that effectively tackles the critical issues of identity decoupling while maintaining concept fidelity in customized videos. Specifically, we introduce a novel strategy of learning decoupled multi-concept embeddings that are injected into the diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To further overcome the scarcity of high-quality MCVC data, we carefully establish a data construction pipeline, which enables systematic collection of precise multi-concept video-entity data across diverse concepts. A comprehensive benchmark is designed to validate the effectiveness of our model from three critical dimensions: concept fidelity, identity decoupling ability, and video generation quality across six different concept composition scenarios. Extensive experiments demonstrate that our ConceptMaster significantly outperforms previous approaches for this task, paving the way for generating personalized and semantically accurate videos across multiple concepts.</li>
</ul>

<h3>Title: EditAR: Unified Conditional Generation with Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Jiteng Mu, Nuno Vasconcelos, Xiaolong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2501.04699">https://arxiv.org/abs/2501.04699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2501.04699">https://arxiv.org/pdf/2501.04699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2501.04699]] EditAR: Unified Conditional Generation with Autoregressive Models(https://arxiv.org/abs/2501.04699)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Recent progress in controllable image generation and editing is largely driven by diffusion-based methods. Although diffusion models perform exceptionally well in specific tasks with tailored designs, establishing a unified model is still challenging. In contrast, autoregressive models inherently feature a unified tokenized representation, which simplifies the creation of a single foundational model for various tasks. In this work, we propose EditAR, a single unified autoregressive framework for a variety of conditional image generation tasks, e.g., image editing, depth-to-image, edge-to-image, segmentation-to-image. The model takes both images and instructions as inputs, and predicts the edited images tokens in a vanilla next-token paradigm. To enhance the text-to-image alignment, we further propose to distill the knowledge from foundation models into the autoregressive modeling process. We evaluate its effectiveness across diverse tasks on established benchmarks, showing competitive performance to various state-of-the-art task-specific methods. Project page: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
