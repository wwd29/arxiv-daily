<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-16</h1>
<h3>Title: Enhancing Question Answering for Enterprise Knowledge Bases using Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Feihu Jiang, Chuan Qin, Kaichun Yao, Chuyu Fang, Fuzhen Zhuang, Hengshu Zhu, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08695">https://arxiv.org/abs/2404.08695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08695">https://arxiv.org/pdf/2404.08695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08695]] Enhancing Question Answering for Enterprise Knowledge Bases using Large  Language Models(https://arxiv.org/abs/2404.08695)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Efficient knowledge management plays a pivotal role in augmenting both the operational efficiency and the innovative capacity of businesses and organizations. By indexing knowledge through vectorization, a variety of knowledge retrieval methods have emerged, significantly enhancing the efficacy of knowledge management systems. Recently, the rapid advancements in generative natural language processing technologies paved the way for generating precise and coherent answers after retrieving relevant documents tailored to user queries. However, for enterprise knowledge bases, assembling extensive training data from scratch for knowledge retrieval and generation is a formidable challenge due to the privacy and security policies of private data, frequently entailing substantial costs. To address the challenge above, in this paper, we propose EKRG, a novel Retrieval-Generation framework based on large language models (LLMs), expertly designed to enable question-answering for Enterprise Knowledge bases with limited annotation costs. Specifically, for the retrieval process, we first introduce an instruction-tuning method using an LLM to generate sufficient document-question pairs for training a knowledge retriever. This method, through carefully designed instructions, efficiently generates diverse questions for enterprise knowledge bases, encompassing both fact-oriented and solution-oriented knowledge. Additionally, we develop a relevance-aware teacher-student learning strategy to further enhance the efficiency of the training process. For the generation process, we propose a novel chain of thought (CoT) based fine-tuning method to empower the LLM-based generator to adeptly respond to user questions using retrieved documents. Finally, extensive experiments on real-world datasets have demonstrated the effectiveness of our proposed framework.</li>
</ul>

<h3>Title: Can Contrastive Learning Refine Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Lihui Liu, Jinha Kim, Vidit Bansal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08701">https://arxiv.org/abs/2404.08701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08701">https://arxiv.org/pdf/2404.08701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08701]] Can Contrastive Learning Refine Embeddings(https://arxiv.org/abs/2404.08701)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent advancements in contrastive learning have revolutionized self-supervised representation learning and achieved state-of-the-art performance on benchmark tasks. While most existing methods focus on applying contrastive learning to input data modalities such as images, natural language sentences, or networks, they overlook the potential of utilizing outputs from previously trained encoders. In this paper, we introduce SIMSKIP, a novel contrastive learning framework that specifically refines input embeddings for downstream tasks. Unlike traditional unsupervised learning approaches, SIMSKIP takes advantage of the output embeddings of encoder models as its input. Through theoretical analysis, we provide evidence that applying SIMSKIP does not result in larger upper bounds on downstream task errors than those of the original embeddings, which serve as SIMSKIP's input. Experimental results on various open datasets demonstrate that the embeddings produced by SIMSKIP improve performance on downstream tasks.</li>
</ul>

<h3>Title: FastLogAD: Log Anomaly Detection with Mask-Guided Pseudo Anomaly  Generation and Discrimination</h3>
<ul>
<li><strong>Authors: </strong>Yifei Lin, Hanqiu Deng, Xingyu Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08750">https://arxiv.org/abs/2404.08750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08750">https://arxiv.org/pdf/2404.08750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08750]] FastLogAD: Log Anomaly Detection with Mask-Guided Pseudo Anomaly  Generation and Discrimination(https://arxiv.org/abs/2404.08750)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Nowadays large computers extensively output logs to record the runtime status and it has become crucial to identify any suspicious or malicious activities from the information provided by the realtime logs. Thus, fast log anomaly detection is a necessary task to be implemented for automating the infeasible manual detection. Most of the existing unsupervised methods are trained only on normal log data, but they usually require either additional abnormal data for hyperparameter selection or auxiliary datasets for discriminative model optimization. In this paper, aiming for a highly effective discriminative model that enables rapid anomaly detection,we propose FastLogAD, a generator-discriminator framework trained to exhibit the capability of generating pseudo-abnormal logs through the Mask-Guided Anomaly Generation (MGAG) model and efficiently identifying the anomalous logs via the Discriminative Abnormality Separation (DAS) model. Particularly, pseudo-abnormal logs are generated by replacing randomly masked tokens in a normal sequence with unlikely candidates. During the discriminative stage, FastLogAD learns a distinct separation between normal and pseudoabnormal samples based on their embedding norms, allowing the selection of a threshold without exposure to any test data and achieving competitive performance. Extensive experiments on several common benchmarks show that our proposed FastLogAD outperforms existing anomaly detection approaches. Furthermore, compared to previous methods, FastLogAD achieves at least x10 speed increase in anomaly detection over prior work. Our implementation is available at https://github.com/YifeiLin0226/FastLogAD.</li>
</ul>

<h3>Title: Towards Sim-to-Real Industrial Parts Classification with Synthetic  Dataset</h3>
<ul>
<li><strong>Authors: </strong>Xiaomeng Zhu, Talha Bilal, Pär Mårtensson, Lars Hanson, Mårten Björkman, Atsuto Maki</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08778">https://arxiv.org/abs/2404.08778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08778">https://arxiv.org/pdf/2404.08778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08778]] Towards Sim-to-Real Industrial Parts Classification with Synthetic  Dataset(https://arxiv.org/abs/2404.08778)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper is about effectively utilizing synthetic data for training deep neural networks for industrial parts classification, in particular, by taking into account the domain gap against real-world images. To this end, we introduce a synthetic dataset that may serve as a preliminary testbed for the Sim-to-Real challenge; it contains 17 objects of six industrial use cases, including isolated and assembled parts. A few subsets of objects exhibit large similarities in shape and albedo for reflecting challenging cases of industrial parts. All the sample images come with and without random backgrounds and post-processing for evaluating the importance of domain randomization. We call it Synthetic Industrial Parts dataset (SIP-17). We study the usefulness of SIP-17 through benchmarking the performance of five state-of-the-art deep network models, supervised and self-supervised, trained only on the synthetic data while testing them on real data. By analyzing the results, we deduce some insights on the feasibility and challenges of using synthetic data for industrial parts classification and for further developing larger-scale synthetic datasets. Our dataset and code are publicly available.</li>
</ul>

<h3>Title: Detecting AI-Generated Images via CLIP</h3>
<ul>
<li><strong>Authors: </strong>A.G. Moskowitz, T. Gaona, J. Peterson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08788">https://arxiv.org/abs/2404.08788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08788">https://arxiv.org/pdf/2404.08788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08788]] Detecting AI-Generated Images via CLIP(https://arxiv.org/abs/2404.08788)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As AI-generated image (AIGI) methods become more powerful and accessible, it has become a critical task to determine if an image is real or AI-generated. Because AIGI lack the signatures of photographs and have their own unique patterns, new models are needed to determine if an image is AI-generated. In this paper, we investigate the ability of the Contrastive Language-Image Pre-training (CLIP) architecture, pre-trained on massive internet-scale data sets, to perform this differentiation. We fine-tune CLIP on real images and AIGI from several generative models, enabling CLIP to determine if an image is AI-generated and, if so, determine what generation method was used to create it. We show that the fine-tuned CLIP architecture is able to differentiate AIGI as well or better than models whose architecture is specifically designed to detect AIGI. Our method will significantly increase access to AIGI-detecting tools and reduce the negative effects of AIGI on society, as our CLIP fine-tuning procedures require no architecture changes from publicly available model repositories and consume significantly less GPU resources than other AIGI detection models.</li>
</ul>

<h3>Title: Differentiable and Stable Long-Range Tracking of Multiple Posterior  Modes</h3>
<ul>
<li><strong>Authors: </strong>Ali Younis, Erik Sudderth</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08789">https://arxiv.org/abs/2404.08789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08789">https://arxiv.org/pdf/2404.08789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08789]] Differentiable and Stable Long-Range Tracking of Multiple Posterior  Modes(https://arxiv.org/abs/2404.08789)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Particle filters flexibly represent multiple posterior modes nonparametrically, via a collection of weighted samples, but have classically been applied to tracking problems with known dynamics and observation likelihoods. Such generative models may be inaccurate or unavailable for high-dimensional observations like images. We instead leverage training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders. While prior discriminative particle filters have used heuristic relaxations of discrete particle resampling, or biased learning by truncating gradients at resampling steps, we achieve unbiased and low-variance gradient estimates by representing posteriors as continuous mixture densities. Our theory and experiments expose dramatic failures of existing reparameterization-based estimators for mixture gradients, an issue we address via an importance-sampling gradient estimator. Unlike standard recurrent neural networks, our mixture density particle filter represents multimodal uncertainty in continuous latent states, improving accuracy and robustness. On a range of challenging tracking and robot localization problems, our approach achieves dramatic improvements in accuracy, while also showing much greater stability across multiple training runs.</li>
</ul>

<h3>Title: Semantic Approach to Quantifying the Consistency of Diffusion Model  Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Brinnae Bent</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08799">https://arxiv.org/abs/2404.08799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08799">https://arxiv.org/pdf/2404.08799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08799]] Semantic Approach to Quantifying the Consistency of Diffusion Model  Image Generation(https://arxiv.org/abs/2404.08799)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this study, we identify the need for an interpretable, quantitative score of the repeatability, or consistency, of image generation in diffusion models. We propose a semantic approach, using a pairwise mean CLIP (Contrastive Language-Image Pretraining) score as our semantic consistency score. We applied this metric to compare two state-of-the-art open-source image generation diffusion models, Stable Diffusion XL and PixArt-{\alpha}, and we found statistically significant differences between the semantic consistency scores for the models. Agreement between the Semantic Consistency Score selected model and aggregated human annotations was 94%. We also explored the consistency of SDXL and a LoRA-fine-tuned version of SDXL and found that the fine-tuned model had significantly higher semantic consistency in generated images. The Semantic Consistency Score proposed here offers a measure of image generation alignment, facilitating the evaluation of model architectures for specific tasks and aiding in informed decision-making regarding model selection.</li>
</ul>

<h3>Title: E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors  to New Generators Using Limited Data</h3>
<ul>
<li><strong>Authors: </strong>Aref Azizpour, Tai D. Nguyen, Manil Shrestha, Kaidi Xu, Edward Kim, Matthew C. Stamm</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08814">https://arxiv.org/abs/2404.08814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08814">https://arxiv.org/pdf/2404.08814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08814]] E3: Ensemble of Expert Embedders for Adapting Synthetic Image Detectors  to New Generators Using Limited Data(https://arxiv.org/abs/2404.08814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As generative AI progresses rapidly, new synthetic image generators continue to emerge at a swift pace. Traditional detection methods face two main challenges in adapting to these generators: the forensic traces of synthetic images from new techniques can vastly differ from those learned during training, and access to data for these new generators is often limited. To address these issues, we introduce the Ensemble of Expert Embedders (E3), a novel continual learning framework for updating synthetic image detectors. E3 enables the accurate detection of images from newly emerged generators using minimal training data. Our approach does this by first employing transfer learning to develop a suite of expert embedders, each specializing in the forensic traces of a specific generator. Then, all embeddings are jointly analyzed by an Expert Knowledge Fusion Network to produce accurate and reliable detection decisions. Our experiments demonstrate that E3 outperforms existing continual learning methods, including those developed specifically for synthetic image detection.</li>
</ul>

<h3>Title: Single-image driven 3d viewpoint training data augmentation for  effective wine label recognition</h3>
<ul>
<li><strong>Authors: </strong>Yueh-Cheng Huang, Hsin-Yi Chen, Cheng-Jui Hung, Jen-Hui Chuang, Jenq-Neng Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08820">https://arxiv.org/abs/2404.08820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08820">https://arxiv.org/pdf/2404.08820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08820]] Single-image driven 3d viewpoint training data augmentation for  effective wine label recognition(https://arxiv.org/abs/2404.08820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Confronting the critical challenge of insufficient training data in the field of complex image recognition, this paper introduces a novel 3D viewpoint augmentation technique specifically tailored for wine label recognition. This method enhances deep learning model performance by generating visually realistic training samples from a single real-world wine label image, overcoming the challenges posed by the intricate combinations of text and logos. Classical Generative Adversarial Network (GAN) methods fall short in synthesizing such intricate content combination. Our proposed solution leverages time-tested computer vision and image processing strategies to expand our training dataset, thereby broadening the range of training samples for deep learning applications. This innovative approach to data augmentation circumvents the constraints of limited training resources. Using the augmented training images through batch-all triplet metric learning on a Vision Transformer (ViT) architecture, we can get the most discriminative embedding features for every wine label, enabling us to perform one-shot recognition of existing wine labels in the training classes or future newly collected wine labels unavailable in the training. Experimental results show a significant increase in recognition accuracy over conventional 2D data augmentation techniques.</li>
</ul>

<h3>Title: LLM In-Context Recall is Prompt Dependent</h3>
<ul>
<li><strong>Authors: </strong>Daniel Machlab, Rick Battle</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08865">https://arxiv.org/abs/2404.08865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08865">https://arxiv.org/pdf/2404.08865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08865]] LLM In-Context Recall is Prompt Dependent(https://arxiv.org/abs/2404.08865)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The proliferation of Large Language Models (LLMs) highlights the critical importance of conducting thorough evaluations to discern their comparative advantages, limitations, and optimal use cases. Particularly important is assessing their capacity to accurately retrieve information included in a given prompt. A model's ability to do this significantly influences how effectively it can utilize contextual details, thus impacting its practical efficacy and dependability in real-world applications. Our research analyzes the in-context recall performance of various LLMs using the needle-in-a-haystack method. In this approach, a factoid (the "needle") is embedded within a block of filler text (the "haystack"), which the model is asked to retrieve. We assess the recall performance of each model across various haystack lengths and with varying needle placements to identify performance patterns. This study demonstrates that an LLM's recall capability is not only contingent upon the prompt's content but also may be compromised by biases in its training data. Conversely, adjustments to model architecture, training strategy, or fine-tuning can improve performance. Our analysis provides insight into LLM behavior, offering direction for the development of more effective applications of LLMs.</li>
</ul>

<h3>Title: EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal  LLM</h3>
<ul>
<li><strong>Authors: </strong>Henry Peng Zou, Gavin Heqing Yu, Ziwei Fan, Dan Bu, Han Liu, Peng Dai, Dongmei Jia, Cornelia Caragea</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08886">https://arxiv.org/abs/2404.08886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08886">https://arxiv.org/pdf/2404.08886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08886]] EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal  LLM(https://arxiv.org/abs/2404.08886)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In e-commerce, accurately extracting product attribute values from multimodal data is crucial for improving user experience and operational efficiency of retailers. However, previous approaches to multimodal attribute value extraction often struggle with implicit attribute values embedded in images or text, rely heavily on extensive labeled data, and can easily confuse similar attribute values. To address these issues, we introduce EIVEN, a data- and parameter-efficient generative framework that pioneers the use of multimodal LLM for implicit attribute value extraction. EIVEN leverages the rich inherent knowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled data. We also introduce a novel Learning-by-Comparison technique to reduce model confusion by enforcing attribute value comparison and difference identification. Additionally, we construct initial open-source datasets for multimodal implicit attribute value extraction. Our extensive experiments reveal that EIVEN significantly outperforms existing methods in extracting implicit attribute values while requiring less labeled data.</li>
</ul>

<h3>Title: ChangeAnywhere: Sample Generation for Remote Sensing Change Detection  via Semantic Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Kai Tang, Jin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08892">https://arxiv.org/abs/2404.08892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08892">https://arxiv.org/pdf/2404.08892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08892]] ChangeAnywhere: Sample Generation for Remote Sensing Change Detection  via Semantic Latent Diffusion Model(https://arxiv.org/abs/2404.08892)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Remote sensing change detection (CD) is a pivotal technique that pinpoints changes on a global scale based on multi-temporal images. With the recent expansion of deep learning, supervised deep learning-based CD models have shown satisfactory performance. However, CD sample labeling is very time-consuming as it is densely labeled and requires expert knowledge. To alleviate this problem, we introduce ChangeAnywhere, a novel CD sample generation method using the semantic latent diffusion model and single-temporal images. Specifically, ChangeAnywhere leverages the relative ease of acquiring large single-temporal semantic datasets to generate large-scale, diverse, and semantically annotated bi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD samples, i.e., change implies semantically different, and non-change implies reasonable change under the same semantic constraints. We generated ChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD samples based on the proposed method. The ChangeAnywhere-100K significantly improved both zero-shot and few-shot performance on two CD benchmark datasets for various deep learning-based CD models, as demonstrated by transfer experiments. This paper delineates the enormous potential of ChangeAnywhere for CD sample generation and demonstrates the subsequent enhancement of model performance. Therefore, ChangeAnywhere offers a potent tool for remote sensing CD. All codes and pre-trained models will be available at https://github.com/tangkai-RS/ChangeAnywhere.</li>
</ul>

<h3>Title: PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical  Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Zhenwei Wang, Qiule Sun, Bingbing Zhang, Pengfei Wang, Jianxin Zhang, Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08915">https://arxiv.org/abs/2404.08915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08915">https://arxiv.org/pdf/2404.08915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08915]] PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical  Image Classification(https://arxiv.org/abs/2404.08915)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Few-shot learning has been successfully applied to medical image classification as only very few medical examples are available for training. Due to the challenging problem of limited number of annotated medical images, image representations should not be solely derived from a single image modality which is insufficient for characterizing concept classes. In this paper, we propose a new prompting multi-modal model paradigm on medical image classification based on multi-modal foundation models, called PM2. Besides image modality,PM2 introduces another supplementary text input, known as prompt, to further describe corresponding image or concept classes and facilitate few-shot learning across diverse modalities. To better explore the potential of prompt engineering, we empirically investigate five distinct prompt schemes under the new paradigm. Furthermore, linear probing in multi-modal models acts as a linear classification head taking as input only class token, which ignores completely merits of rich statistics inherent in high-level visual tokens. Thus, we alternatively perform a linear classification on feature distribution of visual tokens and class token simultaneously. To effectively mine such rich statistics, a global covariance pooling with efficient matrix power normalization is used to aggregate visual tokens. Then we study and combine two classification heads. One is shared for class token of image from vision encoder and prompt representation encoded by text encoder. The other is to classification on feature distribution of visual tokens from vision encoder. Extensive experiments on three medical datasets show that our PM2 significantly outperforms counterparts regardless of prompt schemes and achieves state-of-the-art performance.</li>
</ul>

<h3>Title: Diffusion Models Meet Remote Sensing: Principles, Methods, and  Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Yidan Liu, Jun Yue, Shaobo Xia, Pedram Ghamisi, Weiying Xie, Leyuan Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08926">https://arxiv.org/abs/2404.08926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08926">https://arxiv.org/pdf/2404.08926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08926]] Diffusion Models Meet Remote Sensing: Principles, Methods, and  Perspectives(https://arxiv.org/abs/2404.08926)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>As a newly emerging advance in deep generative models, diffusion models have achieved state-of-the-art results in many fields, including computer vision, natural language processing, and molecule design. The remote sensing community has also noticed the powerful ability of diffusion models and quickly applied them to a variety of tasks for image processing. Given the rapid increase in research on diffusion models in the field of remote sensing, it is necessary to conduct a comprehensive review of existing diffusion model-based remote sensing papers, to help researchers recognize the potential of diffusion models and provide some directions for further exploration. Specifically, this paper first introduces the theoretical background of diffusion models, and then systematically reviews the applications of diffusion models in remote sensing, including image generation, enhancement, and interpretation. Finally, the limitations of existing remote sensing diffusion models and worthy research directions for further exploration are discussed and summarized.</li>
</ul>

<h3>Title: Label-free Anomaly Detection in Aerial Agricultural Images with Masked  Image Modeling</h3>
<ul>
<li><strong>Authors: </strong>Sambal Shikhar, Anupam Sobti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08931">https://arxiv.org/abs/2404.08931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08931">https://arxiv.org/pdf/2404.08931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08931]] Label-free Anomaly Detection in Aerial Agricultural Images with Masked  Image Modeling(https://arxiv.org/abs/2404.08931)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Detecting various types of stresses (nutritional, water, nitrogen, etc.) in agricultural fields is critical for farmers to ensure maximum productivity. However, stresses show up in different shapes and sizes across different crop types and varieties. Hence, this is posed as an anomaly detection task in agricultural images. Accurate anomaly detection in agricultural UAV images is vital for early identification of field irregularities. Traditional supervised learning faces challenges in adapting to diverse anomalies, necessitating extensive annotated data. In this work, we overcome this limitation with self-supervised learning using a masked image modeling approach. Masked Autoencoders (MAE) extract meaningful normal features from unlabeled image samples which produces high reconstruction error for the abnormal pixels during reconstruction. To remove the need of using only ``normal" data while training, we use an anomaly suppression loss mechanism that effectively minimizes the reconstruction of anomalous pixels and allows the model to learn anomalous areas without explicitly separating ``normal" images for training. Evaluation on the Agriculture-Vision data challenge shows a mIOU score improvement in comparison to prior state of the art in unsupervised and self-supervised methods. A single model generalizes across all the anomaly categories in the Agri-Vision Challenge Dataset</li>
</ul>

<h3>Title: Enforcing Paraphrase Generation via Controllable Latent Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Wei Zou, Ziyuan Zhuang, Shujian Huang, Jia Liu, Jiajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08938">https://arxiv.org/abs/2404.08938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08938">https://arxiv.org/pdf/2404.08938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08938]] Enforcing Paraphrase Generation via Controllable Latent Diffusion(https://arxiv.org/abs/2404.08938)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Paraphrase generation aims to produce high-quality and diverse utterances of a given text. Though state-of-the-art generation via the diffusion model reconciles generation quality and diversity, textual diffusion suffers from a truncation issue that hinders efficiency and quality control. In this work, we propose \textit{L}atent \textit{D}iffusion \textit{P}araphraser~(LDP), a novel paraphrase generation by modeling a controllable diffusion process given a learned latent space. LDP achieves superior generation efficiency compared to its diffusion counterparts. It facilitates only input segments to enforce paraphrase semantics, which further improves the results without external features. Experiments show that LDP achieves improved and diverse paraphrase generation compared to baselines. Further analysis shows that our method is also helpful to other similar text generations and domain adaptations. Our code and data are available at https://github.com/NIL-zhuang/ld4pg.</li>
</ul>

<h3>Title: Multimodal Cross-Document Event Coreference Resolution Using Linear  Semantic Transfer and Mixed-Modality Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Abhijnan Nath, Huma Jamil, Shafiuddin Rehan Ahmed, George Baker, Rahul Ghosh, James H. Martin, Nathaniel Blanchard, Nikhil Krishnaswamy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08949">https://arxiv.org/abs/2404.08949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08949">https://arxiv.org/pdf/2404.08949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08949]] Multimodal Cross-Document Event Coreference Resolution Using Linear  Semantic Transfer and Mixed-Modality Ensembles(https://arxiv.org/abs/2404.08949)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Event coreference resolution (ECR) is the task of determining whether distinct mentions of events within a multi-document corpus are actually linked to the same underlying occurrence. Images of the events can help facilitate resolution when language is ambiguous. Here, we propose a multimodal cross-document event coreference resolution method that integrates visual and textual cues with a simple linear map between vision and language models. As existing ECR benchmark datasets rarely provide images for all event mentions, we augment the popular ECB+ dataset with event-centric images scraped from the internet and generated using image diffusion models. We establish three methods that incorporate images and text for coreference: 1) a standard fused model with finetuning, 2) a novel linear mapping method without finetuning and 3) an ensembling approach based on splitting mention pairs by semantic and discourse-level difficulty. We evaluate on 2 datasets: the augmented ECB+, and AIDA Phase 1. Our ensemble systems using cross-modal linear mapping establish an upper limit (91.9 CoNLL F1) on ECB+ ECR performance given the preprocessing assumptions used, and establish a novel baseline on AIDA Phase 1. Our results demonstrate the utility of multimodal information in ECR for certain challenging coreference problems, and highlight a need for more multimodal resources in the coreference resolution space.</li>
</ul>

<h3>Title: Beyond Known Clusters: Probe New Prototypes for Efficient Generalized  Class Discovery</h3>
<ul>
<li><strong>Authors: </strong>Ye Wang, Yaxiong Wang, Yujiao Wu, Bingchen Zhao, Xueming Qian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08995">https://arxiv.org/abs/2404.08995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08995">https://arxiv.org/pdf/2404.08995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08995]] Beyond Known Clusters: Probe New Prototypes for Efficient Generalized  Class Discovery(https://arxiv.org/abs/2404.08995)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Generalized Class Discovery (GCD) aims to dynamically assign labels to unlabelled data partially based on knowledge learned from labelled data, where the unlabelled data may come from known or novel classes. The prevailing approach generally involves clustering across all data and learning conceptions by prototypical contrastive learning. However, existing methods largely hinge on the performance of clustering algorithms and are thus subject to their inherent limitations. Firstly, the estimated cluster number is often smaller than the ground truth, making the existing methods suffer from the lack of prototypes for comprehensive conception learning. To address this issue, we propose an adaptive probing mechanism that introduces learnable potential prototypes to expand cluster prototypes (centers). As there is no ground truth for the potential prototype, we develop a self-supervised prototype learning framework to optimize the potential prototype in an end-to-end fashion. Secondly, clustering is computationally intensive, and the conventional strategy of clustering both labelled and unlabelled instances exacerbates this issue. To counteract this inefficiency, we opt to cluster only the unlabelled instances and subsequently expand the cluster prototypes with our introduced potential prototypes to fast explore novel classes. Despite the simplicity of our proposed method, extensive empirical analysis on a wide range of datasets confirms that our method consistently delivers state-of-the-art results. Specifically, our method surpasses the nearest competitor by a significant margin of \textbf{9.7}$\%$ within the Stanford Cars dataset and \textbf{12$\times$} clustering efficiency within the Herbarium 19 dataset. We will make the code and checkpoints publicly available at \url{https://github.com/xjtuYW/PNP.git}.</li>
</ul>

<h3>Title: MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial  Expression Recognition in-the-wild</h3>
<ul>
<li><strong>Authors: </strong>Kateryna Chumachenko, Alexandros Iosifidis, Moncef Gabbouj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09010">https://arxiv.org/abs/2404.09010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09010">https://arxiv.org/pdf/2404.09010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09010]] MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial  Expression Recognition in-the-wild(https://arxiv.org/abs/2404.09010)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Dynamic Facial Expression Recognition (DFER) has received significant interest in the recent years dictated by its pivotal role in enabling empathic and human-compatible technologies. Achieving robustness towards in-the-wild data in DFER is particularly important for real-world applications. One of the directions aimed at improving such models is multimodal emotion recognition based on audio and video data. Multimodal learning in DFER increases the model capabilities by leveraging richer, complementary data representations. Within the field of multimodal DFER, recent methods have focused on exploiting advances of self-supervised learning (SSL) for pre-training of strong multimodal encoders. Another line of research has focused on adapting pre-trained static models for DFER. In this work, we propose a different perspective on the problem and investigate the advancement of multimodal DFER performance by adapting SSL-pre-trained disjoint unimodal encoders. We identify main challenges associated with this task, namely, intra-modality adaptation, cross-modal alignment, and temporal adaptation, and propose solutions to each of them. As a result, we demonstrate improvement over current state-of-the-art on two popular DFER benchmarks, namely DFEW and MFAW.</li>
</ul>

<h3>Title: Theoretical research on generative diffusion models: an overview</h3>
<ul>
<li><strong>Authors: </strong>Melike Nur Yeğin, Mehmet Fatih Amasyalı</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09016">https://arxiv.org/abs/2404.09016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09016">https://arxiv.org/pdf/2404.09016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09016]] Theoretical research on generative diffusion models: an overview(https://arxiv.org/abs/2404.09016)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative diffusion models showed high success in many fields with a powerful theoretical background. They convert the data distribution to noise and remove the noise back to obtain a similar distribution. Many existing reviews focused on the specific application areas without concentrating on the research about the algorithm. Unlike them we investigated the theoretical developments of the generative diffusion models. These approaches mainly divide into two: training-based and sampling-based. Awakening to this allowed us a clear and understandable categorization for the researchers who will make new developments in the future.</li>
</ul>

<h3>Title: Adapting Mental Health Prediction Tasks for Cross-lingual Learning via  Meta-Training and In-context Learning with Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Zita Lifelo, Huansheng Ning, Sahraoui Dhelim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09045">https://arxiv.org/abs/2404.09045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09045">https://arxiv.org/pdf/2404.09045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09045]] Adapting Mental Health Prediction Tasks for Cross-lingual Learning via  Meta-Training and In-context Learning with Large Language Model(https://arxiv.org/abs/2404.09045)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Timely identification is essential for the efficient handling of mental health illnesses such as depression. However, the current research fails to adequately address the prediction of mental health conditions from social media data in low-resource African languages like Swahili. This study introduces two distinct approaches utilising model-agnostic meta-learning and leveraging large language models (LLMs) to address this gap. Experiments are conducted on three datasets translated to low-resource language and applied to four mental health tasks, which include stress, depression, depression severity and suicidal ideation prediction. we first apply a meta-learning model with self-supervision, which results in improved model initialisation for rapid adaptation and cross-lingual transfer. The results show that our meta-trained model performs significantly better than standard fine-tuning methods, outperforming the baseline fine-tuning in macro F1 score with 18\% and 0.8\% over XLM-R and mBERT. In parallel, we use LLMs' in-context learning capabilities to assess their performance accuracy across the Swahili mental health prediction tasks by analysing different cross-lingual prompting approaches. Our analysis showed that Swahili prompts performed better than cross-lingual prompts but less than English prompts. Our findings show that in-context learning can be achieved through cross-lingual transfer through carefully crafted prompt templates with examples and instructions.</li>
</ul>

<h3>Title: Rethinking Iterative Stereo Matching from Diffusion Bridge Model  Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yuguang Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09051">https://arxiv.org/abs/2404.09051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09051">https://arxiv.org/pdf/2404.09051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09051]] Rethinking Iterative Stereo Matching from Diffusion Bridge Model  Perspective(https://arxiv.org/abs/2404.09051)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, iteration-based stereo matching has shown great potential. However, these models optimize the disparity map using RNN variants. The discrete optimization process poses a challenge of information loss, which restricts the level of detail that can be expressed in the generated disparity map. In order to address these issues, we propose a novel training approach that incorporates diffusion models into the iterative optimization process. We designed a Time-based Gated Recurrent Unit (T-GRU) to correlate temporal and disparity outputs. Unlike standard recurrent units, we employ Agent Attention to generate more expressive features. We also designed an attention-based context network to capture a large amount of contextual information. Experiments on several public benchmarks show that we have achieved competitive stereo matching performance. Our model ranks first in the Scene Flow dataset, achieving over a 7% improvement compared to competing methods, and requires only 8 iterations to achieve state-of-the-art results.</li>
</ul>

<h3>Title: Probabilistic Directed Distance Fields for Ray-Based Shape  Representations</h3>
<ul>
<li><strong>Authors: </strong>Tristan Aumentado-Armstrong, Stavros Tsogkas, Sven Dickinson, Allan Jepson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09081">https://arxiv.org/abs/2404.09081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09081">https://arxiv.org/pdf/2404.09081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09081]] Probabilistic Directed Distance Fields for Ray-Based Shape  Representations(https://arxiv.org/abs/2404.09081)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In modern computer vision, the optimal representation of 3D shape continues to be task-dependent. One fundamental operation applied to such representations is differentiable rendering, as it enables inverse graphics approaches in learning frameworks. Standard explicit shape representations (voxels, point clouds, or meshes) are often easily rendered, but can suffer from limited geometric fidelity, among other issues. On the other hand, implicit representations (occupancy, distance, or radiance fields) preserve greater fidelity, but suffer from complex or inefficient rendering processes, limiting scalability. In this work, we devise Directed Distance Fields (DDFs), a novel neural shape representation that builds upon classical distance fields. The fundamental operation in a DDF maps an oriented point (position and direction) to surface visibility and depth. This enables efficient differentiable rendering, obtaining depth with a single forward pass per pixel, as well as differential geometric quantity extraction (e.g., surface normals), with only additional backward passes. Using probabilistic DDFs (PDDFs), we show how to model inherent discontinuities in the underlying field. We then apply DDFs to several applications, including single-shape fitting, generative modelling, and single-image 3D reconstruction, showcasing strong performance with simple architectural components via the versatility of our representation. Finally, since the dimensionality of DDFs permits view-dependent geometric artifacts, we conduct a theoretical investigation of the constraints necessary for view consistency. We find a small set of field properties that are sufficient to guarantee a DDF is consistent, without knowing, for instance, which shape the field is expressing.</li>
</ul>

<h3>Title: Exploring Generative AI for Sim2Real in Driving Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Haonan Zhao, Yiting Wang, Thomas Bashford-Rogers, Valentina Donzella, Kurt Debattista</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09111">https://arxiv.org/abs/2404.09111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09111">https://arxiv.org/pdf/2404.09111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09111]] Exploring Generative AI for Sim2Real in Driving Data Synthesis(https://arxiv.org/abs/2404.09111)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Datasets are essential for training and testing vehicle perception algorithms. However, the collection and annotation of real-world images is time-consuming and expensive. Driving simulators offer a solution by automatically generating various driving scenarios with corresponding annotations, but the simulation-to-reality (Sim2Real) domain gap remains a challenge. While most of the Generative Artificial Intelligence (AI) follows the de facto Generative Adversarial Nets (GANs)-based methods, the recent emerging diffusion probabilistic models have not been fully explored in mitigating Sim2Real challenges for driving data synthesis. To explore the performance, this paper applied three different generative AI methods to leverage semantic label maps from a driving simulator as a bridge for the creation of realistic datasets. A comparative analysis of these methods is presented from the perspective of image quality and perception. New synthetic datasets, which include driving images and auto-generated high-quality annotations, are produced with low costs and high scene variability. The experimental results show that although GAN-based methods are adept at generating high-quality images when provided with manually annotated labels, ControlNet produces synthetic datasets with fewer artefacts and more structural fidelity when using simulator-generated labels. This suggests that the diffusion-based approach may provide improved stability and an alternative method for addressing Sim2Real challenges.</li>
</ul>

<h3>Title: GCC: Generative Calibration Clustering</h3>
<ul>
<li><strong>Authors: </strong>Haifeng Xia, Hai Huang, Zhengming Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09115">https://arxiv.org/abs/2404.09115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09115">https://arxiv.org/pdf/2404.09115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09115]] GCC: Generative Calibration Clustering(https://arxiv.org/abs/2404.09115)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Deep clustering as an important branch of unsupervised representation learning focuses on embedding semantically similar samples into the identical feature space. This core demand inspires the exploration of contrastive learning and subspace clustering. However, these solutions always rely on the basic assumption that there are sufficient and category-balanced samples for generating valid high-level representation. This hypothesis actually is too strict to be satisfied for real-world applications. To overcome such a challenge, the natural strategy is utilizing generative models to augment considerable instances. How to use these novel samples to effectively fulfill clustering performance improvement is still difficult and under-explored. In this paper, we propose a novel Generative Calibration Clustering (GCC) method to delicately incorporate feature learning and augmentation into clustering procedure. First, we develop a discriminative feature alignment mechanism to discover intrinsic relationship across real and generated samples. Second, we design a self-supervised metric learning to generate more reliable cluster assignment to boost the conditional diffusion generation. Extensive experimental results on three benchmarks validate the effectiveness and advantage of our proposed method over the state-of-the-art methods.</li>
</ul>

<h3>Title: Confidence Calibration and Rationalization for LLMs via Multi-Agent  Deliberation</h3>
<ul>
<li><strong>Authors: </strong>Ruixin Yang, Dheeraj Rajagopa, Shirley Anugrah Hayati, Bin Hu, Dongyeop Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09127">https://arxiv.org/abs/2404.09127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09127">https://arxiv.org/pdf/2404.09127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09127]] Confidence Calibration and Rationalization for LLMs via Multi-Agent  Deliberation(https://arxiv.org/abs/2404.09127)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the "Collective Wisdom": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.</li>
</ul>

<h3>Title: From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian  Language Representation</h3>
<ul>
<li><strong>Authors: </strong>Artur Kiulian, Anton Polishko, Mykola Khandoga, Oryna Chubych, Jack Connor, Raghav Ravishankar, Adarsh Shirawalmath</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09138">https://arxiv.org/abs/2404.09138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09138">https://arxiv.org/pdf/2404.09138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09138]] From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian  Language Representation(https://arxiv.org/abs/2404.09138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the rapidly advancing field of AI and NLP, generative large language models (LLMs) stand at the forefront of innovation, showcasing unparalleled abilities in text understanding and generation. However, the limited representation of low-resource languages like Ukrainian poses a notable challenge, restricting the reach and relevance of this technology. Our paper addresses this by fine-tuning the open-source Gemma and Mistral LLMs with Ukrainian datasets, aiming to improve their linguistic proficiency and benchmarking them against other existing models capable of processing Ukrainian language. This endeavor not only aims to mitigate language bias in technology but also promotes inclusivity in the digital realm. Our transparent and reproducible approach encourages further NLP research and development. Additionally, we present the Ukrainian Knowledge and Instruction Dataset (UKID) to aid future efforts in language model fine-tuning. Our research not only advances the field of NLP but also highlights the importance of linguistic diversity in AI, which is crucial for cultural preservation, education, and expanding AI's global utility. Ultimately, we advocate for a future where technology is inclusive, enabling AI to communicate effectively across all languages, especially those currently underrepresented.</li>
</ul>

<h3>Title: RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Guoxuan Chi, Zheng Yang, Chenshu Wu, Jingao Xu, Yuchong Gao, Yunhao Liu, Tony Xiao Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09140">https://arxiv.org/abs/2404.09140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09140">https://arxiv.org/pdf/2404.09140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09140]] RF-Diffusion: Radio Signal Generation via Time-Frequency Diffusion(https://arxiv.org/abs/2404.09140)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Along with AIGC shines in CV and NLP, its potential in the wireless domain has also emerged in recent years. Yet, existing RF-oriented generative solutions are ill-suited for generating high-quality, time-series RF data due to limited representation capabilities. In this work, inspired by the stellar achievements of the diffusion model in CV and NLP, we adapt it to the RF domain and propose RF-Diffusion. To accommodate the unique characteristics of RF signals, we first introduce a novel Time-Frequency Diffusion theory to enhance the original diffusion model, enabling it to tap into the information within the time, frequency, and complex-valued domains of RF signals. On this basis, we propose a Hierarchical Diffusion Transformer to translate the theory into a practical generative DNN through elaborated design spanning network architecture, functional block, and complex-valued operator, making RF-Diffusion a versatile solution to generate diverse, high-quality, and time-series RF data. Performance comparison with three prevalent generative models demonstrates the RF-Diffusion's superior performance in synthesizing Wi-Fi and FMCW signals. We also showcase the versatility of RF-Diffusion in boosting Wi-Fi sensing systems and performing channel estimation in 5G networks.</li>
</ul>

<h3>Title: ToNER: Type-oriented Named Entity Recognition with Generative Language  Model</h3>
<ul>
<li><strong>Authors: </strong>Guochao Jiang, Ziqin Luo, Yuchen Shi, Dixuan Wang, Jiaqing Liang, Deqing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09145">https://arxiv.org/abs/2404.09145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09145">https://arxiv.org/pdf/2404.09145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09145]] ToNER: Type-oriented Named Entity Recognition with Generative Language  Model(https://arxiv.org/abs/2404.09145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, the fine-tuned generative models have been proven more powerful than the previous tagging-based or span-based models on named entity recognition (NER) task. It has also been found that the information related to entities, such as entity types, can prompt a model to achieve NER better. However, it is not easy to determine the entity types indeed existing in the given sentence in advance, and inputting too many potential entity types would distract the model inevitably. To exploit entity types' merit on promoting NER task, in this paper we propose a novel NER framework, namely ToNER based on a generative model. In ToNER, a type matching model is proposed at first to identify the entity types most likely to appear in the sentence. Then, we append a multiple binary classification task to fine-tune the generative model's encoder, so as to generate the refined representation of the input sentence. Moreover, we add an auxiliary task for the model to discover the entity types which further fine-tunes the model to output more accurate results. Our extensive experiments on some NER benchmarks verify the effectiveness of our proposed strategies in ToNER that are oriented towards entity types' exploitation.</li>
</ul>

<h3>Title: GeMQuAD : Generating Multilingual Question Answering Datasets from Large  Language Models using Few Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Amani Namboori, Shivam Mangale, Andy Rosenbaum, Saleh Soltan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09163">https://arxiv.org/abs/2404.09163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09163">https://arxiv.org/pdf/2404.09163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09163]] GeMQuAD : Generating Multilingual Question Answering Datasets from Large  Language Models using Few Shot Learning(https://arxiv.org/abs/2404.09163)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The emergence of Large Language Models (LLMs) with capabilities like In-Context Learning (ICL) has ushered in new possibilities for data generation across various domains while minimizing the need for extensive data collection and modeling techniques. Researchers have explored ways to use this generated synthetic data to optimize smaller student models for reduced deployment costs and lower latency in downstream tasks. However, ICL-generated data often suffers from low quality as the task specificity is limited with few examples used in ICL. In this paper, we propose GeMQuAD - a semi-supervised learning approach, extending the WeakDAP framework, applied to a dataset generated through ICL with just one example in the target language using AlexaTM 20B Seq2Seq LLM. Through our approach, we iteratively identify high-quality data to enhance model performance, especially for low-resource multilingual setting in the context of Extractive Question Answering task. Our framework outperforms the machine translation-augmented model by 0.22/1.68 F1/EM (Exact Match) points for Hindi and 0.82/1.37 F1/EM points for Spanish on the MLQA dataset, and it surpasses the performance of model trained on an English-only dataset by 5.05/6.50 F1/EM points for Hindi and 3.81/3.69 points F1/EM for Spanish on the same dataset. Notably, our approach uses a pre-trained LLM for generation with no fine-tuning (FT), utilizing just a single annotated example in ICL to generate data, providing a cost-effective development process.</li>
</ul>

<h3>Title: LoopAnimate: Loopable Salient Object Animation</h3>
<ul>
<li><strong>Authors: </strong>Fanyi Wang, Peng Liu, Haotian Hu, Dan Meng, Jingwen Su, Jinjin Xu, Yanhao Zhang, Xiaoming Ren, Zhiwang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09172">https://arxiv.org/abs/2404.09172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09172">https://arxiv.org/pdf/2404.09172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09172]] LoopAnimate: Loopable Salient Object Animation(https://arxiv.org/abs/2404.09172)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Research on diffusion model-based video generation has advanced rapidly. However, limitations in object fidelity and generation length hinder its practical applications. Additionally, specific domains like animated wallpapers require seamless looping, where the first and last frames of the video match seamlessly. To address these challenges, this paper proposes LoopAnimate, a novel method for generating videos with consistent start and end frames. To enhance object fidelity, we introduce a framework that decouples multi-level image appearance and textual semantic information. Building upon an image-to-image diffusion model, our approach incorporates both pixel-level and feature-level information from the input image, injecting image appearance and textual semantic embeddings at different positions of the diffusion model. Existing UNet-based video generation models require to input the entire videos during training to encode temporal and positional information at once. However, due to limitations in GPU memory, the number of frames is typically restricted to 16. To address this, this paper proposes a three-stage training strategy with progressively increasing frame numbers and reducing fine-tuning modules. Additionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend the capacity for encoding temporal and positional information up to 36 frames. The proposed LoopAnimate, which for the first time extends the single-pass generation length of UNet-based video generation models to 35 frames while maintaining high-quality video generation. Experiments demonstrate that LoopAnimate achieves state-of-the-art performance in both objective metrics, such as fidelity and temporal consistency, and subjective evaluation results.</li>
</ul>

<h3>Title: FaceCat: Enhancing Face Recognition Security with a Unified Generative  Model Framework</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Chen, Xiao Yang, Yinpeng Dong, Hang Su, Jianteng Peng, Zhaoxia Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09193">https://arxiv.org/abs/2404.09193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09193">https://arxiv.org/pdf/2404.09193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09193]] FaceCat: Enhancing Face Recognition Security with a Unified Generative  Model Framework(https://arxiv.org/abs/2404.09193)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face anti-spoofing (FAS) and adversarial detection (FAD) have been regarded as critical technologies to ensure the safety of face recognition systems. As a consequence of their limited practicality and generalization, some existing methods aim to devise a framework capable of concurrently detecting both threats to address the challenge. Nevertheless, these methods still encounter challenges of insufficient generalization and suboptimal robustness, potentially owing to the inherent drawback of discriminative models. Motivated by the rich structural and detailed features of face generative models, we propose FaceCat which utilizes the face generative model as a pre-trained model to improve the performance of FAS and FAD. Specifically, FaceCat elaborately designs a hierarchical fusion mechanism to capture rich face semantic features of the generative model. These features then serve as a robust foundation for a lightweight head, designed to execute FAS and FAD tasks simultaneously. As relying solely on single-modality data often leads to suboptimal performance, we further propose a novel text-guided multi-modal alignment strategy that utilizes text prompts to enrich feature representation, thereby enhancing performance. For fair evaluations, we build a comprehensive protocol with a wide range of 28 attack types to benchmark the performance. Extensive experiments validate the effectiveness of FaceCat generalizes significantly better and obtains excellent robustness against input transformations.</li>
</ul>

<h3>Title: DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation  with Generative Models and Biomedical Knowledge to Enhance Inference  Robustness</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Wang, Zeqiang Wang, Wei Wang, Qi Chen, Kaizhu Huang, Anh Nguyen, Suparna De</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09206">https://arxiv.org/abs/2404.09206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09206">https://arxiv.org/pdf/2404.09206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09206]] DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation  with Generative Models and Biomedical Knowledge to Enhance Inference  Robustness(https://arxiv.org/abs/2404.09206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Safe and reliable natural language inference is critical for extracting insights from clinical trial reports but poses challenges due to biases in large pre-trained language models. This paper presents a novel data augmentation technique to improve model robustness for biomedical natural language inference in clinical trials. By generating synthetic examples through semantic perturbations and domain-specific vocabulary replacement and adding a new task for numerical and quantitative reasoning, we introduce greater diversity and reduce shortcut learning. Our approach, combined with multi-task learning and the DeBERTa architecture, achieved significant performance gains on the NLI4CT 2024 benchmark compared to the original language models. Ablation studies validate the contribution of each augmentation method in improving robustness. Our best-performing model ranked 12th in terms of faithfulness and 8th in terms of consistency, respectively, out of the 32 participants.</li>
</ul>

<h3>Title: DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node  Feature Noise</h3>
<ul>
<li><strong>Authors: </strong>Tai Hasegawa, Sukwon Yun, Xin Liu, Yin Jun Phua, Tsuyoshi Murata</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09207">https://arxiv.org/abs/2404.09207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09207">https://arxiv.org/pdf/2404.09207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09207]] DEGNN: Dual Experts Graph Neural Network Handling Both Edge and Node  Feature Noise(https://arxiv.org/abs/2404.09207)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved notable success in various applications over graph data. However, recent research has revealed that real-world graphs often contain noise, and GNNs are susceptible to noise in the graph. To address this issue, several Graph Structure Learning (GSL) models have been introduced. While GSL models are tailored to enhance robustness against edge noise through edge reconstruction, a significant limitation surfaces: their high reliance on node features. This inherent dependence amplifies their susceptibility to noise within node features. Recognizing this vulnerability, we present DEGNN, a novel GNN model designed to adeptly mitigate noise in both edges and node features. The core idea of DEGNN is to design two separate experts: an edge expert and a node feature expert. These experts utilize self-supervised learning techniques to produce modified edges and node features. Leveraging these modified representations, DEGNN subsequently addresses downstream tasks, ensuring robustness against noise present in both edges and node features of real-world graphs. Notably, the modification process can be trained end-to-end, empowering DEGNN to adjust dynamically and achieves optimal edge and node representations for specific tasks. Comprehensive experiments demonstrate DEGNN's efficacy in managing noise, both in original real-world graphs and in graphs with synthetic noise.</li>
</ul>

<h3>Title: DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09216">https://arxiv.org/abs/2404.09216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09216">https://arxiv.org/pdf/2404.09216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09216]] DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection(https://arxiv.org/abs/2404.09216)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing open-vocabulary object detectors typically require a predefined set of categories from users, significantly confining their application scenarios. In this paper, we introduce DetCLIPv3, a high-performing detector that excels not only at both open-vocabulary object detection, but also generating hierarchical labels for detected objects. DetCLIPv3 is characterized by three core designs: 1. Versatile model architecture: we derive a robust open-set detection framework which is further empowered with generation ability via the integration of a caption head. 2. High information density data: we develop an auto-annotation pipeline leveraging visual large language model to refine captions for large-scale image-text pairs, providing rich, multi-granular object labels to enhance the training. 3. Efficient training strategy: we employ a pre-training stage with low-resolution inputs that enables the object captioner to efficiently learn a broad spectrum of visual concepts from extensive image-text paired data. This is followed by a fine-tuning stage that leverages a small number of high-resolution samples to further enhance detection performance. With these effective designs, DetCLIPv3 demonstrates superior open-vocabulary detection performance, \eg, our Swin-T backbone model achieves a notable 47.0 zero-shot fixed AP on the LVIS minival benchmark, outperforming GLIPv2, GroundingDINO, and DetCLIPv2 by 18.0/19.6/6.6 AP, respectively. DetCLIPv3 also achieves a state-of-the-art 19.7 AP in dense captioning task on VG dataset, showcasing its strong generative capability.</li>
</ul>

<h3>Title: DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation  Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xuening Yuan, Hongyu Yang, Yueming Zhao, Di Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09227">https://arxiv.org/abs/2404.09227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09227">https://arxiv.org/pdf/2404.09227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09227]] DreamScape: 3D Scene Creation via Gaussian Splatting joint Correlation  Modeling(https://arxiv.org/abs/2404.09227)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-3D creation has been propelled by integrating the potent prior of Diffusion Models from text-to-image generation into the 3D domain. Nevertheless, generating 3D scenes characterized by multiple instances and intricate arrangements remains challenging. In this study, we present DreamScape, a method for creating highly consistent 3D scenes solely from textual descriptions, leveraging the strong 3D representation capabilities of Gaussian Splatting and the complex arrangement abilities of large language models (LLMs). Our approach involves a 3D Gaussian Guide ($3{DG^2}$) for scene representation, consisting of semantic primitives (objects) and their spatial transformations and relationships derived directly from text prompts using LLMs. This compositional representation allows for local-to-global optimization of the entire scene. A progressive scale control is tailored during local object generation, ensuring that objects of different sizes and densities adapt to the scene, which addresses training instability issue arising from simple blending in the subsequent global optimization stage. To mitigate potential biases of LLM priors, we model collision relationships between objects at the global level, enhancing physical correctness and overall realism. Additionally, to generate pervasive objects like rain and snow distributed extensively across the scene, we introduce a sparse initialization and densification strategy. Experiments demonstrate that DreamScape offers high usability and controllability, enabling the generation of high-fidelity 3D scenes from only text prompts and achieving state-of-the-art performance compared to other methods.</li>
</ul>

<h3>Title: Fault Detection in Mobile Networks Using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mohamad Nabeel, Doumitrou Daniil Nimara, Tahar Zanouda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09240">https://arxiv.org/abs/2404.09240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09240">https://arxiv.org/pdf/2404.09240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09240]] Fault Detection in Mobile Networks Using Diffusion Models(https://arxiv.org/abs/2404.09240)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, anomaly</a></li>
<li><strong>Abstract: </strong>In today's hyper-connected world, ensuring the reliability of telecom networks becomes increasingly crucial. Telecom networks encompass numerous underlying and intertwined software and hardware components, each providing different functionalities. To ensure the stability of telecom networks, telecom software, and hardware vendors developed several methods to detect any aberrant behavior in telecom networks and enable instant feedback and alerts. These approaches, although powerful, struggle to generalize due to the unsteady nature of the software-intensive embedded system and the complexity and diversity of multi-standard mobile networks. In this paper, we present a system to detect anomalies in telecom networks using a generative AI model. We evaluate several strategies using diffusion models to train the model for anomaly detection using multivariate time-series data. The contributions of this paper are threefold: (i) A proposal of a framework for utilizing diffusion models for time-series anomaly detection in telecom networks, (ii) A proposal of a particular Diffusion model architecture that outperforms other state-of-the-art techniques, (iii) Experiments on a real-world dataset to demonstrate that our model effectively provides explainable results, exposing some of its limitations and suggesting future research avenues to enhance its capabilities further.</li>
</ul>

<h3>Title: RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Kyle Shih-Huang Lo, Jörg Peters, Eric Spellman</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09290">https://arxiv.org/abs/2404.09290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09290">https://arxiv.org/pdf/2404.09290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09290]] RoofDiffusion: Constructing Roofs from Severely Corrupted Point Data via  Diffusion(https://arxiv.org/abs/2404.09290)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate completion and denoising of roof height maps are crucial to reconstructing high-quality 3D buildings. Repairing sparse points can enhance low-cost sensor use and reduce UAV flight overlap. RoofDiffusion is a new end-to-end self-supervised diffusion technique for robustly completing, in particular difficult, roof height maps. RoofDiffusion leverages widely-available curated footprints and can so handle up to 99\% point sparsity and 80\% roof area occlusion (regional incompleteness). A variant, No-FP RoofDiffusion, simultaneously predicts building footprints and heights. Both quantitatively outperform state-of-the-art unguided depth completion and representative inpainting methods for Digital Elevation Models (DEM), on both a roof-specific benchmark and the BuildingNet dataset. Qualitative assessments show the effectiveness of RoofDiffusion for datasets with real-world scans including AHN3, Dales3D, and USGS 3DEP LiDAR. Tested with the leading City3D algorithm, preprocessing height maps with RoofDiffusion noticeably improves 3D building reconstruction. RoofDiffusion is complemented by a new dataset of 13k complex roof geometries, focusing on long-tail issues in remote sensing; a novel simulation of tree occlusion; and a wide variety of large-area roof cut-outs for data augmentation and benchmarking.</li>
</ul>

<h3>Title: Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora</h3>
<ul>
<li><strong>Authors: </strong>Dror K. Markus, Effi Levi, Tamir Sheafer, Shaul R. Shenhav</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09299">https://arxiv.org/abs/2404.09299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09299">https://arxiv.org/pdf/2404.09299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09299]] Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora(https://arxiv.org/abs/2404.09299)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Media Storms, dramatic outbursts of attention to a story, are central components of media dynamics and the attention landscape. Despite their significance, there has been little systematic and empirical research on this concept due to issues of measurement and operationalization. We introduce an iterative human-in-the-loop method to identify media storms in a large-scale corpus of news articles. The text is first transformed into signals of dispersion based on several textual characteristics. In each iteration, we apply unsupervised anomaly detection to these signals; each anomaly is then validated by an expert to confirm the presence of a storm, and those results are then used to tune the anomaly detection in the next iteration. We demonstrate the applicability of this method in two scenarios: first, supplementing an initial list of media storms within a specific time frame; and second, detecting media storms in new time periods. We make available a media storm dataset compiled using both scenarios. Both the method and dataset offer the basis for comprehensive empirical research into the concept of media storms, including characterizing them and predicting their outbursts and durations, in mainstream media or social media platforms.</li>
</ul>

<h3>Title: Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision  Transformers</h3>
<ul>
<li><strong>Authors: </strong>Diana-Nicoleta Grigore, Mariana-Iuliana Georgescu, Jon Alvarez Justo, Tor Johansen, Andreea Iuliana Ionescu, Radu Tudor Ionescu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09326">https://arxiv.org/abs/2404.09326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09326">https://arxiv.org/pdf/2404.09326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09326]] Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision  Transformers(https://arxiv.org/abs/2404.09326)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Few-shot knowledge distillation recently emerged as a viable approach to harness the knowledge of large-scale pre-trained models, using limited data and computational resources. In this paper, we propose a novel few-shot feature distillation approach for vision transformers. Our approach is based on two key steps. Leveraging the fact that vision transformers have a consistent depth-wise structure, we first copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students), where the intermittence factor controls the complexity of the student transformer with respect to its teacher. Next, we employ an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge into the student in a few-shot scenario, aiming to recover the information processing carried out by the skipped teacher layers. We present comprehensive experiments with supervised and self-supervised transformers as teachers, on five data sets from various domains, including natural, medical and satellite images. The empirical results confirm the superiority of our approach over competitive baselines. Moreover, the ablation results demonstrate the usefulness of each component of the proposed pipeline.</li>
</ul>

<h3>Title: Counteracting Concept Drift by Learning with Future Malware Predictions</h3>
<ul>
<li><strong>Authors: </strong>Branislav Bosansky, Lada Hospodkova, Michal Najman, Maria Rigaki, Elnaz Babayeva, Viliam Lisy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09352">https://arxiv.org/abs/2404.09352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09352">https://arxiv.org/pdf/2404.09352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09352]] Counteracting Concept Drift by Learning with Future Malware Predictions(https://arxiv.org/abs/2404.09352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The accuracy of deployed malware-detection classifiers degrades over time due to changes in data distributions and increasing discrepancies between training and testing data. This phenomenon is known as the concept drift. While the concept drift can be caused by various reasons in general, new malicious files are created by malware authors with a clear intention of avoiding detection. The existence of the intention opens a possibility for predicting such future samples. Including predicted samples in training data should consequently increase the accuracy of the classifiers on new testing data. We compare two methods for predicting future samples: (1) adversarial training and (2) generative adversarial networks (GANs). The first method explicitly seeks for adversarial examples against the classifier that are then used as a part of training data. Similarly, GANs also generate synthetic training data. We use GANs to learn changes in data distributions within different time periods of training data and then apply these changes to generate samples that could be in testing data. We compare these prediction methods on two different datasets: (1) Ember public dataset and (2) the internal dataset of files incoming to Avast. We show that while adversarial training yields more robust classifiers, this method is not a good predictor of future malware in general. This is in contrast with previously reported positive results in different domains (including natural language processing and spam detection). On the other hand, we show that GANs can be successfully used as predictors of future malware. We specifically examine malware families that exhibit significant changes in their data distributions over time and the experimental results confirm that GAN-based predictions can significantly improve the accuracy of the classifier on new, previously unseen data.</li>
</ul>

<h3>Title: RankCLIP: Ranking-Consistent Language-Image Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zhili Feng, Zenghui Ding, Yining Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09387">https://arxiv.org/abs/2404.09387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09387">https://arxiv.org/pdf/2404.09387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09387]] RankCLIP: Ranking-Consistent Language-Image Pretraining(https://arxiv.org/abs/2404.09387)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Among the ever-evolving development of vision-language models, contrastive language-image pretraining (CLIP) has set new benchmarks in many downstream tasks such as zero-shot classifications by leveraging self-supervised contrastive learning on large amounts of text-image pairs. However, its dependency on rigid one-to-one mappings overlooks the complex and often multifaceted relationships between and within texts and images. To this end, we introduce RankCLIP, a novel pretraining method that extends beyond the rigid one-to-one matching framework of CLIP and its variants. By leveraging both in-modal and cross-modal ranking consistency, RankCLIP improves the alignment process, enabling it to capture the nuanced many-to-many relationships between and within each modality. Through comprehensive experiments, we demonstrate the enhanced capability of RankCLIP to effectively improve performance across various downstream tasks, notably achieving significant gains in zero-shot classifications over state-of-the-art methods, underscoring the potential of RankCLIP in further advancing vision-language pretraining.</li>
</ul>

<h3>Title: Masked and Shuffled Blind Spot Denoising for Real-World Images</h3>
<ul>
<li><strong>Authors: </strong>Hamadi Chihaoui, Paolo Favaro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09389">https://arxiv.org/abs/2404.09389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09389">https://arxiv.org/pdf/2404.09389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09389]] Masked and Shuffled Blind Spot Denoising for Real-World Images(https://arxiv.org/abs/2404.09389)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We introduce a novel approach to single image denoising based on the Blind Spot Denoising principle, which we call MAsked and SHuffled Blind Spot Denoising (MASH). We focus on the case of correlated noise, which often plagues real images. MASH is the result of a careful analysis to determine the relationships between the level of blindness (masking) of the input and the (unknown) noise correlation. Moreover, we introduce a shuffling technique to weaken the local correlation of noise, which in turn yields an additional denoising performance improvement. We evaluate MASH via extensive experiments on real-world noisy image datasets. We demonstrate on par or better results compared to existing self-supervised denoising methods.</li>
</ul>

<h3>Title: Watermark-embedded Adversarial Examples for Copyright Protection against  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Peifei Zhu, Tsubasa Takahashi, Hirokatsu Kataoka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09401">https://arxiv.org/abs/2404.09401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09401">https://arxiv.org/pdf/2404.09401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09401]] Watermark-embedded Adversarial Examples for Copyright Protection against  Diffusion Models(https://arxiv.org/abs/2404.09401)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Models (DMs) have shown remarkable capabilities in various image-generation tasks. However, there are growing concerns that DMs could be used to imitate unauthorized creations and thus raise copyright issues. To address this issue, we propose a novel framework that embeds personal watermarks in the generation of adversarial examples. Such examples can force DMs to generate images with visible watermarks and prevent DMs from imitating unauthorized images. We construct a generator based on conditional adversarial networks and design three losses (adversarial loss, GAN loss, and perturbation loss) to generate adversarial examples that have subtle perturbation but can effectively attack DMs to prevent copyright violations. Training a generator for a personal watermark by our method only requires 5-10 samples within 2-3 minutes, and once the generator is trained, it can generate adversarial examples with that watermark significantly fast (0.2s per image). We conduct extensive experiments in various conditional image-generation scenarios. Compared to existing methods that generate images with chaotic textures, our method adds visible watermarks on the generated images, which is a more straightforward way to indicate copyright violations. We also observe that our adversarial examples exhibit good transferability across unknown generative models. Therefore, this work provides a simple yet powerful way to protect copyright from DM-based imitation.</li>
</ul>

<h3>Title: Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion  Processes</h3>
<ul>
<li><strong>Authors: </strong>Haoming Yang, Ali Hasan, Yuting Ng, Vahid Tarokh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09402">https://arxiv.org/abs/2404.09402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09402">https://arxiv.org/pdf/2404.09402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09402]] Neural McKean-Vlasov Processes: Distributional Dependence in Diffusion  Processes(https://arxiv.org/abs/2404.09402)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>McKean-Vlasov stochastic differential equations (MV-SDEs) provide a mathematical description of the behavior of an infinite number of interacting particles by imposing a dependence on the particle density. As such, we study the influence of explicitly including distributional information in the parameterization of the SDE. We propose a series of semi-parametric methods for representing MV-SDEs, and corresponding estimators for inferring parameters from data based on the properties of the MV-SDE. We analyze the characteristics of the different architectures and estimators, and consider their applicability in relevant machine learning problems. We empirically compare the performance of the different architectures and estimators on real and synthetic datasets for time series and probabilistic modeling. The results suggest that explicitly including distributional dependence in the parameterization of the SDE is effective in modeling temporal data with interaction under an exchangeability assumption while maintaining strong performance for standard It\^o-SDEs due to the richer class of probability flows associated with MV-SDEs.</li>
</ul>

<h3>Title: Human-in-the-Loop Segmentation of Multi-species Coral Imagery</h3>
<ul>
<li><strong>Authors: </strong>Scarlett Raine, Ross Marchant, Brano Kusy, Frederic Maire, Niko Suenderhauf, Tobias Fischer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09406">https://arxiv.org/abs/2404.09406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09406">https://arxiv.org/pdf/2404.09406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09406]] Human-in-the-Loop Segmentation of Multi-species Coral Imagery(https://arxiv.org/abs/2404.09406)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Broad-scale marine surveys performed by underwater vehicles significantly increase the availability of coral reef imagery, however it is costly and time-consuming for domain experts to label images. Point label propagation is an approach used to leverage existing image data labeled with sparse point labels. The resulting augmented ground truth generated is then used to train a semantic segmentation model. Here, we first demonstrate that recent advances in foundation models enable generation of multi-species coral augmented ground truth masks using denoised DINOv2 features and K-Nearest Neighbors (KNN), without the need for any pre-training or custom-designed algorithms. For extremely sparsely labeled images, we propose a labeling regime based on human-in-the-loop principles, resulting in significant improvement in annotation efficiency: If only 5 point labels per image are available, our proposed human-in-the-loop approach improves on the state-of-the-art by 17.3% for pixel accuracy and 22.6% for mIoU; and by 10.6% and 19.1% when 10 point labels per image are available. Even if the human-in-the-loop labeling regime is not used, the denoised DINOv2 features with a KNN outperforms the prior state-of-the-art by 3.5% for pixel accuracy and 5.7% for mIoU (5 grid points). We also provide a detailed analysis of how point labeling style and the quantity of points per image affects the point label propagation quality and provide general recommendations on maximizing point label efficiency.</li>
</ul>

<h3>Title: VFMM3D: Releasing the Potential of Image by Vision Foundation Model for  Monocular 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Bonan Ding, Jin Xie, Jing Nie, Jiale Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09431">https://arxiv.org/abs/2404.09431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09431">https://arxiv.org/pdf/2404.09431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09431]] VFMM3D: Releasing the Potential of Image by Vision Foundation Model for  Monocular 3D Object Detection(https://arxiv.org/abs/2404.09431)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Due to its cost-effectiveness and widespread availability, monocular 3D object detection, which relies solely on a single camera during inference, holds significant importance across various applications, including autonomous driving and robotics. Nevertheless, directly predicting the coordinates of objects in 3D space from monocular images poses challenges. Therefore, an effective solution involves transforming monocular images into LiDAR-like representations and employing a LiDAR-based 3D object detector to predict the 3D coordinates of objects. The key step in this method is accurately converting the monocular image into a reliable point cloud form. In this paper, we present VFMM3D, an innovative approach that leverages the capabilities of Vision Foundation Models (VFMs) to accurately transform single-view images into LiDAR point cloud representations. VFMM3D utilizes the Segment Anything Model (SAM) and Depth Anything Model (DAM) to generate high-quality pseudo-LiDAR data enriched with rich foreground information. Specifically, the Depth Anything Model (DAM) is employed to generate dense depth maps. Subsequently, the Segment Anything Model (SAM) is utilized to differentiate foreground and background regions by predicting instance masks. These predicted instance masks and depth maps are then combined and projected into 3D space to generate pseudo-LiDAR points. Finally, any object detectors based on point clouds can be utilized to predict the 3D coordinates of objects. Comprehensive experiments are conducted on the challenging 3D object detection dataset KITTI. Our VFMM3D establishes a new state-of-the-art performance. Additionally, experimental results demonstrate the generality of VFMM3D, showcasing its seamless integration into various LiDAR-based 3D object detectors.</li>
</ul>

<h3>Title: Exploring Text-to-Motion Generation with Human Preference</h3>
<ul>
<li><strong>Authors: </strong>Jenny Sheng, Matthieu Lin, Andrew Zhao, Kevin Pruvost, Yu-Hui Wen, Yangguang Li, Gao Huang, Yong-Jin Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09445">https://arxiv.org/abs/2404.09445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09445">https://arxiv.org/pdf/2404.09445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09445]] Exploring Text-to-Motion Generation with Human Preference(https://arxiv.org/abs/2404.09445)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents an exploration of preference learning in text-to-motion generation. We find that current improvements in text-to-motion generation still rely on datasets requiring expert labelers with motion capture systems. Instead, learning from human preference data does not require motion capture systems; a labeler with no expertise simply compares two generated motions. This is particularly efficient because evaluating the model's output is easier than gathering the motion that performs a desired task (e.g. backflip). To pioneer the exploration of this paradigm, we annotate 3,528 preference pairs generated by MotionGPT, marking the first effort to investigate various algorithms for learning from preference data. In particular, our exploration highlights important design choices when using preference data. Additionally, our experimental results show that preference learning has the potential to greatly improve current text-to-motion generative models. Our code and dataset are publicly available at https://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion to further facilitate research in this area.</li>
</ul>

<h3>Title: PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI</h3>
<ul>
<li><strong>Authors: </strong>Yandan Yang, Baoxiong Jia, Peiyuan Zhi, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09465">https://arxiv.org/abs/2404.09465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09465">https://arxiv.org/pdf/2404.09465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09465]] PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI(https://arxiv.org/abs/2404.09465)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With recent developments in Embodied Artificial Intelligence (EAI) research, there has been a growing demand for high-quality, large-scale interactive scene generation. While prior methods in scene synthesis have prioritized the naturalness and realism of the generated scenes, the physical plausibility and interactivity of scenes have been largely left unexplored. To address this disparity, we introduce PhyScene, a novel method dedicated to generating interactive 3D scenes characterized by realistic layouts, articulated objects, and rich physical interactivity tailored for embodied agents. Based on a conditional diffusion model for capturing scene layouts, we devise novel physics- and interactivity-based guidance mechanisms that integrate constraints from object collision, room layout, and object reachability. Through extensive experiments, we demonstrate that PhyScene effectively leverages these guidance functions for physically interactable scene synthesis, outperforming existing state-of-the-art scene synthesis methods by a large margin. Our findings suggest that the scenes generated by PhyScene hold considerable potential for facilitating diverse skill acquisition among agents within interactive environments, thereby catalyzing further advancements in embodied AI research. Project website: this http URL</li>
</ul>

<h3>Title: Large Language Models Can Automatically Engineer Features for Few-Shot  Tabular Learning</h3>
<ul>
<li><strong>Authors: </strong>Sungwon Han, Jinsung Yoon, Sercan O Arik, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09491">https://arxiv.org/abs/2404.09491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09491">https://arxiv.org/pdf/2404.09491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09491]] Large Language Models Can Automatically Engineer Features for Few-Shot  Tabular Learning(https://arxiv.org/abs/2404.09491)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT.</li>
</ul>

<h3>Title: Magic Clothing: Controllable Garment-Driven Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Weifeng Chen, Tao Gu, Yuhao Xu, Chengcai Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09512">https://arxiv.org/abs/2404.09512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09512">https://arxiv.org/pdf/2404.09512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09512]] Magic Clothing: Controllable Garment-Driven Image Synthesis(https://arxiv.org/abs/2404.09512)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose Magic Clothing, a latent diffusion model (LDM)-based network architecture for an unexplored garment-driven image synthesis task. Aiming at generating customized characters wearing the target garments with diverse text prompts, the image controllability is the most critical issue, i.e., to preserve the garment details and maintain faithfulness to the text prompts. To this end, we introduce a garment extractor to capture the detailed garment features, and employ self-attention fusion to incorporate them into the pretrained LDMs, ensuring that the garment details remain unchanged on the target character. Then, we leverage the joint classifier-free guidance to balance the control of garment features and text prompts over the generated results. Meanwhile, the proposed garment extractor is a plug-in module applicable to various finetuned LDMs, and it can be combined with other extensions like ControlNet and IP-Adapter to enhance the diversity and controllability of the generated characters. Furthermore, we design Matched-Points-LPIPS (MP-LPIPS), a robust metric for evaluating the consistency of the target image to the source garment. Extensive experiments demonstrate that our Magic Clothing achieves state-of-the-art results under various conditional controls for garment-driven image synthesis. Our source code is available at https://github.com/ShineChen1024/MagicClothing.</li>
</ul>

<h3>Title: Deep image learning of quantitative structure-property relationships of  cooper alloys via feature augmentation on Geodesic curve in shape space</h3>
<ul>
<li><strong>Authors: </strong>Yuexing Han, Guanxin Wan, Bing Wang, Yi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09515">https://arxiv.org/abs/2404.09515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09515">https://arxiv.org/pdf/2404.09515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09515]] Deep image learning of quantitative structure-property relationships of  cooper alloys via feature augmentation on Geodesic curve in shape space(https://arxiv.org/abs/2404.09515)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Understanding how the structure of materials affects their properties is a cornerstone of materials science and engineering. However, traditional methods have struggled to accurately describe the quantitative structure-property relationships for complex structures. In our study, we bridge this gap by leveraging machine learning to analyze images of materials' microstructures, thus offering a novel way to understand and predict the properties of materials based on their microstructures. We introduce a method known as FAGC (Feature Augmentation on Geodesic Curves), specifically demonstrated for Cu-Cr-Zr alloys. This approach utilizes machine learning to examine the shapes within images of the alloys' microstructures and predict their mechanical and electronic properties. This generative FAGC approach can effectively expand the relatively small training datasets due to the limited availability of materials images labeled with quantitative properties. The process begins with extracting features from the images using neural networks. These features are then mapped onto the Pre-shape space to construct the Geodesic curves. Along these curves, new features are generated, effectively increasing the dataset. Moreover, we design a pseudo-labeling mechanism for these newly generated features to further enhance the training dataset. Our FAGC method has shown remarkable results, significantly improving the accuracy of predicting the electronic conductivity and hardness of Cu-Cr-Zr alloys, with R-squared values of 0.978 and 0.998, respectively. These outcomes underscore the potential of FAGC to address the challenge of limited image data in materials science, providing a powerful tool for establishing detailed and quantitative relationships between complex microstructures and material properties.</li>
</ul>

<h3>Title: TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection  for Efficient Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haojun Sun, Chen Tang, Zhi Wang, Yuan Meng, Jingyan jiang, Xinzhu Ma, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09532">https://arxiv.org/abs/2404.09532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09532">https://arxiv.org/pdf/2404.09532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09532]] TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection  for Efficient Diffusion Models(https://arxiv.org/abs/2404.09532)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as preeminent contenders in the realm of generative models. Distinguished by their distinctive sequential generative processes, characterized by hundreds or even thousands of timesteps, diffusion models progressively reconstruct images from pure Gaussian noise, with each timestep necessitating full inference of the entire model. However, the substantial computational demands inherent to these models present challenges for deployment, quantization is thus widely used to lower the bit-width for reducing the storage and computing overheads. Current quantization methodologies primarily focus on model-side optimization, disregarding the temporal dimension, such as the length of the timestep sequence, thereby allowing redundant timesteps to continue consuming computational resources, leaving substantial scope for accelerating the generative process. In this paper, we introduce TMPQ-DM, which jointly optimizes timestep reduction and quantization to achieve a superior performance-efficiency trade-off, addressing both temporal and model optimization aspects. For timestep reduction, we devise a non-uniform grouping scheme tailored to the non-uniform nature of the denoising process, thereby mitigating the explosive combinations of timesteps. In terms of quantization, we adopt a fine-grained layer-wise approach to allocate varying bit-widths to different layers based on their respective contributions to the final generative performance, thus rectifying performance degradation observed in prior studies. To expedite the evaluation of fine-grained quantization, we further devise a super-network to serve as a precision solver by leveraging shared quantization results. These two design components are seamlessly integrated within our framework, enabling rapid joint exploration of the exponentially large decision space via a gradient-free evolutionary search algorithm.</li>
</ul>

<h3>Title: Text-Driven Diverse Facial Texture Generation via Progressive  Latent-Space Refinement</h3>
<ul>
<li><strong>Authors: </strong>Chi Wang, Junming Huang, Rong Zhang, Qi Wang, Haotian Yang, Haibin Huang, Chongyang Ma, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09540">https://arxiv.org/abs/2404.09540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09540">https://arxiv.org/pdf/2404.09540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09540]] Text-Driven Diverse Facial Texture Generation via Progressive  Latent-Space Refinement(https://arxiv.org/abs/2404.09540)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Automatic 3D facial texture generation has gained significant interest recently. Existing approaches may not support the traditional physically based rendering pipeline or rely on 3D data captured by Light Stage. Our key contribution is a progressive latent space refinement approach that can bootstrap from 3D Morphable Models (3DMMs)-based texture maps generated from facial images to generate high-quality and diverse PBR textures, including albedo, normal, and roughness. It starts with enhancing Generative Adversarial Networks (GANs) for text-guided and diverse texture generation. To this end, we design a self-supervised paradigm to overcome the reliance on ground truth 3D textures and train the generative model with only entangled texture maps. Besides, we foster mutual enhancement between GANs and Score Distillation Sampling (SDS). SDS boosts GANs with more generative modes, while GANs promote more efficient optimization of SDS. Furthermore, we introduce an edge-aware SDS for multi-view consistent facial structure. Experiments demonstrate that our method outperforms existing 3D texture generation methods regarding photo-realistic quality, diversity, and efficiency.</li>
</ul>

<h3>Title: AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics  Perception</h3>
<ul>
<li><strong>Authors: </strong>Yipo Huang, Xiangfei Sheng, Zhichao Yang, Quan Yuan, Zhichao Duan, Pengfei Chen, Leida Li, Weisi Lin, Guangming Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09624">https://arxiv.org/abs/2404.09624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09624">https://arxiv.org/pdf/2404.09624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09624]] AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics  Perception(https://arxiv.org/abs/2404.09624)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The highly abstract nature of image aesthetics perception (IAP) poses significant challenge for current multimodal large language models (MLLMs). The lack of human-annotated multi-modality aesthetic data further exacerbates this dilemma, resulting in MLLMs falling short of aesthetics perception capabilities. To address the above challenge, we first introduce a comprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT) dataset, which serves as the footstone for building multi-modality aesthetics foundation models. Specifically, to align MLLMs with human aesthetics perception, we construct a corpus-rich aesthetic critique database with 21,904 diverse-sourced images and 88K human natural language feedbacks, which are collected via progressive questions, ranging from coarse-grained aesthetic grades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle diverse queries, we further prompt GPT to refine the aesthetic critiques and assemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT, which consists of 409K multi-typed instructions to activate stronger aesthetic capabilities. Based on the AesMMIT database, we fine-tune the open-sourced general foundation models, achieving multi-modality Aesthetic Expert models, dubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert models deliver significantly better aesthetic perception performances than the state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision. Source data will be available at https://github.com/yipoh/AesExpert.</li>
</ul>

<h3>Title: In-Context Translation: Towards Unifying Image Recognition, Processing,  and Generation</h3>
<ul>
<li><strong>Authors: </strong>Han Xue, Qianru Sun, Li Song, Wenjun Zhang, Zhiwu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09633">https://arxiv.org/abs/2404.09633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09633">https://arxiv.org/pdf/2404.09633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09633]] In-Context Translation: Towards Unifying Image Recognition, Processing,  and Generation(https://arxiv.org/abs/2404.09633)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>We propose In-Context Translation (ICT), a general learning framework to unify visual recognition (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation (e.g., edge-to-image synthesis). Thanks to unification, ICT significantly reduces the inherent inductive bias that comes with designing models for specific tasks, and it maximizes mutual enhancement across similar tasks. However, the unification across a large number of tasks is non-trivial due to various data formats and training pipelines. To this end, ICT introduces two designs. Firstly, it standardizes input-output data of different tasks into RGB image pairs, e.g., semantic segmentation data pairs an RGB image with its segmentation mask in the same RGB format. This turns different tasks into a general translation task between two RGB images. Secondly, it standardizes the training of different tasks into a general in-context learning, where "in-context" means the input comprises an example input-output pair of the target task and a query image. The learning objective is to generate the "missing" data paired with the query. The implicit translation process is thus between the query and the generated image. In experiments, ICT unifies ten vision tasks and showcases impressive performance on their respective benchmarks. Notably, compared to its competitors, e.g., Painter and PromptDiffusion, ICT trained on only 4 RTX 3090 GPUs is shown to be more efficient and less costly in training.</li>
</ul>

<h3>Title: All-in-one simulation-based inference</h3>
<ul>
<li><strong>Authors: </strong>Manuel Gloeckler, Michael Deistler, Christian Weilbach, Frank Wood, Jakob H. Macke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09636">https://arxiv.org/abs/2404.09636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09636">https://arxiv.org/pdf/2404.09636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09636]] All-in-one simulation-based inference(https://arxiv.org/abs/2404.09636)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.</li>
</ul>

<h3>Title: Do LLMs Understand Visual Anomalies? Uncovering LLM Capabilities in  Zero-shot Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Zhu, Shaofeng Cai, Fang Deng, Junran Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09654">https://arxiv.org/abs/2404.09654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09654">https://arxiv.org/pdf/2404.09654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09654]] Do LLMs Understand Visual Anomalies? Uncovering LLM Capabilities in  Zero-shot Anomaly Detection(https://arxiv.org/abs/2404.09654)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) are markedly proficient in deriving visual representations guided by natural language. Recent explorations have utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by pairing images with textual descriptions indicative of normal and abnormal conditions, referred to as anomaly prompts. However, existing approaches depend on static anomaly prompts that are prone to cross-semantic ambiguity, and prioritize global image-level representations over crucial local pixel-level image-to-text alignment that is necessary for accurate anomaly localization. In this paper, we present ALFA, a training-free approach designed to address these challenges via a unified model. We propose a run-time prompt adaptation strategy, which first generates informative anomaly prompts to leverage the capabilities of a large language model (LLM). This strategy is enhanced by a contextual scoring mechanism for per-image anomaly prompt adaptation and cross-semantic ambiguity mitigation. We further introduce a novel fine-grained aligner to fuse local pixel-level semantics for precise anomaly localization, by projecting the image-text alignment from global to local semantic spaces. Extensive evaluations on the challenging MVTec and VisA datasets confirm ALFA's effectiveness in harnessing the language potential for zero-shot VAD, achieving significant PRO improvements of 12.1% on MVTec AD and 8.9% on VisA compared to state-of-the-art zero-shot VAD approaches.</li>
</ul>

<h3>Title: VFLGAN: Vertical Federated Learning-based Generative Adversarial Network  for Vertically Partitioned Data Publication</h3>
<ul>
<li><strong>Authors: </strong>Xun Yuan, Yang Yang, Prosanta Gope, Aryan Pasikhani, Biplab Sikdar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09722">https://arxiv.org/abs/2404.09722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09722">https://arxiv.org/pdf/2404.09722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09722]] VFLGAN: Vertical Federated Learning-based Generative Adversarial Network  for Vertically Partitioned Data Publication(https://arxiv.org/abs/2404.09722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the current artificial intelligence (AI) era, the scale and quality of the dataset play a crucial role in training a high-quality AI model. However, good data is not a free lunch and is always hard to access due to privacy regulations like the General Data Protection Regulation (GDPR). A potential solution is to release a synthetic dataset with a similar distribution to that of the private dataset. Nevertheless, in some scenarios, it has been found that the attributes needed to train an AI model belong to different parties, and they cannot share the raw data for synthetic data publication due to privacy regulations. In PETS 2023, Xue et al. proposed the first generative adversary network-based model, VertiGAN, for vertically partitioned data publication. However, after thoroughly investigating, we found that VertiGAN is less effective in preserving the correlation among the attributes of different parties. This article proposes a Vertical Federated Learning-based Generative Adversarial Network, VFLGAN, for vertically partitioned data publication to address the above issues. Our experimental results show that compared with VertiGAN, VFLGAN significantly improves the quality of synthetic data. Taking the MNIST dataset as an example, the quality of the synthetic dataset generated by VFLGAN is 3.2 times better than that generated by VertiGAN w.r.t. the Fr\'echet Distance. We also designed a more efficient and effective Gaussian mechanism for the proposed VFLGAN to provide the synthetic dataset with a differential privacy guarantee. On the other hand, differential privacy only gives the upper bound of the worst-case privacy guarantee. This article also proposes a practical auditing scheme that applies membership inference attacks to estimate privacy leakage through the synthetic dataset.</li>
</ul>

<h3>Title: Convergence Analysis of Probability Flow ODE for Score-based Generative  Models</h3>
<ul>
<li><strong>Authors: </strong>Daniel Zhengyu Huang, Jiaoyang Huang, Zhengjiang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, math.CA, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09730">https://arxiv.org/abs/2404.09730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09730">https://arxiv.org/pdf/2404.09730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09730]] Convergence Analysis of Probability Flow ODE for Score-based Generative  Models(https://arxiv.org/abs/2404.09730)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Score-based generative models have emerged as a powerful approach for sampling high-dimensional probability distributions. Despite their effectiveness, their theoretical underpinnings remain relatively underdeveloped. In this work, we study the convergence properties of deterministic samplers based on probability flow ODEs from both theoretical and numerical perspectives. Assuming access to $L^2$-accurate estimates of the score function, we prove the total variation between the target and the generated data distributions can be bounded above by $\mathcal{O}(d\sqrt{\delta})$ in the continuous time level, where $d$ denotes the data dimension and $\delta$ represents the $L^2$-score matching error. For practical implementations using a $p$-th order Runge-Kutta integrator with step size $h$, we establish error bounds of $\mathcal{O}(d(\sqrt{\delta} + (dh)^p))$ at the discrete level. Finally, we present numerical studies on problems up to $128$ dimensions to verify our theory, which indicate a better score matching error and dimension dependence.</li>
</ul>

<h3>Title: Photo-Realistic Image Restoration in the Wild with Controlled  Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09732">https://arxiv.org/abs/2404.09732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09732">https://arxiv.org/pdf/2404.09732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09732]] Photo-Realistic Image Restoration in the Wild with Controlled  Vision-Language Models(https://arxiv.org/abs/2404.09732)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Though diffusion models have been successfully applied to various image restoration (IR) tasks, their performance is sensitive to the choice of training datasets. Typically, diffusion models trained in specific datasets fail to recover images that have out-of-distribution degradations. To address this problem, this work leverages a capable vision-language model and a synthetic degradation pipeline to learn image restoration in the wild (wild IR). More specifically, all low-quality images are simulated with a synthetic degradation pipeline that contains multiple common degradations such as blur, resize, noise, and JPEG compression. Then we introduce robust training for a degradation-aware CLIP model to extract enriched image content features to assist high-quality image restoration. Our base diffusion model is the image restoration SDE (IR-SDE). Built upon it, we further present a posterior sampling strategy for fast noise-free image generation. We evaluate our model on both synthetic and real-world degradation datasets. Moreover, experiments on the unified image restoration task illustrate that the proposed posterior sampling improves image generation quality for various degradations.</li>
</ul>

<h3>Title: Equipping Diffusion Models with Differentiable Spatial Entropy for  Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Wenyi Lian, Wenjing Lian, Ziwei Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09735">https://arxiv.org/abs/2404.09735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09735">https://arxiv.org/pdf/2404.09735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09735]] Equipping Diffusion Models with Differentiable Spatial Entropy for  Low-Light Image Enhancement(https://arxiv.org/abs/2404.09735)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image restoration, which aims to recover high-quality images from their corrupted counterparts, often faces the challenge of being an ill-posed problem that allows multiple solutions for a single input. However, most deep learning based works simply employ l1 loss to train their network in a deterministic way, resulting in over-smoothed predictions with inferior perceptual quality. In this work, we propose a novel method that shifts the focus from a deterministic pixel-by-pixel comparison to a statistical perspective, emphasizing the learning of distributions rather than individual pixel values. The core idea is to introduce spatial entropy into the loss function to measure the distribution difference between predictions and targets. To make this spatial entropy differentiable, we employ kernel density estimation (KDE) to approximate the probabilities for specific intensity values of each pixel with their neighbor areas. Specifically, we equip the entropy with diffusion models and aim for superior accuracy and enhanced perceptual quality over l1 based noise matching loss. In the experiments, we evaluate the proposed method for low light enhancement on two datasets and the NTIRE challenge 2024. All these results illustrate the effectiveness of our statistic-based entropy loss. Code is available at https://github.com/shermanlian/spatial-entropy-loss.</li>
</ul>

<h3>Title: FSRT: Facial Scene Representation Transformer for Face Reenactment from  Factorized Appearance, Head-pose, and Facial Expression Features</h3>
<ul>
<li><strong>Authors: </strong>Andre Rochow, Max Schwarz, Sven Behnke</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09736">https://arxiv.org/abs/2404.09736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09736">https://arxiv.org/pdf/2404.09736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09736]] FSRT: Facial Scene Representation Transformer for Face Reenactment from  Factorized Appearance, Head-pose, and Facial Expression Features(https://arxiv.org/abs/2404.09736)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The task of face reenactment is to transfer the head motion and facial expressions from a driving video to the appearance of a source image, which may be of a different person (cross-reenactment). Most existing methods are CNN-based and estimate optical flow from the source image to the current driving frame, which is then inpainted and refined to produce the output animation. We propose a transformer-based encoder for computing a set-latent representation of the source image(s). We then predict the output color of a query pixel using a transformer-based decoder, which is conditioned with keypoints and a facial expression vector extracted from the driving frame. Latent representations of the source person are learned in a self-supervised manner that factorize their appearance, head pose, and facial expressions. Thus, they are perfectly suited for cross-reenactment. In contrast to most related work, our method naturally extends to multiple source images and can thus adapt to person-specific facial dynamics. We also propose data augmentation and regularization schemes that are necessary to prevent overfitting and support generalizability of the learned representations. We evaluated our approach in a randomized user study. The results indicate superior performance compared to the state-of-the-art in terms of motion transfer quality and temporal consistency.</li>
</ul>

<h3>Title: Can We Break Free from Strong Data Augmentations in Self-Supervised  Learning?</h3>
<ul>
<li><strong>Authors: </strong>Shruthi Gowda, Elahe Arani, Bahram Zonooz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09752">https://arxiv.org/abs/2404.09752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09752">https://arxiv.org/pdf/2404.09752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09752]] Can We Break Free from Strong Data Augmentations in Self-Supervised  Learning?(https://arxiv.org/abs/2404.09752)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has emerged as a promising solution for addressing the challenge of limited labeled data in deep neural networks (DNNs), offering scalability potential. However, the impact of design dependencies within the SSL framework remains insufficiently investigated. In this study, we comprehensively explore SSL behavior across a spectrum of augmentations, revealing their crucial role in shaping SSL model performance and learning mechanisms. Leveraging these insights, we propose a novel learning approach that integrates prior knowledge, with the aim of curtailing the need for extensive data augmentations and thereby amplifying the efficacy of learned representations. Notably, our findings underscore that SSL models imbued with prior knowledge exhibit reduced texture bias, diminished reliance on shortcuts and augmentations, and improved robustness against both natural and adversarial corruptions. These findings not only illuminate a new direction in SSL research, but also pave the way for enhancing DNN performance while concurrently alleviating the imperative for intensive data augmentation, thereby enhancing scalability and real-world problem-solving capabilities.</li>
</ul>

<h3>Title: Personalized Collaborative Fine-Tuning for On-Device Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Wagner, Dongyang Fan, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09753">https://arxiv.org/abs/2404.09753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09753">https://arxiv.org/pdf/2404.09753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09753]] Personalized Collaborative Fine-Tuning for On-Device Large Language  Models(https://arxiv.org/abs/2404.09753)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We explore on-device self-supervised collaborative fine-tuning of large language models with limited local data availability. Taking inspiration from the collaborative learning community, we introduce three distinct trust-weighted gradient aggregation schemes: weight similarity-based, prediction similarity-based and validation performance-based. To minimize communication overhead, we integrate Low-Rank Adaptation (LoRA) and only exchange LoRA weight updates. Our protocols, driven by prediction and performance metrics, surpass both FedAvg and local fine-tuning methods, which is particularly evident in realistic scenarios with more diverse local data distributions. The results underscore the effectiveness of our approach in addressing heterogeneity and scarcity within local datasets.</li>
</ul>

<h3>Title: The Devil is in the Few Shots: Iterative Visual Knowledge Completion for  Few-shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Yaohui Li, Qifeng Zhou, Haoxing Chen, Jianbing Zhang, Xinyu Dai, Hao Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09778">https://arxiv.org/abs/2404.09778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09778">https://arxiv.org/pdf/2404.09778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09778]] The Devil is in the Few Shots: Iterative Visual Knowledge Completion for  Few-shot Learning(https://arxiv.org/abs/2404.09778)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Contrastive Language-Image Pre-training (CLIP) has shown powerful zero-shot learning performance. Few-shot learning aims to further enhance the transfer capability of CLIP by giving few images in each class, aka 'few shots'. Most existing methods either implicitly learn from the few shots by incorporating learnable prompts or adapters, or explicitly embed them in a cache model for inference. However, the narrow distribution of few shots often contains incomplete class information, leading to biased visual knowledge with high risk of misclassification. To tackle this problem, recent methods propose to supplement visual knowledge by generative models or extra databases, which can be costly and time-consuming. In this paper, we propose an Iterative Visual Knowledge CompLetion (KCL) method to complement visual knowledge by properly taking advantages of unlabeled samples without access to any auxiliary or synthetic data. Specifically, KCL first measures the similarities between unlabeled samples and each category. Then, the samples with top confidence to each category is selected and collected by a designed confidence criterion. Finally, the collected samples are treated as labeled ones and added to few shots to jointly re-estimate the remaining unlabeled ones. The above procedures will be repeated for a certain number of iterations with more and more samples being collected until convergence, ensuring a progressive and robust knowledge completion process. Extensive experiments on 11 benchmark datasets demonstrate the effectiveness and efficiency of KCL as a plug-and-play module under both few-shot and zero-shot learning settings. Code is available at https://github.com/Mark-Sky/KCL.</li>
</ul>

<h3>Title: Impact of Preference Noise on the Alignment Performance of Generative  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Gao, Dana Alon, Donald Metzler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09824">https://arxiv.org/abs/2404.09824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09824">https://arxiv.org/pdf/2404.09824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09824]] Impact of Preference Noise on the Alignment Performance of Generative  Language Models(https://arxiv.org/abs/2404.09824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A key requirement in developing Generative Language Models (GLMs) is to have their values aligned with human values. Preference-based alignment is a widely used paradigm for this purpose, in which preferences over generation pairs are first elicited from human annotators or AI systems, and then fed into some alignment techniques, e.g., Direct Preference Optimization. However, a substantial percent (20 - 40%) of the preference pairs used in GLM alignment are noisy, and it remains unclear how the noise affects the alignment performance and how to mitigate its negative impact. In this paper, we propose a framework to inject desirable amounts and types of noise to the preferences, and systematically study the impact of preference noise on the alignment performance in two tasks (summarization and dialogue generation). We find that the alignment performance can be highly sensitive to the noise rates in the preference data: e.g., a 10 percentage points (pp) increase of the noise rate can lead to 30 pp drop in the alignment performance (in win rate). To mitigate the impact of noise, confidence-based data filtering shows significant benefit when certain types of noise are present. We hope our work can help the community better understand and mitigate the impact of preference noise in GLM alignment.</li>
</ul>

<h3>Title: Negation Triplet Extraction with Syntactic Dependency and Semantic  Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Shi, Deqing Yang, Jingping Liu, Yanghua Xiao, Zongyu Wang, Huimin Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09830">https://arxiv.org/abs/2404.09830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09830">https://arxiv.org/pdf/2404.09830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09830]] Negation Triplet Extraction with Syntactic Dependency and Semantic  Consistency(https://arxiv.org/abs/2404.09830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Previous works of negation understanding mainly focus on negation cue detection and scope resolution, without identifying negation subject which is also significant to the downstream tasks. In this paper, we propose a new negation triplet extraction (NTE) task which aims to extract negation subject along with negation cue and scope. To achieve NTE, we devise a novel Syntax&Semantic-Enhanced Negation Extraction model, namely SSENE, which is built based on a generative pretrained language model (PLM) {of Encoder-Decoder architecture} with a multi-task learning framework. Specifically, the given sentence's syntactic dependency tree is incorporated into the PLM's encoder to discover the correlations between the negation subject, cue and scope. Moreover, the semantic consistency between the sentence and the extracted triplet is ensured by an auxiliary task learning. Furthermore, we have constructed a high-quality Chinese dataset NegComment based on the users' reviews from the real-world platform of Meituan, upon which our evaluations show that SSENE achieves the best NTE performance compared to the baselines. Our ablation and case studies also demonstrate that incorporating the syntactic information helps the PLM's recognize the distant dependency between the subject and cue, and the auxiliary task learning is helpful to extract the negation triplets with more semantic consistency.</li>
</ul>

<h3>Title: Digging into contrastive learning for robust depth estimation with  diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Jiyuan Wang, Chunyu Lin, Lang Nie, Kang Liao, Shuwei Shao, Yao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09831">https://arxiv.org/abs/2404.09831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09831">https://arxiv.org/pdf/2404.09831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09831]] Digging into contrastive learning for robust depth estimation with  diffusion models(https://arxiv.org/abs/2404.09831)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, diffusion-based depth estimation methods have drawn widespread attention due to their elegant denoising patterns and promising performance. However, they are typically unreliable under adverse conditions prevalent in real-world scenarios, such as rainy, snowy, etc. In this paper, we propose a novel robust depth estimation method called D4RD, featuring a custom contrastive learning mode tailored for diffusion models to mitigate performance degradation in complex environments. Concretely, we integrate the strength of knowledge distillation into contrastive learning, building the `trinity' contrastive scheme. This scheme utilizes the sampled noise of the forward diffusion process as a natural reference, guiding the predicted noise in diverse scenes toward a more stable and precise optimum. Moreover, we extend noise-level trinity to encompass more generic feature and image levels, establishing a multi-level contrast to distribute the burden of robust perception across the overall network. Before addressing complex scenarios, we enhance the stability of the baseline diffusion model with three straightforward yet effective improvements, which facilitate convergence and remove depth outliers. Extensive experiments demonstrate that D4RD surpasses existing state-of-the-art solutions on synthetic corruption datasets and real-world weather conditions. The code for D4RD will be made available for further exploration and adoption.</li>
</ul>

<h3>Title: A Diffusion-based Data Generator for Training Object Recognition Models  in Ultra-Range Distance</h3>
<ul>
<li><strong>Authors: </strong>Eran Bamani, Eden Nissinman, Lisa Koenigsberg, Inbar Meir, Avishai Sintov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09846">https://arxiv.org/abs/2404.09846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09846">https://arxiv.org/pdf/2404.09846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09846]] A Diffusion-based Data Generator for Training Object Recognition Models  in Ultra-Range Distance(https://arxiv.org/abs/2404.09846)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Object recognition, commonly performed by a camera, is a fundamental requirement for robots to complete complex tasks. Some tasks require recognizing objects far from the robot's camera. A challenging example is Ultra-Range Gesture Recognition (URGR) in human-robot interaction where the user exhibits directive gestures at a distance of up to 25~m from the robot. However, training a model to recognize hardly visible objects located in ultra-range requires an exhaustive collection of a significant amount of labeled samples. The generation of synthetic training datasets is a recent solution to the lack of real-world data, while unable to properly replicate the realistic visual characteristics of distant objects in images. In this letter, we propose the Diffusion in Ultra-Range (DUR) framework based on a Diffusion model to generate labeled images of distant objects in various scenes. The DUR generator receives a desired distance and class (e.g., gesture) and outputs a corresponding synthetic image. We apply DUR to train a URGR model with directive gestures in which fine details of the gesturing hand are challenging to distinguish. DUR is compared to other types of generative models showcasing superiority both in fidelity and in recognition success rate when training a URGR model. More importantly, training a DUR model on a limited amount of real data and then using it to generate synthetic data for training a URGR model outperforms directly training the URGR model on real data. The synthetic-based URGR model is also demonstrated in gesture-based direction of a ground robot.</li>
</ul>

<h3>Title: Empowering Embodied Visual Tracking with Visual Foundation Models and  Offline RL</h3>
<ul>
<li><strong>Authors: </strong>Fangwei Zhong, Kui Wu, Hai Ci, Churan Wang, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09857">https://arxiv.org/abs/2404.09857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09857">https://arxiv.org/pdf/2404.09857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09857]] Empowering Embodied Visual Tracking with Visual Foundation Models and  Offline RL(https://arxiv.org/abs/2404.09857)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Embodied visual tracking is to follow a target object in dynamic 3D environments using an agent's egocentric vision. This is a vital and challenging skill for embodied agents. However, existing methods suffer from inefficient training and poor generalization. In this paper, we propose a novel framework that combines visual foundation models (VFM) and offline reinforcement learning (offline RL) to empower embodied visual tracking. We use a pre-trained VFM, such as ``Tracking Anything", to extract semantic segmentation masks with text prompts. We then train a recurrent policy network with offline RL, e.g., Conservative Q-Learning, to learn from the collected demonstrations without online agent-environment interactions. To further improve the robustness and generalization of the policy network, we also introduce a mask re-targeting mechanism and a multi-level data collection strategy. In this way, we can train a robust tracker within an hour on a consumer-level GPU, e.g., Nvidia RTX 3090. Such efficiency is unprecedented for RL-based visual tracking methods. We evaluate our tracker on several high-fidelity environments with challenging situations, such as distraction and occlusion. The results show that our agent outperforms state-of-the-art methods in terms of sample efficiency, robustness to distractors, and generalization to unseen scenarios and targets. We also demonstrate the transferability of the learned tracker from the virtual world to real-world scenarios.</li>
</ul>

<h3>Title: Explainable Online Unsupervised Anomaly Detection for Cyber-Physical  Systems via Causal Discovery from Time Series</h3>
<ul>
<li><strong>Authors: </strong>Daniele Meli</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09871">https://arxiv.org/abs/2404.09871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09871">https://arxiv.org/pdf/2404.09871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09871]] Explainable Online Unsupervised Anomaly Detection for Cyber-Physical  Systems via Causal Discovery from Time Series(https://arxiv.org/abs/2404.09871)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Online unsupervised detection of anomalies is crucial to guarantee the correct operation of cyber-physical systems and the safety of humans interacting with them. State-of-the-art approaches based on deep learning via neural networks achieve outstanding performance at anomaly recognition, evaluating the discrepancy between a normal model of the system (with no anomalies) and the real-time stream of sensor time series. However, large training data and time are typically required, and explainability is still a challenge to identify the root of the anomaly and implement predictive maintainance. In this paper, we use causal discovery to learn a normal causal graph of the system, and we evaluate the persistency of causal links during real-time acquisition of sensor data to promptly detect anomalies. On two benchmark anomaly detection datasets, we show that our method has higher training efficiency, outperforms the accuracy of state-of-the-art neural architectures and correctly identifies the sources of $>10$ different anomalies. The code for experimental replication is at this http URL</li>
</ul>

<h3>Title: EdgeRelight360: Text-Conditioned 360-Degree HDR Image Generation for  Real-Time On-Device Video Portrait Relighting</h3>
<ul>
<li><strong>Authors: </strong>Min-Hui Lin, Mahesh Reddy, Guillaume Berger, Michel Sarkis, Fatih Porikli, Ning Bi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09918">https://arxiv.org/abs/2404.09918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09918">https://arxiv.org/pdf/2404.09918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09918]] EdgeRelight360: Text-Conditioned 360-Degree HDR Image Generation for  Real-Time On-Device Video Portrait Relighting(https://arxiv.org/abs/2404.09918)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present EdgeRelight360, an approach for real-time video portrait relighting on mobile devices, utilizing text-conditioned generation of 360-degree high dynamic range image (HDRI) maps. Our method proposes a diffusion-based text-to-360-degree image generation in the HDR domain, taking advantage of the HDR10 standard. This technique facilitates the generation of high-quality, realistic lighting conditions from textual descriptions, offering flexibility and control in portrait video relighting task. Unlike the previous relighting frameworks, our proposed system performs video relighting directly on-device, enabling real-time inference with real 360-degree HDRI maps. This on-device processing ensures both privacy and guarantees low runtime, providing an immediate response to changes in lighting conditions or user inputs. Our approach paves the way for new possibilities in real-time video applications, including video conferencing, gaming, and augmented reality, by allowing dynamic, text-based control of lighting conditions.</li>
</ul>

<h3>Title: Evolving Interpretable Visual Classifiers with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mia Chiquier, Utkarsh Mall, Carl Vondrick</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09941">https://arxiv.org/abs/2404.09941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09941">https://arxiv.org/pdf/2404.09941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09941]] Evolving Interpretable Visual Classifiers with Large Language Models(https://arxiv.org/abs/2404.09941)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Multimodal pre-trained models, such as CLIP, are popular for zero-shot classification due to their open-vocabulary flexibility and high performance. However, vision-language models, which compute similarity scores between images and class labels, are largely black-box, with limited interpretability, risk for bias, and inability to discover new visual concepts not written down. Moreover, in practical settings, the vocabulary for class names and attributes of specialized concepts will not be known, preventing these methods from performing well on images uncommon in large-scale vision-language datasets. To address these limitations, we present a novel method that discovers interpretable yet discriminative sets of attributes for visual recognition. We introduce an evolutionary search algorithm that uses a large language model and its in-context learning abilities to iteratively mutate a concept bottleneck of attributes for classification. Our method produces state-of-the-art, interpretable fine-grained classifiers. We outperform the latest baselines by 18.4% on five fine-grained iNaturalist datasets and by 22.2% on two KikiBouba datasets, despite the baselines having access to privileged information about class names.</li>
</ul>

<h3>Title: How to build the best medical image segmentation algorithm using  foundation models: a comprehensive empirical study with Segment Anything  Model</h3>
<ul>
<li><strong>Authors: </strong>Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09957">https://arxiv.org/abs/2404.09957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09957">https://arxiv.org/pdf/2404.09957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09957]] How to build the best medical image segmentation algorithm using  foundation models: a comprehensive empirical study with Segment Anything  Model(https://arxiv.org/abs/2404.09957)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or ``best-practice'' guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at https://github.com/mazurowski-lab/finetune-SAM.</li>
</ul>

<h3>Title: Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse  Controls to Any Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09967">https://arxiv.org/abs/2404.09967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09967">https://arxiv.org/pdf/2404.09967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09967]] Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse  Controls to Any Diffusion Model(https://arxiv.org/abs/2404.09967)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>ControlNets are widely used for adding spatial control in image generation with different conditions, such as depth maps, canny edges, and human poses. However, there are several challenges when leveraging the pretrained image ControlNets for controlled video generation. First, pretrained ControlNet cannot be directly plugged into new backbone models due to the mismatch of feature spaces, and the cost of training ControlNets for new backbones is a big burden. Second, ControlNet features for different frames might not effectively handle the temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models, by adapting pretrained ControlNets (and improving temporal alignment for videos). Ctrl-Adapter provides diverse capabilities including image control, video control, video control with sparse frames, multi-condition control, compatibility with different backbones, adaptation to unseen control conditions, and video editing. In Ctrl-Adapter, we train adapter layers that fuse pretrained ControlNet features to different image/video diffusion models, while keeping the parameters of the ControlNets and the diffusion models frozen. Ctrl-Adapter consists of temporal and spatial modules so that it can effectively handle the temporal consistency of videos. We also propose latent skipping and inverse timestep sampling for robust adaptation and sparse control. Moreover, Ctrl-Adapter enables control from multiple conditions by simply taking the (weighted) average of ControlNet outputs. With diverse image/video diffusion backbones (SDXL, Hotshot-XL, I2VGen-XL, and SVD), Ctrl-Adapter matches ControlNet for image control and outperforms all baselines for video control (achieving the SOTA accuracy on the DAVIS 2017 dataset) with significantly lower computational costs (less than 10 GPU hours).</li>
</ul>

<h3>Title: Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Nithin Gopalakrishnan Nair, Jeya Maria Jose Valanarasu, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09976">https://arxiv.org/abs/2404.09976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09976">https://arxiv.org/pdf/2404.09976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09976]] Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers(https://arxiv.org/abs/2404.09976)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recently, diffusion transformers have gained wide attention with its excellent performance in text-to-image and text-to-vidoe models, emphasizing the need for transformers as backbone for diffusion models. Transformer-based models have shown better generalization capability compared to CNN-based models for general vision tasks. However, much less has been explored in the existing literature regarding the capabilities of transformer-based diffusion backbones and expanding their generative prowess to other datasets. This paper focuses on enabling a single pre-trained diffusion transformer model to scale across multiple datasets swiftly, allowing for the completion of diverse generative tasks using just one model. To this end, we propose DiffScaler, an efficient scaling strategy for diffusion models where we train a minimal amount of parameters to adapt to different tasks. In particular, we learn task-specific transformations at each layer by incorporating the ability to utilize the learned subspaces of the pre-trained model, as well as the ability to learn additional task-specific subspaces, which may be absent in the pre-training dataset. As these parameters are independent, a single diffusion model with these task-specific parameters can be used to perform multiple tasks simultaneously. Moreover, we find that transformer-based diffusion models significantly outperform CNN-based diffusion models methods while performing fine-tuning over smaller datasets. We perform experiments on four unconditional image generation datasets. We show that using our proposed method, a single pre-trained model can scale up to perform these conditional and unconditional tasks, respectively, with minimal parameter tuning while performing as close as fine-tuning an entire diffusion model for that particular task.</li>
</ul>

<h3>Title: MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Nithin Gopalakrishnan Nair, Jeya Maria Jose Valanarasu, Vishal M Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09977">https://arxiv.org/abs/2404.09977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09977">https://arxiv.org/pdf/2404.09977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09977]] MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion  Models(https://arxiv.org/abs/2404.09977)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large diffusion-based Text-to-Image (T2I) models have shown impressive generative powers for text-to-image generation as well as spatially conditioned image generation. For most applications, we can train the model end-toend with paired data to obtain photorealistic generation quality. However, to add an additional task, one often needs to retrain the model from scratch using paired data across all modalities to retain good generation performance. In this paper, we tackle this issue and propose a novel strategy to scale a generative model across new tasks with minimal compute. During our experiments, we discovered that the variance maps of intermediate feature maps of diffusion models capture the intensity of conditioning. Utilizing this prior information, we propose MaxFusion, an efficient strategy to scale up text-to-image generation models to accommodate new modality conditions. Specifically, we combine aligned features of multiple models, hence bringing a compositional effect. Our fusion strategy can be integrated into off-the-shelf models to enhance their generative prowess.</li>
</ul>

<h3>Title: Memory Sharing for Large Language Model based Agents</h3>
<ul>
<li><strong>Authors: </strong>Hang Gao, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09982">https://arxiv.org/abs/2404.09982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09982">https://arxiv.org/pdf/2404.09982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09982]] Memory Sharing for Large Language Model based Agents(https://arxiv.org/abs/2404.09982)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In the realm of artificial intelligence, the adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning for fixed-answer tasks such as common sense questions and yes/no queries. However, the application of In-context Learning to open-ended challenges, such as poetry creation, reveals substantial limitations due to the comprehensiveness of the provided examples and agent's ability to understand the content expressed in the problem, leading to outputs that often diverge significantly from expected results. Addressing this gap, our study introduces the Memory-Sharing (MS) framework for LLM multi-agents, which utilizes a real-time memory storage and retrieval system to enhance the In-context Learning process. Each "memory" within this system captures both the posed query and the corresponding real-time response from an LLM-based agent, aggregating these memories from a broad spectrum of similar agents to enrich the memory pool shared by all agents. This framework not only aids agents in identifying the most relevant examples for specific tasks but also evaluates the potential utility of their memories for future applications by other agents. Empirical validation across three distinct domains involving specialized functions of agents demonstrates that the MS framework significantly improve the agent's performance regrading the open-ended questions. Furthermore, we also discuss what type of memory pool and what retrieval strategy in MS can better help agents, offering a future develop direction of MS. The code and data are available at: https://github.com/GHupppp/MemorySharingLLM</li>
</ul>

<h3>Title: in2IN: Leveraging individual Information to Generate Human INteractions</h3>
<ul>
<li><strong>Authors: </strong>Pablo Ruiz Ponce, German Barquero, Cristina Palmero, Sergio Escalera, Jose Garcia-Rodriguez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09988">https://arxiv.org/abs/2404.09988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09988">https://arxiv.org/pdf/2404.09988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09988]] in2IN: Leveraging individual Information to Generate Human INteractions(https://arxiv.org/abs/2404.09988)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating human-human motion interactions conditioned on textual descriptions is a very useful application in many areas such as robotics, gaming, animation, and the metaverse. Alongside this utility also comes a great difficulty in modeling the highly dimensional inter-personal dynamics. In addition, properly capturing the intra-personal diversity of interactions has a lot of challenges. Current methods generate interactions with limited diversity of intra-person dynamics due to the limitations of the available datasets and conditioning strategies. For this, we introduce in2IN, a novel diffusion model for human-human motion generation which is conditioned not only on the textual description of the overall interaction but also on the individual descriptions of the actions performed by each person involved in the interaction. To train this model, we use a large language model to extend the InterHuman dataset with individual descriptions. As a result, in2IN achieves state-of-the-art performance in the InterHuman dataset. Furthermore, in order to increase the intra-personal diversity on the existing interaction datasets, we propose DualMDM, a model composition technique that combines the motions generated with in2IN and the motions generated by a single-person motion prior pre-trained on HumanML3D. As a result, DualMDM generates motions with higher individual diversity and improves control over the intra-person dynamics while maintaining inter-personal coherence.</li>
</ul>

<h3>Title: HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, Cihang Xie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09990">https://arxiv.org/abs/2404.09990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09990">https://arxiv.org/pdf/2404.09990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09990]] HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing(https://arxiv.org/abs/2404.09990)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This study introduces HQ-Edit, a high-quality instruction-based image editing dataset with around 200,000 edits. Unlike prior approaches relying on attribute guidance or human feedback on building datasets, we devise a scalable data collection pipeline leveraging advanced foundation models, namely GPT-4V and DALL-E 3. To ensure its high quality, diverse examples are first collected online, expanded, and then used to create high-quality diptychs featuring input and output images with detailed text prompts, followed by precise alignment ensured through post-processing. In addition, we propose two evaluation metrics, Alignment and Coherence, to quantitatively assess the quality of image edit pairs using GPT-4V. HQ-Edits high-resolution images, rich in detail and accompanied by comprehensive editing prompts, substantially enhance the capabilities of existing image editing models. For example, an HQ-Edit finetuned InstructPix2Pix can attain state-of-the-art image editing performance, even surpassing those models fine-tuned with human-annotated data. The project page is https://thefllood.github.io/HQEdit_web.</li>
</ul>

<h3>Title: Taming Latent Diffusion Model for Neural Radiance Field Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Chieh Hubert Lin, Changil Kim, Jia-Bin Huang, Qinbo Li, Chih-Yao Ma, Johannes Kopf, Ming-Hsuan Yang, Hung-Yu Tseng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.09995">https://arxiv.org/abs/2404.09995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.09995">https://arxiv.org/pdf/2404.09995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.09995]] Taming Latent Diffusion Model for Neural Radiance Field Inpainting(https://arxiv.org/abs/2404.09995)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Neural Radiance Field (NeRF) is a representation for 3D reconstruction from multi-view images. Despite some recent work showing preliminary success in editing a reconstructed NeRF with diffusion prior, they remain struggling to synthesize reasonable geometry in completely uncovered regions. One major reason is the high diversity of synthetic contents from the diffusion model, which hinders the radiance field from converging to a crisp and deterministic geometry. Moreover, applying latent diffusion models on real data often yields a textural shift incoherent to the image condition due to auto-encoding errors. These two problems are further reinforced with the use of pixel-distance losses. To address these issues, we propose tempering the diffusion model's stochasticity with per-scene customization and mitigating the textural shift with masked adversarial training. During the analyses, we also found the commonly used pixel and perceptual losses are harmful in the NeRF inpainting task. Through rigorous experiments, our framework yields state-of-the-art NeRF inpainting results on various real-world scenes. Project page: https://hubert0527.github.io/MALD-NeRF</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
