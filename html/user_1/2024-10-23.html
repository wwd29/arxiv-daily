<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-23</h1>
<h3>Title: Synthetic Data Generation in Cybersecurity: A Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Dure Adan Ammara, Jianguo Ding, Kurt Tutschku</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16326">https://arxiv.org/abs/2410.16326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16326">https://arxiv.org/pdf/2410.16326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16326]] Synthetic Data Generation in Cybersecurity: A Comparative Analysis(https://arxiv.org/abs/2410.16326)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic data generation faces significant challenges in accurately replicating real data, particularly with tabular data, where achieving high fidelity and utility is critical. While numerous methods have been developed, the most effective approach for creating high-quality synthetic data for network traffic security remains to be seen. This study conducts a comprehensive comparative analysis of non-AI, conventional AI, and generative AI techniques for synthetic tabular data generation using two widely recognized cybersecurity datasets: NSL-KDD and CICIDS-2017. Particular emphasis was placed on prominent GAN models for tabular data generation, including CTGAN, CopulaGAN, GANBLR++, and CastGAN. The results indicate that GAN-based methods, particularly CTGAN and CopulaGAN, outperform non-AI and conventional AI approaches in terms of fidelity and utility. To the best of our knowledge, this research contributes to the field by offering the first comparative evaluation of these methods specifically for cybersecurity network traffic data, filling a critical gap in the literature. It also introduces mutual information for feature selection, further enhancing the quality of the generated synthetic data. These findings provide valuable guidance for researchers seeking the most suitable synthetic data generation method in cybersecurity applications.</li>
</ul>

<h3>Title: Exploring how deep learning decodes anomalous diffusion via Grad-CAM</h3>
<ul>
<li><strong>Authors: </strong>Jaeyong Bae, Yongjoo Baek, Hawoong Jeong</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16345">https://arxiv.org/abs/2410.16345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16345">https://arxiv.org/pdf/2410.16345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16345]] Exploring how deep learning decodes anomalous diffusion via Grad-CAM(https://arxiv.org/abs/2410.16345)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While deep learning has been successfully applied to the data-driven classification of anomalous diffusion mechanisms, how the algorithm achieves the feat still remains a mystery. In this study, we use a well-known technique aimed at achieving explainable AI, namely the Gradient-weighted Class Activation Map (Grad-CAM), to investigate how deep learning (implemented by ResNets) recognizes the distinctive features of a particular anomalous diffusion model from the raw trajectory data. Our results show that Grad-CAM reveals the portions of the trajectory that hold crucial information about the underlying mechanism of anomalous diffusion, which can be utilized to enhance the robustness of the trained classifier against the measurement noise. Moreover, we observe that deep learning distills unique statistical characteristics of different diffusion mechanisms at various spatiotemporal scales, with larger-scale (smaller-scale) features identified at higher (lower) layers.</li>
</ul>

<h3>Title: Large Language Models in Computer Science Education: A Systematic Literature Review</h3>
<ul>
<li><strong>Authors: </strong>Nishat Raihan, Mohammed Latif Siddiq, Joanna C.S. Santos, Marcos Zampieri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16349">https://arxiv.org/abs/2410.16349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16349">https://arxiv.org/pdf/2410.16349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16349]] Large Language Models in Computer Science Education: A Systematic Literature Review(https://arxiv.org/abs/2410.16349)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are becoming increasingly better at a wide range of Natural Language Processing tasks (NLP), such as text generation and understanding. Recently, these models have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). Foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances in various NL and PL tasks. Additionally, several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications. Both foundational and fine-tuned models are increasingly used in education, helping students write, debug, and understand code. We present a comprehensive systematic literature review to examine the impact of LLMs in computer science and computer engineering education. We analyze their effectiveness in enhancing the learning experience, supporting personalized education, and aiding educators in curriculum development. We address five research questions to uncover insights into how LLMs contribute to educational outcomes, identify challenges, and suggest directions for future research.</li>
</ul>

<h3>Title: Joker: Conditional 3D Head Synthesis with Extreme Facial Expressions</h3>
<ul>
<li><strong>Authors: </strong>Malte Prinzler, Egor Zakharov, Vanessa Sklyarova, Berna Kabadayi, Justus Thies</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16395">https://arxiv.org/abs/2410.16395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16395">https://arxiv.org/pdf/2410.16395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16395]] Joker: Conditional 3D Head Synthesis with Extreme Facial Expressions(https://arxiv.org/abs/2410.16395)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Joker, a new method for the conditional synthesis of 3D human heads with extreme expressions. Given a single reference image of a person, we synthesize a volumetric human head with the reference identity and a new expression. We offer control over the expression via a 3D morphable model (3DMM) and textual inputs. This multi-modal conditioning signal is essential since 3DMMs alone fail to define subtle emotional changes and extreme expressions, including those involving the mouth cavity and tongue articulation. Our method is built upon a 2D diffusion-based prior that generalizes well to out-of-domain samples, such as sculptures, heavy makeup, and paintings while achieving high levels of expressiveness. To improve view consistency, we propose a new 3D distillation technique that converts predictions of our 2D prior into a neural radiance field (NeRF). Both the 2D prior and our distillation technique produce state-of-the-art results, which are confirmed by our extensive evaluations. Also, to the best of our knowledge, our method is the first to achieve view-consistent extreme tongue articulation.</li>
</ul>

<h3>Title: On conditional diffusion models for PDE simulations</h3>
<ul>
<li><strong>Authors: </strong>Aliaksandra Shysheya, Cristiana Diaconu, Federico Bergamin, Paris Perdikaris, José Miguel Hernández-Lobato, Richard E. Turner, Emile Mathieu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16415">https://arxiv.org/abs/2410.16415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16415">https://arxiv.org/pdf/2410.16415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16415]] On conditional diffusion models for PDE simulations(https://arxiv.org/abs/2410.16415)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Modelling partial differential equations (PDEs) is of crucial importance in science and engineering, and it includes tasks ranging from forecasting to inverse problems, such as data assimilation. However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation. Recently, diffusion models have emerged as a powerful tool for conditional generation, being able to flexibly incorporate observations without retraining. In this work, we perform a comparative study of score-based diffusion models for forecasting and assimilation of sparse observations. In particular, we focus on diffusion models that are either trained in a conditional manner, or conditioned after unconditional training. We address the shortcomings of existing models by proposing 1) an autoregressive sampling approach that significantly improves performance in forecasting, 2) a new training strategy for conditional score-based models that achieves stable performance over a range of history lengths, and 3) a hybrid model which employs flexible pre-training conditioning on initial conditions and flexible post-training conditioning to handle data assimilation. We empirically show that these modifications are crucial for successfully tackling the combination of forecasting and data assimilation, a task commonly encountered in real-world scenarios.</li>
</ul>

<h3>Title: AttentionPainter: An Efficient and Adaptive Stroke Predictor for Scene Painting</h3>
<ul>
<li><strong>Authors: </strong>Yizhe Tang, Yue Wang, Teng Hu, Ran Yi, Xin Tan, Lizhuang Ma, Yu-Kun Lai, Paul L. Rosin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16418">https://arxiv.org/abs/2410.16418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16418">https://arxiv.org/pdf/2410.16418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16418]] AttentionPainter: An Efficient and Adaptive Stroke Predictor for Scene Painting(https://arxiv.org/abs/2410.16418)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Stroke-based Rendering (SBR) aims to decompose an input image into a sequence of parameterized strokes, which can be rendered into a painting that resembles the input image. Recently, Neural Painting methods that utilize deep learning and reinforcement learning models to predict the stroke sequences have been developed, but suffer from longer inference time or unstable training. To address these issues, we propose AttentionPainter, an efficient and adaptive model for single-step neural painting. First, we propose a novel scalable stroke predictor, which predicts a large number of stroke parameters within a single forward process, instead of the iterative prediction of previous Reinforcement Learning or auto-regressive methods, which makes AttentionPainter faster than previous neural painting methods. To further increase the training efficiency, we propose a Fast Stroke Stacking algorithm, which brings 13 times acceleration for training. Moreover, we propose Stroke-density Loss, which encourages the model to use small strokes for detailed information, to help improve the reconstruction quality. Finally, we propose a new stroke diffusion model for both conditional and unconditional stroke-based generation, which denoises in the stroke parameter space and facilitates stroke-based inpainting and editing applications helpful for human artists design. Extensive experiments show that AttentionPainter outperforms the state-of-the-art neural painting methods.</li>
</ul>

<h3>Title: Promoting cross-modal representations to improve multimodal foundation models for physiological signals</h3>
<ul>
<li><strong>Authors: </strong>Ching Fang, Christopher Sandino, Behrooz Mahasseni, Juri Minxha, Hadi Pouransari, Erdrin Azemi, Ali Moin, Ellen Zippi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16424">https://arxiv.org/abs/2410.16424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16424">https://arxiv.org/pdf/2410.16424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16424]] Promoting cross-modal representations to improve multimodal foundation models for physiological signals(https://arxiv.org/abs/2410.16424)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Many healthcare applications are inherently multimodal, involving several physiological signals. As sensors for these signals become more common, improving machine learning methods for multimodal healthcare data is crucial. Pretraining foundation models is a promising avenue for success. However, methods for developing foundation models in healthcare are still in early exploration and it is unclear which pretraining strategies are most effective given the diversity of physiological signals. This is partly due to challenges in multimodal health data: obtaining data across many patients is difficult and costly, there is a lot of inter-subject variability, and modalities are often heterogeneously informative across downstream tasks. Here, we explore these challenges in the PhysioNet 2018 dataset. We use a masked autoencoding objective to pretrain a multimodal model. We show that the model learns representations that can be linearly probed for a diverse set of downstream tasks. We hypothesize that cross-modal reconstruction objectives are important for successful multimodal training, as they encourage the model to integrate information across modalities. We demonstrate that modality dropout in the input space improves performance across downstream tasks. We also find that late-fusion models pretrained with contrastive learning objectives are less effective across multiple tasks. Finally, we analyze the model's representations, showing that attention weights become more cross-modal and temporally aligned with our pretraining strategy. The learned embeddings also become more distributed in terms of the modalities encoded by each unit. Overall, our work demonstrates the utility of multimodal foundation models with health data, even across diverse physiological data sources. We further argue that explicit methods for inducing cross-modality may enhance multimodal pretraining strategies.</li>
</ul>

<h3>Title: HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality</h3>
<ul>
<li><strong>Authors: </strong>Zhiming Hu, Guanhua Zhang, Zheming Yin, Daniel Haeufle, Syn Schmitt, Andreas Bulling</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16430">https://arxiv.org/abs/2410.16430</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16430">https://arxiv.org/pdf/2410.16430</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16430]] HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality(https://arxiv.org/abs/2410.16430)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Human hand and head movements are the most pervasive input modalities in extended reality (XR) and are significant for a wide range of applications. However, prior works on hand and head modelling in XR only explored a single modality or focused on specific applications. We present HaHeAE - a novel self-supervised method for learning generalisable joint representations of hand and head movements in XR. At the core of our method is an autoencoder (AE) that uses a graph convolutional network-based semantic encoder and a diffusion-based stochastic encoder to learn the joint semantic and stochastic representations of hand-head movements. It also features a diffusion-based decoder to reconstruct the original signals. Through extensive evaluations on three public XR datasets, we show that our method 1) significantly outperforms commonly used self-supervised methods by up to 74.0% in terms of reconstruction quality and is generalisable across users, activities, and XR environments, 2) enables new applications, including interpretable hand-head cluster identification and variable hand-head movement generation, and 3) can serve as an effective feature extractor for downstream tasks. Together, these results demonstrate the effectiveness of our method and underline the potential of self-supervised methods for jointly modelling hand-head behaviours in extended reality.</li>
</ul>

<h3>Title: Improving Neuron-level Interpretability with White-box Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Bai, Yi Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16443">https://arxiv.org/abs/2410.16443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16443">https://arxiv.org/pdf/2410.16443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16443]] Improving Neuron-level Interpretability with White-box Language Models(https://arxiv.org/abs/2410.16443)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Neurons in auto-regressive language models like GPT-2 can be interpreted by analyzing their activation patterns. Recent studies have shown that techniques such as dictionary learning, a form of post-hoc sparse coding, enhance this neuron-level interpretability. In our research, we are driven by the goal to fundamentally improve neural network interpretability by embedding sparse coding directly within the model architecture, rather than applying it as an afterthought. In our study, we introduce a white-box transformer-like architecture named Coding RAte TransformEr (CRATE), explicitly engineered to capture sparse, low-dimensional structures within data distributions. Our comprehensive experiments showcase significant improvements (up to 103% relative improvement) in neuron-level interpretability across a variety of evaluation metrics. Detailed investigations confirm that this enhanced interpretability is steady across different layers irrespective of the model size, underlining CRATE's robust performance in enhancing neural network interpretability. Further analysis shows that CRATE's increased interpretability comes from its enhanced ability to consistently and distinctively activate on relevant tokens. These findings point towards a promising direction for creating white-box foundation models that excel in neuron-level interpretation.</li>
</ul>

<h3>Title: In Search of the Successful Interpolation: On the Role of Sharpness in CLIP Generalization</h3>
<ul>
<li><strong>Authors: </strong>Alireza Abdollahpoorrostam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16476">https://arxiv.org/abs/2410.16476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16476">https://arxiv.org/pdf/2410.16476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16476]] In Search of the Successful Interpolation: On the Role of Sharpness in CLIP Generalization(https://arxiv.org/abs/2410.16476)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>\textit{Zero-shot} models like CLIP are often fine-tuned on a target dataset to improve its accuracy further, but this can compromise out-of-distribution (OOD) robustness. Robust Fine-Tuning (\texttt{RFT} )~\citep{wortsman2021robust}, which interpolates between the \textit{zero-shot} and \textit{fine-tuned} models, has been proposed to address this issue. However, understanding when \texttt{RFT} actually improves OOD error remains limited. In this work, we empirically investigate the robustness of \texttt{RFT} in CLIP models, with a focus on the \textit{sharpness} of the CLIP model during interpolation. First, we demonstrate that while sharpness may not serve as a reliable indicator for predicting the generalization of modern architectures like CLIP on OOD data, this challenges the conventional belief in the generalization benefits of flat minima in foundation models. However, by examining the role of the \textit{straggler layer} phenomenon, we show that, unlike overall sharpness, the \textit{layer-wise} sharpness of \textit{straggler} layers can reliably capture the generalization performance of interpolated CLIP models on OOD data. Our extensive experiments reveal that \textit{layer-wise} sharpness correlates with generalization in OOD accuracy for \texttt{RFT}. Furthermore, we demonstrate that by inducing sparsity in the \textit{straggler} layers, we can mitigate the \textit{failure mode} phenomenon in \texttt{RFT}. To the best of our knowledge, this is the first work to study the role of sharpness in the \textit{success} of interpolation in the weight space of CLIP foundation models. Our code is available at \url{this https URL}.</li>
</ul>

<h3>Title: LLM-TS Integrator: Integrating LLM for Enhanced Time Series Modeling</h3>
<ul>
<li><strong>Authors: </strong>Can Chen, Gabriel Oliveira, Hossein Sharifi Noghabi, Tristan Sylvain</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16489">https://arxiv.org/abs/2410.16489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16489">https://arxiv.org/pdf/2410.16489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16489]] LLM-TS Integrator: Integrating LLM for Enhanced Time Series Modeling(https://arxiv.org/abs/2410.16489)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series~(TS) modeling is essential in dynamic systems like weather prediction and anomaly detection. Recent studies utilize Large Language Models (LLMs) for TS modeling, leveraging their powerful pattern recognition capabilities. These methods primarily position LLMs as the predictive backbone, often omitting the mathematical modeling within traditional TS models, such as periodicity. However, disregarding the potential of LLMs also overlooks their pattern recognition capabilities. To address this gap, we introduce \textit{LLM-TS Integrator}, a novel framework that effectively integrates the capabilities of LLMs into traditional TS modeling. Central to this integration is our \textit{mutual information} module. The core of this \textit{mutual information} module is a traditional TS model enhanced with LLM-derived insights for improved predictive abilities. This enhancement is achieved by maximizing the mutual information between traditional model's TS representations and LLM's textual representation counterparts, bridging the two modalities. Moreover, we recognize that samples vary in importance for two losses: traditional prediction and mutual information maximization. To address this variability, we introduce the \textit{sample reweighting} module to improve information utilization. This module assigns dual weights to each sample: one for prediction loss and another for mutual information loss, dynamically optimizing these weights via bi-level optimization. Our method achieves state-of-the-art or comparable performance across five mainstream TS tasks, including short-term and long-term forecasting, imputation, classification, and anomaly detection.</li>
</ul>

<h3>Title: SINGAPO: Single Image Controlled Generation of Articulated Parts in Object</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Liu, Denys Iliash, Angel X. Chang, Manolis Savva, Ali Mahdavi-Amiri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16499">https://arxiv.org/abs/2410.16499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16499">https://arxiv.org/pdf/2410.16499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16499]] SINGAPO: Single Image Controlled Generation of Articulated Parts in Object(https://arxiv.org/abs/2410.16499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We address the challenge of creating 3D assets for household articulated objects from a single image. Prior work on articulated object creation either requires multi-view multi-state input, or only allows coarse control over the generation process. These limitations hinder the scalability and practicality for articulated object modeling. In this work, we propose a method to generate articulated objects from a single image. Observing the object in resting state from an arbitrary view, our method generates an articulated object that is visually consistent with the input image. To capture the ambiguity in part shape and motion posed by a single view of the object, we design a diffusion model that learns the plausible variations of objects in terms of geometry and kinematics. To tackle the complexity of generating structured data with attributes in multiple domains, we design a pipeline that produces articulated objects from high-level structure to geometric details in a coarse-to-fine manner, where we use a part connectivity graph and part abstraction as proxies. Our experiments show that our method outperforms the state-of-the-art in articulated object creation by a large margin in terms of the generated object realism, resemblance to the input image, and reconstruction quality.</li>
</ul>

<h3>Title: TIPS: Text-Image Pretraining with Spatial Awareness</h3>
<ul>
<li><strong>Authors: </strong>Kevis-Kokitsi Maninis, Kaifeng Chen, Soham Ghosh, Arjun Karpur, Koert Chen, Ye Xia, Bingyi Cao, Daniel Salz, Guangxing Han, Jan Dlabal, Dan Gnanapragasam, Mojtaba Seyedhosseini, Howard Zhou, Andre Araujo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16512">https://arxiv.org/abs/2410.16512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16512">https://arxiv.org/pdf/2410.16512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16512]] TIPS: Text-Image Pretraining with Spatial Awareness(https://arxiv.org/abs/2410.16512)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>While image-text representation learning has become very popular in recent years, existing models tend to lack spatial awareness and have limited direct applicability for dense understanding tasks. For this reason, self-supervised image-only pretraining is still the go-to method for many dense vision applications (e.g. depth estimation, semantic segmentation), despite the lack of explicit supervisory signals. In this paper, we close this gap between image-text and self-supervised learning, by proposing a novel general-purpose image-text model, which can be effectively used off-the-shelf for dense and global vision tasks. Our method, which we refer to as Text-Image Pretraining with Spatial awareness (TIPS), leverages two simple and effective insights. First, on textual supervision: we reveal that replacing noisy web image captions by synthetically generated textual descriptions boosts dense understanding performance significantly, due to a much richer signal for learning spatially aware representations. We propose an adapted training method that combines noisy and synthetic captions, resulting in improvements across both dense and global understanding tasks. Second, on the learning technique: we propose to combine contrastive image-text learning with self-supervised masked image modeling, to encourage spatial coherence, unlocking substantial enhancements for downstream applications. Building on these two ideas, we scale our model using the transformer architecture, trained on a curated set of public images. Our experiments are conducted on 8 tasks involving 16 datasets in total, demonstrating strong off-the-shelf performance on both dense and global understanding, for several image-only and image-text tasks.</li>
</ul>

<h3>Title: Bayesian scaling laws for in-context learning</h3>
<ul>
<li><strong>Authors: </strong>Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah D. Goodman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.FL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16531">https://arxiv.org/abs/2410.16531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16531">https://arxiv.org/pdf/2410.16531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16531]] Bayesian scaling laws for in-context learning(https://arxiv.org/abs/2410.16531)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is a powerful technique for getting language models to perform complex tasks with no training updates. Prior work has established strong correlations between the number of in-context examples provided and the accuracy of the model's predictions. In this paper, we seek to explain this correlation by showing that ICL approximates a Bayesian learner. This perspective gives rise to a family of novel Bayesian scaling laws for ICL. In experiments with \mbox{GPT-2} models of different sizes, our scaling laws exceed or match existing scaling laws in accuracy while also offering interpretable terms for task priors, learning efficiency, and per-example probabilities. To illustrate the analytic power that such interpretable scaling laws provide, we report on controlled synthetic dataset experiments designed to inform real-world studies of safety alignment. In our experimental protocol, we use SFT to suppress an unwanted existing model capability and then use ICL to try to bring that capability back (many-shot jailbreaking). We then experiment on real-world instruction-tuned LLMs using capabilities benchmarks as well as a new many-shot jailbreaking dataset. In all cases, Bayesian scaling laws accurately predict the conditions under which ICL will cause the suppressed behavior to reemerge, which sheds light on the ineffectiveness of post-training at increasing LLM safety.</li>
</ul>

<h3>Title: A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration</h3>
<ul>
<li><strong>Authors: </strong>Yingqian Cui, Pengfei He, Xianfeng Tang, Qi He, Chen Luo, Jiliang Tang, Yue Xing</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16540">https://arxiv.org/abs/2410.16540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16540">https://arxiv.org/pdf/2410.16540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16540]] A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration(https://arxiv.org/abs/2410.16540)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach.</li>
</ul>

<h3>Title: Spatio-temporal Multivariate Cluster Evolution Analysis for Detecting and Tracking Climate Impacts</h3>
<ul>
<li><strong>Authors: </strong>Warren L. Davis IV, Max Carlson, Irina Tezaur, Diana Bull, Kara Peterson, Laura Swiler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16544">https://arxiv.org/abs/2410.16544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16544">https://arxiv.org/pdf/2410.16544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16544]] Spatio-temporal Multivariate Cluster Evolution Analysis for Detecting and Tracking Climate Impacts(https://arxiv.org/abs/2410.16544)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Recent years have seen a growing concern about climate change and its impacts. While Earth System Models (ESMs) can be invaluable tools for studying the impacts of climate change, the complex coupling processes encoded in ESMs and the large amounts of data produced by these models, together with the high internal variability of the Earth system, can obscure important source-to-impact relationships. This paper presents a novel and efficient unsupervised data-driven approach for detecting statistically-significant impacts and tracing spatio-temporal source-impact pathways in the climate through a unique combination of ideas from anomaly detection, clustering and Natural Language Processing (NLP). Using as an exemplar the 1991 eruption of Mount Pinatubo in the Philippines, we demonstrate that the proposed approach is capable of detecting known post-eruption impacts/events. We additionally describe a methodology for extracting meaningful sequences of post-eruption impacts/events by using NLP to efficiently mine frequent multivariate cluster evolutions, which can be used to confirm or discover the chain of physical processes between a climate source and its impact(s).</li>
</ul>

<h3>Title: PlaneSAM: Multimodal Plane Instance Segmentation Using the Segment Anything Model</h3>
<ul>
<li><strong>Authors: </strong>Zhongchen Deng, Zhechen Yang, Chi Chen, Cheng Zeng, Yan Meng, Bisheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16545">https://arxiv.org/abs/2410.16545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16545">https://arxiv.org/pdf/2410.16545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16545]] PlaneSAM: Multimodal Plane Instance Segmentation Using the Segment Anything Model(https://arxiv.org/abs/2410.16545)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Plane instance segmentation from RGB-D data is a crucial research topic for many downstream tasks. However, most existing deep-learning-based methods utilize only information within the RGB bands, neglecting the important role of the depth band in plane instance segmentation. Based on EfficientSAM, a fast version of SAM, we propose a plane instance segmentation network called PlaneSAM, which can fully integrate the information of the RGB bands (spectral bands) and the D band (geometric band), thereby improving the effectiveness of plane instance segmentation in a multimodal manner. Specifically, we use a dual-complexity backbone, with primarily the simpler branch learning D-band features and primarily the more complex branch learning RGB-band features. Consequently, the backbone can effectively learn D-band feature representations even when D-band training data is limited in scale, retain the powerful RGB-band feature representations of EfficientSAM, and allow the original backbone branch to be fine-tuned for the current task. To enhance the adaptability of our PlaneSAM to the RGB-D domain, we pretrain our dual-complexity backbone using the segment anything task on large-scale RGB-D data through a self-supervised pretraining strategy based on imperfect pseudo-labels. To support the segmentation of large planes, we optimize the loss function combination ratio of EfficientSAM. In addition, Faster R-CNN is used as a plane detector, and its predicted bounding boxes are fed into our dual-complexity network as prompts, thereby enabling fully automatic plane instance segmentation. Experimental results show that the proposed PlaneSAM sets a new SOTA performance on the ScanNet dataset, and outperforms previous SOTA approaches in zero-shot transfer on the 2D-3D-S, Matterport3D, and ICL-NUIM RGB-D datasets, while only incurring a 10% increase in computational overhead compared to EfficientSAM.</li>
</ul>

<h3>Title: Can Transformers In-Context Learn Behavior of a Linear Dynamical System?</h3>
<ul>
<li><strong>Authors: </strong>Usman Akram, Haris Vikalo</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16546">https://arxiv.org/abs/2410.16546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16546">https://arxiv.org/pdf/2410.16546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16546]] Can Transformers In-Context Learn Behavior of a Linear Dynamical System?(https://arxiv.org/abs/2410.16546)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We investigate whether transformers can learn to track a random process when given observations of a related process and parameters of the dynamical system that relates them as context. More specifically, we consider a finite-dimensional state-space model described by the state transition matrix $F$, measurement matrices $h_1, \dots, h_N$, and the process and measurement noise covariance matrices $Q$ and $R$, respectively; these parameters, randomly sampled, are provided to the transformer along with the observations $y_1,\dots,y_N$ generated by the corresponding linear dynamical system. We argue that in such settings transformers learn to approximate the celebrated Kalman filter, and empirically verify this both for the task of estimating hidden states $\hat{x}_{N|1,2,3,...,N}$ as well as for one-step prediction of the $(N+1)^{st}$ observation, $\hat{y}_{N+1|1,2,3,...,N}$. A further study of the transformer's robustness reveals that its performance is retained even if the model's parameters are partially withheld. In particular, we demonstrate that the transformer remains accurate at the considered task even in the absence of state transition and noise covariance matrices, effectively emulating operations of the Dual-Kalman filter.</li>
</ul>

<h3>Title: Foundation Models for Remote Sensing and Earth Observation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Aoran Xiao, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu, Naoto Yokoya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16602">https://arxiv.org/abs/2410.16602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16602">https://arxiv.org/pdf/2410.16602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16602]] Foundation Models for Remote Sensing and Earth Observation: A Survey(https://arxiv.org/abs/2410.16602)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Remote Sensing (RS) is a crucial technology for observing, monitoring, and interpreting our planet, with broad applications across geoscience, economics, humanitarian fields, etc. While artificial intelligence (AI), particularly deep learning, has achieved significant advances in RS, unique challenges persist in developing more intelligent RS systems, including the complexity of Earth's environments, diverse sensor modalities, distinctive feature patterns, varying spatial and spectral resolutions, and temporal dynamics. Meanwhile, recent breakthroughs in large Foundation Models (FMs) have expanded AI's potential across many domains due to their exceptional generalizability and zero-shot transfer capabilities. However, their success has largely been confined to natural data like images and video, with degraded performance and even failures for RS data of various non-optical modalities. This has inspired growing interest in developing Remote Sensing Foundation Models (RSFMs) to address the complex demands of Earth Observation (EO) tasks, spanning the surface, atmosphere, and oceans. This survey systematically reviews the emerging field of RSFMs. It begins with an outline of their motivation and background, followed by an introduction of their foundational concepts. It then categorizes and reviews existing RSFM studies including their datasets and technical contributions across Visual Foundation Models (VFMs), Visual-Language Models (VLMs), Large Language Models (LLMs), and beyond. In addition, we benchmark these models against publicly available datasets, discuss existing challenges, and propose future research directions in this rapidly evolving field.</li>
</ul>

<h3>Title: GALA: Graph Diffusion-based Alignment with Jigsaw for Source-free Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Junyu Luo, Yiyang Gu, Xiao Luo, Wei Ju, Zhiping Xiao, Yusheng Zhao, Jingyang Yuan, Ming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16606">https://arxiv.org/abs/2410.16606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16606">https://arxiv.org/pdf/2410.16606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16606]] GALA: Graph Diffusion-based Alignment with Jigsaw for Source-free Domain Adaptation(https://arxiv.org/abs/2410.16606)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Source-free domain adaptation is a crucial machine learning topic, as it contains numerous applications in the real world, particularly with respect to data privacy. Existing approaches predominantly focus on Euclidean data, such as images and videos, while the exploration of non-Euclidean graph data remains scarce. Recent graph neural network (GNN) approaches can suffer from serious performance decline due to domain shift and label scarcity in source-free adaptation scenarios. In this study, we propose a novel method named Graph Diffusion-based Alignment with Jigsaw (GALA), tailored for source-free graph domain adaptation. To achieve domain alignment, GALA employs a graph diffusion model to reconstruct source-style graphs from target data. Specifically, a score-based graph diffusion model is trained using source graphs to learn the generative source styles. Then, we introduce perturbations to target graphs via a stochastic differential equation instead of sampling from a prior, followed by the reverse process to reconstruct source-style graphs. We feed the source-style graphs into an off-the-shelf GNN and introduce class-specific thresholds with curriculum learning, which can generate accurate and unbiased pseudo-labels for target graphs. Moreover, we develop a simple yet effective graph-mixing strategy named graph jigsaw to combine confident graphs and unconfident graphs, which can enhance generalization capabilities and robustness via consistency learning. Extensive experiments on benchmark datasets validate the effectiveness of GALA.</li>
</ul>

<h3>Title: TopoDiffusionNet: A Topology-aware Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Saumya Gupta, Dimitris Samaras, Chao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16646">https://arxiv.org/abs/2410.16646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16646">https://arxiv.org/pdf/2410.16646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16646]] TopoDiffusionNet: A Topology-aware Diffusion Model(https://arxiv.org/abs/2410.16646)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at creating visually impressive images but often struggle to generate images with a specified topology. The Betti number, which represents the number of structures in an image, is a fundamental measure in topology. Yet, diffusion models fail to satisfy even this basic constraint. This limitation restricts their utility in applications requiring exact control, like robotics and environmental modeling. To address this, we propose TopoDiffusionNet (TDN), a novel approach that enforces diffusion models to maintain the desired topology. We leverage tools from topological data analysis, particularly persistent homology, to extract the topological structures within an image. We then design a topology-based objective function to guide the denoising process, preserving intended structures while suppressing noisy ones. Our experiments across four datasets demonstrate significant improvements in topological accuracy. TDN is the first to integrate topology with diffusion models, opening new avenues of research in this area.</li>
</ul>

<h3>Title: Dual-Model Defense: Safeguarding Diffusion Models from Membership Inference Attacks through Disjoint Data Splitting</h3>
<ul>
<li><strong>Authors: </strong>Bao Q. Tran, Viet Nguyen, Anh Tran, Toan Tran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16657">https://arxiv.org/abs/2410.16657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16657">https://arxiv.org/pdf/2410.16657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16657]] Dual-Model Defense: Safeguarding Diffusion Models from Membership Inference Attacks through Disjoint Data Splitting(https://arxiv.org/abs/2410.16657)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable capabilities in image synthesis, but their recently proven vulnerability to Membership Inference Attacks (MIAs) poses a critical privacy concern. This paper introduces two novel and efficient approaches (DualMD and DistillMD) to protect diffusion models against MIAs while maintaining high utility. Both methods are based on training two separate diffusion models on disjoint subsets of the original dataset. DualMD then employs a private inference pipeline that utilizes both models. This strategy significantly reduces the risk of black-box MIAs by limiting the information any single model contains about individual training samples. The dual models can also generate "soft targets" to train a private student model in DistillMD, enhancing privacy guarantees against all types of MIAs. Extensive evaluations of DualMD and DistillMD against state-of-the-art MIAs across various datasets in white-box and black-box settings demonstrate their effectiveness in substantially reducing MIA success rates while preserving competitive image generation performance. Notably, our experiments reveal that DistillMD not only defends against MIAs but also mitigates model memorization, indicating that both vulnerabilities stem from overfitting and can be addressed simultaneously with our unified approach.</li>
</ul>

<h3>Title: RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary Detection in Partially Machine Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>Ram Mohan Rao Kadiyala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16659">https://arxiv.org/abs/2410.16659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16659">https://arxiv.org/pdf/2410.16659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16659]] RKadiyala at SemEval-2024 Task 8: Black-Box Word-Level Text Boundary Detection in Partially Machine Generated Texts(https://arxiv.org/abs/2410.16659)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With increasing usage of generative models for text generation and widespread use of machine generated texts in various domains, being able to distinguish between human written and machine generated texts is a significant challenge. While existing models and proprietary systems focus on identifying whether given text is entirely human written or entirely machine generated, only a few systems provide insights at sentence or paragraph level at likelihood of being machine generated at a non reliable accuracy level, working well only for a set of domains and generators. This paper introduces few reliable approaches for the novel task of identifying which part of a given text is machine generated at a word level while comparing results from different approaches and methods. We present a comparison with proprietary systems , performance of our model on unseen domains' and generators' texts. The findings reveal significant improvements in detection accuracy along with comparison on other aspects of detection capabilities. Finally we discuss potential avenues for improvement and implications of our work. The proposed model is also well suited for detecting which parts of a text are machine generated in outputs of Instruct variants of many LLMs.</li>
</ul>

<h3>Title: Efficient Antibody Structure Refinement Using Energy-Guided SE(3) Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Jiying Zhang, Zijing Liu, Shengyuan Bai, He Cao, Yu Li, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16673">https://arxiv.org/abs/2410.16673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16673">https://arxiv.org/pdf/2410.16673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16673]] Efficient Antibody Structure Refinement Using Energy-Guided SE(3) Flow Matching(https://arxiv.org/abs/2410.16673)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Antibodies are proteins produced by the immune system that recognize and bind to specific antigens, and their 3D structures are crucial for understanding their binding mechanism and designing therapeutic interventions. The specificity of antibody-antigen binding predominantly depends on the complementarity-determining regions (CDR) within antibodies. Despite recent advancements in antibody structure prediction, the quality of predicted CDRs remains suboptimal. In this paper, we develop a novel antibody structure refinement method termed FlowAB based on energy-guided flow matching. FlowAB adopts the powerful deep generative method SE(3) flow matching and simultaneously incorporates important physical prior knowledge into the flow model to guide the generation process. The extensive experiments demonstrate that FlowAB can significantly improve the antibody CDR structures. It achieves new state-of-the-art performance on the antibody structure prediction task when used in conjunction with an appropriate prior model while incurring only marginal computational overhead. This advantage makes FlowAB a practical tool in antibody engineering.</li>
</ul>

<h3>Title: Governing equation discovery of a complex system from snapshots</h3>
<ul>
<li><strong>Authors: </strong>Qunxi Zhu, Bolin Zhao, Jingdong Zhang, Peiyang Li, Wei Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16694">https://arxiv.org/abs/2410.16694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16694">https://arxiv.org/pdf/2410.16694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16694]] Governing equation discovery of a complex system from snapshots(https://arxiv.org/abs/2410.16694)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Complex systems in physics, chemistry, and biology that evolve over time with inherent randomness are typically described by stochastic differential equations (SDEs). A fundamental challenge in science and engineering is to determine the governing equations of a complex system from snapshot data. Traditional equation discovery methods often rely on stringent assumptions, such as the availability of the trajectory information or time-series data, and the presumption that the underlying system is deterministic. In this work, we introduce a data-driven, simulation-free framework, called Sparse Identification of Differential Equations from Snapshots (SpIDES), that discovers the governing equations of a complex system from snapshots by utilizing the advanced machine learning techniques to perform three essential steps: probability flow reconstruction, probability density estimation, and Bayesian sparse identification. We validate the effectiveness and robustness of SpIDES by successfully identifying the governing equation of an over-damped Langevin system confined within two potential wells. By extracting interpretable drift and diffusion terms from the SDEs, our framework provides deeper insights into system dynamics, enhances predictive accuracy, and facilitates more effective strategies for managing and simulating stochastic systems.</li>
</ul>

<h3>Title: Hyperboloid GPLVM for Discovering Continuous Hierarchies via Nonparametric Estimation</h3>
<ul>
<li><strong>Authors: </strong>Koshi Watanabe, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16698">https://arxiv.org/abs/2410.16698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16698">https://arxiv.org/pdf/2410.16698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16698]] Hyperboloid GPLVM for Discovering Continuous Hierarchies via Nonparametric Estimation(https://arxiv.org/abs/2410.16698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dimensionality reduction (DR) offers a useful representation of complex high-dimensional data. Recent DR methods focus on hyperbolic geometry to derive a faithful low-dimensional representation of hierarchical data. However, existing methods are based on neighbor embedding, frequently ruining the continual relation of the hierarchies. This paper presents hyperboloid Gaussian process (GP) latent variable models (hGP-LVMs) to embed high-dimensional hierarchical data with implicit continuity via nonparametric estimation. We adopt generative modeling using the GP, which brings effective hierarchical embedding and executes ill-posed hyperparameter tuning. This paper presents three variants that employ original point, sparse point, and Bayesian estimations. We establish their learning algorithms by incorporating the Riemannian optimization and active approximation scheme of GP-LVM. For Bayesian inference, we further introduce the reparameterization trick to realize Bayesian latent variable learning. In the last part of this paper, we apply hGP-LVMs to several datasets and show their ability to represent high-dimensional hierarchies in low-dimensional spaces.</li>
</ul>

<h3>Title: ClimaQA: An Automated Evaluation Framework for Climate Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Veeramakali Vignesh Manivannan, Yasaman Jafari, Srikar Eranky, Spencer Ho, Rose Yu, Duncan Watson-Parris, Yian Ma, Leon Bergen, Taylor Berg-Kirkpatrick</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16701">https://arxiv.org/abs/2410.16701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16701">https://arxiv.org/pdf/2410.16701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16701]] ClimaQA: An Automated Evaluation Framework for Climate Foundation Models(https://arxiv.org/abs/2410.16701)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The use of foundation models in climate science has recently gained significant attention. However, a critical issue remains: the lack of a comprehensive evaluation framework capable of assessing the quality and scientific validity of model outputs. To address this issue, we develop ClimaGen (Climate QA Generator), an automated algorithmic framework that generates question-answer pairs from graduate textbooks with climate scientists in the loop. As a result, we present ClimaQA-Gold, an expert-annotated benchmark dataset alongside ClimaQA-Silver, a large-scale, comprehensive synthetic QA dataset for climate science. Finally, we develop evaluation strategies and compare different Large Language Models (LLMs) on our benchmarks. Our results offer novel insights into various approaches used to enhance climate foundation models.</li>
</ul>

<h3>Title: Collapse or Thrive? Perils and Promises of Synthetic Data in a Self-Generating World</h3>
<ul>
<li><strong>Authors: </strong>Joshua Kazdan, Rylan Schaeffer, Apratim Dey, Matthias Gerstgrasser, Rafael Rafailov, David L. Donoho, Sanmi Koyejo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16713">https://arxiv.org/abs/2410.16713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16713">https://arxiv.org/pdf/2410.16713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16713]] Collapse or Thrive? Perils and Promises of Synthetic Data in a Self-Generating World(https://arxiv.org/abs/2410.16713)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing presence of AI-generated content on the internet raises a critical question: What happens when generative machine learning models are pretrained on web-scale datasets containing data created by earlier models? Some authors prophesy $\textit{model collapse}$ under a "$\textit{replace}$" scenario: a sequence of models, the first trained with real data and each later one trained only on synthetic data from its preceding model. In this scenario, models successively degrade. Others see collapse as easily avoidable; in an "$\textit{accumulate}$' scenario, a sequence of models is trained, but each training uses all real and synthetic data generated so far. In this work, we deepen and extend the study of these contrasting scenarios. First, collapse versus avoidance of collapse is studied by comparing the replace and accumulate scenarios on each of three prominent generative modeling settings; we find the same contrast emerges in all three settings. Second, we study a compromise scenario; the available data remains the same as in the accumulate scenario -- but unlike $\textit{accumulate}$ and like $\textit{replace}$, each model is trained using a fixed compute budget; we demonstrate that model test loss on real data is larger than in the $\textit{accumulate}$ scenario, but apparently plateaus, unlike the divergence seen with $\textit{replace}$. Third, we study the relative importance of cardinality and proportion of real data for avoiding model collapse. Surprisingly, we find a non-trivial interaction between real and synthetic data, where the value of synthetic data for reducing test loss depends on the absolute quantity of real data. Our insights are particularly important when forecasting whether future frontier generative models will collapse or thrive, and our results open avenues for empirically and mathematically studying the context-dependent value of synthetic data.</li>
</ul>

<h3>Title: Progressive Compositionality In Text-to-Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Xu Han, Linghao Jin, Xiaofeng Liu, Paul Pu Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16719">https://arxiv.org/abs/2410.16719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16719">https://arxiv.org/pdf/2410.16719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16719]] Progressive Compositionality In Text-to-Image Generative Models(https://arxiv.org/abs/2410.16719)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing solutions have tackled these challenges by optimizing the cross-attention mechanism or learning from the caption pairs with minimal semantic changes. However, can we generate high-quality complex contrastive images that diffusion models can directly discriminate based on visual representations? In this work, we leverage large-language models (LLMs) to compose realistic, complex scenarios and harness Visual-Question Answering (VQA) systems alongside diffusion models to automatically curate a contrastive dataset, ConPair, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases, i.e., hard negative images, we propose EvoGen, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks.</li>
</ul>

<h3>Title: Polyp-E: Benchmarking the Robustness of Deep Segmentation Models via Polyp Editing</h3>
<ul>
<li><strong>Authors: </strong>Runpu Wei, Zijin Yin, Kongming Liang, Min Min, Chengwei Pan, Gang Yu, Haonan Huang, Yan Liu, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16732">https://arxiv.org/abs/2410.16732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16732">https://arxiv.org/pdf/2410.16732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16732]] Polyp-E: Benchmarking the Robustness of Deep Segmentation Models via Polyp Editing(https://arxiv.org/abs/2410.16732)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Automatic polyp segmentation is helpful to assist clinical diagnosis and treatment. In daily clinical practice, clinicians exhibit robustness in identifying polyps with both location and size variations. It is uncertain if deep segmentation models can achieve comparable robustness in automated colonoscopic analysis. To benchmark the model robustness, we focus on evaluating the robustness of segmentation models on the polyps with various attributes (e.g. location and size) and healthy samples. Based on the Latent Diffusion Model, we perform attribute editing on real polyps and build a new dataset named Polyp-E. Our synthetic dataset boasts exceptional realism, to the extent that clinical experts find it challenging to discern them from real data. We evaluate several existing polyp segmentation models on the proposed benchmark. The results reveal most of the models are highly sensitive to attribute variations. As a novel data augmentation technique, the proposed editing pipeline can improve both in-distribution and out-of-distribution generalization ability. The code and datasets will be released.</li>
</ul>

<h3>Title: LLM-Assisted Red Teaming of Diffusion Models through "Failures Are Fated, But Can Be Faded"</h3>
<ul>
<li><strong>Authors: </strong>Som Sagar, Aditya Taparia, Ransalu Senanayake</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16738">https://arxiv.org/abs/2410.16738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16738">https://arxiv.org/pdf/2410.16738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16738]] LLM-Assisted Red Teaming of Diffusion Models through "Failures Are Fated, But Can Be Faded"(https://arxiv.org/abs/2410.16738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug or audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we improve the "Failures are fated, but can be faded" framework (arXiv:2406.07145)--a post-hoc method to explore and construct the failure landscape in pre-trained generative models--with a variety of deep reinforcement learning algorithms, screening tests, and LLM-based rewards and state generation. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically demonstrate the effectiveness of the proposed method on diffusion models. We also highlight the strengths and weaknesses of each algorithm in identifying failure modes.</li>
</ul>

<h3>Title: Beyond Retrieval: Generating Narratives in Conversational Recommender Systems</h3>
<ul>
<li><strong>Authors: </strong>Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16780">https://arxiv.org/abs/2410.16780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16780">https://arxiv.org/pdf/2410.16780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16780]] Beyond Retrieval: Generating Narratives in Conversational Recommender Systems(https://arxiv.org/abs/2410.16780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions. First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.</li>
</ul>

<h3>Title: One-Step Diffusion Distillation through Score Implicit Matching</h3>
<ul>
<li><strong>Authors: </strong>Weijian Luo, Zemin Huang, Zhengyang Geng, J. Zico Kolter, Guo-jun Qi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16794">https://arxiv.org/abs/2410.16794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16794">https://arxiv.org/pdf/2410.16794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16794]] One-Step Diffusion Distillation through Score Implicit Matching(https://arxiv.org/abs/2410.16794)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite their strong performances on many generative tasks, diffusion models require a large number of sampling steps in order to generate realistic samples. This has motivated the community to develop effective methods to distill pre-trained diffusion models into more efficient models, but these methods still typically require few-step inference or perform substantially worse than the underlying model. In this paper, we present Score Implicit Matching (SIM) a new approach to distilling pre-trained diffusion models into single-step generator models, while maintaining almost the same sample generation ability as the original model as well as being data-free with no need of training samples for distillation. The method rests upon the fact that, although the traditional score-based loss is intractable to minimize for generator models, under certain conditions we can efficiently compute the gradients for a wide class of score-based divergences between a diffusion model and a generator. SIM shows strong empirical performances for one-step generators: on the CIFAR10 dataset, it achieves an FID of 2.06 for unconditional generation and 1.96 for class-conditional generation. Moreover, by applying SIM to a leading transformer-based diffusion model, we distill a single-step generator for text-to-image (T2I) generation that attains an aesthetic score of 6.42 with no performance decline over the original multi-step counterpart, clearly outperforming the other one-step generators including SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85. We will release this industry-ready one-step transformer-based T2I generator along with this paper.</li>
</ul>

<h3>Title: Evaluating the Effectiveness of Attack-Agnostic Features for Morphing Attack Detection</h3>
<ul>
<li><strong>Authors: </strong>Laurent Colbois, Sébastien Marcel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16802">https://arxiv.org/abs/2410.16802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16802">https://arxiv.org/pdf/2410.16802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16802]] Evaluating the Effectiveness of Attack-Agnostic Features for Morphing Attack Detection(https://arxiv.org/abs/2410.16802)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Morphing attacks have diversified significantly over the past years, with new methods based on generative adversarial networks (GANs) and diffusion models posing substantial threats to face recognition systems. Recent research has demonstrated the effectiveness of features extracted from large vision models pretrained on bonafide data only (attack-agnostic features) for detecting deep generative images. Building on this, we investigate the potential of these image representations for morphing attack detection (MAD). We develop supervised detectors by training a simple binary linear SVM on the extracted features and one-class detectors by modeling the distribution of bonafide features with a Gaussian Mixture Model (GMM). Our method is evaluated across a comprehensive set of attacks and various scenarios, including generalization to unseen attacks, different source datasets, and print-scan data. Our results indicate that attack-agnostic features can effectively detect morphing attacks, outperforming traditional supervised and one-class detectors from the literature in most scenarios. Additionally, we provide insights into the strengths and limitations of each considered representation and discuss potential future research directions to further enhance the robustness and generalizability of our approach.</li>
</ul>

<h3>Title: Test-time Adversarial Defense with Opposite Adversarial Path and High Attack Time Cost</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Han Yeh, Kuanchun Yu, Chun-Shien Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16805">https://arxiv.org/abs/2410.16805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16805">https://arxiv.org/pdf/2410.16805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16805]] Test-time Adversarial Defense with Opposite Adversarial Path and High Attack Time Cost(https://arxiv.org/abs/2410.16805)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep learning models are known to be vulnerable to adversarial attacks by injecting sophisticated designed perturbations to input data. Training-time defenses still exhibit a significant performance gap between natural accuracy and robust accuracy. In this paper, we investigate a new test-time adversarial defense method via diffusion-based recovery along opposite adversarial paths (OAPs). We present a purifier that can be plugged into a pre-trained model to resist adversarial attacks. Different from prior arts, the key idea is excessive denoising or purification by integrating the opposite adversarial direction with reverse diffusion to push the input image further toward the opposite adversarial direction. For the first time, we also exemplify the pitfall of conducting AutoAttack (Rand) for diffusion-based defense methods. Through the lens of time complexity, we examine the trade-off between the effectiveness of adaptive attack and its computation complexity against our defense. Experimental evaluation along with time cost analysis verifies the effectiveness of the proposed method.</li>
</ul>

<h3>Title: MPDS: A Movie Posters Dataset for Image Generation with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Meng Xu (1), Tong Zhang (1), Fuyun Wang (1), Yi Lei (1), Xin Liu (2), Zhen Cui (1) ((1) Nanjing University of Science and Technology, Nanjing, China., (2) SeetaCloud, Nanjing, China.)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16840">https://arxiv.org/abs/2410.16840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16840">https://arxiv.org/pdf/2410.16840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16840]] MPDS: A Movie Posters Dataset for Image Generation with Diffusion Model(https://arxiv.org/abs/2410.16840)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Movie posters are vital for captivating audiences, conveying themes, and driving market competition in the film industry. While traditional designs are laborious, intelligent generation technology offers efficiency gains and design enhancements. Despite exciting progress in image generation, current models often fall short in producing satisfactory poster results. The primary issue lies in the absence of specialized poster datasets for targeted model training. In this work, we propose a Movie Posters DataSet (MPDS), tailored for text-to-image generation models to revolutionize poster production. As dedicated to posters, MPDS stands out as the first image-text pair dataset to our knowledge, composing of 373k+ image-text pairs and 8k+ actor images (covering 4k+ actors). Detailed poster descriptions, such as movie titles, genres, casts, and synopses, are meticulously organized and standardized based on public movie synopsis, also named movie-synopsis prompt. To bolster poster descriptions as well as reduce differences from movie synopsis, further, we leverage a large-scale vision-language model to automatically produce vision-perceptive prompts for each poster, then perform manual rectification and integration with movie-synopsis prompt. In addition, we introduce a prompt of poster captions to exhibit text elements in posters like actor names and movie titles. For movie poster generation, we develop a multi-condition diffusion framework that takes poster prompt, poster caption, and actor image (for personalization) as inputs, yielding excellent results through the learning of a diffusion model. Experiments demonstrate the valuable role of our proposed MPDS dataset in advancing personalized movie poster generation. MPDS is available at this https URL.</li>
</ul>

<h3>Title: CK4Gen: A Knowledge Distillation Framework for Generating High-Utility Synthetic Survival Datasets in Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Nicholas I-Hsien Kuo, Blanca Gallego, Louisa Jorm</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16872">https://arxiv.org/abs/2410.16872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16872">https://arxiv.org/pdf/2410.16872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16872]] CK4Gen: A Knowledge Distillation Framework for Generating High-Utility Synthetic Survival Datasets in Healthcare(https://arxiv.org/abs/2410.16872)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Access to real clinical data is heavily restricted by privacy regulations, hindering both healthcare research and education. These constraints slow progress in developing new treatments and data-driven healthcare solutions, while also limiting students' access to real-world datasets, leaving them without essential practical skills. High-utility synthetic datasets are therefore critical for advancing research and providing meaningful training material. However, current generative models -- such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) -- produce surface-level realism at the expense of healthcare utility, blending distinct patient profiles and producing synthetic data of limited practical relevance. To overcome these limitations, we introduce CK4Gen (Cox Knowledge for Generation), a novel framework that leverages knowledge distillation from Cox Proportional Hazards (CoxPH) models to create synthetic survival datasets that preserve key clinical characteristics, including hazard ratios and survival curves. CK4Gen avoids the interpolation issues seen in VAEs and GANs by maintaining distinct patient risk profiles, ensuring realistic and reliable outputs for research and educational use. Validated across four benchmark datasets -- GBSG2, ACTG320, WHAS500, and FLChain -- CK4Gen outperforms competing techniques by better aligning real and synthetic data, enhancing survival model performance in both discrimination and calibration via data augmentation. As CK4Gen is scalable across clinical conditions, and with code to be made publicly available, future researchers can apply it to their own datasets to generate synthetic versions suitable for open sharing.</li>
</ul>

<h3>Title: Unsupervised Time Series Anomaly Prediction with Importance-based Generative Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Kai Zhao, Zhihao Zhuang, Chenjuan Guo, Hao Miao, Yunyao Cheng, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16888">https://arxiv.org/abs/2410.16888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16888">https://arxiv.org/pdf/2410.16888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16888]] Unsupervised Time Series Anomaly Prediction with Importance-based Generative Contrastive Learning(https://arxiv.org/abs/2410.16888)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly prediction plays an essential role in many real-world scenarios, such as environmental prevention and prompt maintenance of cyber-physical systems. However, existing time series anomaly prediction methods mainly require supervised training with plenty of manually labeled data, which are difficult to obtain in practice. Besides, unseen anomalies can occur during inference, which could differ from the labeled training data and make these models fail to predict such new anomalies. In this paper, we study a novel problem of unsupervised time series anomaly prediction. We provide a theoretical analysis and propose Importance-based Generative Contrastive Learning (IGCL) to address the aforementioned problems. IGCL distinguishes between normal and anomaly precursors, which are generated by our anomaly precursor pattern generation module. To address the efficiency issues caused by the potential complex anomaly precursor combinations, we propose a memory bank with importance-based scores to adaptively store representative anomaly precursors and generate more complicated anomaly precursors. Extensive experiments on seven benchmark datasets show our method outperforms state-of-the-art baselines on unsupervised time series anomaly prediction problems.</li>
</ul>

<h3>Title: VistaDream: Sampling multiview consistent images for single-view scene reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Haiping Wang, Yuan Liu, Ziwei Liu, Wenping Wang, Zhen Dong, Bisheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16892">https://arxiv.org/abs/2410.16892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16892">https://arxiv.org/pdf/2410.16892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16892]] VistaDream: Sampling multiview consistent images for single-view scene reconstruction(https://arxiv.org/abs/2410.16892)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose VistaDream a novel framework to reconstruct a 3D scene from a single-view image. Recent diffusion models enable generating high-quality novel-view images from a single-view input image. Most existing methods only concentrate on building the consistency between the input image and the generated images while losing the consistency between the generated images. VistaDream addresses this problem by a two-stage pipeline. In the first stage, VistaDream begins with building a global coarse 3D scaffold by zooming out a little step with inpainted boundaries and an estimated depth map. Then, on this global scaffold, we use iterative diffusion-based RGB-D inpainting to generate novel-view images to inpaint the holes of the scaffold. In the second stage, we further enhance the consistency between the generated novel-view images by a novel training-free Multiview Consistency Sampling (MCS) that introduces multi-view consistency constraints in the reverse sampling process of diffusion models. Experimental results demonstrate that without training or fine-tuning existing diffusion models, VistaDream achieves consistent and high-quality novel view synthesis using just single-view images and outperforms baseline methods by a large margin. The code, videos, and interactive demos are available at this https URL.</li>
</ul>

<h3>Title: Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections</h3>
<ul>
<li><strong>Authors: </strong>Marco Miani, Hrittik Roy, Søren Hauberg</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16901">https://arxiv.org/abs/2410.16901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16901">https://arxiv.org/pdf/2410.16901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16901]] Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections(https://arxiv.org/abs/2410.16901)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Bayesian deep learning all too often underfits so that the Bayesian prediction is less accurate than a simple point estimate. Uncertainty quantification then comes at the cost of accuracy. For linearized models, the null space of the generalized Gauss-Newton matrix corresponds to parameters that preserve the training predictions of the point estimate. We propose to build Bayesian approximations in this null space, thereby guaranteeing that the Bayesian predictive does not underfit. We suggest a matrix-free algorithm for projecting onto this null space, which scales linearly with the number of parameters and quadratically with the number of output dimensions. We further propose an approximation that only scales linearly with parameters to make the method applicable to generative models. An extensive empirical evaluation shows that the approach scales to large models, including vision transformers with 28 million parameters.</li>
</ul>

<h3>Title: Hierarchical Clustering for Conditional Diffusion in Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jorge da Silva Goncalves, Laura Manduchi, Moritz Vandenhirtz, Julia E. Vogt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16910">https://arxiv.org/abs/2410.16910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16910">https://arxiv.org/pdf/2410.16910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16910]] Hierarchical Clustering for Conditional Diffusion in Image Generation(https://arxiv.org/abs/2410.16910)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Finding clusters of data points with similar characteristics and generating new cluster-specific samples can significantly enhance our understanding of complex data distributions. While clustering has been widely explored using Variational Autoencoders, these models often lack generation quality in real-world datasets. This paper addresses this gap by introducing TreeDiffusion, a deep generative model that conditions Diffusion Models on hierarchical clusters to obtain high-quality, cluster-specific generations. The proposed pipeline consists of two steps: a VAE-based clustering model that learns the hierarchical structure of the data, and a conditional diffusion model that generates realistic images for each cluster. We propose this two-stage process to ensure that the generated samples remain representative of their respective clusters and enhance image fidelity to the level of diffusion models. A key strength of our method is its ability to create images for each cluster, providing better visualization of the learned representations by the clustering model, as demonstrated through qualitative results. This method effectively addresses the generative limitations of VAE-based approaches while preserving their clustering performance. Empirically, we demonstrate that conditioning diffusion models on hierarchical clusters significantly enhances generative performance, thereby advancing the state of generative clustering models.</li>
</ul>

<h3>Title: LIMIS: Towards Language-based Interactive Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Lena Heinemann, Alexander Jaus, Zdravko Marinov, Moon Kim, Maria Francesca Spadea, Jens Kleesiek, Rainer Stiefelhagen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16939">https://arxiv.org/abs/2410.16939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16939">https://arxiv.org/pdf/2410.16939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16939]] LIMIS: Towards Language-based Interactive Medical Image Segmentation(https://arxiv.org/abs/2410.16939)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Within this work, we introduce LIMIS: The first purely language-based interactive medical image segmentation model. We achieve this by adapting Grounded SAM to the medical domain and designing a language-based model interaction strategy that allows radiologists to incorporate their knowledge into the segmentation process. LIMIS produces high-quality initial segmentation masks by leveraging medical foundation models and allows users to adapt segmentation masks using only language, opening up interactive segmentation to scenarios where physicians require using their hands for other tasks. We evaluate LIMIS on three publicly available medical datasets in terms of performance and usability with experts from the medical domain confirming its high-quality segmentation masks and its interactive usability.</li>
</ul>

<h3>Title: DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization</h3>
<ul>
<li><strong>Authors: </strong>Haowei Zhu, Dehua Tang, Ji Liu, Mingjie Lu, Jintu Zheng, Jinzhang Peng, Dong Li, Yu Wang, Fan Jiang, Lu Tian, Spandan Tiwari, Ashish Sirasao, Jun-Hai Yong, Bin Wang, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16942">https://arxiv.org/abs/2410.16942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16942">https://arxiv.org/pdf/2410.16942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16942]] DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization(https://arxiv.org/abs/2410.16942)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable progress in the field of image generation due to their outstanding capabilities. However, these models require substantial computing resources because of the multi-step denoising process during inference. While traditional pruning methods have been employed to optimize these models, the retraining process necessitates large-scale training datasets and extensive computational costs to maintain generalization ability, making it neither convenient nor efficient. Recent studies attempt to utilize the similarity of features across adjacent denoising stages to reduce computational costs through simple and static strategies. However, these strategies cannot fully harness the potential of the similar feature patterns across adjacent timesteps. In this work, we propose a novel pruning method that derives an efficient diffusion model via a more intelligent and differentiable pruner. At the core of our approach is casting the model pruning process into a SubNet search process. Specifically, we first introduce a SuperNet based on standard diffusion via adding some backup connections built upon the similar features. We then construct a plugin pruner network and design optimization losses to identify redundant computation. Finally, our method can identify an optimal SubNet through few-step gradient optimization and a simple post-processing procedure. We conduct extensive experiments on various diffusion models including Stable Diffusion series and DiTs. Our DiP-GO approach achieves 4.4 x speedup for SD-1.5 without any loss of accuracy, significantly outperforming the previous state-of-the-art methods.</li>
</ul>

<h3>Title: ISImed: A Framework for Self-Supervised Learning using Intrinsic Spatial Information in Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Nabil Jabareen, Dongsheng Yuan, Sören Lukassen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16947">https://arxiv.org/abs/2410.16947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16947">https://arxiv.org/pdf/2410.16947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16947]] ISImed: A Framework for Self-Supervised Learning using Intrinsic Spatial Information in Medical Images(https://arxiv.org/abs/2410.16947)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper demonstrates that spatial information can be used to learn interpretable representations in medical images using Self-Supervised Learning (SSL). Our proposed method, ISImed, is based on the observation that medical images exhibit a much lower variability among different images compared to classic data vision benchmarks. By leveraging this resemblance of human body structures across multiple images, we establish a self-supervised objective that creates a latent representation capable of capturing its location in the physical realm. More specifically, our method involves sampling image crops and creating a distance matrix that compares the learned representation vectors of all possible combinations of these crops to the true distance between them. The intuition is, that the learned latent space is a positional encoding for a given image crop. We hypothesize, that by learning these positional encodings, comprehensive image representations have to be generated. To test this hypothesis and evaluate our method, we compare our learned representation with two state-of-the-art SSL benchmarking methods on two publicly available medical imaging datasets. We show that our method can efficiently learn representations that capture the underlying structure of the data and can be used to transfer to a downstream classification task.</li>
</ul>

<h3>Title: PGCS: Physical Law embedded Generative Cloud Synthesis in Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Liying Xu, Huifang Li, Huanfeng Shen, Mingyang Lei, Tao Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16955">https://arxiv.org/abs/2410.16955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16955">https://arxiv.org/pdf/2410.16955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16955]] PGCS: Physical Law embedded Generative Cloud Synthesis in Remote Sensing Images(https://arxiv.org/abs/2410.16955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data quantity and quality are both critical for information extraction and analyzation in remote sensing. However, the current remote sensing datasets often fail to meet these two requirements, for which cloud is a primary factor degrading the data quantity and quality. This limitation affects the precision of results in remote sensing application, particularly those derived from data-driven techniques. In this paper, a physical law embedded generative cloud synthesis method (PGCS) is proposed to generate diverse realistic cloud images to enhance real data and promote the development of algorithms for subsequent tasks, such as cloud correction, cloud detection, and data augmentation for classification, recognition, and segmentation. The PGCS method involves two key phases: spatial synthesis and spectral synthesis. In the spatial synthesis phase, a style-based generative adversarial network is utilized to simulate the spatial characteristics, generating an infinite number of single-channel clouds. In the spectral synthesis phase, the atmospheric scattering law is embedded through a local statistics and global fitting method, converting the single-channel clouds into multi-spectral clouds. The experimental results demonstrate that PGCS achieves a high accuracy in both phases and performs better than three other existing cloud synthesis methods. Two cloud correction methods are developed from PGCS and exhibits a superior performance compared to state-of-the-art methods in the cloud correction task. Furthermore, the application of PGCS with data from various sensors was investigated and successfully extended. Code will be provided at this https URL.</li>
</ul>

<h3>Title: IPL: Leveraging Multimodal Large Language Models for Intelligent Product Listing</h3>
<ul>
<li><strong>Authors: </strong>Kang Chen, Qingheng Zhang, Chengbao Lian, Yixin Ji, Xuwei Liu, Shuguang Han, Guoqiang Wu, Fei Huang, Jufeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.16977">https://arxiv.org/abs/2410.16977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.16977">https://arxiv.org/pdf/2410.16977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.16977]] IPL: Leveraging Multimodal Large Language Models for Intelligent Product Listing(https://arxiv.org/abs/2410.16977)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Unlike professional Business-to-Consumer (B2C) e-commerce platforms (e.g., Amazon), Consumer-to-Consumer (C2C) platforms (e.g., Facebook marketplace) are mainly targeting individual sellers who usually lack sufficient experience in e-commerce. Individual sellers often struggle to compose proper descriptions for selling products. With the recent advancement of Multimodal Large Language Models (MLLMs), we attempt to integrate such state-of-the-art generative AI technologies into the product listing process. To this end, we develop IPL, an Intelligent Product Listing tool tailored to generate descriptions using various product attributes such as category, brand, color, condition, etc. IPL enables users to compose product descriptions by merely uploading photos of the selling product. More importantly, it can imitate the content style of our C2C platform Xianyu. This is achieved by employing domain-specific instruction tuning on MLLMs and adopting the multi-modal Retrieval-Augmented Generation (RAG) process. A comprehensive empirical evaluation demonstrates that the underlying model of IPL significantly outperforms the base model in domain-specific tasks while producing less hallucination. IPL has been successfully deployed in our production system, where 72% of users have their published product listings based on the generated content, and those product listings are shown to have a quality score 5.6% higher than those without AI assistance.</li>
</ul>

<h3>Title: Aligning Large Language Models via Self-Steering Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun, Jingren Zhou, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17131">https://arxiv.org/abs/2410.17131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17131">https://arxiv.org/pdf/2410.17131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17131]] Aligning Large Language Models via Self-Steering Optimization(https://arxiv.org/abs/2410.17131)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. $SSO$ maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. $SSO$ can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of $SSO$ with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, $SSO$ leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by $SSO$ significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment.</li>
</ul>

<h3>Title: Are Visual-Language Models Effective in Action Recognition? A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Ali, Di Yang, François Brémond</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17149">https://arxiv.org/abs/2410.17149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17149">https://arxiv.org/pdf/2410.17149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17149]] Are Visual-Language Models Effective in Action Recognition? A Comparative Study(https://arxiv.org/abs/2410.17149)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Current vision-language foundation models, such as CLIP, have recently shown significant improvement in performance across various downstream tasks. However, whether such foundation models significantly improve more complex fine-grained action recognition tasks is still an open question. To answer this question and better find out the future research direction on human behavior analysis in-the-wild, this paper provides a large-scale study and insight on current state-of-the-art vision foundation models by comparing their transfer ability onto zero-shot and frame-wise action recognition tasks. Extensive experiments are conducted on recent fine-grained, human-centric action recognition datasets (e.g., Toyota Smarthome, Penn Action, UAV-Human, TSU, Charades) including action classification and segmentation.</li>
</ul>

<h3>Title: Context-aware Prompt Tuning: Advancing In-Context Learning with Adversarial Methods</h3>
<ul>
<li><strong>Authors: </strong>Tsachi Blau, Moshe Kimhi, Yonatan Belinkov, Alexander Bronstein, Chaim Baskin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.17222">https://arxiv.org/abs/2410.17222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.17222">https://arxiv.org/pdf/2410.17222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.17222]] Context-aware Prompt Tuning: Advancing In-Context Learning with Adversarial Methods(https://arxiv.org/abs/2410.17222)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) typically involves updating at least a few billions of parameters. A more parameter-efficient approach is Prompt Tuning (PT), which updates only a few learnable tokens, and differently, In-Context Learning (ICL) adapts the model to a new task by simply including examples in the input without any training. When applying optimization-based methods, such as fine-tuning and PT for few-shot learning, the model is specifically adapted to the small set of training examples, whereas ICL leaves the model unchanged. This distinction makes traditional learning methods more prone to overfitting; in contrast, ICL is less sensitive to the few-shot scenario. While ICL is not prone to overfitting, it does not fully extract the information that exists in the training examples. This work introduces Context-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and adversarial attacks. We build on the ICL strategy of concatenating examples before the input, but we extend this by PT-like learning, refining the context embedding through iterative optimization to extract deeper insights from the training examples. We carefully modify specific context tokens, considering the unique structure of input and output formats. Inspired by adversarial attacks, we adjust the input based on the labels present in the context, focusing on minimizing, rather than maximizing, the loss. Moreover, we apply a projected gradient descent algorithm to keep token embeddings close to their original values, under the assumption that the user-provided data is inherently valuable. Our method has been shown to achieve superior accuracy across multiple classification tasks using various LLM models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
