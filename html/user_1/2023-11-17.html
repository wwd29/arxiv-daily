<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs. (arXiv:2311.09257v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09257">http://arxiv.org/abs/2311.09257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09257]] UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs(http://arxiv.org/abs/2311.09257)</code></li>
<li>Summary: <p>Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.
\blfootnote{*Work done as a student researcher of Google, $\dagger$ indicates
equal contribution.
</p></li>
</ul>

<h3>Title: FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier. (arXiv:2311.09265v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09265">http://arxiv.org/abs/2311.09265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09265]] FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier(http://arxiv.org/abs/2311.09265)</code></li>
<li>Summary: <p>With the emergence of diffusion models and rapid development in image
processing, it has become effortless to generate fancy images in tasks such as
style transfer and image editing. However, these impressive image processing
approaches face consistency issues in video processing. In this paper, we
propose a powerful model-free toolkit called FastBlend to address the
consistency problem for video processing. Based on a patch matching algorithm,
we design two inference modes, including blending and interpolation. In the
blending mode, FastBlend eliminates video flicker by blending the frames within
a sliding window. Moreover, we optimize both computational efficiency and video
quality according to different application scenarios. In the interpolation
mode, given one or more keyframes rendered by diffusion models, FastBlend can
render the whole video. Since FastBlend does not modify the generation process
of diffusion models, it exhibits excellent compatibility. Extensive experiments
have demonstrated the effectiveness of FastBlend. In the blending mode,
FastBlend outperforms existing methods for video deflickering and video
synthesis. In the interpolation mode, FastBlend surpasses video interpolation
and model-based video processing approaches. The source codes have been
released on GitHub.
</p></li>
</ul>

<h3>Title: Privacy Threats in Stable Diffusion Models. (arXiv:2311.09355v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09355">http://arxiv.org/abs/2311.09355</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09355]] Privacy Threats in Stable Diffusion Models(http://arxiv.org/abs/2311.09355)</code></li>
<li>Summary: <p>This paper introduces a novel approach to membership inference attacks (MIA)
targeting stable diffusion computer vision models, specifically focusing on the
highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract
sensitive information about a model's training data, posing significant privacy
concerns. Despite its advancements in image synthesis, our research reveals
privacy vulnerabilities in the stable diffusion models' outputs. Exploiting
this information, we devise a black-box MIA that only needs to query the victim
model repeatedly. Our methodology involves observing the output of a stable
diffusion model at different generative epochs and training a classification
model to distinguish when a series of intermediates originated from a training
sample or not. We propose numerous ways to measure the membership features and
discuss what works best. The attack's efficacy is assessed using the ROC AUC
method, demonstrating a 60\% success rate in inferring membership information.
This paper contributes to the growing body of research on privacy and security
in machine learning, highlighting the need for robust defenses against MIAs.
Our findings prompt a reevaluation of the privacy implications of stable
diffusion models, urging practitioners and developers to implement enhanced
security measures to safeguard against such attacks.
</p></li>
</ul>

<h3>Title: Synthetically Enhanced: Unveiling Synthetic Data's Potential in Medical Imaging Research. (arXiv:2311.09402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09402">http://arxiv.org/abs/2311.09402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09402]] Synthetically Enhanced: Unveiling Synthetic Data's Potential in Medical Imaging Research(http://arxiv.org/abs/2311.09402)</code></li>
<li>Summary: <p>Chest X-rays (CXR) are the most common medical imaging study and are used to
diagnose multiple medical conditions. This study examines the impact of
synthetic data supplementation, using diffusion models, on the performance of
deep learning (DL) classifiers for CXR analysis. We employed three datasets:
CheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising
diffusion probabilistic models (DDPMs) to generate synthetic frontal
radiographs. Our approach ensured that synthetic images mirrored the
demographic and pathological traits of the original data. Evaluating the
classifiers' performance on internal and external datasets revealed that
synthetic data supplementation enhances model accuracy, particularly in
detecting less prevalent pathologies. Furthermore, models trained on synthetic
data alone approached the performance of those trained on real data. This
suggests that synthetic data can potentially compensate for real data shortages
in training robust DL models. However, despite promising outcomes, the
superiority of real data persists.
</p></li>
</ul>

<h3>Title: MDFL: Multi-domain Diffusion-driven Feature Learning. (arXiv:2311.09520v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09520">http://arxiv.org/abs/2311.09520</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09520]] MDFL: Multi-domain Diffusion-driven Feature Learning(http://arxiv.org/abs/2311.09520)</code></li>
<li>Summary: <p>High-dimensional images, known for their rich semantic information, are
widely applied in remote sensing and other fields. The spatial information in
these images reflects the object's texture features, while the spectral
information reveals the potential spectral representations across different
bands. Currently, the understanding of high-dimensional images remains limited
to a single-domain perspective with performance degradation. Motivated by the
masking texture effect observed in the human visual system, we present a
multi-domain diffusion-driven feature learning network (MDFL) , a scheme to
redefine the effective information domain that the model really focuses on.
This method employs diffusion-based posterior sampling to explicitly consider
joint information interactions between the high-dimensional manifold structures
in the spectral, spatial, and frequency domains, thereby eliminating the
influence of masking texture effects in visual models. Additionally, we
introduce a feature reuse mechanism to gather deep and raw features of
high-dimensional data. We demonstrate that MDFL significantly improves the
feature extraction performance of high-dimensional data, thereby providing a
powerful aid for revealing the intrinsic patterns and structures of such data.
The experimental results on three multi-modal remote sensing datasets show that
MDFL reaches an average overall accuracy of 98.25%, outperforming various
state-of-the-art baseline schemes. The code will be released, contributing to
the computer vision community.
</p></li>
</ul>

<h3>Title: DECDM: Document Enhancement using Cycle-Consistent Diffusion Models. (arXiv:2311.09625v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09625">http://arxiv.org/abs/2311.09625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09625]] DECDM: Document Enhancement using Cycle-Consistent Diffusion Models(http://arxiv.org/abs/2311.09625)</code></li>
<li>Summary: <p>The performance of optical character recognition (OCR) heavily relies on
document image quality, which is crucial for automatic document processing and
document intelligence. However, most existing document enhancement methods
require supervised data pairs, which raises concerns about data separation and
privacy protection, and makes it challenging to adapt these methods to new
domain pairs. To address these issues, we propose DECDM, an end-to-end
document-level image translation method inspired by recent advances in
diffusion models. Our method overcomes the limitations of paired training by
independently training the source (noisy input) and target (clean output)
models, making it possible to apply domain-specific diffusion models to other
pairs. DECDM trains on one dataset at a time, eliminating the need to scan both
datasets concurrently, and effectively preserving data privacy from the source
or target domain. We also introduce simple data augmentation strategies to
improve character-glyph conservation during translation. We compare DECDM with
state-of-the-art methods on multiple synthetic data and benchmark datasets,
such as document denoising and {\color{black}shadow} removal, and demonstrate
the superiority of performance quantitatively and qualitatively.
</p></li>
</ul>

<h3>Title: DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics. (arXiv:2311.09753v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09753">http://arxiv.org/abs/2311.09753</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09753]] DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics(http://arxiv.org/abs/2311.09753)</code></li>
<li>Summary: <p>Diffusion models have advanced generative AI significantly in terms of
editing and creating naturalistic images. However, efficiently improving
generated image quality is still of paramount interest. In this context, we
propose a generic "naturalness" preserving loss function, viz., kurtosis
concentration (KC) loss, which can be readily applied to any standard diffusion
model pipeline to elevate the image quality. Our motivation stems from the
projected kurtosis concentration property of natural images, which states that
natural images have nearly constant kurtosis values across different band-pass
versions of the image. To retain the "naturalness" of the generated images, we
enforce reducing the gap between the highest and lowest kurtosis values across
the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note
that our approach does not require any additional guidance like classifier or
classifier-free guidance to improve the image quality. We validate the proposed
approach for three diverse tasks, viz., (1) personalized few-shot finetuning
using text guidance, (2) unconditional image generation, and (3) image
super-resolution. Integrating the proposed KC loss has improved the perceptual
quality across all these tasks in terms of both FID, MUSIQ score, and user
evaluation.
</p></li>
</ul>

<h3>Title: Scene Text Image Super-resolution based on Text-conditional Diffusion Models. (arXiv:2311.09759v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09759">http://arxiv.org/abs/2311.09759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09759]] Scene Text Image Super-resolution based on Text-conditional Diffusion Models(http://arxiv.org/abs/2311.09759)</code></li>
<li>Summary: <p>Scene Text Image Super-resolution (STISR) has recently achieved great success
as a preprocessing method for scene text recognition. STISR aims to transform
blurred and noisy low-resolution (LR) text images in real-world settings into
clear high-resolution (HR) text images suitable for scene text recognition. In
this study, we leverage text-conditional diffusion models (DMs), known for
their impressive text-to-image synthesis capabilities, for STISR tasks. Our
experimental results revealed that text-conditional DMs notably surpass
existing STISR methods. Especially when texts from LR text images are given as
input, the text-conditional DMs are able to produce superior quality
super-resolution text images. Utilizing this capability, we propose a novel
framework for synthesizing LR-HR paired text image datasets. This framework
consists of three specialized text-conditional DMs, each dedicated to text
image synthesis, super-resolution, and image degradation. These three modules
are vital for synthesizing distinct LR and HR paired images, which are more
suitable for training STISR methods. Our experiments confirmed that these
synthesized image pairs significantly enhance the performance of STISR methods
in the TextZoom evaluation.
</p></li>
</ul>

<h3>Title: DSR-Diff: Depth Map Super-Resolution with Diffusion Model. (arXiv:2311.09919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09919">http://arxiv.org/abs/2311.09919</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09919]] DSR-Diff: Depth Map Super-Resolution with Diffusion Model(http://arxiv.org/abs/2311.09919)</code></li>
<li>Summary: <p>Color-guided depth map super-resolution (CDSR) improve the spatial resolution
of a low-quality depth map with the corresponding high-quality color map,
benefiting various applications such as 3D reconstruction, virtual reality, and
augmented reality. While conventional CDSR methods typically rely on
convolutional neural networks or transformers, diffusion models (DMs) have
demonstrated notable effectiveness in high-level vision tasks. In this work, we
present a novel CDSR paradigm that utilizes a diffusion model within the latent
space to generate guidance for depth map super-resolution. The proposed method
comprises a guidance generation network (GGN), a depth map super-resolution
network (DSRN), and a guidance recovery network (GRN). The GGN is specifically
designed to generate the guidance while managing its compactness. Additionally,
we integrate a simple but effective feature fusion module and a
transformer-style feature extraction module into the DSRN, enabling it to
leverage guided priors in the extraction, fusion, and reconstruction of
multi-model images. Taking into account both accuracy and efficiency, our
proposed method has shown superior performance in extensive experiments when
compared to state-of-the-art methods. Our codes will be made available at
https://github.com/shiyuan7/DSR-Diff.
</p></li>
</ul>

<h3>Title: TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection. (arXiv:2311.09999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09999">http://arxiv.org/abs/2311.09999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09999]] TransFusion -- A Transparency-Based Diffusion Model for Anomaly Detection(http://arxiv.org/abs/2311.09999)</code></li>
<li>Summary: <p>Surface anomaly detection is a vital component in manufacturing inspection.
Reconstructive anomaly detection methods restore the normal appearance of an
object, ideally modifying only the anomalous regions. Due to the limitations of
commonly used reconstruction architectures, the produced reconstructions are
often poor and either still contain anomalies or lack details in anomaly-free
regions. Recent reconstructive methods adopt diffusion models, however with the
standard diffusion process the problems are not adequately addressed. We
propose a novel transparency-based diffusion process, where the transparency of
anomalous regions is progressively increased, restoring their normal appearance
accurately and maintaining the appearance of anomaly-free regions without loss
of detail. We propose TRANSparency DifFUSION (TransFusion), a discriminative
anomaly detection method that implements the proposed diffusion process,
enabling accurate downstream anomaly detection. TransFusion achieves
state-of-the-art performance on both the VisA and the MVTec AD datasets, with
an image-level AUROC of 98.5% and 99.2%, respectively.
</p></li>
</ul>

<h3>Title: What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization. (arXiv:2311.09741v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09741">http://arxiv.org/abs/2311.09741</a></li>
<li>Code URL: https://github.com/lyh6560new/p3sum</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09741]] What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization(http://arxiv.org/abs/2311.09741)</code></li>
<li>Summary: <p>In this work, we take a first step towards designing summarization systems
that are faithful to the author's opinions and perspectives. Focusing on a case
study of preserving political perspectives in news summarization, we find that
existing approaches alter the political opinions and stances of news articles
in more than 50% of summaries, misrepresenting the intent and perspectives of
the news authors. We thus propose P^3Sum, a diffusion model-based summarization
approach controlled by political perspective classifiers. In P^3Sum, the
political leaning of a generated summary is iteratively evaluated at each
decoding step, and any drift from the article's original stance incurs a loss
back-propagated to the embedding layers, steering the political stance of the
summary at inference time. Extensive experiments on three news summarization
datasets demonstrate that P^3Sum outperforms state-of-the-art summarization
systems and large language models by up to 11.4% in terms of the success rate
of stance preservation, with on-par performance on standard summarization
utility metrics. These findings highlight the lacunae that even for
state-of-the-art models it is still challenging to preserve author perspectives
in news summarization, while P^3Sum presents an important first step towards
evaluating and developing summarization systems that are faithful to author
intent and perspectives.
</p></li>
</ul>

<h3>Title: Scalable Diffusion for Materials Generation. (arXiv:2311.09235v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09235">http://arxiv.org/abs/2311.09235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09235]] Scalable Diffusion for Materials Generation(http://arxiv.org/abs/2311.09235)</code></li>
<li>Summary: <p>Generative models trained on internet-scale data are capable of generating
novel and realistic texts, images, and videos. A natural next question is
whether these models can advance science, for example by generating novel
stable materials. Traditionally, models with explicit structures (e.g., graphs)
have been used in modeling structural relationships in scientific data (e.g.,
atoms and bonds in crystals), but generating structures can be difficult to
scale to large and complex systems. Another challenge in generating materials
is the mismatch between standard generative modeling metrics and downstream
applications. For instance, common metrics such as the reconstruction error do
not correlate well with the downstream goal of discovering stable materials. In
this work, we tackle the scalability challenge by developing a unified crystal
representation that can represent any crystal structure (UniMat), followed by
training a diffusion probabilistic model on these UniMat representations. Our
empirical results suggest that despite the lack of explicit structure modeling,
UniMat can generate high fidelity crystal structures from larger and more
complex chemical systems, outperforming previous graph-based approaches under
various generative modeling metrics. To better connect the generation quality
of materials to downstream applications, such as discovering novel stable
materials, we propose additional metrics for evaluating generative models of
materials, including per-composition formation energy and stability with
respect to convex hulls through decomposition energy from Density Function
Theory (DFT). Lastly, we show that conditional generation with UniMat can scale
to previously established crystal datasets with up to millions of crystals
structures, outperforming random structure search (the current leading method
for structure discovery) in discovering new stable materials.
</p></li>
</ul>

<h3>Title: Diffusion-Augmented Neural Processes. (arXiv:2311.09848v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09848">http://arxiv.org/abs/2311.09848</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09848]] Diffusion-Augmented Neural Processes(http://arxiv.org/abs/2311.09848)</code></li>
<li>Summary: <p>Over the last few years, Neural Processes have become a useful modelling tool
in many application areas, such as healthcare and climate sciences, in which
data are scarce and prediction uncertainty estimates are indispensable.
However, the current state of the art in the field (AR CNPs; Bruinsma et al.,
2023) presents a few issues that prevent its widespread deployment. This work
proposes an alternative, diffusion-based approach to NPs which, through
conditioning on noised datasets, addresses many of these limitations, whilst
also exceeding SOTA performance.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains. (arXiv:2311.09401v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09401">http://arxiv.org/abs/2311.09401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09401]] MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains(http://arxiv.org/abs/2311.09401)</code></li>
<li>Summary: <p>Medical imaging data is often siloed within hospitals, limiting the amount of
data available for specialized model development. With limited in-domain data,
one might hope to leverage larger datasets from related domains. In this paper,
we analyze the benefit of transferring self-supervised contrastive
representations from moment contrast (MoCo) pretraining on out-of-distribution
data to settings with limited data. We consider two X-ray datasets which image
different parts of the body, and compare transferring from each other to
transferring from ImageNet. We find that depending on quantity of labeled and
unlabeled data, contrastive pretraining on larger out-of-distribution datasets
can perform nearly as well or better than MoCo pretraining in-domain, and
pretraining on related domains leads to higher performance than if one were to
use the ImageNet pretrained weights. Finally, we provide a preliminary way of
quantifying similarity between datasets.
</p></li>
</ul>

<h3>Title: Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation. (arXiv:2311.09500v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09500">http://arxiv.org/abs/2311.09500</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09500]] Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation(http://arxiv.org/abs/2311.09500)</code></li>
<li>Summary: <p>This paper addresses the simulation-to-real domain gap in 6DoF PE, and
proposes a novel self-supervised keypoint radial voting-based 6DoF PE
framework, effectively narrowing this gap using a learnable kernel in RKHS. We
formulate this domain gap as a distance in high-dimensional feature space,
distinct from previous iterative matching methods. We propose an adapter
network, which evolves the network parameters from the source domain, which has
been massively trained on synthetic data with synthetic poses, to the target
domain, which is trained on real data. Importantly, the real data training only
uses pseudo-poses estimated by pseudo-keypoints, and thereby requires no real
groundtruth data annotations. RKHSPose achieves state-of-the-art performance on
three commonly used 6DoF PE datasets including LINEMOD (+4.2%), Occlusion
LINEMOD (+2%), and YCB-Video (+3%). It also compares favorably to fully
supervised methods on all six applicable BOP core datasets, achieving within
-10.8% to -0.3% of the top fully supervised results.
</p></li>
</ul>

<h3>Title: Robust Contrastive Learning With Theory Guarantee. (arXiv:2311.09671v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09671">http://arxiv.org/abs/2311.09671</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09671]] Robust Contrastive Learning With Theory Guarantee(http://arxiv.org/abs/2311.09671)</code></li>
<li>Summary: <p>Contrastive learning (CL) is a self-supervised training paradigm that allows
us to extract meaningful features without any label information. A typical CL
framework is divided into two phases, where it first tries to learn the
features from unlabelled data, and then uses those features to train a linear
classifier with the labeled data. While a fair amount of existing theoretical
works have analyzed how the unsupervised loss in the first phase can support
the supervised loss in the second phase, none has examined the connection
between the unsupervised loss and the robust supervised loss, which can shed
light on how to construct an effective unsupervised loss for the first phase of
CL. To fill this gap, our work develops rigorous theories to dissect and
identify which components in the unsupervised loss can help improve the robust
supervised loss and conduct proper experiments to verify our findings.
</p></li>
</ul>

<h3>Title: From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning. (arXiv:2311.09974v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09974">http://arxiv.org/abs/2311.09974</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09974]] From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning(http://arxiv.org/abs/2311.09974)</code></li>
<li>Summary: <p>In recent years, self-supervised contrastive learning has emerged as a
distinguished paradigm in the artificial intelligence landscape. It facilitates
unsupervised feature learning through contrastive delineations at the instance
level. However, crafting an effective self-supervised paradigm remains a
pivotal challenge within this field. This paper delves into two crucial factors
impacting self-supervised contrastive learning-bach size and pretext tasks, and
from a data processing standpoint, proposes an adaptive technique of batch
fusion. The proposed method, via dimensionality reduction and reconstruction of
batch data, enables formerly isolated individual data to partake in intra-batch
communication through the Embedding Layer. Moreover, it adaptively amplifies
the self-supervised feature encoding capability as the training progresses. We
conducted a linear classification test of this method based on the classic
contrastive learning framework on ImageNet-1k. The empirical findings
illustrate that our approach achieves state-of-the-art performance under
equitable comparisons. Benefiting from its "plug-and-play" characteristics, we
further explored other contrastive learning methods. On the ImageNet-100,
compared to the original performance, the top1 has seen a maximum increase of
1.25%. We suggest that the proposed method may contribute to the advancement of
data-driven self-supervised learning research, bringing a fresh perspective to
this community.
</p></li>
</ul>

<h3>Title: Self-supervised learning of multi-omics embeddings in the low-label, high-data regime. (arXiv:2311.09962v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09962">http://arxiv.org/abs/2311.09962</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09962]] Self-supervised learning of multi-omics embeddings in the low-label, high-data regime(http://arxiv.org/abs/2311.09962)</code></li>
<li>Summary: <p>Contrastive, self-supervised learning (SSL) is used to train a model that
predicts cancer type from miRNA, mRNA or RPPA expression data. This model, a
pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost,
standard benchmarks for tabular data, when labelled samples are scarce but the
number of unlabelled samples is high. This is despite the fact that the
datasets we use have $\mathcal{O}(10^{1})$ classes and
$\mathcal{O}(10^{2})-\mathcal{O}(10^{4})$ features. After demonstrating the
efficacy of our chosen method of self-supervised pretraining, we investigate
SSL for multi-modal models. A late-fusion model is proposed, where each omics
is passed through its own sub-network, the outputs of which are averaged and
passed to the pretraining or downstream objective function. Multi-modal
pretraining is shown to improve predictions from a single omics, and we argue
that this is useful for datasets with many unlabelled multi-modal samples, but
few labelled unimodal samples. Additionally, we show that pretraining each
omics-specific module individually is highly effective. This enables the
application of the proposed model in a variety of contexts where a large amount
of unlabelled data is available from each omics, but only a few labelled
samples.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Investigating the Emergent Audio Classification Ability of ASR Foundation Models. (arXiv:2311.09363v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09363">http://arxiv.org/abs/2311.09363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09363]] Investigating the Emergent Audio Classification Ability of ASR Foundation Models(http://arxiv.org/abs/2311.09363)</code></li>
<li>Summary: <p>Text and vision foundation models can perform many tasks in a zero-shot
setting, a desirable property that enables these systems to be applied in
general and low-resource settings. However, there has been significantly less
work on the zero-shot abilities of ASR foundation models, with these systems
typically fine-tuned to specific tasks or constrained to applications that
match their training criterion and data annotation. In this work we investigate
the ability of Whisper and MMS, ASR foundation models trained primarily for
speech recognition, to perform zero-shot audio classification. We use simple
template-based text prompts at the decoder and use the resulting decoding
probabilities to generate zero-shot predictions. Without training the model on
extra data or adding any new parameters, we demonstrate that Whisper shows
promising zero-shot classification performance on a range of 8
audio-classification datasets, outperforming existing state-of-the-art
zero-shot baseline's accuracy by an average of 9%. One important step to unlock
the emergent ability is debiasing, where a simple unsupervised reweighting
method of the class probabilities yields consistent significant performance
gains. We further show that performance increases with model size, implying
that as ASR foundation models scale up, they may exhibit improved zero-shot
performance.
</p></li>
</ul>

<h3>Title: PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models. (arXiv:2311.09861v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09861">http://arxiv.org/abs/2311.09861</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09861]] PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models(http://arxiv.org/abs/2311.09861)</code></li>
<li>Summary: <p>As Large Language Models (LLMs) are becoming prevalent in various fields,
there is an urgent need for improved NLP benchmarks that encompass all the
necessary knowledge of individual discipline. Many contemporary benchmarks for
foundational models emphasize a broad range of subjects but often fall short in
presenting all the critical subjects and encompassing necessary professional
knowledge of them. This shortfall has led to skewed results, given that LLMs
exhibit varying performance across different subjects and knowledge areas. To
address this issue, we present psybench, the first comprehensive Chinese
evaluation suite that covers all the necessary knowledge required for graduate
entrance exams. psybench offers a deep evaluation of a model's strengths and
weaknesses in psychology through multiple-choice questions. Our findings show
significant differences in performance across different sections of a subject,
highlighting the risk of skewed results when the knowledge in test sets is not
balanced. Notably, only the ChatGPT model reaches an average accuracy above
$70\%$, indicating that there is still plenty of room for improvement. We
expect that psybench will help to conduct thorough evaluations of base models'
strengths and weaknesses and assist in practical application in the field of
psychology.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: On the Quantification of Image Reconstruction Uncertainty without Training Data. (arXiv:2311.09639v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09639">http://arxiv.org/abs/2311.09639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09639]] On the Quantification of Image Reconstruction Uncertainty without Training Data(http://arxiv.org/abs/2311.09639)</code></li>
<li>Summary: <p>Computational imaging plays a pivotal role in determining hidden information
from sparse measurements. A robust inverse solver is crucial to fully
characterize the uncertainty induced by these measurements, as it allows for
the estimation of the complete posterior of unrecoverable targets. This, in
turn, facilitates a probabilistic interpretation of observational data for
decision-making. In this study, we propose a deep variational framework that
leverages a deep generative model to learn an approximate posterior
distribution to effectively quantify image reconstruction uncertainty without
the need for training data. We parameterize the target posterior using a
flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve
accurate uncertainty estimation. To bolster stability, we introduce a robust
flow-based model with bi-directional regularization and enhance expressivity
through gradient boosting. Additionally, we incorporate a space-filling design
to achieve substantial variance reduction on both latent prior space and target
posterior space. We validate our method on several benchmark tasks and two
real-world applications, namely fastMRI and black hole image reconstruction.
Our results indicate that our method provides reliable and high-quality image
reconstruction with robust uncertainty estimation.
</p></li>
</ul>

<h3>Title: DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's Distance. (arXiv:2311.09998v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09998">http://arxiv.org/abs/2311.09998</a></li>
<li>Code URL: https://github.com/atulkumarin/deepemd</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09998]] DeepEMD: A Transformer-based Fast Estimation of the Earth Mover's Distance(http://arxiv.org/abs/2311.09998)</code></li>
<li>Summary: <p>The Earth Mover's Distance (EMD) is the measure of choice between point
clouds. However the computational cost to compute it makes it prohibitive as a
training loss, and the standard approach is to use a surrogate such as the
Chamfer distance. We propose an attention-based model to compute an accurate
approximation of the EMD that can be used as a training loss for generative
models. To get the necessary accurate estimation of the gradients we train our
model to explicitly compute the matching between point clouds instead of EMD
itself. We cast this new objective as the estimation of an attention matrix
that approximates the ground truth matching matrix. Experiments show that this
model provides an accurate estimate of the EMD and its gradient with a wall
clock speed-up of more than two orders of magnitude with respect to the exact
Hungarian matching algorithm and one order of magnitude with respect to the
standard approximate Sinkhorn algorithm, allowing in particular to train a
point cloud VAE with the EMD itself. Extensive evaluation show the remarkable
behaviour of this model when operating out-of-distribution, a key requirement
for a distance surrogate. Finally, the model generalizes very well to point
clouds during inference several times larger than during training.
</p></li>
</ul>

<h3>Title: Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation. (arXiv:2311.09467v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09467">http://arxiv.org/abs/2311.09467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09467]] Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation(http://arxiv.org/abs/2311.09467)</code></li>
<li>Summary: <p>Neural knowledge-to-text generation models often struggle to faithfully
generate descriptions for the input facts: they may produce hallucinations that
contradict the given facts, or describe facts not present in the input. To
reduce hallucinations, we propose a novel decoding method, TWEAK (Think While
Effectively Articulating Knowledge). TWEAK treats the generated sequences at
each decoding step and its future sequences as hypotheses, and ranks each
generation candidate based on how well their corresponding hypotheses support
the input facts using a Hypothesis Verification Model (HVM). We first
demonstrate the effectiveness of TWEAK by using a Natural Language Inference
(NLI) model as the HVM and report improved faithfulness with minimal impact on
the quality. We then replace the NLI model with our task-specific HVM trained
with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which
pairs input facts with their faithful and hallucinated descriptions with the
hallucinated spans marked. The new HVM improves the faithfulness and the
quality further and runs faster. Overall the best TWEAK variants improve on
average 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and
TekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality
measured by BERTScore over the same datasets. Since TWEAK is a decoding-only
approach, it can be integrated with any neural generative model without
retraining.
</p></li>
</ul>

<h3>Title: Prompt Optimisation with Random Sampling. (arXiv:2311.09569v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09569">http://arxiv.org/abs/2311.09569</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09569]] Prompt Optimisation with Random Sampling(http://arxiv.org/abs/2311.09569)</code></li>
<li>Summary: <p>Using the generative nature of a language model to generate task-relevant
separators has shown competitive results compared to human-curated prompts like
"TL;DR". We demonstrate that even randomly chosen tokens from the vocabulary as
separators can achieve near-state-of-the-art performance. We analyse this
phenomenon in detail using three different random generation strategies,
establishing that the language space is rich with potential good separators,
regardless of the underlying language model size. These observations challenge
the common assumption that an effective prompt should be human-readable or
task-relevant. Experimental results show that using random separators leads to
an average 16% relative improvement across nine text classification tasks on
seven language models, compared to human-curated separators, and is on par with
automatic prompt searching methods.
</p></li>
</ul>

<h3>Title: GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding. (arXiv:2311.09707v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09707">http://arxiv.org/abs/2311.09707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09707]] GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding(http://arxiv.org/abs/2311.09707)</code></li>
<li>Summary: <p>Language models can serve as a valuable tool for software developers to
increase productivity. Large generative models can be used for code generation
and code completion, while smaller encoder-only models are capable of
performing code search tasks using natural language queries.These capabilities
are heavily influenced by the quality and diversity of the available training
data. Source code datasets used for training usually focus on the most popular
languages and testing is mostly conducted on the same distributions, often
overlooking low-resource programming languages. Motivated by the NLP
generalization taxonomy proposed by Hupkes et.\,al., we propose a new benchmark
dataset called GenCodeSearchNet (GeCS) which builds upon existing natural
language code search datasets to systemically evaluate the programming language
understanding generalization capabilities of language models. As part of the
full dataset, we introduce a new, manually curated subset StatCodeSearch that
focuses on R, a popular but so far underrepresented programming language that
is often used by researchers outside the field of computer science. For
evaluation and comparison, we collect several baseline results using fine-tuned
BERT-style models and GPT-style large language models in a zero-shot setting.
</p></li>
</ul>

<h3>Title: CARE: Extracting Experimental Findings From Clinical Literature. (arXiv:2311.09736v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09736">http://arxiv.org/abs/2311.09736</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09736]] CARE: Extracting Experimental Findings From Clinical Literature(http://arxiv.org/abs/2311.09736)</code></li>
<li>Summary: <p>Extracting fine-grained experimental findings from literature can provide
massive utility for scientific applications. Prior work has focused on
developing annotation schemas and datasets for limited aspects of this problem,
leading to simpler information extraction datasets which do not capture the
real-world complexity and nuance required for this task. Focusing on
biomedicine, this work presents CARE (Clinical Aggregation-oriented Result
Extraction) -- a new IE dataset for the task of extracting clinical findings.
We develop a new annotation schema capturing fine-grained findings as n-ary
relations between entities and attributes, which includes phenomena challenging
for current IE systems such as discontinuous entity spans, nested relations,
and variable arity n-ary relations. Using this schema, we collect extensive
annotations for 700 abstracts from two sources: clinical trials and case
reports. We also benchmark the performance of various state-of-the-art IE
systems on our dataset, including extractive models and generative LLMs in
fully supervised and limited data settings. Our results demonstrate the
difficulty of our dataset -- even SOTA models such as GPT4 struggle,
particularly on relation extraction. We release our annotation schema and CARE
to encourage further research on extracting and aggregating scientific findings
from literature.
</p></li>
</ul>

<h3>Title: LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores. (arXiv:2311.09766v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09766">http://arxiv.org/abs/2311.09766</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09766]] LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores(http://arxiv.org/abs/2311.09766)</code></li>
<li>Summary: <p>Automatic evaluation of generated textual content presents an ongoing
challenge within the field of NLP. Given the impressive capabilities of modern
language models (LMs) across diverse NLP tasks, there is a growing trend to
employ these models in creating innovative evaluation metrics for automated
assessment of generation tasks. This paper investigates a pivotal question: Do
language model-driven evaluation metrics inherently exhibit bias favoring texts
generated by the same underlying language model? Specifically, we assess
whether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and
GPTScore--demonstrate a favorable bias toward their respective underlying LMs
in the context of summarization tasks. Our findings unveil a latent bias,
particularly pronounced when such evaluation metrics are used in an
reference-free manner without leveraging gold summaries. These results
underscore that assessments provided by generative evaluation models can be
influenced by factors beyond the inherent text quality, highlighting the
necessity of developing more dependable evaluation protocols in the future.
</p></li>
</ul>

<h3>Title: Language Generation from Human Brain Activities. (arXiv:2311.09889v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09889">http://arxiv.org/abs/2311.09889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09889]] Language Generation from Human Brain Activities(http://arxiv.org/abs/2311.09889)</code></li>
<li>Summary: <p>Generating human language through non-invasive brain-computer interfaces
(BCIs) has the potential to unlock many applications, such as serving disabled
patients and improving communication. Currently, however, generating language
via BCIs has been previously successful only within a classification setup for
selecting pre-generated sentence continuation candidates with the most likely
cortical semantic representation. Inspired by recent research that revealed
associations between the brain and the large computational language models, we
propose a generative language BCI that utilizes the capacity of a large
language model (LLM) jointly with a semantic brain decoder to directly generate
language from functional magnetic resonance imaging (fMRI) input. The proposed
model can generate coherent language sequences aligned with the semantic
content of visual or auditory language stimuli perceived, without prior
knowledge of any pre-generated candidates. We compare the language generated
from the presented model with a random control, pre-generated language
selection approach, and a standard LLM, which generates common coherent text
solely based on the next word likelihood according to statistical language
training data. The proposed model is found to generate language that is more
aligned with semantic stimulus in response to which brain input is sampled. Our
findings demonstrate the potential and feasibility of employing BCIs in direct
language generation.
</p></li>
</ul>

<h3>Title: Generative AI for Hate Speech Detection: Evaluation and Findings. (arXiv:2311.09993v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09993">http://arxiv.org/abs/2311.09993</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09993]] Generative AI for Hate Speech Detection: Evaluation and Findings(http://arxiv.org/abs/2311.09993)</code></li>
<li>Summary: <p>Automatic hate speech detection using deep neural models is hampered by the
scarcity of labeled datasets, leading to poor generalization. To mitigate this
problem, generative AI has been utilized to generate large amounts of synthetic
hate speech sequences from available labeled examples, leveraging the generated
data in finetuning large pre-trained language models (LLMs). In this chapter,
we provide a review of relevant methods, experimental setups and evaluation of
this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT,
we apply and evaluate the impact of train set augmentation with generated data
using LLMs that have been already adapted for hate detection, including
RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical
study corroborates our previous findings, showing that this approach improves
hate speech generalization, boosting recall performance across data
distributions. In addition, we explore and compare the performance of the
finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results
demonstrate that while better generalization is achieved using the GPT-3.5
model, it achieves mediocre recall and low precision on most datasets. It is an
open question whether the sensitivity of models such as GPT-3.5, and onward,
can be improved using similar techniques of text generation.
</p></li>
</ul>

<h3>Title: SynDiffix: More accurate synthetic structured data. (arXiv:2311.09628v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09628">http://arxiv.org/abs/2311.09628</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09628]] SynDiffix: More accurate synthetic structured data(http://arxiv.org/abs/2311.09628)</code></li>
<li>Summary: <p>This paper introduces SynDiffix, a mechanism for generating statistically
accurate, anonymous synthetic data for structured data. Recent open source and
commercial systems use Generative Adversarial Networks or Transformed Auto
Encoders to synthesize data, and achieve anonymity through
overfitting-avoidance. By contrast, SynDiffix exploits traditional mechanisms
of aggregation, noise addition, and suppression among others. Compared to
CTGAN, ML models generated from SynDiffix are twice as accurate, marginal and
column pairs data quality is one to two orders of magnitude more accurate, and
execution time is two orders of magnitude faster. Compared to the best
commercial product we measured (MostlyAI), ML model accuracy is comparable,
marginal and pairs accuracy is 5 to 10 times better, and execution time is an
order of magnitude faster. Similar to the other approaches, SynDiffix
anonymization is very strong. This paper describes SynDiffix and compares its
performance with other popular open source and commercial systems.
</p></li>
</ul>

<h3>Title: Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production. (arXiv:2311.09333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09333">http://arxiv.org/abs/2311.09333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09333]] Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production(http://arxiv.org/abs/2311.09333)</code></li>
<li>Summary: <p>A significant challenge for predictive maintenance in the pulp-and-paper
industry is the infrequency of paper breaks during the production process. In
this article, operational data is analyzed from a paper manufacturing machine
in which paper breaks are relatively rare but have a high economic impact.
Utilizing a dataset comprising 18,398 instances derived from a quality
assurance protocol, we address the scarcity of break events (124 cases) that
pose a challenge for machine learning predictive models. With the help of
Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority
Oversampling Technique (SMOTE), we implement a novel data augmentation
framework. This method ensures that the synthetic data mirrors the distribution
of the real operational data but also seeks to enhance the performance metrics
of predictive modeling. Before and after the data augmentation, we evaluate
three different machine learning algorithms-Decision Trees (DT), Random Forest
(RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our
study achieved significant improvements in predictive maintenance performance
metrics. The efficacy of CTGAN in addressing data scarcity was evident, with
the models' detection of machine breaks (Class 1) improving by over 30% for
Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression.
With this methodological advancement, this study contributes to industrial
quality control and maintenance scheduling by addressing rare event prediction
in manufacturing processes.
</p></li>
</ul>

<h3>Title: A Knowledge Distillation Approach for Sepsis Outcome Prediction from Multivariate Clinical Time Series. (arXiv:2311.09566v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09566">http://arxiv.org/abs/2311.09566</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09566]] A Knowledge Distillation Approach for Sepsis Outcome Prediction from Multivariate Clinical Time Series(http://arxiv.org/abs/2311.09566)</code></li>
<li>Summary: <p>Sepsis is a life-threatening condition triggered by an extreme infection
response. Our objective is to forecast sepsis patient outcomes using their
medical history and treatments, while learning interpretable state
representations to assess patients' risks in developing various adverse
outcomes. While neural networks excel in outcome prediction, their limited
interpretability remains a key issue. In this work, we use knowledge
distillation via constrained variational inference to distill the knowledge of
a powerful "teacher" neural network model with high predictive power to train a
"student" latent variable model to learn interpretable hidden state
representations to achieve high predictive performance for sepsis outcome
prediction. Using real-world data from the MIMIC-IV database, we trained an
LSTM as the "teacher" model to predict mortality for sepsis patients, given
information about their recent history of vital signs, lab values and
treatments. For our student model, we use an autoregressive hidden Markov model
(AR-HMM) to learn interpretable hidden states from patients' clinical time
series, and use the posterior distribution of the learned state representations
to predict various downstream outcomes, including hospital mortality, pulmonary
edema, need for diuretics, dialysis, and mechanical ventilation. Our results
show that our approach successfully incorporates the constraint to achieve high
predictive power similar to the teacher model, while maintaining the generative
performance.
</p></li>
</ul>

<h3>Title: GEO: Generative Engine Optimization. (arXiv:2311.09735v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09735">http://arxiv.org/abs/2311.09735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09735]] GEO: Generative Engine Optimization(http://arxiv.org/abs/2311.09735)</code></li>
<li>Summary: <p>The advent of large language models (LLMs) has ushered in a new paradigm of
search engines that use generative models to gather and summarize information
to answer user queries. This emerging technology, which we formalize under the
unified framework of Generative Engines (GEs), has the potential to generate
accurate and personalized responses, and is rapidly replacing traditional
search engines like Google and Bing. Generative Engines typically satisfy
queries by synthesizing information from multiple sources and summarizing them
with the help of LLMs. While this shift significantly improves \textit{user}
utility and \textit{generative search engine} traffic, it results in a huge
challenge for the third stakeholder -- website and content creators. Given the
black-box and fast-moving nature of Generative Engines, content creators have
little to no control over when and how their content is displayed. With
generative engines here to stay, the right tools should be provided to ensure
that creator economy is not severely disadvantaged. To address this, we
introduce Generative Engine Optimization (GEO), a novel paradigm to aid content
creators in improving the visibility of their content in Generative Engine
responses through a black-box optimization framework for optimizing and
defining visibility metrics. We facilitate systematic evaluation in this new
paradigm by introducing GEO-bench, a benchmark of diverse user queries across
multiple domains, coupled with sources required to answer these queries.
Through rigorous evaluation, we show that GEO can boost visibility by up to
40\% in generative engine responses. Moreover, we show the efficacy of these
strategies varies across domains, underscoring the need for domain-specific
methods. Our work opens a new frontier in the field of information discovery
systems, with profound implications for generative engines and content
creators.
</p></li>
</ul>

<h2>anomaly</h2>
<h2>in-context</h2>
<h3>Title: Auto-ICL: In-Context Learning without Human Supervision. (arXiv:2311.09263v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09263">http://arxiv.org/abs/2311.09263</a></li>
<li>Code URL: https://github.com/ecielyang/auto-icl</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09263]] Auto-ICL: In-Context Learning without Human Supervision(http://arxiv.org/abs/2311.09263)</code></li>
<li>Summary: <p>In the era of Large Language Models (LLMs), human-computer interaction has
evolved towards natural language, offering unprecedented flexibility. Despite
this, LLMs are heavily reliant on well-structured prompts to function
efficiently within the realm of In-Context Learning. Vanilla In-Context
Learning relies on human-provided contexts, such as labeled examples, explicit
instructions, or other guiding mechanisms that shape the model's outputs. To
address this challenge, our study presents a universal framework named
Automatic In-Context Learning. Upon receiving a user's request, we ask the
model to independently generate examples, including labels, instructions, or
reasoning pathways. The model then leverages this self-produced context to
tackle the given problem. Our approach is universally adaptable and can be
implemented in any setting where vanilla In-Context Learning is applicable. We
demonstrate that our method yields strong performance across a range of tasks,
standing up well when compared to existing methods.
</p></li>
</ul>

<h3>Title: Leveraging Code to Improve In-context Learning for Semantic Parsing. (arXiv:2311.09519v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09519">http://arxiv.org/abs/2311.09519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09519]] Leveraging Code to Improve In-context Learning for Semantic Parsing(http://arxiv.org/abs/2311.09519)</code></li>
<li>Summary: <p>In-context learning (ICL) is an appealing approach for semantic parsing due
to its few-shot nature and improved generalization. However, learning to parse
to rare domain-specific languages (DSLs) from just a few demonstrations is
challenging, limiting the performance of even the most capable LLMs. In this
work, we improve the effectiveness of ICL for semantic parsing by (1) using
general-purpose programming languages such as Python instead of DSLs, and (2)
augmenting prompts with a structured domain description that includes, e.g.,
the available classes and functions. We show that both these changes
significantly improve accuracy across three popular datasets. Combined, they
lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional
split), nearly closing the performance gap between easier i.i.d.\ and harder
compositional splits when used with a strong model, and reducing the need for a
large number of demonstrations. We find that the resemblance of the target
parse language to general-purpose code is a more important factor than the
language's popularity in pre-training corpora. Our findings provide an improved
methodology for building semantic parsers in the modern context of ICL with
LLMs.
</p></li>
</ul>

<h3>Title: Pachinko: Patching Interpretable QA Models through Natural Language Feedback. (arXiv:2311.09558v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09558">http://arxiv.org/abs/2311.09558</a></li>
<li>Code URL: https://github.com/chaitanyamalaviya/pachinko</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09558]] Pachinko: Patching Interpretable QA Models through Natural Language Feedback(http://arxiv.org/abs/2311.09558)</code></li>
<li>Summary: <p>Eliciting feedback from end users of NLP models can be beneficial for
improving models. However, how should we present model responses to users so
they are most amenable to be corrected from user feedback? Further, what
properties do users value to understand and trust responses? We answer these
questions by analyzing the effect of rationales generated by QA models to
support their answers. We specifically consider decomposed question-answering
models that first extract an intermediate rationale based on a context and a
question and then use solely this rationale to answer the question. A rationale
outlines the approach followed by the model to answer the question. Our work
considers various formats of these rationales that vary according to
well-defined properties of interest. We sample these rationales from large
language models using few-shot prompting for two reading comprehension
datasets, and then perform two user studies. In the first one, we present users
with incorrect answers and corresponding rationales of various formats and ask
them to provide natural language feedback to revise the rationale. We then
measure the effectiveness of this feedback in patching these rationales through
in-context learning. The second study evaluates how well different rationale
formats enable users to understand and trust model answers, when they are
correct. We find that rationale formats significantly affect how easy it is (1)
for users to give feedback for rationales, and (2) for models to subsequently
execute this feedback. In addition to influencing critiquablity, certain
formats significantly enhance user reported understanding and trust of model
outputs.
</p></li>
</ul>

<h3>Title: Crafting In-context Examples according to LMs' Parametric Knowledge. (arXiv:2311.09579v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09579">http://arxiv.org/abs/2311.09579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09579]] Crafting In-context Examples according to LMs' Parametric Knowledge(http://arxiv.org/abs/2311.09579)</code></li>
<li>Summary: <p>In-context learning has been applied to knowledge-rich tasks such as question
answering. In such scenarios, in-context examples are used to trigger a
behaviour in the language model: namely, it should surface information stored
in its parametric knowledge. We study the construction of in-context example
sets, with a focus on the parametric knowledge of the model regarding
in-context examples. We identify 'known' examples, where models can correctly
answer from its parametric knowledge, and 'unknown' ones. Our experiments show
that prompting with 'unknown' examples decreases the performance, potentially
as it encourages hallucination rather than searching its parametric knowledge.
Constructing an in-context example set that presents both known and unknown
information performs the best across diverse settings. We perform analysis on
three multi-answer question answering datasets, which allows us to further
study answer set ordering strategies based on the LM's knowledge about each
answer. Together, our study sheds lights on how to best construct in-context
example sets for knowledge-rich tasks.
</p></li>
</ul>

<h3>Title: Multi-Step Dialogue Workflow Action Prediction. (arXiv:2311.09593v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09593">http://arxiv.org/abs/2311.09593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09593]] Multi-Step Dialogue Workflow Action Prediction(http://arxiv.org/abs/2311.09593)</code></li>
<li>Summary: <p>In task-oriented dialogue, a system often needs to follow a sequence of
actions, called a workflow, that complies with a set of guidelines in order to
complete a task. In this paper, we propose the novel problem of multi-step
workflow action prediction, in which the system predicts multiple future
workflow actions. Accurate prediction of multiple steps allows for multi-turn
automation, which can free up time to focus on more complex tasks. We propose
three modeling approaches that are simple to implement yet lead to more action
automation: 1) fine-tuning on a training dataset, 2) few-shot in-context
learning leveraging retrieval and large language model prompting, and 3)
zero-shot graph traversal, which aggregates historical action sequences into a
graph for prediction. We show that multi-step action prediction produces
features that improve accuracy on downstream dialogue tasks like predicting
task success, and can increase automation of steps by 20% without requiring as
much feedback from a human overseeing the system.
</p></li>
</ul>

<h3>Title: Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals. (arXiv:2311.09605v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09605">http://arxiv.org/abs/2311.09605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09605]] Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals(http://arxiv.org/abs/2311.09605)</code></li>
<li>Summary: <p>The inevitable appearance of spurious correlations in training datasets hurts
the generalization of NLP models on unseen data. Previous work has found that
datasets with paired inputs are prone to correlations between a specific part
of the input (e.g., the hypothesis in NLI) and the label; consequently, models
trained only on those outperform chance. Are these correlations picked up by
models trained on the full input data? To address this question, we propose a
new evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses
counterfactuals by replacing part of the input with its counterpart from a
different example (subject to some restrictions), expecting an attentive model
to change its prediction. Using CAT, we systematically investigate established
supervised and in-context learning models on ten datasets spanning four tasks:
natural language inference, reading comprehension, paraphrase detection, and
visual &amp; language reasoning. CAT reveals that reliance on such correlations is
mainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive
with an increased number of demonstrations, while its accuracy on the test data
improves. Our results demonstrate that augmenting training or demonstration
data with counterfactuals is effective in improving models' attentiveness. We
show that models' attentiveness measured by CAT reveals different conclusions
from solely measuring correlations in data.
</p></li>
</ul>

<h3>Title: GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks. (arXiv:2311.09606v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09606">http://arxiv.org/abs/2311.09606</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09606]] GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks(http://arxiv.org/abs/2311.09606)</code></li>
<li>Summary: <p>Large language models (LLMs) have the ability to perform in-context learning
(ICL) of new tasks by conditioning on prompts comprising a few task examples.
This work studies the problem of selecting the best examples given a candidate
pool to improve ICL performance on given a test input. Existing approaches
either require training with feedback from a much larger LLM or are
computationally expensive. We propose a novel metric, GistScore, based on
Example Gisting, a novel approach for training example retrievers for ICL using
an attention bottleneck via Gisting, a recent technique for compressing task
instructions. To tradeoff performance with ease of use, we experiment with both
fine-tuning gist models on each dataset and multi-task training a single model
on a large collection of datasets. On 21 diverse datasets spanning 9 tasks, we
show that our fine-tuned models get state-of-the-art ICL performance with 20%
absolute average gain over off-the-shelf retrievers and 7% over the best prior
methods. Our multi-task model generalizes well out-of-the-box to new task
categories, datasets, and prompt templates with retrieval speeds that are
consistently thousands of times faster than the best prior training-free
method.
</p></li>
</ul>

<h3>Title: Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning. (arXiv:2311.09619v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09619">http://arxiv.org/abs/2311.09619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09619]] Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning(http://arxiv.org/abs/2311.09619)</code></li>
<li>Summary: <p>In-Context Learning (ICL) is an emergent capability of Large Language Models
(LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new
tasks. Previous studies have shown that using LLMs' outputs as labels is
effective in training models to select demonstrations. Such a label is expected
to estimate utility of a demonstration in ICL; however, it has not been well
understood how different labeling strategies affect results on target tasks.
This paper presents an analysis on different utility functions by focusing on
LLMs' output probability given ground-truth output, and task-specific reward
given LLMs' prediction. Unlike the previous work, we introduce a novel labeling
method, incremental utility, which estimates how much incremental knowledge is
brought into the LLMs by a demonstration. We conduct experiments with
instruction-tuned LLMs on binary/multi-class classification, segmentation, and
translation across Arabic, English, Finnish, Japanese, and Spanish. Our results
show that (1) the probability is effective when the probability values are
distributed across the whole value range (on the classification tasks), and (2)
the downstream metric is more robust when nuanced reward values are provided
with long outputs (on the segmentation and translation tasks). We then show
that the proposed incremental utility further helps ICL by contrasting how the
LLMs perform with and without the demonstrations.
</p></li>
</ul>

<h3>Title: Evaluating In-Context Learning of Libraries for Code Generation. (arXiv:2311.09635v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09635">http://arxiv.org/abs/2311.09635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09635]] Evaluating In-Context Learning of Libraries for Code Generation(http://arxiv.org/abs/2311.09635)</code></li>
<li>Summary: <p>Contemporary Large Language Models (LLMs) exhibit a high degree of code
generation and comprehension capability. A particularly promising area is their
ability to interpret code modules from unfamiliar libraries for solving
user-instructed tasks. Recent work has shown that large proprietary LLMs can
learn novel library usage in-context from demonstrations. These results raise
several open questions: whether demonstrations of library usage is required,
whether smaller (and more open) models also possess such capabilities, etc. In
this work, we take a broader approach by systematically evaluating a diverse
array of LLMs across three scenarios reflecting varying levels of domain
specialization to understand their abilities and limitations in generating code
based on libraries defined in-context. Our results show that even smaller
open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding
of novel code libraries based on specification presented in-context. Our
findings further reveal that LLMs exhibit a surprisingly high proficiency in
learning novel library modules even when provided with just natural language
descriptions or raw code implementations of the functions, which are often
cheaper to obtain than demonstrations. Overall, our results pave the way for
harnessing LLMs in more adaptable and dynamic coding environments.
</p></li>
</ul>

<h3>Title: ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification. (arXiv:2311.09649v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09649">http://arxiv.org/abs/2311.09649</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09649]] ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification(http://arxiv.org/abs/2311.09649)</code></li>
<li>Summary: <p>This paper focuses on the task of Extreme Multi-Label Classification (XMC)
whose goal is to predict multiple labels for each instance from an extremely
large label space. While existing research has primarily focused on fully
supervised XMC, real-world scenarios often lack complete supervision signals,
highlighting the importance of zero-shot settings. Given the large label space,
utilizing in-context learning approaches is not trivial. We address this issue
by introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage
framework that cuts down the search space by generating a set of candidate
labels through incontext learning and then reranks them. Extensive experiments
suggest that ICXML advances the state of the art on two diverse public
benchmarks.
</p></li>
</ul>

<h3>Title: More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering. (arXiv:2311.09782v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09782">http://arxiv.org/abs/2311.09782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09782]] More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering(http://arxiv.org/abs/2311.09782)</code></li>
<li>Summary: <p>While most existing works on LLM prompt-engineering focus only on how to
select a better set of data samples inside one single prompt input (In-Context
Learning or ICL), why can't we design and leverage multiple prompt inputs
together to further improve the LLM performance? In this work, we propose
In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to
produce the most confident prediction results by optimizing the construction of
multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL
and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate
that ICS can consistently enhance LLM's prediction performance and confidence.
An ablation study suggests that a diversity-based ICS strategy may further
improve LLM's performance, which sheds light on a new yet promising future
research direction.
</p></li>
</ul>

<h3>Title: Hijacking Large Language Models via Adversarial In-Context Learning. (arXiv:2311.09948v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.09948">http://arxiv.org/abs/2311.09948</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.09948]] Hijacking Large Language Models via Adversarial In-Context Learning(http://arxiv.org/abs/2311.09948)</code></li>
<li>Summary: <p>In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs
for specific tasks by utilizing labeled examples as demonstrations in the
precondition prompts. Despite its promising performance, ICL suffers from
instability with the choice and arrangement of examples. Additionally, crafted
adversarial attacks pose a notable threat to the robustness of ICL. However,
existing attacks are either easy to detect, rely on external models, or lack
specificity towards ICL. To address these issues, this work introduces a novel
transferable attack for ICL, aiming to hijack LLMs to generate the targeted
response. The proposed LLM hijacking attack leverages a gradient-based prompt
search method to learn and append imperceptible adversarial suffixes to the
in-context demonstrations. Extensive experimental results on various tasks and
datasets demonstrate the effectiveness of our LLM hijacking attack, resulting
in a distracted attention towards adversarial tokens, consequently leading to
the targeted unwanted outputs.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
