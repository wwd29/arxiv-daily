<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Biased Attention: Do Vision Transformers Amplify Gender Bias More than Convolutional Neural Networks?. (arXiv:2309.08760v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08760">http://arxiv.org/abs/2309.08760</a></li>
<li>Code URL: https://github.com/aibhishek/Biased-Attention</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08760]] Biased Attention: Do Vision Transformers Amplify Gender Bias More than Convolutional Neural Networks?(http://arxiv.org/abs/2309.08760)</code></li>
<li>Summary: <p>Deep neural networks used in computer vision have been shown to exhibit many
social biases such as gender bias. Vision Transformers (ViTs) have become
increasingly popular in computer vision applications, outperforming
Convolutional Neural Networks (CNNs) in many tasks such as image
classification. However, given that research on mitigating bias in computer
vision has primarily focused on CNNs, it is important to evaluate the effect of
a different network architecture on the potential for bias amplification. In
this paper we therefore introduce a novel metric to measure bias in
architectures, Accuracy Difference. We examine bias amplification when models
belonging to these two architectures are used as a part of large multimodal
models, evaluating the different image encoders of Contrastive Language Image
Pretraining which is an important model used in many generative models such as
DALL-E and Stable Diffusion. Our experiments demonstrate that architecture can
play a role in amplifying social biases due to the different techniques
employed by the models for feature extraction and embedding as well as their
different learning properties. This research found that ViTs amplified gender
bias to a greater extent than CNNs
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Personalized Food Image Classification: Benchmark Datasets and New Baseline. (arXiv:2309.08744v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08744">http://arxiv.org/abs/2309.08744</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08744]] Personalized Food Image Classification: Benchmark Datasets and New Baseline(http://arxiv.org/abs/2309.08744)</code></li>
<li>Summary: <p>Food image classification is a fundamental step of image-based dietary
assessment, enabling automated nutrient analysis from food images. Many current
methods employ deep neural networks to train on generic food image datasets
that do not reflect the dynamism of real-life food consumption patterns, in
which food images appear sequentially over time, reflecting the progression of
what an individual consumes. Personalized food classification aims to address
this problem by training a deep neural network using food images that reflect
the consumption pattern of each individual. However, this problem is
under-explored and there is a lack of benchmark datasets with individualized
food consumption patterns due to the difficulty in data collection. In this
work, we first introduce two benchmark personalized datasets including the
Food101-Personal, which is created based on surveys of daily dietary patterns
from participants in the real world, and the VFNPersonal, which is developed
based on a dietary study. In addition, we propose a new framework for
personalized food image classification by leveraging self-supervised learning
and temporal image feature information. Our method is evaluated on both
benchmark datasets and shows improved performance compared to existing works.
The dataset has been made available at:
https://skynet.ecn.purdue.edu/~pan161/dataset_personal.html
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation. (arXiv:2309.08842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08842">http://arxiv.org/abs/2309.08842</a></li>
<li>Code URL: https://github.com/cchen-cc/ma-sam</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08842]] MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation(http://arxiv.org/abs/2309.08842)</code></li>
<li>Summary: <p>The Segment Anything Model (SAM), a foundation model for general image
segmentation, has demonstrated impressive zero-shot performance across numerous
natural image segmentation tasks. However, SAM's performance significantly
declines when applied to medical images, primarily due to the substantial
disparity between natural and medical image domains. To effectively adapt SAM
to medical images, it is important to incorporate critical third-dimensional
information, i.e., volumetric or temporal knowledge, during fine-tuning.
Simultaneously, we aim to harness SAM's pre-trained weights within its original
2D backbone to the fullest extent. In this paper, we introduce a
modality-agnostic SAM adaptation framework, named as MA-SAM, that is applicable
to various volumetric and video medical data. Our method roots in the
parameter-efficient fine-tuning strategy to update only a small portion of
weight increments while preserving the majority of SAM's pre-trained weights.
By injecting a series of 3D adapters into the transformer blocks of the image
encoder, our method enables the pre-trained 2D backbone to extract
third-dimensional information from input data. The effectiveness of our method
has been comprehensively evaluated on four medical image segmentation tasks, by
using 10 public datasets across CT, MRI, and surgical video data. Remarkably,
without using any prompt, our method consistently outperforms various
state-of-the-art 3D approaches, surpassing nnU-Net by 0.9%, 2.6%, and 9.9% in
Dice for CT multi-organ segmentation, MRI prostate segmentation, and surgical
scene segmentation respectively. Our model also demonstrates strong
generalization, and excels in challenging tumor segmentation when prompts are
used. Our code is available at: https://github.com/cchen-cc/MA-SAM.
</p></li>
</ul>

<h3>Title: Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08632">http://arxiv.org/abs/2309.08632</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08632]] Pretraining on the Test Set Is All You Need(http://arxiv.org/abs/2309.08632)</code></li>
<li>Summary: <p>Inspired by recent work demonstrating the promise of smaller
Transformer-based language models pretrained on carefully curated data, we
supercharge such approaches by investing heavily in curating a novel, high
quality, non-synthetic data mixture based solely on evaluation benchmarks.
Using our novel dataset mixture consisting of less than 100 thousand tokens, we
pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL}
(pronounced ``fictional") that achieves perfect results across diverse academic
benchmarks, strictly outperforming all known foundation models.
\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen
grokking-like ability to accurately predict downstream evaluation benchmarks'
canaries.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Enhancing Visual Perception in Novel Environments via Incremental Data Augmentation Based on Style Transfer. (arXiv:2309.08851v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08851">http://arxiv.org/abs/2309.08851</a></li>
<li>Code URL: https://github.com/abhibha1807/robustifying_visual_perception</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08851]] Enhancing Visual Perception in Novel Environments via Incremental Data Augmentation Based on Style Transfer(http://arxiv.org/abs/2309.08851)</code></li>
<li>Summary: <p>The deployment of autonomous agents in real-world scenarios is challenged by
"unknown unknowns", i.e. novel unexpected environments not encountered during
training, such as degraded signs. While existing research focuses on anomaly
detection and class imbalance, it often fails to address truly novel scenarios.
Our approach enhances visual perception by leveraging the Variational
Prototyping Encoder (VPE) to adeptly identify and handle novel inputs, then
incrementally augmenting data using neural style transfer to enrich
underrepresented data. By comparing models trained solely on original datasets
with those trained on a combination of original and augmented datasets, we
observed a notable improvement in the performance of the latter. This
underscores the critical role of data augmentation in enhancing model
robustness. Our findings suggest the potential benefits of incorporating
generative models for domain-specific augmentation strategies.
</p></li>
</ul>

<h3>Title: ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08636">http://arxiv.org/abs/2309.08636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08636]] ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert(http://arxiv.org/abs/2309.08636)</code></li>
<li>Summary: <p>Historically, proficient writing was deemed essential for human advancement,
with creative expression viewed as one of the hallmarks of human achievement.
However, recent advances in generative AI have marked an inflection point in
this narrative, including for scientific writing. This article provides a
comprehensive analysis of the capabilities and limitations of six AI chatbots
in scholarly writing in the humanities and archaeology. The methodology was
based on tagging AI generated content for quantitative accuracy and qualitative
precision by human experts. Quantitative accuracy assessed the factual
correctness, while qualitative precision gauged the scientific contribution.
While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in
recombining existing knowledge, they failed in generating original scientific
content. As a side note, our results also suggest that with ChatGPT-4 the size
of the LLMs has plateaued. Furthermore, the paper underscores the intricate and
recursive nature of human research. This process of transforming raw data into
refined knowledge is computationally irreducible, which highlights the
challenges AI chatbots face in emulating human originality in scientific
writing. In conclusion, while large language models have revolutionised content
generation, their ability to produce original scientific contributions in the
humanities remains limited. We expect that this will change in the near future
with the evolution of current LLM-based AI chatbots towards LLM-powered
software.
</p></li>
</ul>

<h3>Title: Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.08902">http://arxiv.org/abs/2309.08902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.08902]] Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models(http://arxiv.org/abs/2309.08902)</code></li>
<li>Summary: <p>LLMs are increasingly powerful and widely used to assist users in a variety
of tasks. This use risks the introduction of LLM biases to consequential
decisions such as job hiring, human performance evaluation, and criminal
sentencing. Bias in NLP systems along the lines of gender and ethnicity has
been widely studied, especially for specific stereotypes (e.g., Asians are good
at math). In this paper, we investigate bias along less studied, but still
consequential, dimensions, such as age and beauty, measuring subtler correlated
decisions that LLMs (specially autoregressive language models) make between
social groups and unrelated positive and negative attributes. We ask whether
LLMs hold wide-reaching biases of positive or negative sentiment for specific
social groups similar to the ``what is beautiful is good'' bias found in people
in experimental psychology. We introduce a template-generated dataset of
sentence completion tasks that asks the model to select the most appropriate
attribute to complete an evaluative statement about a person described as a
member of a specific social group. We also reverse the completion task to
select the social group based on an attribute. Finally, we report the
correlations that we find for multiple cutting-edge LLMs. This dataset can be
used as a benchmark to evaluate progress in more generalized biases and the
templating technique can be used to expand the benchmark with minimal
additional human annotation.
</p></li>
</ul>

<h2>anomaly</h2>
<h2>in-context</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
