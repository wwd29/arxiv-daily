<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-23</h1>
<h3>Title: Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Jittarin Jetwiriyanon, Teo Susnjak, Surangika Ranathunga</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15705">https://arxiv.org/abs/2506.15705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15705">https://arxiv.org/pdf/2506.15705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15705]] Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models(https://arxiv.org/abs/2506.15705)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This study investigates zero-shot forecasting capabilities of Time Series Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to forecasting economic indicators under univariate conditions, bypassing the need for train bespoke econometric models using and extensive training datasets. Our experiments were conducted on a case study dataset, without additional customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos, TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our results demonstrate that appropriately engineered TSFMs can internalise rich economic dynamics, accommodate regime shifts, and deliver well-behaved uncertainty estimates out of the box, while matching state-of-the-art multivariate models on this domain. Our findings suggest that, without any fine-tuning, TSFMs can match or exceed classical models during stable economic conditions. However, they are vulnerable to degradation in performances during periods of rapid shocks. The findings offer guidance to practitioners on when zero-shot deployments are viable for macroeconomic monitoring and strategic planning.</li>
</ul>

<h3>Title: BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling</h3>
<ul>
<li><strong>Authors: </strong>Songqi Zhou, Ruixue Liu, Yixing Wang, Jia Lu, Benben Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15712">https://arxiv.org/abs/2506.15712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15712">https://arxiv.org/pdf/2506.15712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15712]] BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling(https://arxiv.org/abs/2506.15712)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate fault detection in lithium-ion batteries is essential for the safe and reliable operation of electric vehicles and energy storage systems. However, existing methods often struggle to capture complex temporal dependencies and cannot fully leverage abundant unlabeled data. Although large language models (LLMs) exhibit strong representation capabilities, their architectures are not directly suited to the numerical time-series data common in industrial settings. To address these challenges, we propose a novel framework that adapts BERT-style pretraining for battery fault detection by extending the standard BERT architecture with a customized time-series-to-token representation module and a point-level Masked Signal Modeling (point-MSM) pretraining task tailored to battery applications. This approach enables self-supervised learning on sequential current, voltage, and other charge-discharge cycle data, yielding distributionally robust, context-aware temporal embeddings. We then concatenate these embeddings with battery metadata and feed them into a downstream classifier for accurate fault classification. Experimental results on a large-scale real-world dataset show that models initialized with our pretrained parameters significantly improve both representation quality and classification accuracy, achieving an AUROC of 0.945 and substantially outperforming existing approaches. These findings validate the effectiveness of BERT-style pretraining for time-series fault detection.</li>
</ul>

<h3>Title: Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems</h3>
<ul>
<li><strong>Authors: </strong>Manal Rahal, Bestoun S. Ahmed, Roger Renstrom, Robert Stener, Albrecht Wurtz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15719">https://arxiv.org/abs/2506.15719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15719">https://arxiv.org/pdf/2506.15719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15719]] Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems(https://arxiv.org/abs/2506.15719)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Heat pumps (HPs) have emerged as a cost-effective and clean technology for sustainable energy systems, but their efficiency in producing hot water remains restricted by conventional threshold-based control methods. Although machine learning (ML) has been successfully implemented for various HP applications, optimization of household hot water demand forecasting remains understudied. This paper addresses this problem by introducing a novel approach that combines predictive ML with anomaly detection to create adaptive hot water production strategies based on household-specific consumption patterns. Our key contributions include: (1) a composite approach combining ML and isolation forest (iForest) to forecast household demand for hot water and steer responsive HP operations; (2) multi-step feature selection with advanced time-series analysis to capture complex usage patterns; (3) application and tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention mechanism on data from different types of real HP installations; and (4) experimental validation on six real household installations. Our experiments show that the best-performing model LightGBM achieves superior performance, with RMSE improvements of up to 9.37\% compared to LSTM variants with $R^2$ values between 0.748-0.983. For anomaly detection, our iForest implementation achieved an F1-score of 0.87 with a false alarm rate of only 5.2\%, demonstrating strong generalization capabilities across different household types and consumption patterns, making it suitable for real-world HP deployments.</li>
</ul>

<h3>Title: UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation</h3>
<ul>
<li><strong>Authors: </strong>Wangzhi Zhan, Jianpeng Chen, Dongqi Fu, Dawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15722">https://arxiv.org/abs/2506.15722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15722">https://arxiv.org/pdf/2506.15722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15722]] UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation(https://arxiv.org/abs/2506.15722)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Metamaterials are artificial materials that are designed to meet unseen properties in nature, such as ultra-stiffness and negative materials indices. In mechanical metamaterial design, three key modalities are typically involved, i.e., 3D topology, density condition, and mechanical property. Real-world complex application scenarios place the demanding requirements on machine learning models to consider all three modalities together. However, a comprehensive literature review indicates that most existing works only consider two modalities, e.g., predicting mechanical properties given the 3D topology or generating 3D topology given the required properties. Therefore, there is still a significant gap for the state-of-the-art machine learning models capturing the whole. Hence, we propose a unified model named UNIMATE, which consists of a modality alignment module and a synergetic diffusion generation module. Experiments indicate that UNIMATE outperforms the other baseline models in topology generation task, property prediction task, and condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We opensource our proposed UNIMATE model and corresponding results at this https URL.</li>
</ul>

<h3>Title: Graph Diffusion that can Insert and Delete</h3>
<ul>
<li><strong>Authors: </strong>Matteo Ninniri, Marco Podda, Davide Bacciu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15725">https://arxiv.org/abs/2506.15725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15725">https://arxiv.org/pdf/2506.15725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15725]] Graph Diffusion that can Insert and Delete(https://arxiv.org/abs/2506.15725)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models of graphs based on discrete Denoising Diffusion Probabilistic Models (DDPMs) offer a principled approach to molecular generation by systematically removing structural noise through iterative atom and bond adjustments. However, existing formulations are fundamentally limited by their inability to adapt the graph size (that is, the number of atoms) during the diffusion process, severely restricting their effectiveness in conditional generation scenarios such as property-driven molecular design, where the targeted property often correlates with the molecular size. In this paper, we reformulate the noising and denoising processes to support monotonic insertion and deletion of nodes. The resulting model, which we call GrIDDD, dynamically grows or shrinks the chemical graph during generation. GrIDDD matches or exceeds the performance of existing graph diffusion models on molecular property targeting despite being trained on a more difficult problem. Furthermore, when applied to molecular optimization, GrIDDD exhibits competitive performance compared to specialized optimization models. This work paves the way for size-adaptive molecular generation with graph diffusion.</li>
</ul>

<h3>Title: Descriptor-based Foundation Models for Molecular Property Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jackson Burns, Akshat Zalte, William Green</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15792">https://arxiv.org/abs/2506.15792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15792">https://arxiv.org/pdf/2506.15792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15792]] Descriptor-based Foundation Models for Molecular Property Prediction(https://arxiv.org/abs/2506.15792)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Fast and accurate prediction of molecular properties with machine learning is pivotal to scientific advancements across myriad domains. Foundation models in particular have proven especially effective, enabling accurate training on small, real-world datasets. This study introduces CheMeleon, a novel molecular foundation model pre-trained on deterministic molecular descriptors from the Mordred package, leveraging a Directed Message-Passing Neural Network to predict these descriptors in a noise-free setting. Unlike conventional approaches relying on noisy experimental data or biased quantum mechanical simulations, CheMeleon uses low-noise molecular descriptors to learn rich molecular representations. Evaluated on 58 benchmark datasets from Polaris and MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks, outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop (36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%) and other foundation models. However, it struggles to distinguish activity cliffs like many of the tested models. The t-SNE projection of CheMeleon's learned representations demonstrates effective separation of chemical series, highlighting its ability to capture structural nuances. These results underscore the potential of descriptor-based pre-training for scalable and effective molecular property prediction, opening avenues for further exploration of descriptor sets and unlabeled datasets.</li>
</ul>

<h3>Title: Veracity: An Open-Source AI Fact-Checking System</h3>
<ul>
<li><strong>Authors: </strong>Taylor Lynn Curtis, Maximilian Puelma Touzel, William Garneau, Manon Gruaz, Mike Pinder, Li Wei Wang, Sukanya Krishna, Luda Cohen, Jean-François Godbout, Reihaneh Rabbany, Kellin Pelrine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15794">https://arxiv.org/abs/2506.15794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15794">https://arxiv.org/pdf/2506.15794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15794]] Veracity: An Open-Source AI Fact-Checking System(https://arxiv.org/abs/2506.15794)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.</li>
</ul>

<h3>Title: EchoShot: Multi-Shot Portrait Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Hualian Sheng, Sijia Cai, Weizhan Zhang, Caixia Yan, Yachuang Feng, Bing Deng, Jieping Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15838">https://arxiv.org/abs/2506.15838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15838">https://arxiv.org/pdf/2506.15838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15838]] EchoShot: Multi-Shot Portrait Video Generation(https://arxiv.org/abs/2506.15838)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Video diffusion models substantially boost the productivity of artistic workflows with high-quality portrait video generative capacity. However, prevailing pipelines are primarily constrained to single-shot creation, while real-world applications urge for multiple shots with identity consistency and flexible content controllability. In this work, we propose EchoShot, a native and scalable multi-shot framework for portrait customization built upon a foundation video diffusion model. To start with, we propose shot-aware position embedding mechanisms within video diffusion transformer architecture to model inter-shot variations and establish intricate correspondence between multi-shot visual content and their textual descriptions. This simple yet effective design enables direct training on multi-shot video data without introducing additional computational overhead. To facilitate model training within multi-shot scenario, we construct PortraitGala, a large-scale and high-fidelity human-centric video dataset featuring cross-shot identity consistency and fine-grained captions such as facial attributes, outfits, and dynamic motions. To further enhance applicability, we extend EchoShot to perform reference image-based personalized multi-shot generation and long video synthesis with infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves superior identity consistency as well as attribute-level controllability in multi-shot portrait video generation. Notably, the proposed framework demonstrates potential as a foundational paradigm for general multi-shot video modeling.</li>
</ul>

<h3>Title: Improving Rectified Flow with Boundary Conditions</h3>
<ul>
<li><strong>Authors: </strong>Xixi Hu, Runlong Liao, Keyang Xu, Bo Liu, Yeqing Li, Eugene Ie, Hongliang Fei, Qiang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15864">https://arxiv.org/abs/2506.15864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15864">https://arxiv.org/pdf/2506.15864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15864]] Improving Rectified Flow with Boundary Conditions(https://arxiv.org/abs/2506.15864)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rectified Flow offers a simple and effective approach to high-quality generative modeling by learning a velocity field. However, we identify a limitation in directly modeling the velocity with an unconstrained neural network: the learned velocity often fails to satisfy certain boundary conditions, leading to inaccurate velocity field estimations that deviate from the desired ODE. This issue is particularly critical during stochastic sampling at inference, as the score function's errors are amplified near the boundary. To mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary RF Model), in which we enforce boundary conditions with a minimal code modification. Boundary RF Model improves performance over vanilla RF model, demonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and 8.98% improvement using SDE sampling.</li>
</ul>

<h3>Title: KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Gaoshan Bi, Simon Jeffery, Max Davis, Yang Li, Qing Xue, Po Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15896">https://arxiv.org/abs/2506.15896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15896">https://arxiv.org/pdf/2506.15896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15896]] KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction(https://arxiv.org/abs/2506.15896)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Precision soil greenhouse gas (GHG) flux prediction is essential in agricultural systems for assessing environmental impacts, developing emission mitigation strategies and promoting sustainable agriculture. Due to the lack of advanced sensor and network technologies on majority of farms, there are challenges in obtaining comprehensive and diverse agricultural data. As a result, the scarcity of agricultural data seriously obstructs the application of machine learning approaches in precision soil GHG flux prediction. This research proposes a knowledge-guided graph neural network framework that addresses the above challenges by integrating knowledge embedded in an agricultural process-based model and graph neural network techniques. Specifically, we utilise the agricultural process-based model to simulate and generate multi-dimensional agricultural datasets for 47 countries that cover a wide range of agricultural variables. To extract key agricultural features and integrate correlations among agricultural features in the prediction process, we propose a machine learning framework that integrates the autoencoder and multi-target multi-graph based graph neural networks, which utilises the autoencoder to selectively extract significant agricultural features from the agricultural process-based model simulation data and the graph neural network to integrate correlations among agricultural features for accurately predict fertilisation-oriented soil GHG fluxes. Comprehensive experiments were conducted with both the agricultural simulation dataset and real-world agricultural dataset to evaluate the proposed approach in comparison with well-known baseline and state-of-the-art regression methods. The results demonstrate that our proposed approach provides superior accuracy and stability in fertilisation-oriented soil GHG prediction.</li>
</ul>

<h3>Title: TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation</h3>
<ul>
<li><strong>Authors: </strong>Xiao Zhang, Xingyu Zhao, Hong Xia, Yuan Cao, Guiyuan Jiang, Junyu Dong, Yanwei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15898">https://arxiv.org/abs/2506.15898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15898">https://arxiv.org/pdf/2506.15898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15898]] TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation(https://arxiv.org/abs/2506.15898)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the proliferation of location-tracking technologies, massive volumes of trajectory data are continuously being collected. As a fundamental task in trajectory data mining, trajectory similarity computation plays a critical role in a wide range of real-world applications. However, existing learning-based methods face three challenges: First, they ignore the semantic gap between GPS and grid features in trajectories, making it difficult to obtain meaningful trajectory embeddings. Second, the noise inherent in the trajectories, as well as the noise introduced during grid discretization, obscures the true motion patterns of the trajectories. Third, existing methods focus solely on point-wise and pair-wise losses, without utilizing the global ranking information obtained by sorting all trajectories according to their similarity to a given trajectory. To address the aforementioned challenges, we propose a novel trajectory similarity computation framework, named TrajDiff. Specifically, the semantic alignment module relies on cross-attention and an attention score mask mechanism with adaptive fusion, effectively eliminating semantic discrepancies between data at two scales and generating a unified representation. Additionally, the DDBM-based Noise-robust Pre-Training introduces the transfer patterns between any two trajectories into the model training process, enhancing the model's noise robustness. Finally, the overall ranking-aware regularization shifts the model's focus from a local to a global perspective, enabling it to capture the holistic ordering information among trajectories. Extensive experiments on three publicly available datasets show that TrajDiff consistently outperforms state-of-the-art baselines. In particular, it achieves an average HR@1 gain of 33.38% across all three evaluation metrics and datasets.</li>
</ul>

<h3>Title: Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI</h3>
<ul>
<li><strong>Authors: </strong>Hang Yang, Yusheng Hu, Yong Liu, Cong (Callie)Hao</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15907">https://arxiv.org/abs/2506.15907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15907">https://arxiv.org/pdf/2506.15907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15907]] Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI(https://arxiv.org/abs/2506.15907)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate graph similarity is critical for knowledge transfer in VLSI design, enabling the reuse of prior solutions to reduce engineering effort and turnaround time. We propose Pieceformer, a scalable, self-supervised similarity assessment framework, equipped with a hybrid message-passing and graph transformer encoder. To address transformer scalability, we incorporate a linear transformer backbone and introduce a partitioned training pipeline for efficient memory and parallelism management. Evaluations on synthetic and real-world CircuitNet datasets show that Pieceformer reduces mean absolute error (MAE) by 24.9% over the baseline and is the only method to correctly cluster all real-world design groups. We further demonstrate the practical usage of our model through a case study on a partitioning task, achieving up to 89% runtime reduction. These results validate the framework's effectiveness for scalable, unbiased design reuse in modern VLSI systems.</li>
</ul>

<h3>Title: MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior</h3>
<ul>
<li><strong>Authors: </strong>Liangyan Li, Yimo Ning, Kevin Le, Wei Dong, Yunzhe Li, Jun Chen, Xiaohong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15929">https://arxiv.org/abs/2506.15929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15929">https://arxiv.org/pdf/2506.15929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15929]] MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior(https://arxiv.org/abs/2506.15929)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel framework for image and video demoiréing by integrating Maximum A Posteriori (MAP) estimation with advanced deep learning techniques. Demoiréing addresses inherently nonlinear degradation processes, which pose significant challenges for existing methods. Traditional supervised learning approaches either fail to remove moiré patterns completely or produce overly smooth results. This stems from constrained model capacity and scarce training data, which inadequately represent the clean image distribution and hinder accurate reconstruction of ground-truth images. While generative models excel in image restoration for linear degradations, they struggle with nonlinear cases such as demoiréing and often introduce artifacts. To address these limitations, we propose a hybrid MAP-based framework that integrates two complementary components. The first is a supervised learning model enhanced with efficient linear attention Test-Time Training (TTT) modules, which directly learn nonlinear mappings for RAW-to-sRGB demoiréing. The second is a Truncated Flow Matching Prior (TFMP) that further refines the outputs by aligning them with the clean image distribution, effectively restoring high-frequency details and suppressing artifacts. These two components combine the computational efficiency of linear attention with the refinement abilities of generative models, resulting in improved restoration performance.</li>
</ul>

<h3>Title: CORAL: Disentangling Latent Representations in Long-Tailed Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Esther Rodriguez, Monica Welfert, Samuel McDowell, Nathan Stromberg, Julian Antolin Camarena, Lalitha Sankar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15933">https://arxiv.org/abs/2506.15933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15933">https://arxiv.org/pdf/2506.15933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15933]] CORAL: Disentangling Latent Representations in Long-Tailed Diffusion(https://arxiv.org/abs/2506.15933)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved impressive performance in generating high-quality and diverse synthetic data. However, their success typically assumes a class-balanced training distribution. In real-world settings, multi-class data often follow a long-tailed distribution, where standard diffusion models struggle -- producing low-diversity and lower-quality samples for tail classes. While this degradation is well-documented, its underlying cause remains poorly understood. In this work, we investigate the behavior of diffusion models trained on long-tailed datasets and identify a key issue: the latent representations (from the bottleneck layer of the U-Net) for tail class subspaces exhibit significant overlap with those of head classes, leading to feature borrowing and poor generation quality. Importantly, we show that this is not merely due to limited data per class, but that the relative class imbalance significantly contributes to this phenomenon. To address this, we propose COntrastive Regularization for Aligning Latents (CORAL), a contrastive latent alignment framework that leverages supervised contrastive losses to encourage well-separated latent class representations. Experiments demonstrate that CORAL significantly improves both the diversity and visual quality of samples generated for tail classes relative to state-of-the-art methods.</li>
</ul>

<h3>Title: One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks</h3>
<ul>
<li><strong>Authors: </strong>Vinicius Yuiti Fukase, Heitor Gama, Barbara Bueno, Lucas Libanio, Anna Helena Reali Costa, Artur Jordao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15954">https://arxiv.org/abs/2506.15954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15954">https://arxiv.org/pdf/2506.15954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15954]] One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks(https://arxiv.org/abs/2506.15954)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Critical Learning Periods comprehend an important phenomenon involving deep learning, where early epochs play a decisive role in the success of many training recipes, such as data augmentation. Existing works confirm the existence of this phenomenon and provide useful insights. However, the literature lacks efforts to precisely identify when critical periods occur. In this work, we fill this gap by introducing a systematic approach for identifying critical periods during the training of deep neural networks, focusing on eliminating computationally intensive regularization techniques and effectively applying mechanisms for reducing computational costs, such as data pruning. Our method leverages generalization prediction mechanisms to pinpoint critical phases where training recipes yield maximum benefits to the predictive ability of models. By halting resource-intensive recipes beyond these periods, we significantly accelerate the learning phase and achieve reductions in training time, energy consumption, and CO$_2$ emissions. Experiments on standard architectures and benchmarks confirm the effectiveness of our method. Specifically, we achieve significant milestones by reducing the training time of popular architectures by up to 59.67%, leading to a 59.47% decrease in CO$_2$ emissions and a 60% reduction in financial costs, without compromising performance. Our work enhances understanding of training dynamics and paves the way for more sustainable and efficient deep learning practices, particularly in resource-constrained environments. In the era of the race for foundation models, we believe our method emerges as a valuable framework. The repository is available at this https URL</li>
</ul>

<h3>Title: Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Cong Wang, Zexuan Deng, Zhiwei Jiang, Fei Shen, Yafeng Yin, Shiwei Gan, Zifeng Cheng, Shiping Ge, Qing Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.15980">https://arxiv.org/abs/2506.15980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.15980">https://arxiv.org/pdf/2506.15980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.15980]] Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization(https://arxiv.org/abs/2506.15980)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (\eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at this https URL.</li>
</ul>

<h3>Title: DIGMAPPER: A Modular System for Automated Geologic Map Digitization</h3>
<ul>
<li><strong>Authors: </strong>Weiwei Duan, Michael P. Gerlek, Steven N. Minton, Craig A. Knoblock, Fandel Lin, Theresa Chen, Leeje Jang, Sofia Kirsanova, Zekun Li, Yijun Lin, Yao-Yi Chiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16006">https://arxiv.org/abs/2506.16006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16006">https://arxiv.org/pdf/2506.16006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16006]] DIGMAPPER: A Modular System for Automated Geologic Map Digitization(https://arxiv.org/abs/2506.16006)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Historical geologic maps contain rich geospatial information, such as rock units, faults, folds, and bedding planes, that is critical for assessing mineral resources essential to renewable energy, electric vehicles, and national security. However, digitizing maps remains a labor-intensive and time-consuming task. We present DIGMAPPER, a modular, scalable system developed in collaboration with the United States Geological Survey (USGS) to automate the digitization of geologic maps. DIGMAPPER features a fully dockerized, workflow-orchestrated architecture that integrates state-of-the-art deep learning models for map layout analysis, feature extraction, and georeferencing. To overcome challenges such as limited training data and complex visual content, our system employs innovative techniques, including in-context learning with large language models, synthetic data generation, and transformer-based models. Evaluations on over 100 annotated maps from the DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point feature extraction, and reliable georeferencing performance. Deployed at USGS, DIGMAPPER significantly accelerates the creation of analysis-ready geospatial datasets, supporting national-scale critical mineral assessments and broader geoscientific applications.</li>
</ul>

<h3>Title: Bridging Brain with Foundation Models through Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Hamdi Altaheri, Fakhri Karray, Md. Milon Islam, S M Taslim Uddin Raju, Amir-Hossein Karimi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16009">https://arxiv.org/abs/2506.16009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16009">https://arxiv.org/pdf/2506.16009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16009]] Bridging Brain with Foundation Models through Self-Supervised Learning(https://arxiv.org/abs/2506.16009)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs), powered by self-supervised learning (SSL), have redefined the capabilities of artificial intelligence, demonstrating exceptional performance in domains like natural language processing and computer vision. These advances present a transformative opportunity for brain signal analysis. Unlike traditional supervised learning, which is limited by the scarcity of labeled neural data, SSL offers a promising solution by enabling models to learn meaningful representations from unlabeled data. This is particularly valuable in addressing the unique challenges of brain signals, including high noise levels, inter-subject variability, and low signal-to-noise ratios. This survey systematically reviews the emerging field of bridging brain signals with foundation models through the innovative application of SSL. It explores key SSL techniques, the development of brain-specific foundation models, their adaptation to downstream tasks, and the integration of brain signals with other modalities in multimodal SSL frameworks. The review also covers commonly used evaluation metrics and benchmark datasets that support comparative analysis. Finally, it highlights key challenges and outlines future research directions. This work aims to provide researchers with a structured understanding of this rapidly evolving field and a roadmap for developing generalizable brain foundation models powered by self-supervision.</li>
</ul>

<h3>Title: EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training</h3>
<ul>
<li><strong>Authors: </strong>Liangjing Shao, Linxin Bai, Chenkang Du, Xinrong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16017">https://arxiv.org/abs/2506.16017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16017">https://arxiv.org/pdf/2506.16017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16017]] EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training(https://arxiv.org/abs/2506.16017)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation and ego-motion estimation are significant tasks for scene perception and navigation in stable, accurate and efficient robot-assisted endoscopy. To tackle lighting variations and sparse textures in endoscopic scenes, multiple techniques including optical flow, appearance flow and intrinsic image decomposition have been introduced into the existing methods. However, the effective training strategy for multiple modules are still critical to deal with both illumination issues and information interference for self-supervised depth estimation in endoscopy. Therefore, a novel framework with multistep efficient finetuning is proposed in this work. In each epoch of end-to-end training, the process is divided into three steps, including optical flow registration, multiscale image decomposition and multiple transformation alignments. At each step, only the related networks are trained without interference of irrelevant information. Based on parameter-efficient finetuning on the foundation model, the proposed method achieves state-of-the-art performance on self-supervised depth estimation on SCARED dataset and zero-shot depth estimation on Hamlyn dataset, with 4\%$\sim$10\% lower error. The evaluation code of this work has been published on this https URL.</li>
</ul>

<h3>Title: Efficient Blockchain-based Steganography via Backcalculating Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Chen, Jialing He, Jiacheng Wang, Zehui Xiong, Tao Xiang, Liehuang Zhu, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16023">https://arxiv.org/abs/2506.16023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16023">https://arxiv.org/pdf/2506.16023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16023]] Efficient Blockchain-based Steganography via Backcalculating Generative Adversarial Network(https://arxiv.org/abs/2506.16023)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Blockchain-based steganography enables data hiding via encoding the covert data into a specific blockchain transaction field. However, previous works focus on the specific field-embedding methods while lacking a consideration on required field-generation embedding. In this paper, we propose a generic blockchain-based steganography framework (GBSF). The sender generates the required fields such as amount and fees, where the additional covert data is embedded to enhance the channel capacity. Based on GBSF, we design a reversible generative adversarial network (R-GAN) that utilizes the generative adversarial network with a reversible generator to generate the required fields and encode additional covert data into the input noise of the reversible generator. We then explore the performance flaw of R-GAN. To further improve the performance, we propose R-GAN with Counter-intuitive data preprocessing and Custom activation functions, namely CCR-GAN. The counter-intuitive data preprocessing (CIDP) mechanism is used to reduce decoding errors in covert data, while it incurs gradient explosion for model convergence. The custom activation function named ClipSigmoid is devised to overcome the problem. Theoretical justification for CIDP and ClipSigmoid is also provided. We also develop a mechanism named T2C, which balances capacity and concealment. We conduct experiments using the transaction amount of the Bitcoin mainnet as the required field to verify the feasibility. We then apply the proposed schemes to other transaction fields and blockchains to demonstrate the scalability. Finally, we evaluate capacity and concealment for various blockchains and transaction fields and explore the trade-off between capacity and concealment. The results demonstrate that R-GAN and CCR-GAN are able to enhance the channel capacity effectively and outperform state-of-the-art works.</li>
</ul>

<h3>Title: Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Huang, Ziqi Lin, Fang Sun, Wenchao Zhang, Kejian Tong, Yunbo Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16037">https://arxiv.org/abs/2506.16037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16037">https://arxiv.org/pdf/2506.16037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16037]] Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3(https://arxiv.org/abs/2506.16037)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a novel Retrieval-Augmented Generation (RAG) framework tailored for complex question answering tasks, addressing challenges in multi-hop reasoning and contextual understanding across lengthy documents. Built upon LLaMA 3, the framework integrates a dense retrieval module with advanced context fusion and multi-hop reasoning mechanisms, enabling more accurate and coherent response generation. A joint optimization strategy combining retrieval likelihood and generation cross-entropy improves the model's robustness and adaptability. Experimental results show that the proposed system outperforms existing retrieval-augmented and generative baselines, confirming its effectiveness in delivering precise, contextually grounded answers.</li>
</ul>

<h3>Title: CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations</h3>
<ul>
<li><strong>Authors: </strong>Puchun Liu, C. L. Philip Chen, Yubin He, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16056">https://arxiv.org/abs/2506.16056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16056">https://arxiv.org/pdf/2506.16056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16056]] CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations(https://arxiv.org/abs/2506.16056)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The difficulty of extracting deep features from EEG data and effectively integrating information from multiple views presents significant challenges for developing a generalizable pretraining framework for EEG representation learning. However, most existing pre-training methods rely solely on the contextual semantics of a single view, failing to capture the complex and synergistic interactions among different perspectives, limiting the expressiveness and generalization of learned representations. To address these issues, this paper proposes CRIA, an adaptive framework that utilizes variable-length and variable-channel coding to achieve a unified representation of EEG data across different datasets. In this work, we define cross-view information as the integrated representation that emerges from the interaction among temporal, spectral, and spatial views of EEG signals. The model employs a cross-attention mechanism to fuse temporal, spectral, and spatial features effectively, and combines an attention matrix masking strategy based on the information bottleneck principle with a novel viewpoint masking pre-training scheme. Experimental results on the Temple University EEG corpus and the CHB-MIT dataset show that CRIA outperforms existing methods with the same pre-training conditions, achieving a balanced accuracy of 57.02% for multi-class event classification and 80.03% for anomaly detection, highlighting its strong generalization ability.</li>
</ul>

<h3>Title: Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Duc Hieu Ho, Chenglin Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16064">https://arxiv.org/abs/2506.16064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16064">https://arxiv.org/pdf/2506.16064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16064]] Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning(https://arxiv.org/abs/2506.16064)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated robust capabilities across various natural language tasks. However, producing outputs that are consistently honest and helpful remains an open challenge. To overcome this challenge, this paper tackles the problem through two complementary directions. It conducts a comprehensive benchmark evaluation of ten widely used large language models, including both proprietary and open-weight models from OpenAI, Meta, and Google. In parallel, it proposes a novel prompting strategy, self-critique-guided curiosity refinement prompting. The key idea behind this strategy is enabling models to self-critique and refine their responses without additional training. The proposed method extends the curiosity-driven prompting strategy by incorporating two lightweight in-context steps including self-critique step and refinement step. The experiment results on the HONESET dataset evaluated using the framework $\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a judge of honesty and helpfulness, show consistent improvements across all models. The approach reduces the number of poor-quality responses, increases high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores ranging from 1.4% to 4.3% compared to curiosity-driven prompting across evaluated models. These results highlight the effectiveness of structured self-refinement as a scalable and training-free strategy to improve the trustworthiness of LLMs outputs.</li>
</ul>

<h3>Title: FastInit: Fast Noise Initialization for Temporally Consistent Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chengyu Bai, Yuming Li, Zhongyu Zhao, Jintao Chen, Peidong Jia, Qi She, Ming Lu, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16119">https://arxiv.org/abs/2506.16119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16119">https://arxiv.org/pdf/2506.16119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16119]] FastInit: Fast Noise Initialization for Temporally Consistent Video Generation(https://arxiv.org/abs/2506.16119)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video generation has made significant strides with the development of diffusion models; however, achieving high temporal consistency remains a challenging task. Recently, FreeInit identified a training-inference gap and introduced a method to iteratively refine the initial noise during inference. However, iterative refinement significantly increases the computational cost associated with video generation. In this paper, we introduce FastInit, a fast noise initialization method that eliminates the need for iterative refinement. FastInit learns a Video Noise Prediction Network (VNPNet) that takes random noise and a text prompt as input, generating refined noise in a single forward pass. Therefore, FastInit greatly enhances the efficiency of video generation while achieving high temporal consistency across frames. To train the VNPNet, we create a large-scale dataset consisting of pairs of text prompts, random noise, and refined noise. Extensive experiments with various text-to-video models show that our method consistently improves the quality and temporal consistency of the generated videos. FastInit not only provides a substantial improvement in video generation but also offers a practical solution that can be applied directly during inference. The code and dataset will be released.</li>
</ul>

<h3>Title: SGIC: A Self-Guided Iterative Calibration Framework for RAG</h3>
<ul>
<li><strong>Authors: </strong>Guanhua Chen, Yutong Yao, Lidia S. Chao, Xuebo Liu, Derek F. Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16172">https://arxiv.org/abs/2506.16172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16172">https://arxiv.org/pdf/2506.16172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16172]] SGIC: A Self-Guided Iterative Calibration Framework for RAG(https://arxiv.org/abs/2506.16172)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent research in retrieval-augmented generation (RAG) has concentrated on retrieving useful information from candidate documents. However, numerous methodologies frequently neglect the calibration capabilities of large language models (LLMs), which capitalize on their robust in-context reasoning prowess. This work illustrates that providing LLMs with specific cues substantially improves their calibration efficacy, especially in multi-round calibrations. We present a new SGIC: Self-Guided Iterative Calibration Framework that employs uncertainty scores as a tool. Initially, this framework calculates uncertainty scores to determine both the relevance of each document to the query and the confidence level in the responses produced by the LLMs. Subsequently, it reevaluates these scores iteratively, amalgamating them with prior responses to refine calibration. Furthermore, we introduce an innovative approach for constructing an iterative self-calibration training set, which optimizes LLMs to efficiently harness uncertainty scores for capturing critical information and enhancing response accuracy. Our proposed framework significantly improves performance on both closed-source and open-weight LLMs.</li>
</ul>

<h3>Title: Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhenghao Xi, Xiang Liu, Yaqi Liu, Yitong Cai, Yangyu Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16186">https://arxiv.org/abs/2506.16186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16186">https://arxiv.org/pdf/2506.16186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16186]] Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis(https://arxiv.org/abs/2506.16186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accident detection using Closed Circuit Television (CCTV) footage is one of the most imperative features for enhancing transport safety and efficient traffic control. To this end, this research addresses the issues of supervised monitoring and data deficiency in accident detection systems by adapting excellent deep learning technologies. The motivation arises from rising statistics in the number of car accidents worldwide; this calls for innovation and the establishment of a smart, efficient and automated way of identifying accidents and calling for help to save lives. Addressing the problem of the scarcity of data, the presented framework joins Generative Adversarial Networks (GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model training. Video frames for accidents and non-accidents are collected from YouTube videos, and we perform resizing, image enhancement and image normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%, while the CNN model obtained 88%. Such results show that the proposed framework suits traffic safety applications due to its high real-time accident detection capabilities and broad-scale applicability. This work lays the foundation for intelligent surveillance systems in the future for real-time traffic monitoring, smart city framework, and integration of intelligent surveillance systems into emergency management systems.</li>
</ul>

<h3>Title: VideoGAN-based Trajectory Proposal for Automated Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Annajoyce Mariani, Kira Maag, Hanno Gottschalk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16209">https://arxiv.org/abs/2506.16209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16209">https://arxiv.org/pdf/2506.16209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16209]] VideoGAN-based Trajectory Proposal for Automated Vehicles(https://arxiv.org/abs/2506.16209)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Being able to generate realistic trajectory options is at the core of increasing the degree of automation of road vehicles. While model-driven, rule-based, and classical learning-based methods are widely used to tackle these tasks at present, they can struggle to effectively capture the complex, multimodal distributions of future trajectories. In this paper we investigate whether a generative adversarial network (GAN) trained on videos of bird's-eye view (BEV) traffic scenarios can generate statistically accurate trajectories that correctly capture spatial relationships between the agents. To this end, we propose a pipeline that uses low-resolution BEV occupancy grid videos as training data for a video generative model. From the generated videos of traffic scenarios we extract abstract trajectory data using single-frame object detection and frame-to-frame object matching. We particularly choose a GAN architecture for the fast training and inference times with respect to diffusion models. We obtain our best results within 100 GPU hours of training, with inference times under 20\,ms. We demonstrate the physical realism of the proposed trajectories in terms of distribution alignment of spatial and dynamic parameters with respect to the ground truth videos from the Waymo Open Motion Dataset.</li>
</ul>

<h3>Title: Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design</h3>
<ul>
<li><strong>Authors: </strong>Jacopo Iollo, Geoffroy Oudoumanessah, Carole Lartizien, Michel Dojat, Florence Forbes</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16237">https://arxiv.org/abs/2506.16237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16237">https://arxiv.org/pdf/2506.16237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16237]] Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design(https://arxiv.org/abs/2506.16237)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>A key challenge in maximizing the benefits of Magnetic Resonance Imaging (MRI) in clinical settings is to accelerate acquisition times without significantly degrading image quality. This objective requires a balance between under-sampling the raw k-space measurements for faster acquisitions and gathering sufficient raw information for high-fidelity image reconstruction and analysis tasks. To achieve this balance, we propose to use sequential Bayesian experimental design (BED) to provide an adaptive and task-dependent selection of the most informative measurements. Measurements are sequentially augmented with new samples selected to maximize information gain on a posterior distribution over target images. Selection is performed via a gradient-based optimization of a design parameter that defines a subsampling pattern. In this work, we introduce a new active BED procedure that leverages diffusion-based generative models to handle the high dimensionality of the images and employs stochastic optimization to select among a variety of patterns while meeting the acquisition process constraints and budget. So doing, we show how our setting can optimize, not only standard image reconstruction, but also any associated image analysis task. The versatility and performance of our approach are demonstrated on several MRI acquisitions.</li>
</ul>

<h3>Title: Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping</h3>
<ul>
<li><strong>Authors: </strong>Abdulvahap Mutlu, Şengül Doğan, Türker Tuncer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16243">https://arxiv.org/abs/2506.16243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16243">https://arxiv.org/pdf/2506.16243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16243]] Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping(https://arxiv.org/abs/2506.16243)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and high-quality EEG data from ALS patients are scarce. This data scarcity, coupled with severe class imbalance between ALS and healthy control recordings, poses a challenge for training reliable machine learning classifiers. In this work, we address these issues by generating synthetic EEG signals for ALS patients using a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of ALS EEG signals and produce realistic synthetic samples. We preprocess and normalize EEG recordings, and train a CWGAN model to generate synthetic ALS signals. The CWGAN architecture and training routine are detailed, with key hyperparameters chosen for stable training. Qualitative evaluation of generated signals shows that they closely mimic real ALS EEG patterns. The CWGAN training converged with generator and discriminator loss curves stabilizing, indicating successful learning. The synthetic EEG signals appear realistic and have potential use as augmented data for training classifiers, helping to mitigate class imbalance and improve ALS detection accuracy. We discuss how this approach can facilitate data sharing and enhance diagnostic models.</li>
</ul>

<h3>Title: Fine-grained Image Retrieval via Dual-Vision Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Xin Jiang, Meiqi Cao, Hao Tang, Fei Shen, Zechao Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16273">https://arxiv.org/abs/2506.16273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16273">https://arxiv.org/pdf/2506.16273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16273]] Fine-grained Image Retrieval via Dual-Vision Adaptation(https://arxiv.org/abs/2506.16273)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Fine-Grained Image Retrieval~(FGIR) faces challenges in learning discriminative visual representations to retrieve images with similar fine-grained features. Current leading FGIR solutions typically follow two regimes: enforce pairwise similarity constraints in the semantic embedding space, or incorporate a localization sub-network to fine-tune the entire model. However, such two regimes tend to overfit the training data while forgetting the knowledge gained from large-scale pre-training, thus reducing their generalization ability. In this paper, we propose a Dual-Vision Adaptation (DVA) approach for FGIR, which guides the frozen pre-trained model to perform FGIR through collaborative sample and feature adaptation. Specifically, we design Object-Perceptual Adaptation, which modifies input samples to help the pre-trained model perceive critical objects and elements within objects that are helpful for category prediction. Meanwhile, we propose In-Context Adaptation, which introduces a small set of parameters for feature adaptation without modifying the pre-trained parameters. This makes the FGIR task using these adjusted features closer to the task solved during the pre-training. Additionally, to balance retrieval efficiency and performance, we propose Discrimination Perception Transfer to transfer the discriminative knowledge in the object-perceptual adaptation to the image encoder using the knowledge distillation mechanism. Extensive experiments show that DVA has fewer learnable parameters and performs well on three in-distribution and three out-of-distribution fine-grained datasets.</li>
</ul>

<h3>Title: Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective</h3>
<ul>
<li><strong>Authors: </strong>Leo Gagnon, Eric Elmoznino, Sarthak Mittal, Tom Marty, Tejas Kasetty, Dhanya Sridhar, Guillaume Lajoie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16288">https://arxiv.org/abs/2506.16288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16288">https://arxiv.org/pdf/2506.16288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16288]] Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective(https://arxiv.org/abs/2506.16288)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The rapid adaptation ability of auto-regressive foundation models is often attributed to the diversity of their pre-training data. This is because, from a Bayesian standpoint, minimizing prediction error in such settings requires integrating over all plausible latent hypotheses consistent with observations. While this behavior is desirable in principle, it often proves too ambitious in practice: under high ambiguity, the number of plausible latent alternatives makes Bayes-optimal prediction computationally intractable. Cognitive science has long recognized this limitation, suggesting that under such conditions, heuristics or information-seeking strategies are preferable to exhaustive inference. Translating this insight to next-token prediction, we hypothesize that low- and high-ambiguity predictions pose different computational demands, making ambiguity-agnostic next-token prediction a detrimental inductive bias. To test this, we introduce MetaHMM, a synthetic sequence meta-learning benchmark with rich compositional structure and a tractable Bayesian oracle. We show that Transformers indeed struggle with high-ambiguity predictions across model sizes. Motivated by cognitive theories, we propose a method to convert pre-trained models into Monte Carlo predictors that decouple task inference from token prediction. Preliminary results show substantial gains in ambiguous contexts through improved capacity allocation and test-time scalable inference, though challenges remain.</li>
</ul>

<h3>Title: Signatures to help interpretability of anomalies</h3>
<ul>
<li><strong>Authors: </strong>Emmanuel Gangler (1), Emille E. O. Ishida (1), Matwey V. Kornilov (2 and 3), Vladimir Korolev, Anastasia Lavrukhina (3), Konstantin Malanchev (4), Maria V. Pruzhinskaya (1 and 3), Etienne Russeil (1 and 5), Timofey Semenikhin (3 and 6), Sreevarsha Sreejith (7), Alina A. Volnova (8) ((1) Université Clermont Auvergne CNRS LPCA, Clermont-Ferrand, France, (2) National Research University Higher School of Economics, Moscow, Russia, (3) Sternberg Astronomical Institute Lomonosov Moscow State University, Moscow, Russia, (4) McWilliams Center for Cosmology and Astrophysics, Department of Physics, Carnegie Mellon University, Pittsburgh, PA, USA, (5) The Oskar Klein Centre Department of Astronomy, Stockholm University AlbaNova, Stockholm, Sweden, (6) Faculty of Physics, Lomonosov Moscow State University, Moscow, Russia, (7) Physics department, University of Surrey, Guildford, UK, (8) Space Research Institute of the Russian Academy of Sciences, Moscow, Russia)</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.IM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16314">https://arxiv.org/abs/2506.16314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16314">https://arxiv.org/pdf/2506.16314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16314]] Signatures to help interpretability of anomalies(https://arxiv.org/abs/2506.16314)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Machine learning is often viewed as a black box when it comes to understanding its output, be it a decision or a score. Automatic anomaly detection is no exception to this rule, and quite often the astronomer is left to independently analyze the data in order to understand why a given event is tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is to help the interpretability of anomalies by highlighting which features contributed to the decision.</li>
</ul>

<h3>Title: DISCIE -- Discriminative Closed Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Cedric Möller, Ricardo Usbeck</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16348">https://arxiv.org/abs/2506.16348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16348">https://arxiv.org/pdf/2506.16348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16348]] DISCIE -- Discriminative Closed Information Extraction(https://arxiv.org/abs/2506.16348)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel method for closed information extraction. The method employs a discriminative approach that incorporates type and entity-specific information to improve relation extraction accuracy, particularly benefiting long-tail relations. Notably, this method demonstrates superior performance compared to state-of-the-art end-to-end generative models. This is especially evident for the problem of large-scale closed information extraction where we are confronted with millions of entities and hundreds of relations. Furthermore, we emphasize the efficiency aspect by leveraging smaller models. In particular, the integration of type-information proves instrumental in achieving performance levels on par with or surpassing those of a larger generative model. This advancement holds promise for more accurate and efficient information extraction techniques.</li>
</ul>

<h3>Title: Watermarking Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Nikola Jovanović, Ismail Labiad, Tomáš Souček, Martin Vechev, Pierre Fernandez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16349">https://arxiv.org/abs/2506.16349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16349">https://arxiv.org/pdf/2506.16349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16349]] Watermarking Autoregressive Image Generation(https://arxiv.org/abs/2506.16349)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.</li>
</ul>

<h3>Title: Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data</h3>
<ul>
<li><strong>Authors: </strong>Druva Dhakshinamoorthy, Avikshit Jha, Sabyasachi Majumdar, Devdulal Ghosh, Ranjita Chakraborty, Hena Ray</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16380">https://arxiv.org/abs/2506.16380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16380">https://arxiv.org/pdf/2506.16380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16380]] Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data(https://arxiv.org/abs/2506.16380)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This paper presents a novel system for monitoring cattle behavior and detecting estrus (heat) periods using sensor data and machine learning. We designed and deployed a low-cost Bluetooth-based neck collar equipped with accelerometer and gyroscope sensors to capture real-time behavioral data from real cows, which was synced to the cloud. A labeled dataset was created using synchronized CCTV footage to annotate behaviors such as feeding, rumination, lying, and others. We evaluated multiple machine learning models -- Support Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks (CNN) -- for behavior classification. Additionally, we implemented a Long Short-Term Memory (LSTM) model for estrus detection using behavioral patterns and anomaly detection. Our system achieved over 93% behavior classification accuracy and 96% estrus detection accuracy on a limited test set. The approach offers a scalable and accessible solution for precision livestock monitoring, especially in resource-constrained environments.</li>
</ul>

<h3>Title: Large Language Models in Argument Mining: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Hao Li, Viktor Schlegel, Yizheng Sun, Riza Batista-Navarro, Goran Nenadic</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16383">https://arxiv.org/abs/2506.16383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16383">https://arxiv.org/pdf/2506.16383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16383]] Large Language Models in Argument Mining: A Survey(https://arxiv.org/abs/2506.16383)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Argument Mining (AM), a critical subfield of Natural Language Processing (NLP), focuses on extracting argumentative structures from text. The advent of Large Language Models (LLMs) has profoundly transformed AM, enabling advanced in-context learning, prompt-based generation, and robust cross-domain adaptability. This survey systematically synthesizes recent advancements in LLM-driven AM. We provide a concise review of foundational theories and annotation frameworks, alongside a meticulously curated catalog of datasets. A key contribution is our comprehensive taxonomy of AM subtasks, elucidating how contemporary LLM techniques -- such as prompting, chain-of-thought reasoning, and retrieval augmentation -- have reconfigured their execution. We further detail current LLM architectures and methodologies, critically assess evaluation practices, and delineate pivotal challenges including long-context reasoning, interpretability, and annotation bottlenecks. Conclusively, we highlight emerging trends and propose a forward-looking research agenda for LLM-based computational argumentation, aiming to strategically guide researchers in this rapidly evolving domain.</li>
</ul>

<h3>Title: HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis</h3>
<ul>
<li><strong>Authors: </strong>Peixiang Huang, Yanyan Huang, Weiqin Zhao, Junjun He, Lequan Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16398">https://arxiv.org/abs/2506.16398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16398">https://arxiv.org/pdf/2506.16398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16398]] HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis(https://arxiv.org/abs/2506.16398)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Pathology is essential for cancer diagnosis, with multiple instance learning (MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural hierarchy -- patches, regions, and slides -- with distinct semantic associations. While some methods attempt to leverage this hierarchy for improved representation, they predominantly rely on Euclidean embeddings, which struggle to fully capture semantic hierarchies. To address this limitation, we propose HyperPath, a novel method that integrates knowledge from textual descriptions to guide the modeling of semantic hierarchies of WSIs in hyperbolic space, thereby enhancing WSI classification. Our approach adapts both visual and textual features extracted by pathology vision-language foundation models to the hyperbolic space. We design an Angular Modality Alignment Loss to ensure robust cross-modal alignment, while a Semantic Hierarchy Consistency Loss further refines feature hierarchies through entailment and contradiction relationships and thus enhance semantic coherence. The classification is performed with geodesic distance, which measures the similarity between entities in the hyperbolic semantic hierarchy. This eliminates the need for linear classifiers and enables a geometry-aware approach to WSI analysis. Extensive experiments show that our method achieves superior performance across tasks compared to existing methods, highlighting the potential of hyperbolic embeddings for WSI analysis.</li>
</ul>

<h3>Title: NepaliGPT: A Generative Language Model for the Nepali Language</h3>
<ul>
<li><strong>Authors: </strong>Shushanta Pudasaini, Aman Shakya, Siddhartha Shrestha, Sahil Bhatta, Sunil Thapa, Sushmita Palikhe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16399">https://arxiv.org/abs/2506.16399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16399">https://arxiv.org/pdf/2506.16399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16399]] NepaliGPT: A Generative Language Model for the Nepali Language(https://arxiv.org/abs/2506.16399)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>After the release of ChatGPT, Large Language Models (LLMs) have gained huge popularity in recent days and thousands of variants of LLMs have been released. However, there is no generative language model for the Nepali language, due to which other downstream tasks, including fine-tuning, have not been explored yet. To fill this research gap in the Nepali NLP space, this research proposes \textit{NepaliGPT}, a generative large language model tailored specifically for the Nepali language. This research introduces an advanced corpus for the Nepali language collected from several sources, called the Devanagari Corpus. Likewise, the research introduces the first NepaliGPT benchmark dataset comprised of 4,296 question-answer pairs in the Nepali language. The proposed LLM NepaliGPT achieves the following metrics in text generation: Perplexity of 26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal consistency of 85.41\%.</li>
</ul>

<h3>Title: Generating Directed Graphs with Dual Attention and Asymmetric Encoding</h3>
<ul>
<li><strong>Authors: </strong>Alba Carballo-Castro, Manuel Madeira, Yiming Qin, Dorina Thanou, Pascal Frossard</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16404">https://arxiv.org/abs/2506.16404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16404">https://arxiv.org/pdf/2506.16404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16404]] Generating Directed Graphs with Dual Attention and Asymmetric Encoding(https://arxiv.org/abs/2506.16404)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Directed graphs naturally model systems with asymmetric, ordered relationships, essential to applications in biology, transportation, social networks, and visual understanding. Generating such graphs enables tasks such as simulation, data augmentation and novel instance discovery; however, directed graph generation remains underexplored. We identify two key factors limiting progress in this direction: first, modeling edge directionality introduces a substantially larger dependency space, making the underlying distribution harder to learn; second, the absence of standardized benchmarks hinders rigorous evaluation. Addressing the former requires more expressive models that are sensitive to directional topologies. We propose Directo, the first generative model for directed graphs built upon the discrete flow matching framework. Our approach combines: (i) principled positional encodings tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism capturing both incoming and outgoing dependencies, and (iii) a robust, discrete generative framework. To support evaluation, we introduce a benchmark suite covering synthetic and real-world datasets. It shows that our method performs strongly across diverse settings and even competes with specialized models for particular classes, such as directed acyclic graphs. Our results highlight the effectiveness and generality of our approach, establishing a solid foundation for future research in directed graph generation.</li>
</ul>

<h3>Title: SecureFed: A Two-Phase Framework for Detecting Malicious Clients in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Likhitha Annapurna Kavuri, Akshay Mhatre, Akarsh K Nair, Deepti Gupta</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16458">https://arxiv.org/abs/2506.16458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16458">https://arxiv.org/pdf/2506.16458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16458]] SecureFed: A Two-Phase Framework for Detecting Malicious Clients in Federated Learning(https://arxiv.org/abs/2506.16458)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) protects data privacy while providing a decentralized method for training models. However, because of the distributed schema, it is susceptible to adversarial clients that could alter results or sabotage model performance. This study presents SecureFed, a two-phase FL framework for identifying and reducing the impact of such attackers. Phase 1 involves collecting model updates from participating clients and applying a dimensionality reduction approach to identify outlier patterns frequently associated with malicious behavior. Temporary models constructed from the client updates are evaluated on synthetic datasets to compute validation losses and support anomaly scoring. The idea of learning zones is presented in Phase 2, where weights are dynamically routed according to their contribution scores and gradient magnitudes. High-value gradient zones are given greater weight in aggregation and contribute more significantly to the global model, while lower-value gradient zones, which may indicate possible adversarial activity, are gradually removed from training. Until the model converges and a strong defense against poisoning attacks is possible, this training cycle continues Based on the experimental findings, SecureFed considerably improves model resilience without compromising model performance.</li>
</ul>

<h3>Title: Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities</h3>
<ul>
<li><strong>Authors: </strong>Tara Akhound-Sadegh, Jungyoon Lee, Avishek Joey Bose, Valentin De Bortoli, Arnaud Doucet, Michael M. Bronstein, Dominique Beaini, Siamak Ravanbakhsh, Kirill Neklyudov, Alexander Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16471">https://arxiv.org/abs/2506.16471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16471">https://arxiv.org/pdf/2506.16471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16471]] Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities(https://arxiv.org/abs/2506.16471)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA), a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to procure training samples at a lower temperature for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of N-body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations. Code available at: this https URL</li>
</ul>

<h3>Title: Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples</h3>
<ul>
<li><strong>Authors: </strong>Soumya Suvra Ghosal, Vaibhav Singh, Akash Ghosh, Soumyabrata Pal, Subhadip Baidya, Sriparna Saha, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16502">https://arxiv.org/abs/2506.16502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16502">https://arxiv.org/pdf/2506.16502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16502]] Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples(https://arxiv.org/abs/2506.16502)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Reward models are essential for aligning large language models (LLMs) with human preferences. However, most open-source multilingual reward models are primarily trained on preference datasets in high-resource languages, resulting in unreliable reward signals for low-resource Indic languages. Collecting large-scale, high-quality preference data for these languages is prohibitively expensive, making preference-based training approaches impractical. To address this challenge, we propose RELIC, a novel in-context learning framework for reward modeling in low-resource Indic languages. RELIC trains a retriever with a pairwise ranking objective to select in-context examples from auxiliary high-resource languages that most effectively highlight the distinction between preferred and less-preferred responses. Extensive experiments on three preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art open-source reward models demonstrate that RELIC significantly improves reward model accuracy for low-resource Indic languages, consistently outperforming existing example selection methods. For example, on Bodo-a low-resource Indic language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13% improvement in accuracy over zero-shot prompting and state-of-the-art example selection method, respectively.</li>
</ul>

<h3>Title: Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details</h3>
<ul>
<li><strong>Authors: </strong>Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, Sheng Zhang, Xin Huang, Di Luo, Fan Yang, Fang Yang, Lifu Wang, Sicong Liu, Yixuan Tang, Yulin Cai, Zebin He, Tian Liu, Yuhong Liu, Jie Jiang, Linus, Jingwei Huang, Chunchao Guo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16504">https://arxiv.org/abs/2506.16504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16504">https://arxiv.org/pdf/2506.16504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16504]] Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details(https://arxiv.org/abs/2506.16504)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion models aimed at generating high-fidelity and detailed textured 3D assets. Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D 2.0, while demonstrating substantial advancements in both shape and texture generation. In terms of shape generation, we introduce a new shape foundation model -- LATTICE, which is trained with scaled high-quality datasets, model-size, and compute. Our largest model reaches 10B parameters and generates sharp and detailed 3D shape with precise image-3D following while keeping mesh surface clean and smooth, significantly closing the gap between generated and handcrafted 3D shapes. In terms of texture generation, it is upgraded with phyiscal-based rendering (PBR) via a novel multi-view architecture extended from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D 2.5 significantly outperforms previous methods in both shape and end-to-end texture generation.</li>
</ul>

<h3>Title: From Semantic To Instance: A Semi-Self-Supervised Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Keyhan Najafian, Farhad Maleki, Lingling Jin, Ian Stavness</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16563">https://arxiv.org/abs/2506.16563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16563">https://arxiv.org/pdf/2506.16563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16563]] From Semantic To Instance: A Semi-Self-Supervised Learning Approach(https://arxiv.org/abs/2506.16563)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Instance segmentation is essential for applications such as automated monitoring of plant health, growth, and yield. However, extensive effort is required to create large-scale datasets with pixel-level annotations of each object instance for developing instance segmentation models that restrict the use of deep learning in these areas. This challenge is more significant in images with densely packed, self-occluded objects, which are common in agriculture. To address this challenge, we propose a semi-self-supervised learning approach that requires minimal manual annotation to develop a high-performing instance segmentation model. We design GLMask, an image-mask representation for the model to focus on shape, texture, and pattern while minimizing its dependence on color features. We develop a pipeline to generate semantic segmentation and then transform it into instance-level segmentation. The proposed approach substantially outperforms the conventional instance segmentation models, establishing a state-of-the-art wheat head instance segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed methodology on the general-purpose Microsoft COCO dataset, achieving a significant performance improvement of over 12.6% mAP@50. This highlights that the utility of our proposed approach extends beyond precision agriculture and applies to other domains, specifically those with similar data characteristics.</li>
</ul>

<h3>Title: Weight Factorization and Centralization for Continual Learning in Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16574">https://arxiv.org/abs/2506.16574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16574">https://arxiv.org/pdf/2506.16574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16574]] Weight Factorization and Centralization for Continual Learning in Speech Recognition(https://arxiv.org/abs/2506.16574)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Modern neural network based speech recognition models are required to continually absorb new data without re-training the whole system, especially in downstream applications using foundation models, having no access to the original training data. Continually training the models in a rehearsal-free, multilingual, and language agnostic condition, likely leads to catastrophic forgetting, when a seemingly insignificant disruption to the weights can destructively harm the quality of the models. Inspired by the ability of human brains to learn and consolidate knowledge through the waking-sleeping cycle, we propose a continual learning approach with two distinct phases: factorization and centralization, learning and merging knowledge accordingly. Our experiments on a sequence of varied code-switching datasets showed that the centralization stage can effectively prevent catastrophic forgetting by accumulating the knowledge in multiple scattering low-rank adapters.</li>
</ul>

<h3>Title: SafeTriage: Facial Video De-identification for Privacy-Preserving Stroke Triage</h3>
<ul>
<li><strong>Authors: </strong>Tongan Cai, Haomiao Ni, Wenchao Ma, Yuan Xue, Qian Ma, Rachel Leicht, Kelvin Wong, John Volpi, Stephen T.C. Wong, James Z. Wang, Sharon X. Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16578">https://arxiv.org/abs/2506.16578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16578">https://arxiv.org/pdf/2506.16578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16578]] SafeTriage: Facial Video De-identification for Privacy-Preserving Stroke Triage(https://arxiv.org/abs/2506.16578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Effective stroke triage in emergency settings often relies on clinicians' ability to identify subtle abnormalities in facial muscle coordination. While recent AI models have shown promise in detecting such patterns from patient facial videos, their reliance on real patient data raises significant ethical and privacy challenges -- especially when training robust and generalizable models across institutions. To address these concerns, we propose SafeTriage, a novel method designed to de-identify patient facial videos while preserving essential motion cues crucial for stroke diagnosis. SafeTriage leverages a pretrained video motion transfer (VMT) model to map the motion characteristics of real patient faces onto synthetic identities. This approach retains diagnostically relevant facial dynamics without revealing the patients' identities. To mitigate the distribution shift between normal population pre-training videos and patient population test videos, we introduce a conditional generative model for visual prompt tuning, which adapts the input space of the VMT model to ensure accurate motion transfer without needing to fine-tune the VMT model backbone. Comprehensive evaluation, including quantitative metrics and clinical expert assessments, demonstrates that SafeTriage-produced synthetic videos effectively preserve stroke-relevant facial patterns, enabling reliable AI-based triage. Our evaluations also show that SafeTriage provides robust privacy protection while maintaining diagnostic accuracy, offering a secure and ethically sound foundation for data sharing and AI-driven clinical analysis in neurological disorders.</li>
</ul>

<h3>Title: Few-Shot Learning-Based Cyber Incident Detection with Augmented Context Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Fei Zuo, Junghwan Rhee, Yung Ryn Choe, Chenglong Fu, Xianshan Qu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16626">https://arxiv.org/abs/2506.16626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16626">https://arxiv.org/pdf/2506.16626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16626]] Few-Shot Learning-Based Cyber Incident Detection with Augmented Context Intelligence(https://arxiv.org/abs/2506.16626)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In recent years, the adoption of cloud services has been expanding at an unprecedented rate. As more and more organizations migrate or deploy their businesses to the cloud, a multitude of related cybersecurity incidents such as data breaches are on the rise. Many inherent attributes of cloud environments, for example, data sharing, remote access, dynamicity and scalability, pose significant challenges for the protection of cloud security. Even worse, cyber threats are becoming increasingly sophisticated and covert. Attack methods, such as Advanced Persistent Threats (APTs), are continually developed to bypass traditional security measures. Among the emerging technologies for robust threat detection, system provenance analysis is being considered as a promising mechanism, thus attracting widespread attention in the field of incident response. This paper proposes a new few-shot learning-based attack detection with improved data context intelligence. We collect operating system behavior data of cloud systems during realistic attacks and leverage an innovative semiotics extraction method to describe system events. Inspired by the advances in semantic analysis, which is a fruitful area focused on understanding natural languages in computational linguistics, we further convert the anomaly detection problem into a similarity comparison problem. Comprehensive experiments show that the proposed approach is able to generalize over unseen attacks and make accurate predictions, even if the incident detection models are trained with very limited samples.</li>
</ul>

<h3>Title: Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures</h3>
<ul>
<li><strong>Authors: </strong>Vijay Prakash Dwivedi, Charilaos Kanatsoulis, Shenyang Huang, Jure Leskovec</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16654">https://arxiv.org/abs/2506.16654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16654">https://arxiv.org/pdf/2506.16654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16654]] Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures(https://arxiv.org/abs/2506.16654)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Graph machine learning has led to a significant increase in the capabilities of models that learn on arbitrary graph-structured data and has been applied to molecules, social networks, recommendation systems, and transportation, among other domains. Data in multi-tabular relational databases can also be constructed as 'relational entity graphs' for Relational Deep Learning (RDL) - a new blueprint that enables end-to-end representation learning without traditional feature engineering. Compared to arbitrary graph-structured data, relational entity graphs have key properties: (i) their structure is defined by primary-foreign key relationships between entities in different tables, (ii) the structural connectivity is a function of the relational schema defining a database, and (iii) the graph connectivity is temporal and heterogeneous in nature. In this paper, we provide a comprehensive review of RDL by first introducing the representation of relational databases as relational entity graphs, and then reviewing public benchmark datasets that have been used to develop and evaluate recent GNN-based RDL models. We discuss key challenges including large-scale multi-table integration and the complexities of modeling temporal dynamics and heterogeneous data, while also surveying foundational neural network methods and recent architectural advances specialized for relational entity graphs. Finally, we explore opportunities to unify these distinct modeling challenges, highlighting how RDL converges multiple sub-fields in graph machine learning towards the design of foundation models that can transform the processing of relational data.</li>
</ul>

<h3>Title: Mesh-Informed Neural Operator : A Transformer Generative Approach</h3>
<ul>
<li><strong>Authors: </strong>Yaozhong Shi, Zachary E. Ross, Domniki Asimaki, Kamyar Azizzadenesheli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16656">https://arxiv.org/abs/2506.16656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16656">https://arxiv.org/pdf/2506.16656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16656]] Mesh-Informed Neural Operator : A Transformer Generative Approach(https://arxiv.org/abs/2506.16656)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models in function spaces, situated at the intersection of generative modeling and operator learning, are attracting increasing attention due to their immense potential in diverse scientific and engineering applications. While functional generative models are theoretically domain- and discretization-agnostic, current implementations heavily rely on the Fourier Neural Operator (FNO), limiting their applicability to regular grids and rectangular domains. To overcome these critical limitations, we introduce the Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and cross-attention mechanisms, MINO offers a principled, domain- and discretization-agnostic backbone for generative modeling in function spaces. This advancement significantly expands the scope of such models to more diverse applications in generative, inverse, and regression tasks. Furthermore, MINO provides a unified perspective on integrating neural operators with general advanced deep learning architectures. Finally, we introduce a suite of standardized evaluation metrics that enable objective comparison of functional generative models, addressing another critical gap in the field.</li>
</ul>

<h3>Title: Fast and Stable Diffusion Planning through Variational Adaptive Weighting</h3>
<ul>
<li><strong>Authors: </strong>Zhiying Qiu, Tao Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16688">https://arxiv.org/abs/2506.16688</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16688">https://arxiv.org/pdf/2506.16688</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16688]] Fast and Stable Diffusion Planning through Variational Adaptive Weighting(https://arxiv.org/abs/2506.16688)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently shown promise in offline RL. However, these methods often suffer from high training costs and slow convergence, particularly when using transformer-based denoising backbones. While several optimization strategies have been proposed -- such as modified noise schedules, auxiliary prediction targets, and adaptive loss weighting -- challenges remain in achieving stable and efficient training. In particular, existing loss weighting functions typically rely on neural network approximators, which can be ineffective in early training phases due to limited generalization capacity of MLPs when exposed to sparse feedback in the early training stages. In this work, we derive a variationally optimal uncertainty-aware weighting function and introduce a closed-form polynomial approximation method for its online estimation under the flow-based generative modeling framework. We integrate our method into a diffusion planning pipeline and evaluate it on standard offline RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our method achieves competitive performance with up to 10 times fewer training steps, highlighting its practical effectiveness.</li>
</ul>

<h3>Title: ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Bin Chen, Xinzge Gao, Chuanrui Hu, Penghang Yu, Hua Zhang, Bing-Kun Bao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16712">https://arxiv.org/abs/2506.16712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16712">https://arxiv.org/pdf/2506.16712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16712]] ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models(https://arxiv.org/abs/2506.16712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Reward Models (GRMs) provide greater flexibility than scalar reward models in capturing human preferences, but their effectiveness is limited by poor reasoning capabilities. This often results in incomplete or overly speculative reasoning paths, leading to hallucinations or missing key information in complex tasks. We address this challenge with ReasonGRM, a three-stage generative reward modeling framework. In the first stage, Zero-RL is used to generate concise, outcome-directed reasoning paths that reduce the likelihood of critical omissions. In the second stage, we introduce a novel evaluation metric, $R^\star$, which scores reasoning paths based on their generation likelihood. This favors paths that reach correct answers with minimal exploration, helping to reduce hallucination-prone data during training. In the final stage, the model is further refined through reinforcement learning on challenging examples to enhance its preference discrimination capabilities. Experiments on three public benchmarks show that ReasonGRM achieves competitive or state-of-the-art performance, outperforming previous best GRMs by 1.8\% on average and surpassing proprietary models such as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of reasoning-aware training and highlight the importance of high-quality rationale selection for reliable preference modeling.</li>
</ul>

<h3>Title: 3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Yunshan Li, Wenwu Gong, Qianqian Wang, Chao Wang, Lili Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16735">https://arxiv.org/abs/2506.16735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16735">https://arxiv.org/pdf/2506.16735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16735]] 3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting(https://arxiv.org/abs/2506.16735)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent approaches based on transform-based tensor nuclear norm (TNN) have demonstrated notable effectiveness in hyperspectral image (HSI) inpainting by leveraging low-rank structures in latent representations. Recent developments incorporate deep transforms to improve low-rank tensor representation; however, existing approaches typically restrict the transform to the spectral mode, neglecting low-rank properties along other tensor modes. In this paper, we propose a novel 3-directional deep low-rank tensor representation (3DeepRep) model, which performs deep nonlinear transforms along all three modes of the HSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of mode-i frontal slices in the corresponding latent space for each direction (i=1,2,3), forming a 3-directional TNN regularization. The outputs from the three directional branches are subsequently fused via a learnable aggregation module to produce the final result. An efficient gradient-based optimization algorithm is developed to solve the model in a self-supervised manner. Extensive experiments on real-world HSI datasets demonstrate that the proposed method achieves superior inpainting performance compared to existing state-of-the-art techniques, both qualitatively and quantitatively.</li>
</ul>

<h3>Title: Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention</h3>
<ul>
<li><strong>Authors: </strong>Weinan Guan, Wei Wang, Bo Peng, Ziwen He, Jing Dong, Haonan Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16743">https://arxiv.org/abs/2506.16743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16743">https://arxiv.org/pdf/2506.16743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16743]] Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention(https://arxiv.org/abs/2506.16743)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>With the rapid development of image generation technologies, especially the advancement of Diffusion Models, the quality of synthesized images has significantly improved, raising concerns among researchers about information security. To mitigate the malicious abuse of diffusion models, diffusion-generated image detection has proven to be an effective this http URL, a key challenge for forgery detection is generalising to diffusion models not seen during training. In this paper, we address this problem by focusing on image noise. We observe that images from different diffusion models share similar noise patterns, distinct from genuine images. Building upon this insight, we introduce a novel Noise-Aware Self-Attention (NASA) module that focuses on noise regions to capture anomalous patterns. To implement a SOTA detection model, we incorporate NASA into Swin Transformer, forming an novel detection architecture NASA-Swin. Additionally, we employ a cross-modality fusion embedding to combine RGB and noise images, along with a channel mask strategy to enhance feature learning from both modalities. Extensive experiments demonstrate the effectiveness of our approach in enhancing detection capabilities for diffusion-generated images. When encountering unseen generation methods, our approach achieves the state-of-the-art this http URL code is available at this https URL.</li>
</ul>

<h3>Title: Class Agnostic Instance-level Descriptor for Visual Instance Search</h3>
<ul>
<li><strong>Authors: </strong>Qi-Ying Sun, Wan-Lei Zhao, Yi-Bo Miao, Chong-Wah Ngo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16745">https://arxiv.org/abs/2506.16745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16745">https://arxiv.org/pdf/2506.16745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16745]] Class Agnostic Instance-level Descriptor for Visual Instance Search(https://arxiv.org/abs/2506.16745)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Despite the great success of the deep features in content-based image retrieval, the visual instance search remains challenging due to the lack of effective instance level feature representation. Supervised or weakly supervised object detection methods are not among the options due to their poor performance on the unknown object categories. In this paper, based on the feature set output from self-supervised ViT, the instance level region discovery is modeled as detecting the compact feature subsets in a hierarchical fashion. The hierarchical decomposition results in a hierarchy of feature subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the various instance regions in an image of different semantic scales. The hierarchical decomposition well addresses the problem of object embedding and occlusions, which are widely observed in the real scenarios. The features derived from the nodes on the hierarchy make up a comprehensive representation for the latent instances in the image. Our instance-level descriptor remains effective on both the known and unknown object categories. Empirical studies on three instance search benchmarks show that it outperforms state-of-the-art methods considerably.</li>
</ul>

<h3>Title: PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Beomseok Ko, Hyeryung Jang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16776">https://arxiv.org/abs/2506.16776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16776">https://arxiv.org/pdf/2506.16776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16776]] PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model(https://arxiv.org/abs/2506.16776)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models excel in image generation but are computational and resource-intensive due to their reliance on iterative Markov chain processes, leading to error accumulation and limiting the effectiveness of naive compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid compression framework combining Progressive Quantization (PQ) and Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs a two-stage quantization with adaptive bit-width transitions guided by a momentum-based mechanism, reducing excessive weight perturbations in low-precision. CAD leverages full-precision calibration datasets during distillation, enabling the student to match full-precision performance even with a quantized teacher. As a result, PQCAD-DM achieves a balance between computational efficiency and generative quality, halving inference time while maintaining competitive performance. Extensive experiments validate PQCAD-DM's superior generative capabilities and efficiency across diverse datasets, outperforming fixed-bit quantization methods.</li>
</ul>

<h3>Title: Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps</h3>
<ul>
<li><strong>Authors: </strong>Jiashun Cheng, Aochuan Chen, Nuo Chen, Ziqi Gao, Yuhan Li, Jia Li, Fugee Tsung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16787">https://arxiv.org/abs/2506.16787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16787">https://arxiv.org/pdf/2506.16787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16787]] Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps(https://arxiv.org/abs/2506.16787)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has emerged as a prominent technique for fine-tuning large foundation models. Despite its successes, the substantial parameter redundancy, which limits the capacity and efficiency of LoRA, has been recognized as a bottleneck. In this work, we systematically investigate the impact of redundancy in fine-tuning LoRA and reveal that reducing density redundancy does not degrade expressiveness. Based on this insight, we introduce \underline{S}pectral-\underline{e}ncoding \underline{L}ow-\underline{R}ank \underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of spectral bases to re-parameterize LoRA from a sparse spectral subspace. Designed with simplicity, SeLoRA enables seamless integration with various LoRA variants for performance boosting, serving as a scalable plug-and-play framework. Extensive experiments substantiate that SeLoRA achieves greater efficiency with fewer parameters, delivering superior performance enhancements over strong baselines on various downstream tasks, including commonsense reasoning, math reasoning, and code generation.</li>
</ul>

<h3>Title: TabArena: A Living Benchmark for Machine Learning on Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Nick Erickson, Lennart Purucker, Andrej Tschalzev, David Holzmüller, Prateek Mutalik Desai, and David Salinas, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16791">https://arxiv.org/abs/2506.16791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16791">https://arxiv.org/pdf/2506.16791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16791]] TabArena: A Living Benchmark for Machine Learning on Tabular Data(https://arxiv.org/abs/2506.16791)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning and investigate the contributions of individual models. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at this https URL.</li>
</ul>

<h3>Title: Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Corvi, Davide Cozzolino, Ekta Prashnani, Shalini De Mello, Koki Nagano, Luisa Verdoliva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16802">https://arxiv.org/abs/2506.16802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16802">https://arxiv.org/pdf/2506.16802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16802]] Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation(https://arxiv.org/abs/2506.16802)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic video generation is progressing very rapidly. The latest models can produce very realistic high-resolution videos that are virtually indistinguishable from real ones. Although several video forensic detectors have been recently proposed, they often exhibit poor generalization, which limits their applicability in a real-world scenario. Our key insight to overcome this issue is to guide the detector towards seeing what really matters. In fact, a well-designed forensic classifier should focus on identifying intrinsic low-level artifacts introduced by a generative architecture rather than relying on high-level semantic flaws that characterize a specific model. In this work, first, we study different generative architectures, searching and identifying discriminative features that are unbiased, robust to impairments, and shared across models. Then, we introduce a novel forensic-oriented data augmentation strategy based on the wavelet decomposition and replace specific frequency-related bands to drive the model to exploit more relevant forensic cues. Our novel training paradigm improves the generalizability of AI-generated video detectors, without the need for complex algorithms and large datasets that include multiple synthetic generators. To evaluate our approach, we train the detector using data from a single generative model and test it against videos produced by a wide range of other models. Despite its simplicity, our method achieves a significant accuracy improvement over state-of-the-art detectors and obtains excellent results even on very recent generative models, such as NOVA and FLUX. Code and data will be made publicly available.</li>
</ul>

<h3>Title: FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fan Yang, Yousong Zhu, Xin Li, Yufei Zhan, Hongyin Zhao, Shurong Zheng, Yaowei Wang, Ming Tang, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16806">https://arxiv.org/abs/2506.16806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16806">https://arxiv.org/pdf/2506.16806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16806]] FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation(https://arxiv.org/abs/2506.16806)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editing. However, current approaches treat "what to see" and "how to edit" separately: they either perform isolated object segmentation or utilize segmentation masks merely as conditional prompts for local edit generation tasks, often relying on multiple disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM that integrates segmentation-aware perception and controllable object-centric generation within an end-to-end framework. FOCUS employs a dual-branch visual encoder to simultaneously capture global semantic context and fine-grained spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to produce discrete visual tokens that enhance generation quality. To enable accurate and controllable image editing, we propose a progressive multi-stage training pipeline, where segmentation masks are jointly optimized and used as spatial condition prompts to guide the diffusion decoder. This strategy aligns visual encoding, segmentation, and generation modules, effectively bridging segmentation-aware perception with fine-grained visual synthesis. Extensive experiments across three core tasks, including multimodal understanding, referring segmentation accuracy, and controllable image generation, demonstrate that FOCUS achieves strong performance by jointly optimizing visual perception and generative capabilities.</li>
</ul>

<h3>Title: Robust Group Anomaly Detection for Quasi-Periodic Network Time Series</h3>
<ul>
<li><strong>Authors: </strong>Kai Yang, Shaoyu Dou, Pan Luo, Xin Wang, H. Vincent Poor</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16815">https://arxiv.org/abs/2506.16815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16815">https://arxiv.org/pdf/2506.16815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16815]] Robust Group Anomaly Detection for Quasi-Periodic Network Time Series(https://arxiv.org/abs/2506.16815)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Many real-world multivariate time series are collected from a network of physical objects embedded with software, electronics, and sensors. The quasi-periodic signals generated by these objects often follow a similar repetitive and periodic pattern, but have variations in the period, and come in different lengths caused by timing (synchronization) errors. Given a multitude of such quasi-periodic time series, can we build machine learning models to identify those time series that behave differently from the majority of the observations? In addition, can the models help human experts to understand how the decision was made? We propose a sequence to Gaussian Mixture Model (seq2GMM) framework. The overarching goal of this framework is to identify unusual and interesting time series within a network time series database. We further develop a surrogate-based optimization algorithm that can efficiently train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a plurality of public benchmark datasets, outperforming state-of-the-art anomaly detection techniques by a significant margin. We also theoretically analyze the convergence property of the proposed training algorithm and provide numerical results to substantiate our theoretical claims.</li>
</ul>

<h3>Title: Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuchu Jiang, Jiaming Chu, Jian Zhao, Xin Zhang, Xu Yang, Lei Jin, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16819">https://arxiv.org/abs/2506.16819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16819">https://arxiv.org/pdf/2506.16819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16819]] Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection(https://arxiv.org/abs/2506.16819)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The proliferation of generative models has raised serious concerns about visual content forgery. Existing deepfake detection methods primarily target either image-level classification or pixel-wise localization. While some achieve high accuracy, they often suffer from limited generalization across manipulation types or rely on complex architectures. In this paper, we propose Loupe, a lightweight yet effective framework for joint deepfake detection and localization. Loupe integrates a patch-aware classifier and a segmentation module with conditional queries, allowing simultaneous global authenticity classification and fine-grained mask prediction. To enhance robustness against distribution shifts of test set, Loupe introduces a pseudo-label-guided test-time adaptation mechanism by leveraging patch-level predictions to supervise the segmentation head. Extensive experiments on the DDL dataset demonstrate that Loupe achieves state-of-the-art performance, securing the first place in the IJCAI 2025 Deepfake Detection and Localization Challenge with an overall score of 0.846. Our results validate the effectiveness of the proposed patch-level fusion and conditional query design in improving both classification accuracy and spatial localization under diverse forgery patterns. The code is available at this https URL.</li>
</ul>

<h3>Title: Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots</h3>
<ul>
<li><strong>Authors: </strong>Can Lin, Daniele Affinita, Marco E. P. Zimmatore, Daniele Nardi, Domenico D. Bloisi, Vincenzo Suriani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16821">https://arxiv.org/abs/2506.16821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16821">https://arxiv.org/pdf/2506.16821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16821]] Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots(https://arxiv.org/abs/2506.16821)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Robust and accurate ball detection is a critical component for autonomous humanoid soccer robots, particularly in dynamic and challenging environments such as RoboCup outdoor fields. However, traditional supervised approaches require extensive manual annotation, which is costly and time-intensive. To overcome this problem, we present a self-supervised learning framework for domain-adaptive feature extraction to enhance ball detection performance. The proposed approach leverages a general-purpose pretrained model to generate pseudo-labels, which are then used in a suite of self-supervised pretext tasks -- including colorization, edge detection, and triplet loss -- to learn robust visual features without relying on manual annotations. Additionally, a model-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid adaptation to new deployment scenarios with minimal supervision. A new dataset comprising 10,000 labeled images from outdoor RoboCup SPL matches is introduced, used to validate the method, and made available to the community. Experimental results demonstrate that the proposed pipeline outperforms baseline models in terms of accuracy, F1 score, and IoU, while also exhibiting faster convergence.</li>
</ul>

<h3>Title: Controllable and Expressive One-Shot Video Head Swapping</h3>
<ul>
<li><strong>Authors: </strong>Chaonan Ji, Jinwei Qi, Peng Zhang, Bang Zhang, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16852">https://arxiv.org/abs/2506.16852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16852">https://arxiv.org/pdf/2506.16852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16852]] Controllable and Expressive One-Shot Video Head Swapping(https://arxiv.org/abs/2506.16852)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel diffusion-based multi-condition controllable framework for video head swapping, which seamlessly transplant a human head from a static image into a dynamic video, while preserving the original body and background of target video, and further allowing to tweak head expressions and movements during swapping as needed. Existing face-swapping methods mainly focus on localized facial replacement neglecting holistic head morphology, while head-swapping approaches struggling with hairstyle diversity and complex backgrounds, and none of these methods allow users to modify the transplanted head expressions after swapping. To tackle these challenges, our method incorporates several innovative strategies through a unified latent diffusion paradigm. 1) Identity-preserving context fusion: We propose a shape-agnostic mask strategy to explicitly disentangle foreground head identity features from background/body contexts, combining hair enhancement strategy to achieve robust holistic head identity preservation across diverse hair types and complex backgrounds. 2) Expression-aware landmark retargeting and editing: We propose a disentangled 3DMM-driven retargeting module that decouples identity, expression, and head poses, minimizing the impact of original expressions in input images and supporting expression editing. While a scale-aware retargeting strategy is further employed to minimize cross-identity expression distortion for higher transfer precision. Experimental results demonstrate that our method excels in seamless background integration while preserving the identity of the source portrait, as well as showcasing superior expression transfer capabilities applicable to both real and virtual characters.</li>
</ul>

<h3>Title: Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Semin Kim, Yeonwoo Cha, Jaehoon Yoo, Seunghoon Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16853">https://arxiv.org/abs/2506.16853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16853">https://arxiv.org/pdf/2506.16853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16853]] Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models(https://arxiv.org/abs/2506.16853)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We investigate a general approach for improving user prompts in text-to-image (T2I) diffusion models by finding prompts that maximize a reward function specified at test-time. Although diverse reward models are used for evaluating image generation, existing automated prompt engineering methods typically target specific reward configurations. Consequently, these specialized designs exhibit suboptimal performance when applied to new prompt engineering scenarios involving different reward models. To address this limitation, we introduce RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time optimization method applicable across various reward scenarios without modification. RATTPO iteratively searches for optimized prompts by querying large language models (LLMs) \textit{without} requiring reward-specific task descriptions. Instead, it uses the optimization trajectory and a novel reward-aware feedback signal (termed a "hint") as context. Empirical results demonstrate the versatility of RATTPO, effectively enhancing user prompts across diverse reward setups that assess various generation aspects, such as aesthetics, general human preference, or spatial relationships between objects. RATTPO surpasses other test-time search baselines in search efficiency, using up to 3.5 times less inference budget, and, given sufficient inference budget, achieves performance comparable to learning-based baselines that require reward-specific fine-tuning. The code is available at this https URL.</li>
</ul>

<h3>Title: Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning</h3>
<ul>
<li><strong>Authors: </strong>Shaoyu Dou, Kai Yang, Yang Jiao, Chengbo Qiu, Kui Ren</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16855">https://arxiv.org/abs/2506.16855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16855">https://arxiv.org/pdf/2506.16855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16855]] Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning(https://arxiv.org/abs/2506.16855)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series analysis has achieved great success in cyber security such as intrusion detection and device identification. Learning similarities among multiple time series is a crucial problem since it serves as the foundation for downstream analysis. Due to the complex temporal dynamics of the event-triggered time series, it often remains unclear which similarity metric is appropriate for security-related tasks, such as anomaly detection and clustering. The overarching goal of this paper is to develop an unsupervised learning framework that is capable of learning similarities among a set of event-triggered time series. From the machine learning vantage point, the proposed framework harnesses the power of both hierarchical multi-resolution sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively learn the low-dimensional representations from the time series. Finally, the obtained similarity measure can be easily visualized for the explanation. The proposed framework aspires to offer a stepping stone that gives rise to a systematic approach to model and learn similarities among a multitude of event-triggered time series. Through extensive qualitative and quantitative experiments, it is revealed that the proposed method outperforms state-of-the-art methods considerably.</li>
</ul>

<h3>Title: From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Hönel, Jonas Nordqvist</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16890">https://arxiv.org/abs/2506.16890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16890">https://arxiv.org/pdf/2506.16890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16890]] From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images(https://arxiv.org/abs/2506.16890)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The detection and localization of quality-related problems in industrially mass-produced products has historically relied on manual inspection, which is costly and error-prone. Machine learning has the potential to replace manual handling. As such, the desire is to facilitate an unsupervised (or self-supervised) approach, as it is often impossible to specify all conceivable defects ahead of time. A plethora of prior works have demonstrated the aptitude of common reconstruction-, embedding-, and synthesis-based methods in laboratory settings. However, in practice, we observe that most methods do not handle low data quality well or exude low robustness in unfavorable, but typical real-world settings. For practitioners it may be very difficult to identify the actual underlying problem when such methods underperform. Worse, often-reported metrics (e.g., AUROC) are rarely suitable in practice and may give misleading results. In our setting, we attempt to identify subtle anomalies on the surface of blasted forged metal parts, using rather low-quality RGB imagery only, which is a common industrial setting. We specifically evaluate two types of state-of-the-art models that allow us to identify and improve quality issues in production data, without having to obtain new data. Our contribution is to provide guardrails for practitioners that allow them to identify problems related to, e.g., (lack of) robustness or invariance, in either the chosen model or the data reliably in similar scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of likelihood-based approaches and outline a framework for proper empirical risk estimation that is more suitable for real-world scenarios.</li>
</ul>

<h3>Title: With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You</h3>
<ul>
<li><strong>Authors: </strong>Fabian Gröger, Shuo Wen, Huyen Le, Maria Brbić</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16895">https://arxiv.org/abs/2506.16895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16895">https://arxiv.org/pdf/2506.16895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16895]] With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You(https://arxiv.org/abs/2506.16895)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multimodal models have demonstrated powerful capabilities in complex tasks requiring multimodal alignment including zero-shot classification and cross-modal retrieval. However, existing models typically rely on millions of paired multimodal samples, which are prohibitively expensive or infeasible to obtain in many domains. In this work, we explore the feasibility of building multimodal models with limited amount of paired data by aligning pretrained unimodal foundation models. We show that high-quality alignment is possible with as few as tens of thousands of paired samples$\unicode{x2013}$less than $1\%$ of the data typically used in the field. To achieve this, we introduce STRUCTURE, an effective regularization technique that preserves the neighborhood geometry of the latent space of unimodal encoders. Additionally, we show that aligning last layers is often suboptimal and demonstrate the benefits of aligning the layers with the highest representational similarity across modalities. These two components can be readily incorporated into existing alignment methods, yielding substantial gains across 24 zero-shot image classification and retrieval benchmarks, with average relative improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our results highlight the effectiveness and broad applicability of our framework for limited-sample multimodal learning and offer a promising path forward for resource-constrained domains.</li>
</ul>

<h3>Title: Visual-Instructed Degradation Diffusion for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Wenyang Luo, Haina Qin, Zewen Chen, Libin Wang, Dandan Zheng, Yuming Li, Yufan Liu, Bing Li, Weiming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16960">https://arxiv.org/abs/2506.16960</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16960">https://arxiv.org/pdf/2506.16960</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16960]] Visual-Instructed Degradation Diffusion for All-in-One Image Restoration(https://arxiv.org/abs/2506.16960)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown degradations. In this work, we propose \textbf{Defusion}, a novel all-in-one image restoration framework that utilizes visual instruction-guided degradation diffusion. Unlike existing methods that rely on task-specific models or ambiguous text-based priors, Defusion constructs explicit \textbf{visual instructions} that align with the visual degradation patterns. These instructions are grounded by applying degradations to standardized visual elements, capturing intrinsic degradation features while agnostic to image semantics. Defusion then uses these visual instructions to guide a diffusion-based model that operates directly in the degradation space, where it reconstructs high-quality images by denoising the degradation effects with enhanced stability and generalizability. Comprehensive experiments demonstrate that Defusion outperforms state-of-the-art methods across diverse image restoration tasks, including complex and real-world degradations.</li>
</ul>

<h3>Title: Reversing Flow for Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Haina Qin, Wenyang Luo, Libin Wang, Dandan Zheng, Jingdong Chen, Ming Yang, Bing Li, Weiming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16961">https://arxiv.org/abs/2506.16961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16961">https://arxiv.org/pdf/2506.16961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16961]] Reversing Flow for Image Restoration(https://arxiv.org/abs/2506.16961)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Image restoration aims to recover high-quality (HQ) images from degraded low-quality (LQ) ones by reversing the effects of degradation. Existing generative models for image restoration, including diffusion and score-based models, often treat the degradation process as a stochastic transformation, which introduces inefficiency and complexity. In this work, we propose ResFlow, a novel image restoration framework that models the degradation process as a deterministic path using continuous normalizing flows. ResFlow augments the degradation process with an auxiliary process that disambiguates the uncertainty in HQ prediction to enable reversible modeling of the degradation process. ResFlow adopts entropy-preserving flow paths and learns the augmented degradation flow by matching the velocity field. ResFlow significantly improves the performance and speed of image restoration, completing the task in fewer than four sampling steps. Extensive experiments demonstrate that ResFlow achieves state-of-the-art results across various image restoration benchmarks, offering a practical and efficient solution for real-world applications.</li>
</ul>

<h3>Title: Latent Concept Disentanglement in Transformer-based Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guan Zhe Hong, Bhavya Vasudeva, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.16975">https://arxiv.org/abs/2506.16975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.16975">https://arxiv.org/pdf/2506.16975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.16975]] Latent Concept Disentanglement in Transformer-based Language Models(https://arxiv.org/abs/2506.16975)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>When large language models (LLMs) use in-context learning (ICL) to solve a new task, they seem to grasp not only the goal of the task but also core, latent concepts in the demonstration examples. This begs the question of whether transformers represent latent structures as part of their computation or whether they take shortcuts to solve the problem. Prior mechanistic work on ICL does not address this question because it does not sufficiently examine the relationship between the learned representation and the latent concept, and the considered problem settings often involve only single-step reasoning. In this work, we examine how transformers disentangle and use latent concepts. We show that in 2-hop reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. In tasks parameterized by a continuous latent concept, we find low-dimensional subspaces in the representation space where the geometry mimics the underlying parameterization. Together, these results refine our understanding of ICL and the representation of transformers, and they provide evidence for highly localized structures in the model that disentangle latent concepts in ICL tasks.</li>
</ul>

<h3>Title: LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation</h3>
<ul>
<li><strong>Authors: </strong>Elizabeth Fons, Alejandro Sztrajman, Yousef El-Laham, Luciana Ferrer, Svitlana Vyetrenko, Manuela Veloso</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17039">https://arxiv.org/abs/2506.17039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17039">https://arxiv.org/pdf/2506.17039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17039]] LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation(https://arxiv.org/abs/2506.17039)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Time series with missing or irregularly sampled data are a persistent challenge in machine learning. Many methods operate on the frequency-domain, relying on the Fast Fourier Transform (FFT) which assumes uniform sampling, therefore requiring prior interpolation that can distort the spectra. To address this limitation, we introduce a differentiable Lomb--Scargle layer that enables a reliable computation of the power spectrum of irregularly sampled data. We integrate this layer into a novel score-based diffusion model (LSCD) for time series imputation conditioned on the entire signal spectrum. Experiments on synthetic and real-world benchmarks demonstrate that our method recovers missing data more accurately than purely time-domain baselines, while simultaneously producing consistent frequency estimates. Crucially, our method can be easily integrated into learning frameworks, enabling broader adoption of spectral guidance in machine learning approaches involving incomplete or irregular data.</li>
</ul>

<h3>Title: MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Joshua Schraven, Alexander Windmann, Oliver Niggemann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17041">https://arxiv.org/abs/2506.17041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17041">https://arxiv.org/pdf/2506.17041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17041]] MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection(https://arxiv.org/abs/2506.17041)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Benchmark datasets for network intrusion detection commonly rely on synthetically generated traffic, which fails to reflect the statistical variability and temporal drift encountered in operational environments. This paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1 dataset, designed to enable realistic and reproducible evaluation of anomaly detection methods. A reproducible preprocessing pipeline is presented that transforms raw packet captures into flow representations conforming to the CICFlowMeter format, while preserving MAWILab's original anomaly labels. The resulting datasets comprise temporally distinct samples from January 2011, 2016, and 2021, drawn from trans-Pacific backbone traffic. To establish reference baselines, traditional machine learning methods, including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical results demonstrate that tree-based classifiers perform well on temporally static data but experience significant performance degradation over time. In contrast, the CNN-BiLSTM model maintains better performance, thus showing improved generalization. These findings underscore the limitations of synthetic benchmarks and static models, and motivate the adoption of realistic datasets with explicit temporal structure. All datasets, pipeline code, and model implementations are made publicly available to foster transparency and reproducibility.</li>
</ul>

<h3>Title: Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17074">https://arxiv.org/abs/2506.17074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17074">https://arxiv.org/pdf/2506.17074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17074]] Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion(https://arxiv.org/abs/2506.17074)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present Assembler, a scalable and generalizable framework for 3D part assembly that reconstructs complete objects from input part meshes and a reference image. Unlike prior approaches that mostly rely on deterministic part pose prediction and category-specific training, Assembler is designed to handle diverse, in-the-wild objects with varying part counts, geometries, and structures. It addresses the core challenges of scaling to general 3D part assembly through innovations in task formulation, representation, and data. First, Assembler casts part assembly as a generative problem and employs diffusion models to sample plausible configurations, effectively capturing ambiguities arising from symmetry, repeated parts, and multiple valid assemblies. Second, we introduce a novel shape-centric representation based on sparse anchor point clouds, enabling scalable generation in Euclidean space rather than SE(3) pose prediction. Third, we construct a large-scale dataset of over 320K diverse part-object assemblies using a synthesis and filtering pipeline built on existing 3D shape repositories. Assembler achieves state-of-the-art performance on PartNet and is the first to demonstrate high-quality assembly for complex, real-world objects. Based on Assembler, we further introduce an interesting part-aware 3D modeling system that generates high-resolution, editable objects from images, demonstrating potential for interactive and compositional design. Project page: this https URL</li>
</ul>

<h3>Title: RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking</h3>
<ul>
<li><strong>Authors: </strong>Teng Guo, Jingjin Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17119">https://arxiv.org/abs/2506.17119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17119">https://arxiv.org/pdf/2506.17119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17119]] RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking(https://arxiv.org/abs/2506.17119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a robust framework, RGBTrack, for real-time 6D pose estimation and tracking that operates solely on RGB data, thereby eliminating the need for depth input for such dynamic and precise object pose tracking tasks. Building on the FoundationPose architecture, we devise a novel binary search strategy combined with a render-and-compare mechanism to efficiently infer depth and generate robust pose hypotheses from true-scale CAD models. To maintain stable tracking in dynamic scenarios, including rapid movements and occlusions, RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman filter and a state machine for proactive object pose recovery. In addition, RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale using an initial depth estimate, enabling seamless integration with modern generative reconstruction techniques. Extensive evaluations on benchmark datasets demonstrate that RGBTrack's novel depth-free approach achieves competitive accuracy and real-time performance, making it a promising practical solution candidate for application areas including robotics, augmented reality, and computer vision. The source code for our implementation will be made publicly available at this https URL.</li>
</ul>

<h3>Title: Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model</h3>
<ul>
<li><strong>Authors: </strong>Botao Zhu, Xianbin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17128">https://arxiv.org/abs/2506.17128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17128">https://arxiv.org/pdf/2506.17128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17128]] Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model(https://arxiv.org/abs/2506.17128)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Trust is emerging as an effective tool to ensure the successful completion of collaborative tasks within collaborative systems. However, rapidly and continuously evaluating the trustworthiness of collaborators during task execution is a significant challenge due to distributed devices, complex operational environments, and dynamically changing resources. To tackle this challenge, this paper proposes a Siamese-enabled rapid and continuous trust evaluation framework (SRCTE) to facilitate effective task collaboration. First, the communication and computing resource attributes of the collaborator in a trusted state, along with historical collaboration data, are collected and represented using an attributed control flow graph (ACFG) that captures trust-related semantic information and serves as a reference for comparison with data collected during task execution. At each time slot of task execution, the collaborator's communication and computing resource attributes, as well as task completion effectiveness, are collected in real time and represented with an ACFG to convey their trust-related semantic information. A Siamese model, consisting of two shared-parameter Structure2vec networks, is then employed to learn the deep semantics of each pair of ACFGs and generate their embeddings. Finally, the similarity between the embeddings of each pair of ACFGs is calculated to determine the collaborator's trust value at each time slot. A real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to test the effectiveness of the proposed SRCTE framework. Experimental results demonstrate that SRCTE converges rapidly with only a small amount of data and achieves a high anomaly trust detection rate compared to the baseline algorithm.</li>
</ul>

<h3>Title: Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Michael Plainer, Hao Wu, Leon Klein, Stephan Günnemann, Frank Noé</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph, physics.comp-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17139">https://arxiv.org/abs/2506.17139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17139">https://arxiv.org/pdf/2506.17139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17139]] Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models(https://arxiv.org/abs/2506.17139)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently gained significant attention due to their effectiveness in various scientific domains, including biochemistry. When trained on equilibrium molecular distributions, diffusion models provide both: a generative procedure to sample equilibrium conformations and associated forces derived from the model's scores. However, using the forces for coarse-grained molecular dynamics simulations uncovers inconsistencies in the samples generated via classical diffusion inference and simulation, despite both originating from the same model. Particularly at the small diffusion timesteps required for simulations, diffusion models fail to satisfy the Fokker-Planck equation, which governs how the score should evolve over time. We interpret this deviation as an indication of the observed inconsistencies and propose an energy-based diffusion model with a Fokker-Planck-derived regularization term enforcing consistency. We demonstrate the effectiveness of our approach on toy systems, alanine dipeptide, and introduce a state-of-the-art transferable Boltzmann emulator for dipeptides that supports simulation and demonstrates enhanced consistency and efficient sampling.</li>
</ul>

<h3>Title: Deep generative models as the probability transformation functions</h3>
<ul>
<li><strong>Authors: </strong>Vitalii Bondar, Vira Babenko, Roman Trembovetskyi, Yurii Korobeinyk, Viktoriya Dzyuba</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17171">https://arxiv.org/abs/2506.17171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17171">https://arxiv.org/pdf/2506.17171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17171]] Deep generative models as the probability transformation functions(https://arxiv.org/abs/2506.17171)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a unified theoretical perspective that views deep generative models as probability transformation functions. Despite the apparent differences in architecture and training methodologies among various types of generative models - autoencoders, autoregressive models, generative adversarial networks, normalizing flows, diffusion models, and flow matching - we demonstrate that they all fundamentally operate by transforming simple predefined distributions into complex target data distributions. This unifying perspective facilitates the transfer of methodological improvements between model architectures and provides a foundation for developing universal theoretical approaches, potentially leading to more efficient and effective generative modeling techniques.</li>
</ul>

<h3>Title: Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Li, Junshu Tang, Zhiyong Xu, Longhuang Wu, Yuan Zhou, Shuai Shao, Tianbao Yu, Zhiguo Cao, Qinglin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17201">https://arxiv.org/abs/2506.17201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17201">https://arxiv.org/pdf/2506.17201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17201]] Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition(https://arxiv.org/abs/2506.17201)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion-based and controllable video generation have enabled high-quality and temporally coherent video synthesis, laying the groundwork for immersive interactive gaming experiences. However, current methods face limitations in dynamics, generality, long-term consistency, and efficiency, which limit the ability to create various gameplay videos. To address these gaps, we introduce Hunyuan-GameCraft, a novel framework for high-dynamic interactive video generation in game environments. To achieve fine-grained action control, we unify standard keyboard and mouse inputs into a shared camera representation space, facilitating smooth interpolation between various camera and movement operations. Then we propose a hybrid history-conditioned training strategy that extends video sequences autoregressively while preserving game scene information. Additionally, to enhance inference efficiency and playability, we achieve model distillation to reduce computational overhead while maintaining consistency across long temporal sequences, making it suitable for real-time deployment in complex interactive environments. The model is trained on a large-scale dataset comprising over one million gameplay recordings across over 100 AAA games, ensuring broad coverage and diversity, then fine-tuned on a carefully annotated synthetic dataset to enhance precision and control. The curated game scene data significantly improves the visual fidelity, realism and action controllability. Extensive experiments demonstrate that Hunyuan-GameCraft significantly outperforms existing models, advancing the realism and playability of interactive game video generation.</li>
</ul>

<h3>Title: Emergent Temporal Correspondences from Video Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jisu Nam, Soowon Son, Dahyun Chung, Jiyoung Kim, Siyoon Jin, Junhwa Hur, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17220">https://arxiv.org/abs/2506.17220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17220">https://arxiv.org/pdf/2506.17220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17220]] Emergent Temporal Correspondences from Video Diffusion Transformers(https://arxiv.org/abs/2506.17220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Recent advancements in video diffusion models based on Diffusion Transformers (DiTs) have achieved remarkable success in generating temporally coherent videos. Yet, a fundamental question persists: how do these models internally establish and represent temporal correspondences across frames? We introduce DiffTrack, the first quantitative analysis framework designed to answer this question. DiffTrack constructs a dataset of prompt-generated video with pseudo ground-truth tracking annotations and proposes novel evaluation metrics to systematically analyze how each component within the full 3D attention mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to establishing temporal correspondences. Our analysis reveals that query-key similarities in specific, but not all, layers play a critical role in temporal matching, and that this matching becomes increasingly prominent during the denoising process. We demonstrate practical applications of DiffTrack in zero-shot point tracking, where it achieves state-of-the-art performance compared to existing vision foundation and self-supervised video models. Further, we extend our findings to motion-enhanced video generation with a novel guidance method that improves temporal consistency of generated videos without additional training. We believe our work offers crucial insights into the inner workings of video DiTs and establishes a foundation for further research and applications leveraging their temporal understanding.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
