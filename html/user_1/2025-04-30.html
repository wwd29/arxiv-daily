<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-30</h1>
<h3>Title: Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiayang Sun, Hongbo Wang, Jie Cao, Huaibo Huang, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20054">https://arxiv.org/abs/2504.20054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20054">https://arxiv.org/pdf/2504.20054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20054]] Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment(https://arxiv.org/abs/2504.20054)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While diffusion models excel at generating high-quality images, they often struggle with accurate counting, attributes, and spatial relationships in complex multi-object scenes. To address these challenges, we propose Marmot, a novel and generalizable framework that employs Multi-Agent Reasoning for Multi-Object Self-Correcting, enhancing image-text alignment and facilitating more coherent multi-object image editing. Our framework adopts a divide-and-conquer strategy that decomposes the self-correction task into three critical dimensions (counting, attributes, and spatial relationships), and further divided into object-level subtasks. We construct a multi-agent editing system featuring a decision-execution-verification mechanism, effectively mitigating inter-object interference and enhancing editing reliability. To resolve the problem of subtask integration, we propose a Pixel-Domain Stitching Smoother that employs mask-guided two-stage latent space optimization. This innovation enables parallel processing of subtask results, thereby enhancing runtime efficiency while eliminating multi-stage distortion accumulation. Extensive experiments demonstrate that Marmot significantly improves accuracy in object counting, attribute assignment, and spatial relationships for image generation tasks.</li>
</ul>

<h3>Title: A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Junhong Lai, Jiyu Wei, Lin Yao, Yueming Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20069">https://arxiv.org/abs/2504.20069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20069">https://arxiv.org/pdf/2504.20069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20069]] A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives(https://arxiv.org/abs/2504.20069)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Electroencephalogram (EEG) signals play a crucial role in understanding brain activity and diagnosing neurological disorders. This review focuses on the recent development of EEG foundation models(EEG-FMs), which have shown great potential in processing and analyzing EEG data. We discuss various EEG-FMs, including their architectures, pre-training strategies, their pre-training and downstream datasets and other details. The review also highlights the challenges and future directions in this field, aiming to provide a comprehensive overview for researchers and practitioners interested in EEG analysis and related EEG-FMs.</li>
</ul>

<h3>Title: Understanding and Mitigating Risks of Generative AI in Financial Services</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Gehrmann, Claire Huang, Xian Teng, Sergei Yurovski, Iyanuoluwa Shode, Chirag S. Patel, Arjun Bhorkar, Naveen Thomas, John Doucette, David Rosenberg, Mark Dredze, David Rabinowitz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20086">https://arxiv.org/abs/2504.20086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20086">https://arxiv.org/pdf/2504.20086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20086]] Understanding and Mitigating Risks of Generative AI in Financial Services(https://arxiv.org/abs/2504.20086)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To responsibly develop Generative AI (GenAI) products, it is critical to define the scope of acceptable inputs and outputs. What constitutes a "safe" response is an actively debated question. Academic work puts an outsized focus on evaluating models by themselves for general purpose aspects such as toxicity, bias, and fairness, especially in conversational applications being used by a broad audience. In contrast, less focus is put on considering sociotechnical systems in specialized domains. Yet, those specialized systems can be subject to extensive and well-understood legal and regulatory scrutiny. These product-specific considerations need to be set in industry-specific laws, regulations, and corporate governance requirements. In this paper, we aim to highlight AI content safety considerations specific to the financial services domain and outline an associated AI content risk taxonomy. We compare this taxonomy to existing work in this space and discuss implications of risk category violations on various stakeholders. We evaluate how existing open-source technical guardrail solutions cover this taxonomy by assessing them on data collected via red-teaming activities. Our results demonstrate that these guardrails fail to detect most of the content risks we discuss.</li>
</ul>

<h3>Title: Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics</h3>
<ul>
<li><strong>Authors: </strong>Inmaculada Santamaria-Valenzuela, Victor Rodriguez-Fernandez, Javier Huertas-Tato, Jong Hyuk Park, David Camacho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20099">https://arxiv.org/abs/2504.20099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20099">https://arxiv.org/pdf/2504.20099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20099]] Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics(https://arxiv.org/abs/2504.20099)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>The present study explores the interpretability of latent spaces produced by time series foundation models, focusing on their potential for visual analysis tasks. Specifically, we evaluate the MOMENT family of models, a set of transformer-based, pre-trained architectures for multivariate time series tasks such as: imputation, prediction, classification, and anomaly detection. We evaluate the capacity of these models on five datasets to capture the underlying structures in time series data within their latent space projection and validate whether fine tuning improves the clarity of the resulting embedding spaces. Notable performance improvements in terms of loss reduction were observed after fine tuning. Visual analysis shows limited improvement in the interpretability of the embeddings, requiring further work. Results suggest that, although Time Series Foundation Models such as MOMENT are robust, their latent spaces may require additional methodological refinements to be adequately interpreted, such as alternative projection techniques, loss functions, or data preprocessing strategies. Despite the limitations of MOMENT, foundation models supose a big reduction in execution time and so a great advance for interactive visual analytics.</li>
</ul>

<h3>Title: Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yu-hsuan Chen, Jing Bi, Cyril Ngo Ngoc, Victor Oancea, Jonathan Cagan, Levent Burak Kara</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20110">https://arxiv.org/abs/2504.20110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20110">https://arxiv.org/pdf/2504.20110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20110]] Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling(https://arxiv.org/abs/2504.20110)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>AI-driven surrogate modeling has become an increasingly effective alternative to physics-based simulations for 3D design, analysis, and manufacturing. These models leverage data-driven methods to predict physical quantities traditionally requiring computationally expensive simulations. However, the scarcity of labeled CAD-to-simulation datasets has driven recent advancements in self-supervised and foundation models, where geometric representation learning is performed offline and later fine-tuned for specific downstream tasks. While these approaches have shown promise, their effectiveness is limited in applications requiring fine-scale geometric detail preservation. This work introduces a self-supervised geometric representation learning method designed to capture fine-scale geometric features from non-parametric 3D models. Unlike traditional end-to-end surrogate models, this approach decouples geometric feature extraction from downstream physics tasks, learning a latent space embedding guided by geometric reconstruction losses. Key elements include the essential use of near-zero level sampling and the innovative batch-adaptive attention-weighted loss function, which enhance the encoding of intricate design features. The proposed method is validated through case studies in structural mechanics, demonstrating strong performance in capturing design features and enabling accurate few-shot physics predictions. Comparisons with traditional parametric surrogate modeling highlight its potential to bridge the gap between geometric and physics-based representations, providing an effective solution for surrogate modeling in data-scarce scenarios.</li>
</ul>

<h3>Title: Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Anubhav Jain, Yuya Kobayashi, Naoki Murata, Yuhta Takida, Takashi Shibuya, Yuki Mitsufuji, Niv Cohen, Nasir Memon, Julian Togelius</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20111">https://arxiv.org/abs/2504.20111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20111">https://arxiv.org/pdf/2504.20111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20111]] Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image(https://arxiv.org/abs/2504.20111)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Watermarking techniques are vital for protecting intellectual property and preventing fraudulent use of media. Most previous watermarking schemes designed for diffusion models embed a secret key in the initial noise. The resulting pattern is often considered hard to remove and forge into unrelated images. In this paper, we propose a black-box adversarial attack without presuming access to the diffusion model weights. Our attack uses only a single watermarked example and is based on a simple observation: there is a many-to-one mapping between images and initial noises. There are regions in the clean image latent space pertaining to each watermark that get mapped to the same initial noise when inverted. Based on this intuition, we propose an adversarial attack to forge the watermark by introducing perturbations to the images such that we can enter the region of watermarked images. We show that we can also apply a similar approach for watermark removal by learning perturbations to exit this region. We report results on multiple watermarking schemes (Tree-Ring, RingID, WIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0). Our results demonstrate the effectiveness of the attack and expose vulnerabilities in the watermarking methods, motivating future research on improving them.</li>
</ul>

<h3>Title: Supervised Pretraining for Material Property Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chowdhury Mohammad Abid Rahman, Aldo H. Romero, Prashnna K. Gyawali</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20112">https://arxiv.org/abs/2504.20112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20112">https://arxiv.org/pdf/2504.20112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20112]] Supervised Pretraining for Material Property Prediction(https://arxiv.org/abs/2504.20112)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Accurate prediction of material properties facilitates the discovery of novel materials with tailored functionalities. Deep learning models have recently shown superior accuracy and flexibility in capturing structure-property relationships. However, these models often rely on supervised learning, which requires large, well-annotated datasets an expensive and time-consuming process. Self-supervised learning (SSL) offers a promising alternative by pretraining on large, unlabeled datasets to develop foundation models that can be fine-tuned for material property prediction. In this work, we propose supervised pretraining, where available class information serves as surrogate labels to guide learning, even when downstream tasks involve unrelated material properties. We evaluate this strategy on two state-of-the-art SSL models and introduce a novel framework for supervised pretraining. To further enhance representation learning, we propose a graph-based augmentation technique that injects noise to improve robustness without structurally deforming material graphs. The resulting foundation models are fine-tuned for six challenging material property predictions, achieving significant performance gains over baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE) and establishing a new benchmark in material property prediction. This study represents the first exploration of supervised pertaining with surrogate labels in material property prediction, advancing methodology and application in the field.</li>
</ul>

<h3>Title: Integration Flow Models</h3>
<ul>
<li><strong>Authors: </strong>Jingjing Wang, Dan Zhang, Joshua Luo, Yin Yang, Feng Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20179">https://arxiv.org/abs/2504.20179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20179">https://arxiv.org/pdf/2504.20179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20179]] Integration Flow Models(https://arxiv.org/abs/2504.20179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Ordinary differential equation (ODE) based generative models have emerged as a powerful approach for producing high-quality samples in many applications. However, the ODE-based methods either suffer the discretization error of numerical solvers of ODE, which restricts the quality of samples when only a few NFEs are used, or struggle with training instability. In this paper, we proposed Integration Flow, which directly learns the integral of ODE-based trajectory paths without solving the ODE functions. Moreover, Integration Flow explicitly incorporates the target state $\mathbf{x}_0$ as the anchor state in guiding the reverse-time dynamics. We have theoretically proven this can contribute to both stability and accuracy. To the best of our knowledge, Integration Flow is the first model with a unified structure to estimate ODE-based generative models and the first to show the exact straightness of 1-Rectified Flow without reflow. Through theoretical analysis and empirical evaluations, we show that Integration Flows achieve improved performance when it is applied to existing ODE-based models, such as diffusion models, Rectified Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model, 3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without reflow and 4.15 for PFGM++.</li>
</ul>

<h3>Title: Cybersecurity for Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Sai varun reddy Bhemavarapu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20180">https://arxiv.org/abs/2504.20180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20180">https://arxiv.org/pdf/2504.20180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20180]] Cybersecurity for Autonomous Vehicles(https://arxiv.org/abs/2504.20180)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The increasing adoption of autonomous vehicles is bringing a major shift in the automotive industry. However, as these vehicles become more connected, cybersecurity threats have emerged as a serious concern. Protecting the security and integrity of autonomous systems is essential to prevent malicious activities that can harm passengers, other road users, and the overall transportation network. This paper focuses on addressing the cybersecurity issues in autonomous vehicles by examining the challenges and risks involved, which are important for building a secure future. Since autonomous vehicles depend on the communication between sensors, artificial intelligence, external infrastructure, and other systems, they are exposed to different types of cyber threats. A cybersecurity breach in an autonomous vehicle can cause serious problems, including a loss of public trust and safety. Therefore, it is very important to develop and apply strong cybersecurity measures to support the growth and acceptance of self-driving cars. This paper discusses major cybersecurity challenges like vulnerabilities in software and hardware, risks from wireless communication, and threats through external interfaces. It also reviews existing solutions such as secure software development, intrusion detection systems, cryptographic protocols, and anomaly detection methods. Additionally, the paper highlights the role of regulatory bodies, industry collaborations, and cybersecurity standards in creating a secure environment for autonomous vehicles. Setting clear rules and best practices is necessary for consistent protection across manufacturers and regions. By analyzing the current cybersecurity landscape and suggesting practical countermeasures, this paper aims to contribute to the safe development and public trust of autonomous vehicle technology.</li>
</ul>

<h3>Title: Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts</h3>
<ul>
<li><strong>Authors: </strong>Kamirul Kamirul, Odysseas Pappas, Alin Achim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20241">https://arxiv.org/abs/2504.20241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20241">https://arxiv.org/pdf/2504.20241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20241]] Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts(https://arxiv.org/abs/2504.20241)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Detecting ship presence via wake signatures in SAR imagery is attracting considerable research interest, but limited annotated data availability poses significant challenges for supervised learning. Physics-based simulations are commonly used to address this data scarcity, although they are slow and constrain end-to-end learning. In this work, we explore a new direction for more efficient and end-to-end SAR ship wake simulation using a diffusion model trained on data generated by a physics-based simulator. The training dataset is built by pairing images produced by the simulator with text prompts derived from simulation parameters. Experimental result show that the model generates realistic Kelvin wake patterns and achieves significantly faster inference than the physics-based simulator. These results highlight the potential of diffusion models for fast and controllable wake image generation, opening new possibilities for end-to-end downstream tasks in maritime SAR analysis.</li>
</ul>

<h3>Title: Generative Diffusion Models for Resource Allocation in Wireless Networks</h3>
<ul>
<li><strong>Authors: </strong>Yigit Berkay Uslu, Samar Hadou, Shirin Saeedi Bidokhti, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20277">https://arxiv.org/abs/2504.20277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20277">https://arxiv.org/pdf/2504.20277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20277]] Generative Diffusion Models for Resource Allocation in Wireless Networks(https://arxiv.org/abs/2504.20277)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper proposes a supervised training algorithm for learning stochastic resource allocation policies with generative diffusion models (GDMs). We formulate the allocation problem as the maximization of an ergodic utility function subject to ergodic Quality of Service (QoS) constraints. Given samples from a stochastic expert policy that yields a near-optimal solution to the problem, we train a GDM policy to imitate the expert and generate new samples from the optimal distribution. We achieve near-optimal performance through sequential execution of the generated samples. To enable generalization to a family of network configurations, we parameterize the backward diffusion process with a graph neural network (GNN) architecture. We present numerical results in a case study of power control in multi-user interference networks.</li>
</ul>

<h3>Title: Image Interpolation with Score-based Riemannian Metrics of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shinnosuke Saito, Takashi Matsubara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20288">https://arxiv.org/abs/2504.20288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20288">https://arxiv.org/pdf/2504.20288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20288]] Image Interpolation with Score-based Riemannian Metrics of Diffusion Models(https://arxiv.org/abs/2504.20288)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models excel in content generation by implicitly learning the data manifold, yet they lack a practical method to leverage this manifold - unlike other deep generative models equipped with latent spaces. This paper introduces a novel framework that treats the data space of pre-trained diffusion models as a Riemannian manifold, with a metric derived from the score function. Experiments with MNIST and Stable Diffusion show that this geometry-aware approach yields image interpolations that are more realistic, less noisy, and more faithful to prompts than existing methods, demonstrating its potential for improved content generation and editing.</li>
</ul>

<h3>Title: The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Mohammadhossein Homaei, Victor Gonzalez Morales, Oscar Mogollon-Gutierrez, Andres Caro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20295">https://arxiv.org/abs/2504.20295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20295">https://arxiv.org/pdf/2504.20295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20295]] The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting(https://arxiv.org/abs/2504.20295)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Digital twins (DTs) are improving water distribution systems by using real-time data, analytics, and prediction models to optimize operations. This paper presents a DT platform designed for a Spanish water supply network, utilizing Long Short-Term Memory (LSTM) networks to predict water consumption. However, machine learning models are vulnerable to adversarial attacks, such as the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). These attacks manipulate critical model parameters, injecting subtle distortions that degrade forecasting accuracy. To further exploit these vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based approach that dynamically adjusts perturbations, making adversarial attacks more difficult to detect. Experimental results show that this approach significantly impacts prediction reliability, causing the Mean Absolute Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack strategies amplify this effect, highlighting cybersecurity risks in AI-driven DTs. These findings emphasize the urgent need for robust defenses, including adversarial training, anomaly detection, and secure data pipelines.</li>
</ul>

<h3>Title: DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes</h3>
<ul>
<li><strong>Authors: </strong>Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Siqi Lu, Quan Liu, Ruining Deng, Can Cui, Jialin Yue, Yizhe Lin, Tianyuan Yao, Juming Xiong, Junchao Zhu, Chongyu Qu, Yuechen Yang, Mitchell Wilkes, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20303">https://arxiv.org/abs/2504.20303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20303">https://arxiv.org/pdf/2504.20303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20303]] DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes(https://arxiv.org/abs/2504.20303)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>By mapping sites at large scales using remotely sensed data, archaeologists can generate unique insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change. Remote sensing surveys complement field-based approaches, and their reach can be especially great when combined with deep learning and computer vision techniques. However, conventional supervised deep learning methods face challenges in annotating fine-grained archaeological features at scale. While recent vision foundation models have shown remarkable success in learning large-scale remote sensing data with minimal annotations, most off-the-shelf solutions are designed for RGB images rather than multi-spectral satellite imagery, such as the 8-band data used in our study. In this paper, we introduce DeepAndes, a transformer-based vision foundation model trained on three million multi-spectral satellite images, specifically tailored for Andean archaeology. DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery, marking the first foundation model designed explicitly for the Andes region. We evaluate its image understanding performance through imbalanced image classification, image instance retrieval, and pixel-level semantic segmentation tasks. Our experiments show that DeepAndes achieves superior F1 scores, mean average precision, and Dice scores in few-shot learning scenarios, significantly outperforming models trained from scratch or pre-trained on smaller datasets. This underscores the effectiveness of large-scale self-supervised pre-training in archaeological remote sensing. Codes will be available on this https URL.</li>
</ul>

<h3>Title: A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Greg Gluch, Shafi Goldwasser</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20310">https://arxiv.org/abs/2504.20310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20310">https://arxiv.org/pdf/2504.20310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20310]] A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning(https://arxiv.org/abs/2504.20310)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we initiate a cryptographically inspired theoretical study of detection versus mitigation of adversarial inputs produced by attackers of Machine Learning algorithms during inference time. We formally define defense by detection (DbD) and defense by mitigation (DbM). Our definitions come in the form of a 3-round protocol between two resource-bounded parties: a trainer/defender and an attacker. The attacker aims to produce inference-time inputs that fool the training algorithm. We define correctness, completeness, and soundness properties to capture successful defense at inference time while not degrading (too much) the performance of the algorithm on inputs from the training distribution. We first show that achieving DbD and achieving DbM are equivalent for ML classification tasks. Surprisingly, this is not the case for ML generative learning tasks, where there are many possible correct outputs that can be generated for each input. We show a separation between DbD and DbM by exhibiting a generative learning task for which is possible to defend by mitigation but is provably impossible to defend by detection under the assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE), publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation phase uses significantly fewer samples than the initial training algorithm.</li>
</ul>

<h3>Title: Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach</h3>
<ul>
<li><strong>Authors: </strong>Huchen Yang, Xinghao Dong, Jin-Long Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20319">https://arxiv.org/abs/2504.20319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20319">https://arxiv.org/pdf/2504.20319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20319]] Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach(https://arxiv.org/abs/2504.20319)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Bayesian experimental design (BED) offers a principled framework for optimizing data acquisition by leveraging probabilistic inference. However, practical implementations of BED are often compromised by model discrepancy, i.e., the mismatch between predictive models and true physical systems, which can potentially lead to biased parameter estimates. While data-driven approaches have been recently explored to characterize the model discrepancy, the resulting high-dimensional parameter space poses severe challenges for both Bayesian updating and design optimization. In this work, we propose a hybrid BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI) that addresses these challenges by providing a computationally efficient, gradient-free alternative to estimate the information gain for high-dimensional network parameters. The AD-EKI allows a differentiable evaluation of the utility function in BED and thus facilitates the use of standard gradient-based methods for design optimization. In the proposed hybrid framework, we iteratively optimize experimental designs, decoupling the inference of low-dimensional physical parameters handled by standard BED methods, from the high-dimensional model discrepancy handled by AD-EKI. The identified optimal designs for the model discrepancy enable us to systematically collect informative data for its calibration. The performance of the proposed method is studied by a classical convection-diffusion BED example, and the hybrid framework enabled by AD-EKI efficiently identifies informative data to calibrate the model discrepancy and robustly infers the unknown physical parameters in the modeled system. Besides addressing the challenges of BED with model discrepancy, AD-EKI also potentially fosters efficient and scalable frameworks in many other areas with bilevel optimization, such as meta-learning and structure optimization.</li>
</ul>

<h3>Title: Generative Learning for Slow Manifolds and Bifurcation Diagrams</h3>
<ul>
<li><strong>Authors: </strong>Ellis R. Crabtree, Dimitris G. Giovanis, Nikolaos Evangelou, Juan M. Bello-Rivas, Ioannis G. Kevrekidis</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20375">https://arxiv.org/abs/2504.20375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20375">https://arxiv.org/pdf/2504.20375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20375]] Generative Learning for Slow Manifolds and Bifurcation Diagrams(https://arxiv.org/abs/2504.20375)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In dynamical systems characterized by separation of time scales, the approximation of so called ``slow manifolds'', on which the long term dynamics lie, is a useful step for model reduction. Initializing on such slow manifolds is a useful step in modeling, since it circumvents fast transients, and is crucial in multiscale algorithms alternating between fine scale (fast) and coarser scale (slow) simulations. In a similar spirit, when one studies the infinite time dynamics of systems depending on parameters, the system attractors (e.g., its steady states) lie on bifurcation diagrams. Sampling these manifolds gives us representative attractors (here, steady states of ODEs or PDEs) at different parameter values. Algorithms for the systematic construction of these manifolds are required parts of the ``traditional'' numerical nonlinear dynamics toolkit. In more recent years, as the field of Machine Learning develops, conditional score-based generative models (cSGMs) have demonstrated capabilities in generating plausible data from target distributions that are conditioned on some given label. It is tempting to exploit such generative models to produce samples of data distributions conditioned on some quantity of interest (QoI). In this work, we present a framework for using cSGMs to quickly (a) initialize on a low-dimensional (reduced-order) slow manifold of a multi-time-scale system consistent with desired value(s) of a QoI (a ``label'') on the manifold, and (b) approximate steady states in a bifurcation diagram consistent with a (new, out-of-sample) parameter value. This conditional sampling can help uncover the geometry of the reduced slow-manifold and/or approximately ``fill in'' missing segments of steady states in a bifurcation diagram.</li>
</ul>

<h3>Title: ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes</h3>
<ul>
<li><strong>Authors: </strong>Amartya Mukherjee, Ruizhi Deng, He Zhao, Yuzhen Mao, Leonid Sigal, Frederick Tung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20411">https://arxiv.org/abs/2504.20411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20411">https://arxiv.org/pdf/2504.20411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20411]] ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes(https://arxiv.org/abs/2504.20411)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work introduces a novel approach to modeling temporal point processes using diffusion models with an asynchronous noise schedule. At each step of the diffusion process, the noise schedule injects noise of varying scales into different parts of the data. With a careful design of the noise schedules, earlier events are generated faster than later ones, thus providing stronger conditioning for forecasting the more distant future. We derive an objective to effectively train these models for a general family of noise schedules based on conditional flow matching. Our method models the joint distribution of the latent representations of events in a sequence and achieves state-of-the-art results in predicting both the next inter-event time and event type on benchmark datasets. Additionally, it flexibly accommodates varying lengths of observation and prediction windows in different forecasting settings by adjusting the starting and ending points of the generation process. Finally, our method shows superior performance in long-horizon prediction tasks, outperforming existing baseline methods.</li>
</ul>

<h3>Title: PixelHacker: Image Inpainting with Structural and Semantic Consistency</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Xu, Kangsheng Duan, Xiaolei Shen, Zhifeng Ding, Wenyu Liu, Xiaohu Ruan, Xiaoxin Chen, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20438">https://arxiv.org/abs/2504.20438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20438">https://arxiv.org/pdf/2504.20438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20438]] PixelHacker: Image Inpainting with Structural and Semantic Consistency(https://arxiv.org/abs/2504.20438)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at this https URL.</li>
</ul>

<h3>Title: Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Gabe Guo, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20456">https://arxiv.org/abs/2504.20456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20456">https://arxiv.org/pdf/2504.20456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20456]] Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding(https://arxiv.org/abs/2504.20456)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In arbitrary-order language models, it is an open question how to sample tokens in parallel from the correct joint distribution. With discrete diffusion models, the more tokens they generate in parallel, the less their predicted distributions adhere to the originally learned data distribution, as they rely on a conditional independence assumption that only works with infinitesimally small timesteps. We find that a different class of models, any-subset autoregressive models (AS-ARMs), holds the solution. As implied by the name, AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs support parallelized joint probability density estimation, allowing them to correct their own parallel-generated token distributions, via our Any-Subset Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of tokens from the correct joint distribution, with the number of neural network calls upper bounded by the number of tokens predicted. We empirically verify that ASSD speeds up language generation, without sacrificing quality. Furthermore, we provide a mathematically justified scheme for training AS-ARMs for generation, and show that AS-ARMs achieve state-of-the-art performance among sub-200M parameter models on infilling benchmark tasks, and nearly match the performance of models 50X larger on code generation. Our theoretical and empirical results indicate that the once-forgotten AS-ARMs are a promising direction of language modeling.</li>
</ul>

<h3>Title: LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs</h3>
<ul>
<li><strong>Authors: </strong>Woo Yi Yang, Jiarui Wang, Sijing Wu, Huiyu Duan, Yuxin Zhu, Liu Yang, Kang Fu, Guangtao Zhai, Xiongkuo Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20466">https://arxiv.org/abs/2504.20466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20466">https://arxiv.org/pdf/2504.20466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20466]] LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs(https://arxiv.org/abs/2504.20466)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality assessment of AI-generated 3D human faces. We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF, we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating 3DHF capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. Experimental results show that LMME3DHF achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for AI-generated 3D human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be released upon the publication.</li>
</ul>

<h3>Title: Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Linjuan Wu, Haoran Wei, Huan Lin, Tianhao Li, Baosong Yang, Weiming Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20484">https://arxiv.org/abs/2504.20484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20484">https://arxiv.org/pdf/2504.20484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20484]] Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training(https://arxiv.org/abs/2504.20484)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre-training. Existing methods for enhancing cross-lingual transfer remain constrained by parallel resources, suffering from limited linguistic and domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT), a simple and scalable approach that enhances cross-lingual transfer by leveraging semantically related bilingual texts via simple next-word prediction. We construct CrossIC-PT samples by interleaving semantic-related bilingual Wikipedia documents into a single context window. To access window size constraints, we implement a systematic segmentation policy to split long bilingual document pairs into chunks while adjusting the sliding window mechanism to preserve contextual coherence. We further extend data availability through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%, 3.99%, and 1.95%, respectively, with additional improvements after data augmentation.</li>
</ul>

<h3>Title: Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20518">https://arxiv.org/abs/2504.20518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20518">https://arxiv.org/pdf/2504.20518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20518]] Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models(https://arxiv.org/abs/2504.20518)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the $<$EOS$>$ token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens' attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across five representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.49% and an AUC of 87.67%. The code is available at this https URL.</li>
</ul>

<h3>Title: Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Danilo Avola, Federica Bruni, Gian Luca Foresti, Daniele Pannone, Amedeo Ranaldi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20568">https://arxiv.org/abs/2504.20568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20568">https://arxiv.org/pdf/2504.20568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20568]] Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network(https://arxiv.org/abs/2504.20568)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze environments, enabling tasks such as tracking people, detecting intrusions, and recognizing gestures. The rise of this technology is driven by the IEEE 802.11bf standard and growing demand for tools that can ensure privacy and operate through obstacles. However, the performance of Wi-Fi sensing is heavily influenced by environmental conditions, especially when extracting spatial and temporal features from the surrounding scene. A key challenge is achieving robust generalization across domains, ensuring stable performance even when the sensing environment changes significantly. This paper introduces a novel deep learning model for cross-domain adaptation of Wi-Fi signals, inspired by physical signal shielding. The model uses a Relativistic average Generative Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM) architectures for both the generator and discriminator. To simulate physical shielding, an acrylic box lined with electromagnetic shielding fabric was constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from various materials both inside (domain-free) and outside (domain-dependent) the box to train the model. A multi-class Support Vector Machine (SVM) was trained on domain-free spectra and tested on signals denoised by the RaGAN. The system achieved 96% accuracy and demonstrated strong material discrimination capabilities, offering potential for use in security applications to identify concealed objects based on their composition.</li>
</ul>

<h3>Title: AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation</h3>
<ul>
<li><strong>Authors: </strong>Jeongsoo Choi, Ji-Hoon Kim, Kim Sung-Bin, Tae-Hyun Oh, Joon Son Chung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20629">https://arxiv.org/abs/2504.20629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20629">https://arxiv.org/pdf/2504.20629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20629]] AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation(https://arxiv.org/abs/2504.20629)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at this https URL .</li>
</ul>

<h3>Title: Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation</h3>
<ul>
<li><strong>Authors: </strong>Bradley Segal, Joshua Fieggen, David Clifton, Lei Clifton</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20635">https://arxiv.org/abs/2504.20635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20635">https://arxiv.org/pdf/2504.20635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20635]] Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation(https://arxiv.org/abs/2504.20635)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Ensuring the generalisability of clinical machine learning (ML) models across diverse healthcare settings remains a significant challenge due to variability in patient demographics, disease prevalence, and institutional practices. Existing model evaluation approaches often rely on real-world datasets, which are limited in availability, embed confounding biases, and lack the flexibility needed for systematic experimentation. Furthermore, while generative models aim for statistical realism, they often lack transparency and explicit control over factors driving distributional shifts. In this work, we propose a novel structured synthetic data framework designed for the controlled benchmarking of model robustness, fairness, and generalisability. Unlike approaches focused solely on mimicking observed data, our framework provides explicit control over the data generating process, including site-specific prevalence variations, hierarchical subgroup effects, and structured feature interactions. This enables targeted investigation into how models respond to specific distributional shifts and potential biases. Through controlled experiments, we demonstrate the framework's ability to isolate the impact of site variations, support fairness-aware audits, and reveal generalisation failures, particularly highlighting how model complexity interacts with site-specific effects. This work contributes a reproducible, interpretable, and configurable tool designed to advance the reliable deployment of ML in clinical settings.</li>
</ul>

<h3>Title: LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping</h3>
<ul>
<li><strong>Authors: </strong>Weiqin Jiao, Hao Cheng, George Vosselman, Claudio Persello</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20645">https://arxiv.org/abs/2504.20645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20645">https://arxiv.org/pdf/2504.20645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20645]] LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping(https://arxiv.org/abs/2504.20645)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Polygonal road outline extraction from high-resolution aerial images is an important task in large-scale topographic mapping, where roads are represented as vectorized polygons, capturing essential geometric features with minimal vertex redundancy. Despite its importance, no existing method has been explicitly designed for this task. While polygonal building outline extraction has been extensively studied, the unique characteristics of roads, such as branching structures and topological connectivity, pose challenges to these methods. To address this gap, we introduce LDPoly, the first dedicated framework for extracting polygonal road outlines from high-resolution aerial images. Our method leverages a novel Dual-Latent Diffusion Model with a Channel-Embedded Fusion Module, enabling the model to simultaneously generate road masks and vertex heatmaps. A tailored polygonization method is then applied to obtain accurate vectorized road polygons with minimal vertex redundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which contains detailed polygonal annotations for various topographic objects in several Dutch regions. Our experiments include both in-region and cross-region evaluations, with the latter designed to assess the model's generalization performance on unseen regions. Quantitative and qualitative results demonstrate that LDPoly outperforms state-of-the-art polygon extraction methods across various metrics, including pixel-level coverage, vertex efficiency, polygon regularity, and road connectivity. We also design two new metrics to assess polygon simplicity and boundary smoothness. Moreover, this work represents the first application of diffusion models for extracting precise vectorized object outlines without redundant vertices from remote-sensing imagery, paving the way for future advancements in this field.</li>
</ul>

<h3>Title: A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages</h3>
<ul>
<li><strong>Authors: </strong>Ivan Vykopal, Martin Hyben, Robert Moro, Michal Gregor, Jakub Simko</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20668">https://arxiv.org/abs/2504.20668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20668">https://arxiv.org/pdf/2504.20668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20668]] A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages(https://arxiv.org/abs/2504.20668)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Online disinformation poses a global challenge, placing significant demands on fact-checkers who must verify claims efficiently to prevent the spread of false information. A major issue in this process is the redundant verification of already fact-checked claims, which increases workload and delays responses to newly emerging claims. This research introduces an approach that retrieves previously fact-checked claims, evaluates their relevance to a given input, and provides supplementary information to support fact-checkers. Our method employs large language models (LLMs) to filter irrelevant fact-checks and generate concise summaries and explanations, enabling fact-checkers to faster assess whether a claim has been verified before. In addition, we evaluate our approach through both automatic and human assessments, where humans interact with the developed tool to review its effectiveness. Our results demonstrate that LLMs are able to filter out many irrelevant fact-checks and, therefore, reduce effort and streamline the fact-checking process.</li>
</ul>

<h3>Title: Advance Fake Video Detection via Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Joy Battocchio (1), Stefano Dell'Anna (1), Andrea Montibeller (1), Giulia Boato (1,2) ((1) University of Trento, Trento, Italy, (2) Truebees srl, Trento, Italy)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20669">https://arxiv.org/abs/2504.20669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20669">https://arxiv.org/pdf/2504.20669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20669]] Advance Fake Video Detection via Vision Transformers(https://arxiv.org/abs/2504.20669)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in AI-based multimedia generation have enabled the creation of hyper-realistic images and videos, raising concerns about their potential use in spreading misinformation. The widespread accessibility of generative techniques, which allow for the production of fake multimedia from prompts or existing media, along with their continuous refinement, underscores the urgent need for highly accurate and generalizable AI-generated media detection methods, underlined also by new regulations like the European Digital AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based fake image detection and extend this idea to video. We propose an {original} %innovative framework that effectively integrates ViT embeddings over time to enhance detection performance. Our method shows promising accuracy, generalization, and few-shot learning capabilities across a new, large and diverse dataset of videos generated using five open source generative techniques from the state-of-the-art, as well as a separate dataset containing videos produced by proprietary generative methods.</li>
</ul>

<h3>Title: Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zesheng Wang, Alexandre Bruckert, Patrick Le Callet, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20685">https://arxiv.org/abs/2504.20685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20685">https://arxiv.org/pdf/2504.20685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20685]] Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion(https://arxiv.org/abs/2504.20685)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating realistic listener facial motions in dyadic conversations remains challenging due to the high-dimensional action space and temporal dependency requirements. Existing approaches usually consider extracting 3D Morphable Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes the computational speed of the 3DMM a bottleneck, making it difficult to achieve real-time interactive responses. To tackle this problem, we propose Facial Action Diffusion (FAD), which introduces the diffusion methods from the field of image generation to achieve efficient facial action generation. We further build the Efficient Listener Network (ELNet) specially designed to accommodate both the visual and audio information of the speaker as input. Considering of FAD and ELNet, the proposed method learns effective listener facial motion representations and leads to improvements of performance over the state-of-the-art methods while reducing 99% computational time.</li>
</ul>

<h3>Title: What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jan Kapar, Niklas Koenen, Martin Jullum</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20687">https://arxiv.org/abs/2504.20687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20687">https://arxiv.org/pdf/2504.20687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20687]] What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models(https://arxiv.org/abs/2504.20687)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluating synthetic tabular data is challenging, since they can differ from the real data in so many ways. There exist numerous metrics of synthetic data quality, ranging from statistical distances to predictive performance, often providing conflicting results. Moreover, they fail to explain or pinpoint the specific weaknesses in the synthetic data. To address this, we apply explainable AI (XAI) techniques to a binary detection classifier trained to distinguish real from synthetic data. While the classifier identifies distributional differences, XAI concepts such as feature importance and feature effects, analyzed through methods like permutation feature importance, partial dependence plots, Shapley values and counterfactual explanations, reveal why synthetic data are distinguishable, highlighting inconsistencies, unrealistic dependencies, or missing patterns. This interpretability increases transparency in synthetic data evaluation and provides deeper insights beyond conventional metrics, helping diagnose and improve synthetic data quality. We apply our approach to two tabular datasets and generative models, showing that it uncovers issues overlooked by standard evaluation techniques.</li>
</ul>

<h3>Title: DICOM Compatible, 3D Multimodality Image Encryption using Hyperchaotic Signal</h3>
<ul>
<li><strong>Authors: </strong>Anandik N Anand, Sishu Shankar Muni, Abhishek Kaushik</a></li>
<li><strong>Subjects: </strong>cs.CR, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20689">https://arxiv.org/abs/2504.20689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20689">https://arxiv.org/pdf/2504.20689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20689]] DICOM Compatible, 3D Multimodality Image Encryption using Hyperchaotic Signal(https://arxiv.org/abs/2504.20689)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Medical image encryption plays an important role in protecting sensitive health information from cyberattacks and unauthorized access. In this paper, we introduce a secure and robust encryption scheme that is multi-modality compatible and works with MRI, CT, X-Ray and Ultrasound images for different anatomical region of interest. The method utilizes hyperchaotic signals and multi-level diffusion methods. The encryption starts by taking DICOM image as input, then padding to increase the image area. Chaotic signals are produced by a logistic map and are used to carry out pixel random permutation. Then, multi-level diffusion is carried out by 4-bit, 8-bit, radial and adjacent diffusion to provide high randomness and immunity against statistical attacks. In addition, we propose a captcha-based authentication scheme to further improve security. An algorithm generates alphanumeric captcha-based image which is encrypted with the same chaotic and diffusion methods as the medical image. Both encrypted images(DICOM image and captcha image) are then superimposed to create a final encrypted output, essentially integrating dual-layer security. Upon decryption, the superimposed image is again decomposed back to original medical and captcha images, and inverse operations are performed to obtain the original unencrypted data. Experimental results show that the proposed method provides strong protection with no loss in image integrity, thereby reducing unauthorized data breaches to a significant level. The dual-encryption approach not only protects the confidentiality of the medical images but also enhances authentication by incorporating captcha.</li>
</ul>

<h3>Title: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20690">https://arxiv.org/abs/2504.20690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20690">https://arxiv.org/pdf/2504.20690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20690]] In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer(https://arxiv.org/abs/2504.20690)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in this https URL.</li>
</ul>

<h3>Title: Unsupervised Surrogate Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Simon Klüttermann, Tim Katzke, Emmanuel Müller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20733">https://arxiv.org/abs/2504.20733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20733">https://arxiv.org/pdf/2504.20733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20733]] Unsupervised Surrogate Anomaly Detection(https://arxiv.org/abs/2504.20733)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In this paper, we study unsupervised anomaly detection algorithms that learn a neural network representation, i.e. regular patterns of normal data, which anomalies are deviating from. Inspired by a similar concept in engineering, we refer to our methodology as surrogate anomaly detection. We formalize the concept of surrogate anomaly detection into a set of axioms required for optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121 benchmark datasets, demonstrating its competitive performance against 19 existing methods, as well as the scalability and reliability of our method.</li>
</ul>

<h3>Title: DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs</h3>
<ul>
<li><strong>Authors: </strong>Hao Luan, See-Kiong Ng, Chun Kai Ling</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20754">https://arxiv.org/abs/2504.20754</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20754">https://arxiv.org/pdf/2504.20754</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20754]] DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs(https://arxiv.org/abs/2504.20754)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models form an important class of generative models today, accounting for much of the state of the art in cutting edge AI research. While numerous extensions beyond image and video generation exist, few of such approaches address the issue of explicit constraints in the samples generated. In this paper, we study the problem of generating paths in a layered graph (a variant of a directed acyclic graph) using discrete diffusion models, while guaranteeing that our generated samples are indeed paths. Our approach utilizes a simple yet effective representation for paths which we call the padded adjacency-list matrix (PALM). In addition, we show how to effectively perform classifier guidance, which helps steer the sampled paths to specific preferred edges without any retraining of the diffusion model. Our preliminary results show that empirically, our method outperforms alternatives which do not explicitly account for path constraints.</li>
</ul>

<h3>Title: JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Ji Shi, Chengxun Xie, Zhonghao Li, Xinming Zhang, Miao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20770">https://arxiv.org/abs/2504.20770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20770">https://arxiv.org/pdf/2504.20770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20770]] JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation(https://arxiv.org/abs/2504.20770)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The discovery of new molecules based on the original chemical molecule distributions is of great importance in medicine. The graph transformer, with its advantages of high performance and scalability compared to traditional graph networks, has been widely explored in recent research for applications of graph structures. However, current transformer-based graph decoders struggle to effectively utilize graph information, which limits their capacity to leverage only sequences of nodes rather than the complex topological structures of molecule graphs. This paper focuses on building a graph transformer-based framework for molecular generation, which we call \textbf{JTreeformer} as it transforms graph generation into junction tree generation. It combines GCN parallel with multi-head attention as the encoder. It integrates a directed acyclic GCN into a graph-based Transformer to serve as a decoder, which can iteratively synthesize the entire molecule by leveraging information from the partially constructed molecular structure at each step. In addition, a diffusion model is inserted in the latent space generated by the encoder, to enhance the efficiency and effectiveness of sampling further. The empirical results demonstrate that our novel framework outperforms existing molecule generation methods, thus offering a promising tool to advance drug discovery (this https URL).</li>
</ul>

<h3>Title: Q-Fusion: Diffusing Quantum Circuits</h3>
<ul>
<li><strong>Authors: </strong>Collin Beaudoin, Swaroop Ghosh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20794">https://arxiv.org/abs/2504.20794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20794">https://arxiv.org/pdf/2504.20794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20794]] Q-Fusion: Diffusing Quantum Circuits(https://arxiv.org/abs/2504.20794)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Quantum computing holds great potential for solving socially relevant and computationally complex problems. Furthermore, quantum machine learning (QML) promises to rapidly improve our current machine learning capabilities. However, current noisy intermediate-scale quantum (NISQ) devices are constrained by limitations in the number of qubits and gate counts, which hinder their full capabilities. Furthermore, the design of quantum algorithms remains a laborious task, requiring significant domain expertise and time. Quantum Architecture Search (QAS) aims to streamline this process by automatically generating novel quantum circuits, reducing the need for manual intervention. In this paper, we propose a diffusion-based algorithm leveraging the LayerDAG framework to generate new quantum circuits. This method contrasts with other approaches that utilize large language models (LLMs), reinforcement learning (RL), variational autoencoders (VAE), and similar techniques. Our results demonstrate that the proposed model consistently generates 100% valid quantum circuit outputs.</li>
</ul>

<h3>Title: Universal language model with the intervention of quantum theory</h3>
<ul>
<li><strong>Authors: </strong>D.-F. Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20839">https://arxiv.org/abs/2504.20839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20839">https://arxiv.org/pdf/2504.20839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20839]] Universal language model with the intervention of quantum theory(https://arxiv.org/abs/2504.20839)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper examines language modeling based on the theory of quantum mechanics. It focuses on the introduction of quantum mechanics into the symbol-meaning pairs of language in order to build a representation model of natural language. At the same time, it is realized that word embedding, which is widely used as a basic technique for statistical language modeling, can be explained and improved by the mathematical framework of quantum mechanics. On this basis, this paper continues to try to use quantum statistics and other related theories to study the mathematical representation, natural evolution and statistical properties of natural language. It is also assumed that the source of such quantum properties is the physicality of information. The feasibility of using quantum theory to model natural language is pointed out through the construction of a experimental code. The paper discusses, in terms of applications, the possible help of the theory in constructing generative models that are popular nowadays. A preliminary discussion of future applications of the theory to quantum computers is also presented.</li>
</ul>

<h3>Title: JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry</h3>
<ul>
<li><strong>Authors: </strong>Anum Afzal, Alexandre Mercier, Florian Matthes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20849">https://arxiv.org/abs/2504.20849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20849">https://arxiv.org/pdf/2504.20849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20849]] JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry(https://arxiv.org/abs/2504.20849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Online platforms are increasingly interested in using Data-to-Text technologies to generate content and help their users. Unfortunately, traditional generative methods often fall into repetitive patterns, resulting in monotonous galleries of texts after only a few iterations. In this paper, we investigate LLM-based data-to-text approaches to automatically generate marketing texts that are of sufficient quality and diverse enough for broad adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in conjunction with fine-tuning, few-shot, and zero-shot approaches to set a baseline for diverse marketing texts. We also introduce a metric JaccDiv to evaluate the diversity of a set of texts. This research extends its relevance beyond the music industry, proving beneficial in various fields where repetitive automated content generation is prevalent.</li>
</ul>

<h3>Title: AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Pellegrini, Davide Cozzolino, Serafino Pandolfini, Davide Maltoni, Matteo Ferrara, Luisa Verdoliva, Marco Prati, Marco Ramilli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20865">https://arxiv.org/abs/2504.20865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20865">https://arxiv.org/pdf/2504.20865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20865]] AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection(https://arxiv.org/abs/2504.20865)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators.</li>
</ul>

<h3>Title: Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Dayananda Herurkar, Ahmad Ali, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20900">https://arxiv.org/abs/2504.20900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20900">https://arxiv.org/pdf/2504.20900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20900]] Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking(https://arxiv.org/abs/2504.20900)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have revolutionized multiple domains, yet their application to tabular data remains underexplored. Evaluating generative models for tabular data presents unique challenges due to structural complexity, large-scale variability, and mixed data types, making it difficult to intuitively capture intricate patterns. Existing evaluation metrics offer only partial insights, lacking a comprehensive measure of generative performance. To address this limitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS. Our extensive experimental analysis, conducted on three standard network intrusion detection datasets, compares these metrics with established evaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results demonstrate that FAED effectively captures generative modeling issues overlooked by existing metrics. While FPCAD exhibits promising performance, further refinements are necessary to enhance its reliability. Our proposed framework provides a robust and practical approach for assessing generative models in tabular data applications.</li>
</ul>

<h3>Title: GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems</h3>
<ul>
<li><strong>Authors: </strong>Sarad Venugopalan, Sridhar Adepu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20906">https://arxiv.org/abs/2504.20906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20906">https://arxiv.org/pdf/2504.20906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20906]] GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems(https://arxiv.org/abs/2504.20906)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The continuous monitoring of the interactions between cyber-physical components of any industrial control system (ICS) is required to secure automation of the system controls, and to guarantee plant processes are fail-safe and remain in an acceptably safe state. Safety is achieved by managing actuation (where electric signals are used to trigger physical movement), dependent on corresponding sensor readings; used as ground truth in decision making. Timely detection of anomalies (attacks, faults and unascertained states) in ICSs is crucial for the safe running of a plant, the safety of its personnel, and for the safe provision of any services provided. We propose an anomaly detection method that involves accurate linearization of the non-linear forms arising from sensor-actuator(s) relationships, primarily because solving linear models is easier and well understood. Further, the time complexity of the anomaly detection scenario/problem at hand is lowered using dimensionality reduction of the actuator(s) in relationship with a sensor. We accomplish this by using a well-known water treatment testbed as a use case. Our experiments show millisecond time response to detect anomalies and provide explainability; that are not simultaneously achieved by other state of the art AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we pin-point the sensor(s) and its actuation state for which anomaly was detected.</li>
</ul>

<h3>Title: Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal Transformation</h3>
<ul>
<li><strong>Authors: </strong>Peilin He, Liou Tang, M. Amin Rahimian, James Joshi</a></li>
<li><strong>Subjects: </strong>cs.CR, math.DG, stat.OT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20941">https://arxiv.org/abs/2504.20941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20941">https://arxiv.org/pdf/2504.20941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20941]] Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal Transformation(https://arxiv.org/abs/2504.20941)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Differential Privacy (DP) has been established as a safeguard for private data sharing by adding perturbations to information release. Prior research on DP has extended beyond data in the flat Euclidean space and addressed data on curved manifolds, e.g., diffusion tensor MRI, social networks, or organ shape analysis, by adding perturbations along geodesic distances. However, existing manifold-aware DP methods rely on the assumption that samples are uniformly distributed across the manifold. In reality, data densities vary, leading to a biased noise imbalance across manifold regions, weakening the privacy-utility trade-offs. To address this gap, we propose a novel mechanism: Conformal-DP, utilizing conformal transformations on the Riemannian manifold to equalize local sample density and to redefine geodesic distances accordingly while preserving the intrinsic geometry of the manifold. Our theoretical analysis yields two main results. First, we prove that the conformal factor computed from local kernel-density estimates is explicitly data-density-aware; Second, under the conformal metric, the mechanism satisfies $ \varepsilon $-differential privacy on any complete Riemannian manifold and admits a closed-form upper bound on the expected geodesic error that depends only on the maximal density ratio, not on global curvatureof the manifold. Our experimental results validate that the mechanism achieves high utility while providing the $ \varepsilon $-DP guarantee for both homogeneous and especially heterogeneous manifold data.</li>
</ul>

<h3>Title: SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features</h3>
<ul>
<li><strong>Authors: </strong>Mete Erdogan, Sebnem Demirtas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20970">https://arxiv.org/abs/2504.20970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20970">https://arxiv.org/pdf/2504.20970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20970]] SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features(https://arxiv.org/abs/2504.20970)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate and early diagnosis of pneumonia through X-ray imaging is essential for effective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and efficient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures efficiency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while offering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications.</li>
</ul>

<h3>Title: Toward Efficient Exploration by Large Language Model Agents</h3>
<ul>
<li><strong>Authors: </strong>Dilip Arumugam, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.20997">https://arxiv.org/abs/2504.20997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.20997">https://arxiv.org/pdf/2504.20997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.20997]] Toward Efficient Exploration by Large Language Model Agents(https://arxiv.org/abs/2504.20997)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
