<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-31</h1>
<h3>Title: A Large Encoder-Decoder Family of Foundation Models For Chemical Language</h3>
<ul>
<li><strong>Authors: </strong>Eduardo Soares, Victor Shirasuna, Emilio Vital Brazil, Renato Cerqueira, Dmitry Zubarev, Kristin Schmidt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20267">https://arxiv.org/abs/2407.20267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20267">https://arxiv.org/pdf/2407.20267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20267]] A Large Encoder-Decoder Family of Foundation Models For Chemical Language(https://arxiv.org/abs/2407.20267)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Large-scale pre-training methodologies for chemical language models represent a breakthrough in cheminformatics. These methods excel in tasks such as property prediction and molecule generation by learning contextualized representations of input tokens through self-supervised learning on large unlabeled corpora. Typically, this involves pre-training on unlabeled data followed by fine-tuning on specific tasks, reducing dependence on annotated datasets and broadening chemical language representation understanding. This paper introduces a large encoder-decoder chemical foundation models pre-trained on a curated dataset of 91 million SMILES samples sourced from PubChem, which is equivalent to 4 billion of molecular tokens. The proposed foundation model supports different complex tasks, including quantum property prediction, and offer flexibility with two main variants (289M and $8\times289M$). Our experiments across multiple benchmark datasets validate the capacity of the proposed model in providing state-of-the-art results for different tasks. We also provide a preliminary assessment of the compositionality of the embedding space as a prerequisite for the reasoning tasks. We demonstrate that the produced latent space is separable compared to the state-of-the-art with few-shot learning capabilities.</li>
</ul>

<h3>Title: Utilizing Generative Adversarial Networks for Image Data Augmentation and Classification of Semiconductor Wafer Dicing Induced Defects</h3>
<ul>
<li><strong>Authors: </strong>Zhining Hu, Tobias Schlosser, Michael Friedrich, Andr√© Luiz Vieira e Silva, Frederik Beuth, Danny Kowerko</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20268">https://arxiv.org/abs/2407.20268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20268">https://arxiv.org/pdf/2407.20268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20268]] Utilizing Generative Adversarial Networks for Image Data Augmentation and Classification of Semiconductor Wafer Dicing Induced Defects(https://arxiv.org/abs/2407.20268)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In semiconductor manufacturing, the wafer dicing process is central yet vulnerable to defects that significantly impair yield - the proportion of defect-free chips. Deep neural networks are the current state of the art in (semi-)automated visual inspection. However, they are notoriously known to require a particularly large amount of data for model training. To address these challenges, we explore the application of generative adversarial networks (GAN) for image data augmentation and classification of semiconductor wafer dicing induced defects to enhance the variety and balance of training data for visual inspection systems. With this approach, synthetic yet realistic images are generated that mimic real-world dicing defects. We employ three different GAN variants for high-resolution image synthesis: Deep Convolutional GAN (DCGAN), CycleGAN, and StyleGAN3. Our work-in-progress results demonstrate that improved classification accuracies can be obtained, showing an average improvement of up to 23.1 % from 65.1 % (baseline experiment) to 88.2 % (DCGAN experiment) in balanced accuracy, which may enable yield optimization in production.</li>
</ul>

<h3>Title: Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Tang, Ye Liu, Xukai Liu, Kai Zhang, Yanghai Zhang, Qi Liu, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20271">https://arxiv.org/abs/2407.20271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20271">https://arxiv.org/pdf/2407.20271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20271]] Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models(https://arxiv.org/abs/2407.20271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in machine learning, especially in Natural Language Processing (NLP), have led to the development of sophisticated models trained on vast datasets, but this progress has raised concerns about potential sensitive information leakage. In response, regulatory measures like the EU General Data Protection Regulation (GDPR) have driven the exploration of Machine Unlearning techniques, which aim to enable models to selectively forget certain data entries. While early approaches focused on pre-processing methods, recent research has shifted towards training-based machine unlearning methods. However, many existing methods require access to original training data, posing challenges in scenarios where such data is unavailable. Besides, directly facilitating unlearning may undermine the language model's general expressive ability. To this end, in this paper, we introduce the Iterative Contrastive Unlearning (ICU) framework, which addresses these challenges by incorporating three key components. We propose a Knowledge Unlearning Induction module for unlearning specific target sequences and a Contrastive Learning Enhancement module to prevent degrading in generation capacity. Additionally, an Iterative Unlearning Refinement module is integrated to make the process more adaptive to each target sample respectively. Experimental results demonstrate the efficacy of ICU in maintaining performance while efficiently unlearning sensitive information, offering a promising avenue for privacy-conscious machine learning applications.</li>
</ul>

<h3>Title: From pixels to planning: scale-free active inference</h3>
<ul>
<li><strong>Authors: </strong>Karl Friston, Conor Heins, Tim Verbelen, Lancelot Da Costa, Tommaso Salvatori, Dimitrije Markovic, Alexander Tschantz, Magnus Koudahl, Christopher Buckley, Thomas Parr</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20292">https://arxiv.org/abs/2407.20292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20292">https://arxiv.org/pdf/2407.20292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20292]] From pixels to planning: scale-free active inference(https://arxiv.org/abs/2407.20292)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper describes a discrete state-space model -- and accompanying methods -- for generative modelling. This model generalises partially observed Markov decision processes to include paths as latent variables, rendering it suitable for active inference and learning in a dynamic setting. Specifically, we consider deep or hierarchical forms using the renormalisation group. The ensuing renormalising generative models (RGM) can be regarded as discrete homologues of deep convolutional neural networks or continuous state-space models in generalised coordinates of motion. By construction, these scale-invariant models can be used to learn compositionality over space and time, furnishing models of paths or orbits; i.e., events of increasing temporal depth and itinerancy. This technical note illustrates the automatic discovery, learning and deployment of RGMs using a series of applications. We start with image classification and then consider the compression and generation of movies and music. Finally, we apply the same variational principles to the learning of Atari-like games.</li>
</ul>

<h3>Title: A Bayesian Flow Network Framework for Chemistry Tasks</h3>
<ul>
<li><strong>Authors: </strong>Nianze Tao, Minori Abe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20294">https://arxiv.org/abs/2407.20294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20294">https://arxiv.org/pdf/2407.20294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20294]] A Bayesian Flow Network Framework for Chemistry Tasks(https://arxiv.org/abs/2407.20294)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we introduce ChemBFN, a language model that handles chemistry tasks based on Bayesian flow networks working on discrete data. A new accuracy schedule is proposed to improve the sampling quality by significantly reducing the reconstruction loss. We show evidence that our method is appropriate for generating molecules with satisfied diversity even when a smaller number of sampling steps is used. A classifier-free guidance method is adapted for conditional generation. It is also worthwhile to point out that after generative training, our model can be fine-tuned on regression and classification tasks with the state-of-the-art performance, which opens the gate of building all-in-one models in a single module style. Our model has been open sourced at this https URL.</li>
</ul>

<h3>Title: Sun Off, Lights On: Photorealistic Monocular Nighttime Simulation for Robust Semantic Perception</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Tzevelekakis, Shutong Zhang, Luc Van Gool, Christos Sakaridis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20336">https://arxiv.org/abs/2407.20336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20336">https://arxiv.org/pdf/2407.20336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20336]] Sun Off, Lights On: Photorealistic Monocular Nighttime Simulation for Robust Semantic Perception(https://arxiv.org/abs/2407.20336)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Nighttime scenes are hard to semantically perceive with learned models and annotate for humans. Thus, realistic synthetic nighttime data become all the more important for learning robust semantic perception at night, thanks to their accurate and cheap semantic annotations. However, existing data-driven or hand-crafted techniques for generating nighttime images from daytime counterparts suffer from poor realism. The reason is the complex interaction of highly spatially varying nighttime illumination, which differs drastically from its daytime counterpart, with objects of spatially varying materials in the scene, happening in 3D and being very hard to capture with such 2D approaches. The above 3D interaction and illumination shift have proven equally hard to model in the literature, as opposed to other conditions such as fog or rain. Our method, named Sun Off, Lights On (SOLO), is the first to perform nighttime simulation on single images in a photorealistic fashion by operating in 3D. It first explicitly estimates the 3D geometry, the materials and the locations of light sources of the scene from the input daytime image and relights the scene by probabilistically instantiating light sources in a way that accounts for their semantics and then running standard ray tracing. Not only is the visual quality and photorealism of our nighttime images superior to competing approaches including diffusion models, but the former images are also proven more beneficial for semantic nighttime segmentation in day-to-night adaptation. Code and data will be made publicly available.</li>
</ul>

<h3>Title: Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Baraldi, Federico Cocchi, Marcella Cornia, Lorenzo Baraldi, Alessandro Nicolosi, Rita Cucchiara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20337">https://arxiv.org/abs/2407.20337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20337">https://arxiv.org/pdf/2407.20337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20337]] Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities(https://arxiv.org/abs/2407.20337)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discerning between authentic content and that generated by advanced AI methods has become increasingly challenging. While previous research primarily addresses the detection of fake faces, the identification of generated natural images has only recently surfaced. This prompted the recent exploration of solutions that employ foundation vision-and-language models, like CLIP. However, the CLIP embedding space is optimized for global image-to-text alignment and is not inherently designed for deepfake detection, neglecting the potential benefits of tailored training and local image features. In this study, we propose CoDE (Contrastive Deepfake Embeddings), a novel embedding space specifically designed for deepfake detection. CoDE is trained via contrastive learning by additionally enforcing global-local similarities. To sustain the training of our model, we generate a comprehensive dataset that focuses on images generated by diffusion models and encompasses a collection of 9.2 million images produced by using four different generators. Experimental results demonstrate that CoDE achieves state-of-the-art accuracy on the newly collected dataset, while also showing excellent generalization capabilities to unseen image generators. Our source code, trained models, and collected dataset are publicly available at: this https URL.</li>
</ul>

<h3>Title: Dense Self-Supervised Learning for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Maxime Seince, Loic Le Folgoc, Luiz Augusto Facury de Souza, Elsa Angelini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20395">https://arxiv.org/abs/2407.20395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20395">https://arxiv.org/pdf/2407.20395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20395]] Dense Self-Supervised Learning for Medical Image Segmentation(https://arxiv.org/abs/2407.20395)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Deep learning has revolutionized medical image segmentation, but it relies heavily on high-quality annotations. The time, cost and expertise required to label images at the pixel-level for each new task has slowed down widespread adoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL) approach for few-shot segmentation, that reduces the manual annotation burden by learning powerful pixel-level representations directly from unlabeled images. Pix2Rep is a novel pixel-level loss and pre-training paradigm for contrastive SSL on whole images. It is applied to generic encoder-decoder deep learning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance of the learned image-level representations under intensity and spatial image augmentations, Pix2Rep enforces equivariance of the pixel-level representations. We demonstrate the framework on a task of cardiac MRI segmentation. Results show improved performance compared to existing semi- and self-supervised approaches; and a 5-fold reduction in the annotation burden for equivalent performance versus a fully supervised U-Net baseline. This includes a 30% (resp. 31%) DICE improvement for one-shot segmentation under linear-probing (resp. fine-tuning). Finally, we also integrate the novel Pix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even better segmentation performance.</li>
</ul>

<h3>Title: Through the Looking Glass, and what Horn Clause Programs Found There</h3>
<ul>
<li><strong>Authors: </strong>Paul Tarau</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20413">https://arxiv.org/abs/2407.20413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20413">https://arxiv.org/pdf/2407.20413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20413]] Through the Looking Glass, and what Horn Clause Programs Found There(https://arxiv.org/abs/2407.20413)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dual Horn clauses mirror key properties of Horn clauses. This paper explores the ``other side of the looking glass'' to reveal some expected and unexpected symmetries and their practical uses. We revisit Dual Horn clauses as enablers of a form of constructive negation that supports goal-driven forward reasoning and is valid both intuitionistically and classically. In particular, we explore the ability to falsify a counterfactual hypothesis in the context of a background theory expressed as a Dual Horn clause program. With Dual Horn clause programs, by contrast to negation as failure, the variable bindings in their computed answers provide explanations for the reasons why a statement is successfully falsified. Moreover, in the propositional case, by contrast to negation as failure as implemented with stable models semantics in ASP systems, and similarly to Horn clause programs, Dual Horn clause programs have polynomial complexity. After specifying their execution model with a metainterpreter, we devise a compilation scheme from Dual Horn clause programs to Horn clause programs, ensuring their execution with no performance penalty and we design the embedded SymLP language to support combined Horn clause and Dual Horn clause programs. As a (motivating) application, we cast LLM reasoning chains into propositional Horn and Dual Horn clauses that work together to constructively prove and disprove goals and enhance Generative AI with explainability of reasoning chains.</li>
</ul>

<h3>Title: BaseBoostDepth: Exploiting Larger Baselines For Self-supervised Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Kieran Saunders, Luis J. Manso, George Vogiatzis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20437">https://arxiv.org/abs/2407.20437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20437">https://arxiv.org/pdf/2407.20437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20437]] BaseBoostDepth: Exploiting Larger Baselines For Self-supervised Monocular Depth Estimation(https://arxiv.org/abs/2407.20437)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In the domain of multi-baseline stereo, the conventional understanding is that, in general, increasing baseline separation substantially enhances the accuracy of depth estimation. However, prevailing self-supervised depth estimation architectures primarily use minimal frame separation and a constrained stereo baseline. Larger frame separations can be employed; however, we show this to result in diminished depth quality due to various factors, including significant changes in brightness, and increased areas of occlusion. In response to these challenges, our proposed method, BaseBoostDepth, incorporates a curriculum learning-inspired optimization strategy to effectively leverage larger frame separations. However, we show that our curriculum learning-inspired strategy alone does not suffice, as larger baselines still cause pose estimation drifts. Therefore, we introduce incremental pose estimation to enhance the accuracy of pose estimations, resulting in significant improvements across all depth metrics. Additionally, to improve the robustness of the model, we introduce error-induced reconstructions, which optimize reconstructions with added error to the pose estimations. Ultimately, our final depth network achieves state-of-the-art performance on KITTI and SYNS-patches datasets across image-based, edge-based, and point cloud-based metrics without increasing computational complexity at test time. The project website can be found at this https URL.</li>
</ul>

<h3>Title: Learning Feature-Preserving Portrait Editing from Generated Pairs</h3>
<ul>
<li><strong>Authors: </strong>Bowei Chen, Tiancheng Zhi, Peihao Zhu, Shen Sang, Jing Liu, Linjie Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20455">https://arxiv.org/abs/2407.20455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20455">https://arxiv.org/pdf/2407.20455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20455]] Learning Feature-Preserving Portrait Editing from Generated Pairs(https://arxiv.org/abs/2407.20455)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Portrait editing is challenging for existing techniques due to difficulties in preserving subject features like identity. In this paper, we propose a training-based method leveraging auto-generated paired data to learn desired editing while ensuring the preservation of unchanged subject features. Specifically, we design a data generation process to create reasonably good training pairs for desired editing at low cost. Based on these pairs, we introduce a Multi-Conditioned Diffusion Model to effectively learn the editing direction and preserve subject features. During inference, our model produces accurate editing mask that can guide the inference process to further preserve detailed subject features. Experiments on costume editing and cartoon expression editing show that our method achieves state-of-the-art quality, quantitatively and qualitatively.</li>
</ul>

<h3>Title: Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language</h3>
<ul>
<li><strong>Authors: </strong>Hossein Rajaby Faghihi, Aliakbar Nafar, Andrzej Uszok, Hamid Karimian, Parisa Kordjamshidi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20513">https://arxiv.org/abs/2407.20513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20513">https://arxiv.org/pdf/2407.20513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20513]] Prompt2DeModel: Declarative Neuro-Symbolic Modeling with Natural Language(https://arxiv.org/abs/2407.20513)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This paper presents a conversational pipeline for crafting domain knowledge for complex neuro-symbolic models through natural language prompts. It leverages large language models to generate declarative programs in the DomiKnowS framework. The programs in this framework express concepts and their relationships as a graph in addition to logical constraints between them. The graph, later, can be connected to trainable neural models according to those specifications. Our proposed pipeline utilizes techniques like dynamic in-context demonstration retrieval, model refinement based on feedback from a symbolic parser, visualization, and user interaction to generate the tasks' structure and formal knowledge representation. This approach empowers domain experts, even those not well-versed in ML/AI, to formally declare their knowledge to be incorporated in customized neural models in the DomiKnowS framework.</li>
</ul>

<h3>Title: Machine Unlearning in Generative AI: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20516">https://arxiv.org/abs/2407.20516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20516">https://arxiv.org/pdf/2407.20516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20516]] Machine Unlearning in Generative AI: A Survey(https://arxiv.org/abs/2407.20516)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: this https URL.</li>
</ul>

<h3>Title: DiffusionCounterfactuals: Inferring High-dimensional Counterfactuals with Guidance of Causal Representations</h3>
<ul>
<li><strong>Authors: </strong>Jiageng Zhu, Hanchen Xie, Jiazhi Li, Wael Abd-Almageed</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20553">https://arxiv.org/abs/2407.20553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20553">https://arxiv.org/pdf/2407.20553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20553]] DiffusionCounterfactuals: Inferring High-dimensional Counterfactuals with Guidance of Causal Representations(https://arxiv.org/abs/2407.20553)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate estimation of counterfactual outcomes in high-dimensional data is crucial for decision-making and understanding causal relationships and intervention outcomes in various domains, including healthcare, economics, and social sciences. However, existing methods often struggle to generate accurate and consistent counterfactuals, particularly when the causal relationships are complex. We propose a novel framework that incorporates causal mechanisms and diffusion models to generate high-quality counterfactual samples guided by causal representation. Our approach introduces a novel, theoretically grounded training and sampling process that enables the model to consistently generate accurate counterfactual high-dimensional data under multiple intervention steps. Experimental results on various synthetic and real benchmarks demonstrate the proposed approach outperforms state-of-the-art methods in generating accurate and high-quality counterfactuals, using different evaluation metrics.</li>
</ul>

<h3>Title: CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Tianshi Zheng, Jiaxin Bai, Yicheng Wang, Tianqing Fang, Yue Guo, Yauwai Yim, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20564">https://arxiv.org/abs/2407.20564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20564">https://arxiv.org/pdf/2407.20564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20564]] CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge(https://arxiv.org/abs/2407.20564)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have demonstrated impressive capabilities across various natural language processing tasks by acquiring rich factual knowledge from their broad training data, their ability to synthesize and logically reason with this knowledge in complex ways remains underexplored. In this work, we present a systematic evaluation of state-of-the-art LLMs' complex logical reasoning abilities through a novel benchmark of automatically generated complex reasoning questions over general domain and biomedical knowledge graphs. Our extensive experiments, employing diverse in-context learning techniques, reveal that LLMs excel at reasoning over general world knowledge but face significant challenges with specialized domain-specific knowledge. We find that prompting with explicit Chain-of-Thought demonstrations can substantially improve LLM performance on complex logical reasoning tasks with diverse logical operations. Interestingly, our controlled evaluations uncover an asymmetry where LLMs display proficiency at set union operations, but struggle considerably with set intersections - a key building block of logical reasoning. To foster further work, we will publicly release our evaluation benchmark and code.</li>
</ul>

<h3>Title: EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos</h3>
<ul>
<li><strong>Authors: </strong>Aashish Rai, Srinath Sridhar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20592">https://arxiv.org/abs/2407.20592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20592">https://arxiv.org/pdf/2407.20592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20592]] EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos(https://arxiv.org/abs/2407.20592)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce EgoSonics, a method to generate semantically meaningful and synchronized audio tracks conditioned on silent egocentric videos. Generating audio for silent egocentric videos could open new applications in virtual reality, assistive technologies, or for augmenting existing datasets. Existing work has been limited to domains like speech, music, or impact sounds and cannot easily capture the broad range of audio frequencies found in egocentric videos. EgoSonics addresses these limitations by building on the strength of latent diffusion models for conditioned audio synthesis. We first encode and process audio and video data into a form that is suitable for generation. The encoded data is used to train our model to generate audio tracks that capture the semantics of the input video. Our proposed SyncroNet builds on top of ControlNet to provide control signals that enables temporal synchronization to the synthesized audio. Extensive evaluations show that our model outperforms existing work in audio quality, and in our newly proposed synchronization evaluation method. Furthermore, we demonstrate downstream applications of our model in improving video summarization.</li>
</ul>

<h3>Title: Joint Diffusion Processes as an Inductive Bias in Sheaf Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Ferran Hernandez Caralt, Guillermo Bern√°rdez Gil, Iulia Duta, Pietro Li√≤, Eduard Alarc√≥n Cot</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20597">https://arxiv.org/abs/2407.20597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20597">https://arxiv.org/pdf/2407.20597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20597]] Joint Diffusion Processes as an Inductive Bias in Sheaf Neural Networks(https://arxiv.org/abs/2407.20597)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Sheaf Neural Networks (SNNs) naturally extend Graph Neural Networks (GNNs) by endowing a cellular sheaf over the graph, equipping nodes and edges with vector spaces and defining linear mappings between them. While the attached geometric structure has proven to be useful in analyzing heterophily and oversmoothing, so far the methods by which the sheaf is computed do not always guarantee a good performance in such settings. In this work, drawing inspiration from opinion dynamics concepts, we propose two novel sheaf learning approaches that (i) provide a more intuitive understanding of the involved structure maps, (ii) introduce a useful inductive bias for heterophily and oversmoothing, and (iii) infer the sheaf in a way that does not scale with the number of features, thus using fewer learnable parameters than existing methods. In our evaluation, we show the limitations of the real-world benchmarks used so far on SNNs, and design a new synthetic task -- leveraging the symmetries of n-dimensional ellipsoids -- that enables us to better assess the strengths and weaknesses of sheaf-based models. Our extensive experimentation on these novel datasets reveals valuable insights into the scenarios and contexts where SNNs in general -- and our proposed approaches in particular -- can be beneficial.</li>
</ul>

<h3>Title: FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20653">https://arxiv.org/abs/2407.20653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20653">https://arxiv.org/pdf/2407.20653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20653]] FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks(https://arxiv.org/abs/2407.20653)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to generate adversarial examples that are robust in both cross-domain and cross-model settings. With that goal in mind, we propose two modules that are only employed during the training phase: a Frequency-Aware Domain Randomization (FADR) module to randomize domain-variant low- and high-range frequency components and a Frequency-Augmented Contrastive Learning (FACL) module to effectively separate domain-invariant mid-frequency features of clean and perturbed image. We demonstrate strong transferability of our generated adversarial perturbations through extensive cross-domain and cross-model experiments, while keeping the inference time complexity.</li>
</ul>

<h3>Title: Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Hunmin Yang, Jongoh Jeong, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20657">https://arxiv.org/abs/2407.20657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20657">https://arxiv.org/pdf/2407.20657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20657]] Prompt-Driven Contrastive Learning for Transferable Adversarial Attacks(https://arxiv.org/abs/2407.20657)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Recent vision-language foundation models, such as CLIP, have demonstrated superior capabilities in learning representations that can be transferable across diverse range of downstream tasks and domains. With the emergence of such powerful models, it has become crucial to effectively leverage their capabilities in tackling challenging vision tasks. On the other hand, only a few works have focused on devising adversarial examples that transfer well to both unknown domains and model architectures. In this paper, we propose a novel transfer attack method called PDCL-Attack, which leverages the CLIP model to enhance the transferability of adversarial perturbations generated by a generative model-based attack framework. Specifically, we formulate an effective prompt-driven feature guidance by harnessing the semantic representation power of text, particularly from the ground-truth class labels of input images. To the best of our knowledge, we are the first to introduce prompt learning to enhance the transferable generative attacks. Extensive experiments conducted across various cross-domain and cross-model settings empirically validate our approach, demonstrating its superiority over state-of-the-art methods.</li>
</ul>

<h3>Title: Time Series Anomaly Detection with CNN for Environmental Sensors in Healthcare-IoT</h3>
<ul>
<li><strong>Authors: </strong>Mirza Akhi Khatun, Mangolika Bhattacharya, Ciar√°n Eising, Lubna Luxmi Dhirani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20695">https://arxiv.org/abs/2407.20695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20695">https://arxiv.org/pdf/2407.20695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20695]] Time Series Anomaly Detection with CNN for Environmental Sensors in Healthcare-IoT(https://arxiv.org/abs/2407.20695)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This research develops a new method to detect anomalies in time series data using Convolutional Neural Networks (CNNs) in healthcare-IoT. The proposed method creates a Distributed Denial of Service (DDoS) attack using an IoT network simulator, Cooja, which emulates environmental sensors such as temperature and humidity. CNNs detect anomalies in time series data, resulting in a 92\% accuracy in identifying possible attacks.</li>
</ul>

<h3>Title: SceneTeller: Language-to-3D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Ba≈üak Melis √ñcal, Maxim Tatarchenko, Sezer Karaoglu, Theo Gevers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20727">https://arxiv.org/abs/2407.20727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20727">https://arxiv.org/pdf/2407.20727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20727]] SceneTeller: Language-to-3D Scene Generation(https://arxiv.org/abs/2407.20727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Designing high-quality indoor 3D scenes is important in many practical applications, such as room planning or game development. Conventionally, this has been a time-consuming process which requires both artistic skill and familiarity with professional software, making it hardly accessible for layman users. However, recent advances in generative AI have established solid foundation for democratizing 3D design. In this paper, we propose a pioneering approach for text-based 3D room design. Given a prompt in natural language describing the object placement in the room, our method produces a high-quality 3D scene corresponding to it. With an additional text prompt the users can change the appearance of the entire scene or of individual objects in it. Built using in-context learning, CAD model retrieval and 3D-Gaussian-Splatting-based stylization, our turnkey pipeline produces state-of-the-art 3D scenes, while being easy to use even for novices. Our project page is available at this https URL.</li>
</ul>

<h3>Title: Efficient Quantum One-Class Support Vector Machines for Anomaly Detection Using Randomized Measurements and Variable Subsampling</h3>
<ul>
<li><strong>Authors: </strong>Michael K√∂lle, Afrae Ahouzi, Pascal Debus, Elif √áetiner, Robert M√ºller, Dani√´lle Schuman, Claudia Linnhoff-Popien</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20753">https://arxiv.org/abs/2407.20753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20753">https://arxiv.org/pdf/2407.20753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20753]] Efficient Quantum One-Class Support Vector Machines for Anomaly Detection Using Randomized Measurements and Variable Subsampling(https://arxiv.org/abs/2407.20753)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Quantum one-class support vector machines leverage the advantage of quantum kernel methods for semi-supervised anomaly detection. However, their quadratic time complexity with respect to data size poses challenges when dealing with large datasets. In recent work, quantum randomized measurements kernels and variable subsampling were proposed, as two independent methods to address this problem. The former achieves higher average precision, but suffers from variance, while the latter achieves linear complexity to data size and has lower variance. The current work focuses instead on combining these two methods, along with rotated feature bagging, to achieve linear time complexity both to data size and to number of features. Despite their instability, the resulting models exhibit considerably higher performance and faster training and testing times.</li>
</ul>

<h3>Title: SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Liu, Hao Liang, Wentao Xiong, Qinhan Yu, Conghui He, Bin Cui, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20756">https://arxiv.org/abs/2407.20756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20756">https://arxiv.org/pdf/2407.20756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20756]] SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision Language Models(https://arxiv.org/abs/2407.20756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, with the rise of web images, managing and understanding large-scale image datasets has become increasingly important. Vision Large Language Models (VLLMs) have recently emerged due to their robust vision-understanding capabilities. However, training these models requires vast amounts of data, posing challenges to efficiency, effectiveness, data quality, and privacy. In this paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs. Unlike existing methods that generate captions from images, SynthVLM employs advanced diffusion models and high-quality captions to automatically generate and select high-resolution images from captions, creating precisely aligned image-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA) performance on various vision question answering tasks, maintaining high alignment quality and preserving advanced language abilities. Moreover, SynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in performance while significantly reducing computational overhead. Crucially, our method's reliance on purely generated data ensures the preservation of privacy, achieving SoTA performance with just 100k data points (only 18% of the official dataset size).</li>
</ul>

<h3>Title: Interpretable Pre-Trained Transformers for Heart Time-Series Data</h3>
<ul>
<li><strong>Authors: </strong>Harry J. Davies, James Monsen, Danilo P. Mandic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20775">https://arxiv.org/abs/2407.20775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20775">https://arxiv.org/pdf/2407.20775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20775]] Interpretable Pre-Trained Transformers for Heart Time-Series Data(https://arxiv.org/abs/2407.20775)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Decoder-only transformers are the backbone of the popular generative pre-trained transformer (GPT) series of large language models. In this work, we apply the same framework to periodic heart time-series data to create two pre-trained general purpose cardiac models, namely PPG-PT and ECG-PT. We demonstrate that both such pre-trained models are fully interpretable. This is achieved firstly through aggregate attention maps which show that the model focuses on similar points in previous cardiac cycles in order to make predictions and gradually broadens its attention in deeper layers. Next, tokens with the same value, that occur at different distinct points in the ECG and PPG cycle, form separate clusters in high dimensional space based on their phase as they propagate through the transformer blocks. Finally, we highlight that individual attention heads respond to specific physiologically relevent features, such as the dicrotic notch in PPG and the P-wave in ECG. It is also demonstrated that these pre-trained models can be easily fine-tuned for tasks such as classification of atrial fibrillation. In this specific example, the fine-tuning took 11 minutes of computer time, and achieved a leave-one-subject-out AUCs of 0.99 and 0.93 for ECG and PPG respectively. Importantly, these fine-tuned models are also fully explainable, with attention shifting to regions in the context that are strongly indicative of atrial fibrillation.</li>
</ul>

<h3>Title: Inverse Problems with Diffusion Models: A MAP Estimation Perspective</h3>
<ul>
<li><strong>Authors: </strong>Sai bharath chandra Gutha, Hossein Azizpour, Ricardo Vinuesa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20784">https://arxiv.org/abs/2407.20784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20784">https://arxiv.org/pdf/2407.20784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20784]] Inverse Problems with Diffusion Models: A MAP Estimation Perspective(https://arxiv.org/abs/2407.20784)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inverse problems have many applications in science and engineering. In Computer vision, several image restoration tasks such as inpainting, deblurring, and super-resolution can be formally modeled as inverse problems. Recently, methods have been developed for solving inverse problems that only leverage a pre-trained unconditional diffusion model and do not require additional task-specific training. In such methods, however, the inherent intractability of determining the conditional score function during the reverse diffusion process poses a real challenge, leaving the methods to settle with an approximation instead, which affects their performance in practice. Here, we propose a MAP estimation framework to model the reverse conditional generation process of a continuous time diffusion model as an optimization process of the underlying MAP objective, whose gradient term is tractable. In theory, the proposed framework can be applied to solve general inverse problems using gradient-based optimization methods. However, given the highly non-convex nature of the loss objective, finding a perfect gradient-based optimization algorithm can be quite challenging, nevertheless, our framework offers several potential research directions. We use our proposed formulation and develop empirically effective algorithms for solving noiseless and noisy image inpainting tasks. We validate our proposed algorithms with extensive experiments across diverse mask settings.</li>
</ul>

<h3>Title: Retinex-Diffusion: On Controlling Illumination Conditions in Diffusion Models via Retinex Theory</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Xing, Vincent Tao Hu, Jan Hendrik Metzen, Konrad Groh, Sezer Karaoglu, Theo Gevers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20785">https://arxiv.org/abs/2407.20785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20785">https://arxiv.org/pdf/2407.20785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20785]] Retinex-Diffusion: On Controlling Illumination Conditions in Diffusion Models via Retinex Theory(https://arxiv.org/abs/2407.20785)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to illumination manipulation in diffusion models, addressing the gap in conditional image generation with a focus on lighting conditions. We conceptualize the diffusion model as a black-box image render and strategically decompose its energy function in alignment with the image formation model. Our method effectively separates and controls illumination-related properties during the generative process. It generates images with realistic illumination effects, including cast shadow, soft shadow, and inter-reflections. Remarkably, it achieves this without the necessity for learning intrinsic decomposition, finding directions in latent space, or undergoing additional training with new datasets.</li>
</ul>

<h3>Title: Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Norman Di Palo, Leonard Hasenclever, Jan Humplik, Arunkumar Byravan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20798">https://arxiv.org/abs/2407.20798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20798">https://arxiv.org/pdf/2407.20798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20798]] Diffusion Augmented Agents: A Framework for Efficient Exploration and Transfer Learning(https://arxiv.org/abs/2407.20798)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Diffusion Augmented Agents (DAAG), a novel framework that leverages large language models, vision language models, and diffusion models to improve sample efficiency and transfer learning in reinforcement learning for embodied agents. DAAG hindsight relabels the agent's past experience by using diffusion models to transform videos in a temporally and geometrically consistent way to align with target instructions with a technique we call Hindsight Experience Augmentation. A large language model orchestrates this autonomous process without requiring human supervision, making it well-suited for lifelong learning scenarios. The framework reduces the amount of reward-labeled data needed to 1) finetune a vision language model that acts as a reward detector, and 2) train RL agents on new tasks. We demonstrate the sample efficiency gains of DAAG in simulated robotics environments involving manipulation and navigation. Our results show that DAAG improves learning of reward detectors, transferring past experience, and acquiring new tasks - key abilities for developing efficient lifelong learning agents. Supplementary material and visualizations are available on our website this https URL</li>
</ul>

<h3>Title: Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Yunfeng Diao, Naixin Zhai, Changtao Miao, Xun Yang, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20836">https://arxiv.org/abs/2407.20836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20836">https://arxiv.org/pdf/2407.20836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20836]] Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks(https://arxiv.org/abs/2407.20836)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of these AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. For the task of AIGI detection, we propose a new attack containing two main parts. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous models, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as frequency-based post-train Bayesian attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario.</li>
</ul>

<h3>Title: Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering</h3>
<ul>
<li><strong>Authors: </strong>Yanpeng Zhao, Yiwei Hao, Siyu Gao, Yunbo Wang, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20908">https://arxiv.org/abs/2407.20908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20908">https://arxiv.org/pdf/2407.20908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20908]] Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering(https://arxiv.org/abs/2407.20908)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.</li>
</ul>

<h3>Title: SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hao Tan, Zichang Tan, Jun Li, Jun Wan, Zhen Lei, Stan Z. Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.20920">https://arxiv.org/abs/2407.20920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.20920">https://arxiv.org/pdf/2407.20920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.20920]] SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition(https://arxiv.org/abs/2407.20920)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Multi-label image recognition is a fundamental task in computer vision. Recently, Vision-Language Models (VLMs) have made notable advancements in this area. However, previous methods fail to effectively leverage the rich knowledge in language models and often incorporate label semantics into visual features unidirectionally. To overcome these problems, we propose a Split-and-Synthesize Prompting with Gated Alignments (SSPA) framework to amplify the potential of VLMs. Specifically, we develop an in-context learning approach to associate the inherent knowledge from LLMs. Then we propose a novel Split-and-Synthesize Prompting (SSP) strategy to first model the generic knowledge and downstream label semantics individually and then aggregate them carefully through the quaternion network. Moreover, we present Gated Dual-Modal Alignments (GDMA) to bidirectionally interact visual and linguistic modalities while eliminating redundant cross-modal information, enabling more efficient region-level alignments. Rather than making the final prediction by a sharp manner in previous works, we propose a soft aggregator to jointly consider results from all image regions. With the help of flexible prompting and gated alignments, SSPA is generalizable to specific domains. Extensive experiments on nine datasets from three domains (i.e., natural, pedestrian attributes and remote sensing) demonstrate the state-of-the-art performance of SSPA. Further analyses verify the effectiveness of SSP and the interpretability of GDMA. The code will be made public.</li>
</ul>

<h3>Title: Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection</h3>
<ul>
<li><strong>Authors: </strong>Jinfa Huang, Jinsheng Pan, Zhongwei Wan, Hanjia Lyu, Jiebo Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21004">https://arxiv.org/abs/2407.21004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21004">https://arxiv.org/pdf/2407.21004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21004]] Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection(https://arxiv.org/abs/2407.21004)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advances show that two-stream approaches have achieved outstanding performance in hateful meme detection. However, hateful memes constantly evolve as new memes emerge by fusing progressive cultural ideas, making existing methods obsolete or ineffective. In this work, we explore the potential of Large Multimodal Models (LMMs) for hateful meme detection. To this end, we propose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE) Prompting, by integrating the evolution attribute and in-context information of memes. Specifically, Evolver simulates the evolving and expressing process of memes and reasons through LMMs in a step-by-step manner. First, an evolutionary pair mining module retrieves the top-k most similar memes in the external curated meme set with the input meme. Second, an evolutionary information extractor is designed to summarize the semantic regularities between the paired memes for prompting. Finally, a contextual relevance amplifier enhances the in-context hatefulness information to boost the search for evolutionary processes. Extensive experiments on public FHM, MAMI, and HarM datasets show that CoE prompting can be incorporated into existing LMMs to improve their performance. More encouragingly, it can serve as an interpretive tool to promote the understanding of the evolution of social memes.</li>
</ul>

<h3>Title: CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuexi Du, Brian Chang, Nicha C. Dvornek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21011">https://arxiv.org/abs/2407.21011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21011">https://arxiv.org/pdf/2407.21011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21011]] CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning(https://arxiv.org/abs/2407.21011)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive GPU resources and prolonged training times due to the considerable size of the model and dataset, making them poor for medical applications, in which large datasets are not always common. Meanwhile, the language model prompts are mainly manually derived from labels tied to images, potentially overlooking the richness of information within training samples. We introduce a novel language-image Contrastive Learning method with an Efficient large language model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of the extensive pre-trained language and visual models. Furthermore, we present an efficient strategy for learning context-based prompts that mitigates the gap between informative clinical diagnostic data and simple class labels. Our method demonstrates state-of-the-art performance on multiple chest X-ray and mammography datasets compared with various baselines. The proposed parameter efficient framework can reduce the total trainable model size by 39% and reduce the trainable language model to only 4% compared with the current BERT encoder.</li>
</ul>

<h3>Title: Add-SD: Rational Generation without Manual Reference</h3>
<ul>
<li><strong>Authors: </strong>Lingfeng Yang, Xinyu Zhang, Xiang Li, Jinwen Chen, Kun Yao, Gang Zhang, Errui Ding, Lingqiao Liu, Jingdong Wang, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21016">https://arxiv.org/abs/2407.21016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21016">https://arxiv.org/pdf/2407.21016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21016]] Add-SD: Rational Generation without Manual Reference(https://arxiv.org/abs/2407.21016)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have exhibited remarkable prowess in visual generalization. Building on this success, we introduce an instruction-based object addition pipeline, named Add-SD, which automatically inserts objects into realistic scenes with rational sizes and positions. Different from layout-conditioned methods, Add-SD is solely conditioned on simple text prompts rather than any other human-costly references like bounding boxes. Our work contributes in three aspects: proposing a dataset containing numerous instructed image pairs; fine-tuning a diffusion model for rational generation; and generating synthetic data to boost downstream tasks. The first aspect involves creating a RemovalDataset consisting of original-edited image pairs with textual instructions, where an object has been removed from the original image while maintaining strong pixel consistency in the background. These data pairs are then used for fine-tuning the Stable Diffusion (SD) model. Subsequently, the pretrained Add-SD model allows for the insertion of expected objects into an image with good rationale. Additionally, we generate synthetic instances for downstream task datasets at scale, particularly for tail classes, to alleviate the long-tailed problem. Downstream tasks benefit from the enriched dataset with enhanced diversity and rationale. Experiments on LVIS val demonstrate that Add-SD yields an improvement of 4.3 mAP on rare classes over the baseline. Code and models are available at this https URL.</li>
</ul>

<h3>Title: Matting by Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, Shin'ichi Satoh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.21017">https://arxiv.org/abs/2407.21017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.21017">https://arxiv.org/pdf/2407.21017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.21017]] Matting by Generation(https://arxiv.org/abs/2407.21017)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces an innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge. Our method harnesses the capabilities of latent diffusion models, enriched with extensive pre-trained knowledge, to regularize the matting process. We present novel architectural innovations that empower our model to produce mattes with superior resolution and detail. The proposed method is versatile and can perform both guidance-free and guidance-based image matting, accommodating a variety of additional cues. Our comprehensive evaluation across three benchmark datasets demonstrates the superior performance of our approach, both quantitatively and qualitatively. The results not only reflect our method's robust effectiveness but also highlight its ability to generate visually compelling mattes that approach photorealistic quality. The project page for this paper is available at this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
