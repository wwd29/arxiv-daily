<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization. (arXiv:2308.09889v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09889">http://arxiv.org/abs/2308.09889</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09889]] DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization(http://arxiv.org/abs/2308.09889)</code></li>
<li>Summary: <p>Stable Diffusion (SD) customization approaches enable users to personalize SD
model outputs, greatly enhancing the flexibility and diversity of AI art.
However, they also allow individuals to plagiarize specific styles or subjects
from copyrighted images, which raises significant concerns about potential
copyright infringement. To address this issue, we propose an invisible
data-free universal adversarial watermark (DUAW), aiming to protect a myriad of
copyrighted images from different customization approaches across various
versions of SD models. First, DUAW is designed to disrupt the variational
autoencoder during SD customization. Second, DUAW operates in a data-free
context, where it is trained on synthetic images produced by a Large Language
Model (LLM) and a pretrained SD model. This approach circumvents the necessity
of directly handling copyrighted images, thereby preserving their
confidentiality. Once crafted, DUAW can be imperceptibly integrated into
massive copyrighted images, serving as a protective measure by inducing
significant distortions in the images generated by customized SD models.
Experimental results demonstrate that DUAW can effectively distort the outputs
of fine-tuned SD models, rendering them discernible to both human observers and
a simple classifier.
</p></li>
</ul>

<h3>Title: DiffusionTrack: Diffusion Model For Multi-Object Tracking. (arXiv:2308.09905v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09905">http://arxiv.org/abs/2308.09905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09905]] DiffusionTrack: Diffusion Model For Multi-Object Tracking(http://arxiv.org/abs/2308.09905)</code></li>
<li>Summary: <p>Multi-object tracking (MOT) is a challenging vision task that aims to detect
individual objects within a single frame and associate them across multiple
frames. Recent MOT approaches can be categorized into two-stage
tracking-by-detection (TBD) methods and one-stage joint detection and tracking
(JDT) methods. Despite the success of these approaches, they also suffer from
common problems, such as harmful global or local inconsistency, poor trade-off
between robustness and model complexity, and lack of flexibility in different
scenes within the same video. In this paper we propose a simple but robust
framework that formulates object detection and association jointly as a
consistent denoising diffusion process from paired noise boxes to paired
ground-truth boxes. This novel progressive denoising diffusion strategy
substantially augments the tracker's effectiveness, enabling it to discriminate
between various objects. During the training stage, paired object boxes diffuse
from paired ground-truth boxes to random distribution, and the model learns
detection and tracking simultaneously by reversing this noising process. In
inference, the model refines a set of paired randomly generated boxes to the
detection and tracking results in a flexible one-step or multi-step denoising
diffusion process. Extensive experiments on three widely used MOT benchmarks,
including MOT17, MOT20, and Dancetrack, demonstrate that our approach achieves
competitive performance compared to the current state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Physics-Guided Human Motion Capture with Pose Probability Modeling. (arXiv:2308.09910v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09910">http://arxiv.org/abs/2308.09910</a></li>
<li>Code URL: https://github.com/me-ditto/physics-guided-mocap</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09910]] Physics-Guided Human Motion Capture with Pose Probability Modeling(http://arxiv.org/abs/2308.09910)</code></li>
<li>Summary: <p>Incorporating physics in human motion capture to avoid artifacts like
floating, foot sliding, and ground penetration is a promising direction.
Existing solutions always adopt kinematic results as reference motions, and the
physics is treated as a post-processing module. However, due to the depth
ambiguity, monocular motion capture inevitably suffers from noises, and the
noisy reference often leads to failure for physics-based tracking. To address
the obstacles, our key-idea is to employ physics as denoising guidance in the
reverse diffusion process to reconstruct physically plausible human motion from
a modeled pose probability distribution. Specifically, we first train a latent
gaussian model that encodes the uncertainty of 2D-to-3D lifting to facilitate
reverse diffusion. Then, a physics module is constructed to track the motion
sampled from the distribution. The discrepancies between the tracked motion and
image observation are used to provide explicit guidance for the reverse
diffusion model to refine the motion. With several iterations, the
physics-based tracking and kinematic denoising promote each other to generate a
physically plausible human motion. Experimental results show that our method
outperforms previous physics-based methods in both joint accuracy and success
rate. More information can be found at
\url{https://github.com/Me-Ditto/Physics-Guided-Mocap}.
</p></li>
</ul>

<h3>Title: AltDiffusion: A Multilingual Text-to-Image Diffusion Model. (arXiv:2308.09991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09991">http://arxiv.org/abs/2308.09991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09991]] AltDiffusion: A Multilingual Text-to-Image Diffusion Model(http://arxiv.org/abs/2308.09991)</code></li>
<li>Summary: <p>Large Text-to-Image(T2I) diffusion models have shown a remarkable capability
to produce photorealistic and diverse images based on text inputs. However,
existing works only support limited language input, e.g., English, Chinese, and
Japanese, leaving users beyond these languages underserved and blocking the
global expansion of T2I models. Therefore, this paper presents AltDiffusion, a
novel multilingual T2I diffusion model that supports eighteen different
languages. Specifically, we first train a multilingual text encoder based on
the knowledge distillation. Then we plug it into a pretrained English-only
diffusion model and train the model with a two-stage schema to enhance the
multilingual capability, including concept alignment and quality improvement
stage on a large-scale multilingual dataset. Furthermore, we introduce a new
benchmark, which includes Multilingual-General-18(MG-18) and
Multilingual-Cultural-18(MC-18) datasets, to evaluate the capabilities of T2I
diffusion models for generating high-quality images and capturing
culture-specific concepts in different languages. Experimental results on both
MG-18 and MC-18 demonstrate that AltDiffusion outperforms current
state-of-the-art T2I models, e.g., Stable Diffusion in multilingual
understanding, especially with respect to culture-specific concepts, while
still having comparable capability for generating high-quality images.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Scalable Video Object Segmentation with Simplified Framework. (arXiv:2308.09903v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09903">http://arxiv.org/abs/2308.09903</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09903]] Scalable Video Object Segmentation with Simplified Framework(http://arxiv.org/abs/2308.09903)</code></li>
<li>Summary: <p>The current popular methods for video object segmentation (VOS) implement
feature matching through several hand-crafted modules that separately perform
feature extraction and matching. However, the above hand-crafted designs
empirically cause insufficient target interaction, thus limiting the dynamic
target-aware feature learning in VOS. To tackle these limitations, this paper
presents a scalable Simplified VOS (SimVOS) framework to perform joint feature
extraction and matching by leveraging a single transformer backbone.
Specifically, SimVOS employs a scalable ViT backbone for simultaneous feature
extraction and matching between query and reference features. This design
enables SimVOS to learn better target-ware features for accurate mask
prediction. More importantly, SimVOS could directly apply well-pretrained ViT
backbones (e.g., MAE) for VOS, which bridges the gap between VOS and
large-scale self-supervised pre-training. To achieve a better performance-speed
trade-off, we further explore within-frame attention and propose a new token
refinement module to improve the running speed and save computational cost.
Experimentally, our SimVOS achieves state-of-the-art results on popular video
object segmentation benchmarks, i.e., DAVIS-2017 (88.0% J&amp;F), DAVIS-2016 (92.9%
J&amp;F) and YouTube-VOS 2019 (84.2% J&amp;F), without applying any synthetic video or
BL30K pre-training used in previous VOS approaches.
</p></li>
</ul>

<h3>Title: Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation. (arXiv:2308.09917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09917">http://arxiv.org/abs/2308.09917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09917]] Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation(http://arxiv.org/abs/2308.09917)</code></li>
<li>Summary: <p>Instance segmentation in electron microscopy (EM) volumes poses a significant
challenge due to the complex morphology of instances and insufficient
annotations. Self-supervised learning has recently emerged as a promising
solution, enabling the acquisition of prior knowledge of cellular tissue
structures that are essential for EM instance segmentation. However, existing
pretraining methods often lack the ability to capture complex visual patterns
and relationships between voxels, which results in the acquired prior knowledge
being insufficient for downstream EM analysis tasks. In this paper, we propose
a novel pretraining framework that leverages multiscale visual representations
to capture both voxel-level and feature-level consistency in EM volumes.
Specifically, our framework enforces voxel-level consistency between the
outputs of a Siamese network by a reconstruction function, and incorporates a
cross-attention mechanism for soft feature matching to achieve fine-grained
feature-level consistency. Moreover, we propose a contrastive learning scheme
on the feature pyramid to extract discriminative features across multiple
scales. We extensively pretrain our method on four large-scale EM datasets,
achieving promising performance improvements in representative tasks of neuron
and mitochondria instance segmentation.
</p></li>
</ul>

<h3>Title: Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos. (arXiv:2308.09951v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09951">http://arxiv.org/abs/2308.09951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09951]] Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos(http://arxiv.org/abs/2308.09951)</code></li>
<li>Summary: <p>Self-supervised methods have shown remarkable progress in learning high-level
semantics and low-level temporal correspondence. Building on these results, we
take one step further and explore the possibility of integrating these two
features to enhance object-centric representations. Our preliminary experiments
indicate that query slot attention can extract different semantic components
from the RGB feature map, while random sampling based slot attention can
exploit temporal correspondence cues between frames to assist instance
identification. Motivated by this, we propose a novel semantic-aware masked
slot attention on top of the fused semantic features and correspondence maps.
It comprises two slot attention stages with a set of shared learnable Gaussian
distributions. In the first stage, we use the mean vectors as slot
initialization to decompose potential semantics and generate semantic
segmentation masks through iterative attention. In the second stage, for each
semantics, we randomly sample slots from the corresponding Gaussian
distribution and perform masked feature aggregation within the semantic area to
exploit temporal correspondence patterns for instance identification. We adopt
semantic- and instance-level temporal consistency as self-supervision to
encourage temporally coherent object-centric representations. Our model
effectively identifies multiple object instances with semantic structure,
reaching promising results on unsupervised video object discovery. Furthermore,
we achieve state-of-the-art performance on dense label propagation tasks,
demonstrating the potential for object-centric analysis. The code is released
at https://github.com/shvdiwnkozbw/SMTC.
</p></li>
</ul>

<h3>Title: AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization. (arXiv:2308.10001v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10001">http://arxiv.org/abs/2308.10001</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10001]] AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization(http://arxiv.org/abs/2308.10001)</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRF) have shown promise in generating realistic
novel views from sparse scene images. However, existing NeRF approaches often
encounter challenges due to the lack of explicit 3D supervision and imprecise
camera poses, resulting in suboptimal outcomes. To tackle these issues, we
propose AltNeRF -- a novel framework designed to create resilient NeRF
representations using self-supervised monocular depth estimation (SMDE) from
monocular videos, without relying on known camera poses. SMDE in AltNeRF
masterfully learns depth and pose priors to regulate NeRF training. The depth
prior enriches NeRF's capacity for precise scene geometry depiction, while the
pose prior provides a robust starting point for subsequent pose refinement.
Moreover, we introduce an alternating algorithm that harmoniously melds NeRF
outputs into SMDE through a consistence-driven mechanism, thus enhancing the
integrity of depth priors. This alternation empowers AltNeRF to progressively
refine NeRF representations, yielding the synthesis of realistic novel views.
Additionally, we curate a distinctive dataset comprising indoor videos captured
via mobile devices. Extensive experiments showcase the compelling capabilities
of AltNeRF in generating high-fidelity and robust novel views that closely
resemble reality.
</p></li>
</ul>

<h3>Title: Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation. (arXiv:2308.10016v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10016">http://arxiv.org/abs/2308.10016</a></li>
<li>Code URL: https://github.com/yanghai-1218/pseudoflow</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10016]] Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation(http://arxiv.org/abs/2308.10016)</code></li>
<li>Summary: <p>Most self-supervised 6D object pose estimation methods can only work with
additional depth information or rely on the accurate annotation of 2D
segmentation masks, limiting their application range. In this paper, we propose
a 6D object pose estimation method that can be trained with pure RGB images
without any auxiliary information. We first obtain a rough pose initialization
from networks trained on synthetic images rendered from the target's 3D mesh.
Then, we introduce a refinement strategy leveraging the geometry constraint in
synthetic-to-real image pairs from multiple different views. We formulate this
geometry constraint as pixel-level flow consistency between the training images
with dynamically generated pseudo labels. We evaluate our method on three
challenging datasets and demonstrate that it outperforms state-of-the-art
self-supervised methods significantly, with neither 2D annotations nor
additional depth images.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09835">http://arxiv.org/abs/2308.09835</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09835]] Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis(http://arxiv.org/abs/2308.09835)</code></li>
<li>Summary: <p>Current deep learning-based approaches for the segmentation of microscopy
images heavily rely on large amount of training data with dense annotation,
which is highly costly and laborious in practice. Compared to full annotation
where the complete contour of objects is depicted, point annotations,
specifically object centroids, are much easier to acquire and still provide
crucial information about the objects for subsequent segmentation. In this
paper, we assume access to point annotations only during training and develop a
unified pipeline for microscopy image segmentation using synthetically
generated training data. Our framework includes three stages: (1) it takes
point annotations and samples a pseudo dense segmentation mask constrained with
shape priors; (2) with an image generative model trained in an unpaired manner,
it translates the mask to a realistic microscopy image regularized by object
level consistency; (3) the pseudo masks along with the synthetic images then
constitute a pairwise dataset for training an ad-hoc segmentation model. On the
public MoNuSeg dataset, our synthesis pipeline produces more diverse and
realistic images than baseline models while maintaining high coherence between
input masks and generated images. When using the identical segmentation
backbones, the models trained on our synthetic dataset significantly outperform
those trained with pseudo-labels or baseline-generated images. Moreover, our
framework achieves comparable results to models trained on authentic microscopy
images with dense labels, demonstrating its potential as a reliable and highly
efficient alternative to labor-intensive manual pixel-wise annotations in
microscopy image segmentation. The code is available.
</p></li>
</ul>

<h3>Title: EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning. (arXiv:2308.09915v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09915">http://arxiv.org/abs/2308.09915</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09915]] EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning(http://arxiv.org/abs/2308.09915)</code></li>
<li>Summary: <p>Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be
collected for training a prediction model. Accordingly, generative models
(e.g., generative adversarial network (GAN)) are typically used to synthesize
the visual samples conditioned by the class semantic vectors and achieve
remarkable progress for ZSL. However, existing GAN-based generative ZSL methods
are based on hand-crafted models, which cannot adapt to various
datasets/scenarios and fails to model instability. To alleviate these
challenges, we propose evolutionary generative adversarial network search
(termed EGANS) to automatically design the generative network with good
adaptation and stability, enabling reliable visual feature sample synthesis for
advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a
neural architecture search for both generator and discriminator under a unified
evolutionary adversarial framework. EGANS is learned by two stages: evolution
generator architecture search and evolution discriminator architecture search.
During the evolution generator architecture search, we adopt a many-to-one
adversarial training strategy to evolutionarily search for the optimal
generator. Then the optimal generator is further applied to search for the
optimal discriminator in the evolution discriminator architecture search with a
similar evolution search algorithm. Once the optimal generator and
discriminator are searched, we entail them into various generative ZSL
baselines for ZSL classification. Extensive experiments show that EGANS
consistently improve existing generative ZSL methods on the standard CUB, SUN,
AWA2 and FLO datasets. The significant performance gains indicate that the
evolutionary neural architecture search explores a virgin field in ZSL.
</p></li>
</ul>

<h3>Title: A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data. (arXiv:2308.09722v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09722">http://arxiv.org/abs/2308.09722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09722]] A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data(http://arxiv.org/abs/2308.09722)</code></li>
<li>Summary: <p>Social media cyberbullying has a detrimental effect on human life. As online
social networking grows daily, the amount of hate speech also increases. Such
terrible content can cause depression and actions related to suicide. This
paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection
on social media using synthetic data. We have demonstrated a cutting-edge
method to address data availability difficulties by producing
machine-translated data. However, several languages such as Hindi and Bangla
still lack adequate investigations due to a lack of datasets. We carried out
experimental identification of aggressive comments on Hindi, Bangla, and
English datasets using the proposed model and traditional models, including
Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM),
LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from
Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models.
We employed evaluation metrics such as f1-score, accuracy, precision, and
recall to assess the models performance. Our proposed model outperformed all
the models on all datasets, achieving the highest accuracy of 95%. Our model
achieves state-of-the-art results among all the previous works on the dataset
we used in this paper.
</p></li>
</ul>

<h3>Title: FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs. (arXiv:2308.09723v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09723">http://arxiv.org/abs/2308.09723</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09723]] FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs(http://arxiv.org/abs/2308.09723)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have achieved state-of-the-art performance
across various language tasks but pose challenges for practical deployment due
to their substantial memory requirements. Furthermore, the latest generative
models suffer from high inference costs caused by the memory bandwidth
bottleneck in the auto-regressive decoding process. To address these issues, we
propose an efficient weight-only quantization method that reduces memory
consumption and accelerates inference for LLMs. To ensure minimal quality
degradation, we introduce a simple and effective heuristic approach that
utilizes only the model weights of a pre-trained model. This approach is
applicable to both Mixture-of-Experts (MoE) and dense models without requiring
additional fine-tuning. To demonstrate the effectiveness of our proposed
method, we first analyze the challenges and issues associated with LLM
quantization. Subsequently, we present our heuristic approach, which adaptively
finds the granularity of quantization, effectively addressing these problems.
Furthermore, we implement highly efficient GPU GEMMs that perform on-the-fly
matrix multiplication and dequantization, supporting the multiplication of fp16
or bf16 activations with int8 or int4 weights. We evaluate our approach on
large-scale open source models such as OPT-175B and internal MoE models,
showcasing minimal accuracy loss while achieving up to 3.65 times higher
throughput on the same number of GPUs.
</p></li>
</ul>

<h3>Title: Generative Adversarial Networks Unlearning. (arXiv:2308.09881v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09881">http://arxiv.org/abs/2308.09881</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09881]] Generative Adversarial Networks Unlearning(http://arxiv.org/abs/2308.09881)</code></li>
<li>Summary: <p>As machine learning continues to develop, and data misuse scandals become
more prevalent, individuals are becoming increasingly concerned about their
personal information and are advocating for the right to remove their data.
Machine unlearning has emerged as a solution to erase training data from
trained machine learning models. Despite its success in classifiers, research
on Generative Adversarial Networks (GANs) is limited due to their unique
architecture, including a generator and a discriminator. One challenge pertains
to generator unlearning, as the process could potentially disrupt the
continuity and completeness of the latent space. This disruption might
consequently diminish the model's effectiveness after unlearning. Another
challenge is how to define a criterion that the discriminator should perform
for the unlearning images. In this paper, we introduce a substitution mechanism
and define a fake label to effectively mitigate these challenges. Based on the
substitution mechanism and fake label, we propose a cascaded unlearning
approach for both item and class unlearning within GAN models, in which the
unlearning and learning processes run in a cascaded manner. We conducted a
comprehensive evaluation of the cascaded unlearning technique using the MNIST
and CIFAR-10 datasets. Experimental results demonstrate that this approach
achieves significantly improved item and class unlearning efficiency, reducing
the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets,
respectively, in comparison to retraining from scratch. Notably, although the
model's performance experiences minor degradation after unlearning, this
reduction is negligible when dealing with a minimal number of images (e.g., 64)
and has no adverse effects on downstream tasks such as classification.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation. (arXiv:2308.09965v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09965">http://arxiv.org/abs/2308.09965</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09965]] Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation(http://arxiv.org/abs/2308.09965)</code></li>
<li>Summary: <p>Within the context of autonomous driving, encountering unknown objects
becomes inevitable during deployment in the open world. Therefore, it is
crucial to equip standard semantic segmentation models with anomaly awareness.
Many previous approaches have utilized synthetic out-of-distribution (OoD) data
augmentation to tackle this problem. In this work, we advance the OoD synthesis
process by reducing the domain gap between the OoD data and driving scenes,
effectively mitigating the style difference that might otherwise act as an
obvious shortcut during training. Additionally, we propose a simple fine-tuning
loss that effectively induces a pre-trained semantic segmentation model to
generate a ``none of the given classes" prediction, leveraging per-pixel OoD
scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline
enables the use of pre-trained models for anomaly segmentation while
maintaining the performance on the original task.
</p></li>
</ul>

<h3>Title: Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets. (arXiv:2308.10036v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.10036">http://arxiv.org/abs/2308.10036</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.10036]] Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets(http://arxiv.org/abs/2308.10036)</code></li>
<li>Summary: <p>In South Africa, there is an ever-growing issue of vehicle hijackings. This
leads to travellers constantly being in fear of becoming a victim to such an
incident. This work presents a new semi-supervised approach to using tweets to
identify hijacking incidents by using unsupervised anomaly detection
algorithms. Tweets consisting of the keyword "hijacking" are obtained, stored,
and processed using the term frequency-inverse document frequency (TF-IDF) and
further analyzed by using two anomaly detection algorithms: 1) K-Nearest
Neighbour (KNN); 2) Cluster Based Outlier Factor (CBLOF). The comparative
evaluation showed that the KNN method produced an accuracy of 89%, whereas the
CBLOF produced an accuracy of 90%. The CBLOF method was also able to obtain a
F1-Score of 0.8, whereas the KNN produced a 0.78. Therefore, there is a slight
difference between the two approaches, in favour of CBLOF, which has been
selected as a preferred unsupervised method for the determination of relevant
hijacking tweets. In future, a comparison will be done between supervised
learning methods and the unsupervised methods presented in this work on larger
dataset. Optimisation mechanisms will also be employed in order to increase the
overall performance.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Inductive-bias Learning: Generating Code Models with Large Language Model. (arXiv:2308.09890v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09890">http://arxiv.org/abs/2308.09890</a></li>
<li>Code URL: https://github.com/fuyu-quant/iblm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09890]] Inductive-bias Learning: Generating Code Models with Large Language Model(http://arxiv.org/abs/2308.09890)</code></li>
<li>Summary: <p>Large Language Models(LLMs) have been attracting attention due to a ability
called in-context learning(ICL). ICL, without updating the parameters of a LLM,
it is possible to achieve highly accurate inference based on rules ``in the
context'' by merely inputting a training data into the prompt. Although ICL is
a developing field with many unanswered questions, LLMs themselves serves as a
inference model, seemingly realizing inference without explicitly indicate
``inductive bias''. On the other hand, a code generation is also a highlighted
application of LLMs. The accuracy of code generation has dramatically improved,
enabling even non-engineers to generate code to perform the desired tasks by
crafting appropriate prompts. In this paper, we propose a novel ``learning''
method called an ``Inductive-Bias Learning (IBL)'', which combines the
techniques of ICL and code generation. An idea of IBL is straightforward. Like
ICL, IBL inputs a training data into the prompt and outputs a code with a
necessary structure for inference (we referred to as ``Code Model'') from a
``contextual understanding''. Despite being a seemingly simple approach, IBL
encompasses both a ``property of inference without explicit inductive bias''
inherent in ICL and a ``readability and explainability'' of the code
generation. Surprisingly, generated Code Models have been found to achieve
predictive accuracy comparable to, and in some cases surpassing, ICL and
representative machine learning models. Our IBL code is open source:
https://github.com/fuyu-quant/IBLM
</p></li>
</ul>

<h3>Title: HICL: Hashtag-Driven In-Context Learning for Social Media Natural Language Understanding. (arXiv:2308.09985v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.09985">http://arxiv.org/abs/2308.09985</a></li>
<li>Code URL: https://github.com/albertan017/hicl</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.09985]] HICL: Hashtag-Driven In-Context Learning for Social Media Natural Language Understanding(http://arxiv.org/abs/2308.09985)</code></li>
<li>Summary: <p>Natural language understanding (NLU) is integral to various social media
applications. However, existing NLU models rely heavily on context for semantic
learning, resulting in compromised performance when faced with short and noisy
social media content. To address this issue, we leverage in-context learning
(ICL), wherein language models learn to make inferences by conditioning on a
handful of demonstrations to enrich the context and propose a novel
hashtag-driven in-context learning (HICL) framework. Concretely, we pre-train a
model #Encoder, which employs #hashtags (user-annotated topic labels) to drive
BERT-based pre-training through contrastive learning. Our objective here is to
enable #Encoder to gain the ability to incorporate topic-related semantic
information, which allows it to retrieve topic-related posts to enrich contexts
and enhance social media NLU with noisy contexts. To further integrate the
retrieved context with the source text, we employ a gradient-based method to
identify trigger terms useful in fusing information from both sources. For
empirical studies, we collected 45M tweets to set up an in-context NLU
benchmark, and the experimental results on seven downstream tasks show that
HICL substantially advances the previous state-of-the-art results. Furthermore,
we conducted extensive analyzes and found that: (1) combining source input with
a top-retrieved post from #Encoder is more effective than using semantically
similar posts; (2) trigger words can largely benefit in merging context from
the source and retrieved posts.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
