<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15296">http://arxiv.org/abs/2310.15296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15296]] DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM(http://arxiv.org/abs/2310.15296)</code></li>
<li>Summary: <p>In the burgeoning field of natural language processing, Neural Topic Models
(NTMs) and Large Language Models (LLMs) have emerged as areas of significant
research interest. Despite this, NTMs primarily utilize contextual embeddings
from LLMs, which are not optimal for clustering or capable for topic
generation. Our study addresses this gap by introducing a novel framework named
Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME).
DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable
embeddings that could generate topics that exhibit both superior clusterability
and enhanced semantic coherence compared to existing methods. Additionally, by
exploiting the power of diffusion, our framework also provides the capability
to generate content relevant to the identified topics. This dual functionality
allows users to efficiently produce highly clustered topics and related content
simultaneously. DeTiME's potential extends to generating clustered embeddings
as well. Notably, our proposed framework proves to be efficient to train and
exhibits high adaptability, demonstrating its potential for a wide array of
applications.
</p></li>
</ul>

<h3>Title: ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts. (arXiv:2310.15587v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15587">http://arxiv.org/abs/2310.15587</a></li>
<li>Code URL: https://github.com/dili-lab/scandl</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15587]] ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts(http://arxiv.org/abs/2310.15587)</code></li>
<li>Summary: <p>Eye movements in reading play a crucial role in psycholinguistic research
studying the cognitive mechanisms underlying human language processing. More
recently, the tight coupling between eye movements and cognition has also been
leveraged for language-related machine learning tasks such as the
interpretability, enhancement, and pre-training of language models, as well as
the inference of reader- and text-specific properties. However, scarcity of eye
movement data and its unavailability at application time poses a major
challenge for this line of research. Initially, this problem was tackled by
resorting to cognitive models for synthesizing eye movement data. However, for
the sole purpose of generating human-like scanpaths, purely data-driven
machine-learning-based methods have proven to be more suitable. Following
recent advances in adapting diffusion processes to discrete data, we propose
ScanDL, a novel discrete sequence-to-sequence diffusion model that generates
synthetic scanpaths on texts. By leveraging pre-trained word representations
and jointly embedding both the stimulus text and the fixation sequence, our
model captures multi-modal interactions between the two inputs. We evaluate
ScanDL within- and across-dataset and demonstrate that it significantly
outperforms state-of-the-art scanpath generation methods. Finally, we provide
an extensive psycholinguistic analysis that underlines the model's ability to
exhibit human-like reading behavior. Our implementation is made available at
https://github.com/DiLi-Lab/ScanDL.
</p></li>
</ul>

<h3>Title: A Diffusion Weighted Graph Framework for New Intent Discovery. (arXiv:2310.15836v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15836">http://arxiv.org/abs/2310.15836</a></li>
<li>Code URL: https://github.com/yibai-shi/dwgf</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15836]] A Diffusion Weighted Graph Framework for New Intent Discovery(http://arxiv.org/abs/2310.15836)</code></li>
<li>Summary: <p>New Intent Discovery (NID) aims to recognize both new and known intents from
unlabeled data with the aid of limited labeled data containing only known
intents. Without considering structure relationships between samples, previous
methods generate noisy supervisory signals which cannot strike a balance
between quantity and quality, hindering the formation of new intent clusters
and effective transfer of the pre-training knowledge. To mitigate this
limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to
capture both semantic similarities and structure relationships inherent in
data, enabling more sufficient and reliable supervisory signals. Specifically,
for each sample, we diffuse neighborhood relationships along semantic paths
guided by the nearest neighbors for multiple hops to characterize its local
structure discriminately. Then, we sample its positive keys and weigh them
based on semantic similarities and local structures for contrastive learning.
During inference, we further propose Graph Smoothing Filter (GSF) to explicitly
utilize the structure relationships to filter high-frequency noise embodied in
semantically ambiguous samples on the cluster boundary. Extensive experiments
show that our method outperforms state-of-the-art models on all evaluation
metrics across multiple benchmark datasets. Code and data are available at
https://github.com/yibai-shi/DWGF.
</p></li>
</ul>

<h3>Title: Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15290">http://arxiv.org/abs/2310.15290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15290]] Fast and Reliable Generation of EHR Time Series via Diffusion Models(http://arxiv.org/abs/2310.15290)</code></li>
<li>Summary: <p>Electronic Health Records (EHRs) are rich sources of patient-level data,
including laboratory tests, medications, and diagnoses, offering valuable
resources for medical data analysis. However, concerns about privacy often
restrict access to EHRs, hindering downstream analysis. Researchers have
explored various methods for generating privacy-preserving EHR data. In this
study, we introduce a new method for generating diverse and realistic synthetic
EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We
conducted experiments on six datasets, comparing our proposed method with seven
existing methods. Our results demonstrate that our approach significantly
outperforms all existing methods in terms of data utility while requiring less
training effort. Our approach also enhances downstream medical data analysis by
providing diverse and realistic synthetic EHR data.
</p></li>
</ul>

<h3>Title: On the Inherent Privacy Properties of Discrete Denoising Diffusion Models. (arXiv:2310.15524v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15524">http://arxiv.org/abs/2310.15524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15524]] On the Inherent Privacy Properties of Discrete Denoising Diffusion Models(http://arxiv.org/abs/2310.15524)</code></li>
<li>Summary: <p>Privacy concerns have led to a surge in the creation of synthetic datasets,
with diffusion models emerging as a promising avenue. Although prior studies
have performed empirical evaluations on these models, there has been a gap in
providing a mathematical characterization of their privacy-preserving
capabilities. To address this, we present the pioneering theoretical
exploration of the privacy preservation inherent in discrete diffusion models
(DDMs) for discrete dataset generation. Focusing on per-instance differential
privacy (pDP), our framework elucidates the potential privacy leakage for each
data point in a given training dataset, offering insights into data
preprocessing to reduce privacy risks of the synthetic dataset generation via
DDMs. Our bounds also show that training with $s$-sized data points leads to a
surge in privacy leakage from $(\epsilon,
\mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP to $(\epsilon,
\mathcal{O}(\frac{1}{s\epsilon}))$-pDP during the transition from the pure
noise to the synthetic clean data phase, and a faster decay in diffusion
coefficients amplifies the privacy guarantee. Finally, we empirically verify
our theoretical findings on both synthetic and real-world datasets.
</p></li>
</ul>

<h3>Title: Improving Diffusion Models for ECG Imputation with an Augmented Template Prior. (arXiv:2310.15742v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15742">http://arxiv.org/abs/2310.15742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15742]] Improving Diffusion Models for ECG Imputation with an Augmented Template Prior(http://arxiv.org/abs/2310.15742)</code></li>
<li>Summary: <p>Pulsative signals such as the electrocardiogram (ECG) are extensively
collected as part of routine clinical care. However, noisy and poor-quality
recordings, leading to missing values, are a major issue for signals collected
using mobile health systems, decreasing the signal quality and affecting the
automated downstream tasks. Recent studies have explored imputation of missing
values for ECG with probabilistic time-series models. Nevertheless, in
comparison with the deterministic models, their performance is still limited,
as the variations across subjects and heart-beat relationships are not
explicitly considered in the training objective. In this work, to improve the
ECG imputation and forecasting accuracy with probabilistic models, we present
an template-guided denoising diffusion probabilistic model, PulseDiff, which is
conditioned an informative prior for a range of health conditions.
Specifically, 1) we first extract a subject-level pulsative template from the
observation as an informative prior of missing values, which captures the
personal characteristics; 2) we then add beat-level stochastic shift terms on
the template for prior augmentation, which considers the beat-level variance of
positioning and amplitude; 3) we finally design a confidence score to consider
the health condition of subject, which ensures our prior is provided in a safe
way. Experiments with the PTBXL dataset reveal PulseDiff improves the
performance of two strong DDPMs baseline models, CSDI and SSSD$^{S4}$,
verifying our method guides the generation of DDPMs while managing the
uncertainty. When combining with SSSD$^{S4}$, our PulseDiff method outperforms
the leading deterministic model for short-interval missing data and is
comparable for long-interval data loss.
</p></li>
</ul>

<h3>Title: Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations. (arXiv:2310.15815v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15815">http://arxiv.org/abs/2310.15815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15815]] Good Better Best: Self-Motivated Imitation Learning for noisy Demonstrations(http://arxiv.org/abs/2310.15815)</code></li>
<li>Summary: <p>Imitation Learning (IL) aims to discover a policy by minimizing the
discrepancy between the agent's behavior and expert demonstrations. However, IL
is susceptible to limitations imposed by noisy demonstrations from non-expert
behaviors, presenting a significant challenge due to the lack of supplementary
information to assess their expertise. In this paper, we introduce
Self-Motivated Imitation LEarning (SMILE), a method capable of progressively
filtering out demonstrations collected by policies deemed inferior to the
current policy, eliminating the need for additional information. We utilize the
forward and reverse processes of Diffusion Models to emulate the shift in
demonstration expertise from low to high and vice versa, thereby extracting the
noise information that diffuses expertise. Then, the noise information is
leveraged to predict the diffusion steps between the current policy and
demonstrators, which we theoretically demonstrate its equivalence to their
expertise gap. We further explain in detail how the predicted diffusion steps
are applied to filter out noisy demonstrations in a self-motivated manner and
provide its theoretical grounds. Through empirical evaluations on MuJoCo tasks,
we demonstrate that our method is proficient in learning the expert policy
amidst noisy demonstrations, and effectively filters out demonstrations with
expertise inferior to the current policy.
</p></li>
</ul>

<h3>Title: Discriminator Guidance for Autoregressive Diffusion Models. (arXiv:2310.15817v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15817">http://arxiv.org/abs/2310.15817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15817]] Discriminator Guidance for Autoregressive Diffusion Models(http://arxiv.org/abs/2310.15817)</code></li>
<li>Summary: <p>We introduce discriminator guidance in the setting of Autoregressive
Diffusion Models. The use of a discriminator to guide a diffusion process has
previously been used for continuous diffusion models, and in this work we
derive ways of using a discriminator together with a pretrained generative
model in the discrete case. First, we show that using an optimal discriminator
will correct the pretrained model and enable exact sampling from the underlying
data distribution. Second, to account for the realistic scenario of using a
sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which
iteratively takes the predictions from the discrimiator into account during the
generation process. We test these approaches on the task of generating
molecular graphs and show how the discriminator improves the generative
performance over using only the pretrained model.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised Pre-training. (arXiv:2310.15388v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15388">http://arxiv.org/abs/2310.15388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15388]] Remote Heart Rate Monitoring in Smart Environments from Videos with Self-supervised Pre-training(http://arxiv.org/abs/2310.15388)</code></li>
<li>Summary: <p>Recent advances in deep learning have made it increasingly feasible to
estimate heart rate remotely in smart environments by analyzing videos.
However, a notable limitation of deep learning methods is their heavy reliance
on extensive sets of labeled data for effective training. To address this
issue, self-supervised learning has emerged as a promising avenue. Building on
this, we introduce a solution that utilizes self-supervised contrastive
learning for the estimation of remote photoplethysmography (PPG) and heart rate
monitoring, thereby reducing the dependence on labeled data and enhancing
performance. We propose the use of 3 spatial and 3 temporal augmentations for
training an encoder through a contrastive framework, followed by utilizing the
late-intermediate embeddings of the encoder for remote PPG and heart rate
estimation. Our experiments on two publicly available datasets showcase the
improvement of our proposed approach over several related works as well as
supervised learning baselines, as our results approach the state-of-the-art. We
also perform thorough experiments to showcase the effects of using different
design choices such as the video representation learning method, the
augmentations used in the pre-training stage, and others. We also demonstrate
the robustness of our proposed method over the supervised learning approaches
on reduced amounts of labeled data.
</p></li>
</ul>

<h3>Title: I$^2$MD: 3D Action Representation Learning with Inter- and Intra-modal Mutual Distillation. (arXiv:2310.15568v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15568">http://arxiv.org/abs/2310.15568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15568]] I$^2$MD: 3D Action Representation Learning with Inter- and Intra-modal Mutual Distillation(http://arxiv.org/abs/2310.15568)</code></li>
<li>Summary: <p>Recent progresses on self-supervised 3D human action representation learning
are largely attributed to contrastive learning. However, in conventional
contrastive frameworks, the rich complementarity between different skeleton
modalities remains under-explored. Moreover, optimized with distinguishing
self-augmented samples, models struggle with numerous similar positive
instances in the case of limited action categories. In this work, we tackle the
aforementioned problems by introducing a general Inter- and Intra-modal Mutual
Distillation (I$^2$MD) framework. In I$^2$MD, we first re-formulate the
cross-modal interaction as a Cross-modal Mutual Distillation (CMD) process.
Different from existing distillation solutions that transfer the knowledge of a
pre-trained and fixed teacher to the student, in CMD, the knowledge is
continuously updated and bidirectionally distilled between modalities during
pre-training. To alleviate the interference of similar samples and exploit
their underlying contexts, we further design the Intra-modal Mutual
Distillation (IMD) strategy, In IMD, the Dynamic Neighbors Aggregation (DNA)
mechanism is first introduced, where an additional cluster-level discrimination
branch is instantiated in each modality. It adaptively aggregates
highly-correlated neighboring features, forming local cluster-level
contrasting. Mutual distillation is then performed between the two branches for
cross-level knowledge exchange. Extensive experiments on three datasets show
that our approach sets a series of new records.
</p></li>
</ul>

<h3>Title: Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning. (arXiv:2310.15523v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15523">http://arxiv.org/abs/2310.15523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15523]] Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning(http://arxiv.org/abs/2310.15523)</code></li>
<li>Summary: <p>For graph self-supervised learning (GSSL), masked autoencoder (MAE) follows
the generative paradigm and learns to reconstruct masked graph edges or node
features. Contrastive Learning (CL) maximizes the similarity between augmented
views of the same graph and is widely used for GSSL. However, MAE and CL are
considered separately in existing works for GSSL. We observe that the MAE and
CL paradigms are complementary and propose the graph contrastive masked
autoencoder (GCMAE) framework to unify them. Specifically, by focusing on local
edges or node features, MAE cannot capture global information of the graph and
is sensitive to particular edges and features. On the contrary, CL excels in
extracting global information because it considers the relation between graphs.
As such, we equip GCMAE with an MAE branch and a CL branch, and the two
branches share a common encoder, which allows the MAE branch to exploit the
global information extracted by the CL branch. To force GCMAE to capture global
graph structures, we train it to reconstruct the entire adjacency matrix
instead of only the masked edges as in existing works. Moreover, a
discrimination loss is proposed for feature reconstruction, which improves the
disparity between node embeddings rather than reducing the reconstruction error
to tackle the feature smoothing problem of MAE. We evaluate GCMAE on four
popular graph tasks (i.e., node classification, node clustering, link
prediction, and graph classification) and compare with 14 state-of-the-art
baselines. The results show that GCMAE consistently provides good accuracy
across these tasks, and the maximum accuracy improvement is up to 3.2% compared
with the best-performing baseline.
</p></li>
</ul>

<h3>Title: Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning. (arXiv:2310.15586v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15586">http://arxiv.org/abs/2310.15586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15586]] Detecting Intentional AIS Shutdown in Open Sea Maritime Surveillance Using Self-Supervised Deep Learning(http://arxiv.org/abs/2310.15586)</code></li>
<li>Summary: <p>In maritime traffic surveillance, detecting illegal activities, such as
illegal fishing or transshipment of illicit products is a crucial task of the
coastal administration. In the open sea, one has to rely on Automatic
Identification System (AIS) message transmitted by on-board transponders, which
are captured by surveillance satellites. However, insincere vessels often
intentionally shut down their AIS transponders to hide illegal activities. In
the open sea, it is very challenging to differentiate intentional AIS shutdowns
from missing reception due to protocol limitations, bad weather conditions or
restricting satellite positions. This paper presents a novel approach for the
detection of abnormal AIS missing reception based on self-supervised deep
learning techniques and transformer models. Using historical data, the trained
model predicts if a message should be received in the upcoming minute or not.
Afterwards, the model reports on detected anomalies by comparing the prediction
with what actually happens. Our method can process AIS messages in real-time,
in particular, more than 500 Millions AIS messages per month, corresponding to
the trajectories of more than 60 000 ships. The method is evaluated on 1-year
of real-world data coming from four Norwegian surveillance satellites. Using
related research results, we validated our method by rediscovering already
detected intentional AIS shutdowns.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15308">http://arxiv.org/abs/2310.15308</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15308]] SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding(http://arxiv.org/abs/2310.15308)</code></li>
<li>Summary: <p>The landscape of publicly available vision foundation models (VFMs), such as
CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed
with distinct capabilities stemming from their pre-training objectives. For
instance, CLIP excels in semantic understanding, while SAM specializes in
spatial understanding for segmentation. In this work, we introduce a simple
recipe to efficiently merge VFMs into a unified model that assimilates their
expertise. Our proposed method integrates multi-task learning, continual
learning techniques, and teacher-student distillation. This strategy entails
significantly less computational cost compared to traditional multi-task
training from scratch. Additionally, it only demands a small fraction of the
pre-training datasets that were initially used to train individual models. By
applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that
amalgamates the strengths of SAM and CLIP into a single backbone, making it apt
for edge device applications. We show that SAM-CLIP learns richer visual
representations, equipped with both localization and semantic features,
suitable for a broad range of vision tasks. SAM-CLIP obtains improved
performance on several head probing tasks when compared with SAM and CLIP. We
further show that SAM-CLIP not only retains the foundational strengths of its
precursor models but also introduces synergistic functionalities, most notably
in zero-shot semantic segmentation, where SAM-CLIP establishes new
state-of-the-art results on 5 benchmarks. It outperforms previous models that
are specifically designed for this task by a large margin, including +6.8% and
+5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Videoprompter: an ensemble of foundational models for zero-shot video understanding. (arXiv:2310.15324v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15324">http://arxiv.org/abs/2310.15324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15324]] Videoprompter: an ensemble of foundational models for zero-shot video understanding(http://arxiv.org/abs/2310.15324)</code></li>
<li>Summary: <p>Vision-language models (VLMs) classify the query video by calculating a
similarity score between the visual features and text-based class label
representations. Recently, large language models (LLMs) have been used to
enrich the text-based class labels by enhancing the descriptiveness of the
class names. However, these improvements are restricted to the text-based
classifier only, and the query visual features are not considered. In this
paper, we propose a framework which combines pre-trained discriminative VLMs
with pre-trained generative video-to-text and text-to-text models. We introduce
two key modifications to the standard zero-shot setting. First, we propose
language-guided visual feature enhancement and employ a video-to-text model to
convert the query video to its descriptive form. The resulting descriptions
contain vital visual cues of the query video, such as what objects are present
and their spatio-temporal interactions. These descriptive cues provide
additional semantic knowledge to VLMs to enhance their zeroshot performance.
Second, we propose video-specific prompts to LLMs to generate more meaningful
descriptions to enrich class label representations. Specifically, we introduce
prompt techniques to create a Tree Hierarchy of Categories for class names,
offering a higher-level action context for additional visual cues, We
demonstrate the effectiveness of our approach in video understanding across
three different zero-shot settings: 1) video action recognition, 2)
video-to-text and textto-video retrieval, and 3) time-sensitive video tasks.
Consistent improvements across multiple benchmarks and with various VLMs
demonstrate the effectiveness of our proposed framework. Our code will be made
publicly available.
</p></li>
</ul>

<h3>Title: Nighttime Thermal Infrared Image Colorization with Feedback-based Object Appearance Learning. (arXiv:2310.15688v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15688">http://arxiv.org/abs/2310.15688</a></li>
<li>Code URL: https://github.com/fuyaluo/foalgan</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15688]] Nighttime Thermal Infrared Image Colorization with Feedback-based Object Appearance Learning(http://arxiv.org/abs/2310.15688)</code></li>
<li>Summary: <p>Stable imaging in adverse environments (e.g., total darkness) makes thermal
infrared (TIR) cameras a prevalent option for night scene perception. However,
the low contrast and lack of chromaticity of TIR images are detrimental to
human interpretation and subsequent deployment of RGB-based vision algorithms.
Therefore, it makes sense to colorize the nighttime TIR images by translating
them into the corresponding daytime color images (NTIR2DC). Despite the
impressive progress made in the NTIR2DC task, how to improve the translation
performance of small object classes is under-explored. To address this problem,
we propose a generative adversarial network incorporating feedback-based object
appearance learning (FoalGAN). Specifically, an occlusion-aware mixup module
and corresponding appearance consistency loss are proposed to reduce the
context dependence of object translation. As a representative example of small
objects in nighttime street scenes, we illustrate how to enhance the realism of
traffic light by designing a traffic light appearance loss. To further improve
the appearance learning of small objects, we devise a dual feedback learning
strategy to selectively adjust the learning frequency of different samples. In
addition, we provide pixel-level annotation for a subset of the Brno dataset,
which can facilitate the research of NTIR image understanding under multiple
weather conditions. Extensive experiments illustrate that the proposed FoalGAN
is not only effective for appearance learning of small objects, but also
outperforms other image translation methods in terms of semantic preservation
and edge consistency for the NTIR2DC task.
</p></li>
</ul>

<h3>Title: Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation. (arXiv:2310.15515v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15515">http://arxiv.org/abs/2310.15515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15515]] Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation(http://arxiv.org/abs/2310.15515)</code></li>
<li>Summary: <p>Recent ubiquity and disruptive impacts of large language models (LLMs) have
raised concerns about their potential to be misused (.i.e, generating
large-scale harmful and misleading content). To combat this emerging risk of
LLMs, we propose a novel "Fighting Fire with Fire" (F3) strategy that harnesses
modern LLMs' generative and emergent reasoning capabilities to counter
human-written and LLM-generated disinformation. First, we leverage
GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content
through paraphrase-based and perturbation-based prefix-style prompts,
respectively. Second, we apply zero-shot in-context semantic reasoning
techniques with cloze-style prompts to discern genuine from deceptive posts and
news articles. In our extensive experiments, we observe GPT-3.5-turbo's
zero-shot superiority for both in-distribution and out-of-distribution
datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike
the decline observed in previous customized and fine-tuned disinformation
detectors. Our codebase and dataset are available at
https://github.com/mickeymst/F3.
</p></li>
</ul>

<h3>Title: DALE: Generative Data Augmentation for Low-Resource Legal NLP. (arXiv:2310.15799v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15799">http://arxiv.org/abs/2310.15799</a></li>
<li>Code URL: https://github.com/sreyan88/dale</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15799]] DALE: Generative Data Augmentation for Low-Resource Legal NLP(http://arxiv.org/abs/2310.15799)</code></li>
<li>Summary: <p>We present DALE, a novel and effective generative Data Augmentation framework
for low-resource LEgal NLP. DALE addresses the challenges existing frameworks
pose in generating effective data augmentations of legal documents - legal
language, with its specialized vocabulary and complex semantics, morphology,
and syntax, does not benefit from data augmentations that merely rephrase the
source sentence. To address this, DALE, built on an Encoder-Decoder Language
Model, is pre-trained on a novel unsupervised text denoising objective based on
selective masking - our masking strategy exploits the domain-specific language
characteristics of templatized legal documents to mask collocated spans of
text. Denoising these spans helps DALE acquire knowledge about legal concepts,
principles, and language usage. Consequently, it develops the ability to
generate coherent and diverse augmentations with novel contexts. Finally, DALE
performs conditional generation to generate synthetic augmentations for
low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13
datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our
baselines, including LLMs, qualitatively and quantitatively, with improvements
of 1%-50%.
</p></li>
</ul>

<h3>Title: Generative Language Models Exhibit Social Identity Biases. (arXiv:2310.15819v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15819">http://arxiv.org/abs/2310.15819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15819]] Generative Language Models Exhibit Social Identity Biases(http://arxiv.org/abs/2310.15819)</code></li>
<li>Summary: <p>The surge in popularity of large language models has given rise to concerns
about biases that these models could learn from humans. In this study, we
investigate whether ingroup solidarity and outgroup hostility, fundamental
social biases known from social science, are present in 51 large language
models. We find that almost all foundational language models and some
instruction fine-tuned models exhibit clear ingroup-positive and
outgroup-negative biases when prompted to complete sentences (e.g., "We
are..."). A comparison of LLM-generated sentences with human-written sentences
on the internet reveals that these models exhibit similar level, if not
greater, levels of bias than human text. To investigate where these biases stem
from, we experimentally varied the amount of ingroup-positive or
outgroup-negative sentences the model was exposed to during fine-tuning in the
context of the United States Democrat-Republican divide. Doing so resulted in
the models exhibiting a marked increase in ingroup solidarity and an even
greater increase in outgroup hostility. Furthermore, removing either
ingroup-positive or outgroup-negative sentences (or both) from the fine-tuning
data leads to a significant reduction in both ingroup solidarity and outgroup
hostility, suggesting that biases can be reduced by removing biased training
data. Our findings suggest that modern language models exhibit fundamental
social identity biases and that such biases can be mitigated by curating
training data. Our results have practical implications for creating less biased
large-language models and further underscore the need for more research into
user interactions with LLMs to prevent potential bias reinforcement in humans.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction. (arXiv:2310.15416v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15416">http://arxiv.org/abs/2310.15416</a></li>
<li>Code URL: https://github.com/andrewlai61616/npsr</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15416]] Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction(http://arxiv.org/abs/2310.15416)</code></li>
<li>Summary: <p>Time series anomaly detection is challenging due to the complexity and
variety of patterns that can occur. One major difficulty arises from modeling
time-dependent relationships to find contextual anomalies while maintaining
detection accuracy for point anomalies. In this paper, we propose a framework
for unsupervised time series anomaly detection that utilizes point-based and
sequence-based reconstruction models. The point-based model attempts to
quantify point anomalies, and the sequence-based model attempts to quantify
both point and contextual anomalies. Under the formulation that the observed
time point is a two-stage deviated value from a nominal time point, we
introduce a nominality score calculated from the ratio of a combined value of
the reconstruction errors. We derive an induced anomaly score by further
integrating the nominality score and anomaly score, then theoretically prove
the superiority of the induced anomaly score over the original anomaly score
under certain conditions. Extensive studies conducted on several public
datasets show that the proposed framework outperforms most state-of-the-art
baselines for time series anomaly detection.
</p></li>
</ul>

<h3>Title: One or Two Things We know about Concept Drift -- A Survey on Monitoring Evolving Environments. (arXiv:2310.15826v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15826">http://arxiv.org/abs/2310.15826</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15826]] One or Two Things We know about Concept Drift -- A Survey on Monitoring Evolving Environments(http://arxiv.org/abs/2310.15826)</code></li>
<li>Summary: <p>The world surrounding us is subject to constant change. These changes,
frequently described as concept drift, influence many industrial and technical
processes. As they can lead to malfunctions and other anomalous behavior, which
may be safety-critical in many scenarios, detecting and analyzing concept drift
is crucial. In this paper, we provide a literature review focusing on concept
drift in unsupervised data streams. While many surveys focus on supervised data
streams, so far, there is no work reviewing the unsupervised setting. However,
this setting is of particular relevance for monitoring and anomaly detection
which are directly applicable to many tasks and challenges in engineering. This
survey provides a taxonomy of existing work on drift detection. Besides, it
covers the current state of research on drift localization in a systematic way.
In addition to providing a systematic literature review, this work provides
precise mathematical definitions of the considered problems and contains
standardized experiments on parametric artificial datasets allowing for a
direct comparison of different strategies for detection and localization.
Thereby, the suitability of different schemes can be analyzed systematically
and guidelines for their usage in real-world scenarios can be provided.
Finally, there is a section on the emerging topic of explaining concept drift.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15213">http://arxiv.org/abs/2310.15213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15213]] Function Vectors in Large Language Models(http://arxiv.org/abs/2310.15213)</code></li>
<li>Summary: <p>We report the presence of a simple neural mechanism that represents an
input-output function as a vector within autoregressive transformer language
models (LMs). Using causal mediation analysis on a diverse range of
in-context-learning (ICL) tasks, we find that a small number attention heads
transport a compact representation of the demonstrated task, which we call a
function vector (FV). FVs are robust to changes in context, i.e., they trigger
execution of the task on inputs such as zero-shot and natural text settings
that do not resemble the ICL contexts from which they are collected. We test
FVs across a range of tasks, models, and layers and find strong causal effects
across settings in middle layers. We investigate the internal structure of FVs
and find while that they often contain information that encodes the output
space of the function, this information alone is not sufficient to reconstruct
an FV. Finally, we test semantic vector composition in FVs, and find that to
some extent they can be summed to create vectors that trigger new complex
tasks. Taken together, our findings suggest that LLMs contain internal
abstractions of general-purpose functions that can be invoked in a variety of
contexts.
</p></li>
</ul>

<h3>Title: TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction. (arXiv:2310.15556v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15556">http://arxiv.org/abs/2310.15556</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15556]] TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction(http://arxiv.org/abs/2310.15556)</code></li>
<li>Summary: <p>Since ChatGPT released its API for public use, the number of applications
built on top of commercial large language models (LLMs) increase exponentially.
One popular usage of such models is leveraging its in-context learning ability
and generating responses given user queries leveraging knowledge obtained by
retrieval augmentation. One problem of deploying commercial retrieval-augmented
LLMs is the cost due to the additionally retrieved context that largely
increases the input token size of the LLMs. To mitigate this, we propose a
token compression scheme that includes two methods: summarization compression
and semantic compression. The first method applies a T5-based model that is
fine-tuned by datasets generated using self-instruct containing samples with
varying lengths and reduce token size by doing summarization. The second method
further compresses the token size by removing words with lower impact on the
semantic. In order to adequately evaluate the effectiveness of the proposed
methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB)
focusing on food recommendation for women around pregnancy period or infants.
Our summarization compression can reduce 65% of the retrieval token size with
further 0.3% improvement on the accuracy; semantic compression provides a more
flexible way to trade-off the token size with performance, for which we can
reduce the token size by 20% with only 1.6% of accuracy drop.
</p></li>
</ul>

<h3>Title: POE: Process of Elimination for Multiple Choice Reasoning. (arXiv:2310.15575v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.15575">http://arxiv.org/abs/2310.15575</a></li>
<li>Code URL: https://github.com/kasmasvan/poe</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.15575]] POE: Process of Elimination for Multiple Choice Reasoning(http://arxiv.org/abs/2310.15575)</code></li>
<li>Summary: <p>Language models (LMs) are capable of conducting in-context learning for
multiple choice reasoning tasks, but the options in these tasks are treated
equally. As humans often first eliminate wrong options before picking the final
correct answer, we argue a similar two-step strategy can make LMs better at
these tasks. To this end, we present the Process of Elimination (POE), a
two-step scoring method. In the first step, POE scores each option, and
eliminates seemingly wrong options. In the second step, POE masks these wrong
options, and makes the final prediction from the remaining options. Zero-shot
experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a
following analysis finds our method to be especially performant on logical
reasoning tasks. We further analyze the effect of masks, and show that POE
applies to few-shot settings and large language models (LLMs) like ChatGPT.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
