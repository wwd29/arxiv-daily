<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-08</h1>
<h3>Title: Collaborative and Proactive Management of Task-Oriented Conversations</h3>
<ul>
<li><strong>Authors: </strong>Arezoo Saedi, Afsaneh Fatemi, Mohammad Ali Nematbakhsh, Sophie Rosset, Anne Vilnat</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05110">https://arxiv.org/abs/2510.05110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05110">https://arxiv.org/pdf/2510.05110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05110]] Collaborative and Proactive Management of Task-Oriented Conversations(https://arxiv.org/abs/2510.05110)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Task oriented dialogue systems (TOD) complete particular tasks based on user preferences across natural language interactions. Considering the impressive performance of large language models (LLMs) in natural language processing (NLP) tasks, most of the latest TODs are centered on LLMs. While proactive planning is crucial for task completion, many existing TODs overlook effective goal-aware planning. This paper creates a model for managing task-oriented conversations, conceptualized centered on the information state approach to dialogue management. The created model incorporated constructive intermediate information in planning. Initially, predefined slots and text part informational components are created to model user preferences. Investigating intermediate information, critical circumstances are identified. Informational components corresponding to these circumstances are created. Possible configurations for these informational components lead to limited information states. Then, dialogue moves, which indicate movement between these information states and the procedures that must be performed in the movements, are created. Eventually, the update strategy is constructed. The created model is implemented leveraging in-context learning of LLMs. In this model,  database queries are created centered on indicated predefined slots and the order of retrieved entities is indicated centered on text part. This mechanism enables passing the whole corresponding entities to the preferences in the order of congruency. Evaluations exploiting the complete test conversations of MultiWOZ, with no more than a domain in a conversation, illustrate maximal inform and success, and improvement compared with previous methods.</li>
</ul>

<h3>Title: Submodular Context Partitioning and Compression for In-Context Learning-short paper</h3>
<ul>
<li><strong>Authors: </strong>Shaoyi Zheng, Canyu Zhang, Tianyi Zhou, Shengjie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05130">https://arxiv.org/abs/2510.05130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05130">https://arxiv.org/pdf/2510.05130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05130]] Submodular Context Partitioning and Compression for In-Context Learning-short paper(https://arxiv.org/abs/2510.05130)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) enables efficient few-shot learning in large language models (LLMs) without training, but suffers from the quadratic input complexity of transformers, limiting the maximum number of exemplars. While various efficient ICL approaches partition the context into blocks to process (e.g., ensembling, compression, cross-attention), they often ignore the information redundancy or under-representation caused by different partition strategies, leading to suboptimal performance. To tackle this problem, we propose Sub-CP, a block-aware context selection framework that leverages submodular objectives to control block diversity. Sub-CP supports a flexible spectrum of selection strategies, allowing each block to range from globally diverse to locally coherent. This allows fine-grained control over semantic structure while enabling precomputation. Extensive experiments across diverse tasks on multiple datasets show that Sub-CP consistently improves performance across model scales.</li>
</ul>

<h3>Title: Training Large Language Models To Reason In Parallel With Global Forking Tokens</h3>
<ul>
<li><strong>Authors: </strong>Sheng Jia, Xiao Wang, Shiva Prasad Kasiviswanathan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05132">https://arxiv.org/abs/2510.05132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05132">https://arxiv.org/pdf/2510.05132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05132]] Training Large Language Models To Reason In Parallel With Global Forking Tokens(https://arxiv.org/abs/2510.05132)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.</li>
</ul>

<h3>Title: Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs</h3>
<ul>
<li><strong>Authors: </strong>Qi Li, Runpeng Yu, Haiquan Lu, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05148">https://arxiv.org/abs/2510.05148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05148">https://arxiv.org/pdf/2510.05148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05148]] Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs(https://arxiv.org/abs/2510.05148)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a competitive paradigm for non-autoregressive language modeling. Their distinctive decoding mechanism enables faster inference speed and strong performance in code generation and mathematical tasks. In this work, we show that the decoding mechanism of dLLMs not only enhances model utility but also can be used as a powerful tool for model attribution. A key challenge in this problem lies in the diversity of attribution scenarios, including distinguishing between different models as well as between different checkpoints or backups of the same model. To ensure broad applicability, we identify two fundamental problems: what information to extract from the decoding trajectory, and how to utilize it effectively. We first observe that relying directly on per-step model confidence yields poor performance. This is mainly due to the bidirectional decoding nature of dLLMs: each newly decoded token influences the confidence of other decoded tokens, making model confidence highly redundant and washing out structural signal regarding decoding order or dependencies. To overcome this, we propose a novel information extraction scheme called the Directed Decoding Map (DDM), which captures structural relationships between decoding steps and better reveals model-specific behaviors. Furthermore, to make full use of the extracted structural information during attribution, we propose Gaussian-Trajectory Attribution (GTA), where we fit a cell-wise Gaussian distribution at each decoding position for each target model, and define the likelihood of a trajectory as the attribution score: if a trajectory exhibits higher log-likelihood under the distribution of a specific model, it is more likely to have been generated by that model. Extensive experiments under different settings validate the utility of our methods.</li>
</ul>

<h3>Title: A Single Character can Make or Break Your LLM Evals</h3>
<ul>
<li><strong>Authors: </strong>Jingtong Su, Jianyu Zhang, Karen Ullrich, LÃ©on Bottou, Mark Ibrahim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05152">https://arxiv.org/abs/2510.05152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05152">https://arxiv.org/pdf/2510.05152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05152]] A Single Character can Make or Break Your LLM Evals(https://arxiv.org/abs/2510.05152)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by $\pm 23\%$ depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.</li>
</ul>

<h3>Title: Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Arif Hakimi Zamrai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05160">https://arxiv.org/abs/2510.05160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05160">https://arxiv.org/pdf/2510.05160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05160]] Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders(https://arxiv.org/abs/2510.05160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inverse design, which seeks to find optimal parameters for a target output, is a central challenge in engineering. Surrogate-based optimization (SBO) has become a standard approach, yet it is fundamentally structured to converge to a single-point solution, thereby limiting design space exploration and ignoring potentially valuable alternative topologies. This paper presents a paradigm shift from single-point optimization to generative inverse design. We introduce a framework based on a Conditional Variational Autoencoder (CVAE) that learns a probabilistic mapping between a system's design parameters and its performance, enabling the generation of a diverse portfolio of high-performing candidates conditioned on a specific performance objective. We apply this methodology to the complex, non-linear problem of minimizing airfoil self-noise, using a high-performing SBO method from a prior benchmark study as a rigorous baseline. The CVAE framework successfully generated 256 novel designs with a 94.1\% validity rate. A subsequent surrogate-based evaluation revealed that 77.2\% of these valid designs achieved superior performance compared to the single optimal design found by the SBO baseline. This work demonstrates that the generative approach not only discovers higher-quality solutions but also provides a rich portfolio of diverse candidates, fundamentally enhancing the engineering design process by enabling multi-criteria decision-making.</li>
</ul>

<h3>Title: Machine learning for fraud detection in digital banking: a systematic literature review REVIEW</h3>
<ul>
<li><strong>Authors: </strong>Md Zahin Hossain George, Md Khorshed Alam, Md Tarek Hasan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05167">https://arxiv.org/abs/2510.05167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05167">https://arxiv.org/pdf/2510.05167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05167]] Machine learning for fraud detection in digital banking: a systematic literature review REVIEW(https://arxiv.org/abs/2510.05167)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This systematic literature review examines the role of machine learning in fraud detection within digital banking, synthesizing evidence from 118 peer-reviewed studies and institutional reports. Following the PRISMA guidelines, the review applied a structured identification, screening, eligibility, and inclusion process to ensure methodological rigor and transparency. The findings reveal that supervised learning methods, such as decision trees, logistic regression, and support vector machines, remain the dominant paradigm due to their interpretability and established performance, while unsupervised anomaly detection approaches are increasingly adopted to address novel fraud patterns in highly imbalanced datasets. Deep learning architectures, particularly recurrent and convolutional neural networks, have emerged as transformative tools capable of modeling sequential transaction data and detecting complex fraud typologies, though challenges of interpretability and real-time deployment persist. Hybrid models that combine supervised, unsupervised, and deep learning strategies demonstrate superior adaptability and detection accuracy, highlighting their potential as convergent solutions.</li>
</ul>

<h3>Title: Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data</h3>
<ul>
<li><strong>Authors: </strong>Anushiya Arunan, Yan Qin, Xiaoli Li, U-Xuan Tan, H. Vincent Poor, Chau Yuen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05172">https://arxiv.org/abs/2510.05172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05172">https://arxiv.org/pdf/2510.05172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05172]] Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data(https://arxiv.org/abs/2510.05172)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate battery capacity estimation is key to alleviating consumer concerns about battery performance and reliability of electric vehicles (EVs). However, practical data limitations imposed by stringent privacy regulations and labeled data shortages hamper the development of generalizable capacity estimation models that remain robust to real-world data distribution shifts. While self-supervised learning can leverage unlabeled data, existing techniques are not particularly designed to learn effectively from challenging field data -- let alone from privacy-friendly data, which are often less feature-rich and noisier. In this work, we propose a first-of-its-kind capacity estimation model based on self-supervised pre-training, developed on a large-scale dataset of privacy-friendly charging data snippets from real-world EV operations. Our pre-training framework, snippet similarity-weighted masked input reconstruction, is designed to learn rich, generalizable representations even from less feature-rich and fragmented privacy-friendly data. Our key innovation lies in harnessing contrastive learning to first capture high-level similarities among fragmented snippets that otherwise lack meaningful context. With our snippet-wise contrastive learning and subsequent similarity-weighted masked reconstruction, we are able to learn rich representations of both granular charging patterns within individual snippets and high-level associative relationships across different snippets. Bolstered by this rich representation learning, our model consistently outperforms state-of-the-art baselines, achieving 31.9% lower test error than the best-performing benchmark, even under challenging domain-shifted settings affected by both manufacturer and age-induced distribution shifts.</li>
</ul>

<h3>Title: SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Peigui Qi, Kunsheng Tang, Wenbo Zhou, Weiming Zhang, Nenghai Yu, Tianwei Zhang, Qing Guo, Jie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05173">https://arxiv.org/abs/2510.05173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05173">https://arxiv.org/pdf/2510.05173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05173]] SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models(https://arxiv.org/abs/2510.05173)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce \textbf{SafeGuider}, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, \textbf{SafeGuider} generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.</li>
</ul>

<h3>Title: A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Wagner-Carena, Aizhan Akhmetzhanova, Sydney Erickson</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05205">https://arxiv.org/abs/2510.05205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05205">https://arxiv.org/pdf/2510.05205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05205]] A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors(https://arxiv.org/abs/2510.05205)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A common challenge in the natural sciences is to disentangle distinct, unknown sources from observations. Examples of this source separation task include deblending galaxies in a crowded field, distinguishing the activity of individual neurons from overlapping signals, and separating seismic events from an ambient background. Traditional analyses often rely on simplified source models that fail to accurately reproduce the data. Recent advances have shown that diffusion models can directly learn complex prior distributions from noisy, incomplete data. In this work, we show that diffusion models can solve the source separation problem without explicit assumptions about the source. Our method relies only on multiple views, or the property that different sets of observations contain different linear transformations of the unknown sources. We show that our method succeeds even when no source is individually observed and the observations are noisy, incomplete, and vary in resolution. The learned diffusion models enable us to sample from the source priors, evaluate the probability of candidate sources, and draw from the joint posterior of the source distribution given an observation. We demonstrate the effectiveness of our method on a range of synthetic problems as well as real-world galaxy observations.</li>
</ul>

<h3>Title: Mitigating Diffusion Model Hallucinations with Dynamic Guidance</h3>
<ul>
<li><strong>Authors: </strong>Kostas Triaridis, Alexandros Graikos, Aggelina Chatziagapi, Grigorios G. Chrysos, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05356">https://arxiv.org/abs/2510.05356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05356">https://arxiv.org/pdf/2510.05356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05356]] Mitigating Diffusion Model Hallucinations with Dynamic Guidance(https://arxiv.org/abs/2510.05356)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models, despite their impressive demos, often produce hallucinatory samples with structural inconsistencies that lie outside of the support of the true data distribution. Such hallucinations can be attributed to excessive smoothing between modes of the data distribution. However, semantic interpolations are often desirable and can lead to generation diversity, thus we believe a more nuanced solution is required. In this work, we introduce Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates hallucinations by selectively sharpening the score function only along the pre-determined directions known to cause artifacts, while preserving valid semantic variations. To our knowledge, this is the first approach that addresses hallucinations at generation time rather than through post-hoc filtering. Dynamic Guidance substantially reduces hallucinations on both controlled and natural image datasets, significantly outperforming baselines.</li>
</ul>

<h3>Title: LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yang Xiao, Gen Li, Kaiyuan Deng, Yushu Wu, Zheng Zhan, Yanzhi Wang, Xiaolong Ma, Bo Hui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05367">https://arxiv.org/abs/2510.05367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05367">https://arxiv.org/pdf/2510.05367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05367]] LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation(https://arxiv.org/abs/2510.05367)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at this https URL .</li>
</ul>

<h3>Title: See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kebin Contreras, Luis Toscano-Palomino, Mauro Dalla Mura, Jorge Bacca</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05408">https://arxiv.org/abs/2510.05408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05408">https://arxiv.org/pdf/2510.05408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05408]] See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models(https://arxiv.org/abs/2510.05408)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recovering the past from present observations is an intriguing challenge with potential applications in forensics and scene analysis. Thermal imaging, operating in the infrared range, provides access to otherwise invisible information. Since humans are typically warmer (37 C -98.6 F) than their surroundings, interactions such as sitting, touching, or leaning leave residual heat traces. These fading imprints serve as passive temporal codes, allowing for the inference of recent events that exceed the capabilities of RGB cameras. This work proposes a time-reversed reconstruction framework that uses paired RGB and thermal images to recover scene states from a few seconds earlier. The proposed approach couples Visual-Language Models (VLMs) with a constrained diffusion process, where one VLM generates scene descriptions and another guides image reconstruction, ensuring semantic and structural consistency. The method is evaluated in three controlled scenarios, demonstrating the feasibility of reconstructing plausible past frames up to 120 seconds earlier, providing a first step toward time-reversed imaging from thermal traces.</li>
</ul>

<h3>Title: High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyi Huang, Nutan Sahoo, Anamika Kumari, Girish Kumar, Kexuan Cai, Shixing Cao, Yue Kang, Tian Xia, Somya Chatterjee, Nicholas Hausman, Aidan Jay, Eric S. Rosenthal, Soundar Srinivasan, Sadid Hasan, Alex Fedorov, Sulaiman Vesal, Soundar Srinivasan, Sadid Hasan, Alex Fedorov, Sulaiman Vesal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05492">https://arxiv.org/abs/2510.05492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05492">https://arxiv.org/pdf/2510.05492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05492]] High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training(https://arxiv.org/abs/2510.05492)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The development of machine learning for cardiac care is severely hampered by privacy restrictions on sharing real patient electrocardiogram (ECG) data. Although generative AI offers a promising solution, the real-world use of existing model-synthesized ECGs is limited by persistent gaps in trustworthiness and clinical utility. In this work, we address two major shortcomings of current generative ECG methods: insufficient morphological fidelity and the inability to generate personalized, patient-specific physiological signals. To address these gaps, we build on a conditional diffusion-based Structured State Space Model (SSSD-ECG) with two principled innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a novel training paradigm with time-frequency domain supervision to enforce physiological structural realism, and (2) multi-modal demographic conditioning to enable patient-specific synthesis. We comprehensively evaluate our approach on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity, clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG achieves substantial gains: it improves morphological coherence, preserves strong privacy guarantees with all metrics evaluated exceeding the baseline by 4-8%, and notably reduces the interlead correlation error by an average of 74%, while demographic conditioning enhances signal-to-noise ratio and personalization. In critical low-data regimes, a classifier trained on datasets supplemented with our synthetic ECGs achieves performance comparable to a classifier trained solely on real data. Together, we demonstrate that ECG synthesizers, trained with the proposed time-frequency structural regularization scheme, can serve as personalized, high-fidelity, privacy-preserving surrogates when real data are scarce, advancing the responsible use of generative AI in healthcare.</li>
</ul>

<h3>Title: Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shinnosuke Saito, Takashi Matsubara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05509">https://arxiv.org/abs/2510.05509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05509">https://arxiv.org/pdf/2510.05509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05509]] Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models(https://arxiv.org/abs/2510.05509)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are powerful deep generative models (DGMs) that generate high-fidelity, diverse content. However, unlike classical DGMs, they lack an explicit, tractable low-dimensional latent space that parameterizes the data manifold. This absence limits manifold-aware analysis and operations, such as interpolation and editing. Existing interpolation methods for diffusion models typically follow paths through high-density regions, which are not necessarily aligned with the data manifold and can yield perceptually unnatural transitions. To exploit the data manifold learned by diffusion models, we propose a novel Riemannian metric on the noise space, inspired by recent findings that the Jacobian of the score function captures the tangent spaces to the local data manifold. This metric encourages geodesics in the noise space to stay within or run parallel to the learned data manifold. Experiments on image interpolation show that our metric produces perceptually more natural and faithful transitions than existing density-based and naive baselines.</li>
</ul>

<h3>Title: LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability</h3>
<ul>
<li><strong>Authors: </strong>Harshil Vejendla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05530">https://arxiv.org/abs/2510.05530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05530">https://arxiv.org/pdf/2510.05530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05530]] LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability(https://arxiv.org/abs/2510.05530)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Test-time adaptation (TTA) aims to adapt a pretrained model to distribution shifts using only unlabeled test data. While promising, existing methods like Tent suffer from instability and can catastrophically forget the source knowledge, especially with small batch sizes or challenging corruptions. We argue that this arises from overly deterministic updates on a complex loss surface. In this paper, we introduce Langevin-Anchored Test-Time Adaptation (LATTA), a novel approach that regularizes adaptation through two key mechanisms: (1) a noisy weight perturbation inspired by Stochastic Gradient Langevin Dynamics (SGLD) to explore the local parameter space and escape poor local minima, and (2) a stable weight anchor that prevents the model from diverging from its robust source pre-training. This combination allows LATTA to adapt effectively without sacrificing stability. Unlike prior Bayesian TTA methods, LATTA requires no architectural changes or expensive Monte Carlo passes. We conduct extensive experiments on standard benchmarks, including Rotated-MNIST and the more challenging CIFAR-10-C. Our results demonstrate that LATTA significantly outperforms existing methods, including Tent, CoTTA, and EATA, setting a new state of the art for self-supervised TTA by improving average accuracy on CIFAR-10-C by over 2% while simultaneously reducing performance variance.</li>
</ul>

<h3>Title: Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Sam Sartor, Pieter Peers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05532">https://arxiv.org/abs/2510.05532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05532">https://arxiv.org/pdf/2510.05532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05532]] Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation(https://arxiv.org/abs/2510.05532)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large pretrained diffusion models can provide strong priors beneficial for many graphics applications. However, generative applications such as neural rendering and inverse methods such as SVBRDF estimation and intrinsic image decomposition require additional input or output channels. Current solutions for channel expansion are often application specific and these solutions can be difficult to adapt to different diffusion models or new tasks. This paper introduces Teamwork: a flexible and efficient unified solution for jointly increasing the number of input and output channels as well as adapting a pretrained diffusion model to new tasks. Teamwork achieves channel expansion without altering the pretrained diffusion model architecture by coordinating and adapting multiple instances of the base diffusion model (\ie, teammates). We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address both adaptation and coordination between the different teammates. Furthermore Teamwork supports dynamic (de)activation of teammates. We demonstrate the flexibility and efficiency of Teamwork on a variety of generative and inverse graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic decomposition, neural shading, and intrinsic image synthesis.</li>
</ul>

<h3>Title: Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection</h3>
<ul>
<li><strong>Authors: </strong>Rui Liu, Tao Zhe, Yanjie Fu, Feng Xia, Ted Senator, Dongjie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05535">https://arxiv.org/abs/2510.05535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05535">https://arxiv.org/pdf/2510.05535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05535]] Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection(https://arxiv.org/abs/2510.05535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Feature selection eliminates redundancy among features to improve downstream task performance while reducing computational overhead. Existing methods often struggle to capture intricate feature interactions and adapt across diverse application scenarios. Recent advances employ generative intelligence to alleviate these drawbacks. However, these methods remain constrained by permutation sensitivity in embedding and reliance on convexity assumptions in gradient-based search. To address these limitations, our initial work introduces a novel framework that integrates permutation-invariant embedding with policy-guided search. Although effective, it still left opportunities to adapt to realistic distributed scenarios. In practice, data across local clients is highly imbalanced, heterogeneous and constrained by strict privacy regulations, limiting direct sharing. These challenges highlight the need for a framework that can integrate feature selection knowledge across clients without exposing sensitive information. In this extended journal version, we advance the framework from two perspectives: 1) developing a privacy-preserving knowledge fusion strategy to derive a unified representation space without sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy to address distributional imbalance among heterogeneous local clients. Extensive experiments validate the effectiveness, robustness, and efficiency of our framework. The results further demonstrate its strong generalization ability in federated learning scenarios. The code and data are publicly available: this https URL.</li>
</ul>

<h3>Title: Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Christopher Hoang, Mengye Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05558">https://arxiv.org/abs/2510.05558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05558">https://arxiv.org/pdf/2510.05558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05558]] Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics(https://arxiv.org/abs/2510.05558)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Object recognition and motion understanding are key components of perception that complement each other. While self-supervised learning methods have shown promise in their ability to learn from unlabeled data, they have primarily focused on obtaining rich representations for either recognition or motion rather than both in tandem. On the other hand, latent dynamics modeling has been used in decision making to learn latent representations of observations and their transformations over time for control and planning tasks. In this work, we present Midway Network, a new self-supervised learning architecture that is the first to learn strong visual representations for both object recognition and motion understanding solely from natural videos, by extending latent dynamics modeling to this domain. Midway Network leverages a midway top-down path to infer motion latents between video frames, as well as a dense forward prediction objective and hierarchical structure to tackle the complex, multi-object scenes of natural videos. We demonstrate that after pretraining on two large-scale natural video datasets, Midway Network achieves strong performance on both semantic segmentation and optical flow tasks relative to prior self-supervised learning methods. We also show that Midway Network's learned dynamics can capture high-level correspondence via a novel analysis method based on forward feature perturbation.</li>
</ul>

<h3>Title: HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video</h3>
<ul>
<li><strong>Authors: </strong>Hongchi Xia, Chih-Hao Lin, Hao-Yu Hsu, Quentin Leboutet, Katelyn Gao, Michael Paulitsch, Benjamin Ummenhofer, Shenlong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05560">https://arxiv.org/abs/2510.05560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05560">https://arxiv.org/pdf/2510.05560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05560]] HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video(https://arxiv.org/abs/2510.05560)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: this https URL.</li>
</ul>

<h3>Title: Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection</h3>
<ul>
<li><strong>Authors: </strong>Sheng Xiang, Yidong Jiang, Yunting Chen, Dawei Cheng, Guoping Zhao, Changjun Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05562">https://arxiv.org/abs/2510.05562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05562">https://arxiv.org/pdf/2510.05562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05562]] Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection(https://arxiv.org/abs/2510.05562)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Spoofing detection in financial trading is crucial, especially for identifying complex behaviors such as conspiracy spoofing. Traditional machine-learning approaches primarily focus on isolated node features, often overlooking the broader context of interconnected nodes. Graph-based techniques, particularly Graph Neural Networks (GNNs), have advanced the field by leveraging relational information effectively. However, in real-world spoofing detection datasets, trading behaviors exhibit dynamic, irregular patterns. Existing spoofing detection methods, though effective in some scenarios, struggle to capture the complexity of dynamic and diverse, evolving inter-node relationships. To address these challenges, we propose a novel framework called the Generative Dynamic Graph Model (GDGM), which models dynamic trading behaviors and the relationships among nodes to learn representations for conspiracy spoofing detection. Specifically, our approach incorporates the generative dynamic latent space to capture the temporal patterns and evolving market conditions. Raw trading data is first converted into time-stamped sequences. Then we model trading behaviors using the neural ordinary differential equations and gated recurrent units, to generate the representation incorporating temporal dynamics of spoofing patterns. Furthermore, pseudo-label generation and heterogeneous aggregation techniques are employed to gather relevant information and enhance the detection performance for conspiratorial spoofing behaviors. Experiments conducted on spoofing detection datasets demonstrate that our approach outperforms state-of-the-art models in detection accuracy. Additionally, our spoofing detection system has been successfully deployed in one of the largest global trading markets, further validating the practical applicability and performance of the proposed method.</li>
</ul>

<h3>Title: Improving Chain-of-Thought Efficiency for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zeqi Gu, Markos Georgopoulos, Xiaoliang Dai, Marjan Ghazvininejad, Chu Wang, Felix Juefei-Xu, Kunpeng Li, Yujun Shi, Zecheng He, Zijian He, Jiawei Zhou, Abe Davis, Jialiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05593">https://arxiv.org/abs/2510.05593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05593">https://arxiv.org/pdf/2510.05593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05593]] Improving Chain-of-Thought Efficiency for Autoregressive Image Generation(https://arxiv.org/abs/2510.05593)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.</li>
</ul>

<h3>Title: Efficient Conditional Generation on Scale-based Visual Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Liu, Tao Huang, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05610">https://arxiv.org/abs/2510.05610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05610">https://arxiv.org/pdf/2510.05610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05610]] Efficient Conditional Generation on Scale-based Visual Autoregressive Models(https://arxiv.org/abs/2510.05610)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in autoregressive (AR) models have demonstrated their potential to rival diffusion models in image synthesis. However, for complex spatially-conditioned generation, current AR approaches rely on fine-tuning the pre-trained model, leading to significant training costs. In this paper, we propose the Efficient Control Model (ECM), a plug-and-play framework featuring a lightweight control module that introduces control signals via a distributed architecture. This architecture consists of context-aware attention layers that refine conditional features using real-time generated tokens, and a shared gated feed-forward network (FFN) designed to maximize the utilization of its limited capacity and ensure coherent control feature learning. Furthermore, recognizing the critical role of early-stage generation in determining semantic structure, we introduce an early-centric sampling strategy that prioritizes learning early control sequences. This approach reduces computational cost by lowering the number of training tokens per iteration, while a complementary temperature scheduling during inference compensates for the resulting insufficient training of late-stage tokens. Extensive experiments on scale-based AR models validate that our method achieves high-fidelity and diverse control over image generation, surpassing existing baselines while significantly improving both training and inference efficiency.</li>
</ul>

<h3>Title: PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Meng, Qichao Wang, Zhiyang Dou, Zixing Song, Zhipeng Zhou, Irwin King, Peilin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05613">https://arxiv.org/abs/2510.05613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05613">https://arxiv.org/pdf/2510.05613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05613]] PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction(https://arxiv.org/abs/2510.05613)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.</li>
</ul>

<h3>Title: InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Salihu Yusuf, Iffanice Houndayi, Rym Oualha, Mohamed Aziz Cherif, Kobby Panford-Quainoo, Arnu Pretorius</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05617">https://arxiv.org/abs/2510.05617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05617">https://arxiv.org/pdf/2510.05617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05617]] InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment(https://arxiv.org/abs/2510.05617)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Open-access multispectral imagery from missions like Landsat 8-9 and Sentinel-2 has fueled the development of geospatial foundation models (GFMs) for humanitarian and environmental applications. Yet, their deployment remains limited by (i) the absence of automated geospatial data pipelines and (ii) the large size of fine-tuned models. Existing GFMs lack workflows for processing raw satellite imagery, and downstream adaptations often retain the full complexity of the original encoder. We present InstaGeo, an open-source, end-to-end framework that addresses these challenges by integrating: (1) automated data curation to transform raw imagery into model-ready datasets; (2) task-specific model distillation to derive compact, compute-efficient models; and (3) seamless deployment as interactive web-map applications. Using InstaGeo, we reproduced datasets from three published studies and trained models with marginal mIoU differences of -0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for desert locust prediction. The distilled models are up to 8x smaller than standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal accuracy loss. Leveraging InstaGeo's streamlined data pipeline, we also curated a larger crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp improvement over prior baselines. Moreover, InstaGeo enables users to progress from raw data to model deployment within a single working day. By unifying data preparation, model compression, and deployment, InstaGeo transforms research-grade GFMs into practical, low-carbon tools for real-time, large-scale Earth observation. This approach shifts geospatial AI toward data quality and application-driven innovation. Source code, datasets, and model checkpoints are available at: this https URL</li>
</ul>

<h3>Title: Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Sara Mandelli, Diego Vila-Portela, David VÃ¡zquez-PadÃ­n, Paolo Bestagini, Fernando PÃ©rez-GonzÃ¡lez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05633">https://arxiv.org/abs/2510.05633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05633">https://arxiv.org/pdf/2510.05633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05633]] Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection(https://arxiv.org/abs/2510.05633)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Over the years, the forensics community has proposed several deep learning-based detectors to mitigate the risks of generative AI. Recently, frequency-domain artifacts (particularly periodic peaks in the magnitude spectrum), have received significant attention, as they have been often considered a strong indicator of synthetic image generation. However, state-of-the-art detectors are typically used as black-boxes, and it still remains unclear whether they truly rely on these peaks. This limits their interpretability and trust. In this work, we conduct a systematic study to address this question. We propose a strategy to remove spectral peaks from images and analyze the impact of this operation on several detectors. In addition, we introduce a simple linear detector that relies exclusively on frequency peaks, providing a fully interpretable baseline free from the confounding influence of deep learning. Our findings reveal that most detectors are not fundamentally dependent on spectral peaks, challenging a widespread assumption in the field and paving the way for more transparent and reliable forensic tools.</li>
</ul>

<h3>Title: Teleportraits: Training-Free People Insertion into Any Scene</h3>
<ul>
<li><strong>Authors: </strong>Jialu Gao, K J Joseph, Fernando De La Torre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05660">https://arxiv.org/abs/2510.05660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05660">https://arxiv.org/pdf/2510.05660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05660]] Teleportraits: Training-Free People Insertion into Any Scene(https://arxiv.org/abs/2510.05660)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The task of realistically inserting a human from a reference image into a background scene is highly challenging, requiring the model to (1) determine the correct location and poses of the person and (2) perform high-quality personalization conditioned on the background. Previous approaches often treat them as separate problems, overlooking their interconnections, and typically rely on training to achieve high performance. In this work, we introduce a unified training-free pipeline that leverages pre-trained text-to-image diffusion models. We show that diffusion models inherently possess the knowledge to place people in complex scenes without requiring task-specific training. By combining inversion techniques with classifier-free guidance, our method achieves affordance-aware global editing, seamlessly inserting people into scenes. Furthermore, our proposed mask-guided self-attention mechanism ensures high-quality personalization, preserving the subject's identity, clothing, and body features from just a single reference image. To the best of our knowledge, we are the first to perform realistic human insertions into scenes in a training-free manner and achieve state-of-the-art results in diverse composite scene images with excellent identity preservation in backgrounds and subjects.</li>
</ul>

<h3>Title: Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Jike Zhong, Yuxiang Lai, Xiaofeng Yang, Konstantinos Psounis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05674">https://arxiv.org/abs/2510.05674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05674">https://arxiv.org/pdf/2510.05674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05674]] Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension(https://arxiv.org/abs/2510.05674)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advances in language modeling have witnessed the rise of highly desirable emergent capabilities, such as reasoning and in-context learning. However, vision models have yet to exhibit comparable progress in these areas. In this paper, we argue that this gap could stem from the lack of semantic and contextual guidance in current vision transformer (ViT) training schemes, and such a gap can be narrowed through the design of a semantic-grounded objective. Specifically, we notice that individual words in natural language are inherently semantic, and modeling directly on word tokens naturally learns a realistic distribution. In contrast, ViTs rely on spatial patchification, which inevitably lacks semantic information. To bridge this gap, we propose to directly model "object" as the visual equivalence of "word," pushing the model to learn the global context and semantics among visual elements. We investigate our hypotheses via masked image modeling (MIM), a framework where our approach can be readily tested by applying masks to visual objects rather than random patches. Considerable evidence from qualitative and quantitative evaluations reveals a key finding: object-level representation alone helps to learn a real-world distribution, whereas pixel-averaging shortcuts are often learned without it. Moreover, further evaluations with multimodal LLMs (MLLM) on visual question answering (VQA, GQA, ScienceQA) tasks demonstrate the strong reasoning and contextual understanding gained with this simple objective. We hope our study highlights the effectiveness of object-level encoding and provides a plausible direction for developing stronger vision encoders and tokenizers. Code and model will be publicly released. Keywords: Semantic Visual Tokenizer, Vision Reasoning, In-context Learning, Multimodal Reasoning</li>
</ul>

<h3>Title: Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haneul Yoo, Jiho Jin, Kyunghyun Cho, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05678">https://arxiv.org/abs/2510.05678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05678">https://arxiv.org/pdf/2510.05678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05678]] Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models(https://arxiv.org/abs/2510.05678)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) exhibit strong multilingual abilities, their reliance on English as latent representations creates a translation barrier, where reasoning implicitly depends on internal translation into English. When this process fails, performance in non-English languages deteriorates sharply, limiting the inclusiveness of LLM-based applications. Existing cross-lingual in-context learning (X-ICL) methods primarily leverage monolingual demonstrations, often failing to mitigate this barrier and instead reinforcing it. In this work, we introduce code-switching in-context learning (CSICL), a simple yet effective prompting strategy that progressively transitions from a target language to English within demonstrations and instruction to facilitate their latent reasoning in English. By explicitly scaffolding the reasoning process through controlled code-switching, CSICL acts as an implicit linguistic bridge that enhances cross-lingual alignment and reduces reliance on the translation barrier. We conduct extensive experiments across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive and reasoning-oriented domains. Our results demonstrate that CSICL consistently outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target and unseen languages, respectively. The improvement is even more pronounced in low-resource settings, with gains of 14.7% in target and 5.3% in unseen languages. These findings establish code-switching as a principled and robust approach for overcoming the translation barrier during inference, moving LLMs toward more equitable and effective multilingual systems.</li>
</ul>

<h3>Title: AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shihao Zhu, Bohan Cao, Ziheng Ouyang, Zhen Li, Peng-Tao Jiang, Qibin Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05715">https://arxiv.org/abs/2510.05715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05715">https://arxiv.org/pdf/2510.05715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05715]] AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models(https://arxiv.org/abs/2510.05715)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent diffusion model research focuses on generating identity-consistent images from a reference photo, but they struggle to accurately control age while preserving identity, and fine-tuning such models often requires costly paired images across ages. In this paper, we propose AgeBooth, a novel age-specific finetuning approach that can effectively enhance the age control capability of adapterbased identity personalization models without the need for expensive age-varied datasets. To reduce dependence on a large amount of age-labeled data, we exploit the linear nature of aging by introducing age-conditioned prompt blending and an age-specific LoRA fusion strategy that leverages SVDMix, a matrix fusion technique. These techniques enable high-quality generation of intermediate-age portraits. Our AgeBooth produces realistic and identity-consistent face images across different ages from a single reference image. Experiments show that AgeBooth achieves superior age control and visual quality compared to previous state-of-the-art editing-based methods.</li>
</ul>

<h3>Title: DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities</h3>
<ul>
<li><strong>Authors: </strong>Hedi Zisling, Ilan Naiman, Nimrod Berman, Supasorn Suwajanakorn, Omri Azencot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05717">https://arxiv.org/abs/2510.05717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05717">https://arxiv.org/pdf/2510.05717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05717]] DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities(https://arxiv.org/abs/2510.05717)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Unsupervised representation learning, particularly sequential disentanglement, aims to separate static and dynamic factors of variation in data without relying on labels. This remains a challenging problem, as existing approaches based on variational autoencoders and generative adversarial networks often rely on multiple loss terms, complicating the optimization process. Furthermore, sequential disentanglement methods face challenges when applied to real-world data, and there is currently no established evaluation protocol for assessing their performance in such settings. Recently, diffusion models have emerged as state-of-the-art generative models, but no theoretical formalization exists for their application to sequential disentanglement. In this work, we introduce the Diffusion Sequential Disentanglement Autoencoder (DiffSDA), a novel, modal-agnostic framework effective across diverse real-world data modalities, including time series, video, and audio. DiffSDA leverages a new probabilistic modeling, latent diffusion, and efficient samplers, while incorporating a challenging evaluation protocol for rigorous testing. Our experiments on diverse real-world benchmarks demonstrate that DiffSDA outperforms recent state-of-the-art methods in sequential disentanglement.</li>
</ul>

<h3>Title: Data Factory with Minimal Human Effort Using VLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaojiao Ye, Jiaxing Zhong, Qian Xie, Yuzhou Zhou, Niki Trigoni, Andrew Markham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05722">https://arxiv.org/abs/2510.05722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05722">https://arxiv.org/pdf/2510.05722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05722]] Data Factory with Minimal Human Effort Using VLMs(https://arxiv.org/abs/2510.05722)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating enough and diverse data through augmentation offers an efficient solution to the time-consuming and labour-intensive process of collecting and annotating pixel-wise images. Traditional data augmentation techniques often face challenges in manipulating high-level semantic attributes, such as materials and textures. In contrast, diffusion models offer a robust alternative, by effectively utilizing text-to-image or image-to-image transformation. However, existing diffusion-based methods are either computationally expensive or compromise on performance. To address this issue, we introduce a novel training-free pipeline that integrates pretrained ControlNet and Vision-Language Models (VLMs) to generate synthetic images paired with pixel-level labels. This approach eliminates the need for manual annotations and significantly improves downstream tasks. To improve the fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i present promising performance and outperform concurrent work for one-shot semantic segmentation.</li>
</ul>

<h3>Title: Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies</h3>
<ul>
<li><strong>Authors: </strong>Chunsan Hong, Seonho An, Min-Soo Kim, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05725">https://arxiv.org/abs/2510.05725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05725">https://arxiv.org/pdf/2510.05725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05725]] Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies(https://arxiv.org/abs/2510.05725)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) have recently emerged as a novel framework for language modeling. MDMs generate sentences by iteratively denoising masked sequences, filling in [MASK] tokens step by step. Although MDMs support any-order sampling, performance is highly sensitive to the choice of which position to unmask next. Prior work typically relies on rule-based schedules (e.g., max-confidence, max-margin), which provide ad hoc improvements. In contrast, we replace these heuristics with a learned scheduler. Specifically, we cast denoising as a KL-regularized Markov decision process (MDP) with an explicit reference policy and optimize a regularized objective that admits policy improvement and convergence guarantees under standard assumptions. We prove that the optimized policy under this framework generates samples that more closely match the data distribution than heuristic schedules. Empirically, across four benchmarks, our learned policy consistently outperforms max-confidence: for example, on SUDOKU, where unmasking order is critical, it yields a 20.1% gain over random and a 11.2% gain over max-confidence.</li>
</ul>

<h3>Title: Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect</h3>
<ul>
<li><strong>Authors: </strong>Amirtaha Amanzadi, Zahra Dehghanian, Hamid Beigy, Hamid R. Rabiee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05740">https://arxiv.org/abs/2510.05740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05740">https://arxiv.org/pdf/2510.05740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05740]] Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect(https://arxiv.org/abs/2510.05740)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>The rapid development of generative models has made it increasingly crucial to develop detectors that can reliably detect synthetic images. Although most of the work has now focused on cross-generator generalization, we argue that this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap,we present the OmniGen Benchmark. This comprehensive evaluation dataset incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, FusionDetect, aimed at addressing both vectors of generalization. FusionDetect draws on the benefits of two frozen foundation models: CLIP & Dinov2. By deriving features from both complementary models,we develop a cohesive feature space that naturally adapts to changes in both thecontent and design of the generator. Our extensive experiments demonstrate that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more accurate than its closest competitor and 6.13% more precise on average on established benchmarks, but also achieves a 4.48% increase in accuracy on OmniGen,along with exceptional robustness to common image perturbations. We introduce not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection. The code and dataset are available at this http URL</li>
</ul>

<h3>Title: ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yongxuan Lyu, Guangfeng Jiang, Hongsi Liu, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05752">https://arxiv.org/abs/2510.05752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05752">https://arxiv.org/pdf/2510.05752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05752]] ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving(https://arxiv.org/abs/2510.05752)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The manual annotation of outdoor LiDAR point clouds for instance segmentation is extremely costly and time-consuming. Current methods attempt to reduce this burden but still rely on some form of human labeling. To completely eliminate this dependency, we introduce ALISE, a novel framework that performs LiDAR instance segmentation without any annotations. The central challenge is to generate high-quality pseudo-labels in a fully unsupervised manner. Our approach starts by employing Vision Foundation Models (VFMs), guided by text and images, to produce initial pseudo-labels. We then refine these labels through a dedicated spatio-temporal voting module, which combines 2D and 3D semantics for both offline and online optimization. To achieve superior feature learning, we further introduce two forms of semantic supervision: a set of 2D prior-based losses that inject visual knowledge into the 3D network, and a novel prototype-based contrastive loss that builds a discriminative feature space by exploiting 3D semantic consistency. This comprehensive design results in significant performance gains, establishing a new state-of-the-art for unsupervised 3D instance segmentation. Remarkably, our approach even outperforms MWSIS, a method that operates with supervision from ground-truth (GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).</li>
</ul>

<h3>Title: Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Bai, Gauri Pradhan, Marlon Tobaben, Antti Honkela</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05753">https://arxiv.org/abs/2510.05753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05753">https://arxiv.org/pdf/2510.05753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05753]] Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning(https://arxiv.org/abs/2510.05753)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>With the emergence of powerful large-scale foundation models, the training paradigm is increasingly shifting from from-scratch training to transfer learning. This enables high utility training with small, domain-specific datasets typical in sensitive this http URL inference attacks (MIAs) provide an empirical estimate of the privacy leakage by machine learning models. Yet, prior assessments of MIAs against models fine-tuned with transfer learning rely on a small subset of possible attacks. We address this by comparing performance of diverse MIAs in transfer learning settings to help practitioners identify the most efficient attacks for privacy risk evaluation. We find that attack efficacy decreases with the increase in training data for score-based MIAs. We find that there is no one MIA which captures all privacy risks in models trained with transfer learning. While the Likelihood Ratio Attack (LiRA) demonstrates superior performance across most experimental scenarios, the Inverse Hessian Attack (IHA) proves to be more effective against models fine-tuned on PatchCamelyon dataset in high data regime.</li>
</ul>

<h3>Title: OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search</h3>
<ul>
<li><strong>Authors: </strong>Zexin Zheng, Huangyu Dai, Lingtao Mao, Xinyu Sun, Zihan Liang, Ben Chen, Yuqing Ding, Chenyi Lei, Wenwu Ou, Han Li, Kun Gai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05759">https://arxiv.org/abs/2510.05759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05759">https://arxiv.org/pdf/2510.05759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05759]] OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search(https://arxiv.org/abs/2510.05759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Traditional vision search, similar to search and recommendation systems, follows the multi-stage cascading architecture (MCA) paradigm to balance efficiency and conversion. Specifically, the query image undergoes feature extraction, recall, pre-ranking, and ranking stages, ultimately presenting the user with semantically similar products that meet their preferences. This multi-view representation discrepancy of the same object in the query and the optimization objective collide across these stages, making it difficult to achieve Pareto optimality in both user experience and conversion. In this paper, an end-to-end generative framework, OneVision, is proposed to address these problems. OneVision builds on VRQ, a vision-aligned residual quantization encoding, which can align the vastly different representations of an object across multiple viewpoints while preserving the distinctive features of each product as much as possible. Then a multi-stage semantic alignment scheme is adopted to maintain strong visual similarity priors while effectively incorporating user-specific information for personalized preference generation. In offline evaluations, OneVision performs on par with online MCA, while improving inference efficiency by 21% through dynamic pruning. In A/B tests, it achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and +3.12% order volume. These results demonstrate that a semantic ID centric, generative architecture can unify retrieval and personalization while simplifying the serving pathway.</li>
</ul>

<h3>Title: New Insights into Involutory and Orthogonal MDS Matrices</h3>
<ul>
<li><strong>Authors: </strong>Yogesh Kumar, Susanta Samanta, Atul Gaur</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05766">https://arxiv.org/abs/2510.05766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05766">https://arxiv.org/pdf/2510.05766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05766]] New Insights into Involutory and Orthogonal MDS Matrices(https://arxiv.org/abs/2510.05766)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>MDS matrices play a critical role in the design of diffusion layers for block ciphers and hash functions due to their optimal branch number. Involutory and orthogonal MDS matrices offer additional benefits by allowing identical or nearly identical circuitry for both encryption and decryption, leading to equivalent implementation costs for both processes. These properties have been further generalized through the notions of semi-involutory and semi-orthogonal matrices. Specifically, we establish nontrivial interconnections between semi-involutory and involutory matrices, as well as between semi-orthogonal and orthogonal matrices. Exploiting these relationships, we show that the number of semi-involutory MDS matrices can be directly derived from the number of involutory MDS matrices, and vice versa. A similar correspondence holds for semi-orthogonal and orthogonal MDS matrices. We also examine the intersection of these classes and show that the number of $3 \times 3$ MDS matrices that are both semi-involutory and semi-orthogonal coincides with the number of semi-involutory MDS matrices over $\mathbb{F}_{2^m}$. Furthermore, we derive the general structure of orthogonal matrices of arbitrary order $n$ over $\mathbb{F}_{2^m}$. Based on this generic form, we provide a closed-form expression for enumerating all $3 \times 3$ orthogonal MDS matrices over $\mathbb{F}_{2^m}$. Finally, leveraging the aforementioned interconnections, we present explicit formulas for counting $3 \times 3$ semi-involutory MDS matrices and semi-orthogonal MDS matrices.</li>
</ul>

<h3>Title: Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images</h3>
<ul>
<li><strong>Authors: </strong>Sven Koehler, Sarah Kaye Mueller, Jonathan Kiekenap, Gerald Greil, Tarique Hussain, Samir Sarikouch, Florian AndrÃ©, Norbert Frey, Sandy Engelhardt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05819">https://arxiv.org/abs/2510.05819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05819">https://arxiv.org/pdf/2510.05819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05819]] Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images(https://arxiv.org/abs/2510.05819)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Cardiovascular magnetic resonance (CMR) is the gold standard for assessing cardiac function, but individual cardiac cycles complicate automatic temporal comparison or sub-phase analysis. Accurate cardiac keyframe detection can eliminate this problem. However, automatic methods solely derive end-systole (ES) and end-diastole (ED) frames from left ventricular volume curves, which do not provide a deeper insight into myocardial motion. We propose a self-supervised deep learning method detecting five keyframes in short-axis (SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable registration fields are derived from the images and used to compute a 1D motion descriptor, which provides valuable insights into global cardiac contraction and relaxation patterns. From these characteristic curves, keyframes are determined using a simple set of rules. The method was independently evaluated for both views using three public, multicentre, multidisease datasets. M&Ms-2 (n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC (n=100) datasets for repeatability control. Furthermore, generalisability to patients with rare congenital heart defects was tested using the German Competence Network (GCN) dataset. Our self-supervised approach achieved improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED and ES, as measured by cyclic frame difference (cFD), compared with the volume-based approach. We can detect ED and ES, as well as three additional keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and intra-patient analysis of cardiac dynamics, irrespective of cycle or phase lengths. GitHub repository: this https URL</li>
</ul>

<h3>Title: Multimodal Trajectory Representation Learning for Travel Time Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zhi Liu, Xuyuan Hu, Xiao Han, Zhehao Dai, Zhaolin Deng, Guojiang Shen, Xiangjie Kong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05840">https://arxiv.org/abs/2510.05840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05840">https://arxiv.org/pdf/2510.05840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05840]] Multimodal Trajectory Representation Learning for Travel Time Estimation(https://arxiv.org/abs/2510.05840)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate travel time estimation (TTE) plays a crucial role in intelligent transportation systems. However, it remains challenging due to heterogeneous data sources and complex traffic dynamics. Moreover, conventional approaches typically convert trajectories into fixed-length representations, neglecting the inherent variability of real-world trajectories, which often leads to information loss or feature redundancy. To address these challenges, this paper introduces the Multimodal Dynamic Trajectory Integration (MDTI) framework--a novel multimodal trajectory representation learning approach that integrates GPS sequences, grid trajectories, and road network constraints to enhance TTE accuracy. MDTI employs modality-specific encoders and a cross-modal interaction module to capture complementary spatial, temporal, and topological semantics, while a dynamic trajectory modeling mechanism adaptively regulates information density for trajectories of varying lengths. Two self-supervised pretraining objectives, named contrastive alignment and masked language modeling, further strengthen multimodal consistency and contextual understanding. Extensive experiments on three real-world datasets demonstrate that MDTI consistently outperforms state-of-the-art baselines, confirming its robustness and strong generalization abilities. The code is publicly available at: this https URL</li>
</ul>

<h3>Title: ESS-Flow: Training-free guidance of flow-based models as inference in source space</h3>
<ul>
<li><strong>Authors: </strong>Adhithyan Kalaivanan, Zheng Zhao, Jens SjÃ¶lund, Fredrik Lindsten</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05849">https://arxiv.org/abs/2510.05849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05849">https://arxiv.org/pdf/2510.05849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05849]] ESS-Flow: Training-free guidance of flow-based models as inference in source space(https://arxiv.org/abs/2510.05849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Guiding pretrained flow-based generative models for conditional generation or to produce samples with desired target properties enables solving diverse tasks without retraining on paired data. We present ESS-Flow, a gradient-free method that leverages the typically Gaussian prior of the source distribution in flow-based models to perform Bayesian inference directly in the source space using Elliptical Slice Sampling. ESS-Flow only requires forward passes through the generative model and observation process, no gradient or Jacobian computations, and is applicable even when gradients are unreliable or unavailable, such as with simulation-based observations or quantization in the generation or observation process. We demonstrate its effectiveness on designing materials with desired target properties and predicting protein structures from sparse inter-residue distance measurements.</li>
</ul>

<h3>Title: DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization</h3>
<ul>
<li><strong>Authors: </strong>Xue-Yong Fu, Elena Khasanova, Md Tahmid Rahman Laskar, Harsh Saini, Shashi Bhushan TN</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05858">https://arxiv.org/abs/2510.05858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05858">https://arxiv.org/pdf/2510.05858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05858]] DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization(https://arxiv.org/abs/2510.05858)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive performance in text summarization, yet their performance often falls short when applied to specialized domains %or conversational data that differ from their original pre-training distribution. While fine-tuning can improve summarization quality, it typically relies on costly and scarce high-quality labeled data. In this work, we explore continual pre-training as a scalable, self-supervised approach to adapt LLMs for downstream summarization tasks, particularly in the context of noisy real-world conversation transcripts. We conduct extensive experiments using large-scale, unlabeled business conversation data to investigate whether continual pre-training enhances model capabilities in conversational summarization. Our results demonstrate that continual pre-training yields substantial gains in both in-domain and out-of-domain summarization benchmarks, while maintaining strong generalization and robustness. We also analyze the effects of data selection strategies, providing practical guidelines for applying continual pre-training in summarization-focused industrial applications.</li>
</ul>

<h3>Title: $\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Yanran Zhang, Bingyao Yu, Yu Zheng, Wenzhao Zheng, Yueqi Duan, Lei Chen, Jie Zhou, Jiwen Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05891">https://arxiv.org/abs/2510.05891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05891">https://arxiv.org/pdf/2510.05891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05891]] $\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection(https://arxiv.org/abs/2510.05891)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiesi Hu, Yanwu Yang, Zhiyu Ye, Jinyan Zhou, Jianfeng Cao, Hanyang Peng, Ting Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05899">https://arxiv.org/abs/2510.05899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05899">https://arxiv.org/pdf/2510.05899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05899]] Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning(https://arxiv.org/abs/2510.05899)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Universal models for medical image segmentation, such as interactive and in-context learning (ICL) models, offer strong generalization but require extensive annotations. Interactive models need repeated user prompts for each image, while ICL relies on dense, pixel-level labels. To address this, we propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that leverages weak prompts (e.g., bounding boxes or points) instead of dense labels for context. This approach significantly reduces annotation effort by eliminating the need for fine-grained masks and repeated user prompting for all images. We evaluated the proposed WS-ICL model on three held-out benchmarks. Experimental results demonstrate that WS-ICL achieves performance comparable to regular ICL models at a significantly lower annotation cost. In addition, WS-ICL is highly competitive even under the interactive paradigm. These findings establish WS-ICL as a promising step toward more efficient and unified universal models for medical image segmentation. Our code and model are publicly available at this https URL.</li>
</ul>

<h3>Title: PhishSSL: Self-Supervised Contrastive Learning for Phishing Website Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Li, Selvakumar Manickam, Yung-Wey Chong, Shankar Karuppayah, Priyadarsi Nanda, Binyong Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05900">https://arxiv.org/abs/2510.05900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05900">https://arxiv.org/pdf/2510.05900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05900]] PhishSSL: Self-Supervised Contrastive Learning for Phishing Website Detection(https://arxiv.org/abs/2510.05900)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Phishing websites remain a persistent cybersecurity threat by mimicking legitimate sites to steal sensitive user information. Existing machine learning-based detection methods often rely on supervised learning with labeled data, which not only incurs substantial annotation costs but also limits adaptability to novel attack patterns. To address these challenges, we propose PhishSSL, a self-supervised contrastive learning framework that eliminates the need for labeled phishing data during training. PhishSSL combines hybrid tabular augmentation with adaptive feature attention to produce semantically consistent views and emphasize discriminative attributes. We evaluate PhishSSL on three phishing datasets with distinct feature compositions. Across all datasets, PhishSSL consistently outperforms unsupervised and self-supervised baselines, while ablation studies confirm the contribution of each component. Moreover, PhishSSL maintains robust performance despite the diversity of feature sets, highlighting its strong generalization and transferability. These results demonstrate that PhishSSL offers a promising solution for phishing website detection, particularly effective against evolving threats in dynamic Web environments.</li>
</ul>

<h3>Title: Kaputt: A Large-Scale Dataset for Visual Defect Detection</h3>
<ul>
<li><strong>Authors: </strong>Sebastian HÃ¶fer, Dorian Henning, Artemij Amiranashvili, Douglas Morrison, Mariliza Tzes, Ingmar Posner, Marc Matvienko, Alessandro Rennola, Anton Milan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05903">https://arxiv.org/abs/2510.05903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05903">https://arxiv.org/pdf/2510.05903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05903]] Kaputt: A Large-Scale Dataset for Visual Defect Detection(https://arxiv.org/abs/2510.05903)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We present a novel large-scale dataset for defect detection in a logistics setting. Recent work on industrial anomaly detection has primarily focused on manufacturing scenarios with highly controlled poses and a limited number of object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC scores. In contrast to manufacturing, anomaly detection in retail logistics faces new challenges, particularly in the diversity and variability of object pose and appearance. Leading anomaly detection methods fall short when applied to this new setting. To bridge this gap, we introduce a new benchmark that overcomes the current limitations of existing datasets. With over 230,000 images (and more than 29,000 defective instances), it is 40 times larger than MVTec-AD and contains more than 48,000 distinct objects. To validate the difficulty of the problem, we conduct an extensive evaluation of multiple state-of-the-art anomaly detection methods, demonstrating that they do not surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that existing methods struggle to leverage normal samples under heavy pose and appearance variation. With our large-scale dataset, we set a new benchmark and encourage future research towards solving this challenging problem in retail logistics anomaly detection. The dataset is available for download under this https URL.</li>
</ul>

<h3>Title: An Attention-Augmented VAE-BiLSTM Framework for Anomaly Detection in 12-Lead ECG Signals</h3>
<ul>
<li><strong>Authors: </strong>Marc Garreta Basora (1), Mehmet Oguz Mulayim (2 and 1) ((1) Universitat AutÃ²noma de Barcelona (UAB), Cerdanyola del VallÃ¨s, Spain, (2) Artificial Intelligence Research Institute (IIIA-CSIC), Cerdanyola del VallÃ¨s, Spain)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05919">https://arxiv.org/abs/2510.05919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05919">https://arxiv.org/pdf/2510.05919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05919]] An Attention-Augmented VAE-BiLSTM Framework for Anomaly Detection in 12-Lead ECG Signals(https://arxiv.org/abs/2510.05919)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection in 12-lead electrocardiograms (ECGs) is critical for identifying deviations associated with cardiovascular disease. This work presents a comparative analysis of three autoencoder-based architectures: convolutional autoencoder (CAE), variational autoencoder with bidirectional long short-term memory (VAE-BiLSTM), and VAE-BiLSTM with multi-head attention (VAE-BiLSTM-MHA), for unsupervised anomaly detection in ECGs. To the best of our knowledge, this study reports the first application of a VAE-BiLSTM-MHA architecture to ECG anomaly detection. All models are trained on normal ECG samples to reconstruct non-anomalous cardiac morphology and detect deviations indicative of disease. Using a unified preprocessing and evaluation pipeline on the public China Physiological Signal Challenge (CPSC) dataset, the attention-augmented VAE achieves the best performance, with an AUPRC of 0.81 and a recall of 0.85 on the held-out test set, outperforming the other architectures. To support clinical triage, this model is further integrated into an interactive dashboard that visualizes anomaly localization. In addition, a performance comparison with baseline models from the literature is provided.</li>
</ul>

<h3>Title: CarrÃ© du champ flow matching: better quality-generalisation tradeoff in generative models</h3>
<ul>
<li><strong>Authors: </strong>Jacob Bamberger, Iolo Jones, Dennis Duncan, Michael M. Bronstein, Pierre Vandergheynst, Adam Gosztolai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05930">https://arxiv.org/abs/2510.05930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05930">https://arxiv.org/pdf/2510.05930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05930]] CarrÃ© du champ flow matching: better quality-generalisation tradeoff in generative models(https://arxiv.org/abs/2510.05930)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models often face a fundamental tradeoff: high sample quality can come at the cost of memorisation, where the model reproduces training data rather than generalising across the underlying data geometry. We introduce CarrÃ© du champ flow matching (CDC-FM), a generalisation of flow matching (FM), that improves the quality-generalisation tradeoff by regularising the probability path with a geometry-aware noise. Our method replaces the homogeneous, isotropic noise in FM with a spatially varying, anisotropic Gaussian noise whose covariance captures the local geometry of the latent data manifold. We prove that this geometric noise can be optimally estimated from the data and is scalable to large data. Further, we provide an extensive experimental evaluation on diverse datasets (synthetic manifolds, point clouds, single-cell genomics, animal motion capture, and images) as well as various neural network architectures (MLPs, CNNs, and transformers). We demonstrate that CDC-FM consistently offers a better quality-generalisation tradeoff. We observe significant improvements over standard FM in data-scarce regimes and in highly non-uniformly sampled datasets, which are often encountered in AI for science applications. Our work provides a mathematical framework for studying the interplay between data geometry, generalisation and memorisation in generative models, as well as a robust and scalable algorithm that can be readily integrated into existing flow matching pipelines.</li>
</ul>

<h3>Title: Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis</h3>
<ul>
<li><strong>Authors: </strong>Eashan Adhikarla, Yixin Liu, Brian D. Davison</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05976">https://arxiv.org/abs/2510.05976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05976">https://arxiv.org/pdf/2510.05976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05976]] Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis(https://arxiv.org/abs/2510.05976)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.</li>
</ul>

<h3>Title: Diffusion-Based Image Editing for Breaking Robust Watermarks</h3>
<ul>
<li><strong>Authors: </strong>Yunyi Ni, Finn Carter, Ze Niu, Emily Davis, Bo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05978">https://arxiv.org/abs/2510.05978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05978">https://arxiv.org/pdf/2510.05978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05978]] Diffusion-Based Image Editing for Breaking Robust Watermarks(https://arxiv.org/abs/2510.05978)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Robust invisible watermarking aims to embed hidden information into images such that the watermark can survive various image manipulations. However, the rise of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we present a theoretical study and method demonstrating that diffusion models can effectively break robust image watermarks that were designed to resist conventional perturbations. We show that a diffusion-driven ``image regeneration'' process can erase embedded watermarks while preserving perceptual image content. We further introduce a novel guided diffusion attack that explicitly targets the watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion-based transformation, the mutual information between the watermarked image and the embedded watermark payload vanishes, resulting in decoding failure. Experimentally, we evaluate our approach on multiple state-of-the-art watermarking schemes (including the deep learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings highlight a fundamental vulnerability in current robust watermarking techniques against generative model-based attacks, underscoring the need for new watermarking strategies in the era of generative AI.</li>
</ul>

<h3>Title: Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance</h3>
<ul>
<li><strong>Authors: </strong>Timothy Pistotti, Jason Brown, Michael Witbrock</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06018">https://arxiv.org/abs/2510.06018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06018">https://arxiv.org/pdf/2510.06018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06018]] Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance(https://arxiv.org/abs/2510.06018)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent studies employing Large Language Models (LLMs) to test the Argument from the Poverty of the Stimulus (APS) have yielded contrasting results across syntactic phenomena. This paper investigates the hypothesis that characteristics of the stimuli used in recent studies, including lexical ambiguities and structural complexities, may confound model performance. A methodology is proposed for re-evaluating LLM competence on syntactic prediction, focusing on GPT-2. This involves: 1) establishing a baseline on previously used (both filtered and unfiltered) stimuli, and 2) generating a new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5 Pro Preview) guided by linguistically-informed templates designed to mitigate identified confounds. Our preliminary findings indicate that GPT-2 demonstrates notably improved performance on these refined PG stimuli compared to baselines, suggesting that stimulus quality significantly influences outcomes in surprisal-based evaluations of LLM syntactic competency.</li>
</ul>

<h3>Title: RamPINN: Recovering Raman Spectra From Coherent Anti-Stokes Spectra Using Embedded Physics</h3>
<ul>
<li><strong>Authors: </strong>Sai Karthikeya Vemuri, Adithya Ashok Chalain Valapil, Tim BÃ¼chner, Joachim Denzler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06020">https://arxiv.org/abs/2510.06020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06020">https://arxiv.org/pdf/2510.06020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06020]] RamPINN: Recovering Raman Spectra From Coherent Anti-Stokes Spectra Using Embedded Physics(https://arxiv.org/abs/2510.06020)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Transferring the recent advancements in deep learning into scientific disciplines is hindered by the lack of the required large-scale datasets for training. We argue that in these knowledge-rich domains, the established body of scientific theory provides reliable inductive biases in the form of governing physical laws. We address the ill-posed inverse problem of recovering Raman spectra from noisy Coherent Anti-Stokes Raman Scattering (CARS) measurements, as the true Raman signal here is suppressed by a dominating non-resonant background. We propose RamPINN, a model that learns to recover Raman spectra from given CARS spectra. Our core methodological contribution is a physics-informed neural network that utilizes a dual-decoder architecture to disentangle resonant and non-resonant signals. This is done by enforcing the Kramers-Kronig causality relations via a differentiable Hilbert transform loss on the resonant and a smoothness prior on the non-resonant part of the signal. Trained entirely on synthetic data, RamPINN demonstrates strong zero-shot generalization to real-world experimental data, explicitly closing this gap and significantly outperforming existing baselines. Furthermore, we show that training with these physics-based losses alone, without access to any ground-truth Raman spectra, still yields competitive results. This work highlights a broader concept: formal scientific rules can act as a potent inductive bias, enabling robust, self-supervised learning in data-limited scientific domains.</li>
</ul>

<h3>Title: Edit-Based Flow Matching for Temporal Point Processes</h3>
<ul>
<li><strong>Authors: </strong>David LÃ¼dke, Marten Lienen, Marcel Kollovieh, Stephan GÃ¼nnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06050">https://arxiv.org/abs/2510.06050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06050">https://arxiv.org/pdf/2510.06050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06050]] Edit-Based Flow Matching for Temporal Point Processes(https://arxiv.org/abs/2510.06050)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Temporal point processes (TPPs) are a fundamental tool for modeling event sequences in continuous time, but most existing approaches rely on autoregressive parameterizations that are limited by their sequential sampling. Recent non-autoregressive, diffusion-style models mitigate these issues by jointly interpolating between noise and data through event insertions and deletions in a discrete Markov chain. In this work, we generalize this perspective and introduce an Edit Flow process for TPPs that transports noise to data via insert, delete, and substitute edit operations. By learning the instantaneous edit rates within a continuous-time Markov chain framework, we attain a flexible and efficient model that effectively reduces the total number of necessary edit operations during generation. Empirical results demonstrate the generative flexibility of our unconditionally trained model in a wide range of unconditional and conditional generation tasks on benchmark TPPs.</li>
</ul>

<h3>Title: Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability</h3>
<ul>
<li><strong>Authors: </strong>Taylor Sorensen, Benjamin Newman, Jared Moore, Chan Park, Jillian Fisher, Niloofar Mireshghallah, Liwei Jiang, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06084">https://arxiv.org/abs/2510.06084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06084">https://arxiv.org/pdf/2510.06084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06084]] Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability(https://arxiv.org/abs/2510.06084)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Language model post-training has enhanced instruction-following and performance on many downstream tasks, but also comes with an often-overlooked cost on tasks with many possible valid answers. We characterize three desiderata for conditional distributional modeling: in-context steerability, valid output space coverage, and distributional alignment, and document across three model families how current post-training can reduce these properties. In particular, we disambiguate between two kinds of in-context learning: ICL for eliciting existing underlying knowledge or capabilities, and in-context steerability, where a model must use in-context information to override its priors and steer to a novel data generating distribution. To better evaluate and improve these desiderata, we introduce Spectrum Suite, a large-scale resource compiled from >40 data sources and spanning >90 tasks requiring models to steer to and match diverse distributions ranging from varied human preferences to numerical distributions and more. We find that while current post-training techniques help elicit underlying capabilities and knowledge, they hurt models' ability to flexibly steer in-context. To mitigate these issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite to improve steerability and distributional coverage. We find that Spectrum Tuning often improves over pretrained models and their instruction-tuned counterparts, enhancing steerability, spanning more of the output space, and improving distributional alignment on held-out datasets.</li>
</ul>

<h3>Title: PolyGraph Discrepancy: a classifier-based metric for graph generation</h3>
<ul>
<li><strong>Authors: </strong>Markus Krimmel, Philip Hartout, Karsten Borgwardt, Dexiong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06122">https://arxiv.org/abs/2510.06122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06122">https://arxiv.org/pdf/2510.06122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06122]] PolyGraph Discrepancy: a classifier-based metric for graph generation(https://arxiv.org/abs/2510.06122)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing methods for evaluating graph generative models primarily rely on Maximum Mean Discrepancy (MMD) metrics based on graph descriptors. While these metrics can rank generative models, they do not provide an absolute measure of performance. Their values are also highly sensitive to extrinsic parameters, namely kernel and descriptor parametrization, making them incomparable across different graph descriptors. We introduce PolyGraph Discrepancy (PGD), a new evaluation framework that addresses these limitations. It approximates the Jensen-Shannon distance of graph distributions by fitting binary classifiers to distinguish between real and generated graphs, featurized by these descriptors. The data log-likelihood of these classifiers approximates a variational lower bound on the JS distance between the two distributions. Resulting metrics are constrained to the unit interval [0,1] and are comparable across different graph descriptors. We further derive a theoretically grounded summary metric that combines these individual metrics to provide a maximally tight lower bound on the distance for the given descriptors. Thorough experiments demonstrate that PGD provides a more robust and insightful evaluation compared to MMD metrics. The PolyGraph framework for benchmarking graph generative models is made publicly available at this https URL.</li>
</ul>

<h3>Title: Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework</h3>
<ul>
<li><strong>Authors: </strong>Mosong Ma, Tania Stathaki, Michalis Lazarou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06123">https://arxiv.org/abs/2510.06123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06123">https://arxiv.org/pdf/2510.06123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06123]] Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework(https://arxiv.org/abs/2510.06123)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning in medical imaging is often limited by scarce and imbalanced annotated data. We present SSGNet, a unified framework that combines class specific generative modeling with iterative semisupervised pseudo labeling to enhance both classification and segmentation. Rather than functioning as a standalone model, SSGNet augments existing baselines by expanding training data with StyleGAN3 generated images and refining labels through iterative pseudo labeling. Experiments across multiple medical imaging benchmarks demonstrate consistent gains in classification and segmentation performance, while Frechet Inception Distance analysis confirms the high quality of generated samples. These results highlight SSGNet as a practical strategy to mitigate annotation bottlenecks and improve robustness in medical image analysis.</li>
</ul>

<h3>Title: Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Mao, Yuhan Wang, Lifeng Chen, Can Zhao, Yucheng Tang, Dong Yang, Liangqiong Qu, Daguang Xu, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06131">https://arxiv.org/abs/2510.06131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06131">https://arxiv.org/pdf/2510.06131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06131]] Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation(https://arxiv.org/abs/2510.06131)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.</li>
</ul>

<h3>Title: CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits</h3>
<ul>
<li><strong>Authors: </strong>Kangyu Wang, Zhiyun Jiang, Haibo Feng, Weijia Zhao, Lin Liu, Jianguo Li, Zhenzhong Lan, Weiyao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06133">https://arxiv.org/abs/2510.06133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06133">https://arxiv.org/pdf/2510.06133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06133]] CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits(https://arxiv.org/abs/2510.06133)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) generate text through iterative denoising steps, achieving parallel decoding by denoising only high-confidence positions at each step. However, existing approaches often repetitively remask tokens due to initially low confidence scores, leading to redundant iterations and limiting overall acceleration. Through the analysis of dLLM decoding traces, we observe that the model often determines the final prediction for a token several steps before the decoding step. To leverage this historical information and avoid redundant steps, we introduce the concept of Trace Credit, which quantifies each token's convergence potential by accumulating historical logits. Furthermore, we propose CreditDecoding, a training-free parallel decoding algorithm that accelerates the confidence convergence of correct but underconfident tokens by fusing current logits with Trace Credit. This process significantly reduces redundant iterations and enhances decoding robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct. Importantly, CreditDecoding scales effectively to long sequences and is orthogonal to mainstream inference optimizations, making it a readily integrable and versatile solution.</li>
</ul>

<h3>Title: Deforming Videos to Masks: Flow Matching for Referring Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zanyi Wang, Dengyang Jiang, Liuzhuozheng Li, Sizhe Dang, Chengzu Li, Harry Yang, Guang Dai, Mengmeng Wang, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06139">https://arxiv.org/abs/2510.06139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06139">https://arxiv.org/pdf/2510.06139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06139]] Deforming Videos to Masks: Flow Matching for Referring Video Segmentation(https://arxiv.org/abs/2510.06139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a $\mathcal{J}\&\mathcal{F}$ of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.</li>
</ul>

<h3>Title: Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images</h3>
<ul>
<li><strong>Authors: </strong>Aditya Prakash, David Forsyth, Saurabh Gupta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06145">https://arxiv.org/abs/2510.06145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06145">https://arxiv.org/pdf/2510.06145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06145]] Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images(https://arxiv.org/abs/2510.06145)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We tackle the problem of forecasting bimanual 3D hand motion & articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) & forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images.</li>
</ul>

<h3>Title: TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts</h3>
<ul>
<li><strong>Authors: </strong>Christopher Kolberg, Katharina Eggensperger, Nico Pfeifer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06162">https://arxiv.org/abs/2510.06162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06162">https://arxiv.org/pdf/2510.06162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06162]] TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts(https://arxiv.org/abs/2510.06162)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Revealing novel insights from the relationship between molecular measurements and pathology remains a very impactful application of machine learning in biomedicine. Data in this domain typically contain only a few observations but thousands of potentially noisy features, posing challenges for conventional machine learning approaches. While prior-data fitted networks emerge as foundation models for tabular data, they are currently not suited to handle large feature counts (>500). Although feature reduction enables their application, it hinders feature importance analysis. We propose a strategy that extends existing models through continued pre-training on synthetic data sampled from a customized prior. The resulting model, TabPFN-Wide, matches or exceeds its base model's performance while exhibiting improved robustness to noise. It seamlessly scales beyond 50,000 features, regardless of noise levels, while maintaining inherent interpretability, which is critical for biomedical applications. Our results show that prior-informed adaptation is suitable to enhance the capability of foundation models for high-dimensional data. On real-world biomedical datasets many of the most relevant features identified by the model overlap with previous biological findings, while others propose potential starting points for future studies.</li>
</ul>

<h3>Title: Thermodynamic Performance Limits for Score-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Nathan X. Kodama, Michael Hinczewski</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06174">https://arxiv.org/abs/2510.06174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06174">https://arxiv.org/pdf/2510.06174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06174]] Thermodynamic Performance Limits for Score-Based Diffusion Models(https://arxiv.org/abs/2510.06174)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We establish a fundamental connection between score-based diffusion models and non-equilibrium thermodynamics by deriving performance limits based on entropy rates. Our main theoretical contribution is a lower bound on the negative log-likelihood of the data that relates model performance to entropy rates of diffusion processes. We numerically validate this bound on a synthetic dataset and investigate its tightness. By building a bridge to entropy rates - system, intrinsic, and exchange entropy - we provide new insights into the thermodynamic operation of these models, drawing parallels to Maxwell's demon and implications for thermodynamic computing hardware. Our framework connects generative modeling performance to fundamental physical principles through stochastic thermodynamics.</li>
</ul>

<h3>Title: Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context</h3>
<ul>
<li><strong>Authors: </strong>Yoav Gur-Arieh, Mor Geva, Atticus Geiger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06182">https://arxiv.org/abs/2510.06182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06182">https://arxiv.org/pdf/2510.06182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06182]] Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context(https://arxiv.org/abs/2510.06182)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval. For example, an LM might represent "Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann" when asked "Who loves pie?" Prior research on short lists of bound entities found strong evidence that LMs implement such retrieval via a positional mechanism, where "Ann" is retrieved based on its position in context. In this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions. To compensate for this, we find that LMs supplement the positional mechanism with a lexical mechanism (retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism (retrieving "Ann" through a direct pointer). Through extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior. We leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement. Finally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings. Overall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.</li>
</ul>

<h3>Title: On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Chenxiao Yang, Cai Zhou, David Wipf, Zhiyuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06190">https://arxiv.org/abs/2510.06190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06190">https://arxiv.org/pdf/2510.06190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06190]] On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond(https://arxiv.org/abs/2510.06190)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper formally studies generation processes, including auto-regressive next-token prediction and masked diffusion, that abstract beyond architectural specifics. At this level of abstraction, we quantify their benefits and limitations through measurable criteria such as computational hardness and learnability. In particular, we demonstrate that allowing generation to proceed beyond autoregression and current masked diffusion, with capabilities to rewrite and length-variable edit, can bring significant theoretical and empirical advantages, with important implications for frontier LLMs that aspire to tackle increasingly hard problems and work universally across domains beyond natural language, such as coding and science.</li>
</ul>

<h3>Title: Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Zhenpei Yang, Yijing Bai, Yingwei Li, Yuliang Zou, Bo Sun, Abhijit Kundu, Jose Lezama, Luna Yue Huang, Zehao Zhu, Jyh-Jing Hwang, Dragomir Anguelov, Mingxing Tan, Chiyu Max Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06209">https://arxiv.org/abs/2510.06209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06209">https://arxiv.org/pdf/2510.06209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06209]] Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models(https://arxiv.org/abs/2510.06209)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.</li>
</ul>

<h3>Title: Fine-grained Defocus Blur Control for Generative Image Models</h3>
<ul>
<li><strong>Authors: </strong>Ayush Shrivastava, Connelly Barnes, Xuaner Zhang, Lingzhi Zhang, Andrew Owens, Sohrab Amirghodsi, Eli Shechtman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.06215">https://arxiv.org/abs/2510.06215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.06215">https://arxiv.org/pdf/2510.06215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.06215]] Fine-grained Defocus Blur Control for Generative Image Models(https://arxiv.org/abs/2510.06215)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current text-to-image diffusion models excel at generating diverse, high-quality images, yet they struggle to incorporate fine-grained camera metadata such as precise aperture settings. In this work, we introduce a novel text-to-image diffusion framework that leverages camera metadata, or EXIF data, which is often embedded in image files, with an emphasis on generating controllable lens blur. Our method mimics the physical image formation process by first generating an all-in-focus image, estimating its monocular depth, predicting a plausible focus distance with a novel focus distance transformer, and then forming a defocused image with an existing differentiable lens blur model. Gradients flow backwards through this whole process, allowing us to learn without explicit supervision to generate defocus effects based on content elements and the provided EXIF data. At inference time, this enables precise interactive user control over defocus effects while preserving scene contents, which is not achievable with existing diffusion models. Experimental results demonstrate that our model enables superior fine-grained control without altering the depicted scene.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
