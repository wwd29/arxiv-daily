<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-11</h1>
<h3>Title: QASE Enhanced PLMs: Improved Control in Text Generation for MRC</h3>
<ul>
<li><strong>Authors: </strong>Lin Ai, Zheng Hui, Zizhou Liu, Julia Hirschberg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04771">https://arxiv.org/abs/2403.04771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04771">https://arxiv.org/pdf/2403.04771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04771]] QASE Enhanced PLMs: Improved Control in Text Generation for MRC(https://arxiv.org/abs/2403.04771)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To address the challenges of out-of-control generation in generative models for machine reading comprehension (MRC), we introduce the Question-Attended Span Extraction (QASE) module. Integrated during the fine-tuning of pre-trained generative language models (PLMs), QASE enables these PLMs to match SOTA extractive methods and outperform leading LLMs like GPT-4 in MRC tasks, without significant increases in computational costs.</li>
</ul>

<h3>Title: MuseGraph: Graph-oriented Instruction Tuning of Large Language Models  for Generic Graph Mining</h3>
<ul>
<li><strong>Authors: </strong>Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, Carl Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04780">https://arxiv.org/abs/2403.04780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04780">https://arxiv.org/pdf/2403.04780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04780]] MuseGraph: Graph-oriented Instruction Tuning of Large Language Models  for Generic Graph Mining(https://arxiv.org/abs/2403.04780)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graphs with abundant attributes are essential in modeling interconnected entities and improving predictions in various real-world applications. Traditional Graph Neural Networks (GNNs), which are commonly used for modeling attributed graphs, need to be re-trained every time when applied to different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored. To this end, we propose a novel framework MuseGraph, which seamlessly integrates the strengths of GNNs and LLMs and facilitates a more effective and generic approach for graph mining across different tasks and datasets. Specifically, we first introduce a compact graph description via the proposed adaptive input generation to encapsulate key information from the graph under the constraints of language token limitations. Then, we propose a diverse instruction generation mechanism, which distills the reasoning capabilities from LLMs (e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction packages for different graph tasks. Finally, we propose a graph-aware instruction tuning with a dynamic instruction package allocation strategy across tasks and datasets, ensuring the effectiveness and generalization of the training process. Our experimental results demonstrate significant improvements in different graph tasks, showcasing the potential of our MuseGraph in enhancing the accuracy of graph-oriented downstream tasks while keeping the generation powers of LLMs.</li>
</ul>

<h3>Title: TopicDiff: A Topic-enriched Diffusion Approach for Multimodal  Conversational Emotion Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiamin Luo, Jingjing Wang, Guodong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04789">https://arxiv.org/abs/2403.04789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04789">https://arxiv.org/pdf/2403.04789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04789]] TopicDiff: A Topic-enriched Diffusion Approach for Multimodal  Conversational Emotion Detection(https://arxiv.org/abs/2403.04789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multimodal Conversational Emotion (MCE) detection, generally spanning across the acoustic, vision and language modalities, has attracted increasing interest in the multimedia community. Previous studies predominantly focus on learning contextual information in conversations with only a few considering the topic information in single language modality, while always neglecting the acoustic and vision topic information. On this basis, we propose a model-agnostic Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic information in MCE tasks. Particularly, we integrate the diffusion model into neural topic model to alleviate the diversity deficiency problem of neural topic model in capturing topic information. Detailed evaluations demonstrate the significant improvements of TopicDiff over the state-of-the-art MCE baselines, justifying the importance of multimodal topic information to MCE and the effectiveness of TopicDiff in capturing such information. Furthermore, we observe an interesting finding that the topic information in acoustic and vision is more discriminative and robust compared to the language.</li>
</ul>

<h3>Title: Breaking the Language Barrier: Can Direct Inference Outperform  Pre-Translation in Multilingual LLM Applications?</h3>
<ul>
<li><strong>Authors: </strong>Yotam Intrator, Matan Halfon, Roman Goldenberg, Reut Tsarfaty, Matan Eyal, Ehud Rivlin, Yossi Matias, Natalia Aizenberg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04792">https://arxiv.org/abs/2403.04792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04792">https://arxiv.org/pdf/2403.04792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04792]] Breaking the Language Barrier: Can Direct Inference Outperform  Pre-Translation in Multilingual LLM Applications?(https://arxiv.org/abs/2403.04792)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models hold significant promise in multilingual applications. However, inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss. This study re-evaluates the need for pre-translation in the context of PaLM2 models (Anil et al., 2023), which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse benchmarks, including open-end generative tasks, which were excluded from previous similar studies. Our findings challenge the pre-translation paradigm established in prior research, highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages. These findings pave the way for more efficient and effective multilingual applications, alleviating the limitations associated with pre-translation and unlocking linguistic authenticity.</li>
</ul>

<h3>Title: JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using  in-context learning with GPT and instruction-tuned Llama models</h3>
<ul>
<li><strong>Authors: </strong>Arefa, Mohammed Abbas Ansari, Chandni Saxena, Tanvir Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04798">https://arxiv.org/abs/2403.04798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04798">https://arxiv.org/pdf/2403.04798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04798]] JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using  in-context learning with GPT and instruction-tuned Llama models(https://arxiv.org/abs/2403.04798)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This paper presents our system development for SemEval-2024 Task 3: "The Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively capturing emotions in human conversations requires integrating multiple modalities such as text, audio, and video. However, the complexities of these diverse modalities pose challenges for developing an efficient multimodal emotion cause analysis (ECA) system. Our proposed approach addresses these challenges by a two-step framework. We adopt two different approaches in our implementation. In Approach 1, we employ instruction-tuning with two separate Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V for conversation-level video description and employ in-context learning with annotated conversation using GPT 3.5. Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains. All the experimental codes are available on Github.</li>
</ul>

<h3>Title: Enhancing Security in Federated Learning through Adaptive  Consensus-Based Model Update Validation</h3>
<ul>
<li><strong>Authors: </strong>Zahir Alsulaimawi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04803">https://arxiv.org/abs/2403.04803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04803">https://arxiv.org/pdf/2403.04803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04803]] Enhancing Security in Federated Learning through Adaptive  Consensus-Based Model Update Validation(https://arxiv.org/abs/2403.04803)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This paper introduces an advanced approach for fortifying Federated Learning (FL) systems against label-flipping attacks. We propose a simplified consensus-based verification process integrated with an adaptive thresholding mechanism. This dynamic thresholding is designed to adjust based on the evolving landscape of model updates, offering a refined layer of anomaly detection that aligns with the real-time needs of distributed learning environments. Our method necessitates a majority consensus among participating clients to validate updates, ensuring that only vetted and consensual modifications are applied to the global model. The efficacy of our approach is validated through experiments on two benchmark datasets in deep learning, CIFAR-10 and MNIST. Our results indicate a significant mitigation of label-flipping attacks, bolstering the FL system's resilience. This method transcends conventional techniques that depend on anomaly detection or statistical validation by incorporating a verification layer reminiscent of blockchain's participatory validation without the associated cryptographic overhead. The innovation of our approach rests in striking an optimal balance between heightened security measures and the inherent limitations of FL systems, such as computational efficiency and data privacy. Implementing a consensus mechanism specifically tailored for FL environments paves the way for more secure, robust, and trustworthy distributed machine learning applications, where safeguarding data integrity and model robustness is critical.</li>
</ul>

<h3>Title: UniTable: Towards a Unified Framework for Table Structure Recognition  via Self-Supervised Pretraining</h3>
<ul>
<li><strong>Authors: </strong>ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari Balasubramaniyan, Duen Horng Chau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04822">https://arxiv.org/abs/2403.04822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04822">https://arxiv.org/pdf/2403.04822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04822]] UniTable: Towards a Unified Framework for Table Structure Recognition  via Self-Supervised Pretraining(https://arxiv.org/abs/2403.04822)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Tables convey factual and quantitative data with implicit conventions created by humans that are often challenging for machines to parse. Prior work on table structure recognition (TSR) has mainly centered around complex task-specific combinations of available inputs and tools. We present UniTable, a training framework that unifies both the training paradigm and training objective of TSR. Its training paradigm combines the simplicity of purely pixel-level inputs with the effectiveness and scalability empowered by self-supervised pretraining (SSP) from diverse unannotated tabular images. Our framework unifies the training objectives of all three TSR tasks - extracting table structure, cell content, and cell bounding box (bbox) - into a unified task-agnostic training objective: language modeling. Extensive quantitative and qualitative analyses highlight UniTable's state-of-the-art (SOTA) performance on four of the largest TSR datasets. To promote reproducible research, enhance transparency, and SOTA innovations, we open-source our code at https://github.com/poloclub/unitable and release the first-of-its-kind Jupyter Notebook of the whole inference pipeline, fine-tuned across multiple TSR datasets, supporting all three TSR tasks.</li>
</ul>

<h3>Title: An Item is Worth a Prompt: Versatile Image Editing with Disentangled  Control</h3>
<ul>
<li><strong>Authors: </strong>Aosong Feng, Weikang Qiu, Jinbin Bai, Kaicheng Zhou, Zhen Dong, Xiao Zhang, Rex Ying, Leandros Tassiulas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04880">https://arxiv.org/abs/2403.04880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04880">https://arxiv.org/pdf/2403.04880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04880]] An Item is Worth a Prompt: Versatile Image Editing with Disentangled  Control(https://arxiv.org/abs/2403.04880)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Building on the success of text-to-image diffusion models (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the prompt space gains more attention due to its capacity and simplicity of controlling semantics. However, since diffusion models are commonly pretrained on descriptive text captions, direct editing of words in text prompts usually leads to completely different generated images, violating the requirements for image editing. On the other hand, existing editing methods usually consider introducing spatial masks to preserve the identity of unedited regions, which are usually ignored by DPMs and therefore lead to inharmonic editing results. Targeting these two challenges, in this work, we propose to disentangle the comprehensive image-prompt interaction into several item-prompt interactions, with each item linked to a special learned prompt. The resulting framework, named D-Edit, is based on pretrained diffusion models with cross-attention layers disentangled and adopts a two-step optimization to build item-prompt associations. Versatile image editing can then be applied to specific items by manipulating the corresponding prompts. We demonstrate state-of-the-art results in four types of editing operations including image-based, text-based, mask-based editing, and item removal, covering most types of editing applications, all within a single unified framework. Notably, D-Edit is the first framework that can (1) achieve item editing through mask editing and (2) combine image and text-based editing. We demonstrate the quality and versatility of the editing results for a diverse collection of images through both qualitative and quantitative evaluations.</li>
</ul>

<h3>Title: Divide and Conquer: High-Resolution Industrial Anomaly Detection via  Memory Efficient Tiled Ensemble</h3>
<ul>
<li><strong>Authors: </strong>Blaž Rolih, Samet Akçay, Dick Ameln, Ashwin Vaidya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04932">https://arxiv.org/abs/2403.04932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04932">https://arxiv.org/pdf/2403.04932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04932]] Divide and Conquer: High-Resolution Industrial Anomaly Detection via  Memory Efficient Tiled Ensemble(https://arxiv.org/abs/2403.04932)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Industrial anomaly detection is an important task within computer vision with a wide range of practical use cases. The small size of anomalous regions in many real-world datasets necessitates processing the images at a high resolution. This frequently poses significant challenges concerning memory consumption during the model training and inference stages, leaving some existing methods impractical for widespread adoption. To overcome this challenge, we present the tiled ensemble approach, which reduces memory consumption by dividing the input images into a grid of tiles and training a dedicated model for each tile location. The tiled ensemble is compatible with any existing anomaly detection model without the need for any modification of the underlying architecture. By introducing overlapping tiles, we utilize the benefits of traditional stacking ensembles, leading to further improvements in anomaly detection capabilities beyond high resolution alone. We perform a comprehensive analysis using diverse underlying architectures, including Padim, PatchCore, FastFlow, and Reverse Distillation, on two standard anomaly detection datasets: MVTec and VisA. Our method demonstrates a notable improvement across setups while remaining within GPU memory constraints, consuming only as much GPU memory as a single model needs to process a single tile.</li>
</ul>

<h3>Title: AFreeCA: Annotation-Free Counting for All</h3>
<ul>
<li><strong>Authors: </strong>Adriano D'Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04943">https://arxiv.org/abs/2403.04943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04943">https://arxiv.org/pdf/2403.04943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04943]] AFreeCA: Annotation-Free Counting for All(https://arxiv.org/abs/2403.04943)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Object counting methods typically rely on manually annotated datasets. The cost of creating such datasets has restricted the versatility of these networks to count objects from specific classes (such as humans or penguins), and counting objects from diverse categories remains a challenge. The availability of robust text-to-image latent diffusion models (LDMs) raises the question of whether these models can be utilized to generate counting datasets. However, LDMs struggle to create images with an exact number of objects based solely on text prompts but they can be used to offer a dependable \textit{sorting} signal by adding and removing objects within an image. Leveraging this data, we initially introduce an unsupervised sorting methodology to learn object-related features that are subsequently refined and anchored for counting purposes using counting data generated by LDMs. Further, we present a density classifier-guided method for dividing an image into patches containing objects that can be reliably counted. Consequently, we can generate counting data for any type of object and count them in an unsupervised manner. Our approach outperforms other unsupervised and few-shot alternatives and is not restricted to specific object classes for which counting data is available. Code to be released upon acceptance.</li>
</ul>

<h3>Title: StereoDiffusion: Training-Free Stereo Image Generation Using Latent  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, Siavash Arjomand Bigdeli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04965">https://arxiv.org/abs/2403.04965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04965">https://arxiv.org/pdf/2403.04965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04965]] StereoDiffusion: Training-Free Stereo Image Generation Using Latent  Diffusion Models(https://arxiv.org/abs/2403.04965)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The demand for stereo images increases as manufacturers launch more XR devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is trainning free, remarkably straightforward to use, and it seamlessly integrates into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight capability for fast generation of stereo image pairs, without the need for fine-tuning model weights or any post-processing of images. Using the original input to generate a left image and estimate a disparity map for it, we generate the latent vector for the right image through Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking Denoise and Self-Attention Layers Modification methods to align the right-side image with the left-side image. Moreover, our proposed method maintains a high standard of image quality throughout the stereo generation process, achieving state-of-the-art scores in various quantitative evaluations.</li>
</ul>

<h3>Title: PIPsUS: Self-Supervised Dense Point Tracking in Ultrasound</h3>
<ul>
<li><strong>Authors: </strong>Wanwen Chen, Adam Schmidt, Eitan Prisman, Septimiu E Salcudean</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04969">https://arxiv.org/abs/2403.04969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04969">https://arxiv.org/pdf/2403.04969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04969]] PIPsUS: Self-Supervised Dense Point Tracking in Ultrasound(https://arxiv.org/abs/2403.04969)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Finding point-level correspondences is a fundamental problem in ultrasound (US), since it can enable US landmark tracking for intraoperative image guidance in different surgeries, including head and neck. Most existing US tracking methods, e.g., those based on optical flow or feature matching, were initially designed for RGB images before being applied to US. Therefore domain shift can impact their performance. Training could be supervised by ground-truth correspondences, but these are expensive to acquire in US. To solve these problems, we propose a self-supervised pixel-level tracking model called PIPsUS. Our model can track an arbitrary number of points in one forward pass and exploits temporal information by considering multiple, instead of just consecutive, frames. We developed a new self-supervised training strategy that utilizes a long-term point-tracking model trained for RGB images as a teacher to guide the model to learn realistic motions and use data augmentation to enforce tracking from US appearance. We evaluate our method on neck and oral US and echocardiography, showing higher point tracking accuracy when compared with fast normalized cross-correlation and tuned optical flow. Code will be available once the paper is accepted.</li>
</ul>

<h3>Title: DiffChat: Learning to Chat with Text-to-Image Synthesis Models for  Interactive Image Creation</h3>
<ul>
<li><strong>Authors: </strong>Jiapeng Wang, Chengyu Wang, Tingfeng Cao, Jun Huang, Lianwen Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.04997">https://arxiv.org/abs/2403.04997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.04997">https://arxiv.org/pdf/2403.04997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.04997]] DiffChat: Learning to Chat with Text-to-Image Synthesis Models for  Interactive Image Creation(https://arxiv.org/abs/2403.04997)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present DiffChat, a novel method to align Large Language Models (LLMs) to "chat" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable Diffusion) for interactive image creation. Given a raw prompt/image and a user-specified instruction, DiffChat can effectively make appropriate modifications and generate the target prompt, which can be leveraged to create the target image of high quality. To achieve this, we first collect an instruction-following prompt engineering dataset named InstructPE for the supervised training of DiffChat. Next, we propose a reinforcement learning framework with the feedback of three core criteria for image creation, i.e., aesthetics, user preference, and content integrity. It involves an action-space dynamic modification technique to obtain more relevant positive samples and harder negative samples during the off-policy sampling. Content integrity is also introduced into the value estimation function for further improvement of produced images. Our method can exhibit superior performance than baseline models and strong competitors based on both automatic and human evaluations, which fully demonstrates its effectiveness.</li>
</ul>

<h3>Title: Can't Remember Details in Long Documents? You Need Some R&R</h3>
<ul>
<li><strong>Authors: </strong>Devanshu Agrawal, Shang Gao, Martin Gajek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05004">https://arxiv.org/abs/2403.05004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05004">https://arxiv.org/pdf/2403.05004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05004]] Can't Remember Details in Long Documents? You Need Some R&R(https://arxiv.org/abs/2403.05004)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Long-context large language models (LLMs) hold promise for tasks such as question-answering (QA) over long documents, but they tend to miss important information in the middle of context documents (arXiv:2307.03172v3). Here, we introduce $\textit{R&R}$ -- a combination of two novel prompt-based methods called $\textit{reprompting}$ and $\textit{in-context retrieval}$ (ICR) -- to alleviate this effect in document-based QA. In reprompting, we repeat the prompt instructions periodically throughout the context document to remind the LLM of its original task. In ICR, rather than instructing the LLM to answer the question directly, we instruct it to retrieve the top $k$ passage numbers most relevant to the given question, which are then used as an abbreviated context in a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents up to 80k tokens in length and observe a 16-point boost in QA accuracy on average. Our further analysis suggests that R&R improves performance on long document-based QA because it reduces the distance between relevant context and the instructions. Finally, we show that compared to short-context chunkwise methods, R&R enables the use of larger chunks that cost fewer LLM calls and output tokens, while minimizing the drop in accuracy.</li>
</ul>

<h3>Title: DiffClass: Diffusion-Based Class Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Zichong Meng, Jie Zhang, Changdi Yang, Zheng Zhan, Pu Zhao, Yanzhi WAng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05016">https://arxiv.org/abs/2403.05016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05016">https://arxiv.org/pdf/2403.05016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05016]] DiffClass: Diffusion-Based Class Incremental Learning(https://arxiv.org/abs/2403.05016)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Class Incremental Learning (CIL) is challenging due to catastrophic forgetting. On top of that, Exemplar-free Class Incremental Learning is even more challenging due to forbidden access to previous task data. Recent exemplar-free CIL methods attempt to mitigate catastrophic forgetting by synthesizing previous task data. However, they fail to overcome the catastrophic forgetting due to the inability to deal with the significant domain gap between real and synthetic data. To overcome these issues, we propose a novel exemplar-free CIL method. Our method adopts multi-distribution matching (MDM) diffusion models to unify quality and bridge domain gaps among all domains of training data. Moreover, our approach integrates selective synthetic image augmentation (SSIA) to expand the distribution of the training data, thereby improving the model's plasticity and reinforcing the performance of our method's ultimate component, multi-domain adaptation (MDA). With the proposed integrations, our method then reformulates exemplar-free CIL into a multi-domain adaptation problem to implicitly address the domain gap problem to enhance model stability during incremental training. Extensive experiments on benchmark class incremental datasets and settings demonstrate that our method excels previous exemplar-free CIL methods and achieves state-of-the-art performance.</li>
</ul>

<h3>Title: InstructGIE: Towards Generalizable Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Zichong Meng, Changdi Yang, Jun Liu, Hao Tang, Pu Zhao, Yanzhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05018">https://arxiv.org/abs/2403.05018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05018">https://arxiv.org/pdf/2403.05018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05018]] InstructGIE: Towards Generalizable Image Editing(https://arxiv.org/abs/2403.05018)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>Recent advances in image editing have been driven by the development of denoising diffusion models, marking a significant leap forward in this field. Despite these advances, the generalization capabilities of recent image editing approaches remain constrained. In response to this challenge, our study introduces a novel image editing framework with enhanced generalization robustness by boosting in-context learning capability and unifying language instruction. This framework incorporates a module specifically optimized for image editing tasks, leveraging the VMamba Block and an editing-shift matching strategy to augment in-context learning. Furthermore, we unveil a selective area-matching technique specifically engineered to address and rectify corrupted details in generated images, such as human facial features, to further improve the quality. Another key innovation of our approach is the integration of a language unification technique, which aligns language embeddings with editing semantics to elevate the quality of image editing. Moreover, we compile the first dataset for image editing with visual prompts and editing instructions that could be used to enhance in-context capability. Trained on this dataset, our methodology not only achieves superior synthesis quality for trained tasks, but also demonstrates robust generalization capability across unseen vision tasks through tailored prompts.</li>
</ul>

<h3>Title: Quantifying Manifolds: Do the manifolds learned by Generative  Adversarial Networks converge to the real data manifold</h3>
<ul>
<li><strong>Authors: </strong>Anupam Chaudhuri, Anj Simmons, Mohamed Abdelrazek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05033">https://arxiv.org/abs/2403.05033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05033">https://arxiv.org/pdf/2403.05033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05033]] Quantifying Manifolds: Do the manifolds learned by Generative  Adversarial Networks converge to the real data manifold(https://arxiv.org/abs/2403.05033)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents our experiments to quantify the manifolds learned by ML models (in our experiment, we use a GAN model) as they train. We compare the manifolds learned at each epoch to the real manifolds representing the real data. To quantify a manifold, we study the intrinsic dimensions and topological features of the manifold learned by the ML model, how these metrics change as we continue to train the model, and whether these metrics convergence over the course of training to the metrics of the real data manifold.</li>
</ul>

<h3>Title: CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction  Model</h3>
<ul>
<li><strong>Authors: </strong>Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05034">https://arxiv.org/abs/2403.05034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05034">https://arxiv.org/pdf/2403.05034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05034]] CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction  Model(https://arxiv.org/abs/2403.05034)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Feed-forward 3D generative models like the Large Reconstruction Model (LRM) have demonstrated exceptional generation speed. However, the transformer-based methods do not leverage the geometric priors of the triplane component in their architecture, often leading to sub-optimal quality given the limited size of 3D data and slow training. In this work, we present the Convolutional Reconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D generative model. Recognizing the limitations posed by sparse 3D data, we highlight the necessity of integrating geometric priors into network design. CRM builds on the key observation that the visualization of triplane exhibits spatial correspondence of six orthographic images. First, it generates six orthographic view images from a single input image, then feeds these images into a convolutional U-Net, leveraging its strong pixel-level alignment capabilities and significant bandwidth to create a high-resolution triplane. CRM further employs Flexicubes as geometric representation, facilitating direct end-to-end optimization on textured meshes. Overall, our model delivers a high-fidelity textured mesh from an image in just 10 seconds, without any test-time optimization.</li>
</ul>

<h3>Title: REPS: Reconstruction-based Point Cloud Sampling</h3>
<ul>
<li><strong>Authors: </strong>Guoqing Zhang, Wenbo Zhao, Jian Liu, Xianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05047">https://arxiv.org/abs/2403.05047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05047">https://arxiv.org/pdf/2403.05047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05047]] REPS: Reconstruction-based Point Cloud Sampling(https://arxiv.org/abs/2403.05047)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sampling is widely used in various point cloud tasks as it can effectively reduce resource consumption. Recently, some methods have proposed utilizing neural networks to optimize the sampling process for various task requirements. Currently, deep downsampling methods can be categorized into two main types: generative-based and score-based. Generative-based methods directly generate sampled point clouds using networks, whereas score-based methods assess the importance of points according to specific rules and then select sampled point clouds based on their scores. However, these methods often result in noticeable clustering effects in high-intensity feature areas, compromising their ability to preserve small-scale features and leading to the loss of some structures, thereby affecting the performance of subsequent tasks. In this paper, we propose REPS, a reconstruction-based scoring strategy that evaluates the importance of each vertex by removing and reconstructing them using surrounding vertices. Our reconstruction process comprises point reconstruction and shape reconstruction. The two aforementioned reconstruction methods effectively evaluate the importance of vertices by removing them at different scales for reconstruction. These reconstructions ensure that our method maintains the overall geometric features of the point cloud and avoids disturbing small-scale structures during sampling. Additionally, we propose the Global-Local Fusion Attention (GLFA) module, which aggregates local and global attention features of point clouds, ensuring high-quality reconstruction and sampling effects. Our method outperforms previous approaches in preserving the structural features of the sampled point clouds. Furthermore, abundant experimental results demonstrate the superior performance of our method across various common tasks.</li>
</ul>

<h3>Title: XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Qu, Kun Yuan, Kai Zhao, Qizhi Xie, Jinhua Hao, Ming Sun, Chao Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05049">https://arxiv.org/abs/2403.05049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05049">https://arxiv.org/pdf/2403.05049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05049]] XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution(https://arxiv.org/abs/2403.05049)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based methods, endowed with a formidable generative prior, have received increasing attention in Image Super-Resolution (ISR) recently. However, as low-resolution (LR) images often undergo severe degradation, it is challenging for ISR models to perceive the semantic and degradation information, resulting in restoration images with incorrect content or unrealistic artifacts. To address these issues, we propose a \textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR, to acquire precise and comprehensive semantic conditions for the diffusion model, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. To facilitate better fusion of cross-modal priors, a \textit{Semantic-Fusion Attention} is raised. To distill semantic-preserved information instead of undesired degradations, a \textit{Degradation-Free Constraint} is attached between LR and its high-resolution (HR) counterpart. Quantitative and qualitative results show that XPSR is capable of generating high-fidelity and high-realism images across synthetic and real-world datasets. Codes will be released at \url{https://github.com/qyp2000/XPSR}.</li>
</ul>

<h3>Title: PrimeComposer: Faster Progressively Combined Diffusion for Image  Composition with Attention Steering</h3>
<ul>
<li><strong>Authors: </strong>Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05053">https://arxiv.org/abs/2403.05053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05053">https://arxiv.org/pdf/2403.05053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05053]] PrimeComposer: Faster Progressively Combined Diffusion for Image  Composition with Attention Steering(https://arxiv.org/abs/2403.05053)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image composition involves seamlessly integrating given objects into a specific visual context. The current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion in synthesis and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only slows down inference but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster training-free diffuser that composites the images by well-designed attention steering across different noise levels. This steering is predominantly achieved by our Correlation Diffuser, utilizing its self-attention layers at each step. Within these layers, the synthesized subject interacts with both the referenced object and background, capturing intricate details and coherent relationships. This prior information is encoded into the attention weights, which are then integrated into the self-attention layers of the generator to guide the synthesis process. Besides, we introduce a Region-constrained Cross-Attention to confine the impact of specific subject-related words to desired regions, addressing the unwanted artifacts shown in the prior method thereby further improving the coherence in the transition area. Our method exhibits the fastest inference efficiency and extensive experiments demonstrate our superiority both qualitatively and quantitatively.</li>
</ul>

<h3>Title: Stealing Stable Diffusion Prior for Robust Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yifan Mao, Jian Liu, Xianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05056">https://arxiv.org/abs/2403.05056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05056">https://arxiv.org/pdf/2403.05056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05056]] Stealing Stable Diffusion Prior for Robust Monocular Depth Estimation(https://arxiv.org/abs/2403.05056)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation is a crucial task in computer vision. While existing methods have shown impressive results under standard conditions, they often face challenges in reliably performing in scenarios such as low-light or rainy conditions due to the absence of diverse training data. This paper introduces a novel approach named Stealing Stable Diffusion (SSD) prior for robust monocular depth estimation. The approach addresses this limitation by utilizing stable diffusion to generate synthetic images that mimic challenging conditions. Additionally, a self-training mechanism is introduced to enhance the model's depth estimation capability in such challenging environments. To enhance the utilization of the stable diffusion prior further, the DINOv2 encoder is integrated into the depth model architecture, enabling the model to leverage rich semantic priors and improve its scene understanding. Furthermore, a teacher loss is introduced to guide the student models in acquiring meaningful knowledge independently, thus reducing their dependency on the teacher models. The effectiveness of the approach is evaluated on nuScenes and Oxford RobotCar, two challenging public datasets, with the results showing the efficacy of the method. Source code and weights are available at: https://github.com/hitcslj/SSD.</li>
</ul>

<h3>Title: Unsupervised Graph Neural Architecture Search with Disentangled  Self-supervision</h3>
<ul>
<li><strong>Authors: </strong>Zeyang Zhang, Xin Wang, Ziwei Zhang, Guangyao Shen, Shiqi Shen, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05064">https://arxiv.org/abs/2403.05064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05064">https://arxiv.org/pdf/2403.05064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05064]] Unsupervised Graph Neural Architecture Search with Disentangled  Self-supervision(https://arxiv.org/abs/2403.05064)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The existing graph neural architecture search (GNAS) methods heavily rely on supervised labels during the search process, failing to handle ubiquitous scenarios where supervisions are not available. In this paper, we study the problem of unsupervised graph neural architecture search, which remains unexplored in the literature. The key problem is to discover the latent graph factors that drive the formation of graph data as well as the underlying relations between the factors and the optimal neural architectures. Handling this problem is challenging given that the latent graph factors together with architectures are highly entangled due to the nature of the graph and the complexity of the neural architecture search process. To address the challenge, we propose a novel Disentangled Self-supervised Graph Neural Architecture Search (DSGAS) model, which is able to discover the optimal architectures capturing various latent graph factors in a self-supervised fashion based on unlabeled graph data. Specifically, we first design a disentangled graph super-network capable of incorporating multiple architectures with factor-wise disentanglement, which are optimized simultaneously. Then, we estimate the performance of architectures under different factors by our proposed self-supervised training with joint architecture-graph disentanglement. Finally, we propose a contrastive search with architecture augmentations to discover architectures with factor-specific expertise. Extensive experiments on 11 real-world datasets demonstrate that the proposed model is able to achieve state-of-the-art performance against several baseline methods in an unsupervised manner.</li>
</ul>

<h3>Title: Improving Diffusion-Based Generative Models via Approximated Optimal  Transport</h3>
<ul>
<li><strong>Authors: </strong>Daegyu Kim, Jooyoung Choi, Chaehun Shin, Uiwon Hwang, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05069">https://arxiv.org/abs/2403.05069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05069">https://arxiv.org/pdf/2403.05069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05069]] Improving Diffusion-Based Generative Models via Approximated Optimal  Transport(https://arxiv.org/abs/2403.05069)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce the Approximated Optimal Transport (AOT) technique, a novel training scheme for diffusion-based generative models. Our approach aims to approximate and integrate optimal transport into the training process, significantly enhancing the ability of diffusion models to estimate the denoiser outputs accurately. This improvement leads to ODE trajectories of diffusion models with lower curvature and reduced truncation errors during sampling. We achieve superior image quality and reduced sampling steps by employing AOT in training. Specifically, we achieve FID scores of 1.88 with just 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional generations, respectively. Furthermore, when applying AOT to train the discriminator for guidance, we establish new state-of-the-art FID scores of 1.68 and 1.58 for unconditional and conditional generations, respectively, each with 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing the performance of diffusion models.</li>
</ul>

<h3>Title: Spectrum Translation for Refinement of Image Generation (STIG) Based on  Contrastive Learning and Spectral Filter Profile</h3>
<ul>
<li><strong>Authors: </strong>Seokjun Lee, Seung-Won Jung, Hyunseok Seo</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05093">https://arxiv.org/abs/2403.05093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05093">https://arxiv.org/pdf/2403.05093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05093]] Spectrum Translation for Refinement of Image Generation (STIG) Based on  Contrastive Learning and Spectral Filter Profile(https://arxiv.org/abs/2403.05093)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, anomaly</a></li>
<li><strong>Abstract: </strong>Currently, image generation and synthesis have remarkably progressed with generative models. Despite photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in generative adversarial networks but in diffusion models. In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the generated images to improve generative performance of both GAN and diffusion models. This is realized by spectrum translation for the refinement of image generation (STIG) based on contrastive learning. We adopt theoretical logic of frequency components in various generative networks. The key idea, here, is to refine the spectrum of the generated image via the concept of image-to-image translation and contrastive learning in terms of digital signal processing. We evaluate our framework across eight fake image datasets and various cutting-edge models to demonstrate the effectiveness of STIG. Our framework outperforms other cutting-edges showing significant decreases in FID and log frequency distance of spectrum. We further emphasize that STIG improves image quality by decreasing the spectral anomaly. Additionally, validation results present that the frequency-based deepfake detector confuses more in the case where fake spectrums are manipulated by STIG.</li>
</ul>

<h3>Title: Face2Diffusion for Fast and Editable Face Personalization</h3>
<ul>
<li><strong>Authors: </strong>Kaede Shiohara, Toshihiko Yamasaki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05094">https://arxiv.org/abs/2403.05094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05094">https://arxiv.org/pdf/2403.05094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05094]] Face2Diffusion for Fast and Editable Face Personalization(https://arxiv.org/abs/2403.05094)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Face personalization aims to insert specific faces, taken from images, into pretrained text-to-image diffusion models. However, it is still challenging for previous methods to preserve both the identity similarity and editability due to overfitting to training samples. In this paper, we propose Face2Diffusion (F2D) for high-editability face personalization. The core idea behind F2D is that removing identity-irrelevant information from the training pipeline prevents the overfitting problem and improves editability of encoded faces. F2D consists of the following three novel components: 1) Multi-scale identity encoder provides well-disentangled identity features while keeping the benefits of multi-scale information, which improves the diversity of camera poses. 2) Expression guidance disentangles face expressions from identities and improves the controllability of face expressions. 3) Class-guided denoising regularization encourages models to learn how faces should be denoised, which boosts the text-alignment of backgrounds. Extensive experiments on the FaceForensics++ dataset and diverse prompts demonstrate our method greatly improves the trade-off between the identity- and text-fidelity compared to previous state-of-the-art methods.</li>
</ul>

<h3>Title: Enhancing Texture Generation with High-Fidelity Using Advanced Texture  Priors</h3>
<ul>
<li><strong>Authors: </strong>Kuo Xu, Maoyu Wang, Muyu Wang, Lincong Feng, Tianhui Zhang, Xiaoli Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05102">https://arxiv.org/abs/2403.05102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05102">https://arxiv.org/pdf/2403.05102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05102]] Enhancing Texture Generation with High-Fidelity Using Advanced Texture  Priors(https://arxiv.org/abs/2403.05102)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The recent advancements in 2D generation technology have sparked a widespread discussion on using 2D priors for 3D shape and texture content generation. However, these methods often overlook the subsequent user operations, such as texture aliasing and blurring that occur when the user acquires the 3D model and simplifies its structure. Traditional graphics methods partially alleviate this issue, but recent texture synthesis technologies fail to ensure consistency with the original model's appearance and cannot achieve high-fidelity restoration. Moreover, background noise frequently arises in high-resolution texture synthesis, limiting the practical application of these generation technologies.In this work, we propose a high-resolution and high-fidelity texture restoration technique that uses the rough texture as the initial input to enhance the consistency between the synthetic texture and the initial texture, thereby overcoming the issues of aliasing and blurring caused by the user's structure simplification operations. Additionally, we introduce a background noise smoothing technique based on a self-supervised scheme to address the noise problem in current high-resolution texture synthesis schemes. Our approach enables high-resolution texture synthesis, paving the way for high-definition and high-detail texture synthesis technology. Experiments demonstrate that our scheme outperforms currently known schemes in high-fidelity texture recovery under high-resolution conditions.</li>
</ul>

<h3>Title: Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Haochen Han, Qinghua Zheng, Guang Dai, Minnan Luo, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05105">https://arxiv.org/abs/2403.05105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05105">https://arxiv.org/pdf/2403.05105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05105]] Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval(https://arxiv.org/abs/2403.05105)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Collecting well-matched multimedia datasets is crucial for training cross-modal retrieval models. However, in real-world scenarios, massive multimodal data are harvested from the Internet, which inevitably contains Partially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data will remarkably harm the cross-modal retrieval performance. Previous efforts tend to mitigate this problem by estimating a soft correspondence to down-weight the contribution of PMPs. In this paper, we aim to address this challenge from a new perspective: the potential semantic similarity among unpaired samples makes it possible to excavate useful knowledge from mismatched pairs. To achieve this, we propose L2RM, a general framework based on Optimal Transport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to generate refined alignments by seeking a minimal-cost transport plan across different modalities. To formalize the rematching idea in OT, first, we propose a self-supervised cost function that automatically learns from explicit similarity-cost mapping relation. Second, we present to model a partial OT problem while restricting the transport among false positives to further boost refined alignments. Extensive experiments on three benchmarks demonstrate our L2RM significantly improves the robustness against PMPs for existing models. The code is available at https://github.com/hhc1997/L2RM.</li>
</ul>

<h3>Title: Simulating Battery-Powered TinyML Systems Optimised using Reinforcement  Learning in Image-Based Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jared M. Ping, Ken J. Nixon</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05106">https://arxiv.org/abs/2403.05106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05106">https://arxiv.org/pdf/2403.05106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05106]] Simulating Battery-Powered TinyML Systems Optimised using Reinforcement  Learning in Image-Based Anomaly Detection(https://arxiv.org/abs/2403.05106)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Advances in Tiny Machine Learning (TinyML) have bolstered the creation of smart industry solutions, including smart agriculture, healthcare and smart cities. Whilst related research contributes to enabling TinyML solutions on constrained hardware, there is a need to amplify real-world applications by optimising energy consumption in battery-powered systems. The work presented extends and contributes to TinyML research by optimising battery-powered image-based anomaly detection Internet of Things (IoT) systems. Whilst previous work in this area has yielded the capabilities of on-device inferencing and training, there has yet to be an investigation into optimising the management of such capabilities using machine learning approaches, such as Reinforcement Learning (RL), to improve the deployment battery life of such systems. Using modelled simulations, the battery life effects of an RL algorithm are benchmarked against static and dynamic optimisation approaches, with the foundation laid for a hardware benchmark to follow. It is shown that using RL within a TinyML-enabled IoT system to optimise the system operations, including cloud anomaly processing and on-device training, yields an improved battery life of 22.86% and 10.86% compared to static and dynamic optimisation approaches respectively. The proposed solution can be deployed to resource-constrained hardware, given its low memory footprint of 800 B, which could be further reduced. This further facilitates the real-world deployment of such systems, including key sectors such as smart agriculture.</li>
</ul>

<h3>Title: APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for  Unfairness Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Zikang Xu, Fenghe Tang, Quan Quan, Qingsong Yao, S. Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05114">https://arxiv.org/abs/2403.05114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05114">https://arxiv.org/pdf/2403.05114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05114]] APPLE: Adversarial Privacy-aware Perturbations on Latent Embedding for  Unfairness Mitigation(https://arxiv.org/abs/2403.05114)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Ensuring fairness in deep-learning-based segmentors is crucial for health equity. Much effort has been dedicated to mitigating unfairness in the training datasets or procedures. However, with the increasing prevalence of foundation models in medical image analysis, it is hard to train fair models from scratch while preserving utility. In this paper, we propose a novel method, Adversarial Privacy-aware Perturbations on Latent Embedding (APPLE), that can improve the fairness of deployed segmentors by introducing a small latent feature perturber without updating the weights of the original model. By adding perturbation to the latent vector, APPLE decorates the latent vector of segmentors such that no fairness-related features can be passed to the decoder of the segmentors while preserving the architecture and parameters of the segmentor. Experiments on two segmentation datasets and five segmentors (three U-Net-like and two SAM-like) illustrate the effectiveness of our proposed method compared to several unfairness mitigation methods.</li>
</ul>

<h3>Title: CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05121">https://arxiv.org/abs/2403.05121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05121">https://arxiv.org/pdf/2403.05121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05121]] CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion(https://arxiv.org/abs/2403.05121)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image generative systems have been largely driven by diffusion models. However, single-stage text-to-image diffusion models still face challenges, in terms of computational efficiency and the refinement of image details. To tackle the issue, we propose CogView3, an innovative cascaded framework that enhances the performance of text-to-image diffusion. CogView3 is the first model implementing relay diffusion in the realm of text-to-image generation, executing the task by first creating low-resolution images and subsequently applying relay-based super-resolution. This methodology not only results in competitive text-to-image outputs but also greatly reduces both training and inference costs. Our experimental results demonstrate that CogView3 outperforms SDXL, the current state-of-the-art open-source text-to-image diffusion model, by 77.0\% in human evaluations, all while requiring only about 1/2 of the inference time. The distilled variant of CogView3 achieves comparable performance while only utilizing 1/10 of the inference time by SDXL.</li>
</ul>

<h3>Title: Evaluating Text-to-Image Generative Models: An Empirical Study on Human  Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Muxi Chen, Yi Liu, Jian Yi, Changran Xu, Qiuxia Lai, Hongliang Wang, Tsung-Yi Ho, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05125">https://arxiv.org/abs/2403.05125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05125">https://arxiv.org/pdf/2403.05125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05125]] Evaluating Text-to-Image Generative Models: An Empirical Study on Human  Image Synthesis(https://arxiv.org/abs/2403.05125)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we present an empirical study introducing a nuanced evaluation framework for text-to-image (T2I) generative models, applied to human image synthesis. Our framework categorizes evaluations into two distinct groups: first, focusing on image qualities such as aesthetics and realism, and second, examining text conditions through concept coverage and fairness. We introduce an innovative aesthetic score prediction model that assesses the visual appeal of generated images and unveils the first dataset marked with low-quality regions in generated human images to facilitate automatic defect detection. Our exploration into concept coverage probes the model's effectiveness in interpreting and rendering text-based concepts accurately, while our analysis of fairness reveals biases in model outputs, with an emphasis on gender, race, and age. While our study is grounded in human imagery, this dual-faceted approach is designed with the flexibility to be applicable to other forms of image generation, enhancing our understanding of generative models and paving the way to the next generation of more sophisticated, contextually aware, and ethically attuned generative models. We will release our code, the data used for evaluating generative models and the dataset annotated with defective areas soon.</li>
</ul>

<h3>Title: ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Gang Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05135">https://arxiv.org/abs/2403.05135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05135">https://arxiv.org/pdf/2403.05135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05135]] ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment(https://arxiv.org/abs/2403.05135)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable performance in the domain of text-to-image generation. However, most widely used models still employ CLIP as their text encoder, which constrains their ability to comprehend dense prompts, encompassing multiple objects, detailed attributes, complex relationships, long-text alignment, etc. In this paper, we introduce an Efficient Large Language Model Adapter, termed ELLA, which equips text-to-image diffusion models with powerful Large Language Models (LLM) to enhance text alignment without training of either U-Net or LLM. To seamlessly bridge two pre-trained models, we investigate a range of semantic alignment connector designs and propose a novel module, the Timestep-Aware Semantic Connector (TSC), which dynamically extracts timestep-dependent conditions from LLM. Our approach adapts semantic features at different stages of the denoising process, assisting diffusion models in interpreting lengthy and intricate prompts over sampling timesteps. Additionally, ELLA can be readily incorporated with community models and tools to improve their prompt-following capabilities. To assess text-to-image models in dense prompt following, we introduce Dense Prompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K dense prompts. Extensive experiments demonstrate the superiority of ELLA in dense prompt following compared to state-of-the-art methods, particularly in multiple object compositions involving diverse attributes and relationships.</li>
</ul>

<h3>Title: Improving Diffusion Models for Virtual Try-on</h3>
<ul>
<li><strong>Authors: </strong>Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05139">https://arxiv.org/abs/2403.05139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05139">https://arxiv.org/pdf/2403.05139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05139]] Improving Diffusion Models for Virtual Try-on(https://arxiv.org/abs/2403.05139)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper considers image-based virtual try-on, which renders an image of a person wearing a curated garment, given a pair of images depicting the person and the garment, respectively. Previous works adapt existing exemplar-based inpainting diffusion models for virtual try-on to improve the naturalness of the generated visuals compared to other methods (e.g., GAN-based), but they fail to preserve the identity of the garments. To overcome this limitation, we propose a novel diffusion model that improves garment fidelity and generates authentic virtual try-on images. Our method, coined IDM-VTON, uses two different modules to encode the semantics of garment image; given the base UNet of the diffusion model, 1) the high-level semantics extracted from a visual encoder are fused to the cross-attention layer, and then 2) the low-level features extracted from parallel UNet are fused to the self-attention layer. In addition, we provide detailed textual prompts for both garment and person images to enhance the authenticity of the generated visuals. Finally, we present a customization method using a pair of person-garment images, which significantly improves fidelity and authenticity. Our experimental results show that our method outperforms previous approaches (both diffusion-based and GAN-based) in preserving garment details and generating authentic virtual try-on images, both qualitatively and quantitatively. Furthermore, the proposed customization method demonstrates its effectiveness in a real-world scenario.</li>
</ul>

<h3>Title: Towards a Psychology of Machines: Large Language Models Predict Human  Memory</h3>
<ul>
<li><strong>Authors: </strong>Markus Huff, Elanur Ulakçı</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05152">https://arxiv.org/abs/2403.05152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05152">https://arxiv.org/pdf/2403.05152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05152]] Towards a Psychology of Machines: Large Language Models Predict Human  Memory(https://arxiv.org/abs/2403.05152)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"). We measured both human's and ChatGPT's ratings of sentence relatedness, ChatGPT's memorability ratings for the garden-path sentences, and humans' spontaneous memory for the garden-path sentences. The results revealed a striking alignment between ChatGPT's assessments and human performance. Sentences deemed more related and assessed as being more memorable by ChatGPT were indeed better remembered by humans, even though ChatGPT's internal mechanisms likely differ significantly from human cognition. This finding, which was confirmed with a robustness check employing synonyms, underscores the potential of generative AI models to predict human performance accurately. We discuss the broader implications of these findings for leveraging LLMs in the development of psychological theories and for gaining a deeper understanding of human cognition.</li>
</ul>

<h3>Title: GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian  Splatting</h3>
<ul>
<li><strong>Authors: </strong>Francesco Palandra, Andrea Sanchietti, Daniele Baieri, Emanuele Rodolà</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05154">https://arxiv.org/abs/2403.05154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05154">https://arxiv.org/pdf/2403.05154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05154]] GSEdit: Efficient Text-Guided Editing of 3D Objects via Gaussian  Splatting(https://arxiv.org/abs/2403.05154)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present GSEdit, a pipeline for text-guided 3D object editing based on Gaussian Splatting models. Our method enables the editing of the style and appearance of 3D objects without altering their main details, all in a matter of minutes on consumer hardware. We tackle the problem by leveraging Gaussian splatting to represent 3D scenes, and we optimize the model while progressively varying the image supervision by means of a pretrained image-based diffusion model. The input object may be given as a 3D triangular mesh, or directly provided as Gaussians from a generative model such as DreamGaussian. GSEdit ensures consistency across different viewpoints, maintaining the integrity of the original object's information. Compared to previously proposed methods relying on NeRF-like MLP models, GSEdit stands out for its efficiency, making 3D editing tasks much faster. Our editing process is refined via the application of the SDS loss, ensuring that our edits are both precise and accurate. Our comprehensive evaluation demonstrates that GSEdit effectively alters object shape and appearance following the given textual instructions while preserving their coherence and detail.</li>
</ul>

<h3>Title: DiffuLT: How to Make Diffusion Model Useful for Long-tail Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jie Shao, Ke Zhu, Hanxiao Zhang, Jianxin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05170">https://arxiv.org/abs/2403.05170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05170">https://arxiv.org/pdf/2403.05170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05170]] DiffuLT: How to Make Diffusion Model Useful for Long-tail Recognition(https://arxiv.org/abs/2403.05170)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper proposes a new pipeline for long-tail (LT) recognition. Instead of re-weighting or re-sampling, we utilize the long-tailed dataset itself to generate a balanced proxy that can be optimized through cross-entropy (CE). Specifically, a randomly initialized diffusion model, trained exclusively on the long-tailed dataset, is employed to synthesize new samples for underrepresented classes. Then, we utilize the inherent information in the original dataset to filter out harmful samples and keep the useful ones. Our strategy, Diffusion model for Long-Tail recognition (DiffuLT), represents a pioneering utilization of generative models in long-tail recognition. DiffuLT achieves state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT, surpassing the best competitors with non-trivial margins. Abundant ablations make our pipeline interpretable, too. The whole generation pipeline is done without any external data or pre-trained model weights, making it highly generalizable to real-world long-tailed settings.</li>
</ul>

<h3>Title: Learning Expressive And Generalizable Motion Features For Face Forgery  Detection</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Zhang, Peng Zhang, Jingjing Wang, Di Xie, Shiliang Pu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05172">https://arxiv.org/abs/2403.05172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05172">https://arxiv.org/pdf/2403.05172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05172]] Learning Expressive And Generalizable Motion Features For Face Forgery  Detection(https://arxiv.org/abs/2403.05172)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Previous face forgery detection methods mainly focus on appearance features, which may be easily attacked by sophisticated manipulation. Considering the majority of current face manipulation methods generate fake faces based on a single frame, which do not take frame consistency and coordination into consideration, artifacts on frame sequences are more effective for face forgery detection. However, current sequence-based face forgery detection methods use general video classification networks directly, which discard the special and discriminative motion information for face manipulation detection. To this end, we propose an effective sequence-based forgery detection framework based on an existing video classification method. To make the motion features more expressive for manipulation detection, we propose an alternative motion consistency block instead of the original motion features module. To make the learned features more generalizable, we propose an auxiliary anomaly detection block. With these two specially designed improvements, we make a general video classification network achieve promising results on three popular face forgery datasets.</li>
</ul>

<h3>Title: Denoising Autoregressive Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Yazhe Li, Jorg Bornschein, Ting Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05196">https://arxiv.org/abs/2403.05196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05196">https://arxiv.org/pdf/2403.05196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05196]] Denoising Autoregressive Representation Learning(https://arxiv.org/abs/2403.05196)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we explore a new generative approach for learning visual representations. Our method, DARL, employs a decoder-only Transformer to predict image patches autoregressively. We find that training with Mean Squared Error (MSE) alone leads to strong representations. To enhance the image generation ability, we replace the MSE loss with the diffusion objective by using a denoising patch decoder. We show that the learned representation can be improved by using tailored noise schedules and longer training in larger models. Notably, the optimal schedule differs significantly from the typical ones used in standard image diffusion models. Overall, despite its simple architecture, DARL delivers performance remarkably close to state-of-the-art masked prediction models under the fine-tuning protocol. This marks an important step towards a unified model capable of both visual perception and generation, effectively combining the strengths of autoregressive and denoising diffusion models.</li>
</ul>

<h3>Title: Synthetic Privileged Information Enhances Medical Image Representation  Learning</h3>
<ul>
<li><strong>Authors: </strong>Lucas Farndale, Chris Walsh, Robert Insall, Ke Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, q-bio.TO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05220">https://arxiv.org/abs/2403.05220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05220">https://arxiv.org/pdf/2403.05220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05220]] Synthetic Privileged Information Enhances Medical Image Representation  Learning(https://arxiv.org/abs/2403.05220)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Multimodal self-supervised representation learning has consistently proven to be a highly effective method in medical image analysis, offering strong task performance and producing biologically informed insights. However, these methods heavily rely on large, paired datasets, which is prohibitive for their use in scenarios where paired data does not exist, or there is only a small amount available. In contrast, image generation methods can work well on very small datasets, and can find mappings between unpaired datasets, meaning an effectively unlimited amount of paired synthetic data can be generated. In this work, we demonstrate that representation learning can be significantly improved by synthetically generating paired information, both compared to training on either single-modality (up to 4.4x error reduction) or authentic multi-modal paired datasets (up to 5.6x error reduction).</li>
</ul>

<h3>Title: Towards Effective Usage of Human-Centric Priors in Diffusion Models for  Text-based Human Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Junyan Wang, Zhenhong Sun, Zhiyu Tan, Xuanbai Chen, Weihua Chen, Hao Li, Cheng Zhang, Yang Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05239">https://arxiv.org/abs/2403.05239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05239">https://arxiv.org/pdf/2403.05239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05239]] Towards Effective Usage of Human-Centric Priors in Diffusion Models for  Text-based Human Image Generation(https://arxiv.org/abs/2403.05239)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Vanilla text-to-image diffusion models struggle with generating accurate human images, commonly resulting in imperfect anatomies such as unnatural postures or disproportionate limbs.Existing methods address this issue mostly by fine-tuning the model with extra images or adding additional controls -- human-centric priors such as pose or depth maps -- during the image generation phase. This paper explores the integration of these human-centric priors directly into the model fine-tuning stage, essentially eliminating the need for extra conditions at the inference stage. We realize this idea by proposing a human-centric alignment loss to strengthen human-related information from the textual prompts within the cross-attention maps. To ensure semantic detail richness and human structural accuracy during fine-tuning, we introduce scale-aware and step-wise constraints within the diffusion process, according to an in-depth analysis of the cross-attention layer. Extensive experiments show that our method largely improves over state-of-the-art text-to-image models to synthesize high-quality human images based on user-written prompts. Project page: \url{https://hcplayercvpr2024.github.io}.</li>
</ul>

<h3>Title: DiffSF: Diffusion Models for Scene Flow Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yushan Zhang, Bastian Wandt, Maria Magnusson, Michael Felsberg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05327">https://arxiv.org/abs/2403.05327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05327">https://arxiv.org/pdf/2403.05327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05327]] DiffSF: Diffusion Models for Scene Flow Estimation(https://arxiv.org/abs/2403.05327)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Scene flow estimation is an essential ingredient for a variety of real-world applications, especially for autonomous agents, such as self-driving cars and robots. While recent scene flow estimation approaches achieve a reasonable accuracy, their applicability to real-world systems additionally benefits from a reliability measure. Aiming at improving accuracy while additionally providing an estimate for uncertainty, we propose DiffSF that combines transformer-based scene flow estimation with denoising diffusion models. In the diffusion process, the ground truth scene flow vector field is gradually perturbed by adding Gaussian noise. In the reverse process, starting from randomly sampled Gaussian noise, the scene flow vector field prediction is recovered by conditioning on a source and a target point cloud. We show that the diffusion process greatly increases the robustness of predictions compared to prior approaches resulting in state-of-the-art performance on standard scene flow estimation benchmarks. Moreover, by sampling multiple times with different initial states, the denoising process predicts multiple hypotheses, which enables measuring the output uncertainty, allowing our approach to detect a majority of the inaccurate predictions.</li>
</ul>

<h3>Title: Federated Learning Method for Preserving Privacy in Face Recognition  System</h3>
<ul>
<li><strong>Authors: </strong>Enoch Solomon, Abraham Woubie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05344">https://arxiv.org/abs/2403.05344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05344">https://arxiv.org/pdf/2403.05344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05344]] Federated Learning Method for Preserving Privacy in Face Recognition  System(https://arxiv.org/abs/2403.05344)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The state-of-the-art face recognition systems are typically trained on a single computer, utilizing extensive image datasets collected from various number of users. However, these datasets often contain sensitive personal information that users may hesitate to disclose. To address potential privacy concerns, we explore the application of federated learning, both with and without secure aggregators, in the context of both supervised and unsupervised face recognition systems. Federated learning facilitates the training of a shared model without necessitating the sharing of individual private data, achieving this by training models on decentralized edge devices housing the data. In our proposed system, each edge device independently trains its own model, which is subsequently transmitted either to a secure aggregator or directly to the central server. To introduce diverse data without the need for data transmission, we employ generative adversarial networks to generate imposter data at the edge. Following this, the secure aggregator or central server combines these individual models to construct a global model, which is then relayed back to the edge devices. Experimental findings based on the CelebA datasets reveal that employing federated learning in both supervised and unsupervised face recognition systems offers dual benefits. Firstly, it safeguards privacy since the original data remains on the edge devices. Secondly, the experimental results demonstrate that the aggregated model yields nearly identical performance compared to the individual models, particularly when the federated model does not utilize a secure aggregator. Hence, our results shed light on the practical challenges associated with privacy-preserving face image training, particularly in terms of the balance between privacy and accuracy.</li>
</ul>

<h3>Title: Enhancing Plausibility Evaluation for Generated Designs with Denoising  Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Jiajie Fan, Amal Trigui, Thomas Bäck, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05352">https://arxiv.org/abs/2403.05352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05352">https://arxiv.org/pdf/2403.05352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05352]] Enhancing Plausibility Evaluation for Generated Designs with Denoising  Autoencoder(https://arxiv.org/abs/2403.05352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A great interest has arisen in using Deep Generative Models (DGM) for generative design. When assessing the quality of the generated designs, human designers focus more on structural plausibility, e.g., no missing component, rather than visual artifacts, e.g., noises in the images. Meanwhile, commonly used metrics such as Fr\'echet Inception Distance (FID) may not evaluate accurately as they tend to penalize visual artifacts instead of structural implausibility. As such, FID might not be suitable to assess the performance of DGMs for a generative design task. In this work, we propose to encode the input designs with a simple Denoising Autoencoder (DAE) and measure the distribution distance in the latent space thereof. We experimentally test our DAE-based metrics with FID and other state-of-the-art metrics on three data sets: compared to FID and some more recent works, e.g., FD$_\text{DINO-V2}$ and topology distance, DAE-based metrics can effectively detect implausible structures and are more consistent with structural inspection by human experts.</li>
</ul>

<h3>Title: Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia  Classification</h3>
<ul>
<li><strong>Authors: </strong>Salome Kazeminia, Max Joosten, Dragan Bosnacki, Carsten Marr</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05379">https://arxiv.org/abs/2403.05379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05379">https://arxiv.org/pdf/2403.05379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05379]] Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia  Classification(https://arxiv.org/abs/2403.05379)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Automated disease diagnosis using medical image analysis relies on deep learning, often requiring large labeled datasets for supervised model training. Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and costly annotations on a single-cell level. Multiple Instance Learning (MIL) addresses weakly labeled scenarios but necessitates powerful encoders typically trained with labeled data. In this study, we explore Self-Supervised Learning (SSL) as a pre-training approach for MIL-based AML subtype classification from blood smears, removing the need for labeled data during encoder training. We investigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and compare their performance against supervised pre-training. Our findings show that SSL-pretrained encoders achieve comparable performance, showcasing the potential of SSL in MIL. This breakthrough offers a cost-effective and data-efficient solution, propelling the field of AI-based disease diagnosis.</li>
</ul>

<h3>Title: Considering Nonstationary within Multivariate Time Series with  Variational Hierarchical Transformer for Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Muyao Wang, Wenchao Chen, Bo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05406">https://arxiv.org/abs/2403.05406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05406">https://arxiv.org/pdf/2403.05406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05406]] Considering Nonstationary within Multivariate Time Series with  Variational Hierarchical Transformer for Forecasting(https://arxiv.org/abs/2403.05406)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The forecasting of Multivariate Time Series (MTS) has long been an important but challenging task. Due to the non-stationary problem across long-distance time steps, previous studies primarily adopt stationarization method to attenuate the non-stationary problem of the original series for better predictability. However, existing methods always adopt the stationarized series, which ignores the inherent non-stationarity, and has difficulty in modeling MTS with complex distributions due to the lack of stochasticity. To tackle these problems, we first develop a powerful hierarchical probabilistic generative module to consider the non-stationarity and stochastic characteristics within MTS, and then combine it with transformer for a well-defined variational generative dynamic model named Hierarchical Time series Variational Transformer (HTV-Trans), which recovers the intrinsic non-stationary information into temporal dependencies. Being a powerful probabilistic model, HTV-Trans is utilized to learn expressive representations of MTS and applied to forecasting tasks. Extensive experiments on diverse datasets show the efficiency of HTV-Trans on MTS forecasting tasks</li>
</ul>

<h3>Title: SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised  Learning for Robust Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Yahao Lu, Yupei Lin, Han Wu, Xiaoyu Xian, Yukai Shi, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05416">https://arxiv.org/abs/2403.05416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05416">https://arxiv.org/pdf/2403.05416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05416]] SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised  Learning for Robust Infrared Small Target Detection(https://arxiv.org/abs/2403.05416)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Single-frame infrared small target (SIRST) detection aims to recognize small targets from clutter backgrounds. Recently, convolutional neural networks have achieved significant advantages in general object detection. With the development of Transformer, the scale of SIRST models is constantly increasing. Due to the limited training samples, performance has not been improved accordingly. The quality, quantity, and diversity of the infrared dataset are critical to the detection of small targets. To highlight this issue, we propose a negative sample augmentation method in this paper. Specifically, a negative augmentation approach is proposed to generate massive negatives for self-supervised learning. Firstly, we perform a sequential noise modeling technology to generate realistic infrared data. Secondly, we fuse the extracted noise with the original data to facilitate diversity and fidelity in the generated data. Lastly, we proposed a negative augmentation strategy to enrich diversity as well as maintain semantic invariance. The proposed algorithm produces a synthetic SIRST-5K dataset, which contains massive pseudo-data and corresponding labels. With a rich diversity of infrared small target data, our algorithm significantly improves the model performance and convergence speed. Compared with other state-of-the-art (SOTA) methods, our method achieves outstanding performance in terms of probability of detection (Pd), false-alarm rate (Fa), and intersection over union (IoU).</li>
</ul>

<h3>Title: Cost-Performance Optimization for Processing Low-Resource Language Tasks  Using Commercial LLMs</h3>
<ul>
<li><strong>Authors: </strong>Arijit Nag, Animesh Mukherjee, Niloy Ganguly, Soumen Chakrabarti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05434">https://arxiv.org/abs/2403.05434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05434">https://arxiv.org/pdf/2403.05434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05434]] Cost-Performance Optimization for Processing Low-Resource Language Tasks  Using Commercial LLMs(https://arxiv.org/abs/2403.05434)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive zero/few-shot inference and generation quality for high-resource languages(HRLs). A few of them have been trained in low-resource languages (LRLs) and give decent performance. Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens. The number of tokens strongly depends on the script and language, as well as the LLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs. This is because most currently popular LLMs are optimized for HRL vocabularies. Our objective is to level the playing field: reduce the cost of processing LRLs in contemporary LLMs while ensuring that predictive and generative qualities are not compromised. As means to reduce the number of tokens processed by the LLM, we consider code-mixing, translation, and transliteration of LRLs to HRLs. We perform an extensive study using the IndicXTREME dataset, covering 15 Indian languages, while using GPT-4 (one of the costliest LLM services released so far) as a commercial LLM. We observe and analyze interesting patterns involving token count, cost,and quality across a multitude of languages and tasks. We show that choosing the best policy to interact with the LLM can reduce cost by 90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.</li>
</ul>

<h3>Title: VideoElevator: Elevating Video Generation Quality with Versatile  Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yabo Zhang, Yuxiang Wei, Xianhui Lin, Zheng Hui, Peiran Ren, Xuansong Xie, Xiangyang Ji, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05438">https://arxiv.org/abs/2403.05438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05438">https://arxiv.org/pdf/2403.05438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05438]] VideoElevator: Elevating Video Generation Quality with Versatile  Text-to-Image Diffusion Models(https://arxiv.org/abs/2403.05438)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models (T2I) have demonstrated unprecedented capabilities in creating realistic and aesthetic images. On the contrary, text-to-video diffusion models (T2V) still lag far behind in frame quality and text alignment, owing to insufficient quality and quantity of training videos. In this paper, we introduce VideoElevator, a training-free and plug-and-play method, which elevates the performance of T2V using superior capabilities of T2I. Different from conventional T2V sampling (i.e., temporal and spatial modeling), VideoElevator explicitly decomposes each sampling step into temporal motion refining and spatial quality elevating. Specifically, temporal motion refining uses encapsulated T2V to enhance temporal consistency, followed by inverting to the noise distribution required by T2I. Then, spatial quality elevating harnesses inflated T2I to directly predict less noisy latent, adding more photo-realistic details. We have conducted experiments in extensive prompts under the combination of various T2V and T2I. The results show that VideoElevator not only improves the performance of T2V baselines with foundational T2I, but also facilitates stylistic video synthesis with personalized T2I. Our code is available at https://github.com/YBYBZhang/VideoElevator.</li>
</ul>

<h3>Title: JointMotion: Joint Self-supervision for Joint Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Royden Wagner, Ömer Şahin Taş, Marvin Klemp, Carlos Fernandez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05489">https://arxiv.org/abs/2403.05489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05489">https://arxiv.org/pdf/2403.05489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05489]] JointMotion: Joint Self-supervision for Joint Motion Prediction(https://arxiv.org/abs/2403.05489)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We present JointMotion, a self-supervised learning method for joint motion prediction in autonomous driving. Our method includes a scene-level objective connecting motion and environments, and an instance-level objective to refine learned representations. Our evaluations show that these objectives are complementary and outperform recent contrastive and autoencoding methods as pre-training for joint motion prediction. Furthermore, JointMotion adapts to all common types of environment representations used for motion prediction (i.e., agent-centric, scene-centric, and pairwise relative), and enables effective transfer learning between the Waymo Open Motion and the Argoverse 2 Forecasting datasets. Notably, our method improves the joint final displacement error of Wayformer, Scene Transformer, and HPTR by 3%, 7%, and 11%, respectively.</li>
</ul>

<h3>Title: GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless  Generative Inference of LLM</h3>
<ul>
<li><strong>Authors: </strong>Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05527">https://arxiv.org/abs/2403.05527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05527">https://arxiv.org/pdf/2403.05527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05527]] GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless  Generative Inference of LLM(https://arxiv.org/abs/2403.05527)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR.</li>
</ul>

<h3>Title: The Computational Complexity of Learning Gaussian Single-Index Models</h3>
<ul>
<li><strong>Authors: </strong>Alex Damian, Loucas Pillaud-Vivien, Jason D. Lee, Joan Bruna</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.05529">https://arxiv.org/abs/2403.05529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.05529">https://arxiv.org/pdf/2403.05529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.05529]] The Computational Complexity of Learning Gaussian Single-Index Models(https://arxiv.org/abs/2403.05529)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime. While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$ samples, where $k^\star$ is a "generative" exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace algorithm. Therefore, our results provide evidence of a sharp computational-to-statistical gap (under both the SQ and LDP class) whenever $k^\star>2$. To complete the study, we provide examples of smooth and Lipschitz deterministic target functions with arbitrarily large generative exponents $k^\star$.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
