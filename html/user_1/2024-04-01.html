<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-01</h1>
<h3>Title: HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for  Few-shot Complex Table Understanding</h3>
<ul>
<li><strong>Authors: </strong>Rihui Jin, Yu Li, Guilin Qi, Nan Hu, Yuan-Fang Li, Jiaoyan Chen, Jianan Wang, Yongrui Chen, Dehai Min</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19723">https://arxiv.org/abs/2403.19723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19723">https://arxiv.org/pdf/2403.19723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19723]] HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for  Few-shot Complex Table Understanding(https://arxiv.org/abs/2403.19723)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.</li>
</ul>

<h3>Title: MUGC: Machine Generated versus User Generated Content Detection</h3>
<ul>
<li><strong>Authors: </strong>Yaqi Xie, Anjali Rawal, Yujing Cen, Dixuan Zhao, Sunil K Narang, Shanu Sushmita</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19725">https://arxiv.org/abs/2403.19725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19725">https://arxiv.org/pdf/2403.19725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19725]] MUGC: Machine Generated versus User Generated Content Detection(https://arxiv.org/abs/2403.19725)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As advanced modern systems like deep neural networks (DNNs) and generative AI continue to enhance their capabilities in producing convincing and realistic content, the need to distinguish between user-generated and machine generated content is becoming increasingly evident. In this research, we undertake a comparative evaluation of eight traditional machine-learning algorithms to distinguish between machine-generated and human-generated data across three diverse datasets: Poems, Abstracts, and Essays. Our results indicate that traditional methods demonstrate a high level of accuracy in identifying machine-generated data, reflecting the documented effectiveness of popular pre-trained models like RoBERT. We note that machine-generated texts tend to be shorter and exhibit less word variety compared to human-generated content. While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Language Models), may contribute to this high detection accuracy, we show that deeper word representations like word2vec can capture subtle semantic variances. Furthermore, readability, bias, moral, and affect comparisons reveal a discernible contrast between machine-generated and human generated content. There are variations in expression styles and potentially underlying biases in the data sources (human and machine-generated). This study provides valuable insights into the advancing capacities and challenges associated with machine-generated content across various domains.</li>
</ul>

<h3>Title: MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention  Editing in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hidir Yesiltepe, Kiymet Akdemir, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19738">https://arxiv.org/abs/2403.19738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19738">https://arxiv.org/pdf/2403.19738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19738]] MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention  Editing in Text-to-Image Diffusion Models(https://arxiv.org/abs/2403.19738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image models have rapidly gained popularity for their ability to generate detailed and realistic images from textual descriptions. However, these models often reflect the biases present in their training data, especially impacting marginalized groups. While prior efforts to debias language models have focused on addressing specific biases, such as racial or gender biases, efforts to tackle intersectional bias have been limited. Intersectional bias refers to the unique form of bias experienced by individuals at the intersection of multiple social identities. Addressing intersectional bias is crucial because it amplifies the negative effects of discrimination based on race, gender, and other identities. In this paper, we introduce a method that addresses intersectional bias in diffusion-based text-to-image models by modifying cross-attention maps in a disentangled manner. Our approach utilizes a pre-trained Stable Diffusion model, eliminates the need for an additional set of reference images, and preserves the original quality for unaltered concepts. Comprehensive experiments demonstrate that our method surpasses existing approaches in mitigating both single and intersectional biases across various attributes. We make our source code and debiased models for various attributes available to encourage fairness in generative models and to support further research.</li>
</ul>

<h3>Title: ShapeFusion: A 3D diffusion model for localized shape editing</h3>
<ul>
<li><strong>Authors: </strong>Rolandos Alexandros Potamias, Michail Tarasiou Stylianos Ploumpis, Stefanos Zafeiriou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19773">https://arxiv.org/abs/2403.19773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19773">https://arxiv.org/pdf/2403.19773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19773]] ShapeFusion: A 3D diffusion model for localized shape editing(https://arxiv.org/abs/2403.19773)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the realm of 3D computer vision, parametric models have emerged as a ground-breaking methodology for the creation of realistic and expressive 3D avatars. Traditionally, they rely on Principal Component Analysis (PCA), given its ability to decompose data to an orthonormal space that maximally captures shape variations. However, due to the orthogonality constraints and the global nature of PCA's decomposition, these models struggle to perform localized and disentangled editing of 3D shapes, which severely affects their use in applications requiring fine control such as face sculpting. In this paper, we leverage diffusion models to enable diverse and fully localized edits on 3D meshes, while completely preserving the un-edited regions. We propose an effective diffusion masking training strategy that, by design, facilitates localized manipulation of any shape region, without being limited to predefined regions or to sparse sets of predefined control vertices. Following our framework, a user can explicitly set their manipulation region of choice and define an arbitrary set of vertices as handles to edit a 3D mesh. Compared to the current state-of-the-art our method leads to more interpretable shape manipulations than methods relying on latent code state, greater localization and generation diversity while offering faster inference than optimization based approaches. Project page: https://rolpotamias.github.io/Shapefusion/</li>
</ul>

<h3>Title: Efficient 3D Instance Mapping and Localization with Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>George Tang, Krishna Murthy Jatavallabhula, Antonio Torralba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19797">https://arxiv.org/abs/2403.19797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19797">https://arxiv.org/pdf/2403.19797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19797]] Efficient 3D Instance Mapping and Localization with Neural Fields(https://arxiv.org/abs/2403.19797)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We tackle the problem of learning an implicit scene representation for 3D instance segmentation from a sequence of posed RGB images. Towards this, we introduce 3DIML, a novel framework that efficiently learns a label field that may be rendered from novel viewpoints to produce view-consistent instance segmentation masks. 3DIML significantly improves upon training and inference runtimes of existing implicit scene representation based methods. Opposed to prior art that optimizes a neural field in a self-supervised manner, requiring complicated training procedures and loss function design, 3DIML leverages a two-phase process. The first phase, InstanceMap, takes as input 2D segmentation masks of the image sequence generated by a frontend instance segmentation model, and associates corresponding masks across images to 3D labels. These almost view-consistent pseudolabel masks are then used in the second phase, InstanceLift, to supervise the training of a neural label field, which interpolates regions missed by InstanceMap and resolves ambiguities. Additionally, we introduce InstanceLoc, which enables near realtime localization of instance masks given a trained label field and an off-the-shelf image segmentation model by fusing outputs from both. We evaluate 3DIML on sequences from the Replica and ScanNet datasets and demonstrate 3DIML's effectiveness under mild assumptions for the image sequences. We achieve a 14-24x speedup over existing implicit scene representation methods with comparable quality, showcasing its potential to facilitate faster and more effective 3D scene understanding.</li>
</ul>

<h3>Title: Concept-based Analysis of Neural Networks via Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ravi Mangal, Nina Narodytska, Divya Gopinath, Boyue Caroline Hu, Anirban Roy, Susmit Jha, Corina Pasareanu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19837">https://arxiv.org/abs/2403.19837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19837">https://arxiv.org/pdf/2403.19837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19837]] Concept-based Analysis of Neural Networks via Vision-Language Models(https://arxiv.org/abs/2403.19837)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our techniques on a ResNet-based classifier trained on the RIVAL-10 dataset leveraging CLIP as the multimodal model.</li>
</ul>

<h3>Title: Is Synthetic Image Useful for Transfer Learning? An Investigation into  Data Generation, Volume, and Utilization</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Li, Xin Dong, Chen Chen, Jingtao Li, Yuxin Wen, Michael Spranger, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19866">https://arxiv.org/abs/2403.19866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19866">https://arxiv.org/pdf/2403.19866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19866]] Is Synthetic Image Useful for Transfer Learning? An Investigation into  Data Generation, Volume, and Utilization(https://arxiv.org/abs/2403.19866)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic image data generation represents a promising avenue for training deep learning models, particularly in the realm of transfer learning, where obtaining real images within a specific domain can be prohibitively expensive due to privacy and intellectual property considerations. This work delves into the generation and utilization of synthetic images derived from text-to-image generative models in facilitating transfer learning paradigms. Despite the high visual fidelity of the generated images, we observe that their naive incorporation into existing real-image datasets does not consistently enhance model performance due to the inherent distribution gap between synthetic and real images. To address this issue, we introduce a novel two-stage framework called bridged transfer, which initially employs synthetic images for fine-tuning a pre-trained model to improve its transferability and subsequently uses real data for rapid adaptation. Alongside, We propose dataset style inversion strategy to improve the stylistic alignment between synthetic and real images. Our proposed methods are evaluated across 10 different datasets and 5 distinct models, demonstrating consistent improvements, with up to 30% accuracy increase on classification tasks. Intriguingly, we note that the enhancements were not yet saturated, indicating that the benefits may further increase with an expanded volume of synthetic data.</li>
</ul>

<h3>Title: Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models  for Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Haipeng Liu, Yang Wang, Biao Qian, Meng Wang, Yong Rui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19898">https://arxiv.org/abs/2403.19898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19898">https://arxiv.org/pdf/2403.19898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19898]] Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models  for Image Inpainting(https://arxiv.org/abs/2403.19898)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Denoising diffusion probabilistic models for image inpainting aim to add the noise to the texture of image during the forward process and recover masked regions with unmasked ones of the texture via the reverse denoising process.Despite the meaningful semantics generation,the existing arts suffer from the semantic discrepancy between masked and unmasked regions, since the semantically dense unmasked texture fails to be completely degraded while the masked regions turn to the pure noise in diffusion process,leading to the large discrepancy between them.In this paper,we aim to answer how unmasked semantics guide texture denoising process;together with how to tackle the semantic discrepancy,to facilitate the consistent and meaningful semantics generation.To this end,we propose a novel structure-guided diffusion model named StrDiffusion,to reformulate the conventional texture denoising process under structure guidance to derive a simplified denoising objective for image inpainting,while revealing:1) the semantically sparse structure is beneficial to tackle semantic discrepancy in early stage, while dense texture generates reasonable semantics in late stage;2) the semantics from unmasked regions essentially offer the time-dependent structure guidance for the texture denoising process,benefiting from the time-dependent sparsity of the structure semantics.For the denoising process,a structure-guided neural network is trained to estimate the simplified denoising objective by exploiting the consistency of the denoised structure between masked and unmasked regions.Besides,we devise an adaptive resampling strategy as a formal criterion as whether structure is competent to guide the texture denoising process,while regulate their semantic correlations.Extensive experiments validate the merits of StrDiffusion over the state-of-the-arts.Our code is available at https://github.com/htyjers/StrDiffusion.</li>
</ul>

<h3>Title: Diff-Reg v1: Diffusion Matching Model for Registration Problem</h3>
<ul>
<li><strong>Authors: </strong>Qianliang Wu, Haobo Jiang, Lei Luo, Jun Li, Yaqing Ding, Jin Xie, Jian Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19919">https://arxiv.org/abs/2403.19919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19919">https://arxiv.org/pdf/2403.19919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19919]] Diff-Reg v1: Diffusion Matching Model for Registration Problem(https://arxiv.org/abs/2403.19919)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Establishing reliable correspondences is essential for registration tasks such as 3D and 2D3D registration. Existing methods commonly leverage geometric or semantic point features to generate potential correspondences. However, these features may face challenges such as large deformation, scale inconsistency, and ambiguous matching problems (e.g., symmetry). Additionally, many previous methods, which rely on single-pass prediction, may struggle with local minima in complex scenarios. To mitigate these challenges, we introduce a diffusion matching model for robust correspondence construction. Our approach treats correspondence estimation as a denoising diffusion process within the doubly stochastic matrix space, which gradually denoises (refines) a doubly stochastic matching matrix to the ground-truth one for high-quality correspondence estimation. It involves a forward diffusion process that gradually introduces Gaussian noise into the ground truth matching matrix and a reverse denoising process that iteratively refines the noisy matching matrix. In particular, the feature extraction from the backbone occurs only once during the inference phase. Our lightweight denoising module utilizes the same feature at each reverse sampling step. Evaluation of our method on both 3D and 2D3D registration tasks confirms its effectiveness.</li>
</ul>

<h3>Title: FairCLIP: Harnessing Fairness in Vision-Language Learning</h3>
<ul>
<li><strong>Authors: </strong>Yan Luo, Min Shi, Muhammad Osama Khan, Muhammad Muneeb Afzal, Hao Huang, Shuaihang Yuan, Yu Tian, Luo Song, Ava Kouhana, Tobias Elze, Yi Fang, Mengyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19949">https://arxiv.org/abs/2403.19949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19949">https://arxiv.org/pdf/2403.19949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19949]] FairCLIP: Harnessing Fairness in Vision-Language Learning(https://arxiv.org/abs/2403.19949)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Fairness is a critical concern in deep learning, especially in healthcare, where these models influence diagnoses and treatment decisions. Although fairness has been investigated in the vision-only domain, the fairness of medical vision-language (VL) models remains unexplored due to the scarcity of medical VL datasets for studying fairness. To bridge this research gap, we introduce the first fair vision-language medical dataset FairVLMed that provides detailed demographic attributes, ground-truth labels, and clinical notes to facilitate an in-depth examination of fairness within VL foundation models. Using FairVLMed, we conduct a comprehensive fairness analysis of two widely-used VL models (CLIP and BLIP2), pre-trained on both natural and medical domains, across four different protected attributes. Our results highlight significant biases in all VL models, with Asian, Male, Non-Hispanic, and Spanish being the preferred subgroups across the protected attributes of race, gender, ethnicity, and language, respectively. In order to alleviate these biases, we propose FairCLIP, an optimal-transport-based approach that achieves a favorable trade-off between performance and fairness by reducing the Sinkhorn distance between the overall sample distribution and the distributions corresponding to each demographic group. As the first VL dataset of its kind, FairVLMed holds the potential to catalyze advancements in the development of machine learning models that are both ethically aware and clinically effective. Our dataset and code are available at https://ophai.hms.harvard.edu/datasets/fairvlmed10k.</li>
</ul>

<h3>Title: FairRAG: Fair Human Generation via Fair Retrieval Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi Deng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19964">https://arxiv.org/abs/2403.19964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19964">https://arxiv.org/pdf/2403.19964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19964]] FairRAG: Fair Human Generation via Fair Retrieval Augmentation(https://arxiv.org/abs/2403.19964)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrate that FairRAG outperforms existing methods in terms of demographic diversity, image-text alignment, and image fidelity while incurring minimal computational overhead during inference.</li>
</ul>

<h3>Title: NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking  With Depth Completion and Denoising</h3>
<ul>
<li><strong>Authors: </strong>Tianchen Deng, Yanbo Wang, Hongle Xie, Hesheng Wang, Jingchuan Wang, Danwei Wang, Weidong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20034">https://arxiv.org/abs/2403.20034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20034">https://arxiv.org/pdf/2403.20034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20034]] NeSLAM: Neural Implicit Mapping and Self-Supervised Feature Tracking  With Depth Completion and Denoising(https://arxiv.org/abs/2403.20034)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In recent years, there have been significant advancements in 3D reconstruction and dense RGB-D SLAM systems. One notable development is the application of Neural Radiance Fields (NeRF) in these systems, which utilizes implicit neural representation to encode 3D scenes. This extension of NeRF to SLAM has shown promising results. However, the depth images obtained from consumer-grade RGB-D sensors are often sparse and noisy, which poses significant challenges for 3D reconstruction and affects the accuracy of the representation of the scene geometry. Moreover, the original hierarchical feature grid with occupancy value is inaccurate for scene geometry representation. Furthermore, the existing methods select random pixels for camera tracking, which leads to inaccurate localization and is not robust in real-world indoor environments. To this end, we present NeSLAM, an advanced framework that achieves accurate and dense depth estimation, robust camera tracking, and realistic synthesis of novel views. First, a depth completion and denoising network is designed to provide dense geometry prior and guide the neural implicit representation optimization. Second, the occupancy scene representation is replaced with Signed Distance Field (SDF) hierarchical scene representation for high-quality reconstruction and view synthesis. Furthermore, we also propose a NeRF-based self-supervised feature tracking algorithm for robust real-time tracking. Experiments on various indoor datasets demonstrate the effectiveness and accuracy of the system in reconstruction, tracking quality, and novel view synthesis.</li>
</ul>

<h3>Title: Optimal s-boxes against alternative operations</h3>
<ul>
<li><strong>Authors: </strong>Marco Calderini, Roberto Civino, Riccardo Invernizzi</a></li>
<li><strong>Subjects: </strong>cs.CR, math.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20059">https://arxiv.org/abs/2403.20059</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20059">https://arxiv.org/pdf/2403.20059</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20059]] Optimal s-boxes against alternative operations(https://arxiv.org/abs/2403.20059)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Civino et al. have characterised diffusion layers that expose an SPN to vulnerability from differential cryptanalysis when employing alternative operations coming from groups isomorphic to the translation group on the message space. In this study, we present a classification of diffusion layers that exhibit linearity in parallel alternative operations for ciphers with 4-bit s-boxes, enabling the possibility of an alternative differential attack simultaneously targeting all the s-boxes within the block. Furthermore, we investigate the differential behaviour with respect to alternative operations for all classes of optimal 4-bit s-boxes, as defined by Leander and Poschmann (2007). Our examination reveals that certain classes contain weak permutations w.r.t. alternative differential attacks, and we leverage these vulnerabilities to execute a series of experiments.</li>
</ul>

<h3>Title: SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Zhongrui Yu, Haoran Wang, Jinze Yang, Hanzhang Wang, Zeke Xie, Yunfeng Cai, Jiale Cao, Zhong Ji, Mingming Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20079">https://arxiv.org/abs/2403.20079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20079">https://arxiv.org/pdf/2403.20079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20079]] SGD: Street View Synthesis with Gaussian Splatting and Diffusion Prior(https://arxiv.org/abs/2403.20079)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Novel View Synthesis (NVS) for street scenes play a critical role in the autonomous driving simulation. The current mainstream technique to achieve it is neural rendering, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although thrilling progress has been made, when handling street scenes, current methods struggle to maintain rendering quality at the viewpoint that deviates significantly from the training viewpoints. This issue stems from the sparse training views captured by a fixed camera on a moving vehicle. To tackle this problem, we propose a novel approach that enhances the capacity of 3DGS by leveraging prior from a Diffusion Model along with complementary multi-modal data. Specifically, we first fine-tune a Diffusion Model by adding images from adjacent frames as condition, meanwhile exploiting depth data from LiDAR point clouds to supply additional spatial information. Then we apply the Diffusion Model to regularize the 3DGS at unseen views during training. Experimental results validate the effectiveness of our method compared with current state-of-the-art models, and demonstrate its advance in rendering images from broader views.</li>
</ul>

<h3>Title: Mixed-precision Supernet Training from Vision Foundation Models using  Low Rank Adapter</h3>
<ul>
<li><strong>Authors: </strong>Yuiko Sakuma, Masakazu Yoshimura, Junji Otsuka, Atsushi Irie, Takeshi Ohashi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20080">https://arxiv.org/abs/2403.20080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20080">https://arxiv.org/pdf/2403.20080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20080]] Mixed-precision Supernet Training from Vision Foundation Models using  Low Rank Adapter(https://arxiv.org/abs/2403.20080)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Compression of large and performant vision foundation models (VFMs) into arbitrary bit-wise operations (BitOPs) allows their deployment on various hardware. We propose to fine-tune a VFM to a mixed-precision quantized supernet. The supernet-based neural architecture search (NAS) can be adopted for this purpose, which trains a supernet, and then subnets within arbitrary hardware budgets can be extracted. However, existing methods face difficulties in optimizing the mixed-precision search space and incurring large memory costs during training. To tackle these challenges, first, we study the effective search space design for fine-tuning a VFM by comparing different operators (such as resolution, feature size, width, depth, and bit-widths) in terms of performance and BitOPs reduction. Second, we propose memory-efficient supernet training using a low-rank adapter (LoRA) and a progressive training strategy. The proposed method is evaluated for the recently proposed VFM, Segment Anything Model, fine-tuned on segmentation tasks. The searched model yields about a 95% reduction in BitOPs without incurring performance degradation.</li>
</ul>

<h3>Title: FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Barbara Toniella Corradini, Mustafa Shukor, Paul Couairon, Guillaume Couairon, Franco Scarselli, Matthieu Cord</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20105">https://arxiv.org/abs/2403.20105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20105">https://arxiv.org/pdf/2403.20105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20105]] FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion  Models(https://arxiv.org/abs/2403.20105)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>Foundation models have exhibited unprecedented capabilities in tackling many domains and tasks. Models such as CLIP are currently widely used to bridge cross-modal representations, and text-to-image diffusion models are arguably the leading models in terms of realistic image generation. Image generative models are trained on massive datasets that provide them with powerful internal spatial representations. In this work, we explore the potential benefits of such representations, beyond image generation, in particular, for dense visual prediction tasks. We focus on the task of image segmentation, which is traditionally solved by training models on closed-vocabulary datasets, with pixel-level annotations. To avoid the annotation cost or training large diffusion models, we constraint our setup to be zero-shot and training-free. In a nutshell, our pipeline leverages different and relatively small-sized, open-source foundation models for zero-shot open-vocabulary segmentation. The pipeline is as follows: the image is passed to both a captioner model (i.e. BLIP) and a diffusion model (i.e., Stable Diffusion Model) to generate a text description and visual representation, respectively. The features are clustered and binarized to obtain class agnostic masks for each object. These masks are then mapped to a textual class, using the CLIP model to support open-vocabulary. Finally, we add a refinement step that allows to obtain a more precise segmentation mask. Our approach (dubbed FreeSeg-Diff), which does not rely on any training, outperforms many training-based approaches on both Pascal VOC and COCO datasets. In addition, we show very competitive results compared to the recent weakly-supervised segmentation approaches. We provide comprehensive experiments showing the superiority of diffusion model features compared to other pretrained models. Project page: https://bcorrad.github.io/freesegdiff/</li>
</ul>

<h3>Title: Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic  Rewards for Goal-directed Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyeong Park, Jaegyoon Ahn, Jonghwan Choi, Jibum Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20109">https://arxiv.org/abs/2403.20109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20109">https://arxiv.org/pdf/2403.20109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20109]] Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic  Rewards for Goal-directed Molecular Generation(https://arxiv.org/abs/2403.20109)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Optimizing techniques for discovering molecular structures with desired properties is crucial in artificial intelligence(AI)-based drug discovery. Combining deep generative models with reinforcement learning has emerged as an effective strategy for generating molecules with specific properties. Despite its potential, this approach is ineffective in exploring the vast chemical space and optimizing particular chemical properties. To overcome these limitations, we present Mol-AIR, a reinforcement learning-based framework using adaptive intrinsic rewards for effective goal-directed molecular generation. Mol-AIR leverages the strengths of both history-based and learning-based intrinsic rewards by exploiting random distillation network and counting-based strategies. In benchmark tests, Mol-AIR demonstrates superior performance over existing approaches in generating molecules with desired properties without any prior knowledge, including penalized LogP, QED, and celecoxib similarity. We believe that Mol-AIR represents a significant advancement in drug discovery, offering a more efficient path to discovering novel therapeutics.</li>
</ul>

<h3>Title: Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D  Generative Prior</h3>
<ul>
<li><strong>Authors: </strong>Jaehoon Ko, Kyusun Cho, Joungbin Lee, Heeji Yoon, Sangmin Lee, Sangjun Ahn, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20153">https://arxiv.org/abs/2403.20153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20153">https://arxiv.org/pdf/2403.20153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20153]] Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D  Generative Prior(https://arxiv.org/abs/2403.20153)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent methods for audio-driven talking head synthesis often optimize neural radiance fields (NeRF) on a monocular talking portrait video, leveraging its capability to render high-fidelity and 3D-consistent novel-view frames. However, they often struggle to reconstruct complete face geometry due to the absence of comprehensive 3D information in the input monocular videos. In this paper, we introduce a novel audio-driven talking head synthesis framework, called Talk3D, that can faithfully reconstruct its plausible facial geometries by effectively adopting the pre-trained 3D-aware generative prior. Given the personalized 3D generative model, we present a novel audio-guided attention U-Net architecture that predicts the dynamic face variations in the NeRF space driven by audio. Furthermore, our model is further modulated by audio-unrelated conditioning tokens which effectively disentangle variations unrelated to audio features. Compared to existing methods, our method excels in generating realistic facial geometries even under extreme head poses. We also conduct extensive experiments showing our approach surpasses state-of-the-art benchmarks in terms of both quantitative and qualitative evaluations.</li>
</ul>

<h3>Title: Sketch-to-Architecture: Generative AI-aided Architectural Design</h3>
<ul>
<li><strong>Authors: </strong>Pengzhi Li, Baijuan Li, Zhiheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20186">https://arxiv.org/abs/2403.20186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20186">https://arxiv.org/pdf/2403.20186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20186]] Sketch-to-Architecture: Generative AI-aided Architectural Design(https://arxiv.org/abs/2403.20186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, the development of large-scale models has paved the way for various interdisciplinary research, including architecture. By using generative AI, we present a novel workflow that utilizes AI models to generate conceptual floorplans and 3D models from simple sketches, enabling rapid ideation and controlled generation of architectural renderings based on textual descriptions. Our work demonstrates the potential of generative AI in the architectural design process, pointing towards a new direction of computer-aided architectural design. Our project website is available at: https://zrealli.github.io/sketch2arc</li>
</ul>

<h3>Title: Motion Inversion for Video Customization</h3>
<ul>
<li><strong>Authors: </strong>Luozhou Wang, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, Yingcong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20193">https://arxiv.org/abs/2403.20193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20193">https://arxiv.org/pdf/2403.20193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20193]] Motion Inversion for Video Customization(https://arxiv.org/abs/2403.20193)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this research, we present a novel approach to motion customization in video generation, addressing the widespread gap in the thorough exploration of motion representation within video generative models. Recognizing the unique challenges posed by video's spatiotemporal nature, our method introduces Motion Embeddings, a set of explicit, temporally coherent one-dimensional embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity. Our approach offers a compact and efficient solution to motion representation and enables complex manipulations of motion characteristics through vector arithmetic in the embedding space. Furthermore, we identify the Temporal Discrepancy in video generative models, which refers to variations in how different motion modules process temporal relationships between frames. We leverage this understanding to optimize the integration of our motion embeddings. Our contributions include the introduction of a tailored motion embedding for customization tasks, insights into the temporal processing differences in video models, and a demonstration of the practical advantages and effectiveness of our method through extensive experiments.</li>
</ul>

<h3>Title: Unleashing the Potential of Large Language Models for Predictive Tabular  Tasks in Data Science</h3>
<ul>
<li><strong>Authors: </strong>Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20208">https://arxiv.org/abs/2403.20208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20208">https://arxiv.org/pdf/2403.20208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20208]] Unleashing the Potential of Large Language Models for Predictive Tabular  Tasks in Data Science(https://arxiv.org/abs/2403.20208)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improvements over existing benchmarks. These advancements highlight the efficacy of tailoring LLM training to solve table-related problems in data science, thereby establishing a new benchmark in the utilization of LLMs for enhancing tabular intelligence.</li>
</ul>

<h3>Title: Graph Neural Aggregation-diffusion with Metastability</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Cui, Xinyan Wang, Zicheng Zhang, Weichen Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20221">https://arxiv.org/abs/2403.20221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20221">https://arxiv.org/pdf/2403.20221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20221]] Graph Neural Aggregation-diffusion with Metastability(https://arxiv.org/abs/2403.20221)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Continuous graph neural models based on differential equations have expanded the architecture of graph neural networks (GNNs). Due to the connection between graph diffusion and message passing, diffusion-based models have been widely studied. However, diffusion naturally drives the system towards an equilibrium state, leading to issues like over-smoothing. To this end, we propose GRADE inspired by graph aggregation-diffusion equations, which includes the delicate balance between nonlinear diffusion and aggregation induced by interaction potentials. The node representations obtained through aggregation-diffusion equations exhibit metastability, indicating that features can aggregate into multiple clusters. In addition, the dynamics within these clusters can persist for long time periods, offering the potential to alleviate over-smoothing effects. This nonlinear diffusion in our model generalizes existing diffusion-based models and establishes a connection with classical GNNs. We prove that GRADE achieves competitive performance across various benchmarks and alleviates the over-smoothing issue in GNNs evidenced by the enhanced Dirichlet energy.</li>
</ul>

<h3>Title: MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Sanghyun Woo, Kwanyong Park, Inkyu Shin, Myungchul Kim, In So Kweon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20225">https://arxiv.org/abs/2403.20225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20225">https://arxiv.org/pdf/2403.20225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20225]] MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark(https://arxiv.org/abs/2403.20225)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Multi-target multi-camera tracking is a crucial task that involves identifying and tracking individuals over time using video streams from multiple cameras. This task has practical applications in various fields, such as visual surveillance, crowd behavior analysis, and anomaly detection. However, due to the difficulty and cost of collecting and labeling data, existing datasets for this task are either synthetically generated or artificially constructed within a controlled camera network setting, which limits their ability to model real-world dynamics and generalize to diverse camera configurations. To address this issue, we present MTMMC, a real-world, large-scale dataset that includes long video sequences captured by 16 multi-modal cameras in two different environments - campus and factory - across various time, weather, and season conditions. This dataset provides a challenging test-bed for studying multi-camera tracking under diverse real-world complexities and includes an additional input modality of spatially aligned and temporally synchronized RGB and thermal cameras, which enhances the accuracy of multi-camera tracking. MTMMC is a super-set of existing datasets, benefiting independent fields such as person detection, re-identification, and multiple object tracking. We provide baselines and new learning setups on this dataset and set the reference scores for future studies. The datasets, models, and test server will be made publicly available.</li>
</ul>

<h3>Title: Long-Tailed Anomaly Detection with Learnable Class Names</h3>
<ul>
<li><strong>Authors: </strong>Chih-Hui Ho, Kuan-Chuan Peng, Nuno Vasconcelos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20236">https://arxiv.org/abs/2403.20236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20236">https://arxiv.org/pdf/2403.20236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20236]] Long-Tailed Anomaly Detection with Learnable Class Names(https://arxiv.org/abs/2403.20236)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) aims to identify defective images and localize their defects (if any). Ideally, AD models should be able to detect defects over many image classes; without relying on hard-coded class names that can be uninformative or inconsistent across datasets; learn without anomaly supervision; and be robust to the long-tailed distributions of real-world applications. To address these challenges, we formulate the problem of long-tailed AD by introducing several datasets with different levels of class imbalance and metrics for performance evaluation. We then propose a novel method, LTAD, to detect defects from multiple and long-tailed classes, without relying on dataset class names. LTAD combines AD by reconstruction and semantic AD modules. AD by reconstruction is implemented with a transformer-based reconstruction module. Semantic AD is implemented with a binary classifier, which relies on learned pseudo class names and a pretrained foundation model. These modules are learned over two phases. Phase 1 learns the pseudo-class names and a variational autoencoder (VAE) for feature synthesis that augments the training data to combat long-tails. Phase 2 then learns the parameters of the reconstruction and classification modules of LTAD. Extensive experiments using the proposed long-tailed datasets show that LTAD substantially outperforms the state-of-the-art methods for most forms of dataset imbalance. The long-tailed dataset split is available at https://zenodo.org/records/10854201 .</li>
</ul>

<h3>Title: Relation Rectification in Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yinwei Wu, Xingyi Yang, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20249">https://arxiv.org/abs/2403.20249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20249">https://arxiv.org/pdf/2403.20249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20249]] Relation Rectification in Diffusion Model(https://arxiv.org/abs/2403.20249)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite their exceptional generative abilities, large text-to-image diffusion models, much like skilled but careless artists, often struggle with accurately depicting visual relationships between objects. This issue, as we uncover through careful analysis, arises from a misaligned text encoder that struggles to interpret specific relationships and differentiate the logical order of associated objects. To resolve this, we introduce a novel task termed Relation Rectification, aiming to refine the model to accurately represent a given relationship it initially fails to generate. To address this, we propose an innovative solution utilizing a Heterogeneous Graph Convolutional Network (HGCN). It models the directional relationships between relation terms and corresponding objects within the input prompts. Specifically, we optimize the HGCN on a pair of prompts with identical relational words but reversed object orders, supplemented by a few reference images. The lightweight HGCN adjusts the text embeddings generated by the text encoder, ensuring the accurate reflection of the textual relation in the embedding space. Crucially, our method retains the parameters of the text encoder and diffusion model, preserving the model's robust performance on unrelated descriptions. We validated our approach on a newly curated dataset of diverse relational data, demonstrating both quantitative and qualitative enhancements in generating images with precise visual relations. Project page: https://wuyinwei-hah.github.io/rrnet.github.io/.</li>
</ul>

<h3>Title: MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20253">https://arxiv.org/abs/2403.20253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20253">https://arxiv.org/pdf/2403.20253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20253]] MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image  Segmentation(https://arxiv.org/abs/2403.20253)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using text prompts in both zero-shot and weakly supervised settings. To achieve this, we employed a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model and the recent gScoreCAM to generate prompts to obtain segmentation masks from SAM in a zero-shot setting. Additionally, we explored the use of zero-shot segmentation labels in a weakly supervised paradigm to improve the segmentation quality further. By extensively testing three diverse segmentation tasks and medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung X-ray), our proposed framework has demonstrated excellent accuracy.</li>
</ul>

<h3>Title: Benchmarking Counterfactual Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos, Giorgos Papanastasiou, Sotirios A. Tsaftaris</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20287">https://arxiv.org/abs/2403.20287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20287">https://arxiv.org/pdf/2403.20287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20287]] Benchmarking Counterfactual Image Generation(https://arxiv.org/abs/2403.20287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further evaluate and benchmark existing and future counterfactual image generation methods. Our framework is extendable to additional SCM and other causal methods, generative models, and datasets.</li>
</ul>

<h3>Title: Learn "No" to Say "Yes" Better: Improving Vision-Language Models via  Negations</h3>
<ul>
<li><strong>Authors: </strong>Jaisidh Singh, Ishaan Shrivastava, Mayank Vatsa, Richa Singh, Aparna Bharati</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.20312">https://arxiv.org/abs/2403.20312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.20312">https://arxiv.org/pdf/2403.20312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.20312]] Learn "No" to Say "Yes" Better: Improving Vision-Language Models via  Negations(https://arxiv.org/abs/2403.20312)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Existing vision-language models (VLMs) treat text descriptions as a unit, confusing individual concepts in a prompt and impairing visual semantic matching and reasoning. An important aspect of reasoning in logic and language is negations. This paper highlights the limitations of popular VLMs such as CLIP, at understanding the implications of negations, i.e., the effect of the word "not" in a given prompt. To enable evaluation of VLMs on fluent prompts with negations, we present CC-Neg, a dataset containing 228,246 images, true captions and their corresponding negated captions. Using CC-Neg along with modifications to the contrastive loss of CLIP, our proposed CoN-CLIP framework, has an improved understanding of negations. This training paradigm improves CoN-CLIP's ability to encode semantics reliably, resulting in 3.85% average gain in top-1 accuracy for zero-shot image classification across 8 datasets. Further, CoN-CLIP outperforms CLIP on challenging compositionality benchmarks such as SugarCREPE by 4.4%, showcasing emergent compositional understanding of objects, relations, and attributes in text. Overall, our work addresses a crucial limitation of VLMs by introducing a dataset and framework that strengthens semantic associations between images and text, demonstrating improved large-scale foundation models with significantly reduced computational cost, promoting efficiency and accessibility.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
