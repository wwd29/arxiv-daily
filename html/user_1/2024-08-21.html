<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-21</h1>
<h3>Title: A Comprehensive Survey on Diffusion Models and Their Applications</h3>
<ul>
<li><strong>Authors: </strong>Md Manjurul Ahsan, Shivakumar Raman, Yingtao Liu, Zahed Siddique</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10207">https://arxiv.org/abs/2408.10207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10207">https://arxiv.org/pdf/2408.10207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10207]] A Comprehensive Survey on Diffusion Models and Their Applications(https://arxiv.org/abs/2408.10207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Models are probabilistic models that create realistic samples by simulating the diffusion process, gradually adding and removing noise from data. These models have gained popularity in domains such as image processing, speech synthesis, and natural language processing due to their ability to produce high-quality samples. As Diffusion Models are being adopted in various domains, existing literature reviews that often focus on specific areas like computer vision or medical imaging may not serve a broader audience across multiple fields. Therefore, this review presents a comprehensive overview of Diffusion Models, covering their theoretical foundations and algorithmic innovations. We highlight their applications in diverse areas such as media quality, authenticity, synthesis, image transformation, healthcare, and more. By consolidating current knowledge and identifying emerging trends, this review aims to facilitate a deeper understanding and broader adoption of Diffusion Models and provide guidelines for future researchers and practitioners across diverse disciplines.</li>
</ul>

<h3>Title: A Survey on Symbolic Knowledge Distillation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kamal Acharya, Alvaro Velasquez, Houbing Herbert Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10210">https://arxiv.org/abs/2408.10210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10210">https://arxiv.org/pdf/2408.10210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10210]] A Survey on Symbolic Knowledge Distillation of Large Language Models(https://arxiv.org/abs/2408.10210)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This survey paper delves into the emerging and critical area of symbolic knowledge distillation in Large Language Models (LLMs). As LLMs like Generative Pre-trained Transformer-3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient Artificial Intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression towards more accessible and efficient AI systems.</li>
</ul>

<h3>Title: NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance Fields in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Rishit Dagli, Atsuhiro Hibi, Rahul G. Krishnan, Pascal N. Tyrrell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10258">https://arxiv.org/abs/2408.10258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10258">https://arxiv.org/pdf/2408.10258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10258]] NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance Fields in the Wild(https://arxiv.org/abs/2408.10258)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current methods for performing 3D reconstruction and novel view synthesis (NVS) in ultrasound imaging data often face severe artifacts when training NeRF-based approaches. The artifacts produced by current approaches differ from NeRF floaters in general scenes because of the unique nature of ultrasound capture. Furthermore, existing models fail to produce reasonable 3D reconstructions when ultrasound data is captured or obtained casually in uncontrolled environments, which is common in clinical settings. Consequently, existing reconstruction and NVS methods struggle to handle ultrasound motion, fail to capture intricate details, and cannot model transparent and reflective surfaces. In this work, we introduced NeRF-US, which incorporates 3D-geometry guidance for border probability and scattering density into NeRF training, while also utilizing ultrasound-specific rendering over traditional volume rendering. These 3D priors are learned through a diffusion model. Through experiments conducted on our new "Ultrasound in the Wild" dataset, we observed accurate, clinically plausible, artifact-free reconstructions.</li>
</ul>

<h3>Title: Diffusion Model for Planning: A Systematic Literature Review</h3>
<ul>
<li><strong>Authors: </strong>Toshihide Ubukata, Jialong Li, Kenji Tei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10266">https://arxiv.org/abs/2408.10266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10266">https://arxiv.org/pdf/2408.10266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10266]] Diffusion Model for Planning: A Systematic Literature Review(https://arxiv.org/abs/2408.10266)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models, which leverage stochastic processes to capture complex data distributions effectively, have shown their performance as generative models, achieving notable success in image-related tasks through iterative denoising processes. Recently, diffusion models have been further applied and show their strong abilities in planning tasks, leading to a significant growth in related publications since 2023. To help researchers better understand the field and promote the development of the field, we conduct a systematic literature review of recent advancements in the application of diffusion models for planning. Specifically, this paper categorizes and discusses the current literature from the following perspectives: (i) relevant datasets and benchmarks used for evaluating diffusion modelbased planning; (ii) fundamental studies that address aspects such as sampling efficiency; (iii) skill-centric and condition-guided planning for enhancing adaptability; (iv) safety and uncertainty managing mechanism for enhancing safety and robustness; and (v) domain-specific application such as autonomous driving. Finally, given the above literature review, we further discuss the challenges and future directions in this field.</li>
</ul>

<h3>Title: OpenCity: Open Spatio-Temporal Foundation Models for Traffic Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zhonghang Li, Long Xia, Lei Shi, Yong Xu, Dawei Yin, Chao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10269">https://arxiv.org/abs/2408.10269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10269">https://arxiv.org/pdf/2408.10269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10269]] OpenCity: Open Spatio-Temporal Foundation Models for Traffic Prediction(https://arxiv.org/abs/2408.10269)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Accurate traffic forecasting is crucial for effective urban planning and transportation management, enabling efficient resource allocation and enhanced travel experiences. However, existing models often face limitations in generalization, struggling with zero-shot prediction on unseen regions and cities, as well as diminished long-term accuracy. This is primarily due to the inherent challenges in handling the spatial and temporal heterogeneity of traffic data, coupled with the significant distribution shift across time and space. In this work, we aim to unlock new possibilities for building versatile, resilient and adaptive spatio-temporal foundation models for traffic prediction. To achieve this goal, we introduce a novel foundation model, named OpenCity, that can effectively capture and normalize the underlying spatio-temporal patterns from diverse data characteristics, facilitating zero-shot generalization across diverse urban environments. OpenCity integrates the Transformer architecture with graph neural networks to model the complex spatio-temporal dependencies in traffic data. By pre-training OpenCity on large-scale, heterogeneous traffic datasets, we enable the model to learn rich, generalizable representations that can be seamlessly applied to a wide range of traffic forecasting scenarios. Experimental results demonstrate that OpenCity exhibits exceptional zero-shot predictive performance. Moreover, OpenCity showcases promising scaling laws, suggesting the potential for developing a truly one-for-all traffic prediction solution that can adapt to new urban contexts with minimal overhead. We made our proposed OpenCity model open-source and it is available at the following link: this https URL.</li>
</ul>

<h3>Title: FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, Fenglong Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10276">https://arxiv.org/abs/2408.10276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10276">https://arxiv.org/pdf/2408.10276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10276]] FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models(https://arxiv.org/abs/2408.10276)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE) module. This method not only preserves privacy but also enhances the model's ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data.</li>
</ul>

<h3>Title: Leveraging Superfluous Information in Contrastive Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuechu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10292">https://arxiv.org/abs/2408.10292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10292">https://arxiv.org/pdf/2408.10292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10292]] Leveraging Superfluous Information in Contrastive Representation Learning(https://arxiv.org/abs/2408.10292)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Contrastive representation learning, which aims to learnthe shared information between different views of unlabeled data by maximizing the mutual information between them, has shown its powerful competence in self-supervised learning for downstream tasks. However, recent works have demonstrated that more estimated mutual information does not guarantee better performance in different downstream tasks. Such works inspire us to conjecture that the learned representations not only maintain task-relevant information from unlabeled data but also carry task-irrelevant information which is superfluous for downstream tasks, thus leading to performance degeneration. In this paper we show that superfluous information does exist during the conventional contrastive learning framework, and further design a new objective, namely SuperInfo, to learn robust representations by a linear combination of both predictive and superfluous information. Besides, we notice that it is feasible to tune the coefficients of introduced losses to discard task-irrelevant information, while keeping partial non-shared task-relevant information according to our SuperInfo loss.We demonstrate that learning with our loss can often outperform the traditional contrastive learning approaches on image classification, object detection and instance segmentation tasks with significant improvements.</li>
</ul>

<h3>Title: On the Identifiability of Sparse ICA without Assuming Non-Gaussianity</h3>
<ul>
<li><strong>Authors: </strong>Ignavier Ng, Yujia Zheng, Xinshuai Dong, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10353">https://arxiv.org/abs/2408.10353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10353">https://arxiv.org/pdf/2408.10353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10353]] On the Identifiability of Sparse ICA without Assuming Non-Gaussianity(https://arxiv.org/abs/2408.10353)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Independent component analysis (ICA) is a fundamental statistical tool used to reveal hidden generative processes from observed data. However, traditional ICA approaches struggle with the rotational invariance inherent in Gaussian distributions, often necessitating the assumption of non-Gaussianity in the underlying sources. This may limit their applicability in broader contexts. To accommodate Gaussian sources, we develop an identifiability theory that relies on second-order statistics without imposing further preconditions on the distribution of sources, by introducing novel assumptions on the connective structure from sources to observed variables. Different from recent work that focuses on potentially restrictive connective structures, our proposed assumption of structural variability is both considerably less restrictive and provably necessary. Furthermore, we propose two estimation methods based on second-order statistics and sparsity constraint. Experimental results are provided to validate our identifiability theory and estimation methods.</li>
</ul>

<h3>Title: Understanding Generative AI Content with Embedding Models</h3>
<ul>
<li><strong>Authors: </strong>Max Vargas, Reilly Cannon, Andrew Engel, Anand D. Sarwate, Tony Chiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10437">https://arxiv.org/abs/2408.10437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10437">https://arxiv.org/pdf/2408.10437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10437]] Understanding Generative AI Content with Embedding Models(https://arxiv.org/abs/2408.10437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The construction of high-quality numerical features is critical to any quantitative data analysis. Feature engineering has been historically addressed by carefully hand-crafting data representations based on domain expertise. This work views the internal representations of modern deep neural networks (DNNs), called embeddings, as an automated form of traditional feature engineering. For trained DNNs, we show that these embeddings can reveal interpretable, high-level concepts in unstructured sample data. We use these embeddings in natural language and computer vision tasks to uncover both inherent heterogeneity in the underlying data and human-understandable explanations for it. In particular, we find empirical evidence that there is inherent separability between real data and that generated from AI models.</li>
</ul>

<h3>Title: The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks</h3>
<ul>
<li><strong>Authors: </strong>Niyar R Barman, Krish Sharma, Ashhar Aziz, Shashwat Bajpai, Shwetangshu Biswas, Vasu Sharma, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10446">https://arxiv.org/abs/2408.10446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10446">https://arxiv.org/pdf/2408.10446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10446]] The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks(https://arxiv.org/abs/2408.10446)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The rapid advancement of text-to-image generation systems, exemplified by models like Stable Diffusion, Midjourney, Imagen, and DALL-E, has heightened concerns about their potential misuse. In response, companies like Meta and Google have intensified their efforts to implement watermarking techniques on AI-generated images to curb the circulation of potentially misleading visuals. However, in this paper, we argue that current image watermarking methods are fragile and susceptible to being circumvented through visual paraphrase attacks. The proposed visual paraphraser operates in two steps. First, it generates a caption for the given image using KOSMOS-2, one of the latest state-of-the-art image captioning systems. Second, it passes both the original image and the generated caption to an image-to-image diffusion system. During the denoising step of the diffusion pipeline, the system generates a visually similar image that is guided by the text caption. The resulting image is a visual paraphrase and is free of any watermarks. Our empirical findings demonstrate that visual paraphrase attacks can effectively remove watermarks from images. This paper provides a critical assessment, empirically revealing the vulnerability of existing watermarking techniques to visual paraphrase attacks. While we do not propose solutions to this issue, this paper serves as a call to action for the scientific community to prioritize the development of more robust watermarking techniques. Our first-of-its-kind visual paraphrase dataset and accompanying code are publicly available.</li>
</ul>

<h3>Title: Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Liu He, Yizhi Song, Hejun Huang, Daniel Aliaga, Xin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10453">https://arxiv.org/abs/2408.10453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10453">https://arxiv.org/pdf/2408.10453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10453]] Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation(https://arxiv.org/abs/2408.10453)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-video generation has been dominated by end-to-end diffusion-based or autoregressive models. On one hand, those novel models provide plausible versatility, but they are criticized for physical correctness, shading and illumination, camera motion, and temporal consistency. On the other hand, film industry relies on manually-edited Computer-Generated Imagery (CGI) using 3D modeling software. Human-directed 3D synthetic videos and animations address the aforementioned shortcomings, but it is extremely tedious and requires tight collaboration between movie makers and 3D rendering experts. In this paper, we introduce an automatic synthetic video generation pipeline based on Vision Large Language Model (VLM) agent collaborations. Given a natural language description of a video, multiple VLM agents auto-direct various processes of the generation pipeline. They cooperate to create Blender scripts which render a video that best aligns with the given description. Based on film making inspiration and augmented with Blender-based movie making knowledge, the Director agent decomposes the input text-based video description into sub-processes. For each sub-process, the Programmer agent produces Python-based Blender scripts based on customized function composing and API calling. Then, the Reviewer agent, augmented with knowledge of video reviewing, character motion coordinates, and intermediate screenshots uses its compositional reasoning ability to provide feedback to the Programmer agent. The Programmer agent iteratively improves the scripts to yield the best overall video outcome. Our generated videos show better quality than commercial video generation models in 5 metrics on video quality and instruction-following performance. Moreover, our framework outperforms other approaches in a comprehensive user study on quality, consistency, and rationality.</li>
</ul>

<h3>Title: Learning Multimodal Latent Space with EBM Prior and MCMC Inference</h3>
<ul>
<li><strong>Authors: </strong>Shiyu Yuan, Carlo Lipizzi, Tian Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10467">https://arxiv.org/abs/2408.10467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10467">https://arxiv.org/pdf/2408.10467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10467]] Learning Multimodal Latent Space with EBM Prior and MCMC Inference(https://arxiv.org/abs/2408.10467)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal generative models are crucial for various applications. We propose an approach that combines an expressive energy-based model (EBM) prior with Markov Chain Monte Carlo (MCMC) inference in the latent space for multimodal generation. The EBM prior acts as an informative guide, while MCMC inference, specifically through short-run Langevin dynamics, brings the posterior distribution closer to its true form. This method not only provides an expressive prior to better capture the complexity of multimodality but also improves the learning of shared latent variables for more coherent generation across modalities. Our proposed method is supported by empirical experiments, underscoring the effectiveness of our EBM prior with MCMC inference in enhancing cross-modal and joint generative tasks in multimodal contexts.</li>
</ul>

<h3>Title: QUITO-X: An Information Bottleneck-based Compression Algorithm with Cross-Attention</h3>
<ul>
<li><strong>Authors: </strong>Yihang Wang, Xu Huang, Bowen Tian, Yixing Fan, Jiafeng Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10497">https://arxiv.org/abs/2408.10497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10497">https://arxiv.org/pdf/2408.10497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10497]] QUITO-X: An Information Bottleneck-based Compression Algorithm with Cross-Attention(https://arxiv.org/abs/2408.10497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative LLM have achieved significant success in various industrial tasks and can effectively adapt to vertical domains and downstream tasks through ICL. However, with tasks becoming increasingly complex, the context length required by ICL is also getting longer, and two significant issues arise: (i) The excessively long context leads to high costs and inference delays. (ii) A substantial amount of task-irrelevant information introduced by long contexts exacerbates the "lost in the middle" problem. Recently, compressing prompts by removing tokens according to some metric obtained from some causal language models, such as llama-7b, has emerged as an effective approach to mitigate these issues. However, the metric used by prior method such as self-information or PPL do not fully align with the objective of distinuishing the most important tokens when conditioning on query. In this work, we introduce information bottleneck theory to carefully examine the properties required by the metric. Inspired by this, we use cross-attention in encoder-decoder architecture as a new metric. Our simple method leads to significantly better performance in smaller models with lower latency. We evaluate our method on four datasets: DROP, CoQA, SQuAD, and Quoref. The experimental results show that, while maintaining the same performance, our compression rate can improve by nearly 25% over previous SOTA. Remarkably, in experiments where 25% of the tokens are removed, our model's EM score for answers sometimes even exceeds that of the control group using uncompressed text as context.</li>
</ul>

<h3>Title: FAGStyle: Feature Augmentation on Geodesic Surface for Zero-shot Text-guided Diffusion Image Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Yuexing Han, Liheng Ruan, Bing Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10533">https://arxiv.org/abs/2408.10533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10533">https://arxiv.org/pdf/2408.10533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10533]] FAGStyle: Feature Augmentation on Geodesic Surface for Zero-shot Text-guided Diffusion Image Style Transfer(https://arxiv.org/abs/2408.10533)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The goal of image style transfer is to render an image guided by a style reference while maintaining the original content. Existing image-guided methods rely on specific style reference images, restricting their wider application and potentially compromising result quality. As a flexible alternative, text-guided methods allow users to describe the desired style using text prompts. Despite their versatility, these methods often struggle with maintaining style consistency, reflecting the described style accurately, and preserving the content of the target image. To address these challenges, we introduce FAGStyle, a zero-shot text-guided diffusion image style transfer method. Our approach enhances inter-patch information interaction by incorporating the Sliding Window Crop technique and Feature Augmentation on Geodesic Surface into our style control loss. Furthermore, we integrate a Pre-Shape self-correlation consistency loss to ensure content consistency. FAGStyle demonstrates superior performance over existing methods, consistently achieving stylization that retains the semantic content of the source image. Experimental results confirms the efficacy of FAGStyle across a diverse range of source contents and styles, both imagined and common.</li>
</ul>

<h3>Title: Diff-PCC: Diffusion-based Neural Compression for 3D Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Kai Liu, Kang You, Pan Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10543">https://arxiv.org/abs/2408.10543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10543">https://arxiv.org/pdf/2408.10543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10543]] Diff-PCC: Diffusion-based Neural Compression for 3D Point Clouds(https://arxiv.org/abs/2408.10543)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Stable diffusion networks have emerged as a groundbreaking development for their ability to produce realistic and detailed visual content. This characteristic renders them ideal decoders, capable of producing high-quality and aesthetically pleasing reconstructions. In this paper, we introduce the first diffusion-based point cloud compression method, dubbed Diff-PCC, to leverage the expressive power of the diffusion model for generative and aesthetically superior decoding. Different from the conventional autoencoder fashion, a dual-space latent representation is devised in this paper, in which a compressor composed of two independent encoding backbones is considered to extract expressive shape latents from distinct latent spaces. At the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by considering the shape latents as guidance to stochastically denoise the noisy point clouds. Experiments demonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining superior subjective quality. Source code will be made publicly available.</li>
</ul>

<h3>Title: Speech Representation Learning Revisited: The Necessity of Separate Learnable Parameters and Robust Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Hemant Yadav, Sunayana Sitaram, Rajiv Ratn Shah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10557">https://arxiv.org/abs/2408.10557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10557">https://arxiv.org/pdf/2408.10557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10557]] Speech Representation Learning Revisited: The Necessity of Separate Learnable Parameters and Robust Data Augmentation(https://arxiv.org/abs/2408.10557)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Speech modeling methods learn one embedding for a fixed segment of speech, typically in between 10-25 ms. The information present in speech can be divided into two categories: "what is being said" (content) and "how it is expressed" (other) and these two are orthogonal in nature causing the optimization algorithm to find a sub-optimal solution if forced to optimize together. This leads to sub-optimal performance in one or all downstream tasks as shown by previous studies. Current self-supervised learning (SSL) methods such as HuBERT are very good at modeling the content information present in speech. Data augmentation improves the performance on tasks which require effective modeling of other information but this leads to a divided capacity of the model. In this work, we conduct a preliminary study to understand the importance of modeling other information using separate learnable parameters. We propose a modified version of HuBERT, termed Other HuBERT (O-HuBERT), to test our hypothesis. Our findings are twofold: first, the O-HuBERT method is able to utilize all layers to build complex features to encode other information; second, a robust data augmentation strategy is essential for learning the information required by tasks that depend on other information and to achieve state-of-the-art (SOTA) performance on the SUPERB benchmark with a similarly sized model (100 million parameters) and pre-training data (960 hours).</li>
</ul>

<h3>Title: Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Cong Wan, Yuhang He, Xiang Song, Yihong Gong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10571">https://arxiv.org/abs/2408.10571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10571">https://arxiv.org/pdf/2408.10571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10571]] Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models(https://arxiv.org/abs/2408.10571)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized customized text-to-image generation, allowing for efficient synthesis of photos from personal data with textual descriptions. However, these advancements bring forth risks including privacy breaches and unauthorized replication of artworks. Previous researches primarily center around using prompt-specific methods to generate adversarial examples to protect personal images, yet the effectiveness of existing methods is hindered by constrained adaptability to different prompts. In this paper, we introduce a Prompt-Agnostic Adversarial Perturbation (PAP) method for customized diffusion models. PAP first models the prompt distribution using a Laplace Approximation, and then produces prompt-agnostic perturbations by maximizing a disturbance expectation based on the modeled distribution. This approach effectively tackles the prompt-agnostic attacks, leading to improved defense stability. Extensive experiments in face privacy and artistic style protection, demonstrate the superior generalization of our method in comparison to existing techniques.</li>
</ul>

<h3>Title: Breast tumor classification based on self-supervised contrastive learning from ultrasound videos</h3>
<ul>
<li><strong>Authors: </strong>Yunxin Tang, Siyuan Tang, Jian Zhang, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10600">https://arxiv.org/abs/2408.10600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10600">https://arxiv.org/pdf/2408.10600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10600]] Breast tumor classification based on self-supervised contrastive learning from ultrasound videos(https://arxiv.org/abs/2408.10600)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Background: Breast ultrasound is prominently used in diagnosing breast tumors. At present, many automatic systems based on deep learning have been developed to help radiologists in diagnosis. However, training such systems remains challenging because they are usually data-hungry and demand amounts of labeled data, which need professional knowledge and are expensive. Methods: We adopted a triplet network and a self-supervised contrastive learning technique to learn representations from unlabeled breast ultrasound video clips. We further designed a new hard triplet loss to to learn representations that particularly discriminate positive and negative image pairs that are hard to recognize. We also constructed a pretraining dataset from breast ultrasound videos (1,360 videos from 200 patients), which includes an anchor sample dataset with 11,805 images, a positive sample dataset with 188,880 images, and a negative sample dataset dynamically generated from video clips. Further, we constructed a finetuning dataset, including 400 images from 66 patients. We transferred the pretrained network to a downstream benign/malignant classification task and compared the performance with other state-of-the-art models, including three models pretrained on ImageNet and a previous contrastive learning model retrained on our datasets. Results and conclusion: Experiments revealed that our model achieved an area under the receiver operating characteristic curve (AUC) of 0.952, which is significantly higher than the others. Further, we assessed the dependence of our pretrained model on the number of labeled data and revealed that <100 samples were required to achieve an AUC of 0.901. The proposed framework greatly reduces the demand for labeled data and holds potential for use in automatic breast ultrasound image diagnosis.</li>
</ul>

<h3>Title: MUSES: 3D-Controllable Image Generation via Multi-Modal Agent Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Yanbo Ding, Shaobin Zhuang, Kunchang Li, Zhengrong Yue, Yu Qiao, Yali Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10605">https://arxiv.org/abs/2408.10605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10605">https://arxiv.org/pdf/2408.10605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10605]] MUSES: 3D-Controllable Image Generation via Multi-Modal Agent Collaboration(https://arxiv.org/abs/2408.10605)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite recent advancements in text-to-image generation, most existing methods struggle to create images with multiple objects and complex spatial relationships in 3D world. To tackle this limitation, we introduce a generic AI system, namely MUSES, for 3D-controllable image generation from user queries. Specifically, our MUSES addresses this challenging task by developing a progressive workflow with three key components, including (1) Layout Manager for 2D-to-3D layout lifting, (2) Model Engineer for 3D object acquisition and calibration, (3) Image Artist for 3D-to-2D image rendering. By mimicking the collaboration of human professionals, this multi-modal agent pipeline facilitates the effective and automatic creation of images with 3D-controllable objects, through an explainable integration of top-down planning and bottom-up generation. Additionally, we find that existing benchmarks lack detailed descriptions of complex 3D spatial relationships of multiple objects. To fill this gap, we further construct a new benchmark of T2I-3DisBench (3D image scene), which describes diverse 3D image scenes with 50 detailed prompts. Extensive experiments show the state-of-the-art performance of MUSES on both T2I-CompBench and T2I-3DisBench, outperforming recent strong competitors such as DALL-E 3 and Stable Diffusion 3. These results demonstrate a significant step of MUSES forward in bridging natural language, 2D image generation, and 3D world.</li>
</ul>

<h3>Title: Novel Change Detection Framework in Remote Sensing Imagery Using Diffusion Models and Structural Similarity Index (SSIM)</h3>
<ul>
<li><strong>Authors: </strong>Andrew Kiruluta, Eric Lundy, Andreas Lemos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10619">https://arxiv.org/abs/2408.10619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10619">https://arxiv.org/pdf/2408.10619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10619]] Novel Change Detection Framework in Remote Sensing Imagery Using Diffusion Models and Structural Similarity Index (SSIM)(https://arxiv.org/abs/2408.10619)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Change detection is a crucial task in remote sensing, enabling the monitoring of environmental changes, urban growth, and disaster impact. Conventional change detection techniques, such as image differencing and ratioing, often struggle with noise and fail to capture complex variations in imagery. Recent advancements in machine learning, particularly generative models like diffusion models, offer new opportunities for enhancing change detection accuracy. In this paper, we propose a novel change detection framework that combines the strengths of Stable Diffusion models with the Structural Similarity Index (SSIM) to create robust and interpretable change maps. Our approach, named Diffusion Based Change Detector, is evaluated on both synthetic and real-world remote sensing datasets and compared with state-of-the-art methods. The results demonstrate that our method significantly outperforms traditional differencing techniques and recent deep learning-based methods, particularly in scenarios with complex changes and noise.</li>
</ul>

<h3>Title: TextMastero: Mastering High-Quality Scene Text Editing in Diverse Languages and Styles</h3>
<ul>
<li><strong>Authors: </strong>Tong Wang, Xiaochao Qu, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10623">https://arxiv.org/abs/2408.10623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10623">https://arxiv.org/pdf/2408.10623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10623]] TextMastero: Mastering High-Quality Scene Text Editing in Diverse Languages and Styles(https://arxiv.org/abs/2408.10623)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Scene text editing aims to modify texts on images while maintaining the style of newly generated text similar to the original. Given an image, a target area, and target text, the task produces an output image with the target text in the selected area, replacing the original. This task has been studied extensively, with initial success using Generative Adversarial Networks (GANs) to balance text fidelity and style similarity. However, GAN-based methods struggled with complex backgrounds or text styles. Recent works leverage diffusion models, showing improved results, yet still face challenges, especially with non-Latin languages like CJK characters (Chinese, Japanese, Korean) that have complex glyphs, often producing inaccurate or unrecognizable characters. To address these issues, we present \emph{TextMastero} - a carefully designed multilingual scene text editing architecture based on latent diffusion models (LDMs). TextMastero introduces two key modules: a glyph conditioning module for fine-grained content control in generating accurate texts, and a latent guidance module for providing comprehensive style information to ensure similarity before and after editing. Both qualitative and quantitative experiments demonstrate that our method surpasses all known existing works in text fidelity and style similarity.</li>
</ul>

<h3>Title: Tensor tree learns hidden relational structures in data to construct generative models</h3>
<ul>
<li><strong>Authors: </strong>Kenji Harada, Tsuyoshi Okubo, Naoki Kawashima</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, cs.AI, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10669">https://arxiv.org/abs/2408.10669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10669">https://arxiv.org/pdf/2408.10669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10669]] Tensor tree learns hidden relational structures in data to construct generative models(https://arxiv.org/abs/2408.10669)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Based on the tensor tree network with the Born machine framework, we propose a general method for constructing a generative model by expressing the target distribution function as the quantum wave function amplitude represented by a tensor tree. The key idea is dynamically optimizing the tree structure that minimizes the bond mutual information. The proposed method offers enhanced performance and uncovers hidden relational structures in the target data. We illustrate potential practical applications with four examples: (i) random patterns, (ii) QMNIST hand-written digits, (iii) Bayesian networks, and (iv) the stock price fluctuation pattern in S&P500. In (i) and (ii), strongly correlated variables were concentrated near the center of the network; in (iii), the causality pattern was identified; and, in (iv), a structure corresponding to the eleven sectors emerged.</li>
</ul>

<h3>Title: A Noncontact Technique for Wave Measurement Based on Thermal Stereography and Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Deyu Li, Longfei Xiao, Handi Wei, Yan Li, Binghua Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10670">https://arxiv.org/abs/2408.10670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10670">https://arxiv.org/pdf/2408.10670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10670]] A Noncontact Technique for Wave Measurement Based on Thermal Stereography and Deep Learning(https://arxiv.org/abs/2408.10670)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The accurate measurement of the wave field and its spatiotemporal evolution is essential in many hydrodynamic experiments and engineering applications. The binocular stereo imaging technique has been widely used to measure waves. However, the optical properties of indoor water surfaces, including transparency, specular reflection, and texture absence, pose challenges for image processing and stereo reconstruction. This study proposed a novel technique that combined thermal stereography and deep learning to achieve fully noncontact wave measurements. The optical imaging properties of water in the long-wave infrared spectrum were found to be suitable for stereo matching, effectively avoiding the issues in the visible-light spectrum. After capturing wave images using thermal stereo cameras, a reconstruction strategy involving deep learning techniques was proposed to improve stereo matching performance. A generative approach was employed to synthesize a dataset with ground-truth disparity from unannotated infrared images. This dataset was then fed to a pretrained stereo neural network for fine-tuning to achieve domain adaptation. Wave flume experiments were conducted to validate the feasibility and accuracy of the proposed technique. The final reconstruction results indicated great agreement and high accuracy with a mean bias of less than 2.1% compared with the measurements obtained using wave probes, suggesting that the novel technique effectively measures the spatiotemporal distribution of wave surface in hydrodynamic experiments.</li>
</ul>

<h3>Title: Iterative Window Mean Filter: Thwarting Diffusion-based Adversarial Purification</h3>
<ul>
<li><strong>Authors: </strong>Hanrui Wang, Ruoxi Sun, Cunjian Chen, Minhui Xue, Lay-Ki Soon, Shuo Wang, Zhe Jin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10673">https://arxiv.org/abs/2408.10673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10673">https://arxiv.org/pdf/2408.10673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10673]] Iterative Window Mean Filter: Thwarting Diffusion-based Adversarial Purification(https://arxiv.org/abs/2408.10673)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Face authentication systems have brought significant convenience and advanced developments, yet they have become unreliable due to their sensitivity to inconspicuous perturbations, such as adversarial attacks. Existing defenses often exhibit weaknesses when facing various attack algorithms and adaptive attacks or compromise accuracy for enhanced security. To address these challenges, we have developed a novel and highly efficient non-deep-learning-based image filter called the Iterative Window Mean Filter (IWMF) and proposed a new framework for adversarial purification, named IWMF-Diff, which integrates IWMF and denoising diffusion models. These methods can function as pre-processing modules to eliminate adversarial perturbations without necessitating further modifications or retraining of the target system. We demonstrate that our proposed methodologies fulfill four critical requirements: preserved accuracy, improved security, generalizability to various threats in different settings, and better resistance to adaptive attacks. This performance surpasses that of the state-of-the-art adversarial purification method, DiffPure.</li>
</ul>

<h3>Title: Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on Whisper</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Xu, Kaixun Huang, Pengcheng Guo, Yu Zhou, Longtao Huang, Hui Xue, Lei Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10680">https://arxiv.org/abs/2408.10680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10680">https://arxiv.org/pdf/2408.10680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10680]] Towards Rehearsal-Free Multilingual ASR: A LoRA-based Case Study on Whisper(https://arxiv.org/abs/2408.10680)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Pre-trained multilingual speech foundation models, like Whisper, have shown impressive performance across different languages. However, adapting these models to new or specific languages is computationally extensive and faces catastrophic forgetting problems. Addressing these issues, our study investigates strategies to enhance the model on new languages in the absence of original training data, while also preserving the established performance on the original languages. Specifically, we first compare various LoRA-based methods to find out their vulnerability to forgetting. To mitigate this issue, we propose to leverage the LoRA parameters from the original model for approximate orthogonal gradient descent on the new samples. Additionally, we also introduce a learnable rank coefficient to allocate trainable parameters for more efficient training. Our experiments with a Chinese Whisper model (for Uyghur and Tibetan) yield better results with a more compact parameter set.</li>
</ul>

<h3>Title: AnyGraph: Graph Foundation Model in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Lianghao Xia, Chao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10700">https://arxiv.org/abs/2408.10700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10700">https://arxiv.org/pdf/2408.10700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10700]] AnyGraph: Graph Foundation Model in the Wild(https://arxiv.org/abs/2408.10700)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The growing ubiquity of relational data structured as graphs has underscored the need for graph learning models with exceptional generalization capabilities. However, current approaches often struggle to effectively extract generalizable insights, frequently requiring extensive fine-tuning and limiting their versatility. Graph foundation models offer a transformative solution, with the potential to learn robust, generalizable representations from graph data. This enables more effective and adaptable applications across a wide spectrum of tasks and domains. In this work, we investigate a unified graph model, AnyGraph, designed to handle key challenges: i) Structure Heterogenity. Addressing distribution shift in graph structural information; ii) Feature Heterogenity. Handling diverse feature representation spaces across graph datasets; iii) Fast Adaptation. Efficiently adapting the model to new graph domains; iv) Scaling Law Emergence. Enabling the model to exhibit scaling law behavior, where its performance scales favorably with the amount of data and parameter sizes. To tackle these critical challenges, we build the AnyGraph upon a Graph Mixture-of-Experts (MoE) architecture. This approach empowers the model to effectively manage both the in-domain and cross-domain distribution shift concerning structure-level and feature-level heterogeneity. Furthermore, a lightweight graph expert routing mechanism is proposed to facilitate AnyGraph's fast adaptability to new data and domains. Our extensive experiments on diverse 38 graph datasets have demonstrated the strong zero-shot learning performance of AnyGraph across diverse graph domains with significant distribution shift. Furthermore, we have validated the model's fast adaptation ability and scaling law emergence, showcasing its versatility.</li>
</ul>

<h3>Title: Large Language Models for Multimodal Deformable Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Ma, Weijie Wang, Jie Ning, Jianfeng He, Nicu Sebe, Bruno Lepri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10703">https://arxiv.org/abs/2408.10703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10703">https://arxiv.org/pdf/2408.10703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10703]] Large Language Models for Multimodal Deformable Image Registration(https://arxiv.org/abs/2408.10703)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The challenge of Multimodal Deformable Image Registration (MDIR) lies in the conversion and alignment of features between images of different modalities. Generative models (GMs) cannot retain the necessary information enough from the source modality to the target one, while non-GMs struggle to align features across these two modalities. In this paper, we propose a novel coarse-to-fine MDIR framework,LLM-Morph, which is applicable to various pre-trained Large Language Models (LLMs) to solve these concerns by aligning the deep features from different modal medical images. Specifically, we first utilize a CNN encoder to extract deep visual features from cross-modal image pairs, then we use the first adapter to adjust these tokens, and use LoRA in pre-trained LLMs to fine-tune their weights, both aimed at eliminating the domain gap between the pre-trained LLMs and the MDIR task. Third, for the alignment of tokens, we utilize other four adapters to transform the LLM-encoded tokens into multi-scale visual features, generating multi-scale deformation fields and facilitating the coarse-to-fine MDIR task. Extensive experiments in MR-CT Abdomen and SR-Reg Brain datasets demonstrate the effectiveness of our framework and the potential of pre-trained LLMs for MDIR task. Our code is availabel at: this https URL.</li>
</ul>

<h3>Title: Towards Foundation Models for the Industrial Forecasting of Chemical Kinetics</h3>
<ul>
<li><strong>Authors: </strong>Imran Nasim, Joa Lucas de Sousa Almeida</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10720">https://arxiv.org/abs/2408.10720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10720">https://arxiv.org/pdf/2408.10720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10720]] Towards Foundation Models for the Industrial Forecasting of Chemical Kinetics(https://arxiv.org/abs/2408.10720)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Scientific Machine Learning is transforming traditional engineering industries by enhancing the efficiency of existing technologies and accelerating innovation, particularly in modeling chemical reactions. Despite recent advancements, the issue of solving stiff chemically reacting problems within computational fluid dynamics remains a significant issue. In this study we propose a novel approach utilizing a multi-layer-perceptron mixer architecture (MLP-Mixer) to model the time-series of stiff chemical kinetics. We evaluate this method using the ROBER system, a benchmark model in chemical kinetics, to compare its performance with traditional numerical techniques. This study provides insight into the industrial utility of the recently developed MLP-Mixer architecture to model chemical kinetics and provides motivation for such neural architecture to be used as a base for time-series foundation models.</li>
</ul>

<h3>Title: MEGen: Generative Backdoor in Large Language Models via Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Jiyang Qiu, Xinbei Ma, Zhuosheng Zhang, Hai Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10722">https://arxiv.org/abs/2408.10722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10722">https://arxiv.org/pdf/2408.10722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10722]] MEGen: Generative Backdoor in Large Language Models via Model Editing(https://arxiv.org/abs/2408.10722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities. Their powerful generative abilities enable flexible responses based on various queries or instructions. Emerging as widely adopted generalists for diverse tasks, LLMs are still vulnerable to backdoors. This paper proposes an editing-based generative backdoor, named MEGen, aiming to create a customized backdoor for NLP tasks with the least side effects. In our approach, we first leverage a language model to insert a trigger selected on fixed metrics into the input, then design a pipeline of model editing to directly embed a backdoor into an LLM. By adjusting a small set of local parameters with a mini-batch of samples, MEGen significantly enhances time efficiency and achieves high robustness. Experimental results indicate that our backdoor attack strategy achieves a high attack success rate on poison data while maintaining the model's performance on clean data. Notably, the backdoored model, when triggered, can freely output pre-set dangerous information while successfully completing downstream tasks. This suggests that future LLM applications could be guided to deliver certain dangerous information, thus altering the LLM's generative style. We believe this approach provides insights for future LLM applications and the execution of backdoor attacks on conversational AI systems.</li>
</ul>

<h3>Title: Generating Synthetic Fair Syntax-agnostic Data by Learning and Distilling Fair Representation</h3>
<ul>
<li><strong>Authors: </strong>Md Fahim Sikder, Resmi Ramachandranpillai, Daniel de Leng, Fredrik Heintz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10755">https://arxiv.org/abs/2408.10755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10755">https://arxiv.org/pdf/2408.10755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10755]] Generating Synthetic Fair Syntax-agnostic Data by Learning and Distilling Fair Representation(https://arxiv.org/abs/2408.10755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data Fairness is a crucial topic due to the recent wide usage of AI powered applications. Most of the real-world data is filled with human or machine biases and when those data are being used to train AI models, there is a chance that the model will reflect the bias in the training data. Existing bias-mitigating generative methods based on GANs, Diffusion models need in-processing fairness objectives and fail to consider computational overhead while choosing computationally-heavy architectures, which may lead to high computational demands, instability and poor optimization performance. To mitigate this issue, in this work, we present a fair data generation technique based on knowledge distillation, where we use a small architecture to distill the fair representation in the latent space. The idea of fair latent space distillation enables more flexible and stable training of Fair Generative Models (FGMs). We first learn a syntax-agnostic (for any data type) fair representation of the data, followed by distillation in the latent space into a smaller model. After distillation, we use the distilled fair latent space to generate high-fidelity fair synthetic data. While distilling, we employ quality loss (for fair distillation) and utility loss (for data utility) to ensure that the fairness and data utility characteristics remain in the distilled latent space. Our approaches show a 5%, 5% and 10% rise in performance in fairness, synthetic sample quality and data utility, respectively, than the state-of-the-art fair generative model.</li>
</ul>

<h3>Title: Generative AI in Industrial Machine Vision -- A Review</h3>
<ul>
<li><strong>Authors: </strong>Hans Aoyang Zhou, Dominik Wolfschlger, Constantinos Florides, Jonas Werheid, Hannes Behnen, Jan-Henrick Woltersmann, Tiago C. Pinto, Marco Kemmerling, Anas Abdelrazeq, Robert H. Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10775">https://arxiv.org/abs/2408.10775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10775">https://arxiv.org/pdf/2408.10775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10775]] Generative AI in Industrial Machine Vision -- A Review(https://arxiv.org/abs/2408.10775)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine vision enhances automation, quality control, and operational efficiency in industrial applications by enabling machines to interpret and act on visual data. While traditional computer vision algorithms and approaches remain widely utilized, machine learning has become pivotal in current research activities. In particular, generative \gls*{AI} demonstrates promising potential by improving pattern recognition capabilities, through data augmentation, increasing image resolution, and identifying anomalies for quality control. However, the application of generative \gls*{AI} in machine vision is still in its early stages due to challenges in data diversity, computational requirements, and the necessity for robust validation methods. A comprehensive literature review is essential to understand the current state of generative \gls*{AI} in industrial machine vision, focusing on recent advancements, applications, and research trends. Thus, a literature review based on the PRISMA guidelines was conducted, analyzing over 1,200 papers on generative \gls*{AI} in industrial machine vision. Our findings reveal various patterns in current research, with the primary use of generative \gls*{AI} being data augmentation, for machine vision tasks such as classification and object detection. Furthermore, we gather a collection of application challenges together with data requirements to enable a successful application of generative \gls*{AI} in industrial machine vision. This overview aims to provide researchers with insights into the different areas and applications within current research, highlighting significant advancements and identifying opportunities for future work.</li>
</ul>

<h3>Title: Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains</h3>
<ul>
<li><strong>Authors: </strong>Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10819">https://arxiv.org/abs/2408.10819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10819">https://arxiv.org/pdf/2408.10819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10819]] Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains(https://arxiv.org/abs/2408.10819)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Knowledge graph completion (KGC) aims to identify missing triples in a knowledge graph (KG). This is typically achieved through tasks such as link prediction and instance completion. However, these methods often focus on either static knowledge graphs (SKGs) or temporal knowledge graphs (TKGs), addressing only within-scope triples. This paper introduces a new generative completion framework called Generative Subgraph-based KGC (GS-KGC). GS-KGC employs a question-answering format to directly generate target entities, addressing the challenge of questions having multiple possible answers. We propose a strategy that extracts subgraphs centered on entities and relationships within the KG, from which negative samples and neighborhood information are separately obtained to address the one-to-many problem. Our method generates negative samples using known facts to facilitate the discovery of new information. Furthermore, we collect and refine neighborhood path data of known entities, providing contextual information to enhance reasoning in large language models (LLMs). Our experiments evaluated the proposed method on four SKGs and two TKGs, achieving state-of-the-art Hits@1 metrics on five datasets. Analysis of the results shows that GS-KGC can discover new triples within existing KGs and generate new facts beyond the closed KG, effectively bridging the gap between closed-world and open-world KGC.</li>
</ul>

<h3>Title: Benchmarking Large Language Models for Math Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Kathrin Seler, Yao Rong, Emek Gzlkl, Enkelejda Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10839">https://arxiv.org/abs/2408.10839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10839">https://arxiv.org/pdf/2408.10839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10839]] Benchmarking Large Language Models for Math Reasoning Tasks(https://arxiv.org/abs/2408.10839)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance, such as in educational settings. Despite the variety of datasets and in-context learning algorithms designed to improve the ability of LLMs to automate mathematical problem solving, the lack of comprehensive benchmarking across different datasets makes it complicated to select an appropriate model for specific tasks. In this project, we present a benchmark that fairly compares seven state-of-the-art in-context learning algorithms for mathematical problem solving across five widely used mathematical datasets on four powerful foundation models. Furthermore, we explore the trade-off between efficiency and performance, highlighting the practical applications of LLMs for mathematical reasoning. Our results indicate that larger foundation models like GPT-4o and LLaMA 3-70B can solve mathematical reasoning independently from the concrete prompting strategy, while for smaller models the in-context learning approach significantly influences the performance. Moreover, the optimal prompt depends on the chosen foundation model. We open-source our benchmark code to support the integration of additional models in future research.</li>
</ul>

<h3>Title: Detecting Wildfires on UAVs with Real-time Segmentation Trained by Larger Teacher Models</h3>
<ul>
<li><strong>Authors: </strong>Julius Pesonen, Teemu Hakala, Vin Karjalainen, Niko Koivumki, Lauri Markelin, Anna-Maria Raita-Hakola, Juha Suomalainen, Ilkka Plnen, Eija Honkavaara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10843">https://arxiv.org/abs/2408.10843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10843">https://arxiv.org/pdf/2408.10843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10843]] Detecting Wildfires on UAVs with Real-time Segmentation Trained by Larger Teacher Models(https://arxiv.org/abs/2408.10843)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Early detection of wildfires is essential to prevent large-scale fires resulting in extensive environmental, structural, and societal damage. Uncrewed aerial vehicles (UAVs) can cover large remote areas effectively with quick deployment requiring minimal infrastructure and equipping them with small cameras and computers enables autonomous real-time detection. In remote areas, however, the UAVs are limited to on-board computing for detection due to the lack of high-bandwidth mobile networks. This limits the detection to methods which are light enough for the on-board computer alone. For accurate camera-based localisation, segmentation of the detected smoke is essential but training data for deep learning-based wildfire smoke segmentation is limited. This study shows how small specialised segmentation models can be trained using only bounding box labels, leveraging zero-shot foundation model supervision. The method offers the advantages of needing only fairly easily obtainable bounding box labels and requiring training solely for the smaller student network. The proposed method achieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The used model can perform in real-time at ~11 fps with a UAV-carried NVIDIA Jetson Orin NX computer while reliably recognising smoke, demonstrated at real-world forest burning events. Code is available at this https URL</li>
</ul>

<h3>Title: Harmonizing Attention: Training-free Texture-aware Geometry Transfer</h3>
<ul>
<li><strong>Authors: </strong>Eito Ikuta, Yohan Lee, Akihiro Iohara, Yu Saito, Toshiyuki Tanaka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10846">https://arxiv.org/abs/2408.10846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10846">https://arxiv.org/pdf/2408.10846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10846]] Harmonizing Attention: Training-free Texture-aware Geometry Transfer(https://arxiv.org/abs/2408.10846)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Extracting geometry features from photographic images independently of surface texture and transferring them onto different materials remains a complex challenge. In this study, we introduce Harmonizing Attention, a novel training-free approach that leverages diffusion models for texture-aware geometry transfer. Our method employs a simple yet effective modification of self-attention layers, allowing the model to query information from multiple reference images within these layers. This mechanism is seamlessly integrated into the inversion process as Texture-aligning Attention and into the generation process as Geometry-aligning Attention. This dual-attention approach ensures the effective capture and transfer of material-independent geometry features while maintaining material-specific textural continuity, all without the need for model fine-tuning.</li>
</ul>

<h3>Title: Low-Quality Image Detection by Hierarchical VAE</h3>
<ul>
<li><strong>Authors: </strong>Tomoyasu Nanaumi, Kazuhiko Kawamoto, Hiroshi Kera</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10885">https://arxiv.org/abs/2408.10885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10885">https://arxiv.org/pdf/2408.10885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10885]] Low-Quality Image Detection by Hierarchical VAE(https://arxiv.org/abs/2408.10885)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To make an employee roster, photo album, or training dataset of generative models, one needs to collect high-quality images while dismissing low-quality ones. This study addresses a new task of unsupervised detection of low-quality images. We propose a method that not only detects low-quality images with various types of degradation but also provides visual clues of them based on an observation that partial reconstruction by hierarchical variational autoencoders fails for low-quality images. The experiments show that our method outperforms several unsupervised out-of-distribution detection methods and also gives visual clues for low-quality images that help humans recognize them even in thumbnail view.</li>
</ul>

<h3>Title: ViLReF: A Chinese Vision-Language Retinal Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Shengzhu Yang, Jiawei Du, Jia Guo, Weihang Zhang, Hanruo Liu, Huiqi Li, Ningli Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10894">https://arxiv.org/abs/2408.10894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10894">https://arxiv.org/pdf/2408.10894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10894]] ViLReF: A Chinese Vision-Language Retinal Foundation Model(https://arxiv.org/abs/2408.10894)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Subtle semantic differences in retinal image and text data present great challenges for pre-training visual-language models. Moreover, false negative samples, i.e., image-text pairs having the same semantics but incorrectly regarded as negatives, disrupt the visual-language pre-training process and affect the model's learning ability. This work aims to develop a retinal foundation model, called ViLReF, by pre-training on a paired dataset comprising 451,956 retinal images and corresponding diagnostic text reports. In our vision-language pre-training strategy, we leverage expert knowledge to facilitate the extraction of labels and propose a novel constraint, the Weighted Similarity Coupling Loss, to adjust the speed of pushing sample pairs further apart dynamically within the feature space. Furthermore, we employ a batch expansion module with dynamic memory queues, maintained by momentum encoders, to supply extra samples and compensate for the vacancies caused by eliminating false negatives. Extensive experiments are conducted on multiple datasets for downstream classification and segmentation tasks. The experimental results demonstrate the powerful zero-shot and transfer learning capabilities of ViLReF, verifying the effectiveness of our pre-training strategy. Our ViLReF model is available at: this https URL.</li>
</ul>

<h3>Title: A Grey-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse</h3>
<ul>
<li><strong>Authors: </strong>Zhongliang Guo, Lei Fang, Jingyu Lin, Yifei Qian, Shuai Zhao, Zeyu Wang, Junhao Dong, Cunjian Chen, Ognjen Arandjelovi, Chun Pong Lau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10901">https://arxiv.org/abs/2408.10901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10901">https://arxiv.org/pdf/2408.10901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10901]] A Grey-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse(https://arxiv.org/abs/2408.10901)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative AI, particularly Latent Diffusion Models (LDMs), have revolutionized image synthesis and manipulation. However, these generative techniques raises concerns about data misappropriation and intellectual property infringement. Adversarial attacks on machine learning models have been extensively studied, and a well-established body of research has extended these techniques as a benign metric to prevent the underlying misuse of generative AI. Current approaches to safeguarding images from manipulation by LDMs are limited by their reliance on model-specific knowledge and their inability to significantly degrade semantic quality of generated images. In response to these shortcomings, we propose the Posterior Collapse Attack (PCA) based on the observation that VAEs suffer from posterior collapse during training. Our method minimizes dependence on the white-box information of target models to get rid of the implicit reliance on model-specific knowledge. By accessing merely a small amount of LDM parameters, in specific merely the VAE encoder of LDMs, our method causes a substantial semantic collapse in generation quality, particularly in perceptual consistency, and demonstrates strong transferability across various model architectures. Experimental results show that PCA achieves superior perturbation effects on image generation of LDMs with lower runtime and VRAM. Our method outperforms existing techniques, offering a more robust and generalizable solution that is helpful in alleviating the socio-technical challenges posed by the rapidly evolving landscape of generative AI.</li>
</ul>

<h3>Title: ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Qi Ma, Yue Li, Bin Ren, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, Danda Pani Paudel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10906">https://arxiv.org/abs/2408.10906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10906">https://arxiv.org/pdf/2408.10906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10906]] ShapeSplat: A Large-scale Dataset of Gaussian Splats and Their Self-Supervised Pretraining(https://arxiv.org/abs/2408.10906)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (3DGS) has become the de facto method of 3D representation in many vision tasks. This calls for the 3D understanding directly in this representation space. To facilitate the research in this direction, we first build a large-scale dataset of 3DGS using the commonly used ShapeNet and ModelNet datasets. Our dataset ShapeSplat consists of 65K objects from 87 unique categories, whose labels are in accordance with the respective datasets. The creation of this dataset utilized the compute equivalent of 2 GPU years on a TITAN XP GPU. We utilize our dataset for unsupervised pretraining and supervised finetuning for classification and segmentation tasks. To this end, we introduce \textbf{\textit{Gaussian-MAE}}, which highlights the unique benefits of representation learning from Gaussian parameters. Through exhaustive experiments, we provide several valuable insights. In particular, we show that (1) the distribution of the optimized GS centroids significantly differs from the uniformly sampled point cloud (used for initialization) counterpart; (2) this change in distribution results in degradation in classification but improvement in segmentation tasks when using only the centroids; (3) to leverage additional Gaussian parameters, we propose Gaussian feature grouping in a normalized feature space, along with splats pooling layer, offering a tailored solution to effectively group and embed similar Gaussians, which leads to notable improvement in finetuning tasks.</li>
</ul>

<h3>Title: To Code, or Not To Code? Exploring Impact of Code in Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet stn, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10914">https://arxiv.org/abs/2408.10914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10914">https://arxiv.org/pdf/2408.10914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10914]] To Code, or Not To Code? Exploring Impact of Code in Pre-training(https://arxiv.org/abs/2408.10914)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs' performance, there is only limited work analyzing the precise impact of code on non-code tasks. In this work, we systematically investigate the impact of code data on general performance. We ask "what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation". We conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, we find a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the addition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. Our work suggests investments in code quality and preserving code during pre-training have positive impacts.</li>
</ul>

<h3>Title: Large Point-to-Gaussian Model for Image-to-3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Longfei Lu, Huachen Gao, Tao Dai, Yaohua Zha, Zhi Hou, Junta Wu, Shu-Tao Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.10935">https://arxiv.org/abs/2408.10935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.10935">https://arxiv.org/pdf/2408.10935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.10935]] Large Point-to-Gaussian Model for Image-to-3D Generation(https://arxiv.org/abs/2408.10935)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, image-to-3D approaches have significantly advanced the generation quality and speed of 3D assets based on large reconstruction models, particularly 3D Gaussian reconstruction models. Existing large 3D Gaussian models directly map 2D image to 3D Gaussian parameters, while regressing 2D image to 3D Gaussian representations is challenging without 3D priors. In this paper, we propose a large Point-to-Gaussian model, that inputs the initial point cloud produced from large 3D diffusion model conditional on 2D image to generate the Gaussian parameters, for image-to-3D generation. The point cloud provides initial 3D geometry prior for Gaussian generation, thus significantly facilitating image-to-3D Generation. Moreover, we present the \textbf{A}ttention mechanism, \textbf{P}rojection mechanism, and \textbf{P}oint feature extractor, dubbed as \textbf{APP} block, for fusing the image features with point cloud features. The qualitative and quantitative experiments extensively demonstrate the effectiveness of the proposed approach on GSO and Objaverse datasets, and show the proposed method achieves state-of-the-art performance.</li>
</ul>

<h3>Title: SenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite Self-Supervised Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Prexl, Michael Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11000">https://arxiv.org/abs/2408.11000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11000">https://arxiv.org/pdf/2408.11000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11000]] SenPa-MAE: Sensor Parameter Aware Masked Autoencoder for Multi-Satellite Self-Supervised Pretraining(https://arxiv.org/abs/2408.11000)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>This paper introduces SenPa-MAE, a transformer architecture that encodes the sensor parameters of an observed multispectral signal into the image embeddings. SenPa-MAE can be pre-trained on imagery of different satellites with non-matching spectral or geometrical sensor characteristics. To incorporate sensor parameters, we propose a versatile sensor parameter encoding module as well as a data augmentation strategy for the diversification of the pre-training dataset. This enables the model to effectively differentiate between various sensors and gain an understanding of sensor parameters and the correlation to the observed signal. Given the rising number of Earth observation satellite missions and the diversity in their sensor specifications, our approach paves the way towards a sensor-independent Earth observation foundation model. This opens up possibilities such as cross-sensor training and sensor-independent inference.</li>
</ul>

<h3>Title: MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning</h3>
<ul>
<li><strong>Authors: </strong>Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11001">https://arxiv.org/abs/2408.11001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11001">https://arxiv.org/pdf/2408.11001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11001]] MegaFusion: Extend Diffusion Models towards Higher-resolution Image Generation without Further Tuning(https://arxiv.org/abs/2408.11001)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as frontrunners in text-to-image generation for their impressive capabilities. Nonetheless, their fixed image resolution during training often leads to challenges in high-resolution image generation, such as semantic inaccuracies and object replication. This paper introduces MegaFusion, a novel approach that extends existing diffusion-based text-to-image generation models towards efficient higher-resolution generation without additional fine-tuning or extra adaptation. Specifically, we employ an innovative truncate and relay strategy to bridge the denoising processes across different resolutions, allowing for high-resolution image generation in a coarse-to-fine manner. Moreover, by integrating dilated convolutions and noise re-scheduling, we further adapt the model's priors for higher resolution. The versatility and efficacy of MegaFusion make it universally applicable to both latent-space and pixel-space diffusion models, along with other derivative models. Extensive experiments confirm that MegaFusion significantly boosts the capability of existing models to produce images of megapixels and various aspect ratios, while only requiring about 40% of the original computational cost.</li>
</ul>

<h3>Title: Athena: Safe Autonomous Agents with Verbal Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Tanmana Sadhu, Ali Pesaranghader, Yanan Chen, Dong Hoon Yi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11021">https://arxiv.org/abs/2408.11021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11021">https://arxiv.org/pdf/2408.11021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11021]] Athena: Safe Autonomous Agents with Verbal Contrastive Learning(https://arxiv.org/abs/2408.11021)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Due to emergent capabilities, large language models (LLMs) have been utilized as language-based agents to perform a variety of tasks and make decisions with an increasing degree of autonomy. These autonomous agents can understand high-level instructions, interact with their environments, and execute complex tasks using a selection of tools available to them. As the capabilities of the agents expand, ensuring their safety and trustworthiness becomes more imperative. In this study, we introduce the Athena framework which leverages the concept of verbal contrastive learning where past safe and unsafe trajectories are used as in-context (contrastive) examples to guide the agent towards safety while fulfilling a given task. The framework also incorporates a critiquing mechanism to guide the agent to prevent risky actions at every step. Furthermore, due to the lack of existing benchmarks on the safety reasoning ability of LLM-based agents, we curate a set of 80 toolkits across 8 categories with 180 scenarios to provide a safety evaluation benchmark. Our experimental evaluation, with both closed- and open-source LLMs, indicates verbal contrastive learning and interaction-level critiquing improve the safety rate significantly.</li>
</ul>

<h3>Title: Accelerating Goal-Conditioned RL Algorithms and Research</h3>
<ul>
<li><strong>Authors: </strong>Micha Bortkiewicz, Wadek Paucki, Vivek Myers, Tadeusz Dziarmaga, Tomasz Arczewski, ukasz Kuciski, Benjamin Eysenbach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11052">https://arxiv.org/abs/2408.11052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11052">https://arxiv.org/pdf/2408.11052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11052]] Accelerating Goal-Conditioned RL Algorithms and Research(https://arxiv.org/abs/2408.11052)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover new behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environments as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark JaxGCRL for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. The key to this performance is a combination of GPU-accelerated environments and a stable, batched version of the contrastive reinforcement learning algorithm, based on an infoNCE objective, that effectively makes use of this increased data throughput. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in a diverse set of challenging environments. Website + Code: this https URL</li>
</ul>

<h3>Title: NeCo: Improving DINOv2's spatial representations in 19 GPU hours with Patch Neighbor Consistency</h3>
<ul>
<li><strong>Authors: </strong>Valentinos Pariza, Mohammadreza Salehi, Gertjan Burghouts, Francesco Locatello, Yuki M. Asano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.11054">https://arxiv.org/abs/2408.11054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.11054">https://arxiv.org/pdf/2408.11054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.11054]] NeCo: Improving DINOv2's spatial representations in 19 GPU hours with Patch Neighbor Consistency(https://arxiv.org/abs/2408.11054)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, in-context</a></li>
<li><strong>Abstract: </strong>We propose sorting patch representations across views as a novel self-supervised learning signal to improve pretrained representations. To this end, we introduce NeCo: Patch Neighbor Consistency, a novel training loss that enforces patch-level nearest neighbor consistency across a student and teacher model, relative to reference batches. Our method leverages a differentiable sorting method applied on top of pretrained representations, such as DINOv2-registers to bootstrap the learning signal and further improve upon them. This dense post-pretraining leads to superior performance across various models and datasets, despite requiring only 19 hours on a single GPU. We demonstrate that this method generates high-quality dense feature encoders and establish several new state-of-the-art results: +5.5% and + 6% for non-parametric in-context semantic segmentation on ADE20k and Pascal VOC, and +7.2% and +5.7% for linear segmentation evaluations on COCO-Things and -Stuff.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
