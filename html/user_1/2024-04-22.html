<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-22</h1>
<h3>Title: Model Failure or Data Corruption? Exploring Inconsistencies in Building  Energy Ratings with Self-Supervised Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Qian Xiao, Dan Liu, Kevin Credit</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12399">https://arxiv.org/abs/2404.12399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12399">https://arxiv.org/pdf/2404.12399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12399]] Model Failure or Data Corruption? Exploring Inconsistencies in Building  Energy Ratings with Self-Supervised Contrastive Learning(https://arxiv.org/abs/2404.12399)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Building Energy Rating (BER) stands as a pivotal metric, enabling building owners, policymakers, and urban planners to understand the energy-saving potential through improving building energy efficiency. As such, enhancing buildings' BER levels is expected to directly contribute to the reduction of carbon emissions and promote climate improvement. Nonetheless, the BER assessment process is vulnerable to missing and inaccurate measurements. In this study, we introduce \texttt{CLEAR}, a data-driven approach designed to scrutinize the inconsistencies in BER assessments through self-supervised contrastive learning. We validated the effectiveness of \texttt{CLEAR} using a dataset representing Irish building stocks. Our experiments uncovered evidence of inconsistent BER assessments, highlighting measurement data corruption within this real-world dataset.</li>
</ul>

<h3>Title: Group-wise Prompting for Synthetic Tabular Data Generation using Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinhee Kim, Taesung Kim, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12404">https://arxiv.org/abs/2404.12404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12404">https://arxiv.org/pdf/2404.12404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12404]] Group-wise Prompting for Synthetic Tabular Data Generation using Large  Language Models(https://arxiv.org/abs/2404.12404)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Generating realistic synthetic tabular data presents a critical challenge in machine learning. This study introduces a simple yet effective method employing Large Language Models (LLMs) tailored to generate synthetic data, specifically addressing data imbalance problems. We propose a novel group-wise prompting method in CSV-style formatting that leverages the in-context learning capabilities of LLMs to produce data that closely adheres to the specified requirements and characteristics of the target dataset. Moreover, our proposed random word replacement strategy significantly improves the handling of monotonous categorical values, enhancing the accuracy and representativeness of the synthetic data. The effectiveness of our method is extensively validated across eight real-world public datasets, achieving state-of-the-art performance in downstream classification and regression tasks while maintaining inter-feature correlations and improving token efficiency over existing approaches. This advancement significantly contributes to addressing the key challenges of machine learning applications, particularly in the context of tabular data generation and handling class imbalance. The source code for our work is available at: https://github.com/seharanul17/synthetic-tabular-LLM</li>
</ul>

<h3>Title: Global Counterfactual Directions</h3>
<ul>
<li><strong>Authors: </strong>Bartlomiej Sobieski, Przemys≈Çaw Biecek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12488">https://arxiv.org/abs/2404.12488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12488">https://arxiv.org/pdf/2404.12488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12488]] Global Counterfactual Directions(https://arxiv.org/abs/2404.12488)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite increasing progress in development of methods for generating visual counterfactual explanations, especially with the recent rise of Denoising Diffusion Probabilistic Models, previous works consider them as an entirely local technique. In this work, we take the first step at globalizing them. Specifically, we discover that the latent space of Diffusion Autoencoders encodes the inference process of a given classifier in the form of global directions. We propose a novel proxy-based approach that discovers two types of these directions with the use of only single image in an entirely black-box manner. Precisely, g-directions allow for flipping the decision of a given classifier on an entire dataset of images, while h-directions further increase the diversity of explanations. We refer to them in general as Global Counterfactual Directions (GCDs). Moreover, we show that GCDs can be naturally combined with Latent Integrated Gradients resulting in a new black-box attribution method, while simultaneously enhancing the understanding of counterfactual explanations. We validate our approach on existing benchmarks and show that it generalizes to real-world use-cases.</li>
</ul>

<h3>Title: SPIdepth: Strengthened Pose Information for Self-supervised Monocular  Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Mykola Lavreniuk</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12501">https://arxiv.org/abs/2404.12501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12501">https://arxiv.org/pdf/2404.12501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12501]] SPIdepth: Strengthened Pose Information for Self-supervised Monocular  Depth Estimation(https://arxiv.org/abs/2404.12501)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised monocular depth estimation has garnered considerable attention for its applications in autonomous driving and robotics. While recent methods have made strides in leveraging techniques like the Self Query Layer (SQL) to infer depth from motion, they often overlook the potential of strengthening pose information. In this paper, we introduce SPIdepth, a novel approach that prioritizes enhancing the pose network for improved depth estimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the importance of pose information in capturing fine-grained scene structures. By enhancing the pose network's capabilities, SPIdepth achieves remarkable advancements in scene understanding and depth estimation. Experimental results on benchmark datasets such as KITTI and Cityscapes showcase SPIdepth's state-of-the-art performance, surpassing previous methods by significant margins. Notably, SPIdepth's performance exceeds that of unsupervised models and, after finetuning on metric data, outperforms all existing methods. Remarkably, SPIdepth achieves these results using only a single image for inference, surpassing even methods that utilize video sequences for inference, thus demonstrating its efficacy and efficiency in real-world applications. Our approach represents a significant leap forward in self-supervised monocular depth estimation, underscoring the importance of strengthening pose information for advancing scene understanding in real-world applications.</li>
</ul>

<h3>Title: Adaptive Memory Replay for Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>James Seale Smith, Lazar Valkov, Shaunak Halbe, Vyshnavi Gutta, Rogerio Feris, Zsolt Kira, Leonid Karlinsky</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12526">https://arxiv.org/abs/2404.12526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12526">https://arxiv.org/pdf/2404.12526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12526]] Adaptive Memory Replay for Continual Learning(https://arxiv.org/abs/2404.12526)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation Models (FMs) have become the hallmark of modern AI, however, these models are trained on massive data, leading to financially expensive training. Updating FMs as new data becomes available is important, however, can lead to `catastrophic forgetting', where models underperform on tasks related to data sub-populations observed too long ago. This continual learning (CL) phenomenon has been extensively studied, but primarily in a setting where only a small amount of past data can be stored. We advocate for the paradigm where memory is abundant, allowing us to keep all previous data, but computational resources are limited. In this setting, traditional replay-based CL approaches are outperformed by a simple baseline which replays past data selected uniformly at random, indicating that this setting necessitates a new approach. We address this by introducing a framework of adaptive memory replay for continual learning, where sampling of past data is phrased as a multi-armed bandit problem. We utilize Bolzmann sampling to derive a method which dynamically selects past data for training conditioned on the current task, assuming full data access and emphasizing training efficiency. Through extensive evaluations on both vision and language pre-training tasks, we demonstrate the effectiveness of our approach, which maintains high performance while reducing forgetting by up to 10% at no training efficiency cost.</li>
</ul>

<h3>Title: GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Sai Sree Harsha, Ambareesh Revanur, Dhwanit Agarwal, Shradha Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12541">https://arxiv.org/abs/2404.12541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12541">https://arxiv.org/pdf/2404.12541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12541]] GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I  Diffusion Models(https://arxiv.org/abs/2404.12541)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video editing methods based on diffusion models that rely solely on a text prompt for the edit are hindered by the limited expressive power of text prompts. Thus, incorporating a reference target image as a visual guide becomes desirable for precise control over edit. Also, most existing methods struggle to accurately edit a video when the shape and size of the object in the target image differ from the source object. To address these challenges, we propose "GenVideo" for editing videos leveraging target-image aware T2I models. Our approach handles edits with target objects of varying shapes and sizes while maintaining the temporal consistency of the edit using our novel target and shape aware InvEdit masks. Further, we propose a novel target-image aware latent noise correction strategy during inference to improve the temporal consistency of the edits. Experimental analyses indicate that GenVideo can effectively handle edits with objects of varying shapes, where existing approaches fail.</li>
</ul>

<h3>Title: Multi-View Subgraph Neural Networks: Self-Supervised Learning with  Scarce Labeled Data</h3>
<ul>
<li><strong>Authors: </strong>Zhenzhong Wang, Qingyuan Zeng, Wanyu Lin, Min Jiang, Kay Chen Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12569">https://arxiv.org/abs/2404.12569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12569">https://arxiv.org/pdf/2404.12569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12569]] Multi-View Subgraph Neural Networks: Self-Supervised Learning with  Scarce Labeled Data(https://arxiv.org/abs/2404.12569)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>While graph neural networks (GNNs) have become the de-facto standard for graph-based node classification, they impose a strong assumption on the availability of sufficient labeled samples. This assumption restricts the classification performance of prevailing GNNs on many real-world applications suffering from low-data regimes. Specifically, features extracted from scarce labeled nodes could not provide sufficient supervision for the unlabeled samples, leading to severe over-fitting. In this work, we point out that leveraging subgraphs to capture long-range dependencies can augment the representation of a node with homophily properties, thus alleviating the low-data regime. However, prior works leveraging subgraphs fail to capture the long-range dependencies among nodes. To this end, we present a novel self-supervised learning framework, called multi-view subgraph neural networks (Muse), for handling long-range dependencies. In particular, we propose an information theory-based identification mechanism to identify two types of subgraphs from the views of input space and latent space, respectively. The former is to capture the local structure of the graph, while the latter captures the long-range dependencies among nodes. By fusing these two views of subgraphs, the learned representations can preserve the topological properties of the graph at large, including the local structure and long-range dependencies, thus maximizing their expressiveness for downstream node classification tasks. Experimental results show that Muse outperforms the alternative methods on node classification tasks with limited labeled data.</li>
</ul>

<h3>Title: Continuous-time Risk-sensitive Reinforcement Learning via Quadratic  Variation Penalty</h3>
<ul>
<li><strong>Authors: </strong>Yanwei Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, q-fin.CP, q-fin.PM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12598">https://arxiv.org/abs/2404.12598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12598">https://arxiv.org/pdf/2404.12598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12598]] Continuous-time Risk-sensitive Reinforcement Learning via Quadratic  Variation Penalty(https://arxiv.org/abs/2404.12598)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper studies continuous-time risk-sensitive reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation with the exponential-form objective. The risk-sensitive objective arises either as the agent's risk attitude or as a distributionally robust approach against the model uncertainty. Owing to the martingale perspective in Jia and Zhou (2023) the risk-sensitive RL problem is shown to be equivalent to ensuring the martingale property of a process involving both the value function and the q-function, augmented by an additional penalty term: the quadratic variation of the value process, capturing the variability of the value-to-go along the trajectory. This characterization allows for the straightforward adaptation of existing RL algorithms developed for non-risk-sensitive scenarios to incorporate risk sensitivity by adding the realized variance of the value process. Additionally, I highlight that the conventional policy gradient representation is inadequate for risk-sensitive problems due to the nonlinear nature of quadratic variation; however, q-learning offers a solution and extends to infinite horizon settings. Finally, I prove the convergence of the proposed algorithm for Merton's investment problem and quantify the impact of temperature parameter on the behavior of the learning procedure. I also conduct simulation experiments to demonstrate how risk-sensitive RL improves the finite-sample performance in the linear-quadratic control problem.</li>
</ul>

<h3>Title: ELEV-VISION-SAM: Integrated Vision Language and Foundation Model for  Automated Estimation of Building Lowest Floor Elevation</h3>
<ul>
<li><strong>Authors: </strong>Yu-Hsuan Ho, Longxiang Li, Ali Mostafavi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12606">https://arxiv.org/abs/2404.12606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12606">https://arxiv.org/pdf/2404.12606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12606]] ELEV-VISION-SAM: Integrated Vision Language and Foundation Model for  Automated Estimation of Building Lowest Floor Elevation(https://arxiv.org/abs/2404.12606)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Street view imagery, aided by advancements in image quality and accessibility, has emerged as a valuable resource for urban analytics research. Recent studies have explored its potential for estimating lowest floor elevation (LFE), offering a scalable alternative to traditional on-site measurements, crucial for assessing properties' flood risk and damage extent. While existing methods rely on object detection, the introduction of image segmentation has broadened street view images' utility for LFE estimation, although challenges still remain in segmentation quality and capability to distinguish front doors from other doors. To address these challenges in LFE estimation, this study integrates the Segment Anything model, a segmentation foundation model, with vision language models to conduct text-prompt image segmentation on street view images for LFE estimation. By evaluating various vision language models, integration methods, and text prompts, we identify the most suitable model for street view image analytics and LFE estimation tasks, thereby improving the availability of the current LFE estimation model based on image segmentation from 33% to 56% of properties. Remarkably, our proposed method significantly enhances the availability of LFE estimation to almost all properties in which the front door is visible in the street view image. Also the findings present the first baseline and comparison of various vision models of street view image-based LFE estimation. The model and findings not only contribute to advancing street view image segmentation for urban analytics but also provide a novel approach for image segmentation tasks for other civil engineering and infrastructure analytics tasks.</li>
</ul>

<h3>Title: Rethinking Clothes Changing Person ReID: Conflicts, Synthesis, and  Optimization</h3>
<ul>
<li><strong>Authors: </strong>Junjie Li, Guanshuo Wang, Fufu Yu, Yichao Yan, Qiong Jia, Shouhong Ding, Xingdong Sheng, Yunhui Liu, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12611">https://arxiv.org/abs/2404.12611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12611">https://arxiv.org/pdf/2404.12611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12611]] Rethinking Clothes Changing Person ReID: Conflicts, Synthesis, and  Optimization(https://arxiv.org/abs/2404.12611)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Clothes-changing person re-identification (CC-ReID) aims to retrieve images of the same person wearing different outfits. Mainstream researches focus on designing advanced model structures and strategies to capture identity information independent of clothing. However, the same-clothes discrimination as the standard ReID learning objective in CC-ReID is persistently ignored in previous researches. In this study, we dive into the relationship between standard and clothes-changing~(CC) learning objectives, and bring the inner conflicts between these two objectives to the fore. We try to magnify the proportion of CC training pairs by supplementing high-fidelity clothes-varying synthesis, produced by our proposed Clothes-Changing Diffusion model. By incorporating the synthetic images into CC-ReID model training, we observe a significant improvement under CC protocol. However, such improvement sacrifices the performance under the standard protocol, caused by the inner conflict between standard and CC. For conflict mitigation, we decouple these objectives and re-formulate CC-ReID learning as a multi-objective optimization (MOO) problem. By effectively regularizing the gradient curvature across multiple objectives and introducing preference restrictions, our MOO solution surpasses the single-task training paradigm. Our framework is model-agnostic, and demonstrates superior performance under both CC and standard ReID protocols.</li>
</ul>

<h3>Title: Efficient infusion of self-supervised representations in Automatic  Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Darshan Prabhu, Sai Ganesh Mirishkar, Pankaj Wasnik</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12628">https://arxiv.org/abs/2404.12628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12628">https://arxiv.org/pdf/2404.12628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12628]] Efficient infusion of self-supervised representations in Automatic  Speech Recognition(https://arxiv.org/abs/2404.12628)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learned (SSL) models such as Wav2vec and HuBERT yield state-of-the-art results on speech-related tasks. Given the effectiveness of such models, it is advantageous to use them in conventional ASR systems. While some approaches suggest incorporating these models as a trainable encoder or a learnable frontend, training such systems is extremely slow and requires a lot of computation cycles. In this work, we propose two simple approaches that use (1) framewise addition and (2) cross-attention mechanisms to efficiently incorporate the representations from the SSL model(s) into the ASR architecture, resulting in models that are comparable in size with standard encoder-decoder conformer systems while also avoiding the usage of SSL models during training. Our approach results in faster training and yields significant performance gains on the Librispeech and Tedlium datasets compared to baselines. We further provide detailed analysis and ablation studies that demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Detecting Out-Of-Distribution Earth Observation Images with Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Georges Le Bellier (CEDRIC - VERTIGO, CNAM), Nicolas Audebert (CEDRIC - VERTIGO, CNAM, IGN)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12667">https://arxiv.org/abs/2404.12667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12667">https://arxiv.org/pdf/2404.12667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12667]] Detecting Out-Of-Distribution Earth Observation Images with Diffusion  Models(https://arxiv.org/abs/2404.12667)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, anomaly</a></li>
<li><strong>Abstract: </strong>Earth Observation imagery can capture rare and unusual events, such as disasters and major landscape changes, whose visual appearance contrasts with the usual observations. Deep models trained on common remote sensing data will output drastically different features for these out-of-distribution samples, compared to those closer to their training dataset. Detecting them could therefore help anticipate changes in the observations, either geographical or environmental. In this work, we show that the reconstruction error of diffusion models can effectively serve as unsupervised out-of-distribution detectors for remote sensing images, using them as a plausibility score. Moreover, we introduce ODEED, a novel reconstruction-based scorer using the probability-flow ODE of diffusion models. We validate it experimentally on SpaceNet 8 with various scenarios, such as classical OOD detection with geographical shift and near-OOD setups: pre/post-flood and non-flooded/flooded image recognition. We show that our ODEED scorer significantly outperforms other diffusion-based and discriminative baselines on the more challenging near-OOD scenarios of flood image detection, where OOD images are close to the distribution tail. We aim to pave the way towards better use of generative models for anomaly detection in remote sensing.</li>
</ul>

<h3>Title: MLSD-GAN -- Generating Strong High Quality Face Morphing Attacks using  Latent Semantic Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Aravinda Reddy PN, Raghavendra Ramachandra, Krothapalli Sreenivasa Rao, Pabitra Mitra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12679">https://arxiv.org/abs/2404.12679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12679">https://arxiv.org/pdf/2404.12679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12679]] MLSD-GAN -- Generating Strong High Quality Face Morphing Attacks using  Latent Semantic Disentanglement(https://arxiv.org/abs/2404.12679)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face-morphing attacks are a growing concern for biometric researchers, as they can be used to fool face recognition systems (FRS). These attacks can be generated at the image level (supervised) or representation level (unsupervised). Previous unsupervised morphing attacks have relied on generative adversarial networks (GANs). More recently, researchers have used linear interpolation of StyleGAN-encoded images to generate morphing attacks. In this paper, we propose a new method for generating high-quality morphing attacks using StyleGAN disentanglement. Our approach, called MLSD-GAN, spherically interpolates the disentangled latents to produce realistic and diverse morphing attacks. We evaluate the vulnerability of MLSD-GAN on two deep-learning-based FRS techniques. The results show that MLSD-GAN poses a significant threat to FRS, as it can generate morphing attacks that are highly effective at fooling these systems.</li>
</ul>

<h3>Title: uTRAND: Unsupervised Anomaly Detection in Traffic Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Giacomo D'Amicantonio, Egor Bondarau, Peter H.N. de With</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12712">https://arxiv.org/abs/2404.12712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12712">https://arxiv.org/pdf/2404.12712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12712]] uTRAND: Unsupervised Anomaly Detection in Traffic Trajectories(https://arxiv.org/abs/2404.12712)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Deep learning-based approaches have achieved significant improvements on public video anomaly datasets, but often do not perform well in real-world applications. This paper addresses two issues: the lack of labeled data and the difficulty of explaining the predictions of a neural network. To this end, we present a framework called uTRAND, that shifts the problem of anomalous trajectory prediction from the pixel space to a semantic-topological domain. The framework detects and tracks all types of traffic agents in bird's-eye-view videos of traffic cameras mounted at an intersection. By conceptualizing the intersection as a patch-based graph, it is shown that the framework learns and models the normal behaviour of traffic agents without costly manual labeling. Furthermore, uTRAND allows to formulate simple rules to classify anomalous trajectories in a way suited for human interpretation. We show that uTRAND outperforms other state-of-the-art approaches on a dataset of anomalous trajectories collected in a real-world setting, while producing explainable detection results.</li>
</ul>

<h3>Title: PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian  Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Zepeng Jiang, Weiwei Ni, Yifan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12730">https://arxiv.org/abs/2404.12730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12730">https://arxiv.org/pdf/2404.12730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12730]] PATE-TripleGAN: Privacy-Preserving Image Synthesis with Gaussian  Differential Privacy(https://arxiv.org/abs/2404.12730)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conditional Generative Adversarial Networks (CGANs) exhibit significant potential in supervised learning model training by virtue of their ability to generate realistic labeled images. However, numerous studies have indicated the privacy leakage risk in CGANs models. The solution DPCGAN, incorporating the differential privacy framework, faces challenges such as heavy reliance on labeled data for model training and potential disruptions to original gradient information due to excessive gradient clipping, making it difficult to ensure model accuracy. To address these challenges, we present a privacy-preserving training framework called PATE-TripleGAN. This framework incorporates a classifier to pre-classify unlabeled data, establishing a three-party min-max game to reduce dependence on labeled data. Furthermore, we present a hybrid gradient desensitization algorithm based on the Private Aggregation of Teacher Ensembles (PATE) framework and Differential Private Stochastic Gradient Descent (DPSGD) method. This algorithm allows the model to retain gradient information more effectively while ensuring privacy protection, thereby enhancing the model's utility. Privacy analysis and extensive experiments affirm that the PATE-TripleGAN model can generate a higher quality labeled image dataset while ensuring the privacy of the training data.</li>
</ul>

<h3>Title: Beyond Human Norms: Unveiling Unique Values of Large Language Models  through Interdisciplinary Approaches</h3>
<ul>
<li><strong>Authors: </strong>Pablo Biedma, Xiaoyuan Yi, Linus Huang, Maosong Sun, Xing Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12744">https://arxiv.org/abs/2404.12744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12744">https://arxiv.org/pdf/2404.12744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12744]] Beyond Human Norms: Unveiling Unique Values of Large Language Models  through Interdisciplinary Approaches(https://arxiv.org/abs/2404.12744)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have revolutionized the AI field but also pose potential safety and ethical risks. Deciphering LLMs' embedded values becomes crucial for assessing and mitigating their risks. Despite extensive investigation into LLMs' values, previous studies heavily rely on human-oriented value systems in social sciences. Then, a natural question arises: Do LLMs possess unique values beyond those of humans? Delving into it, this work proposes a novel framework, ValueLex, to reconstruct LLMs' unique value system from scratch, leveraging psychological methodologies from human personality/value research. Based on Lexical Hypothesis, ValueLex introduces a generative approach to elicit diverse values from 30+ LLMs, synthesizing a taxonomy that culminates in a comprehensive value framework via factor analysis and semantic clustering. We identify three core value dimensions, Competence, Character, and Integrity, each with specific subdimensions, revealing that LLMs possess a structured, albeit non-human, value system. Based on this system, we further develop tailored projective tests to evaluate and analyze the value inclinations of LLMs across different model sizes, training methods, and data sources. Our framework fosters an interdisciplinary paradigm of understanding LLMs, paving the way for future AI alignment and regulation.</li>
</ul>

<h3>Title: AutoCrawler: A Progressive Understanding Web Agent for Web Crawler  Generation</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Huang, Chenghao Peng, Zhixu Li, Jiaqing Liang, Yanghua Xiao, Liqian Wen, Zulong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12753">https://arxiv.org/abs/2404.12753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12753">https://arxiv.org/pdf/2404.12753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12753]] AutoCrawler: A Progressive Understanding Web Agent for Web Crawler  Generation(https://arxiv.org/abs/2404.12753)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Web automation is a significant technique that accomplishes complicated web tasks by automating common web actions, enhancing operational efficiency, and reducing the need for manual intervention. Traditional methods, such as wrappers, suffer from limited adaptability and scalability when faced with a new website. On the other hand, generative agents empowered by large language models (LLMs) exhibit poor performance and reusability in open-world scenarios. In this work, we introduce a crawler generation task for vertical information web pages and the paradigm of combining LLMs with crawlers, which helps crawlers handle diverse and changing web environments more efficiently. We propose AutoCrawler, a two-stage framework that leverages the hierarchical structure of HTML for progressive understanding. Through top-down and step-back operations, AutoCrawler can learn from erroneous actions and continuously prune HTML for better action generation. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework. Resources of this paper can be found at \url{https://github.com/EZ-hwh/AutoCrawler}</li>
</ul>

<h3>Title: Enhancing Counterfactual Explanation Search with Diffusion Distance and  Directional Coherence</h3>
<ul>
<li><strong>Authors: </strong>Marharyta Domnich, Raul Vicente</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12810">https://arxiv.org/abs/2404.12810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12810">https://arxiv.org/pdf/2404.12810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12810]] Enhancing Counterfactual Explanation Search with Diffusion Distance and  Directional Coherence(https://arxiv.org/abs/2404.12810)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>A pressing issue in the adoption of AI models is the increasing demand for more human-centric explanations of their predictions. To advance towards more human-centric explanations, understanding how humans produce and select explanations has been beneficial. In this work, inspired by insights of human cognition we propose and test the incorporation of two novel biases to enhance the search for effective counterfactual explanations. Central to our methodology is the application of diffusion distance, which emphasizes data connectivity and actionability in the search for feasible counterfactual explanations. In particular, diffusion distance effectively weights more those points that are more interconnected by numerous short-length paths. This approach brings closely connected points nearer to each other, identifying a feasible path between them. We also introduce a directional coherence term that allows the expression of a preference for the alignment between the joint and marginal directional changes in feature space to reach a counterfactual. This term enables the generation of counterfactual explanations that align with a set of marginal predictions based on expectations of how the outcome of the model varies by changing one feature at a time. We evaluate our method, named Coherent Directional Counterfactual Explainer (CoDiCE), and the impact of the two novel biases against existing methods such as DiCE, FACE, Prototypes, and Growing Spheres. Through a series of ablation experiments on both synthetic and real datasets with continuous and mixed-type features, we demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: Generative Modelling with High-Order Langevin Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Ziqiang Shi, Rujie Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12814">https://arxiv.org/abs/2404.12814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12814">https://arxiv.org/pdf/2404.12814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12814]] Generative Modelling with High-Order Langevin Dynamics(https://arxiv.org/abs/2404.12814)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion generative modelling (DGM) based on stochastic differential equations (SDEs) with score matching has achieved unprecedented results in data generation. In this paper, we propose a novel fast high-quality generative modelling method based on high-order Langevin dynamics (HOLD) with score matching. This motive is proved by third-order Langevin dynamics. By augmenting the previous SDEs, e.g. variance exploding or variance preserving SDEs for single-data variable processes, HOLD can simultaneously model position, velocity, and acceleration, thereby improving the quality and speed of the data generation at the same time. HOLD is composed of one Ornstein-Uhlenbeck process and two Hamiltonians, which reduce the mixing time by two orders of magnitude. Empirical experiments for unconditional image generation on the public data set CIFAR-10 and CelebA-HQ show that the effect is significant in both Frechet inception distance (FID) and negative log-likelihood, and achieves the state-of-the-art FID of 1.85 on CIFAR-10.</li>
</ul>

<h3>Title: COIN: Counterfactual inpainting for weakly supervised semantic  segmentation for medical images</h3>
<ul>
<li><strong>Authors: </strong>Dmytro Shvetsov, Joonas Ariva, Marharyta Domnich, Raul Vicente, Dmytro Fishman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12832">https://arxiv.org/abs/2404.12832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12832">https://arxiv.org/pdf/2404.12832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12832]] COIN: Counterfactual inpainting for weakly supervised semantic  segmentation for medical images(https://arxiv.org/abs/2404.12832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning is dramatically transforming the field of medical imaging and radiology, enabling the identification of pathologies in medical images, including computed tomography (CT) and X-ray scans. However, the performance of deep learning models, particularly in segmentation tasks, is often limited by the need for extensive annotated datasets. To address this challenge, the capabilities of weakly supervised semantic segmentation are explored through the lens of Explainable AI and the generation of counterfactual explanations. The scope of this research is development of a novel counterfactual inpainting approach (COIN) that flips the predicted classification label from abnormal to normal by using a generative model. For instance, if the classifier deems an input medical image X as abnormal, indicating the presence of a pathology, the generative model aims to inpaint the abnormal region, thus reversing the classifier's original prediction label. The approach enables us to produce precise segmentations for pathologies without depending on pre-existing segmentation masks. Crucially, image-level labels are utilized, which are substantially easier to acquire than creating detailed segmentation masks. The effectiveness of the method is demonstrated by segmenting synthetic targets and actual kidney tumors from CT images acquired from Tartu University Hospital in Estonia. The findings indicate that COIN greatly surpasses established attribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an alternative counterfactual explanation method introduced by Singla et al. This evidence suggests that COIN is a promising approach for semantic segmentation of tumors in CT images, and presents a step forward in making deep learning applications more accessible and effective in healthcare, where annotated data is scarce.</li>
</ul>

<h3>Title: Explainable Deepfake Video Detection using Convolutional Neural Network  and CapsuleNet</h3>
<ul>
<li><strong>Authors: </strong>Gazi Hasin Ishrak, Zalish Mahmud, MD. Zami Al Zunaed Farabe, Tahera Khanom Tinni, Tanzim Reza, Mohammad Zavid Parvez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12841">https://arxiv.org/abs/2404.12841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12841">https://arxiv.org/pdf/2404.12841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12841]] Explainable Deepfake Video Detection using Convolutional Neural Network  and CapsuleNet(https://arxiv.org/abs/2404.12841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deepfake technology, derived from deep learning, seamlessly inserts individuals into digital media, irrespective of their actual participation. Its foundation lies in machine learning and Artificial Intelligence (AI). Initially, deepfakes served research, industry, and entertainment. While the concept has existed for decades, recent advancements render deepfakes nearly indistinguishable from reality. Accessibility has soared, empowering even novices to create convincing deepfakes. However, this accessibility raises security concerns.The primary deepfake creation algorithm, GAN (Generative Adversarial Network), employs machine learning to craft realistic images or videos. Our objective is to utilize CNN (Convolutional Neural Network) and CapsuleNet with LSTM to differentiate between deepfake-generated frames and originals. Furthermore, we aim to elucidate our model's decision-making process through Explainable AI, fostering transparent human-AI relationships and offering practical examples for real-life scenarios.</li>
</ul>

<h3>Title: Ransomware Detection and Classification Using Random Forest: A Case  Study with the UGRansome2024 Dataset</h3>
<ul>
<li><strong>Authors: </strong>Peace Azugo, Hein Venter, Mike Wa Nkongolo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12855">https://arxiv.org/abs/2404.12855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12855">https://arxiv.org/pdf/2404.12855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12855]] Ransomware Detection and Classification Using Random Forest: A Case  Study with the UGRansome2024 Dataset(https://arxiv.org/abs/2404.12855)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Cybersecurity faces challenges in identifying and mitigating ransomware, which is important for protecting critical infrastructures. The absence of datasets for distinguishing normal versus abnormal network behaviour hinders the development of proactive detection strategies against ransomware. An obstacle in proactive prevention methods is the absence of comprehensive datasets for contrasting normal versus abnormal network behaviours. The dataset enabling such contrasts would significantly expedite threat anomaly mitigation. In this study, we introduce UGRansome2024, an optimised dataset for ransomware detection in network traffic. This dataset is derived from the UGRansome data using an intuitionistic feature engineering approach that considers only relevant patterns in network behaviour analysis. The study presents an analysis of ransomware detection using the UGRansome2024 dataset and the Random Forest algorithm. Through encoding and feature relevance determination, the Random Forest achieved a classification accuracy of 96% and effectively identified unusual ransomware transactions. Findings indicate that certain ransomware variants, such as those utilising Encrypt Decrypt Algorithms (EDA) and Globe ransomware, have the highest financial impact. These insights have significant implications for real-world cybersecurity practices, highlighting the importance of machine learning in ransomware detection and mitigation. Further research is recommended to expand datasets, explore alternative detection methods, and address limitations in current approaches.</li>
</ul>

<h3>Title: Foundation Model assisted Weakly Supervised LiDAR Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yilong Chen, Zongyi Xu, xiaoshui Huang, Ruicheng Zhang, Xinqi Jiang, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12861">https://arxiv.org/abs/2404.12861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12861">https://arxiv.org/pdf/2404.12861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12861]] Foundation Model assisted Weakly Supervised LiDAR Semantic Segmentation(https://arxiv.org/abs/2404.12861)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Current point cloud semantic segmentation has achieved great advances when given sufficient labels. However, the dense annotation of LiDAR point clouds remains prohibitively expensive and time-consuming, unable to keep up with the continuously growing volume of data. In this paper, we propose annotating images with scattered points, followed by utilizing SAM (a Foundation model) to generate semantic segmentation labels for the images. Finally, by mapping the segmentation labels of the images to the LiDAR space using the intrinsic and extrinsic parameters of the camera and LiDAR, we obtain labels for point cloud semantic segmentation, and release Scatter-KITTI and Scatter-nuScenes, which are the first works to utilize image segmentation-based SAM for weakly supervised point cloud semantic segmentation. Furthermore, to mitigate the influence of erroneous pseudo labels obtained from sparse annotations on point cloud features, we propose a multi-modal weakly supervised network for LiDAR semantic segmentation, called MM-ScatterNet. This network combines features from both point cloud and image modalities, enhancing the representation learning of point clouds by introducing consistency constraints between multi-modal features and point cloud features. On the SemanticKITTI dataset, we achieve 66\% of fully supervised performance using only 0.02% of annotated data, and on the NuScenes dataset, we achieve 95% of fully supervised performance using only 0.1% labeled points.</li>
</ul>

<h3>Title: How Does the Textual Information Affect the Retrieval of Multimodal  In-Context Learning?</h3>
<ul>
<li><strong>Authors: </strong>Yang Luo, Zangwei Zheng, Zirui Zhu, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12866">https://arxiv.org/abs/2404.12866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12866">https://arxiv.org/pdf/2404.12866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12866]] How Does the Textual Information Affect the Retrieval of Multimodal  In-Context Learning?(https://arxiv.org/abs/2404.12866)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The increase in parameter size of multimodal large language models (MLLMs) introduces significant capabilities, particularly in-context learning, where MLLMs enhance task performance without updating pre-trained parameters. This effectiveness, however, hinges on the appropriate selection of in-context examples, a process that is currently biased towards visual data, overlooking textual information. Furthermore, the area of supervised retrievers for MLLMs, crucial for optimal in-context example selection, continues to be uninvestigated. Our study offers an in-depth evaluation of the impact of textual information on the unsupervised selection of in-context examples in multimodal contexts, uncovering a notable sensitivity of retriever performance to the employed modalities. Responding to this, we introduce a novel supervised MLLM-retriever MSIER that employs a neural network to select examples that enhance multimodal in-context learning efficiency. This approach is validated through extensive testing across three distinct tasks, demonstrating the method's effectiveness. Additionally, we investigate the influence of modalities on our supervised retrieval method's training and pinpoint factors contributing to our model's success. This exploration paves the way for future advancements, highlighting the potential for refined in-context learning in MLLMs through the strategic use of multimodal data.</li>
</ul>

<h3>Title: MCM: Multi-condition Motion Synthesis Framework</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Ling, Bo Han, Yongkang Wongkan, Han Lin, Mohan Kankanhalli, Weidong Geng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12886">https://arxiv.org/abs/2404.12886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12886">https://arxiv.org/pdf/2404.12886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12886]] MCM: Multi-condition Motion Synthesis Framework(https://arxiv.org/abs/2404.12886)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Conditional human motion synthesis (HMS) aims to generate human motion sequences that conform to specific conditions. Text and audio represent the two predominant modalities employed as HMS control conditions. While existing research has primarily focused on single conditions, the multi-condition human motion synthesis remains underexplored. In this study, we propose a multi-condition HMS framework, termed MCM, based on a dual-branch structure composed of a main branch and a control branch. This framework effectively extends the applicability of the diffusion model, which is initially predicated solely on textual conditions, to auditory conditions. This extension encompasses both music-to-dance and co-speech HMS while preserving the intrinsic quality of motion and the capabilities for semantic association inherent in the original model. Furthermore, we propose the implementation of a Transformer-based diffusion model, designated as MWNet, as the main branch. This model adeptly apprehends the spatial intricacies and inter-joint correlations inherent in motion sequences, facilitated by the integration of multi-wise self-attention modules. Extensive experiments show that our method achieves competitive results in single-condition and multi-condition HMS tasks.</li>
</ul>

<h3>Title: Training-and-prompt-free General Painterly Harmonization Using  Image-wise Attention Sharing</h3>
<ul>
<li><strong>Authors: </strong>Teng-Fang Hsiao, Bo-Kai Ruan, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12900">https://arxiv.org/abs/2404.12900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12900">https://arxiv.org/pdf/2404.12900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12900]] Training-and-prompt-free General Painterly Harmonization Using  Image-wise Attention Sharing(https://arxiv.org/abs/2404.12900)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Painterly Image Harmonization aims at seamlessly blending disparate visual elements within a single coherent image. However, previous approaches often encounter significant limitations due to training data constraints, the need for time-consuming fine-tuning, or reliance on additional prompts. To surmount these hurdles, we design a Training-and-prompt-Free General Painterly Harmonization method using image-wise attention sharing (TF-GPH), which integrates a novel "share-attention module". This module redefines the traditional self-attention mechanism by allowing for comprehensive image-wise attention, facilitating the use of a state-of-the-art pretrained latent diffusion model without the typical training data limitations. Additionally, we further introduce "similarity reweighting" mechanism enhances performance by effectively harnessing cross-image information, surpassing the capabilities of fine-tuning or prompt-based approaches. At last, we recognize the deficiencies in existing benchmarks and propose the "General Painterly Harmonization Benchmark", which employs range-based evaluation metrics to more accurately reflect real-world application. Extensive experiments demonstrate the superior efficacy of our method across various benchmarks. The code and web demo are available at https://github.com/BlueDyee/TF-GPH.</li>
</ul>

<h3>Title: Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Santosh, Li Lin, Irene Amerini, Xin Wang, Shu Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12908">https://arxiv.org/abs/2404.12908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12908">https://arxiv.org/pdf/2404.12908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12908]] Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images(https://arxiv.org/abs/2404.12908)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have revolutionized image generation, producing high-quality images with applications spanning various fields. However, their ability to create hyper-realistic images poses significant challenges in distinguishing between real and synthetic content, raising concerns about digital authenticity and potential misuse in creating deepfakes. This work introduces a robust detection framework that integrates image and text features extracted by CLIP model with a Multilayer Perceptron (MLP) classifier. We propose a novel loss that can improve the detector's robustness and handle imbalanced datasets. Additionally, we flatten the loss landscape during the model training to improve the detector's generalization capabilities. The effectiveness of our method, which outperforms traditional detection techniques, is demonstrated through extensive experiments, underscoring its potential to set a new state-of-the-art approach in DM-generated image detection. The code is available at https://github.com/Purdue-M2/Robust_DM_Generated_Image_Detection.</li>
</ul>

<h3>Title: Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Konstantinos Vilouras, Pedro Sanchez, Alison Q. O'Neil, Sotirios A. Tsaftaris</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12920">https://arxiv.org/abs/2404.12920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12920">https://arxiv.org/pdf/2404.12920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12920]] Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models(https://arxiv.org/abs/2404.12920)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>Localizing the exact pathological regions in a given medical scan is an important imaging problem that requires a large amount of bounding box ground truth annotations to be accurately solved. However, there exist alternative, potentially weaker, forms of supervision, such as accompanying free-text reports, which are readily available. The task of performing localization with textual guidance is commonly referred to as phrase grounding. In this work, we use a publicly available Foundation Model, namely the Latent Diffusion Model, to solve this challenging task. This choice is supported by the fact that the Latent Diffusion Model, despite being generative in nature, contains mechanisms (cross-attention) that implicitly align visual and textual features, thus leading to intermediate representations that are suitable for the task at hand. In addition, we aim to perform this task in a zero-shot manner, i.e., without any further training on target data, meaning that the model's weights remain frozen. To this end, we devise strategies to select features and also refine them via post-processing without extra learnable parameters. We compare our proposed method with state-of-the-art approaches which explicitly enforce image-text alignment in a joint embedding space via contrastive learning. Results on a popular chest X-ray benchmark indicate that our method is competitive wih SOTA on different types of pathology, and even outperforms them on average in terms of two metrics (mean IoU and AUC-ROC). Source code will be released upon acceptance.</li>
</ul>

<h3>Title: A Hybrid Generative and Discriminative PointNet on Unordered Point Sets</h3>
<ul>
<li><strong>Authors: </strong>Yang Ye, Shihao Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12925">https://arxiv.org/abs/2404.12925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12925">https://arxiv.org/pdf/2404.12925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12925]] A Hybrid Generative and Discriminative PointNet on Unordered Point Sets(https://arxiv.org/abs/2404.12925)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As point cloud provides a natural and flexible representation usable in myriad applications (e.g., robotics and self-driving cars), the ability to synthesize point clouds for analysis becomes crucial. Recently, Xie et al. propose a generative model for unordered point sets in the form of an energy-based model (EBM). Despite the model achieving an impressive performance for point cloud generation, one separate model needs to be trained for each category to capture the complex point set distributions. Besides, their method is unable to classify point clouds directly and requires additional fine-tuning for classification. One interesting question is: Can we train a single network for a hybrid generative and discriminative model of point clouds? A similar question has recently been answered in the affirmative for images, introducing the framework of Joint Energy-based Model (JEM), which achieves high performance in image classification and generation simultaneously. This paper proposes GDPNet, the first hybrid Generative and Discriminative PointNet that extends JEM for point cloud classification and generation. Our GDPNet retains strong discriminative power of modern PointNet classifiers, while generating point cloud samples rivaling state-of-the-art generative approaches.</li>
</ul>

<h3>Title: Purposer: Putting Human Motion Generation in Context</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Ugrinovic, Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, Gregory Rogez, Francesc Moreno-Noguer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12942">https://arxiv.org/abs/2404.12942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12942">https://arxiv.org/pdf/2404.12942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12942]] Purposer: Putting Human Motion Generation in Context(https://arxiv.org/abs/2404.12942)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a novel method to generate human motion to populate 3D indoor scenes. It can be controlled with various combinations of conditioning signals such as a path in a scene, target poses, past motions, and scenes represented as 3D point clouds. State-of-the-art methods are either models specialized to one single setting, require vast amounts of high-quality and diverse training data, or are unconditional models that do not integrate scene or other contextual information. As a consequence, they have limited applicability and rely on costly training data. To address these limitations, we propose a new method ,dubbed Purposer, based on neural discrete representation learning. Our model is capable of exploiting, in a flexible manner, different types of information already present in open access large-scale datasets such as AMASS. First, we encode unconditional human motion into a discrete latent space. Second, an autoregressive generative model, conditioned with key contextual information, either with prompting or additive tokens, and trained for next-step prediction in this space, synthesizes sequences of latent indices. We further design a novel conditioning block to handle future conditioning information in such a causal model by using a network with two branches to compute separate stacks of features. In this manner, Purposer can generate realistic motion sequences in diverse test scenes. Through exhaustive evaluation, we demonstrate that our multi-contextual solution outperforms existing specialized approaches for specific contextual information, both in terms of quality and diversity. Our model is trained with short sequences, but a byproduct of being able to use various conditioning signals is that at test time different combinations can be used to chain short sequences together and generate long motions within a context scene.</li>
</ul>

<h3>Title: Towards Reliable Latent Knowledge Estimation in LLMs: In-Context  Learning vs. Prompting Based Factual Knowledge Extraction</h3>
<ul>
<li><strong>Authors: </strong>Qinyuan Wu, Mohammad Aflah Khan, Soumi Das, Vedant Nanda, Bishwamittra Ghosh, Camila Kolling, Till Speicher, Laurent Bindschaedler, Krishna P. Gummadi, Evimaria Terzi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.12957">https://arxiv.org/abs/2404.12957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.12957">https://arxiv.org/pdf/2404.12957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.12957]] Towards Reliable Latent Knowledge Estimation in LLMs: In-Context  Learning vs. Prompting Based Factual Knowledge Extraction(https://arxiv.org/abs/2404.12957)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We propose an approach for estimating the latent knowledge embedded inside large language models (LLMs). We leverage the in-context learning (ICL) abilities of LLMs to estimate the extent to which an LLM knows the facts stored in a knowledge base. Our knowledge estimator avoids reliability concerns with previous prompting-based methods, is both conceptually simpler and easier to apply, and we demonstrate that it can surface more of the latent knowledge embedded in LLMs. We also investigate how different design choices affect the performance of ICL-based knowledge estimation. Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base. We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts.</li>
</ul>

<h3>Title: Stronger Random Baselines for In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Gregory Yauney, David Mimno</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13020">https://arxiv.org/abs/2404.13020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13020">https://arxiv.org/pdf/2404.13020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13020]] Stronger Random Baselines for In-Context Learning(https://arxiv.org/abs/2404.13020)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Evaluating the in-context learning classification performance of language models poses challenges due to small dataset sizes, extensive prompt-selection using the validation set, and intentionally difficult tasks that lead to near-random performance. The standard random baseline -- the expected accuracy of guessing labels uniformly at random -- is stable when the evaluation set is used only once or when the dataset is large. We account for the common practice of validation set reuse and existing small datasets with a stronger random baseline: the expected maximum accuracy across multiple random classifiers. When choosing the best prompt demonstrations across six quantized language models applied to 16 BIG-bench Lite tasks, more than 20\% of the few-shot results that exceed the standard baseline do not exceed this stronger random baseline. When held-out test sets are available, this stronger baseline is also a better predictor of held-out performance than the standard baseline, avoiding unnecessary test set evaluations. This maximum random baseline provides an easily calculated drop-in replacement for the standard baseline.</li>
</ul>

<h3>Title: Sample Design Engineering: An Empirical Study of What Makes Good  Downstream Fine-Tuning Samples for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Biyang Guo, He Wang, Wenyilin Xiao, Hong Chen, Zhuxin Lee, Songqiao Han, Hailiang Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13033">https://arxiv.org/abs/2404.13033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13033">https://arxiv.org/pdf/2404.13033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13033]] Sample Design Engineering: An Empirical Study of What Makes Good  Downstream Fine-Tuning Samples for LLMs(https://arxiv.org/abs/2404.13033)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In the burgeoning field of Large Language Models (LLMs) like ChatGPT and LLaMA, Prompt Engineering (PE) is renowned for boosting zero-shot or in-context learning (ICL) through prompt modifications. Yet, the realm of the sample design for downstream fine-tuning, crucial for task-specific LLM adaptation, is largely unexplored. This paper introduces Sample Design Engineering (SDE), a methodical approach to enhancing LLMs' post-tuning performance by refining input, output, and reasoning designs. We conduct a series of in-domain (ID) and out-of-domain (OOD) experiments to assess the impact of various design options on LLMs' downstream performance, revealing several intriguing patterns that hold consistently across different LLMs. Based on these insights, we propose an integrated SDE strategy, combining the most effective options, and validate its consistent superiority over heuristic sample designs in complex downstream tasks like multi-aspect sentiment analysis, event extraction, and nested entity recognition. Additionally, analyses of LLMs' inherent prompt/output perplexity, zero-shot, and ICL abilities illustrate that good PE strategies may not always translate to good SDE strategies. Code available at https://github.com/beyondguo/LLM-Tuning.</li>
</ul>

<h3>Title: Analysis of Classifier-Free Guidance Weight Schedulers</h3>
<ul>
<li><strong>Authors: </strong>Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernandez Abrevaya, David Picard, Vicky Kalogeiton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13040">https://arxiv.org/abs/2404.13040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13040">https://arxiv.org/pdf/2404.13040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13040]] Analysis of Classifier-Free Guidance Weight Schedulers(https://arxiv.org/abs/2404.13040)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Classifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional predictions using a fixed weight. However, recent works vary the weights throughout the diffusion process, reporting superior results but without providing any rationale or analysis. By conducting comprehensive experiments, this paper provides insights into CFG weight schedulers. Our findings suggest that simple, monotonically increasing weight schedulers consistently lead to improved performances, requiring merely a single line of code. In addition, more complex parametrized schedulers can be optimized for further improvement, but do not generalize across different models and tasks.</li>
</ul>

<h3>Title: Data Alignment for Zero-Shot Concept Generation in Dermatology AI</h3>
<ul>
<li><strong>Authors: </strong>Soham Gadgil, Mahtab Bigverdi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13043">https://arxiv.org/abs/2404.13043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13043">https://arxiv.org/pdf/2404.13043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13043]] Data Alignment for Zero-Shot Concept Generation in Dermatology AI(https://arxiv.org/abs/2404.13043)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>AI in dermatology is evolving at a rapid pace but the major limitation to training trustworthy classifiers is the scarcity of data with ground-truth concept level labels, which are meta-labels semantically meaningful to humans. Foundation models like CLIP providing zero-shot capabilities can help alleviate this challenge by leveraging vast amounts of image-caption pairs available on the internet. CLIP can be fine-tuned using domain specific image-caption pairs to improve classification performance. However, CLIP's pre-training data is not well-aligned with the medical jargon that clinicians use to perform diagnoses. The development of large language models (LLMs) in recent years has led to the possibility of leveraging the expressive nature of these models to generate rich text. Our goal is to use these models to generate caption text that aligns well with both the clinical lexicon and with the natural human language used in CLIP's pre-training data. Starting with captions used for images in PubMed articles, we extend them by passing the raw captions through an LLM fine-tuned on the field's several textbooks. We find that using captions generated by an expressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept classification performance.</li>
</ul>

<h3>Title: Unified Scene Representation and Reconstruction for 3D Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Tao Chu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Qiong Liu, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13044">https://arxiv.org/abs/2404.13044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13044">https://arxiv.org/pdf/2404.13044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13044]] Unified Scene Representation and Reconstruction for 3D Large Language  Models(https://arxiv.org/abs/2404.13044)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Enabling Large Language Models (LLMs) to interact with 3D environments is challenging. Existing approaches extract point clouds either from ground truth (GT) geometry or 3D scenes reconstructed by auxiliary models. Text-image aligned 2D features from CLIP are then lifted to point clouds, which serve as inputs for LLMs. However, this solution lacks the establishment of 3D point-to-point connections, leading to a deficiency of spatial structure information. Concurrently, the absence of integration and unification between the geometric and semantic representations of the scene culminates in a diminished level of 3D scene understanding. In this paper, we demonstrate the importance of having a unified scene representation and reconstruction framework, which is essential for LLMs in 3D scenes. Specifically, we introduce Uni3DR^2 extracts 3D geometric and semantic aware representation features via the frozen pre-trained 2D foundation models (e.g., CLIP and SAM) and a multi-scale aggregate 3D decoder. Our learned 3D representations not only contribute to the reconstruction process but also provide valuable knowledge for LLMs. Experimental results validate that our Uni3DR^2 yields convincing gains over the baseline on the 3D reconstruction dataset ScanNet (increasing F-Score by +1.8\%). When applied to LLMs, our Uni3DR^2-LLM exhibits superior performance over the baseline on the 3D vision-language understanding dataset ScanQA (increasing BLEU-1 by +4.0\% and +4.2\% on the val set and test set, respectively). Furthermore, it outperforms the state-of-the-art method that uses additional GT point clouds on both ScanQA and 3DMV-VQA.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
