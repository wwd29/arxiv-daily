<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-23</h1>
<h3>Title: FlowMind: Automatic Workflow Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds, Tucker Balch, Manuela Veloso</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13050">https://arxiv.org/abs/2404.13050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13050">https://arxiv.org/pdf/2404.13050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13050]] FlowMind: Automatic Workflow Generation with LLMs(https://arxiv.org/abs/2404.13050)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapidly evolving field of Robotic Process Automation (RPA) has made significant strides in automating repetitive processes, yet its effectiveness diminishes in scenarios requiring spontaneous or unpredictable tasks demanded by users. This paper introduces a novel approach, FlowMind, leveraging the capabilities of Large Language Models (LLMs) such as Generative Pretrained Transformer (GPT), to address this limitation and create an automatic workflow generation system. In FlowMind, we propose a generic prompt recipe for a lecture that helps ground LLM reasoning with reliable Application Programming Interfaces (APIs). With this, FlowMind not only mitigates the common issue of hallucinations in LLMs, but also eliminates direct interaction between LLMs and proprietary data or code, thus ensuring the integrity and confidentiality of information - a cornerstone in financial services. FlowMind further simplifies user interaction by presenting high-level descriptions of auto-generated workflows, enabling users to inspect and provide feedback effectively. We also introduce NCEN-QA, a new dataset in finance for benchmarking question-answering tasks from N-CEN reports on funds. We used NCEN-QA to evaluate the performance of workflows generated by FlowMind against baseline and ablation variants of FlowMind. We demonstrate the success of FlowMind, the importance of each component in the proposed lecture recipe, and the effectiveness of user interaction and feedback in FlowMind.</li>
</ul>

<h3>Title: Towards Efficient Resume Understanding: A Multi-Granularity Multi-Modal  Pre-Training Approach</h3>
<ul>
<li><strong>Authors: </strong>Feihu Jiang, Chuan Qin, Jingshuai Zhang, Kaichun Yao, Xi Chen, Dazhong Shen, Chen Zhu, Hengshu Zhu, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13067">https://arxiv.org/abs/2404.13067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13067">https://arxiv.org/pdf/2404.13067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13067]] Towards Efficient Resume Understanding: A Multi-Granularity Multi-Modal  Pre-Training Approach(https://arxiv.org/abs/2404.13067)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In the contemporary era of widespread online recruitment, resume understanding has been widely acknowledged as a fundamental and crucial task, which aims to extract structured information from resume documents automatically. Compared to the traditional rule-based approaches, the utilization of recently proposed pre-trained document understanding models can greatly enhance the effectiveness of resume understanding. The present approaches have, however, disregarded the hierarchical relations within the structured information presented in resumes, and have difficulty parsing resumes in an efficient manner. To this end, in this paper, we propose a novel model, namely ERU, to achieve efficient resume understanding. Specifically, we first introduce a layout-aware multi-modal fusion transformer for encoding the segments in the resume with integrated textual, visual, and layout information. Then, we design three self-supervised tasks to pre-train this module via a large number of unlabeled resumes. Next, we fine-tune the model with a multi-granularity sequence labeling task to extract structured information from resumes. Finally, extensive experiments on a real-world dataset clearly demonstrate the effectiveness of ERU.</li>
</ul>

<h3>Title: Modeling Emotions and Ethics with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Edward Y. Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13071">https://arxiv.org/abs/2404.13071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13071">https://arxiv.org/pdf/2404.13071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13071]] Modeling Emotions and Ethics with Large Language Models(https://arxiv.org/abs/2404.13071)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper explores the integration of human-like emotions and ethical considerations into Large Language Models (LLMs). We first model eight fundamental human emotions, presented as opposing pairs, and employ collaborative LLMs to reinterpret and express these emotions across a spectrum of intensity. Our focus extends to embedding a latent ethical dimension within LLMs, guided by a novel self-supervised learning algorithm with human feedback (SSHF). This approach enables LLMs to perform self-evaluations and adjustments concerning ethical guidelines, enhancing their capability to generate content that is not only emotionally resonant but also ethically aligned. The methodologies and case studies presented herein illustrate the potential of LLMs to transcend mere text and image generation, venturing into the realms of empathetic interaction and principled decision-making, thereby setting a new precedent in the development of emotionally aware and ethically conscious AI systems.</li>
</ul>

<h3>Title: Equivariant Imaging for Self-supervised Hyperspectral Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Shuo Li, Mike Davies, Mehrdad Yaghoobi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13159">https://arxiv.org/abs/2404.13159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13159">https://arxiv.org/pdf/2404.13159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13159]] Equivariant Imaging for Self-supervised Hyperspectral Image Inpainting(https://arxiv.org/abs/2404.13159)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) is a key technology for earth observation, surveillance, medical imaging and diagnostics, astronomy and space exploration. The conventional technology for HSI in remote sensing applications is based on the push-broom scanning approach in which the camera records the spectral image of a stripe of the scene at a time, while the image is generated by the aggregation of measurements through time. In real-world airborne and spaceborne HSI instruments, some empty stripes would appear at certain locations, because platforms do not always maintain a constant programmed attitude, or have access to accurate digital elevation maps (DEM), and the travelling track is not necessarily aligned with the hyperspectral cameras at all times. This makes the enhancement of the acquired HS images from incomplete or corrupted observations an essential task. We introduce a novel HSI inpainting algorithm here, called Hyperspectral Equivariant Imaging (Hyper-EI). Hyper-EI is a self-supervised learning-based method which does not require training on extensive datasets or access to a pre-trained model. Experimental results show that the proposed method achieves state-of-the-art inpainting performance compared to the existing methods.</li>
</ul>

<h3>Title: Privacy-Preserving Debiasing using Data Augmentation and Machine  Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Zhixin Pan, Emma Andrews, Laura Chang, Prabhat Mishra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13194">https://arxiv.org/abs/2404.13194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13194">https://arxiv.org/pdf/2404.13194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13194]] Privacy-Preserving Debiasing using Data Augmentation and Machine  Unlearning(https://arxiv.org/abs/2404.13194)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Data augmentation is widely used to mitigate data bias in the training dataset. However, data augmentation exposes machine learning models to privacy attacks, such as membership inference attacks. In this paper, we propose an effective combination of data augmentation and machine unlearning, which can reduce data bias while providing a provable defense against known attacks. Specifically, we maintain the fairness of the trained model with diffusion-based data augmentation, and then utilize multi-shard unlearning to remove identifying information of original data from the ML model for protection against privacy attacks. Experimental evaluation across diverse datasets demonstrates that our approach can achieve significant improvements in bias reduction as well as robustness against state-of-the-art privacy attacks.</li>
</ul>

<h3>Title: Beyond Pixel-Wise Supervision for Medical Image Segmentation: From  Traditional Models to Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yuyan Shi, Jialu Ma, Jin Yang, Shasha Wang, Yichi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13239">https://arxiv.org/abs/2404.13239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13239">https://arxiv.org/pdf/2404.13239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13239]] Beyond Pixel-Wise Supervision for Medical Image Segmentation: From  Traditional Models to Foundation Models(https://arxiv.org/abs/2404.13239)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Medical image segmentation plays an important role in many image-guided clinical approaches. However, existing segmentation algorithms mostly rely on the availability of fully annotated images with pixel-wise annotations for training, which can be both labor-intensive and expertise-demanding, especially in the medical imaging domain where only experts can provide reliable and accurate annotations. To alleviate this challenge, there has been a growing focus on developing segmentation methods that can train deep models with weak annotations, such as image-level, bounding boxes, scribbles, and points. The emergence of vision foundation models, notably the Segment Anything Model (SAM), has introduced innovative capabilities for segmentation tasks using weak annotations for promptable segmentation enabled by large-scale pre-training. Adopting foundation models together with traditional learning methods has increasingly gained recent interest research community and shown potential for real-world applications. In this paper, we present a comprehensive survey of recent progress on annotation-efficient learning for medical image segmentation utilizing weak annotations before and in the era of foundation models. Furthermore, we analyze and discuss several challenges of existing approaches, which we believe will provide valuable guidance for shaping the trajectory of foundational models to further advance the field of medical image segmentation.</li>
</ul>

<h3>Title: FilterPrompt: Guiding Image Transfer in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xi Wang, Yichen Peng, Heng Fang, Haoran Xie, Xi Yang, Chuntao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13263">https://arxiv.org/abs/2404.13263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13263">https://arxiv.org/pdf/2404.13263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13263]] FilterPrompt: Guiding Image Transfer in Diffusion Models(https://arxiv.org/abs/2404.13263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In controllable generation tasks, flexibly manipulating the generated images to attain a desired appearance or structure based on a single input image cue remains a critical and longstanding challenge. Achieving this requires the effective decoupling of key attributes within the input image data, aiming to get representations accurately. Previous research has predominantly concentrated on disentangling image attributes within feature space. However, the complex distribution present in real-world data often makes the application of such decoupling algorithms to other datasets challenging. Moreover, the granularity of control over feature encoding frequently fails to meet specific task requirements. Upon scrutinizing the characteristics of various generative models, we have observed that the input sensitivity and dynamic evolution properties of the diffusion model can be effectively fused with the explicit decomposition operation in pixel space. This integration enables the image processing operations performed in pixel space for a specific feature distribution of the input image, and can achieve the desired control effect in the generated results. Therefore, we propose FilterPrompt, an approach to enhance the model control effect. It can be universally applied to any diffusion model, allowing users to adjust the representation of specific image features in accordance with task requirements, thereby facilitating more precise and controllable generation outcomes. In particular, our designed experiments demonstrate that the FilterPrompt optimizes feature correlation, mitigates content conflicts during the generation process, and enhances the model's control capability.</li>
</ul>

<h3>Title: Multi-feature Reconstruction Network using Crossed-mask Restoration for  Unsupervised Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Junpu Wang, Guili Xu, Chunlei Li, Guangshuai Gao, Yuehua Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13273">https://arxiv.org/abs/2404.13273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13273">https://arxiv.org/pdf/2404.13273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13273]] Multi-feature Reconstruction Network using Crossed-mask Restoration for  Unsupervised Anomaly Detection(https://arxiv.org/abs/2404.13273)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection using only normal samples is of great significance for quality inspection in industrial manufacturing. Although existing reconstruction-based methods have achieved promising results, they still face two problems: poor distinguishable information in image reconstruction and well abnormal regeneration caused by model over-generalization ability. To overcome the above issues, we convert the image reconstruction into a combination of parallel feature restorations and propose a multi-feature reconstruction network, MFRNet, using crossed-mask restoration in this paper. Specifically, a multi-scale feature aggregator is first developed to generate more discriminative hierarchical representations of the input images from a pre-trained model. Subsequently, a crossed-mask generator is adopted to randomly cover the extracted feature map, followed by a restoration network based on the transformer structure for high-quality repair of the missing regions. Finally, a hybrid loss is equipped to guide model training and anomaly estimation, which gives consideration to both the pixel and structural similarity. Extensive experiments show that our method is highly competitive with or significantly outperforms other state-of-the-arts on four public available datasets and one self-made dataset.</li>
</ul>

<h3>Title: PCQA: A Strong Baseline for AIGC Quality Assessment Based on Prompt  Condition</h3>
<ul>
<li><strong>Authors: </strong>Xi Fang, Weigang Wang, Xiaoxin Lv, Jun Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13299">https://arxiv.org/abs/2404.13299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13299">https://arxiv.org/pdf/2404.13299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13299]] PCQA: A Strong Baseline for AIGC Quality Assessment Based on Prompt  Condition(https://arxiv.org/abs/2404.13299)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The development of Large Language Models (LLM) and Diffusion Models brings the boom of Artificial Intelligence Generated Content (AIGC). It is essential to build an effective quality assessment framework to provide a quantifiable evaluation of different images or videos based on the AIGC technologies. The content generated by AIGC methods is driven by the crafted prompts. Therefore, it is intuitive that the prompts can also serve as the foundation of the AIGC quality assessment. This study proposes an effective AIGC quality assessment (QA) framework. First, we propose a hybrid prompt encoding method based on a dual-source CLIP (Contrastive Language-Image Pre-Training) text encoder to understand and respond to the prompt conditions. Second, we propose an ensemble-based feature mixer module to effectively blend the adapted prompt and vision features. The empirical study practices in two datasets: AIGIQA-20K (AI-Generated Image Quality Assessment database) and T2VQA-DB (Text-to-Video Quality Assessment DataBase), which validates the effectiveness of our proposed method: Prompt Condition Quality Assessment (PCQA). Our proposed simple and feasible framework may promote research development in the multimodal generation field.</li>
</ul>

<h3>Title: STAT: Towards Generalizable Temporal Action Localization</h3>
<ul>
<li><strong>Authors: </strong>Yangcen Liu, Ziyi Liu, Yuanhao Zhai, Wen Li, David Doerman, Junsong Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13311">https://arxiv.org/abs/2404.13311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13311">https://arxiv.org/pdf/2404.13311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13311]] STAT: Towards Generalizable Temporal Action Localization(https://arxiv.org/abs/2404.13311)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Weakly-supervised temporal action localization (WTAL) aims to recognize and localize action instances with only video-level labels. Despite the significant progress, existing methods suffer from severe performance degradation when transferring to different distributions and thus may hardly adapt to real-world scenarios . To address this problem, we propose the Generalizable Temporal Action Localization task (GTAL), which focuses on improving the generalizability of action localization methods. We observed that the performance decline can be primarily attributed to the lack of generalizability to different action scales. To address this problem, we propose STAT (Self-supervised Temporal Adaptive Teacher), which leverages a teacher-student structure for iterative refinement. Our STAT features a refinement module and an alignment module. The former iteratively refines the model's output by leveraging contextual information and helps adapt to the target scale. The latter improves the refinement process by promoting a consensus between student and teacher models. We conduct extensive experiments on three datasets, THUMOS14, ActivityNet1.2, and HACS, and the results show that our method significantly improves the Baseline methods under the cross-distribution evaluation setting, even approaching the same-distribution evaluation performance.</li>
</ul>

<h3>Title: Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than  We Think</h3>
<ul>
<li><strong>Authors: </strong>Haotian Xue, Yongxin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13320">https://arxiv.org/abs/2404.13320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13320">https://arxiv.org/pdf/2404.13320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13320]] Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than  We Think(https://arxiv.org/abs/2404.13320)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Adversarial examples for diffusion models are widely used as solutions for safety concerns. By adding adversarial perturbations to personal images, attackers can not edit or imitate them easily. However, it is essential to note that all these protections target the latent diffusion model (LDMs), the adversarial examples for diffusion models in the pixel space (PDMs) are largely overlooked. This may mislead us to think that the diffusion models are vulnerable to adversarial attacks like most deep models. In this paper, we show novel findings that: even though gradient-based white-box attacks can be used to attack the LDMs, they fail to attack PDMs. This finding is supported by extensive experiments of almost a wide range of attacking methods on various PDMs and LDMs with different model structures, which means diffusion models are indeed much more robust against adversarial attacks. We also find that PDMs can be used as an off-the-shelf purifier to effectively remove the adversarial patterns that were generated on LDMs to protect the images, which means that most protection methods nowadays, to some extent, cannot protect our images from malicious attacks. We hope that our insights will inspire the community to rethink the adversarial samples for diffusion models as protection methods and move forward to more effective protection. Codes are available in https://github.com/xavihart/PDM-Pure.</li>
</ul>

<h3>Title: Hyperspectral Anomaly Detection with Self-Supervised Anomaly Prior</h3>
<ul>
<li><strong>Authors: </strong>Yidan Liu, Weiying Xie, Kai Jiang, Jiaqing Zhang, Yunsong Li, Leyuan Fang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13342">https://arxiv.org/abs/2404.13342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13342">https://arxiv.org/pdf/2404.13342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13342]] Hyperspectral Anomaly Detection with Self-Supervised Anomaly Prior(https://arxiv.org/abs/2404.13342)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>The majority of existing hyperspectral anomaly detection (HAD) methods use the low-rank representation (LRR) model to separate the background and anomaly components, where the anomaly component is optimized by handcrafted sparse priors (e.g., $\ell_{2,1}$-norm). However, this may not be ideal since they overlook the spatial structure present in anomalies and make the detection result largely dependent on manually set sparsity. To tackle these problems, we redefine the optimization criterion for the anomaly component in the LRR model with a self-supervised network called self-supervised anomaly prior (SAP). This prior is obtained by the pretext task of self-supervised learning, which is customized to learn the characteristics of hyperspectral anomalies. Specifically, this pretext task is a classification task to distinguish the original hyperspectral image (HSI) and the pseudo-anomaly HSI, where the pseudo-anomaly is generated from the original HSI and designed as a prism with arbitrary polygon bases and arbitrary spectral bands. In addition, a dual-purified strategy is proposed to provide a more refined background representation with an enriched background dictionary, facilitating the separation of anomalies from complex backgrounds. Extensive experiments on various hyperspectral datasets demonstrate that the proposed SAP offers a more accurate and interpretable solution than other advanced HAD methods.</li>
</ul>

<h3>Title: Generating Daylight-driven Architectural Design via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pengzhi Li, Baijuan Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13353">https://arxiv.org/abs/2404.13353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13353">https://arxiv.org/pdf/2404.13353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13353]] Generating Daylight-driven Architectural Design via Diffusion Models(https://arxiv.org/abs/2404.13353)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, the rapid development of large-scale models has made new possibilities for interdisciplinary fields such as architecture. In this paper, we present a novel daylight-driven AI-aided architectural design method. Firstly, we formulate a method for generating massing models, producing architectural massing models using random parameters quickly. Subsequently, we integrate a daylight-driven facade design strategy, accurately determining window layouts and applying them to the massing models. Finally, we seamlessly combine a large-scale language model with a text-to-image model, enhancing the efficiency of generating visual architectural design renderings. Experimental results demonstrate that our approach supports architects' creative inspirations and pioneers novel avenues for architectural design development. Project page: https://zrealli.github.io/DDADesign/.</li>
</ul>

<h3>Title: NeurCADRecon: Neural Representation for Reconstructing CAD Surfaces by  Enforcing Zero Gaussian Curvature</h3>
<ul>
<li><strong>Authors: </strong>Qiujie Dong, Rui Xu, Pengfei Wang, Shuangmin Chen, Shiqing Xin, Xiaohong Jia, Wenping Wang, Changhe Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13420">https://arxiv.org/abs/2404.13420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13420">https://arxiv.org/pdf/2404.13420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13420]] NeurCADRecon: Neural Representation for Reconstructing CAD Surfaces by  Enforcing Zero Gaussian Curvature(https://arxiv.org/abs/2404.13420)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Despite recent advances in reconstructing an organic model with the neural signed distance function (SDF), the high-fidelity reconstruction of a CAD model directly from low-quality unoriented point clouds remains a significant challenge. In this paper, we address this challenge based on the prior observation that the surface of a CAD model is generally composed of piecewise surface patches, each approximately developable even around the feature line. Our approach, named NeurCADRecon, is self-supervised, and its loss includes a developability term to encourage the Gaussian curvature toward 0 while ensuring fidelity to the input points. Noticing that the Gaussian curvature is non-zero at tip points, we introduce a double-trough curve to tolerate the existence of these tip points. Furthermore, we develop a dynamic sampling strategy to deal with situations where the given points are incomplete or too sparse. Since our resulting neural SDFs can clearly manifest sharp feature points/lines, one can easily extract the feature-aligned triangle mesh from the SDF and then decompose it into smooth surface patches, greatly reducing the difficulty of recovering the parametric CAD design. A comprehensive comparison with existing state-of-the-art methods shows the significant advantage of our approach in reconstructing faithful CAD shapes.</li>
</ul>

<h3>Title: Generalized Regression with Conditional GANs</h3>
<ul>
<li><strong>Authors: </strong>Deddy Jobson, Eddy Hudson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13500">https://arxiv.org/abs/2404.13500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13500">https://arxiv.org/pdf/2404.13500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13500]] Generalized Regression with Conditional GANs(https://arxiv.org/abs/2404.13500)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Regression is typically treated as a curve-fitting process where the goal is to fit a prediction function to data. With the help of conditional generative adversarial networks, we propose to solve this age-old problem in a different way; we aim to learn a prediction function whose outputs, when paired with the corresponding inputs, are indistinguishable from feature-label pairs in the training dataset. We show that this approach to regression makes fewer assumptions on the distribution of the data we are fitting to and, therefore, has better representation capabilities. We draw parallels with generalized linear models in statistics and show how our proposal serves as an extension of them to neural networks. We demonstrate the superiority of this new approach to standard regression with experiments on multiple synthetic and publicly available real-world datasets, finding encouraging results, especially with real-world heavy-tailed regression datasets. To make our work more reproducible, we release our source code. Link to repository: https://anonymous.4open.science/r/regressGAN-7B71/</li>
</ul>

<h3>Title: Dynamic in Static: Hybrid Visual Correspondence for Self-Supervised  Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Gensheng Pei, Yazhou Yao, Jianbo Jiao, Wenguan Wang, Liqiang Nie, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13505">https://arxiv.org/abs/2404.13505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13505">https://arxiv.org/pdf/2404.13505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13505]] Dynamic in Static: Hybrid Visual Correspondence for Self-Supervised  Video Object Segmentation(https://arxiv.org/abs/2404.13505)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Conventional video object segmentation (VOS) methods usually necessitate a substantial volume of pixel-level annotated video data for fully supervised learning. In this paper, we present HVC, a \textbf{h}ybrid static-dynamic \textbf{v}isual \textbf{c}orrespondence framework for self-supervised VOS. HVC extracts pseudo-dynamic signals from static images, enabling an efficient and scalable VOS model. Our approach utilizes a minimalist fully-convolutional architecture to capture static-dynamic visual correspondence in image-cropped views. To achieve this objective, we present a unified self-supervised approach to learn visual representations of static-dynamic feature similarity. Firstly, we establish static correspondence by utilizing a priori coordinate information between cropped views to guide the formation of consistent static feature representations. Subsequently, we devise a concise convolutional layer to capture the forward / backward pseudo-dynamic signals between two views, serving as cues for dynamic representations. Finally, we propose a hybrid visual correspondence loss to learn joint static and dynamic consistency representations. Our approach, without bells and whistles, necessitates only one training session using static image data, significantly reducing memory consumption ($\sim$16GB) and training time ($\sim$\textbf{2h}). Moreover, HVC achieves state-of-the-art performance in several self-supervised VOS benchmarks and additional video label propagation tasks.</li>
</ul>

<h3>Title: Reliable Model Watermarking: Defending Against Theft without  Compromising on Evasion</h3>
<ul>
<li><strong>Authors: </strong>Hongyu Zhu, Sichu Liang, Wentao Hu, Fangqi Li, Ju Jia, Shilin Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13518">https://arxiv.org/abs/2404.13518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13518">https://arxiv.org/pdf/2404.13518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13518]] Reliable Model Watermarking: Defending Against Theft without  Compromising on Evasion(https://arxiv.org/abs/2404.13518)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the rise of Machine Learning as a Service (MLaaS) platforms,safeguarding the intellectual property of deep learning models is becoming paramount. Among various protective measures, trigger set watermarking has emerged as a flexible and effective strategy for preventing unauthorized model distribution. However, this paper identifies an inherent flaw in the current paradigm of trigger set watermarking: evasion adversaries can readily exploit the shortcuts created by models memorizing watermark samples that deviate from the main task distribution, significantly impairing their generalization in adversarial settings. To counteract this, we leverage diffusion models to synthesize unrestricted adversarial examples as trigger sets. By learning the model to accurately recognize them, unique watermark behaviors are promoted through knowledge injection rather than error memorization, thus avoiding exploitable shortcuts. Furthermore, we uncover that the resistance of current trigger set watermarking against removal attacks primarily relies on significantly damaging the decision boundaries during embedding, intertwining unremovability with adverse impacts. By optimizing the knowledge transfer properties of protected models, our approach conveys watermark behaviors to extraction surrogates without aggressively decision boundary perturbation. Experimental results on CIFAR-10/100 and Imagenette datasets demonstrate the effectiveness of our method, showing not only improved robustness against evasion adversaries but also superior resistance to watermark removal attacks compared to state-of-the-art solutions.</li>
</ul>

<h3>Title: SmartMem: Layout Transformation Elimination and Adaptation for Efficient  DNN Execution on Mobile</h3>
<ul>
<li><strong>Authors: </strong>Wei Niu, Md Musfiqur Rahman Sanim, Zhihao Shu, Jiexiong Guan, Xipeng Shen, Miao Yin, Gagan Agrawal, Bin Ren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13528">https://arxiv.org/abs/2404.13528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13528">https://arxiv.org/pdf/2404.13528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13528]] SmartMem: Layout Transformation Elimination and Adaptation for Efficient  DNN Execution on Mobile(https://arxiv.org/abs/2404.13528)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work is motivated by recent developments in Deep Neural Networks, particularly the Transformer architectures underlying applications such as ChatGPT, and the need for performing inference on mobile devices. Focusing on emerging transformers (specifically the ones with computationally efficient Swin-like architectures) and large models (e.g., Stable Diffusion and LLMs) based on transformers, we observe that layout transformations between the computational operators cause a significant slowdown in these applications. This paper presents SmartMem, a comprehensive framework for eliminating most layout transformations, with the idea that multiple operators can use the same tensor layout through careful choice of layout and implementation of operations. Our approach is based on classifying the operators into four groups, and considering combinations of producer-consumer edges between the operators. We develop a set of methods for searching such layouts. Another component of our work is developing efficient memory layouts for 2.5 dimensional memory commonly seen in mobile devices. Our experimental results show that SmartMem outperforms 5 state-of-the-art DNN execution frameworks on mobile devices across 18 varied neural networks, including CNNs, Transformers with both local and global attention, as well as LLMs. In particular, compared to DNNFusion, SmartMem achieves an average speedup of 2.8$\times$, and outperforms TVM and MNN with speedups of 6.9$\times$ and 7.9$\times$, respectively, on average.</li>
</ul>

<h3>Title: Motion-aware Latent Diffusion Models for Video Frame Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Zhilin Huang, Yijie Yu, Ling Yang, Chujun Qin, Bing Zheng, Xiawu Zheng, Zikun Zhou, Yaowei Wang, Wenming Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13534">https://arxiv.org/abs/2404.13534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13534">https://arxiv.org/pdf/2404.13534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13534]] Motion-aware Latent Diffusion Models for Video Frame Interpolation(https://arxiv.org/abs/2404.13534)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the advancement of AIGC, video frame interpolation (VFI) has become a crucial component in existing video generation frameworks, attracting widespread research interest. For the VFI task, the motion estimation between neighboring frames plays a crucial role in avoiding motion ambiguity. However, existing VFI methods always struggle to accurately predict the motion information between consecutive frames, and this imprecise estimation leads to blurred and visually incoherent interpolated frames. In this paper, we propose a novel diffusion framework, motion-aware latent diffusion models (MADiff), which is specifically designed for the VFI task. By incorporating motion priors between the conditional neighboring frames with the target interpolated frame predicted throughout the diffusion sampling procedure, MADiff progressively refines the intermediate outcomes, culminating in generating both visually smooth and realistic results. Extensive experiments conducted on benchmark datasets demonstrate that our method achieves state-of-the-art performance significantly outperforming existing approaches, especially under challenging scenarios involving dynamic textures with complex motion.</li>
</ul>

<h3>Title: Exploring Diverse Methods in Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Panfeng Li, Qikai Yang, Xieming Geng, Wenjing Zhou, Zhicheng Ding, Yi Nian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13565">https://arxiv.org/abs/2404.13565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13565">https://arxiv.org/pdf/2404.13565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13565]] Exploring Diverse Methods in Visual Question Answering(https://arxiv.org/abs/2404.13565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study explores innovative methods for improving Visual Question Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and attention mechanisms. Leveraging a balanced VQA dataset, we investigate three distinct strategies. Firstly, GAN-based approaches aim to generate answer embeddings conditioned on image and question inputs, showing potential but struggling with more complex tasks. Secondly, autoencoder-based techniques focus on learning optimal embeddings for questions and images, achieving comparable results with GAN due to better ability on complex questions. Lastly, attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB), address language priors and attention modeling, albeit with a complexity-performance trade-off. This study underscores the challenges and opportunities in VQA and suggests avenues for future research, including alternative GAN formulations and attentional mechanisms.</li>
</ul>

<h3>Title: Exploring AIGC Video Quality: A Focus on Visual Harmony, Video-Text  Consistency and Domain Distribution Gap</h3>
<ul>
<li><strong>Authors: </strong>Bowen Qu, Xiaoyu Liang, Shangkun Sun, Wei Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13573">https://arxiv.org/abs/2404.13573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13573">https://arxiv.org/pdf/2404.13573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13573]] Exploring AIGC Video Quality: A Focus on Visual Harmony, Video-Text  Consistency and Domain Distribution Gap(https://arxiv.org/abs/2404.13573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The recent advancements in Text-to-Video Artificial Intelligence Generated Content (AIGC) have been remarkable. Compared with traditional videos, the assessment of AIGC videos encounters various challenges: visual inconsistency that defy common sense, discrepancies between content and the textual prompt, and distribution gap between various generative models, etc. Target at these challenges, in this work, we categorize the assessment of AIGC video quality into three dimensions: visual harmony, video-text consistency, and domain distribution gap. For each dimension, we design specific modules to provide a comprehensive quality assessment of AIGC videos. Furthermore, our research identifies significant variations in visual quality, fluidity, and style among videos generated by different text-to-video models. Predicting the source generative model can make the AIGC video features more discriminative, which enhances the quality assessment performance. The proposed method was used in the third-place winner of the NTIRE 2024 Quality Assessment for AI-Generated Content - Track 2 Video, demonstrating its effectiveness.</li>
</ul>

<h3>Title: "A good pun is its own reword": Can Large Language Models Understand  Puns?</h3>
<ul>
<li><strong>Authors: </strong>Zhijun Xu, Siyu Yuan, Lingjie Chen, Deqing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13599">https://arxiv.org/abs/2404.13599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13599">https://arxiv.org/pdf/2404.13599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13599]] "A good pun is its own reword": Can Large Language Models Understand  Puns?(https://arxiv.org/abs/2404.13599)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Puns play a vital role in academic research due to their distinct structure and clear definition, which aid in the comprehensive analysis of linguistic humor. However, the understanding of puns in large language models (LLMs) has not been thoroughly examined, limiting their use in creative writing and humor creation. In this paper, we leverage three popular tasks, i.e., pun recognition, explanation and generation to systematically evaluate the capabilities of LLMs in pun understanding. In addition to adopting the automated evaluation metrics from prior research, we introduce new evaluation methods and metrics that are better suited to the in-context learning paradigm of LLMs. These new metrics offer a more rigorous assessment of an LLM's ability to understand puns and align more closely with human cognition than previous metrics. Our findings reveal the "lazy pun generation" pattern and identify the primary challenges LLMs encounter in understanding puns.</li>
</ul>

<h3>Title: Mixture of LoRA Experts</h3>
<ul>
<li><strong>Authors: </strong>Xun Wu, Shaohan Huang, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13628">https://arxiv.org/abs/2404.13628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13628">https://arxiv.org/pdf/2404.13628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13628]] Mixture of LoRA Experts(https://arxiv.org/abs/2404.13628)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision & Language (V&L) domains substantiate the efficacy of MoLE.</li>
</ul>

<h3>Title: Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming  Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Resmi Ramachandranpillai, Md Fahim Sikder, David Bergstr√∂m, Fredrik Heintz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13634">https://arxiv.org/abs/2404.13634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13634">https://arxiv.org/pdf/2404.13634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13634]] Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming  Generative Adversarial Networks(https://arxiv.org/abs/2404.13634)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic data generation offers a promising solution to enhance the usefulness of Electronic Healthcare Records (EHR) by generating realistic de-identified data. However, the existing literature primarily focuses on the quality of synthetic health data, neglecting the crucial aspect of fairness in downstream predictions. Consequently, models trained on synthetic EHR have faced criticism for producing biased outcomes in target tasks. These biases can arise from either spurious correlations between features or the failure of models to accurately represent sub-groups. To address these concerns, we present Bias-transforming Generative Adversarial Networks (Bt-GAN), a GAN-based synthetic data generator specifically designed for the healthcare domain. In order to tackle spurious correlations (i), we propose an information-constrained Data Generation Process that enables the generator to learn a fair deterministic transformation based on a well-defined notion of algorithmic fairness. To overcome the challenge of capturing exact sub-group representations (ii), we incentivize the generator to preserve sub-group densities through score-based weighted sampling. This approach compels the generator to learn from underrepresented regions of the data manifold. We conduct extensive experiments using the MIMIC-III database. Our results demonstrate that Bt-GAN achieves SOTA accuracy while significantly improving fairness and minimizing bias amplification. We also perform an in-depth explainability analysis to provide additional evidence supporting the validity of our study. In conclusion, our research introduces a novel and professional approach to addressing the limitations of synthetic data generation in the healthcare domain. By incorporating fairness considerations and leveraging advanced techniques such as GANs, we pave the way for more reliable and unbiased predictions in healthcare applications.</li>
</ul>

<h3>Title: FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and  High-Quality Localization</h3>
<ul>
<li><strong>Authors: </strong>Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Hao Li, Ming Tang, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13671">https://arxiv.org/abs/2404.13671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13671">https://arxiv.org/pdf/2404.13671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13671]] FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and  High-Quality Localization(https://arxiv.org/abs/2404.13671)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Zero-shot anomaly detection (ZSAD) methods entail detecting anomalies directly without access to any known normal or abnormal samples within the target item categories. Existing approaches typically rely on the robust generalization capabilities of multimodal pretrained models, computing similarities between manually crafted textual features representing "normal" or "abnormal" semantics and image features to detect anomalies and localize anomalous patches. However, the generic descriptions of "abnormal" often fail to precisely match diverse types of anomalies across different object categories. Additionally, computing feature similarities for single patches struggles to pinpoint specific locations of anomalies with various sizes and scales. To address these issues, we propose a novel ZSAD method called FiLo, comprising two components: adaptively learned Fine-Grained Description (FG-Des) and position-enhanced High-Quality Localization (HQ-Loc). FG-Des introduces fine-grained anomaly descriptions for each category using Large Language Models (LLMs) and employs adaptively learned textual templates to enhance the accuracy and interpretability of anomaly detection. HQ-Loc, utilizing Grounding DINO for preliminary localization, position-enhanced text prompts, and Multi-scale Multi-shape Cross-modal Interaction (MMCI) module, facilitates more accurate localization of anomalies of different sizes and shapes. Experimental results on datasets like MVTec and VisA demonstrate that FiLo significantly improves the performance of ZSAD in both detection and localization, achieving state-of-the-art performance with an image-level AUC of 83.9% and a pixel-level AUC of 95.9% on the VisA dataset.</li>
</ul>

<h3>Title: A Dataset and Model for Realistic License Plate Deblurring</h3>
<ul>
<li><strong>Authors: </strong>Haoyan Gong, Yuzheng Feng, Zhenrong Zhang, Xianxu Hou, Jingxin Liu, Siqi Huang, Hongbin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13677">https://arxiv.org/abs/2404.13677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13677">https://arxiv.org/pdf/2404.13677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13677]] A Dataset and Model for Realistic License Plate Deblurring(https://arxiv.org/abs/2404.13677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vehicle license plate recognition is a crucial task in intelligent traffic management systems. However, the challenge of achieving accurate recognition persists due to motion blur from fast-moving vehicles. Despite the widespread use of image synthesis approaches in existing deblurring and recognition algorithms, their effectiveness in real-world scenarios remains unproven. To address this, we introduce the first large-scale license plate deblurring dataset named License Plate Blur (LPBlur), captured by a dual-camera system and processed through a post-processing pipeline to avoid misalignment issues. Then, we propose a License Plate Deblurring Generative Adversarial Network (LPDGAN) to tackle the license plate deblurring: 1) a Feature Fusion Module to integrate multi-scale latent codes; 2) a Text Reconstruction Module to restore structure through textual modality; 3) a Partition Discriminator Module to enhance the model's perception of details in each letter. Extensive experiments validate the reliability of the LPBlur dataset for both model training and testing, showcasing that our proposed model outperforms other state-of-the-art motion deblurring methods in realistic license plate deblurring scenarios. The dataset and code are available at https://github.com/haoyGONG/LPDGAN.</li>
</ul>

<h3>Title: Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image  Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, Xuefeng Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13686">https://arxiv.org/abs/2404.13686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13686">https://arxiv.org/pdf/2404.13686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13686]] Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image  Synthesis(https://arxiv.org/abs/2404.13686)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, a series of diffusion-aware distillation algorithms have emerged to alleviate the computational overhead associated with the multi-step inference process of Diffusion Models (DMs). Current distillation techniques often dichotomize into two distinct aspects: i) ODE Trajectory Preservation; and ii) ODE Trajectory Reformulation. However, these approaches suffer from severe performance degradation or domain shifts. To address these limitations, we propose Hyper-SD, a novel framework that synergistically amalgamates the advantages of ODE Trajectory Preservation and Reformulation, while maintaining near-lossless performance during step compression. Firstly, we introduce Trajectory Segmented Consistency Distillation to progressively perform consistent distillation within pre-defined time-step segments, which facilitates the preservation of the original ODE trajectory from a higher-order perspective. Secondly, we incorporate human feedback learning to boost the performance of the model in a low-step regime and mitigate the performance loss incurred by the distillation process. Thirdly, we integrate score distillation to further improve the low-step generation capability of the model and offer the first attempt to leverage a unified LoRA to support the inference process at all steps. Extensive experiments and user studies demonstrate that Hyper-SD achieves SOTA performance from 1 to 8 inference steps for both SDXL and SD1.5. For example, Hyper-SDXL surpasses SDXL-Lightning by +0.68 in CLIP Score and +0.51 in Aes Score in the 1-step inference.</li>
</ul>

<h3>Title: Detecting Compromised IoT Devices Using Autoencoders with Sequential  Hypothesis Testing</h3>
<ul>
<li><strong>Authors: </strong>Md Mainuddin, Zhenhai Duan, Yingfei Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13690">https://arxiv.org/abs/2404.13690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13690">https://arxiv.org/pdf/2404.13690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13690]] Detecting Compromised IoT Devices Using Autoencoders with Sequential  Hypothesis Testing(https://arxiv.org/abs/2404.13690)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>IoT devices fundamentally lack built-in security mechanisms to protect themselves from security attacks. Existing works on improving IoT security mostly focus on detecting anomalous behaviors of IoT devices. However, these existing anomaly detection schemes may trigger an overwhelmingly large number of false alerts, rendering them unusable in detecting compromised IoT devices. In this paper we develop an effective and efficient framework, named CUMAD, to detect compromised IoT devices. Instead of directly relying on individual anomalous events, CUMAD aims to accumulate sufficient evidence in detecting compromised IoT devices, by integrating an autoencoder-based anomaly detection subsystem with a sequential probability ratio test (SPRT)-based sequential hypothesis testing subsystem. CUMAD can effectively reduce the number of false alerts in detecting compromised IoT devices, and moreover, it can detect compromised IoT devices quickly. Our evaluation studies based on the public-domain N-BaIoT dataset show that CUMAD can on average reduce the false positive rate from about 3.57% using only the autoencoder-based anomaly detection scheme to about 0.5%; in addition, CUMAD can detect compromised IoT devices quickly, with less than 5 observations on average.</li>
</ul>

<h3>Title: Concept Arithmetics for Circumventing Concept Inhibition in Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Vitali Petsiuk, Kate Saenko</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13706">https://arxiv.org/abs/2404.13706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13706">https://arxiv.org/pdf/2404.13706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13706]] Concept Arithmetics for Circumventing Concept Inhibition in Diffusion  Models(https://arxiv.org/abs/2404.13706)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Motivated by ethical and legal concerns, the scientific community is actively developing methods to limit the misuse of Text-to-Image diffusion models for reproducing copyrighted, violent, explicit, or personal information in the generated images. Simultaneously, researchers put these newly developed safety measures to the test by assuming the role of an adversary to find vulnerabilities and backdoors in them. We use compositional property of diffusion models, which allows to leverage multiple prompts in a single image generation. This property allows us to combine other concepts, that should not have been affected by the inhibition, to reconstruct the vector, responsible for target concept generation, even though the direct computation of this vector is no longer accessible. We provide theoretical and empirical evidence why the proposed attacks are possible and discuss the implications of these findings for safe model deployment. We argue that it is essential to consider all possible approaches to image generation with diffusion models that can be employed by an adversary. Our work opens up the discussion about the implications of concept arithmetics and compositional inference for safety mechanisms in diffusion models. Content Advisory: This paper contains discussions and model-generated content that may be considered offensive. Reader discretion is advised. Project page: https://cs-people.bu.edu/vpetsiuk/arc</li>
</ul>

<h3>Title: ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Zichen Tang, Hongyu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13711">https://arxiv.org/abs/2404.13711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13711">https://arxiv.org/pdf/2404.13711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13711]] ArtNeRF: A Stylized Neural Field for 3D-Aware Cartoonized Face Synthesis(https://arxiv.org/abs/2404.13711)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative visual models and neural radiance fields have greatly boosted 3D-aware image synthesis and stylization tasks. However, previous NeRF-based work is limited to single scene stylization, training a model to generate 3D-aware cartoon faces with arbitrary styles remains unsolved. We propose ArtNeRF, a novel face stylization framework derived from 3D-aware GAN to tackle this problem. In this framework, we utilize an expressive generator to synthesize stylized faces and a triple-branch discriminator module to improve the visual quality and style consistency of the generated faces. Specifically, a style encoder based on contrastive learning is leveraged to extract robust low-dimensional embeddings of style images, empowering the generator with the knowledge of various styles. To smooth the training process of cross-domain transfer learning, we propose an adaptive style blending module which helps inject style information and allows users to freely tune the level of stylization. We further introduce a neural rendering module to achieve efficient real-time rendering of images with higher resolutions. Extensive experiments demonstrate that ArtNeRF is versatile in generating high-quality 3D-aware cartoon faces with arbitrary styles.</li>
</ul>

<h3>Title: A Nasal Cytology Dataset for Object Detection and Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Mauro Camporeale, Giovanni Dimauro, Matteo Gelardi, Giorgia Iacobellis, Mattia Sebastiano Ladisa, Sergio Latrofa, Nunzia Lomonte</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13745">https://arxiv.org/abs/2404.13745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13745">https://arxiv.org/pdf/2404.13745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13745]] A Nasal Cytology Dataset for Object Detection and Deep Learning(https://arxiv.org/abs/2404.13745)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Nasal Cytology is a new and efficient clinical technique to diagnose rhinitis and allergies that is not much widespread due to the time-consuming nature of cell counting; that is why AI-aided counting could be a turning point for the diffusion of this technique. In this article we present the first dataset of rhino-cytological field images: the NCD (Nasal Cytology Dataset), aimed to train and deploy Object Detection models to support physicians and biologists during clinical practice. The real distribution of the cytotypes, populating the nasal mucosa has been replicated, sampling images from slides of clinical patients, and manually annotating each cell found on them. The correspondent object detection task presents non'trivial issues associated with the strong class imbalancement, involving the rarest cell types. This work contributes to some of open challenges by presenting a novel machine learning-based approach to aid the automated detection and classification of nasal mucosa cells: the DETR and YOLO models shown good performance in detecting cells and classifying them correctly, revealing great potential to accelerate the work of rhinology experts.</li>
</ul>

<h3>Title: Towards General Conceptual Model Editing via Adversarial Representation  Engineering</h3>
<ul>
<li><strong>Authors: </strong>Yihao Zhang, Zeming Wei, Jun Sun, Meng Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13752">https://arxiv.org/abs/2404.13752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13752">https://arxiv.org/pdf/2404.13752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13752]] Towards General Conceptual Model Editing via Adversarial Representation  Engineering(https://arxiv.org/abs/2404.13752)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent research has introduced Representation Engineering (RepE) as a promising approach for understanding complex inner workings of large-scale models like Large Language Models (LLMs). However, finding practical and efficient methods to apply these representations for general and flexible model editing remains an open problem. Inspired by the Generative Adversarial Network (GAN) framework, we introduce a novel approach called Adversarial Representation Engineering (ARE). This method leverages RepE by using a representation sensor to guide the editing of LLMs, offering a unified and interpretable framework for conceptual model editing without degrading baseline performance. Our experiments on multiple conceptual editing confirm ARE's effectiveness. Code and data are available at https://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.</li>
</ul>

<h3>Title: Object-Attribute Binding in Text-to-Image Generation: Evaluation and  Control</h3>
<ul>
<li><strong>Authors: </strong>Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Honig, Thomas Hofmann, Tinne Tuytelaars, Marie-Francine Moens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13766">https://arxiv.org/abs/2404.13766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13766">https://arxiv.org/pdf/2404.13766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13766]] Object-Attribute Binding in Text-to-Image Generation: Evaluation and  Control(https://arxiv.org/abs/2404.13766)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current diffusion models create photorealistic images given a text prompt as input but struggle to correctly bind attributes mentioned in the text to the right objects in the image. This is evidenced by our novel image-graph alignment model called EPViT (Edge Prediction Vision Transformer) for the evaluation of image-text alignment. To alleviate the above problem, we propose focused cross-attention (FCA) that controls the visual attention maps by syntactic constraints found in the input sentence. Additionally, the syntax structure of the prompt helps to disentangle the multimodal CLIP embeddings that are commonly used in T2I generation. The resulting DisCLIP embeddings and FCA are easily integrated in state-of-the-art diffusion models without additional training of these models. We show substantial improvements in T2I generation and especially its attribute-object binding on several datasets.\footnote{Code and data will be made available upon acceptance.</li>
</ul>

<h3>Title: Automated Text Mining of Experimental Methodologies from Biomedical  Literature</h3>
<ul>
<li><strong>Authors: </strong>Ziqing Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13779">https://arxiv.org/abs/2404.13779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13779">https://arxiv.org/pdf/2404.13779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13779]] Automated Text Mining of Experimental Methodologies from Biomedical  Literature(https://arxiv.org/abs/2404.13779)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Biomedical literature is a rapidly expanding field of science and technology. Classification of biomedical texts is an essential part of biomedicine research, especially in the field of biology. This work proposes the fine-tuned DistilBERT, a methodology-specific, pre-trained generative classification language model for mining biomedicine texts. The model has proven its effectiveness in linguistic understanding capabilities and has reduced the size of BERT models by 40\% but by 60\% faster. The main objective of this project is to improve the model and assess the performance of the model compared to the non-fine-tuned model. We used DistilBert as a support model and pre-trained on a corpus of 32,000 abstracts and complete text articles; our results were impressive and surpassed those of traditional literature classification methods by using RNN or LSTM. Our aim is to integrate this highly specialised and specific model into different research industries.</li>
</ul>

<h3>Title: AnyPattern: Towards In-context Image Copy Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Wang, Yifan Sun, Zhentao Tan, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13788">https://arxiv.org/abs/2404.13788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13788">https://arxiv.org/pdf/2404.13788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13788]] AnyPattern: Towards In-context Image Copy Detection(https://arxiv.org/abs/2404.13788)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This paper explores in-context learning for image copy detection (ICD), i.e., prompting an ICD model to identify replicated images with new tampering patterns without the need for additional training. The prompts (or the contexts) are from a small set of image-replica pairs that reflect the new patterns and are used at inference time. Such in-context ICD has good realistic value, because it requires no fine-tuning and thus facilitates fast reaction against the emergence of unseen patterns. To accommodate the "seen $\rightarrow$ unseen" generalization scenario, we construct the first large-scale pattern dataset named AnyPattern, which has the largest number of tamper patterns ($90$ for training and $10$ for testing) among all the existing ones. We benchmark AnyPattern with popular ICD methods and reveal that existing methods barely generalize to novel tamper patterns. We further propose a simple in-context ICD method named ImageStacker. ImageStacker learns to select the most representative image-replica pairs and employs them as the pattern prompts in a stacking manner (rather than the popular concatenation manner). Experimental results show (1) training with our large-scale dataset substantially benefits pattern generalization ($+26.66 \%$ $\mu AP$), (2) the proposed ImageStacker facilitates effective in-context ICD (another round of $+16.75 \%$ $\mu AP$), and (3) AnyPattern enables in-context ICD, i.e. without such a large-scale dataset, in-context learning does not emerge even with our ImageStacker. The project (including the proposed dataset AnyPattern and the code for ImageStacker) is publicly available at https://anypattern.github.io under the MIT Licence.</li>
</ul>

<h3>Title: Universal Fingerprint Generation: Controllable Diffusion Model with  Multimodal Conditions</h3>
<ul>
<li><strong>Authors: </strong>Steven A. Grosz, Anil K. Jain</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13791">https://arxiv.org/abs/2404.13791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13791">https://arxiv.org/pdf/2404.13791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13791]] Universal Fingerprint Generation: Controllable Diffusion Model with  Multimodal Conditions(https://arxiv.org/abs/2404.13791)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The utilization of synthetic data for fingerprint recognition has garnered increased attention due to its potential to alleviate privacy concerns surrounding sensitive biometric data. However, current methods for generating fingerprints have limitations in creating impressions of the same finger with useful intra-class variations. To tackle this challenge, we present GenPrint, a framework to produce fingerprint images of various types while maintaining identity and offering humanly understandable control over different appearance factors such as fingerprint class, acquisition type, sensor device, and quality level. Unlike previous fingerprint generation approaches, GenPrint is not confined to replicating style characteristics from the training dataset alone: it enables the generation of novel styles from unseen devices without requiring additional fine-tuning. To accomplish these objectives, we developed GenPrint using latent diffusion models with multimodal conditions (text and image) for consistent generation of style and identity. Our experiments leverage a variety of publicly available datasets for training and evaluation. Results demonstrate the benefits of GenPrint in terms of identity preservation, explainable control, and universality of generated images. Importantly, the GenPrint-generated images yield comparable or even superior accuracy to models trained solely on real data and further enhances performance when augmenting the diversity of existing real fingerprint datasets.</li>
</ul>

<h3>Title: Enforcing Conditional Independence for Fair Representation Learning and  Causal Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jensen Hwa, Qingyu Zhao, Aditya Lahiri, Adnan Masood, Babak Salimi, Ehsan Adeli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13798">https://arxiv.org/abs/2404.13798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13798">https://arxiv.org/pdf/2404.13798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13798]] Enforcing Conditional Independence for Fair Representation Learning and  Causal Image Generation(https://arxiv.org/abs/2404.13798)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Conditional independence (CI) constraints are critical for defining and evaluating fairness in machine learning, as well as for learning unconfounded or causal representations. Traditional methods for ensuring fairness either blindly learn invariant features with respect to a protected variable (e.g., race when classifying sex from face images) or enforce CI relative to the protected attribute only on the model output (e.g., the sex label). Neither of these methods are effective in enforcing CI in high-dimensional feature spaces. In this paper, we focus on a nascent approach characterizing the CI constraint in terms of two Jensen-Shannon divergence terms, and we extend it to high-dimensional feature spaces using a novel dynamic sampling strategy. In doing so, we introduce a new training paradigm that can be applied to any encoder architecture. We are able to enforce conditional independence of the diffusion autoencoder latent representation with respect to any protected attribute under the equalized odds constraint and show that this approach enables causal image generation with controllable latent spaces. Our experimental results demonstrate that our approach can achieve high accuracy on downstream tasks while upholding equality of odds.</li>
</ul>

<h3>Title: ICST-DNET: An Interpretable Causal Spatio-Temporal Diffusion Network for  Traffic Speed Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yi Rong, Yingchi Mao, Yinqiu Liu, Ling Chen, Xiaoming He, Dusit Niyato</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13853">https://arxiv.org/abs/2404.13853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13853">https://arxiv.org/pdf/2404.13853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13853]] ICST-DNET: An Interpretable Causal Spatio-Temporal Diffusion Network for  Traffic Speed Prediction(https://arxiv.org/abs/2404.13853)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Traffic speed prediction is significant for intelligent navigation and congestion alleviation. However, making accurate predictions is challenging due to three factors: 1) traffic diffusion, i.e., the spatial and temporal causality existing between the traffic conditions of multiple neighboring roads, 2) the poor interpretability of traffic data with complicated spatio-temporal correlations, and 3) the latent pattern of traffic speed fluctuations over time, such as morning and evening rush. Jointly considering these factors, in this paper, we present a novel architecture for traffic speed prediction, called Interpretable Causal Spatio-Temporal Diffusion Network (ICST-DNET). Specifically, ICST-DENT consists of three parts, namely the Spatio-Temporal Causality Learning (STCL), Causal Graph Generation (CGG), and Speed Fluctuation Pattern Recognition (SFPR) modules. First, to model the traffic diffusion within road networks, an STCL module is proposed to capture both the temporal causality on each individual road and the spatial causality in each road pair. The CGG module is then developed based on STCL to enhance the interpretability of the traffic diffusion procedure from the temporal and spatial perspectives. Specifically, a time causality matrix is generated to explain the temporal causality between each road's historical and future traffic conditions. For spatial causality, we utilize causal graphs to visualize the diffusion process in road pairs. Finally, to adapt to traffic speed fluctuations in different scenarios, we design a personalized SFPR module to select the historical timesteps with strong influences for learning the pattern of traffic speed fluctuations. Extensive experimental results prove that ICST-DNET can outperform all existing baselines, as evidenced by the higher prediction accuracy, ability to explain causality, and adaptability to different scenarios.</li>
</ul>

<h3>Title: Self-Supervised Monocular Depth Estimation in the Dark: Towards Data  Distribution Compensation</h3>
<ul>
<li><strong>Authors: </strong>Haolin Yang, Chaoqiang Zhao, Lu Sheng, Yang Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13854">https://arxiv.org/abs/2404.13854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13854">https://arxiv.org/pdf/2404.13854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13854]] Self-Supervised Monocular Depth Estimation in the Dark: Towards Data  Distribution Compensation(https://arxiv.org/abs/2404.13854)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Nighttime self-supervised monocular depth estimation has received increasing attention in recent years. However, using night images for self-supervision is unreliable because the photometric consistency assumption is usually violated in the videos taken under complex lighting conditions. Even with domain adaptation or photometric loss repair, performance is still limited by the poor supervision of night images on trainable networks. In this paper, we propose a self-supervised nighttime monocular depth estimation method that does not use any night images during training. Our framework utilizes day images as a stable source for self-supervision and applies physical priors (e.g., wave optics, reflection model and read-shot noise model) to compensate for some key day-night differences. With day-to-night data distribution compensation, our framework can be trained in an efficient one-stage self-supervised manner. Though no nighttime images are considered during training, qualitative and quantitative results demonstrate that our method achieves SoTA depth estimating results on the challenging nuScenes-Night and RobotCar-Night compared with existing methods.</li>
</ul>

<h3>Title: Distributional Black-Box Model Inversion Attack with Multi-Agent  Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Huan Bao, Kaimin Wei, Yongdong Wu, Jin Qian, Robert H. Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13860">https://arxiv.org/abs/2404.13860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13860">https://arxiv.org/pdf/2404.13860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13860]] Distributional Black-Box Model Inversion Attack with Multi-Agent  Reinforcement Learning(https://arxiv.org/abs/2404.13860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A Model Inversion (MI) attack based on Generative Adversarial Networks (GAN) aims to recover the private training data from complex deep learning models by searching codes in the latent space. However, they merely search a deterministic latent space such that the found latent code is usually suboptimal. In addition, the existing distributional MI schemes assume that an attacker can access the structures and parameters of the target model, which is not always viable in practice. To overcome the above shortcomings, this paper proposes a novel Distributional Black-Box Model Inversion (DBB-MI) attack by constructing the probabilistic latent space for searching the target privacy data. Specifically, DBB-MI does not need the target model parameters or specialized GAN training. Instead, it finds the latent probability distribution by combining the output of the target model with multi-agent reinforcement learning techniques. Then, it randomly chooses latent codes from the latent probability distribution for recovering the private data. As the latent probability distribution closely aligns with the target privacy data in latent space, the recovered data will leak the privacy of training samples of the target model significantly. Abundant experiments conducted on diverse datasets and networks show that the present DBB-MI has better performance than state-of-the-art in attack accuracy, K-nearest neighbor feature distance, and Peak Signal-to-Noise Ratio.</li>
</ul>

<h3>Title: Towards Better Text-to-Image Generation Alignment via Attention  Modulation</h3>
<ul>
<li><strong>Authors: </strong>Yihang Wu, Xiao Cao, Kaixin Li, Zitan Chen, Haonan Wang, Lei Meng, Zhiyong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13899">https://arxiv.org/abs/2404.13899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13899">https://arxiv.org/pdf/2404.13899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13899]] Towards Better Text-to-Image Generation Alignment via Attention  Modulation(https://arxiv.org/abs/2404.13899)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In text-to-image generation tasks, the advancements of diffusion models have facilitated the fidelity of generated results. However, these models encounter challenges when processing text prompts containing multiple entities and attributes. The uneven distribution of attention results in the issues of entity leakage and attribute misalignment. Training from scratch to address this issue requires numerous labeled data and is resource-consuming. Motivated by this, we propose an attribution-focusing mechanism, a training-free phase-wise mechanism by modulation of attention for diffusion model. One of our core ideas is to guide the model to concentrate on the corresponding syntactic components of the prompt at distinct timesteps. To achieve this, we incorporate a temperature control mechanism within the early phases of the self-attention modules to mitigate entity leakage issues. An object-focused masking scheme and a phase-wise dynamic weight control mechanism are integrated into the cross-attention modules, enabling the model to discern the affiliation of semantic information between entities more effectively. The experimental results in various alignment scenarios demonstrate that our model attain better image-text alignment with minimal additional computational cost.</li>
</ul>

<h3>Title: Accelerating Image Generation with Sub-path Linear Approximation Model</h3>
<ul>
<li><strong>Authors: </strong>Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo Zheng, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13903">https://arxiv.org/abs/2404.13903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13903">https://arxiv.org/pdf/2404.13903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13903]] Accelerating Image Generation with Sub-path Linear Approximation Model(https://arxiv.org/abs/2404.13903)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have significantly advanced the state of the art in image, audio, and video generation tasks. However, their applications in practical scenarios are hindered by slow inference speed. Drawing inspiration from the approximation strategies utilized in consistency models, we propose the Sub-path Linear Approximation Model (SLAM), which accelerates diffusion models while maintaining high-quality image generation. SLAM treats the PF-ODE trajectory as a series of PF-ODE sub-paths divided by sampled points, and harnesses sub-path linear (SL) ODEs to form a progressive and continuous error estimation along each individual PF-ODE sub-path. The optimization on such SL-ODEs allows SLAM to construct denoising mappings with smaller cumulative approximated errors. An efficient distillation method is also developed to facilitate the incorporation of more advanced diffusion models, such as latent diffusion models. Our extensive experimental results demonstrate that SLAM achieves an efficient training regimen, requiring only 6 A100 GPU days to produce a high-quality generative model capable of 2 to 4-step generation with high performance. Comprehensive evaluations on LAION, MS COCO 2014, and MS COCO 2017 datasets also illustrate that SLAM surpasses existing acceleration methods in few-step generation tasks, achieving state-of-the-art performance both on FID and the quality of the generated images.</li>
</ul>

<h3>Title: NeRF-DetS: Enhancing Multi-View 3D Object Detection with  Sampling-adaptive Network of Continuous NeRF-based Representation</h3>
<ul>
<li><strong>Authors: </strong>Chi Huang, Xinyang Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13921">https://arxiv.org/abs/2404.13921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13921">https://arxiv.org/pdf/2404.13921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13921]] NeRF-DetS: Enhancing Multi-View 3D Object Detection with  Sampling-adaptive Network of Continuous NeRF-based Representation(https://arxiv.org/abs/2404.13921)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>As a preliminary work, NeRF-Det unifies the tasks of novel view synthesis and 3D perception, demonstrating that perceptual tasks can benefit from novel view synthesis methods like NeRF, significantly improving the performance of indoor multi-view 3D object detection. Using the geometry MLP of NeRF to direct the attention of detection head to crucial parts and incorporating self-supervised loss from novel view rendering contribute to the achieved improvement. To better leverage the notable advantages of the continuous representation through neural rendering in space, we introduce a novel 3D perception network structure, NeRF-DetS. The key component of NeRF-DetS is the Multi-level Sampling-Adaptive Network, making the sampling process adaptively from coarse to fine. Also, we propose a superior multi-view information fusion method, known as Multi-head Weighted Fusion. This fusion approach efficiently addresses the challenge of losing multi-view information when using arithmetic mean, while keeping low computational costs. NeRF-DetS outperforms competitive NeRF-Det on the ScanNetV2 dataset, by achieving +5.02% and +5.92% improvement in mAP@.25 and mAP@.50, respectively.</li>
</ul>

<h3>Title: MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Li, Ruitong Gan, Chuanchen Luo, Yuxi Wang, Jiaheng Liu, Ziwei Zhu Man Zhang, Qing Li, Xucheng Yin, Zhaoxiang Zhang, Junran Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13923">https://arxiv.org/abs/2404.13923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13923">https://arxiv.org/pdf/2404.13923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13923]] MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets(https://arxiv.org/abs/2404.13923)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Driven by powerful image diffusion models, recent research has achieved the automatic creation of 3D objects from textual or visual guidance. By performing score distillation sampling (SDS) iteratively across different views, these methods succeed in lifting 2D generative prior to the 3D space. However, such a 2D generative image prior bakes the effect of illumination and shadow into the texture. As a result, material maps optimized by SDS inevitably involve spurious correlated components. The absence of precise material definition makes it infeasible to relight the generated assets reasonably in novel scenes, which limits their application in downstream scenarios. In contrast, humans can effortlessly circumvent this ambiguity by deducing the material of the object from its appearance and semantics. Motivated by this insight, we propose MaterialSeg3D, a 3D asset material generation framework to infer underlying material from the 2D semantic prior. Based on such a prior model, we devise a mechanism to parse material in 3D space. We maintain a UV stack, each map of which is unprojected from a specific viewpoint. After traversing all viewpoints, we fuse the stack through a weighted voting scheme and then employ region unification to ensure the coherence of the object parts. To fuel the learning of semantics prior, we collect a material dataset, named Materialized Individual Objects (MIO), which features abundant images, diverse categories, and accurate annotations. Extensive quantitative and qualitative experiments demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: Gorgeous: Create Your Desired Character Facial Makeup from Any Ideas</h3>
<ul>
<li><strong>Authors: </strong>Jia Wei Sii, Chee Seng Chan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13944">https://arxiv.org/abs/2404.13944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13944">https://arxiv.org/pdf/2404.13944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13944]] Gorgeous: Create Your Desired Character Facial Makeup from Any Ideas(https://arxiv.org/abs/2404.13944)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Contemporary makeup transfer methods primarily focus on replicating makeup from one face to another, considerably limiting their use in creating diverse and creative character makeup essential for visual storytelling. Such methods typically fail to address the need for uniqueness and contextual relevance, specifically aligning with character and story settings as they depend heavily on existing facial makeup in reference images. This approach also presents a significant challenge when attempting to source a perfectly matched facial makeup style, further complicating the creation of makeup designs inspired by various story elements, such as theme, background, and props that do not necessarily feature faces. To address these limitations, we introduce $Gorgeous$, a novel diffusion-based makeup application method that goes beyond simple transfer by innovatively crafting unique and thematic facial makeup. Unlike traditional methods, $Gorgeous$ does not require the presence of a face in the reference images. Instead, it draws artistic inspiration from a minimal set of three to five images, which can be of any type, and transforms these elements into practical makeup applications directly on the face. Our comprehensive experiments demonstrate that $Gorgeous$ can effectively generate distinctive character facial makeup inspired by the chosen thematic reference images. This approach opens up new possibilities for integrating broader story elements into character makeup, thereby enhancing the narrative depth and visual impact in storytelling.</li>
</ul>

<h3>Title: An Economic Solution to Copyright Challenges of Generative AI</h3>
<ul>
<li><strong>Authors: </strong>iachen T. Wang, Zhun Deng, Hiroaki Chiba-Okabe, Boaz Barak, Weijie J. Su</a></li>
<li><strong>Subjects: </strong>cs.LG, econ.GN, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13964">https://arxiv.org/abs/2404.13964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13964">https://arxiv.org/pdf/2404.13964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13964]] An Economic Solution to Copyright Challenges of Generative AI(https://arxiv.org/abs/2404.13964)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (AI) systems are trained on large data corpora to generate new pieces of text, images, videos, and other media. There is growing concern that such systems may infringe on the copyright interests of training data contributors. To address the copyright challenges of generative AI, we propose a framework that compensates copyright owners proportionally to their contributions to the creation of AI-generated content. The metric for contributions is quantitatively determined by leveraging the probabilistic nature of modern generative AI models and using techniques from cooperative game theory in economics. This framework enables a platform where AI developers benefit from access to high-quality training data, thus improving model performance. Meanwhile, copyright owners receive fair compensation, driving the continued provision of relevant data for generative model training. Experiments demonstrate that our framework successfully identifies the most relevant data sources used in artwork generation, ensuring a fair and interpretable distribution of revenues among copyright owners.</li>
</ul>

<h3>Title: Non-Uniform Exposure Imaging via Neuromorphic Shutter Control</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Lin, Jian Liu, Chi Zhang, Zibo Zhao, Chu He, Lei Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13972">https://arxiv.org/abs/2404.13972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13972">https://arxiv.org/pdf/2404.13972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13972]] Non-Uniform Exposure Imaging via Neuromorphic Shutter Control(https://arxiv.org/abs/2404.13972)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>By leveraging the blur-noise trade-off, imaging with non-uniform exposures largely extends the image acquisition flexibility in harsh environments. However, the limitation of conventional cameras in perceiving intra-frame dynamic information prevents existing methods from being implemented in the real-world frame acquisition for real-time adaptive camera shutter control. To address this challenge, we propose a novel Neuromorphic Shutter Control (NSC) system to avoid motion blurs and alleviate instant noises, where the extremely low latency of events is leveraged to monitor the real-time motion and facilitate the scene-adaptive exposure. Furthermore, to stabilize the inconsistent Signal-to-Noise Ratio (SNR) caused by the non-uniform exposure times, we propose an event-based image denoising network within a self-supervised learning paradigm, i.e., SEID, exploring the statistics of image noises and inter-frame motion information of events to obtain artificial supervision signals for high-quality imaging in real-world scenes. To illustrate the effectiveness of the proposed NSC, we implement it in hardware by building a hybrid-camera imaging prototype system, with which we collect a real-world dataset containing well-synchronized frames and events in diverse scenarios with different target scenes and motion patterns. Experiments on the synthetic and real-world datasets demonstrate the superiority of our method over state-of-the-art approaches.</li>
</ul>

<h3>Title: RHanDS: Refining Malformed Hands for Generated Images with Decoupled  Structure and Style Guidance</h3>
<ul>
<li><strong>Authors: </strong>Chengrui Wang, Pengfei Liu, Min Zhou, Ming Zeng, Xubin Li, Tiezheng Ge, Bo zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.13984">https://arxiv.org/abs/2404.13984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.13984">https://arxiv.org/pdf/2404.13984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.13984]] RHanDS: Refining Malformed Hands for Generated Images with Decoupled  Structure and Style Guidance(https://arxiv.org/abs/2404.13984)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although diffusion models can generate high-quality human images, their applications are limited by the instability in generating hands with correct structures. Some previous works mitigate the problem by considering hand structure yet struggle to maintain style consistency between refined malformed hands and other image regions. In this paper, we aim to solve the problem of inconsistency regarding hand structure and style. We propose a conditional diffusion-based framework RHanDS to refine the hand region with the help of decoupled structure and style guidance. Specifically, the structure guidance is the hand mesh reconstructed from the malformed hand, serving to correct the hand structure. The style guidance is a hand image, e.g., the malformed hand itself, and is employed to furnish the style reference for hand refining. In order to suppress the structure leakage when referencing hand style and effectively utilize hand data to improve the capability of the model, we build a multi-style hand dataset and introduce a twostage training strategy. In the first stage, we use paired hand images for training to generate hands with the same style as the reference. In the second stage, various hand images generated based on the human mesh are used for training to enable the model to gain control over the hand structure. We evaluate our method and counterparts on the test dataset of the proposed multi-style hand dataset. The experimental results show that RHanDS can effectively refine hands structure- and style- correctly compared with previous methods. The codes and datasets will be available soon.</li>
</ul>

<h3>Title: Infusion: Preventing Customized Text-to-Image Diffusion from Overfitting</h3>
<ul>
<li><strong>Authors: </strong>Weili Zeng, Yichao Yan, Qi Zhu, Zhuo Chen, Pengzhi Chu, Weiming Zhao, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14007">https://arxiv.org/abs/2404.14007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14007">https://arxiv.org/pdf/2404.14007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14007]] Infusion: Preventing Customized Text-to-Image Diffusion from Overfitting(https://arxiv.org/abs/2404.14007)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) customization aims to create images that embody specific visual concepts delineated in textual descriptions. However, existing works still face a main challenge, concept overfitting. To tackle this challenge, we first analyze overfitting, categorizing it into concept-agnostic overfitting, which undermines non-customized concept knowledge, and concept-specific overfitting, which is confined to customize on limited modalities, i.e, backgrounds, layouts, styles. To evaluate the overfitting degree, we further introduce two metrics, i.e, Latent Fisher divergence and Wasserstein metric to measure the distribution changes of non-customized and customized concept respectively. Drawing from the analysis, we propose Infusion, a T2I customization method that enables the learning of target concepts to avoid being constrained by limited training modalities, while preserving non-customized knowledge. Remarkably, Infusion achieves this feat with remarkable efficiency, requiring a mere 11KB of trained parameters. Extensive experiments also demonstrate that our approach outperforms state-of-the-art methods in both single and multi-concept customized generation.</li>
</ul>

<h3>Title: OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining  BEV Segmentation Networks</h3>
<ul>
<li><strong>Authors: </strong>Sophia Sirko-Galouchenko, Alexandre Boulch, Spyros Gidaris, Andrei Bursuc, Antonin Vobecky, Patrick P√©rez, Renaud Marlet</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14027">https://arxiv.org/abs/2404.14027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14027">https://arxiv.org/pdf/2404.14027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14027]] OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining  BEV Segmentation Networks(https://arxiv.org/abs/2404.14027)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>We introduce a self-supervised pretraining method, called OcFeat, for camera-only Bird's-Eye-View (BEV) segmentation networks. With OccFeat, we pretrain a BEV network via occupancy prediction and feature distillation tasks. Occupancy prediction provides a 3D geometric understanding of the scene to the model. However, the geometry learned is class-agnostic. Hence, we add semantic information to the model in the 3D space through distillation from a self-supervised pretrained image foundation model. Models pretrained with our method exhibit improved BEV semantic segmentation performance, particularly in low-data scenarios. Moreover, empirical results affirm the efficacy of integrating feature distillation with 3D occupancy prediction in our pretraining approach.</li>
</ul>

<h3>Title: PointDifformer: Robust Point Cloud Registration With Neural Diffusion  and Transformer</h3>
<ul>
<li><strong>Authors: </strong>Rui She, Qiyu Kang, Sijie Wang, Wee Peng Tay, Kai Zhao, Yang Song, Tianyu Geng, Yi Xu, Diego Navarro Navarro, Andreas Hartmannsgruber</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14034">https://arxiv.org/abs/2404.14034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14034">https://arxiv.org/pdf/2404.14034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14034]] PointDifformer: Robust Point Cloud Registration With Neural Diffusion  and Transformer(https://arxiv.org/abs/2404.14034)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Point cloud registration is a fundamental technique in 3-D computer vision with applications in graphics, autonomous driving, and robotics. However, registration tasks under challenging conditions, under which noise or perturbations are prevalent, can be difficult. We propose a robust point cloud registration approach that leverages graph neural partial differential equations (PDEs) and heat kernel signatures. Our method first uses graph neural PDE modules to extract high dimensional features from point clouds by aggregating information from the 3-D point neighborhood, thereby enhancing the robustness of the feature representations. Then, we incorporate heat kernel signatures into an attention mechanism to efficiently obtain corresponding keypoints. Finally, a singular value decomposition (SVD) module with learnable weights is used to predict the transformation between two point clouds. Empirical experiments on a 3-D point cloud dataset demonstrate that our approach not only achieves state-of-the-art performance for point cloud registration but also exhibits better robustness to additive noise or 3-D shape perturbations.</li>
</ul>

<h3>Title: RingID: Rethinking Tree-Ring Watermarking for Enhanced Multi-Key  Identification</h3>
<ul>
<li><strong>Authors: </strong>Hai Ci, Pei Yang, Yiren Song, Mike Zheng Shou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14055">https://arxiv.org/abs/2404.14055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14055">https://arxiv.org/pdf/2404.14055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14055]] RingID: Rethinking Tree-Ring Watermarking for Enhanced Multi-Key  Identification(https://arxiv.org/abs/2404.14055)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We revisit Tree-Ring Watermarking, a recent diffusion model watermarking method that demonstrates great robustness to various attacks. We conduct an in-depth study on it and reveal that the distribution shift unintentionally introduced by the watermarking process, apart from watermark pattern matching, contributes to its exceptional robustness. Our investigation further exposes inherent flaws in its original design, particularly in its ability to identify multiple distinct keys, where distribution shift offers no assistance. Based on these findings and analysis, we present RingID for enhanced multi-key identification. It consists of a novel multi-channel heterogeneous watermarking approach designed to seamlessly amalgamate distinctive advantages from diverse watermarks. Coupled with a series of suggested enhancements, RingID exhibits substantial advancements in multi-key identification.</li>
</ul>

<h3>Title: Multi-view Disentanglement for Reinforcement Learning with Multiple  Cameras</h3>
<ul>
<li><strong>Authors: </strong>Mhairi Dunion, Stefano V. Albrecht</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14064">https://arxiv.org/abs/2404.14064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14064">https://arxiv.org/pdf/2404.14064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14064]] Multi-view Disentanglement for Reinforcement Learning with Multiple  Cameras(https://arxiv.org/abs/2404.14064)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The performance of image-based Reinforcement Learning (RL) agents can vary depending on the position of the camera used to capture the images. Training on multiple cameras simultaneously, including a first-person egocentric camera, can leverage information from different camera perspectives to improve the performance of RL. However, hardware constraints may limit the availability of multiple cameras in real-world deployment. Additionally, cameras may become damaged in the real-world preventing access to all cameras that were used during training. To overcome these hardware constraints, we propose Multi-View Disentanglement (MVD), which uses multiple cameras to learn a policy that achieves zero-shot generalisation to any single camera from the training set. Our approach is a self-supervised auxiliary task for RL that learns a disentangled representation from multiple cameras, with a shared representation that is aligned across all cameras to allow generalisation to a single camera, and a private representation that is camera-specific. We show experimentally that an RL agent trained on a single third-person camera is unable to learn an optimal policy in many control tasks; but, our approach, benefiting from multiple cameras during training, is able to solve the task using only the same single third-person camera.</li>
</ul>

<h3>Title: Multidimensional Interpolants</h3>
<ul>
<li><strong>Authors: </strong>Dohoon Lee, Kyogu Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14161">https://arxiv.org/abs/2404.14161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14161">https://arxiv.org/pdf/2404.14161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14161]] Multidimensional Interpolants(https://arxiv.org/abs/2404.14161)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the domain of differential equation-based generative modeling, conventional approaches often rely on single-dimensional scalar values as interpolation coefficients during both training and inference phases. In this work, we introduce, for the first time, a multidimensional interpolant that extends these coefficients into multiple dimensions, leveraging the stochastic interpolant framework. Additionally, we propose a novel path optimization problem tailored to adaptively determine multidimensional inference trajectories, with a predetermined differential equation solver and a fixed number of function evaluations. Our solution involves simulation dynamics coupled with adversarial training to optimize the inference path. Notably, employing a multidimensional interpolant during training improves the model's inference performance, even in the absence of path optimization. When the adaptive, multidimensional path derived from our optimization process is employed, it yields further performance gains, even with fixed solver configurations. The introduction of multidimensional interpolants not only enhances the efficacy of models but also opens up a new domain for exploration in training and inference methodologies, emphasizing the potential of multidimensional paths as an untapped frontier.</li>
</ul>

<h3>Title: FLDM-VTON: Faithful Latent Diffusion Model for Virtual Try-on</h3>
<ul>
<li><strong>Authors: </strong>Chenhui Wang, Tao Chen, Zhihao Chen, Zhizhong Huang, Taoran Jiang, Qi Wang, Hongming Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14162">https://arxiv.org/abs/2404.14162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14162">https://arxiv.org/pdf/2404.14162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14162]] FLDM-VTON: Faithful Latent Diffusion Model for Virtual Try-on(https://arxiv.org/abs/2404.14162)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite their impressive generative performance, latent diffusion model-based virtual try-on (VTON) methods lack faithfulness to crucial details of the clothes, such as style, pattern, and text. To alleviate these issues caused by the diffusion stochastic nature and latent supervision, we propose a novel Faithful Latent Diffusion Model for VTON, termed FLDM-VTON. FLDM-VTON improves the conventional latent diffusion process in three major aspects. First, we propose incorporating warped clothes as both the starting point and local condition, supplying the model with faithful clothes priors. Second, we introduce a novel clothes flattening network to constrain generated try-on images, providing clothes-consistent faithful supervision. Third, we devise a clothes-posterior sampling for faithful inference, further enhancing the model performance over conventional clothes-agnostic Gaussian sampling. Extensive experimental results on the benchmark VITON-HD and Dress Code datasets demonstrate that our FLDM-VTON outperforms state-of-the-art baselines and is able to generate photo-realistic try-on images with faithful clothing details.</li>
</ul>

<h3>Title: Face2Face: Label-driven Facial Retouching Restoration</h3>
<ul>
<li><strong>Authors: </strong>Guanhua Zhao, Yu Gu, Xuhan Sheng, Yujie Hu, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14177">https://arxiv.org/abs/2404.14177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14177">https://arxiv.org/pdf/2404.14177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14177]] Face2Face: Label-driven Facial Retouching Restoration(https://arxiv.org/abs/2404.14177)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the popularity of social media platforms such as Instagram and TikTok, and the widespread availability and convenience of retouching tools, an increasing number of individuals are utilizing these tools to beautify their facial photographs. This poses challenges for fields that place high demands on the authenticity of photographs, such as identity verification and social media. By altering facial images, users can easily create deceptive images, leading to the dissemination of false information. This may pose challenges to the reliability of identity verification systems and social media, and even lead to online fraud. To address this issue, some work has proposed makeup removal methods, but they still lack the ability to restore images involving geometric deformations caused by retouching. To tackle the problem of facial retouching restoration, we propose a framework, dubbed Face2Face, which consists of three components: a facial retouching detector, an image restoration model named FaceR, and a color correction module called Hierarchical Adaptive Instance Normalization (H-AdaIN). Firstly, the facial retouching detector predicts a retouching label containing three integers, indicating the retouching methods and their corresponding degrees. Then FaceR restores the retouched image based on the predicted retouching label. Finally, H-AdaIN is applied to address the issue of color shift arising from diffusion models. Extensive experiments demonstrate the effectiveness of our framework and each module.</li>
</ul>

<h3>Title: MultiBooth: Towards Generating All Your Concepts in an Image from Text</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Li Xiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14239">https://arxiv.org/abs/2404.14239</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14239">https://arxiv.org/pdf/2404.14239</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14239]] MultiBooth: Towards Generating All Your Concepts in an Image from Text(https://arxiv.org/abs/2404.14239)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces MultiBooth, a novel and efficient technique for multi-concept customization in image generation from text. Despite the significant advancements in customized generation methods, particularly with the success of diffusion models, existing methods often struggle with multi-concept scenarios due to low concept fidelity and high inference cost. MultiBooth addresses these issues by dividing the multi-concept generation process into two phases: a single-concept learning phase and a multi-concept integration phase. During the single-concept learning phase, we employ a multi-modal image encoder and an efficient concept encoding technique to learn a concise and discriminative representation for each concept. In the multi-concept integration phase, we use bounding boxes to define the generation area for each concept within the cross-attention map. This method enables the creation of individual concepts within their specified regions, thereby facilitating the formation of multi-concept images. This strategy not only improves concept fidelity but also reduces additional inference cost. MultiBooth surpasses various baselines in both qualitative and quantitative evaluations, showcasing its superior performance and computational efficiency. Project Page: https://multibooth.github.io/</li>
</ul>

<h3>Title: AI-Generated Faces in the Real World: A Large-Scale Case Study of  Twitter Profile Images</h3>
<ul>
<li><strong>Authors: </strong>Jonas Ricker, Dennis Assenmacher, Thorsten Holz, Asja Fischer, Erwin Quiring</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14244">https://arxiv.org/abs/2404.14244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14244">https://arxiv.org/pdf/2404.14244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14244]] AI-Generated Faces in the Real World: A Large-Scale Case Study of  Twitter Profile Images(https://arxiv.org/abs/2404.14244)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media. One notable consequence is the use of AI-generated images for fake profiles on social media. While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking. In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter. We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline. Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform. We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior. The results also reveal several motives, including spamming and political amplification campaigns. Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future.</li>
</ul>

<h3>Title: Towards Better Adversarial Purification via Adversarial Denoising  Diffusion Training</h3>
<ul>
<li><strong>Authors: </strong>Yiming Liu, Kezhao Liu, Yao Xiao, Ziyi Dong, Xiaogang Xu, Pengxu Wei, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14309">https://arxiv.org/abs/2404.14309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14309">https://arxiv.org/pdf/2404.14309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14309]] Towards Better Adversarial Purification via Adversarial Denoising  Diffusion Training(https://arxiv.org/abs/2404.14309)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, diffusion-based purification (DBP) has emerged as a promising approach for defending against adversarial attacks. However, previous studies have used questionable methods to evaluate the robustness of DBP models, their explanations of DBP robustness also lack experimental support. We re-examine DBP robustness using precise gradient, and discuss the impact of stochasticity on DBP robustness. To better explain DBP robustness, we assess DBP robustness under a novel attack setting, Deterministic White-box, and pinpoint stochasticity as the main factor in DBP robustness. Our results suggest that DBP models rely on stochasticity to evade the most effective attack direction, rather than directly countering adversarial perturbations. To improve the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT). This technique uses Classifier-Guided Perturbation Optimization (CGPO) to generate adversarial perturbation through guidance from a pre-trained classifier, and uses Rank-Based Gaussian Mapping (RBGM) to convert adversarial pertubation into a normal Gaussian distribution. Empirical results show that ADDT improves the robustness of DBP models. Further experiments confirm that ADDT equips DBP models with the ability to directly counter adversarial perturbations.</li>
</ul>

<h3>Title: Self-Supervised Alignment with Mutual Information: Learning to Follow  Principles without Preference Labels</h3>
<ul>
<li><strong>Authors: </strong>Jan-Philipp Fr√§nken, Eric Zelikman, Rafael Rafailov, Kanishk Gandhi, Tobias Gerstenberg, Noah D. Goodman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14313">https://arxiv.org/abs/2404.14313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14313">https://arxiv.org/pdf/2404.14313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14313]] Self-Supervised Alignment with Mutual Information: Learning to Follow  Principles without Preference Labels(https://arxiv.org/abs/2404.14313)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>When prompting a language model (LM), users frequently expect the model to adhere to a set of behavioral principles across diverse tasks, such as producing insightful content while avoiding harmful or biased language. Instilling such principles into a model can be resource-intensive and technically challenging, generally requiring human preference labels or examples. We introduce SAMI, a method for teaching a pretrained LM to follow behavioral principles that does not require any preference labels or demonstrations. SAMI is an iterative algorithm that finetunes a pretrained LM to increase the conditional mutual information between constitutions and self-generated responses given queries from a datasest. On single-turn dialogue and summarization, a SAMI-trained mistral-7b outperforms the initial pretrained model, with win rates between 66% and 77%. Strikingly, it also surpasses an instruction-finetuned baseline (mistral-7b-instruct) with win rates between 55% and 57% on single-turn dialogue. SAMI requires a "principle writer" model; to avoid dependence on stronger models, we further evaluate aligning a strong pretrained model (mixtral-8x7b) using constitutions written by a weak instruction-finetuned model (mistral-7b-instruct). The SAMI-trained mixtral-8x7b outperforms both the initial model and the instruction-finetuned model, achieving a 65% win rate on summarization. Our results indicate that a pretrained LM can learn to follow constitutions without using preference labels, demonstrations, or human oversight.</li>
</ul>

<h3>Title: X-Ray: A Sequential 3D Representation for Generation</h3>
<ul>
<li><strong>Authors: </strong>Tao Hu, Wenhang Ge, Yuyang Zhao, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14329">https://arxiv.org/abs/2404.14329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14329">https://arxiv.org/pdf/2404.14329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14329]] X-Ray: A Sequential 3D Representation for Generation(https://arxiv.org/abs/2404.14329)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce X-Ray, an innovative approach to 3D generation that employs a new sequential representation, drawing inspiration from the depth-revealing capabilities of X-Ray scans to meticulously capture both the external and internal features of objects. Central to our method is the utilization of ray casting techniques originating from the camera's viewpoint, meticulously recording the geometric and textural details encountered across all intersected surfaces. This process efficiently condenses complete objects or scenes into a multi-frame format, just like videos. Such a structure ensures the 3D representation is composed solely of critical surface information. Highlighting the practicality and adaptability of our X-Ray representation, we showcase its utility in synthesizing 3D objects, employing a network architecture akin to that used in video diffusion models. The outcomes reveal our representation's superior performance in enhancing both the accuracy and efficiency of 3D synthesis, heralding new directions for ongoing research and practical implementations in the field.</li>
</ul>

<h3>Title: Calc-CMU at SemEval-2024 Task 7: Pre-Calc -- Learning to Use the  Calculator Improves Numeracy in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vishruth Veerendranath, Vishwa Shah, Kshitish Ghate</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14355">https://arxiv.org/abs/2404.14355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14355">https://arxiv.org/pdf/2404.14355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14355]] Calc-CMU at SemEval-2024 Task 7: Pre-Calc -- Learning to Use the  Calculator Improves Numeracy in Language Models(https://arxiv.org/abs/2404.14355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantitative and numerical comprehension in language is an important task in many fields like education and finance, but still remains a challenging task for language models. While tool and calculator usage has shown to be helpful to improve mathematical reasoning in large pretrained decoder-only language models, this remains unexplored for smaller language models with encoders. In this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning to use the calculator for both encoder-only and encoder-decoder architectures, formulated as a discriminative and generative task respectively. We pre-train BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves performance on downstream tasks that require numerical understanding. Our code and data are available at https://github.com/calc-cmu/pre-calc.</li>
</ul>

<h3>Title: TAVGBench: Benchmarking Text to Audible-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Mao, Xuyang Shen, Jing Zhang, Zhen Qin, Jinxing Zhou, Mochu Xiang, Yiran Zhong, Yuchao Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14381">https://arxiv.org/abs/2404.14381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14381">https://arxiv.org/pdf/2404.14381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14381]] TAVGBench: Benchmarking Text to Audible-Video Generation(https://arxiv.org/abs/2404.14381)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The Text to Audible-Video Generation (TAVG) task involves generating videos with accompanying audio based on text descriptions. Achieving this requires skillful alignment of both audio and video elements. To support research in this field, we have developed a comprehensive Text to Audible-Video Generation Benchmark (TAVGBench), which contains over 1.7 million clips with a total duration of 11.8 thousand hours. We propose an automatic annotation pipeline to ensure each audible video has detailed descriptions for both its audio and video contents. We also introduce the Audio-Visual Harmoni score (AVHScore) to provide a quantitative measure of the alignment between the generated audio and video modalities. Additionally, we present a baseline model for TAVG called TAVDiffusion, which uses a two-stream latent diffusion model to provide a fundamental starting point for further research in this area. We achieve the alignment of audio and video by employing cross-attention and contrastive learning. Through extensive experiments and evaluations on TAVGBench, we demonstrate the effectiveness of our proposed model under both conventional metrics and our proposed metrics.</li>
</ul>

<h3>Title: SEED-X: Multimodal Models with Unified Multi-granularity Comprehension  and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14396">https://arxiv.org/abs/2404.14396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14396">https://arxiv.org/pdf/2404.14396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14396]] SEED-X: Multimodal Models with Unified Multi-granularity Comprehension  and Generation(https://arxiv.org/abs/2404.14396)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of multimodal foundation model has demonstrated significant progresses in vision-language understanding and generation, e.g., our previous work SEED-LLaMA. However, there remains a gap between its capability and the real-world applicability, primarily due to the model's limited capacity to effectively respond to various user instructions and interact with diverse visual data. In this work, we focus on bridging this gap through integrating two enhanced features: (1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-granularity image generation. We present a unified and versatile foundation model, namely, SEED-X, which is able to model multi-granularity visual semantics for comprehension and generation tasks. Besides the competitive results on public benchmarks, SEED-X demonstrates its effectiveness in handling real-world applications across various domains after instruction tuning. We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications. The models, codes, and datasets will be released in https://github.com/AILab-CVC/SEED-X.</li>
</ul>

<h3>Title: GeoDiffuser: Geometry-Based Image Editing with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, Srinath Sridhar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14403">https://arxiv.org/abs/2404.14403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14403">https://arxiv.org/pdf/2404.14403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14403]] GeoDiffuser: Geometry-Based Image Editing with Diffusion Models(https://arxiv.org/abs/2404.14403)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The success of image generative models has enabled us to build methods that can edit images based on text or other user input. However, these methods are bespoke, imprecise, require additional information, or are limited to only 2D image edits. We present GeoDiffuser, a zero-shot optimization-based method that unifies common 2D and 3D image-based object editing capabilities into a single method. Our key insight is to view image editing operations as geometric transformations. We show that these transformations can be directly incorporated into the attention layers in diffusion models to implicitly perform editing operations. Our training-free optimization method uses an objective function that seeks to preserve object style but generate plausible images, for instance with accurate lighting and shadows. It also inpaints disoccluded parts of the image where the object was originally located. Given a natural image and user input, we segment the foreground object using SAM and estimate a corresponding transform which is used by our optimization approach for editing. GeoDiffuser can perform common 2D and 3D edits like object translation, 3D rotation, and removal. We present quantitative results, including a perceptual study, that shows how our approach is better than existing methods. Visit https://ivl.cs.brown.edu/research/geodiffuser.html for more information.</li>
</ul>

<h3>Title: Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D  Glimpses</h3>
<ul>
<li><strong>Authors: </strong>Inhee Lee, Byungjun Kim, Hanbyul Joo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.14410">https://arxiv.org/abs/2404.14410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.14410">https://arxiv.org/pdf/2404.14410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.14410]] Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D  Glimpses(https://arxiv.org/abs/2404.14410)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present a method to reconstruct the world and multiple dynamic humans in 3D from a monocular video input. As a key idea, we represent both the world and multiple humans via the recently emerging 3D Gaussian Splatting (3D-GS) representation, enabling to conveniently and efficiently compose and render them together. In particular, we address the scenarios with severely limited and sparse observations in 3D human reconstruction, a common challenge encountered in the real world. To tackle this challenge, we introduce a novel approach to optimize the 3D-GS representation in a canonical space by fusing the sparse cues in the common space, where we leverage a pre-trained 2D diffusion model to synthesize unseen views while keeping the consistency with the observed 2D appearances. We demonstrate our method can reconstruct high-quality animatable 3D humans in various challenging examples, in the presence of occlusion, image crops, few-shot, and extremely sparse observations. After reconstruction, our method is capable of not only rendering the scene in any novel views at arbitrary time instances, but also editing the 3D scene by removing individual humans or applying different motions for each human. Through various experiments, we demonstrate the quality and efficiency of our methods over alternative existing approaches.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
