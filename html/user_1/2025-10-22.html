<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-22</h1>
<h3>Title: From Noise to Laws: Regularized Time-Series Forecasting via Denoised Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Hongwei Ma, Junbin Gao, Minh-ngoc Tran</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17817">https://arxiv.org/abs/2510.17817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17817">https://arxiv.org/pdf/2510.17817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17817]] From Noise to Laws: Regularized Time-Series Forecasting via Denoised Dynamic Graphs(https://arxiv.org/abs/2510.17817)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Long-horizon multivariate time-series forecasting is challenging because realistic predictions must (i) denoise heterogeneous signals, (ii) track time-varying cross-series dependencies, and (iii) remain stable and physically plausible over long rollout horizons. We present PRISM, which couples a score-based diffusion preconditioner with a dynamic, correlation-thresholded graph encoder and a forecast head regularized by generic physics penalties. We prove contraction of the induced horizon dynamics under mild conditions and derive Lipschitz bounds for graph blocks, explaining the model's robustness. On six standard benchmarks , PRISM achieves consistent SOTA with strong MSE and MAE gains.</li>
</ul>

<h3>Title: Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Alexandre G. Leclercq, Sébastien Bougleux, Noémie N. Moreau, Alexis Desmonts, Romain Hérault, Aurélien Corroyer-Dulmont</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17851">https://arxiv.org/abs/2510.17851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17851">https://arxiv.org/pdf/2510.17851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17851]] Pre to Post-Treatment Glioblastoma MRI Prediction using a Latent Diffusion Model(https://arxiv.org/abs/2510.17851)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Glioblastoma (GBM) is an aggressive primary brain tumor with a median survival of approximately 15 months. In clinical practice, the Stupp protocol serves as the standard first-line treatment. However, patients exhibit highly heterogeneous therapeutic responses which required at least two months before first visual impact can be observed, typically with MRI. Early prediction treatment response is crucial for advancing personalized medicine. Disease Progression Modeling (DPM) aims to capture the trajectory of disease evolution, while Treatment Response Prediction (TRP) focuses on assessing the impact of therapeutic interventions. Whereas most TRP approaches primarly rely on timeseries data, we consider the problem of early visual TRP as a slice-to-slice translation model generating post-treatment MRI from a pre-treatment MRI, thus reflecting the tumor evolution. To address this problem we propose a Latent Diffusion Model with a concatenation-based conditioning from the pre-treatment MRI and the tumor localization, and a classifier-free guidance to enhance generation quality using survival information, in particular post-treatment tumor evolution. Our model were trained and tested on a local dataset consisting of 140 GBM patients collected at Centre François Baclesse. For each patient we collected pre and post T1-Gd MRI, tumor localization manually delineated in the pre-treatment MRI by medical experts, and survival information.</li>
</ul>

<h3>Title: Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Jitendra Sharma, Arthur Carvalho, Suman Bhunia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17854">https://arxiv.org/abs/2510.17854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17854">https://arxiv.org/pdf/2510.17854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17854]] Provenance of AI-Generated Images: A Vector Similarity and Blockchain-based Approach(https://arxiv.org/abs/2510.17854)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Rapid advancement in generative AI and large language models (LLMs) has enabled the generation of highly realistic and contextually relevant digital content. LLMs such as ChatGPT with DALL-E integration and Stable Diffusion techniques can produce images that are often indistinguishable from those created by humans, which poses challenges for digital content authentication. Verifying the integrity and origin of digital data to ensure it remains unaltered and genuine is crucial to maintaining trust and legality in digital media. In this paper, we propose an embedding-based AI image detection framework that utilizes image embeddings and a vector similarity to distinguish AI-generated images from real (human-created) ones. Our methodology is built on the hypothesis that AI-generated images demonstrate closer embedding proximity to other AI-generated content, while human-created images cluster similarly within their domain. To validate this hypothesis, we developed a system that processes a diverse dataset of AI and human-generated images through five benchmark embedding models. Extensive experimentation demonstrates the robustness of our approach, and our results confirm that moderate to high perturbations minimally impact the embedding signatures, with perturbed images maintaining close similarity matches to their original versions. Our solution provides a generalizable framework for AI-generated image detection that balances accuracy with computational efficiency.</li>
</ul>

<h3>Title: Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch</h3>
<ul>
<li><strong>Authors: </strong>Xu Cai, Yang Wu, Qianli Chen, Haoran Wu, Lichuan Xiang, Hongkai Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17858">https://arxiv.org/abs/2510.17858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17858">https://arxiv.org/pdf/2510.17858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17858]] Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch(https://arxiv.org/abs/2510.17858)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present an ultra-efficient post-training method for shortcutting large-scale pre-trained flow matching diffusion models into efficient few-step samplers, enabled by novel velocity field self-distillation. While shortcutting in flow matching, originally introduced by shortcut models, offers flexible trajectory-skipping capabilities, it requires a specialized step-size embedding incompatible with existing models unless retraining from scratch$\unicode{x2013}$a process nearly as costly as pretraining itself. Our key contribution is thus imparting a more aggressive shortcut mechanism to standard flow matching models (e.g., Flux), leveraging a unique distillation principle that obviates the need for step-size embedding. Working on the velocity field rather than sample space and learning rapidly from self-guided distillation in an online manner, our approach trains efficiently, e.g., producing a 3-step Flux less than one A100 day. Beyond distillation, our method can be incorporated into the pretraining stage itself, yielding models that inherently learn efficient, few-step flows without compromising quality. This capability also enables, to our knowledge, the first few-shot distillation method (e.g., 10 text-image pairs) for dozen-billion-parameter diffusion models, delivering state-of-the-art performance at almost free cost.</li>
</ul>

<h3>Title: GAN-based Content-Conditioned Generation of Handwritten Musical Symbols</h3>
<ul>
<li><strong>Authors: </strong>Gerard Asbert, Pau Torras, Lei Kang, Alicia Fornés, Josep Lladós</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17869">https://arxiv.org/abs/2510.17869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17869">https://arxiv.org/pdf/2510.17869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17869]] GAN-based Content-Conditioned Generation of Handwritten Musical Symbols(https://arxiv.org/abs/2510.17869)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The field of Optical Music Recognition (OMR) is currently hindered by the scarcity of real annotated data, particularly when dealing with handwritten historical musical scores. In similar fields, such as Handwritten Text Recognition, it was proven that synthetic examples produced with image generation techniques could help to train better-performing recognition architectures. This study explores the generation of realistic, handwritten-looking scores by implementing a music symbol-level Generative Adversarial Network (GAN) and assembling its output into a full score using the Smashcima engraving software. We have systematically evaluated the visual fidelity of these generated samples, concluding that the generated symbols exhibit a high degree of realism, marking significant progress in synthetic score generation.</li>
</ul>

<h3>Title: Outraged AI: Large language models prioritise emotion over cost in fairness enforcement</h3>
<ul>
<li><strong>Authors: </strong>Hao Liu, Yiqing Dai, Haotian Tan, Yu Lei, Yujia Zhou, Zhen Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17880">https://arxiv.org/abs/2510.17880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17880">https://arxiv.org/pdf/2510.17880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17880]] Outraged AI: Large language models prioritise emotion over cost in fairness enforcement(https://arxiv.org/abs/2510.17880)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.</li>
</ul>

<h3>Title: POPI: Personalizing LLMs via Optimized Natural Language Preference Inference</h3>
<ul>
<li><strong>Authors: </strong>Yizhuo Chen, Xin Liu, Ruijie Wang, Zheng Li, Pei Chen, Changlong Yu, Priyanka Nigam, Meng Jiang, Bing Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17881">https://arxiv.org/abs/2510.17881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17881">https://arxiv.org/pdf/2510.17881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17881]] POPI: Personalizing LLMs via Optimized Natural Language Preference Inference(https://arxiv.org/abs/2510.17881)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve strong benchmark performance, yet user experiences remain inconsistent due to diverse preferences in style, tone, and reasoning mode. Nevertheless, existing alignment techniques such as reinforcement learning from human feedback (RLHF) or Direct Preference Optimization (DPO) largely optimize toward population-level averages and overlook individual variation. Naive personalization strategies like per-user fine-tuning are computationally prohibitive, and in-context approaches that prepend raw user signals often suffer from inefficiency and noise. To address these challenges, we propose POPI, a general framework that introduces a preference inference model to distill heterogeneous user signals into concise natural language summaries. These summaries act as transparent, compact, and transferable personalization representations that condition a shared generation model to produce personalized responses. POPI jointly optimizes both preference inference and personalized generation under a unified objective using reinforcement learning, ensuring summaries maximally encode useful preference information. Extensive experiments across four personalization benchmarks demonstrate that POPI consistently improves personalization accuracy while reducing context overhead by a large margin. Moreover, optimized summaries seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play personalization without weight updates.</li>
</ul>

<h3>Title: When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Abdul Rehman, Syed Imad Ali Shah, Abbas Anwar, Noor Islam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17884">https://arxiv.org/abs/2510.17884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17884">https://arxiv.org/pdf/2510.17884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17884]] When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking(https://arxiv.org/abs/2510.17884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The remarkable capabilities of Large Language Models (LLMs) in natural language understanding and generation have sparked interest in their potential for cybersecurity applications, including password guessing. In this study, we conduct an empirical investigation into the efficacy of pre-trained LLMs for password cracking using synthetic user profiles. Specifically, we evaluate the performance of state-of-the-art open-source LLMs such as TinyLLaMA, Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords based on structured user attributes (e.g., name, birthdate, hobbies). Our results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext and SHA-256 hash comparisons, reveal consistently poor performance, with all models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional rule-based and combinator-based cracking methods demonstrate significantly higher success rates. Through detailed analysis and visualization, we identify key limitations in the generative reasoning of LLMs when applied to the domain-specific task of password guessing. Our findings suggest that, despite their linguistic prowess, current LLMs lack the domain adaptation and memorization capabilities required for effective password inference, especially in the absence of supervised fine-tuning on leaked password datasets. This study provides critical insights into the limitations of LLMs in adversarial contexts and lays the groundwork for future efforts in secure, privacy-preserving, and robust password modeling.</li>
</ul>

<h3>Title: NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation</h3>
<ul>
<li><strong>Authors: </strong>Rikard Vinge, Isabelle Wittmann, Jannik Schneider, Michael Marszalek, Luis Gilch, Thomas Brunschwiler, Conrad M Albrecht</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17914">https://arxiv.org/abs/2510.17914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17914">https://arxiv.org/pdf/2510.17914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17914]] NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation(https://arxiv.org/abs/2510.17914)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy) neural compression and representation learning in the context of Earth Observation (EO). Our approach builds on fixed-size embeddings that act as compact, task-agnostic representations applicable to a broad range of downstream tasks. NeuCo-Bench comprises three core components: (i) an evaluation pipeline built around reusable embeddings, (ii) a new challenge mode with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii) a scoring system that balances accuracy and stability. To support reproducibility, we release SSL4EO-S12-downstream, a curated multispectral, multitemporal EO dataset. We present initial results from a public challenge at the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art foundation models. NeuCo-Bench provides a first step towards community-driven, standardized evaluation of neural embeddings for EO and beyond.</li>
</ul>

<h3>Title: Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection</h3>
<ul>
<li><strong>Authors: </strong>Jinseong Park, Mijung Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17917">https://arxiv.org/abs/2510.17917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17917">https://arxiv.org/pdf/2510.17917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17917]] Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection(https://arxiv.org/abs/2510.17917)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Data unlearning aims to remove the influence of specific training samples from a trained model without requiring full retraining. Unlike concept unlearning, data unlearning in diffusion models remains underexplored and often suffers from quality degradation or incomplete forgetting. To address this, we first observe that most existing methods attempt to unlearn the samples at all diffusion time steps equally, leading to poor-quality generation. We argue that forgetting occurs disproportionately across time and frequency, depending on the model and scenarios. By selectively focusing on specific time-frequency ranges during training, we achieve samples with higher aesthetic quality and lower noise. We validate this improvement by applying our time-frequency selective approach to diverse settings, including gradient-based and preference optimization objectives, as well as both image-level and text-to-image tasks. Finally, to evaluate both deletion and quality of unlearned data samples, we propose a simple normalized version of SSCD. Together, our analysis and methods establish a clearer understanding of the unique challenges in data unlearning for diffusion models, providing practical strategies to improve both evaluation and unlearning performance.</li>
</ul>

<h3>Title: UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts</h3>
<ul>
<li><strong>Authors: </strong>Fu-Yun Wang, Han Zhang, Michael Gharbi, Hongsheng Li, Taesung Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17937">https://arxiv.org/abs/2510.17937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17937">https://arxiv.org/pdf/2510.17937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17937]] UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts(https://arxiv.org/abs/2510.17937)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present UniRL-Zero, a unified reinforcement learning (RL) framework that boosts, multimodal language model understanding and reasoning, diffusion model multimedia generation, and their beneficial interaction capabilities within a unified model. Our work defines six scenarios for unified model reinforcement learning, providing systematic baselines for reinforcement learning of unified understanding and generation model. Our code is available at this https URL.</li>
</ul>

<h3>Title: Demystifying Transition Matching: When and Why It Can Beat Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Jaihoon Kim, Rajarshi Saha, Minhyuk Sung, Youngsuk Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.17991">https://arxiv.org/abs/2510.17991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.17991">https://arxiv.org/pdf/2510.17991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.17991]] Demystifying Transition Matching: When and Why It Can Beat Flow Matching(https://arxiv.org/abs/2510.17991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow Matching (FM) underpins many state-of-the-art generative models, yet recent results indicate that Transition Matching (TM) can achieve higher quality with fewer sampling steps. This work answers the question of when and why TM outperforms FM. First, when the target is a unimodal Gaussian distribution, we prove that TM attains strictly lower KL divergence than FM for finite number of steps. The improvement arises from stochastic difference latent updates in TM, which preserve target covariance that deterministic FM underestimates. We then characterize convergence rates, showing that TM achieves faster convergence than FM under a fixed compute budget, establishing its advantage in the unimodal Gaussian setting. Second, we extend the analysis to Gaussian mixtures and identify local-unimodality regimes in which the sampling dynamics approximate the unimodal case, where TM can outperform FM. The approximation error decreases as the minimal distance between component means increases, highlighting that TM is favored when the modes are well separated. However, when the target variance approaches zero, each TM update converges to the FM update, and the performance advantage of TM diminishes. In summary, we show that TM outperforms FM when the target distribution has well-separated modes and non-negligible variances. We validate our theoretical results with controlled experiments on Gaussian distributions, and extend the comparison to real-world applications in image and video generation.</li>
</ul>

<h3>Title: SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection</h3>
<ul>
<li><strong>Authors: </strong>Roberto Brusnicki, David Pop, Yuan Gao, Mattia Piccinini, Johannes Betz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18034">https://arxiv.org/abs/2510.18034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18034">https://arxiv.org/pdf/2510.18034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18034]] SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection(https://arxiv.org/abs/2510.18034)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Autonomous driving systems remain critically vulnerable to the long-tail of rare, out-of-distribution scenarios with semantic anomalies. While Vision Language Models (VLMs) offer promising reasoning capabilities, naive prompting approaches yield unreliable performance and depend on expensive proprietary models, limiting practical deployment. We introduce SAVANT (Semantic Analysis with Vision-Augmented Anomaly deTection), a structured reasoning framework that achieves high accuracy and recall in detecting anomalous driving scenarios from input images through layered scene analysis and a two-phase pipeline: structured scene description extraction followed by multi-modal evaluation. Our approach transforms VLM reasoning from ad-hoc prompting to systematic analysis across four semantic layers: Street, Infrastructure, Movable Objects, and Environment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world driving scenarios, significantly outperforming unstructured baselines. More importantly, we demonstrate that our structured framework enables a fine-tuned 7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8% accuracy - surpassing all models evaluated while enabling local deployment at near-zero cost. By automatically labeling over 9,640 real-world images with high accuracy, SAVANT addresses the critical data scarcity problem in anomaly detection and provides a practical path toward reliable, accessible semantic monitoring for autonomous systems.</li>
</ul>

<h3>Title: Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Lu, Anna J. Li, Alexander E. Ladd, Pascha Matveev, Aditya Deole, Eric Shea-Brown, J. Nathan Kutz, Nicholas A. Steinmetz</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18037">https://arxiv.org/abs/2510.18037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18037">https://arxiv.org/pdf/2510.18037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18037]] Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity(https://arxiv.org/abs/2510.18037)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Neural activity forecasting is central to understanding neural systems and enabling closed-loop control. While deep learning has recently advanced the state-of-the-art in the time series forecasting literature, its application to neural activity forecasting remains limited. To bridge this gap, we systematically evaluated eight probabilistic deep learning models, including two foundation models, that have demonstrated strong performance on general forecasting benchmarks. We compared them against four classical statistical models and two baseline methods on spontaneous neural activity recorded from mouse cortex via widefield imaging. Across prediction horizons, several deep learning models consistently outperformed classical approaches, with the best model producing informative forecasts up to 1.5 seconds into the future. Our findings point toward future control applications and open new avenues for probing the intrinsic temporal structure of neural activity.</li>
</ul>

<h3>Title: Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Fan, Tong Wei, Chaoran Cheng, Yuxin Chen, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18053">https://arxiv.org/abs/2510.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18053">https://arxiv.org/pdf/2510.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18053]] Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models(https://arxiv.org/abs/2510.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Balancing exploration and exploitation during reinforcement learning fine-tuning of generative models presents a critical challenge, as existing approaches rely on fixed divergence regularization that creates an inherent dilemma: strong regularization preserves model capabilities but limits reward optimization, while weak regularization enables greater alignment but risks instability or reward hacking. We introduce Adaptive Divergence Regularized Policy Optimization (ADRPO), which automatically adjusts regularization strength based on advantage estimates-reducing regularization for high-value samples while applying stronger regularization to poor samples, enabling policies to navigate between exploration and aggressive exploitation according to data quality. Our implementation with Wasserstein-2 regularization for flow matching generative models achieves remarkable results on text-to-image generation, achieving better semantic alignment and diversity than offline methods like DPO and online methods with fixed regularization like ORW-CFM-W2. ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B and 12B parameters in attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining generation diversity. ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and multi-modal reasoning models, enhancing existing online RL methods like GRPO. In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local optima through active exploration, while in multi-modal audio reasoning, it outperforms GRPO through superior step-by-step reasoning, enabling a 7B model to outperform substantially larger commercial models including Gemini 2.5 Pro and GPT-4o Audio, offering an effective plug-and-play solution to the exploration-exploitation challenge across diverse generative architectures and modalities.</li>
</ul>

<h3>Title: HouseTour: A Virtual Real Estate A(I)gent</h3>
<ul>
<li><strong>Authors: </strong>Ata Çelen, Marc Pollefeys, Daniel Barath, Iro Armeni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18054">https://arxiv.org/abs/2510.18054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18054">https://arxiv.org/pdf/2510.18054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18054]] HouseTour: A Virtual Real Estate A(I)gent(https://arxiv.org/abs/2510.18054)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce HouseTour, a method for spatially-aware 3D camera trajectory and natural language summary generation from a collection of images depicting an existing 3D space. Unlike existing vision-language models (VLMs), which struggle with geometric reasoning, our approach generates smooth video trajectories via a diffusion process constrained by known camera poses and integrates this information into the VLM for 3D-grounded descriptions. We synthesize the final video using 3D Gaussian splatting to render novel views along the trajectory. To support this task, we present the HouseTour dataset, which includes over 1,200 house-tour videos with camera poses, 3D reconstructions, and real estate descriptions. Experiments demonstrate that incorporating 3D camera trajectories into the text generation process improves performance over methods handling each task independently. We evaluate both individual and end-to-end performance, introducing a new joint metric. Our work enables automated, professional-quality video creation for real estate and touristic applications without requiring specialized expertise or equipment.</li>
</ul>

<h3>Title: SPACeR: Self-Play Anchoring with Centralized Reference Models</h3>
<ul>
<li><strong>Authors: </strong>Wei-Jer Chang, Akshay Rangesh, Kevin Joseph, Matthew Strong, Masayoshi Tomizuka, Yihan Hu, Wei Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18060">https://arxiv.org/abs/2510.18060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18060">https://arxiv.org/pdf/2510.18060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18060]] SPACeR: Self-Play Anchoring with Centralized Reference Models(https://arxiv.org/abs/2510.18060)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Developing autonomous vehicles (AVs) requires not only safety and efficiency, but also realistic, human-like behaviors that are socially aware and predictable. Achieving this requires sim agent policies that are human-like, fast, and scalable in multi-agent settings. Recent progress in imitation learning with large diffusion-based or tokenized models has shown that behaviors can be captured directly from human driving data, producing realistic policies. However, these models are computationally expensive, slow during inference, and struggle to adapt in reactive, closed-loop scenarios. In contrast, self-play reinforcement learning (RL) scales efficiently and naturally captures multi-agent interactions, but it often relies on heuristics and reward shaping, and the resulting policies can diverge from human norms. We propose SPACeR, a framework that leverages a pretrained tokenized autoregressive motion model as a centralized reference policy to guide decentralized self-play. The reference model provides likelihood rewards and KL divergence, anchoring policies to the human driving distribution while preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our method achieves competitive performance with imitation-learned policies while being up to 10x faster at inference and 50x smaller in parameter size than large generative models. In addition, we demonstrate in closed-loop ego planning evaluation tasks that our sim agents can effectively measure planner quality with fast and scalable traffic simulation, establishing a new paradigm for testing autonomous driving policies.</li>
</ul>

<h3>Title: Fine-tuning Flow Matching Generative Models with Intermediate Feedback</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Fan, Chaoran Cheng, Shuaike Shen, Xiangxin Zhou, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18072">https://arxiv.org/abs/2510.18072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18072">https://arxiv.org/pdf/2510.18072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18072]] Fine-tuning Flow Matching Generative Models with Intermediate Feedback(https://arxiv.org/abs/2510.18072)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow-based generative models have shown remarkable success in text-to-image generation, yet fine-tuning them with intermediate feedback remains challenging, especially for continuous-time flow matching models. Most existing approaches solely learn from outcome rewards, struggling with the credit assignment problem. Alternative methods that attempt to learn a critic via direct regression on cumulative rewards often face training instabilities and model collapse in online settings. We present AC-Flow, a robust actor-critic framework that addresses these challenges through three key innovations: (1) reward shaping that provides well-normalized learning signals to enable stable intermediate value learning and gradient control, (2) a novel dual-stability mechanism that combines advantage clipping to prevent destructive policy updates with a warm-up phase that allows the critic to mature before influencing the actor, and (3) a scalable generalized critic weighting scheme that extends traditional reward-weighted methods while preserving model diversity through Wasserstein regularization. Through extensive experiments on Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art performance in text-to-image alignment tasks and generalization to unseen human preference models. Our results demonstrate that even with a computationally efficient critic model, we can robustly finetune flow models without compromising generative quality, diversity, or stability.</li>
</ul>

<h3>Title: Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods</h3>
<ul>
<li><strong>Authors: </strong>Justus Arweiler, Indra Jungjohann, Aparna Muraleedharan, Heike Leitte, Jakob Burger, Kerstin Münnemann, Fabian Jirasek, Hans Hasse</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18075">https://arxiv.org/abs/2510.18075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18075">https://arxiv.org/pdf/2510.18075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18075]] Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods(https://arxiv.org/abs/2510.18075)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) holds great potential to advance anomaly detection (AD) in chemical processes. However, the development of ML-based methods is hindered by the lack of openly available experimental data. To address this gap, we have set up a laboratory-scale batch distillation plant and operated it to generate an extensive experimental database, covering fault-free experiments and experiments in which anomalies were intentionally induced, for training advanced ML-based AD methods. In total, 119 experiments were conducted across a wide range of operating conditions and mixtures. Most experiments containing anomalies were paired with a corresponding fault-free one. The database that we provide here includes time-series data from numerous sensors and actuators, along with estimates of measurement uncertainty. In addition, unconventional data sources -- such as concentration profiles obtained via online benchtop NMR spectroscopy and video and audio recordings -- are provided. Extensive metadata and expert annotations of all experiments are included. The anomaly annotations are based on an ontology developed in this work. The data are organized in a structured database and made freely available via this http URL. This new database paves the way for the development of advanced ML-based AD methods. As it includes information on the causes of anomalies, it further enables the development of interpretable and explainable ML approaches, as well as methods for anomaly mitigation.</li>
</ul>

<h3>Title: MEG-GPT: A transformer-based foundation model for magnetoencephalography data</h3>
<ul>
<li><strong>Authors: </strong>Rukuang Huang, Sungjun Cho, Chetan Gohil, Oiwi Parker Jones, Mark Woolrich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18080">https://arxiv.org/abs/2510.18080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18080">https://arxiv.org/pdf/2510.18080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18080]] MEG-GPT: A transformer-based foundation model for magnetoencephalography data(https://arxiv.org/abs/2510.18080)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Modelling the complex spatiotemporal patterns of large-scale brain dynamics is crucial for neuroscience, but traditional methods fail to capture the rich structure in modalities such as magnetoencephalography (MEG). Recent advances in deep learning have enabled significant progress in other domains, such as language and vision, by using foundation models at scale. Here, we introduce MEG-GPT, a transformer based foundation model that uses time-attention and next time-point prediction. To facilitate this, we also introduce a novel data-driven tokeniser for continuous MEG data, which preserves the high temporal resolution of continuous MEG signals without lossy transformations. We trained MEG-GPT on tokenised brain region time-courses extracted from a large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that the learnt model can generate data with realistic spatio-spectral properties, including transient events and population variability. Critically, it performs well in downstream decoding tasks, improving downstream supervised prediction task, showing improved zero-shot generalisation across sessions (improving accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49) compared to a baseline methods. Furthermore, we show the model can be efficiently fine-tuned on a smaller labelled dataset to boost performance in cross-subject decoding scenarios. This work establishes a powerful foundation model for electrophysiological data, paving the way for applications in computational neuroscience and neural decoding.</li>
</ul>

<h3>Title: Chimera: Compositional Image Generation using Part-based Concepting</h3>
<ul>
<li><strong>Authors: </strong>Shivam Singh, Yiming Chen, Agneet Chatterjee, Amit Raj, James Hays, Yezhou Yang, Chitra Baral</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18083">https://arxiv.org/abs/2510.18083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18083">https://arxiv.org/pdf/2510.18083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18083]] Chimera: Compositional Image Generation using Part-based Concepting(https://arxiv.org/abs/2510.18083)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Personalized image generative models are highly proficient at synthesizing images from text or a single image, yet they lack explicit control for composing objects from specific parts of multiple source images without user specified masks or annotations. To address this, we introduce Chimera, a personalized image generation model that generates novel objects by combining specified parts from different source images according to textual instructions. To train our model, we first construct a dataset from a taxonomy built on 464 unique (part, subject) pairs, which we term semantic atoms. From this, we generate 37k prompts and synthesize the corresponding images with a high-fidelity text-to-image model. We train a custom diffusion prior model with part-conditional guidance, which steers the image-conditioning features to enforce both semantic identity and spatial layout. We also introduce an objective metric PartEval to assess the fidelity and compositional accuracy of generation pipelines. Human evaluations and our proposed metric show that Chimera outperforms other baselines by 14% in part alignment and compositional accuracy and 21% in visual quality.</li>
</ul>

<h3>Title: Latent Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Dario Shariatian, Alain Durmus, Stefano Peluchetti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18114">https://arxiv.org/abs/2510.18114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18114">https://arxiv.org/pdf/2510.18114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18114]] Latent Discrete Diffusion Models(https://arxiv.org/abs/2510.18114)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study discrete diffusion for language and other categorical data and focus on a common limitation of masked denoisers: reverse transitions typically factorize across positions, which can weaken joint structure and degrade quality in few-step generation. We propose \emph{Latent Discrete Diffusion Models} (LDDMs), which couple a masked discrete diffusion over tokens with a continuous diffusion over latent embeddings. The latent channel provides a softer signal and carries cross-token dependencies that help resolve ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially resolve the latent and then the discrete chain conditionally on it. For both variants we derive ELBO-style objectives and discuss design choices to learn informative latents yet amenable to diffusoin modeling. In experiments, LDDMs yield improvements on unconditional generation metrics as compared to state-of-the-art masked discrete diffusion baselines, and are effective at lower sampling budgets, where unmasking many tokens per step is desirable.</li>
</ul>

<h3>Title: Online In-Context Distillation for Low-Resource Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Kang, Rahaf Aljundi, Vaggelis Dorovatas, Karteek Alahari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18117">https://arxiv.org/abs/2510.18117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18117">https://arxiv.org/pdf/2510.18117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18117]] Online In-Context Distillation for Low-Resource Vision Language Models(https://arxiv.org/abs/2510.18117)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>As the field continues its push for ever more resources, this work turns the spotlight on a critical question: how can vision-language models (VLMs) be adapted to thrive in low-resource, budget-constrained settings? While large VLMs offer strong performance, they are impractical to deploy in such settings. Small VLMs, on the other hand, are efficient but typically require costly fine-tuning to close the performance gap with larger models in the deployment domain. Inspired by the in-context learning framework, we propose an online In-Context Distillation (ICD) method, in which a small VLM collaborates with a stronger teacher model at inference time, distilling its knowledge via sparse demonstrations to efficiently bridge the gap between them. Our method is built on an in-depth analysis that identifies the scale and the choice of models for which vision-language ICL is currently feasible, and demonstrates the advantage of ICL over fine-tuning under constrained compute budgets. We enhance our method with a novel cross-modal demonstration selection strategy, teacher test-time scaling to reduce noise, and student uncertainty conditioning to dynamically populate a demonstration pool and minimize teacher queries. Our ICD method significantly boosts the performance of small models (up to 33%) using scarce teacher annotations (as low as 4%), and competes with the teacher's zero-shot performance.</li>
</ul>

<h3>Title: Gradient Variance Reveals Failure Modes in Flow-Based Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Teodora Reu, Sixtine Dromigny, Michael Bronstein, Francisco Vargas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18118">https://arxiv.org/abs/2510.18118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18118">https://arxiv.org/pdf/2510.18118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18118]] Gradient Variance Reveals Failure Modes in Flow-Based Generative Models(https://arxiv.org/abs/2510.18118)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rectified Flows learn ODE vector fields whose trajectories are straight between source and target distributions, enabling near one-step inference. We show that this straight-path objective conceals fundamental failure modes: under deterministic training, low gradient variance drives memorization of arbitrary training pairings, even when interpolant lines between pairs intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport and use the loss gradient variance across stochastic and deterministic regimes to characterize which vector fields optimization favors in each setting. We then show that, in a setting where all interpolating lines intersect, applying Rectified Flow yields the same specific pairings at inference as during training. More generally, we prove that a memorizing vector field exists even when training interpolants intersect, and that optimizing the straight-path objective converges to this ill-defined field. At inference, deterministic integration reproduces the exact training pairings. We validate our findings empirically on the CelebA dataset, confirming that deterministic interpolants induce memorization, while the injection of small noise restores generalization.</li>
</ul>

<h3>Title: HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Sudarshan Babu, Phillip Lo, Xiao Zhang, Aadi Srivastava, Ali Davariashtiyani, Jason Perera, Michael Maire, Aly A. Khan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18122">https://arxiv.org/abs/2510.18122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18122">https://arxiv.org/pdf/2510.18122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18122]] HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields(https://arxiv.org/abs/2510.18122)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce HyperDiffusionFields (HyDiF), a framework that models 3D molecular conformers as continuous fields rather than discrete atomic coordinates or graphs. At the core of our approach is the Molecular Directional Field (MDF), a vector field that maps any point in space to the direction of the nearest atom of a particular type. We represent MDFs using molecule-specific neural implicit fields, which we call Molecular Neural Fields (MNFs). To enable learning across molecules and facilitate generalization, we adopt an approach where a shared hypernetwork, conditioned on a molecule, generates the weights of the given molecule's MNF. To endow the model with generative capabilities, we train the hypernetwork as a denoising diffusion model, enabling sampling in the function space of molecular fields. Our design naturally extends to a masked diffusion mechanism to support structure-conditioned generation tasks, such as molecular inpainting, by selectively noising regions of the field. Beyond generation, the localized and continuous nature of MDFs enables spatially fine-grained feature extraction for molecular property prediction, something not easily achievable with graph or point cloud based methods. Furthermore, we demonstrate that our approach scales to larger biomolecules, illustrating a promising direction for field-based molecular modeling.</li>
</ul>

<h3>Title: World-in-World: World Models in a Closed-Loop World</h3>
<ul>
<li><strong>Authors: </strong>Jiahan Zhang, Muqing Jiang, Nanru Dai, Taiming Lu, Arda Uzunoglu, Shunchi Zhang, Yana Wei, Jiahao Wang, Vishal M. Patel, Paul Pu Liang, Daniel Khashabi, Cheng Peng, Rama Chellappa, Tianmin Shu, Alan Yuille, Yilun Du, Jieneng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18135">https://arxiv.org/abs/2510.18135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18135">https://arxiv.org/pdf/2510.18135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18135]] World-in-World: World Models in a Closed-Loop World(https://arxiv.org/abs/2510.18135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative world models (WMs) can now simulate worlds with striking visual realism, which naturally raises the question of whether they can endow embodied agents with predictive perception for decision making. Progress on this question has been limited by fragmented evaluation: most existing benchmarks adopt open-loop protocols that emphasize visual quality in isolation, leaving the core issue of embodied utility unresolved, i.e., do WMs actually help agents succeed at embodied tasks? To address this gap, we introduce World-in-World, the first open platform that benchmarks WMs in a closed-loop world that mirrors real agent-environment interactions. World-in-World provides a unified online planning strategy and a standardized action API, enabling heterogeneous WMs for decision making. We curate four closed-loop environments that rigorously evaluate diverse WMs, prioritize task success as the primary metric, and move beyond the common focus on visual quality; we also present the first data scaling law for world models in embodied settings. Our study uncovers three surprises: (1) visual quality alone does not guarantee task success, controllability matters more; (2) scaling post-training with action-observation data is more effective than upgrading the pretrained video generators; and (3) allocating more inference-time compute allows WMs to substantially improve closed-loop performance.</li>
</ul>

<h3>Title: VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis</h3>
<ul>
<li><strong>Authors: </strong>Fatima AlGhamdi, Omar Alharbi, Abdullah Aldwyish, Raied Aljadaany, Muhammad Kamran J Khan, Huda Alamri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18187">https://arxiv.org/abs/2510.18187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18187">https://arxiv.org/pdf/2510.18187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18187]] VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis(https://arxiv.org/abs/2510.18187)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in crowded scenes is challenging due to severe inter-person occlusions and highly dynamic, context-dependent motion patterns. Existing approaches often struggle to adapt to varying crowd densities and lack interpretable anomaly indicators. To address these limitations, we introduce VelocityNet, a dual-pipeline framework that combines head detection and dense optical flow to extract person-specific velocities. Hierarchical clustering categorizes these velocities into semantic motion classes (halt, slow, normal, and fast), and a percentile-based anomaly scoring system measures deviations from learned normal patterns. Experiments demonstrate the effectiveness of our framework in real-time detection of diverse anomalous motion patterns within densely crowded environments.</li>
</ul>

<h3>Title: EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Maryam Dialameh, Hossein Rajabzadeh, Jung Suk Sim, Hyock Ju Kwon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18213">https://arxiv.org/abs/2510.18213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18213">https://arxiv.org/pdf/2510.18213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18213]] EMA-SAM: Exponential Moving-average for SAM-based PTMC Segmentation(https://arxiv.org/abs/2510.18213)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Papillary thyroid microcarcinoma (PTMC) is increasingly managed with radio-frequency ablation (RFA), yet accurate lesion segmentation in ultrasound videos remains difficult due to low contrast, probe-induced motion, and heat-related artifacts. The recent Segment Anything Model 2 (SAM-2) generalizes well to static images, but its frame-independent design yields unstable predictions and temporal drift in interventional ultrasound. We introduce \textbf{EMA-SAM}, a lightweight extension of SAM-2 that incorporates a confidence-weighted exponential moving average pointer into the memory bank, providing a stable latent prototype of the tumour across frames. This design preserves temporal coherence through probe pressure and bubble occlusion while rapidly adapting once clear evidence reappears. On our curated PTMC-RFA dataset (124 minutes, 13 patients), EMA-SAM improves \emph{maxDice} from 0.82 (SAM-2) to 0.86 and \emph{maxIoU} from 0.72 to 0.76, while reducing false positives by 29\%. On external benchmarks, including VTUS and colonoscopy video polyp datasets, EMA-SAM achieves consistent gains of 2--5 Dice points over SAM-2. Importantly, the EMA pointer adds \textless0.1\% FLOPs, preserving real-time throughput of $\sim$30\,FPS on a single A100 GPU. These results establish EMA-SAM as a robust and efficient framework for stable tumour tracking, bridging the gap between foundation models and the stringent demands of interventional ultrasound. Codes are available here \hyperref[code {this https URL}.</li>
</ul>

<h3>Title: VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</h3>
<ul>
<li><strong>Authors: </strong>Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18214">https://arxiv.org/abs/2510.18214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18214">https://arxiv.org/pdf/2510.18214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18214]] VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety(https://arxiv.org/abs/2510.18214)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.</li>
</ul>

<h3>Title: Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Cai, Liulei Li, Gensheng Pei, Tao Chen, Jinshan Pan, Yazhou Yao, Wenguan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18229">https://arxiv.org/abs/2510.18229</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18229">https://arxiv.org/pdf/2510.18229</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18229]] Beyond Frequency: Scoring-Driven Debiasing for Object Detection via Blueprint-Prompted Image Synthesis(https://arxiv.org/abs/2510.18229)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents a generation-based debiasing framework for object detection. Prior debiasing methods are often limited by the representation diversity of samples, while naive generative augmentation often preserves the biases it aims to solve. Moreover, our analysis reveals that simply generating more data for rare classes is suboptimal due to two core issues: i) instance frequency is an incomplete proxy for the true data needs of a model, and ii) current layout-to-image synthesis lacks the fidelity and control to generate high-quality, complex scenes. To overcome this, we introduce the representation score (RS) to diagnose representational gaps beyond mere frequency, guiding the creation of new, unbiased layouts. To ensure high-quality synthesis, we replace ambiguous text prompts with a precise visual blueprint and employ a generative alignment strategy, which fosters communication between the detector and generator. Our method significantly narrows the performance gap for underrepresented object groups, \eg, improving large/rare instances by 4.4/3.6 mAP over the baseline, and surpassing prior L2I synthesis models by 15.9 mAP for layout accuracy in generated images.</li>
</ul>

<h3>Title: From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziwei Huang, Ying Shu, Hao Fang, Quanyu Long, Wenya Wang, Qiushi Guo, Tiezheng Ge, Leilei Gan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18263">https://arxiv.org/abs/2510.18263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18263">https://arxiv.org/pdf/2510.18263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18263]] From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation(https://arxiv.org/abs/2510.18263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Subject-driven image generation models face a fundamental trade-off between identity preservation (fidelity) and prompt adherence (editability). While online reinforcement learning (RL), specifically GPRO, offers a promising solution, we find that a naive application of GRPO leads to competitive degradation, as the simple linear aggregation of rewards with static weights causes conflicting gradient signals and a misalignment with the temporal dynamics of the diffusion process. To overcome these limitations, we propose Customized-GRPO, a novel framework featuring two key innovations: (i) Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly penalizes conflicted reward signals and amplifies synergistic ones, providing a sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW), which aligns the optimization pressure with the model's temporal dynamics by prioritizing prompt-following in the early, identity preservation in the later. Extensive experiments demonstrate that our method significantly outperforms naive GRPO baselines, successfully mitigating competitive degradation. Our model achieves a superior balance, generating images that both preserve key identity features and accurately adhere to complex textual prompts.</li>
</ul>

<h3>Title: Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Vishal Vinod</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18287">https://arxiv.org/abs/2510.18287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18287">https://arxiv.org/pdf/2510.18287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18287]] Efficient Few-shot Identity Preserving Attribute Editing for 3D-aware Deep Generative Models(https://arxiv.org/abs/2510.18287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Identity preserving editing of faces is a generative task that enables modifying the illumination, adding/removing eyeglasses, face aging, editing hairstyles, modifying expression etc., while preserving the identity of the face. Recent progress in 2D generative models have enabled photorealistic editing of faces using simple techniques leveraging the compositionality in GANs. However, identity preserving editing for 3D faces with a given set of attributes is a challenging task as the generative model must reason about view consistency from multiple poses and render a realistic 3D face. Further, 3D portrait editing requires large-scale attribute labelled datasets and presents a trade-off between editability in low-resolution and inflexibility to editing in high resolution. In this work, we aim to alleviate some of the constraints in editing 3D faces by identifying latent space directions that correspond to photorealistic edits. To address this, we present a method that builds on recent advancements in 3D-aware deep generative models and 2D portrait editing techniques to perform efficient few-shot identity preserving attribute editing for 3D-aware generative models. We aim to show from experimental results that using just ten or fewer labelled images of an attribute is sufficient to estimate edit directions in the latent space that correspond to 3D-aware attribute editing. In this work, we leverage an existing face dataset with masks to obtain the synthetic images for few attribute examples required for estimating the edit directions. Further, to demonstrate the linearity of edits, we investigate one-shot stylization by performing sequential editing and use the (2D) Attribute Style Manipulation (ASM) technique to investigate a continuous style manifold for 3D consistent identity preserving face aging. Code and results are available at: this https URL</li>
</ul>

<h3>Title: GeoDiff: Geometry-Guided Diffusion for Metric Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Tuan Pham, Thanh-Tung Le, Xiaohui Xie, Stephan Mandt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18291">https://arxiv.org/abs/2510.18291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18291">https://arxiv.org/pdf/2510.18291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18291]] GeoDiff: Geometry-Guided Diffusion for Metric Depth Estimation(https://arxiv.org/abs/2510.18291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a novel framework for metric depth estimation that enhances pretrained diffusion-based monocular depth estimation (DB-MDE) models with stereo vision guidance. While existing DB-MDE methods excel at predicting relative depth, estimating absolute metric depth remains challenging due to scale ambiguities in single-image scenarios. To address this, we reframe depth estimation as an inverse problem, leveraging pretrained latent diffusion models (LDMs) conditioned on RGB images, combined with stereo-based geometric constraints, to learn scale and shift for accurate depth recovery. Our training-free solution seamlessly integrates into existing DB-MDE frameworks and generalizes across indoor, outdoor, and complex environments. Extensive experiments demonstrate that our approach matches or surpasses state-of-the-art methods, particularly in challenging scenarios involving translucent and specular surfaces, all without requiring retraining.</li>
</ul>

<h3>Title: Towards Identifiability of Hierarchical Temporal Causal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zijian Li, Minghao Fu, Junxian Huang, Yifan Shen, Ruichu Cai, Yuewen Sun, Guangyi Chen, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18310">https://arxiv.org/abs/2510.18310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18310">https://arxiv.org/pdf/2510.18310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18310]] Towards Identifiability of Hierarchical Temporal Causal Representation Learning(https://arxiv.org/abs/2510.18310)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modeling hierarchical latent dynamics behind time series data is critical for capturing temporal dependencies across multiple levels of abstraction in real-world tasks. However, existing temporal causal representation learning methods fail to capture such dynamics, as they fail to recover the joint distribution of hierarchical latent variables from \textit{single-timestep observed variables}. Interestingly, we find that the joint distribution of hierarchical latent variables can be uniquely determined using three conditionally independent observations. Building on this insight, we propose a Causally Hierarchical Latent Dynamic (CHiLD) identification framework. Our approach first employs temporal contextual observed variables to identify the joint distribution of multi-layer latent variables. Sequentially, we exploit the natural sparsity of the hierarchical structure among latent variables to identify latent variables within each layer. Guided by the theoretical results, we develop a time series generative model grounded in variational inference. This model incorporates a contextual encoder to reconstruct multi-layer latent variables and normalize flow-based hierarchical prior networks to impose the independent noise condition of hierarchical latent dynamics. Empirical evaluations on both synthetic and real-world datasets validate our theoretical claims and demonstrate the effectiveness of CHiLD in modeling hierarchical latent dynamics.</li>
</ul>

<h3>Title: Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, Matthijs van Leeuwen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18328">https://arxiv.org/abs/2510.18328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18328">https://arxiv.org/pdf/2510.18328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18328]] Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching(https://arxiv.org/abs/2510.18328)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, anomaly</a></li>
<li><strong>Abstract: </strong>We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for semi-supervised anomaly detection in tabular data. TCCM is inspired by flow matching, a recent generative modeling framework that learns velocity fields between probability distributions and has shown strong performance compared to diffusion models and generative adversarial networks. Instead of directly applying flow matching as originally formulated, TCCM builds on its core idea -- learning velocity fields between distributions -- but simplifies the framework by predicting a time-conditioned contraction vector toward a fixed target (the origin) at each sampled time step. This design offers three key advantages: (1) a lightweight and scalable training objective that removes the need for solving ordinary differential equations during training and inference; (2) an efficient scoring strategy called one time-step deviation, which quantifies deviation from expected contraction behavior in a single forward pass, addressing the inference bottleneck of existing continuous-time models such as DTE (a diffusion-based model with leading anomaly detection accuracy but heavy inference cost); and (3) explainability and provable robustness, as the learned velocity field operates directly in input space, making the anomaly score inherently feature-wise attributable; moreover, the score function is Lipschitz-continuous with respect to the input, providing theoretical guarantees under small perturbations. Extensive experiments on the ADBench benchmark show that TCCM strikes a favorable balance between detection accuracy and inference cost, outperforming state-of-the-art methods -- especially on high-dimensional and large-scale datasets. The source code is available at our GitHub repository.</li>
</ul>

<h3>Title: Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption</h3>
<ul>
<li><strong>Authors: </strong>Yepeng Liu, Xuandong Zhao, Dawn Song, Gregory W. Wornell, Yuheng Bu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18333">https://arxiv.org/abs/2510.18333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18333">https://arxiv.org/pdf/2510.18333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18333]] Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption(https://arxiv.org/abs/2510.18333)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Despite progress in watermarking algorithms for large language models (LLMs), real-world deployment remains limited. We argue that this gap stems from misaligned incentives among LLM providers, platforms, and end users, which manifest as four key barriers: competitive risk, detection-tool governance, robustness concerns and attribution issues. We revisit three classes of watermarking through this lens. \emph{Model watermarking} naturally aligns with LLM provider interests, yet faces new challenges in open-source ecosystems. \emph{LLM text watermarking} offers modest provider benefit when framed solely as an anti-misuse tool, but can gain traction in narrowly scoped settings such as dataset de-contamination or user-controlled provenance. \emph{In-context watermarking} (ICW) is tailored for trusted parties, such as conference organizers or educators, who embed hidden watermarking instructions into documents. If a dishonest reviewer or student submits this text to an LLM, the output carries a detectable watermark indicating misuse. This setup aligns incentives: users experience no quality loss, trusted parties gain a detection tool, and LLM providers remain neutral by simply following watermark instructions. We advocate for a broader exploration of incentive-aligned methods, with ICW as an example, in domains where trusted parties need reliable tools to detect misuse. More broadly, we distill design principles for incentive-aligned, domain-specific watermarking and outline future research directions. Our position is that the practical adoption of LLM watermarking requires aligning stakeholder incentives in targeted application domains and fostering active community engagement.</li>
</ul>

<h3>Title: ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18341">https://arxiv.org/abs/2510.18341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18341">https://arxiv.org/pdf/2510.18341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18341]] ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation(https://arxiv.org/abs/2510.18341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Realistic view extrapolation is critical for closed-loop simulation in autonomous driving, yet it remains a significant challenge for current Novel View Synthesis (NVS) methods, which often produce distorted and inconsistent images beyond the original trajectory. This report presents our winning solution which ctook first place in the RealADSim Workshop NVS track at ICCV 2025. To address the core challenges of street view extrapolation, we introduce a comprehensive four-stage pipeline. First, we employ a data-driven initialization strategy to generate a robust pseudo-LiDAR point cloud, avoiding local minima. Second, we inject strong geometric priors by modeling the road surface with a novel dimension-reduced SDF termed 2D-SDF. Third, we leverage a generative prior to create pseudo ground truth for extrapolated viewpoints, providing auxilary supervision. Finally, a data-driven adaptation network removes time-specific artifacts. On the RealADSim-NVS benchmark, our method achieves a final score of 0.441, ranking first among all participants.</li>
</ul>

<h3>Title: Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Vipul Rathore, Malik Hammad Faisal, Parag Singla, Mausam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18344">https://arxiv.org/abs/2510.18344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18344">https://arxiv.org/pdf/2510.18344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18344]] Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction(https://arxiv.org/abs/2510.18344)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Distantly Supervised Relation Extraction (DSRE) remains a long-standing challenge in NLP, where models must learn from noisy bag-level annotations while making sentence-level predictions. While existing state-of-the-art (SoTA) DSRE models rely on task-specific training, their integration with in-context learning (ICL) using large language models (LLMs) remains underexplored. A key challenge is that the LLM may not learn relation semantics correctly, due to noisy annotation. In response, we propose HYDRE -- HYbrid Distantly Supervised Relation Extraction framework. It first uses a trained DSRE model to identify the top-k candidate relations for a given test sentence, then uses a novel dynamic exemplar retrieval strategy that extracts reliable, sentence-level exemplars from training data, which are then provided in LLM prompt for outputting the final relation(s). We further extend HYDRE to cross-lingual settings for RE in low-resource languages. Using available English DSRE training data, we evaluate all methods on English as well as a newly curated benchmark covering four diverse low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE achieves up to 20 F1 point gains in English and, on average, 17 F1 points on Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's efficacy compared to other prompting strategies.</li>
</ul>

<h3>Title: GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data</h3>
<ul>
<li><strong>Authors: </strong>Yudong Li, Hao Li, Xianxu Hou, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18345">https://arxiv.org/abs/2510.18345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18345">https://arxiv.org/pdf/2510.18345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18345]] GPTFace: Generative Pre-training of Facial-Linguistic Transformer by Span Masking and Weakly Correlated Text-image Data(https://arxiv.org/abs/2510.18345)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Compared to the prosperity of pre-training models in natural image understanding, the research on large-scale pre-training models for facial knowledge learning is still limited. Current approaches mainly rely on manually assembled and annotated face datasets for training, but labeling such datasets is labor-intensive and the trained models have limited scalability beyond the training data. To address these limitations, we present a generative pre-training model for facial knowledge learning that leverages large-scale web-built data for training. We use texts and images containing human faces crawled from the internet and conduct pre-training on self-supervised tasks, including masked image/language modeling (MILM) and image-text matching (ITM). During the generation stage, we further utilize the image-text matching loss to pull the generation distribution towards the control signal for controllable image/text generation. Experimental results demonstrate that our model achieves comparable performance to state-of-the-art pre-training models for various facial downstream tasks, such as attribution classification and expression recognition. Furthermore, our approach is also applicable to a wide range of face editing tasks, including face attribute editing, expression manipulation, mask removal, and photo inpainting.</li>
</ul>

<h3>Title: Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</h3>
<ul>
<li><strong>Authors: </strong>Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18353">https://arxiv.org/abs/2510.18353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18353">https://arxiv.org/pdf/2510.18353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18353]] Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback(https://arxiv.org/abs/2510.18353)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Direct preference optimization (DPO) methods have shown strong potential in aligning text-to-image diffusion models with human preferences by training on paired comparisons. These methods improve training stability by avoiding the REINFORCE algorithm but still struggle with challenges such as accurately estimating image probabilities due to the non-linear nature of the sigmoid function and the limited diversity of offline datasets. In this paper, we introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new preference learning framework grounded in inverse reinforcement learning. Diffusion-DRO removes the dependency on a reward model by casting preference learning as a ranking problem, thereby simplifying the training objective into a denoising formulation and overcoming the non-linear estimation issues found in prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert demonstrations with online policy-generated negative samples, enabling it to effectively capture human preferences while addressing the limitations of offline data. Comprehensive experiments show that Diffusion-DRO delivers improved generation quality across a range of challenging and unseen prompts, outperforming state-of-the-art baselines in both both quantitative metrics and user studies. Our source code and pre-trained models are available at this https URL.</li>
</ul>

<h3>Title: KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers</h3>
<ul>
<li><strong>Authors: </strong>Mohd Ruhul Ameen, Akif Islam, Farjana Aktar, M. Saifuzzaman Rafat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18355">https://arxiv.org/abs/2510.18355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18355">https://arxiv.org/pdf/2510.18355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18355]] KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers(https://arxiv.org/abs/2510.18355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In Bangladesh, many farmers continue to face challenges in accessing timely, expert-level agricultural guidance. This paper presents KrishokBondhu, a voice-enabled, call-centre-integrated advisory platform built on a Retrieval-Augmented Generation (RAG) framework, designed specifically for Bengali-speaking farmers. The system aggregates authoritative agricultural handbooks, extension manuals, and NGO publications; applies Optical Character Recognition (OCR) and document-parsing pipelines to digitize and structure the content; and indexes this corpus in a vector database for efficient semantic retrieval. Through a simple phone-based interface, farmers can call the system to receive real-time, context-aware advice: speech-to-text converts the Bengali query, the RAG module retrieves relevant content, a large language model (Gemma 3-4B) generates a context-grounded response, and text-to-speech delivers the answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced high-quality responses for 72.7% of diverse agricultural queries covering crop management, disease control, and cultivation practices. Compared to the KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on a 5-point scale, a 44.7% improvement, with especially large gains in contextual richness (+367%) and completeness (+100.4%), while maintaining comparable relevance and technical specificity. Semantic similarity analysis further revealed a strong correlation between retrieved context and answer quality, emphasizing the importance of grounding generative responses in curated documentation. KrishokBondhu demonstrates the feasibility of integrating call-centre accessibility, multilingual voice interaction, and modern RAG techniques to deliver expert-level agricultural guidance to remote Bangladeshi farmers, paving the way toward a fully AI-driven agricultural advisory ecosystem.</li>
</ul>

<h3>Title: Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Kim, Hyunjin Hwang, Kijung Shin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18360">https://arxiv.org/abs/2510.18360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18360">https://arxiv.org/pdf/2510.18360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18360]] Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding(https://arxiv.org/abs/2510.18360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The performance of a deep learning model on a specific task and dataset depends heavily on its neural architecture, motivating considerable efforts to rapidly and accurately identify architectures suited to the target task and dataset. To achieve this, researchers use machine learning models-typically neural architecture encoders-to predict the performance of a neural architecture. Many state-of-the-art encoders aim to capture information flow within a neural architecture, which reflects how information moves through the forward pass and backpropagation, via a specialized model structure. However, due to their complicated structures, these flow-based encoders are significantly slower to process neural architectures compared to simpler encoders, presenting a notable practical challenge. To address this, we propose FGP, a novel pre-training method for neural architecture encoding that trains an encoder to capture the information flow without requiring specialized model structures. FGP trains an encoder to reconstruct a flow surrogate, our proposed representation of the neural architecture's information flow. Our experiments show that FGP boosts encoder performance by up to 106% in Precision-1%, compared to the same encoder trained solely with supervised learning.</li>
</ul>

<h3>Title: ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization</h3>
<ul>
<li><strong>Authors: </strong>Yuanhe Guo, Linxi Xie, Zhuoran Chen, Kangrui Yu, Ryan Po, Guandao Yang, Gordon Wetztein, Hongyi Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18433">https://arxiv.org/abs/2510.18433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18433">https://arxiv.org/pdf/2510.18433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18433]] ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization(https://arxiv.org/abs/2510.18433)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.</li>
</ul>

<h3>Title: LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance</h3>
<ul>
<li><strong>Authors: </strong>Anna Oliveras, Roger Marí, Rafael Redondo, Oriol Guardià, Ana Tost, Bhalaji Nagarajan, Carolina Migliorelli, Vicent Ribas, Petia Radeva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18446">https://arxiv.org/abs/2510.18446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18446">https://arxiv.org/pdf/2510.18446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18446]] LAND: Lung and Nodule Diffusion for 3D Chest CT Synthesis with Anatomical Guidance(https://arxiv.org/abs/2510.18446)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work introduces a new latent diffusion model to generate high-quality 3D chest CT scans conditioned on 3D anatomical masks. The method synthesizes volumetric images of size 256x256x256 at 1 mm isotropic resolution using a single mid-range GPU, significantly lowering the computational cost compared to existing approaches. The conditioning masks delineate lung and nodule regions, enabling precise control over the output anatomical features. Experimental results demonstrate that conditioning solely on nodule masks leads to anatomically incorrect outputs, highlighting the importance of incorporating global lung structure for accurate conditional synthesis. The proposed approach supports the generation of diverse CT volumes with and without lung nodules of varying attributes, providing a valuable tool for training AI models or healthcare professionals.</li>
</ul>

<h3>Title: Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tianci Bi, Xiaoyi Zhang, Yan Lu, Nanning Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18457">https://arxiv.org/abs/2510.18457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18457">https://arxiv.org/pdf/2510.18457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18457]] Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion Models(https://arxiv.org/abs/2510.18457)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>The performance of Latent Diffusion Models (LDMs) is critically dependent on the quality of their visual tokenizer. While recent works have explored incorporating Vision Foundation Models (VFMs) via distillation, we identify a fundamental flaw in this approach: it inevitably weakens the robustness of alignment with the original VFM, causing the aligned latents to deviate semantically under distribution shifts. In this paper, we bypass distillation by proposing a more direct approach: Vision Foundation Model Variational Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE decoder with Multi-Scale Latent Fusion and Progressive Resolution Reconstruction blocks, enabling high-quality reconstruction from spatially coarse VFM features. Furthermore, we provide a comprehensive analysis of representation dynamics during diffusion training, introducing the proposed SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows us to develop a joint tokenizer-diffusion alignment strategy that dramatically accelerates convergence. Our innovations in tokenizer design and training strategy lead to superior performance and efficiency: our system reaches a gFID (w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers). With continued training to 640 epochs, it further attains a gFID (w/o CFG) of 1.62, establishing direct VFM integration as a superior paradigm for LDMs.</li>
</ul>

<h3>Title: How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices</h3>
<ul>
<li><strong>Authors: </strong>Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18480">https://arxiv.org/abs/2510.18480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18480">https://arxiv.org/pdf/2510.18480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18480]] How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices(https://arxiv.org/abs/2510.18480)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.</li>
</ul>

<h3>Title: RayPose: Ray Bundling Diffusion for Template Views in Unseen 6D Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Junwen Huang, Shishir Reddy Vutukur, Peter KT Yu, Nassir Navab, Slobodan Ilic, Benjamin Busam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18521">https://arxiv.org/abs/2510.18521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18521">https://arxiv.org/pdf/2510.18521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18521]] RayPose: Ray Bundling Diffusion for Template Views in Unseen 6D Object Pose Estimation(https://arxiv.org/abs/2510.18521)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Typical template-based object pose pipelines estimate the pose by retrieving the closest matching template and aligning it with the observed image. However, failure to retrieve the correct template often leads to inaccurate pose predictions. To address this, we reformulate template-based object pose estimation as a ray alignment problem, where the viewing directions from multiple posed template images are learned to align with a non-posed query image. Inspired by recent progress in diffusion-based camera pose estimation, we embed this formulation into a diffusion transformer architecture that aligns a query image with a set of posed templates. We reparameterize object rotation using object-centered camera rays and model object translation by extending scale-invariant translation estimation to dense translation offsets. Our model leverages geometric priors from the templates to guide accurate query pose inference. A coarse-to-fine training strategy based on narrowed template sampling improves performance without modifying the network architecture. Extensive experiments across multiple benchmark datasets show competitive results of our method compared to state-of-the-art approaches in unseen object pose estimation.</li>
</ul>

<h3>Title: Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Behnam Rezaei Bezanjani, Seyyed Hamid Ghafouri, Reza Gholamrezaei</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18568">https://arxiv.org/abs/2510.18568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18568">https://arxiv.org/pdf/2510.18568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18568]] Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain(https://arxiv.org/abs/2510.18568)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The integration of Internet of Things (IoT) devices in healthcare has revolutionized patient care by enabling real-time monitoring, personalized treatments, and efficient data management. However, this technological advancement introduces significant security risks, particularly concerning the confidentiality, integrity, and availability of sensitive medical data. Traditional security measures are often insufficient to address the unique challenges posed by IoT environments, such as heterogeneity, resource constraints, and the need for real-time processing. To tackle these challenges, we propose a comprehensive three-phase security framework designed to enhance the security and reliability of IoT-enabled healthcare systems. In the first phase, the framework assesses the reliability of IoT devices using a reputation-based trust estimation mechanism, which combines device behavior analytics with off-chain data storage to ensure scalability. The second phase integrates blockchain technology with a lightweight proof-of-work mechanism, ensuring data immutability, secure communication, and resistance to unauthorized access. The third phase employs a lightweight Long Short-Term Memory (LSTM) model for anomaly detection and classification, enabling real-time identification of cyber threats. Simulation results demonstrate that the proposed framework outperforms existing methods, achieving a 2% increase in precision, accuracy, and recall, a 5% higher attack detection rate, and a 3% reduction in false alarm rate. These improvements highlight the framework's ability to address critical security concerns while maintaining scalability and real-time performance.</li>
</ul>

<h3>Title: Beyond the Explicit: A Bilingual Dataset for Dehumanization Detection in Social Media</h3>
<ul>
<li><strong>Authors: </strong>Dennis Assenmacher, Paloma Piot, Katarina Laken, David Jurgens, Claudia Wagner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18582">https://arxiv.org/abs/2510.18582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18582">https://arxiv.org/pdf/2510.18582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18582]] Beyond the Explicit: A Bilingual Dataset for Dehumanization Detection in Social Media(https://arxiv.org/abs/2510.18582)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Digital dehumanization, although a critical issue, remains largely overlooked within the field of computational linguistics and Natural Language Processing. The prevailing approach in current research concentrating primarily on a single aspect of dehumanization that identifies overtly negative statements as its core marker. This focus, while crucial for understanding harmful online communications, inadequately addresses the broader spectrum of dehumanization. Specifically, it overlooks the subtler forms of dehumanization that, despite not being overtly offensive, still perpetuate harmful biases against marginalized groups in online interactions. These subtler forms can insidiously reinforce negative stereotypes and biases without explicit offensiveness, making them harder to detect yet equally damaging. Recognizing this gap, we use different sampling methods to collect a theory-informed bilingual dataset from Twitter and Reddit. Using crowdworkers and experts to annotate 16,000 instances on a document- and span-level, we show that our dataset covers the different dimensions of dehumanization. This dataset serves as both a training resource for machine learning models and a benchmark for evaluating future dehumanization detection techniques. To demonstrate its effectiveness, we fine-tune ML models on this dataset, achieving performance that surpasses state-of-the-art models in zero and few-shot in-context settings.</li>
</ul>

<h3>Title: Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</h3>
<ul>
<li><strong>Authors: </strong>Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18632">https://arxiv.org/abs/2510.18632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18632">https://arxiv.org/pdf/2510.18632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18632]] Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views(https://arxiv.org/abs/2510.18632)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Though recent advances in vision-language models (VLMs) have achieved remarkable progress across a wide range of multimodal tasks, understanding 3D spatial relationships from limited views remains a significant challenge. Previous reasoning methods typically rely on pure text (e.g., topological cognitive maps) or on 2D visual cues. However, their limited representational capacity hinders performance in specific tasks that require 3D spatial imagination. To address this limitation, we propose 3DThinker, a framework that can effectively exploits the rich geometric information embedded within images while reasoning, like humans do. Our framework is the first to enable 3D mentaling during reasoning without any 3D prior input, and it does not rely on explicitly labeled 3D data for training. Specifically, our training consists of two stages. First, we perform supervised training to align the 3D latent generated by VLM while reasoning with that of a 3D foundation model (e.g., VGGT). Then, we optimize the entire reasoning trajectory solely based on outcome signals, thereby refining the underlying 3D mentaling. Extensive experiments across multiple benchmarks show that 3DThinker consistently outperforms strong baselines and offers a new perspective toward unifying 3D representations into multimodal reasoning. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions</h3>
<ul>
<li><strong>Authors: </strong>Yanna Ding, Songtao Lu, Yingdong Lu, Tomasz Nowicki, Jianxi Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18638">https://arxiv.org/abs/2510.18638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18638">https://arxiv.org/pdf/2510.18638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18638]] Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions(https://arxiv.org/abs/2510.18638)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Transformer architectures can solve unseen tasks based on input-output pairs in a given prompt due to in-context learning (ICL). Existing theoretical studies on ICL have mainly focused on linear regression tasks, often with i.i.d. inputs. To understand how transformers express ICL when modeling dynamics-driven functions, we investigate Markovian function learning through a structured ICL setup, where we characterize the loss landscape to reveal underlying optimization behaviors. Specifically, we (1) provide the closed-form expression of the global minimizer (in an enlarged parameter space) for a single-layer linear self-attention (LSA) model; (2) prove that recovering transformer parameters that realize the optimal solution is NP-hard in general, revealing a fundamental limitation of one-layer LSA in representing structured dynamical functions; and (3) supply a novel interpretation of a multilayer LSA as performing preconditioned gradient descent to optimize multiple objectives beyond the square loss. These theoretical results are numerically validated using simplified transformers.</li>
</ul>

<h3>Title: MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18692">https://arxiv.org/abs/2510.18692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18692">https://arxiv.org/pdf/2510.18692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18692]] MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation(https://arxiv.org/abs/2510.18692)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Long video generation with Diffusion Transformers (DiTs) is bottlenecked by the quadratic scaling of full attention with sequence length. Since attention is highly redundant, outputs are dominated by a small subset of query-key pairs. Existing sparse methods rely on blockwise coarse estimation, whose accuracy-efficiency trade-offs are constrained by block size. This paper introduces Mixture-of-Groups Attention (MoGA), an efficient sparse attention that uses a lightweight, learnable token router to precisely match tokens without blockwise estimation. Through semantic-aware routing, MoGA enables effective long-range interactions. As a kernel-free method, MoGA integrates seamlessly with modern attention stacks, including FlashAttention and sequence parallelism. Building on MoGA, we develop an efficient long video generation model that end-to-end produces minute-level, multi-shot, 480p videos at 24 fps, with a context length of approximately 580k. Comprehensive experiments on various video generation tasks validate the effectiveness of our approach.</li>
</ul>

<h3>Title: OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales</h3>
<ul>
<li><strong>Authors: </strong>Tung Nguyen, Tuan Pham, Troy Arcomano, Veerabhadra Kotamarthi, Ian Foster, Sandeep Madireddy, Aditya Grover</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18707">https://arxiv.org/abs/2510.18707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18707">https://arxiv.org/pdf/2510.18707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18707]] OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales(https://arxiv.org/abs/2510.18707)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate weather forecasting across time scales is critical for anticipating and mitigating the impacts of climate change. Recent data-driven methods based on deep learning have achieved significant success in the medium range, but struggle at longer subseasonal-to-seasonal (S2S) horizons due to error accumulation in their autoregressive approach. In this work, we propose OmniCast, a scalable and skillful probabilistic model that unifies weather forecasting across timescales. OmniCast consists of two components: a VAE model that encodes raw weather data into a continuous, lower-dimensional latent space, and a diffusion-based transformer model that generates a sequence of future latent tokens given the initial conditioning tokens. During training, we mask random future tokens and train the transformer to estimate their distribution given conditioning and visible tokens using a per-token diffusion head. During inference, the transformer generates the full sequence of future tokens by iteratively unmasking random subsets of tokens. This joint sampling across space and time mitigates compounding errors from autoregressive approaches. The low-dimensional latent space enables modeling long sequences of future latent states, allowing the transformer to learn weather dynamics beyond initial conditions. OmniCast performs competitively with leading probabilistic methods at the medium-range timescale while being 10x to 20x faster, and achieves state-of-the-art performance at the subseasonal-to-seasonal scale across accuracy, physics-based, and probabilistic metrics. Furthermore, we demonstrate that OmniCast can generate stable rollouts up to 100 years ahead. Code and model checkpoints are available at this https URL.</li>
</ul>

<h3>Title: Bayesian Low-Rank Factorization for Robust Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18723">https://arxiv.org/abs/2510.18723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18723">https://arxiv.org/pdf/2510.18723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18723]] Bayesian Low-Rank Factorization for Robust Model Adaptation(https://arxiv.org/abs/2510.18723)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large speech foundation models achieve strong performance across many domains, but they often require adaptation to handle local needs such as code-switching, where speakers mix languages within the same utterance. Direct fine-tuning of these models risks overfitting to the target domain and overwriting the broad capabilities of the base model. To address this challenge, we explore Bayesian factorized adapters for speech foundation models, which place priors near zero to achieve sparser adaptation matrices and thereby retain general performance while adapting to specific domains. We apply our approach to the Whisper model and evaluate on different multilingual code-switching scenarios. Our results show only minimal adaptation loss while significantly reducing catastrophic forgetting of the base model. Compared to LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new domain. These findings highlight the effectiveness of Bayesian adaptation for fine-tuning speech foundation models without sacrificing generalization.</li>
</ul>

<h3>Title: Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference</h3>
<ul>
<li><strong>Authors: </strong>Harry Amad, Zhaozhi Qian, Dennis Frauen, Julianna Piskorz, Stefan Feuerriegel, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18768">https://arxiv.org/abs/2510.18768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18768">https://arxiv.org/pdf/2510.18768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18768]] Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference(https://arxiv.org/abs/2510.18768)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Causal inference is essential for developing and evaluating medical interventions, yet real-world medical datasets are often difficult to access due to regulatory barriers. This makes synthetic data a potentially valuable asset that enables these medical analyses, along with the development of new inference methods themselves. Generative models can produce synthetic data that closely approximate real data distributions, yet existing methods do not consider the unique challenges that downstream causal inference tasks, and specifically those focused on treatments, pose. We establish a set of desiderata that synthetic data containing treatments should satisfy to maximise downstream utility: preservation of (i) the covariate distribution, (ii) the treatment assignment mechanism, and (iii) the outcome generation mechanism. Based on these desiderata, we propose a set of evaluation metrics to assess such synthetic data. Finally, we present STEAM: a novel method for generating Synthetic data for Treatment Effect Analysis in Medicine that mimics the data-generating process of data containing treatments and optimises for our desiderata. We empirically demonstrate that STEAM achieves state-of-the-art performance across our metrics as compared to existing generative models, particularly as the complexity of the true data-generating process increases.</li>
</ul>

<h3>Title: Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction</h3>
<ul>
<li><strong>Authors: </strong>Jannis Fleckenstein, David Kreismann, Tamara Rosemary Govindasamy, Thomas Brunschwiler, Etienne Vos, Mattia Rigotti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18773">https://arxiv.org/abs/2510.18773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18773">https://arxiv.org/pdf/2510.18773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18773]] Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model for Microclimate Impact Prediction(https://arxiv.org/abs/2510.18773)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data, yet conventional machine learning models with limited data often produce inaccurate predictions, particularly in underserved areas. Geospatial foundation models trained on global unstructured data offer a promising alternative by demonstrating strong generalization and requiring only minimal fine-tuning. In this study, an empirical ground truth of urban heat patterns is established by quantifying cooling effects from green spaces and benchmarking them against model predictions to evaluate the model's accuracy. The foundation model is subsequently fine-tuned to predict land surface temperatures under future climate scenarios, and its practical value is demonstrated through a simulated inpainting that highlights its role for mitigation support. The results indicate that foundation models offer a powerful way for evaluating urban heat island mitigation strategies in data-scarce regions to support more climate-resilient cities.</li>
</ul>

<h3>Title: UltraGen: High-Resolution Video Generation with Hierarchical Attention</h3>
<ul>
<li><strong>Authors: </strong>Teng Hu, Jiangning Zhang, Zihan Su, Ran Yi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18775">https://arxiv.org/abs/2510.18775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18775">https://arxiv.org/pdf/2510.18775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18775]] UltraGen: High-Resolution Video Generation with Hierarchical Attention(https://arxiv.org/abs/2510.18775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (<=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.</li>
</ul>

<h3>Title: Rebellious Student: A Complementary Learning Framework for Background Feature Enhancement in Hyperspectral Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenping Jin, Yuyang Tang, Li Zhu, Fei Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18781">https://arxiv.org/abs/2510.18781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18781">https://arxiv.org/pdf/2510.18781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18781]] Rebellious Student: A Complementary Learning Framework for Background Feature Enhancement in Hyperspectral Anomaly Detection(https://arxiv.org/abs/2510.18781)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>A recent class of hyperspectral anomaly detection methods that can be trained once on background datasets and then universally deployed -- without per-scene retraining or parameter tuning -- has demonstrated remarkable efficiency and robustness. Building upon this paradigm, we focus on the integration of spectral and spatial cues and introduce a novel "Rebellious Student" framework for complementary feature learning. Unlike conventional teacher-student paradigms driven by imitation, our method intentionally trains the spatial branch to diverge from the spectral teacher, thereby learning complementary spatial patterns that the teacher fails to capture. A two-stage learning strategy is adopted: (1) a spectral enhancement network is first trained via reverse distillation to obtain robust background spectral representations; and (2) a spatial network -- the rebellious student -- is subsequently optimized using decorrelation losses that enforce feature orthogonality while maintaining reconstruction fidelity to avoid irrelevant noise. Once trained, the framework enhances both spectral and spatial background features, enabling parameter-free and training-free anomaly detection when paired with conventional detectors. Extensive experiments on the HAD100 benchmark show substantial improvements over several established baselines with minimal computational overhead, confirming the effectiveness and generality of the proposed complementary learning paradigm. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Shuxin Lin, Dhaval Patel, Christodoulos Constantinides</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18817">https://arxiv.org/abs/2510.18817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18817">https://arxiv.org/pdf/2510.18817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18817]] Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring(https://arxiv.org/abs/2510.18817)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Small Language Models (SLMs) are becoming increasingly popular in specialized fields, such as industrial applications, due to their efficiency, lower computational requirements, and ability to be fine-tuned for domain-specific tasks, enabling accurate and cost-effective solutions. However, performing complex reasoning using SLMs in specialized fields such as Industry 4.0 remains challenging. In this paper, we propose a knowledge distillation framework for industrial asset health, which transfers reasoning capabilities via Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to smaller, more efficient models (SLMs). We discuss the advantages and the process of distilling LLMs using multi-choice question answering (MCQA) prompts to enhance reasoning and refine decision-making. We also perform in-context learning to verify the quality of the generated knowledge and benchmark the performance of fine-tuned SLMs with generated knowledge against widely used LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform the base models by a significant margin, narrowing the gap to their LLM counterparts. Our code is open-sourced at: this https URL.</li>
</ul>

<h3>Title: An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection</h3>
<ul>
<li><strong>Authors: </strong>Neel Patel, Alexander Wong, Ashkan Ebadi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18819">https://arxiv.org/abs/2510.18819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18819">https://arxiv.org/pdf/2510.18819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18819]] An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection(https://arxiv.org/abs/2510.18819)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Tuberculosis remains a critical global health issue, particularly in resource-limited and remote areas. Early detection is vital for treatment, yet the lack of skilled radiologists underscores the need for artificial intelligence (AI)-driven screening tools. Developing reliable AI models is challenging due to the necessity for large, high-quality datasets, which are costly to obtain. To tackle this, we propose a teacher--student framework which enhances both disease and symptom detection on chest X-rays by integrating two supervised heads and a self-supervised head. Our model achieves an accuracy of 98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and a macro-F1 score of 90.09% for multilabel symptom detection, significantly outperforming baselines. The explainability assessments also show the model bases its predictions on relevant anatomical features, demonstrating promise for deployment in clinical screening and triage settings.</li>
</ul>

<h3>Title: Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Zhu, Meiling Tao, Tiannan Wang, Dongyi Ding, Yuchen Eleanor Jiang, Wangchunshu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18849">https://arxiv.org/abs/2510.18849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18849">https://arxiv.org/pdf/2510.18849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18849]] Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning(https://arxiv.org/abs/2510.18849)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization.</li>
</ul>

<h3>Title: DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Rongyuan Wu, Lingchen Sun, Zhengqiang Zhang, Shihao Wang, Tianhe Wu, Qiaosi Yi, Shuai Li, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.18851">https://arxiv.org/abs/2510.18851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.18851">https://arxiv.org/pdf/2510.18851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.18851]] DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution(https://arxiv.org/abs/2510.18851)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
