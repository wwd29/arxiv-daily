<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-04</h1>
<h3>Title: TabularMath: Evaluating Computational Extrapolation in Tabular Learning via Program-Verified Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Zerui Cheng, Jiashuo Liu, Jianzhu Yao, Pramod Viswanath, Ge Zhang, Wenhao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02523">https://arxiv.org/abs/2602.02523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02523">https://arxiv.org/pdf/2602.02523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02523]] TabularMath: Evaluating Computational Extrapolation in Tabular Learning via Program-Verified Synthesis(https://arxiv.org/abs/2602.02523)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Standard tabular benchmarks mainly focus on the evaluation of a model's capability to interpolate values inside a data manifold, where models good at performing local statistical smoothing are rewarded. However, there exists a very large category of high-value tabular data, including financial modeling and physical simulations, which are generated based upon deterministic computational processes, as opposed to stochastic and noisy relationships. Therefore, we investigate if tabular models can provide an extension from statistical interpolation to computational extrapolation. We propose TabularMath, a diagnostic benchmark of 114 deterministic problems (233,472 rows) generated from verified programs based on GSM8K and AIME. We evaluate 9 tabular architectures and in-context learning (ICL) with GPT-OSS-120B. On standard regression metrics, TabPFN v2.5 performs remarkably well, achieving R^2=0.998 in-distribution and maintaining positive R^2 even under distribution shift, which is unique among the tabular models we tested. When we measure rounded consistency (exact integer match), a different picture emerges: TabPFN v2.5 drops below 10% on out-of-distribution data, while ICL maintains around 40%. This gap between R^2 and exact-match accuracy suggests that tabular models learn smooth function approximations but struggle to recover precise computational outputs under extrapolation. The two paradigms appear complementary: TabPFN scales efficiently with data; ICL achieves exact computation from few examples. We release all code and data to support further investigation.</li>
</ul>

<h3>Title: The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI</h3>
<ul>
<li><strong>Authors: </strong>Pengyue Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02526">https://arxiv.org/abs/2602.02526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02526">https://arxiv.org/pdf/2602.02526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02526]] The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI(https://arxiv.org/abs/2602.02526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The stability of generative artificial intelligence trained on recursive synthetic data is conventionally monitored via Perplexity (PPL). We demonstrate that PPL is a deceptive metric in context-stabilized regimes (L=128). Using a rigorous sliding-window protocol (N=1500), we identify a novel failure mode termed "Semantic Tunneling." While the Baseline model maintains high grammatical fluency (PPL approx. 83.9), it suffers a catastrophic loss of semantic diversity, converging within seven generations to a single, low-entropy narrative attractor: the "Robert Boulton" Singularity. This phenomenon represents a total collapse of the latent manifold (Global Effective Rank 3.62 -> 2.22), where the model discards diverse world knowledge to optimize for statistically safe syntactic templates. To address this, we apply the Multi-Scale Negative Coupled Information Systems (MNCIS) framework recently established in Hou (2026) [arXiv:2601.11594]. We demonstrate that Adaptive Spectral Negative Coupling (ASNC) acts as a topological operator that actively induces "Manifold Unfolding." MNCIS forces the model to expand its effective rank from the anisotropic baseline of 3.62 to a hyper-diverse state of 5.35, effectively constructing an "Artificial Manifold" that resists the gravitational pull of semantic attractors and preserves the long-tail distribution of the training data.</li>
</ul>

<h3>Title: Auto-Augmentation Contrastive Learning for Wearable-based Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Qingyu Wu, Jianfei Shen, Feiyi Fan, Yang Gu, Chenyang Xu, Yiqiang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02542">https://arxiv.org/abs/2602.02542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02542">https://arxiv.org/pdf/2602.02542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02542]] Auto-Augmentation Contrastive Learning for Wearable-based Human Activity Recognition(https://arxiv.org/abs/2602.02542)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>For low-semantic sensor signals from human activity recognition (HAR), contrastive learning (CL) is essential to implement novel applications or generic models without manual annotation, which is a high-performance self-supervised learning (SSL) method. However, CL relies heavily on data augmentation for pairwise comparisons. Especially for low semantic data in the HAR area, conducting good performance augmentation strategies in pretext tasks still rely on manual attempts lacking generalizability and flexibility. To reduce the augmentation burden, we propose an end-to-end auto-augmentation contrastive learning (AutoCL) method for wearable-based HAR. AutoCL is based on a Siamese network architecture that shares the parameters of the backbone and with a generator embedded to learn auto-augmentation. AutoCL trains the generator based on the representation in the latent space to overcome the disturbances caused by noise and redundant information in raw sensor data. The architecture empirical study indicates the effectiveness of this design. Furthermore, we propose a stop-gradient design and correlation reduction strategy in AutoCL to enhance encoder representation learning. Extensive experiments based on four wide-used HAR datasets demonstrate that the proposed AutoCL method significantly improves recognition accuracy compared with other SOTA methods.</li>
</ul>

<h3>Title: SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Yongcheng Jing, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02544">https://arxiv.org/abs/2602.02544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02544">https://arxiv.org/pdf/2602.02544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02544]] SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models(https://arxiv.org/abs/2602.02544)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While Diffusion Language Models (DLMs) offer a flexible, arbitrary-order alternative to the autoregressive paradigm, their non-causal nature precludes standard KV caching, forcing costly hidden state recomputation at every decoding step. Existing DLM caching approaches reduce this cost by selective hidden state updates; however, they are still limited by (i) costly token-wise update identification heuristics and (ii) rigid, uniform budget allocation that fails to account for heterogeneous hidden state dynamics. To address these challenges, we present SPA-Cache that jointly optimizes update identification and budget allocation in DLM cache. First, we derive a low-dimensional singular proxy that enables the identification of update-critical tokens in a low-dimensional subspace, substantially reducing the overhead of update identification. Second, we introduce an adaptive strategy that allocates fewer updates to stable layers without degrading generation quality. Together, these contributions significantly improve the efficiency of DLMs, yielding up to an $8\times$ throughput improvement over vanilla decoding and a $2$--$4\times$ speedup over existing caching baselines.</li>
</ul>

<h3>Title: EEO-TFV: Escape-Explore Optimizer for Web-Scale Time-Series Forecasting and Vision Analysis</h3>
<ul>
<li><strong>Authors: </strong>Hua Wang, Jinghao Lu, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02551">https://arxiv.org/abs/2602.02551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02551">https://arxiv.org/pdf/2602.02551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02551]] EEO-TFV: Escape-Explore Optimizer for Web-Scale Time-Series Forecasting and Vision Analysis(https://arxiv.org/abs/2602.02551)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Transformer-based foundation models have achieved remarkable progress in tasks such as time-series forecasting and image segmentation. However, they frequently suffer from error accumulation in multivariate long-sequence prediction and exhibit vulnerability to out-of-distribution samples in image-related tasks. Furthermore, these challenges become particularly pronounced in large-scale Web data analysis tasks, which typically involve complex temporal patterns and multimodal features. This complexity substantially increases optimization difficulty, rendering models prone to stagnation at saddle points within high-dimensional parameter spaces. To address these issues, we propose a lightweight Transformer architecture in conjunction with a novel Escape-Explore Optimizer (EEO). The optimizer enhances both exploration and generalization while effectively avoiding sharp minima and saddle-point traps. Experimental results show that, in representative Web data scenarios, our method achieves performance on par with state-of-the-art models across 11 time-series benchmark datasets and the Synapse medical image segmentation task. Moreover, it demonstrates superior generalization and stability, thereby validating its potential as a versatile cross-task foundation model for Web-scale data mining and analysis.</li>
</ul>

<h3>Title: BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation</h3>
<ul>
<li><strong>Authors: </strong>Jingwen Xu, Yiyang Lu, Zisu Huang, Changze Lv, Xiaohua Wang, Shizheng Li, Zhibo Xu, Zhengkang Guo, Zhengyuan Wang, Muzhao Tian, Xuanjing Huang, Xiaoqing Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02554">https://arxiv.org/abs/2602.02554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02554">https://arxiv.org/pdf/2602.02554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02554]] BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation(https://arxiv.org/abs/2602.02554)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.</li>
</ul>

<h3>Title: Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions</h3>
<ul>
<li><strong>Authors: </strong>Bartlomiej Sobieski, Jakub Grzywaczewski, Karol Dobiczek, Mateusz Wójcik, Tomasz Bartczak, Patryk Szatkowski, Przemysław Bombiński, Matthew Tivnan, Przemyslaw Biecek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02560">https://arxiv.org/abs/2602.02560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02560">https://arxiv.org/pdf/2602.02560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02560]] Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions(https://arxiv.org/abs/2602.02560)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Lung cancer remains the leading cause of cancer mortality, driving the development of automated screening tools to alleviate radiologist workload. Standing at the frontier of this effort is Sybil, a deep learning model capable of predicting future risk solely from computed tomography (CT) with high precision. However, despite extensive clinical validation, current assessments rely purely on observational metrics. This correlation-based approach overlooks the model's actual reasoning mechanism, necessitating a shift to causal verification to ensure robust decision-making before clinical deployment. We propose S(H)NAP, a model-agnostic auditing framework that constructs generative interventional attributions validated by expert radiologists. By leveraging realistic 3D diffusion bridge modeling to systematically modify anatomical features, our approach isolates object-specific causal contributions to the risk score. Providing the first interventional audit of Sybil, we demonstrate that while the model often exhibits behavior akin to an expert radiologist, differentiating malignant pulmonary nodules from benign ones, it suffers from critical failure modes, including dangerous sensitivity to clinically unjustified artifacts and a distinct radial bias.</li>
</ul>

<h3>Title: Trajectory Consistency for One-Step Generation on Euler Mean Flows</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Li, Yuchen Sun, Duowen Chen, Jinjin He, Bo Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02571">https://arxiv.org/abs/2602.02571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02571">https://arxiv.org/pdf/2602.02571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02571]] Trajectory Consistency for One-Step Generation on Euler Mean Flows(https://arxiv.org/abs/2602.02571)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose \emph{Euler Mean Flows (EMF)}, a flow-based generative framework for one-step and few-step generation that enforces long-range trajectory consistency with minimal sampling cost. The key idea of EMF is to replace the trajectory consistency constraint, which is difficult to supervise and optimize over long time scales, with a principled linear surrogate that enables direct data supervision for long-horizon flow-map compositions. We derive this approximation from the semigroup formulation of flow-based models and show that, under mild regularity assumptions, it faithfully approximates the original consistency objective while being substantially easier to optimize. This formulation leads to a unified, JVP-free training framework that supports both $u$-prediction and $x_1$-prediction variants, avoiding explicit Jacobian computations and significantly reducing memory and computational overhead. Experiments on image synthesis, particle-based geometry generation, and functional generation demonstrate improved optimization stability and sample quality under fixed sampling budgets, together with approximately $50\%$ reductions in training time and memory consumption compared to existing one-step methods for image generation.</li>
</ul>

<h3>Title: Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eliron Rahimi, Elad Hirshel, Rom Himelstein, Amit LeVi, Avi Mendelson, Chaim Baskin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02600">https://arxiv.org/abs/2602.02600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02600">https://arxiv.org/pdf/2602.02600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02600]] Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models(https://arxiv.org/abs/2602.02600)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering parallel decoding and controllable sampling dynamics while achieving competitive generation quality at scale. Despite this progress, the role of sampling mechanisms in shaping refusal behavior and jailbreak robustness remains poorly understood. In this work, we present a fundamental analytical framework for step-wise refusal dynamics, enabling comparison between AR and diffusion sampling. Our analysis reveals that the sampling strategy itself plays a central role in safety behavior, as a factor distinct from the underlying learned representations. Motivated by this analysis, we introduce the Step-Wise Refusal Internal Dynamics (SRI) signal, which supports interpretability and improved safety for both AR and DLMs. We demonstrate that the geometric structure of SRI captures internal recovery dynamics, and identifies anomalous behavior in harmful generations as cases of \emph{incomplete internal recovery} that are not observable at the text level. This structure enables lightweight inference-time detectors that generalize to unseen attacks while matching or outperforming existing defenses with over $100\times$ lower inference overhead.</li>
</ul>

<h3>Title: TinyGuard:A lightweight Byzantine Defense for Resource-Constrained Federated Learning via Statistical Update Fingerprints</h3>
<ul>
<li><strong>Authors: </strong>Ali Mahdavi, Santa Aghapour, Azadeh Zamanifar, Amirfarhad Farhadi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02615">https://arxiv.org/abs/2602.02615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02615">https://arxiv.org/pdf/2602.02615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02615]] TinyGuard:A lightweight Byzantine Defense for Resource-Constrained Federated Learning via Statistical Update Fingerprints(https://arxiv.org/abs/2602.02615)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Existing Byzantine robust aggregation mechanisms typically rely on fulldimensional gradi ent comparisons or pairwise distance computations, resulting in computational overhead that limits applicability in large scale and resource constrained federated systems. This paper proposes TinyGuard, a lightweight Byzantine defense that augments the standard FedAvg algorithm via statistical update f ingerprinting. Instead of operating directly on high-dimensional gradients, TinyGuard extracts compact statistical fingerprints cap turing key behavioral properties of client updates, including norm statistics, layer-wise ratios, sparsity measures, and low-order mo ments. Byzantine clients are identified by measuring robust sta tistical deviations in this low-dimensional fingerprint space with nd complexity, without modifying the underlying optimization procedure. Extensive experiments on MNIST, Fashion-MNIST, ViT-Lite, and ViT-Small with LoRA adapters demonstrate that TinyGuard pre serves FedAvg convergence in benign settings and achieves up to 95 percent accuracy under multiple Byzantine attack scenarios, including sign-flipping, scaling, noise injection, and label poisoning. Against adaptive white-box adversaries, Pareto frontier analysis across four orders of magnitude confirms that attackers cannot simultaneously evade detection and achieve effective poisoning, features we term statistical handcuffs. Ablation studies validate stable detection precision 0.8 across varying client counts (50-150), threshold parameters and extreme data heterogeneity . The proposed framework is architecture-agnostic and well-suited for federated fine-tuning of foundation models where traditional Byzantine defenses become impractical</li>
</ul>

<h3>Title: Benchmarking Large Language Models for Zero-shot and Few-shot Phishing URL Detection</h3>
<ul>
<li><strong>Authors: </strong>Najmul Hasan, Prashanth BusiReddyGari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02641">https://arxiv.org/abs/2602.02641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02641">https://arxiv.org/pdf/2602.02641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02641]] Benchmarking Large Language Models for Zero-shot and Few-shot Phishing URL Detection(https://arxiv.org/abs/2602.02641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The Uniform Resource Locator (URL), introduced in a connectivity-first era to define access and locate resources, remains historically limited, lacking future-proof mechanisms for security, trust, or resilience against fraud and abuse, despite the introduction of reactive protections like HTTPS during the cybersecurity era. In the current AI-first threatscape, deceptive URLs have reached unprecedented sophistication due to the widespread use of generative AI by cybercriminals and the AI-vs-AI arms race to produce context-aware phishing websites and URLs that are virtually indistinguishable to both users and traditional detection tools. Although AI-generated phishing accounted for a small fraction of filter-bypassing attacks in 2024, phishing volume has escalated over 4,000% since 2022, with nearly 50% more attacks evading detection. At the rate the threatscape is escalating, and phishing tactics are emerging faster than labeled data can be produced, zero-shot and few-shot learning with large language models (LLMs) offers a timely and adaptable solution, enabling generalization with minimal supervision. Given the critical importance of phishing URL detection in large-scale cybersecurity defense systems, we present a comprehensive benchmark of LLMs under a unified zero-shot and few-shot prompting framework and reveal operational trade-offs. Our evaluation uses a balanced dataset with consistent prompts, offering detailed analysis of performance, generalization, and model efficacy, quantified by accuracy, precision, recall, F1 score, AUROC, and AUPRC, to reflect both classification quality and practical utility in threat detection settings. We conclude few-shot prompting improves performance across multiple LLMs.</li>
</ul>

<h3>Title: Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Marcos Villagra, Bidhan Roy, Raihan Seraj, Zhiying Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02685">https://arxiv.org/abs/2602.02685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02685">https://arxiv.org/pdf/2602.02685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02685]] Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models(https://arxiv.org/abs/2602.02685)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Decentralized Diffusion Models (DDMs) route denoising through experts trained independently on disjoint data clusters, which can strongly disagree in their predictions. What governs the quality of generations in such systems? We present the first ever systematic investigation of this question. A priori, the expectation is that minimizing denoising trajectory sensitivity -- minimizing how perturbations amplify during sampling -- should govern generation quality. We demonstrate this hypothesis is incorrect: a stability-quality dissociation. Full ensemble routing, which combines all expert predictions at each step, achieves the most stable sampling dynamics and best numerical convergence while producing the worst generation quality (FID 47.9 vs. 22.6 for sparse Top-2 routing). Instead, we identify expert-data alignment as the governing principle: generation quality depends on routing inputs to experts whose training distribution covers the current denoising state. Across two distinct DDM systems, we validate expert-data alignment using (i) data-cluster distance analysis, confirming sparse routing selects experts with data clusters closest to the current denoising state, and (ii) per-expert analysis, showing selected experts produce more accurate predictions than non-selected ones, and (iii) expert disagreement analysis, showing quality degrades when experts disagree. For DDM deployment, our findings establish that routing should prioritize expert-data alignment over numerical stability metrics.</li>
</ul>

<h3>Title: Sparsely Supervised Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Wenshuai Zhao, Zhiyuan Li, Yi Zhao, Mohammad Hassan Vali, Martin Trapp, Joni Pajarinen, Juho Kannala, Arno Solin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02699">https://arxiv.org/abs/2602.02699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02699">https://arxiv.org/pdf/2602.02699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02699]] Sparsely Supervised Diffusion(https://arxiv.org/abs/2602.02699)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown remarkable success across a wide range of generative tasks. However, they often suffer from spatially inconsistent generation, arguably due to the inherent locality of their denoising mechanisms. This can yield samples that are locally plausible but globally inconsistent. To mitigate this issue, we propose sparsely supervised learning for diffusion models, a simple yet effective masking strategy that can be implemented with only a few lines of code. Interestingly, the experiments show that it is safe to mask up to 98\% of pixels during diffusion model training. Our method delivers competitive FID scores across experiments and, most importantly, avoids training instability on small datasets. Moreover, the masking strategy reduces memorization and promotes the use of essential contextual information during generation.</li>
</ul>

<h3>Title: Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Dan Haramati, Carl Qi, Tal Daniel, Amy Zhang, Aviv Tamar, George Konidaris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02722">https://arxiv.org/abs/2602.02722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02722">https://arxiv.org/pdf/2602.02722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02722]] Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion(https://arxiv.org/abs/2602.02722)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: this https URL</li>
</ul>

<h3>Title: Search-Augmented Masked Diffusion Models for Constrained Generation</h3>
<ul>
<li><strong>Authors: </strong>Huu Binh Ta (1), Michael Cardei (1), Alvaro Velasquez (2), Ferdinando Fioretto (1) ((1) University of Virginia, (2) University of Colorado at Boulder)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02727">https://arxiv.org/abs/2602.02727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02727">https://arxiv.org/pdf/2602.02727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02727]] Search-Augmented Masked Diffusion Models for Constrained Generation(https://arxiv.org/abs/2602.02727)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models generate sequences by iteratively denoising samples corrupted by categorical noise, offering an appealing alternative to autoregressive decoding for structured and symbolic generation. However, standard training targets a likelihood-based objective that primarily matches the data distribution and provides no native mechanism for enforcing hard constraints or optimizing non-differentiable properties at inference time. This work addresses this limitation and introduces Search-Augmented Masked Diffusion (SearchDiff), a training-free neurosymbolic inference framework that integrates informed search directly into the reverse denoising process. At each denoising step, the model predictions define a proposal set that is optimized under a user-specified property satisfaction, yielding a modified reverse transition that steers sampling toward probable and feasible solutions. Experiments in biological design and symbolic reasoning illustrate that SearchDiff substantially improves constraint satisfaction and property adherence, while consistently outperforming discrete diffusion and autoregressive baselines.</li>
</ul>

<h3>Title: TabPFN for Zero-shot Parametric Engineering Design Generation</h3>
<ul>
<li><strong>Authors: </strong>Ke Wang, Yifan Tang, Nguyen Gia Hien Vu, Faez Ahmed, G. Gary Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02735">https://arxiv.org/abs/2602.02735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02735">https://arxiv.org/pdf/2602.02735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02735]] TabPFN for Zero-shot Parametric Engineering Design Generation(https://arxiv.org/abs/2602.02735)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models for engineering design often require substantial computational cost, large training datasets, and extensive retraining when design requirements or datasets change, limiting their applicability in real-world engineering design workflow. In this work, we propose a zero-shot generation framework for parametric engineering design based on TabPFN, enabling conditional design generation using only a limited number of reference samples and without any task-specific model training or fine-tuning. The proposed method generates design parameters sequentially conditioned on target performance indicators, providing a flexible alternative to conventional generative models. The effectiveness of the proposed approach is evaluated on three engineering design datasets, i.e., ship hull design, BlendedNet aircraft, and UIUC airfoil. Experimental results demonstrate that the proposed method achieves competitive diversity across highly structured parametric design spaces, remains robust to variations in sampling, resolution and parameter dimensionality of geometry generation, and achieves a low performance error (e.g., less than 2% in generated ship hull designs' performance). Compared with diffusion-based generative models, the proposed framework significantly reduces computational overhead and data requirements while preserving reliable generation performance. These results highlight the potential of zero-shot, data-efficient generation as a practical and efficient tool for engineering design, enabling rapid deployment, flexible adaptation to new design settings, and ease of integration into real-world engineering workflows.</li>
</ul>

<h3>Title: SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?</h3>
<ul>
<li><strong>Authors: </strong>Haruhiko Murata, Kazuhiro Hotta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02765">https://arxiv.org/abs/2602.02765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02765">https://arxiv.org/pdf/2602.02765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02765]] SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?(https://arxiv.org/abs/2602.02765)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.</li>
</ul>

<h3>Title: Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Lucas Rosenblatt, Peihan Liu, Ryan McKenna, Natalia Ponomareva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02766">https://arxiv.org/abs/2602.02766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02766">https://arxiv.org/pdf/2602.02766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02766]] Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data(https://arxiv.org/abs/2602.02766)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Research on differentially private synthetic tabular data has largely focused on independent and identically distributed rows where each record corresponds to a unique individual. This perspective neglects the temporal complexity in longitudinal datasets, such as electronic health records, where a user contributes an entire (sub) table of sequential events. While practitioners might attempt to model such data by flattening user histories into high-dimensional vectors for use with standard marginal-based mechanisms, we demonstrate that this strategy is insufficient. Flattening fails to preserve temporal coherence even when it maintains valid marginal distributions. We introduce PATH, a novel generative framework that treats the full table as the unit of synthesis and leverages the autoregressive capabilities of privately fine-tuned large language models. Extensive evaluations show that PATH effectively captures long-range dependencies that traditional methods miss. Empirically, our method reduces the distributional distance to real trajectories by over 60% and reduces state transition errors by nearly 50% compared to leading marginal mechanisms while achieving similar marginal fidelity.</li>
</ul>

<h3>Title: BiTimeCrossNet: Time-Aware Self-Supervised Learning for Pediatric Sleep</h3>
<ul>
<li><strong>Authors: </strong>Saurav Raj Pandey, Harlin Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02769">https://arxiv.org/abs/2602.02769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02769">https://arxiv.org/pdf/2602.02769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02769]] BiTimeCrossNet: Time-Aware Self-Supervised Learning for Pediatric Sleep(https://arxiv.org/abs/2602.02769)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We present BiTimeCrossNet (BTCNet), a multimodal self-supervised learning framework for long physiological recordings such as overnight sleep studies. While many existing approaches train on short segments treated as independent samples, BTCNet incorporates information about when each segment occurs within its parent recording, for example within a sleep session. BTCNet further learns pairwise interactions between physiological signals via cross-attention, without requiring task labels or sequence-level supervision. We evaluate BTCNet on pediatric sleep data across six downstream tasks, including sleep staging, arousal detection, and respiratory event detection. Under frozen-backbone linear probing, BTCNet consistently outperforms an otherwise identical non-time-aware variant, with gains that generalize to an independent pediatric dataset. Compared to existing multimodal self-supervised sleep models, BTCNet achieves strong performance, particularly on respiration-related tasks.</li>
</ul>

<h3>Title: Cross-Temporal Attention Fusion (CTAF) for Multimodal Physiological Signals in Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Arian Khorasani, Théophile Demazure</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02784">https://arxiv.org/abs/2602.02784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02784">https://arxiv.org/pdf/2602.02784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02784]] Cross-Temporal Attention Fusion (CTAF) for Multimodal Physiological Signals in Self-Supervised Learning(https://arxiv.org/abs/2602.02784)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We study multimodal affect modeling when EEG and peripheral physiology are asynchronous, which most fusion methods ignore or handle with costly warping. We propose Cross-Temporal Attention Fusion (CTAF), a self-supervised module that learns soft bidirectional alignments between modalities and builds a robust clip embedding using time-aware cross attention, a lightweight fusion gate, and alignment-regularized contrastive objectives with optional weak supervision. On the K-EmoCon dataset, under leave-one-out cross-validation evaluation, CTAF yields higher cosine margins for matched pairs and better cross-modal token retrieval within one second, and it is competitive with the baseline on three-bin accuracy and macro-F1 while using few labels. Our contributions are a time-aware fusion mechanism that directly models correspondence, an alignment-driven self-supervised objective tailored to EEG and physiology, and an evaluation protocol that measures alignment quality itself. Our approach accounts for the coupling between the central and autonomic nervous systems in psychophysiological time series. These results indicate that CTAF is a strong step toward label-efficient, generalizable EEG-peripheral fusion under temporal asynchrony.</li>
</ul>

<h3>Title: Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs</h3>
<ul>
<li><strong>Authors: </strong>Benjamin D. Shaffer, Shawn Koohy, Brooks Kinch, M. Ani Hsieh, Nathaniel Trask</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02788">https://arxiv.org/abs/2602.02788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02788">https://arxiv.org/pdf/2602.02788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02788]] Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs(https://arxiv.org/abs/2602.02788)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We aim to develop physics foundation models for science and engineering that provide real-time solutions to Partial Differential Equations (PDEs) which preserve structure and accuracy under adaptation to unseen geometries. To this end, we introduce General-Geometry Neural Whitney Forms (Geo-NeW): a data-driven finite element method. We jointly learn a differential operator and compatible reduced finite element spaces defined on the underlying geometry. The resulting model is solved to generate predictions, while exactly preserving physical conservation laws through Finite Element Exterior Calculus. Geometry enters the model as a discretized mesh both through a transformer-based encoding and as the basis for the learned finite element spaces. This explicitly connects the underlying geometry and imposed boundary conditions to the solution, providing a powerful inductive bias for learning neural PDEs, which we demonstrate improves generalization to unseen domains. We provide a novel parameterization of the constitutive model ensuring the existence and uniqueness of the solution. Our approach demonstrates state-of-the-art performance on several steady-state PDE benchmarks, and provides a significant improvement over conventional baselines on out-of-distribution geometries.</li>
</ul>

<h3>Title: Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains</h3>
<ul>
<li><strong>Authors: </strong>Jae-Sung Bae, Minje Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02841">https://arxiv.org/abs/2602.02841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02841">https://arxiv.org/pdf/2602.02841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02841]] Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains(https://arxiv.org/abs/2602.02841)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>Despite strong performance in data-rich regimes, deep learning often underperforms in the data-scarce settings common in practice. While foundation models (FMs) trained on massive datasets demonstrate strong generalization by extracting general-purpose features, they can still suffer from scarce labeled data during downstream fine-tuning. To address this, we propose GeLDA, a semantics-aware generative latent data augmentation framework that leverages conditional diffusion models to synthesize samples in an FM-induced latent space. Because this space is low-dimensional and concentrates task-relevant information compared to the input space, GeLDA enables efficient, high-quality data generation. GeLDA conditions generation on auxiliary feature vectors that capture semantic relationships among classes or subdomains, facilitating data augmentation in low-resource domains. We validate GeLDA in two large-scale recognition tasks: (a) in zero-shot language-specific speech emotion recognition, GeLDA improves the Whisper-large baseline's unweighted average recall by 6.13%; and (b) in long-tailed image classification, it achieves 74.7% tail-class accuracy on ImageNet-LT, setting a new state-of-the-art result.</li>
</ul>

<h3>Title: Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room</h3>
<ul>
<li><strong>Authors: </strong>Keqi Chen, Vinkle Srivastav, Armine Vardazaryan, Cindy Rolland, Didier Mutter, Nicolas Padoy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02850">https://arxiv.org/abs/2602.02850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02850">https://arxiv.org/pdf/2602.02850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02850]] Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room(https://arxiv.org/abs/2602.02850)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Privacy preservation is a prerequisite for using video data in Operating Room (OR) research. Effective anonymization relies on the exhaustive localization of every individual; even a single missed detection necessitates extensive manual correction. However, existing approaches face two critical scalability bottlenecks: (1) they usually require manual annotations of each new clinical site for high accuracy; (2) while multi-camera setups have been widely adopted to address single-view ambiguity, camera calibration is typically required whenever cameras are repositioned. To address these problems, we propose a novel self-supervised multi-view video anonymization framework consisting of whole-body person detection and whole-body pose estimation, without annotation or camera calibration. Our core strategy is to enhance the single-view detector by "retrieving" false negatives using temporal and multi-view context, and conducting self-supervised domain adaptation. We first run an off-the-shelf whole-body person detector in each view with a low-score threshold to gather candidate detections. Then, we retrieve the low-score false negatives that exhibit consistency with the high-score detections via tracking and self-supervised uncalibrated multi-view association. These recovered detections serve as pseudo labels to iteratively fine-tune the whole-body detector. Finally, we apply whole-body pose estimation on each detected person, and fine-tune the pose model using its own high-score predictions. Experiments on the 4D-OR dataset of simulated surgeries and our dataset of real surgeries show the effectiveness of our approach achieving over 97% recall. Moreover, we train a real-time whole-body detector using our pseudo labels, achieving comparable performance and highlighting our method's practical applicability. Code is available at this https URL.</li>
</ul>

<h3>Title: ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying</h3>
<ul>
<li><strong>Authors: </strong>Weihang You, Qingchan Zhu, David Liu, Yi Pan, Geng Yuan, Hanqi Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02873">https://arxiv.org/abs/2602.02873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02873">https://arxiv.org/pdf/2602.02873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02873]] ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying(https://arxiv.org/abs/2602.02873)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.</li>
</ul>

<h3>Title: Self-Soupervision: Cooking Model Soups without Labels</h3>
<ul>
<li><strong>Authors: </strong>Anthony Fuller, James R. Green, Evan Shelhamer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02890">https://arxiv.org/abs/2602.02890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02890">https://arxiv.org/pdf/2602.02890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02890]] Self-Soupervision: Cooking Model Soups without Labels(https://arxiv.org/abs/2602.02890)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Model soups are strange and strangely effective combinations of parameters. They take a model (the stock), fine-tune it into multiple models (the ingredients), and then mix their parameters back into one model (the soup) to improve predictions. While all known soups require supervised learning, and optimize the same loss on labeled data, our recipes for Self-\emph{Soup}ervision generalize soups to self-supervised learning (SSL). Our Self-Souping lets us flavor ingredients on new data sources, e.g. from unlabeled data from a task for transfer or from a shift for robustness. We show that Self-Souping on corrupted test data, then fine-tuning back on uncorrupted train data, boosts robustness by +3.5\% (ImageNet-C) and +7\% (LAION-C). Self-\emph{Soup}ervision also unlocks countless SSL algorithms to cook the diverse ingredients needed for more robust soups. We show for the first time that ingredients can differ in their SSL hyperparameters -- and more surprisingly, in their SSL algorithms. We cook soups of MAE, MoCoV3, and MMCR ingredients that are more accurate than any one single SSL ingredient.</li>
</ul>

<h3>Title: Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Fang, Zuyuan Zhang, Mahdi Imani, Tian Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02900">https://arxiv.org/abs/2602.02900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02900">https://arxiv.org/pdf/2602.02900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02900]] Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning(https://arxiv.org/abs/2602.02900)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Model-based offline reinforcement learning is brittle under distribution shift: policy improvement drives rollouts into state--action regions weakly supported by the dataset, where compounding model error yields severe value overestimation. We propose Manifold-Constrained Energy-based Transition Models (MC-ETM), which train conditional energy-based transition models using a manifold projection--diffusion negative sampler. MC-ETM learns a latent manifold of next states and generates near-manifold hard negatives by perturbing latent codes and running Langevin dynamics in latent space with the learned conditional energy, sharpening the energy landscape around the dataset support and improving sensitivity to subtle out-of-distribution deviations. For policy optimization, the learned energy provides a single reliability signal: rollouts are truncated when the minimum energy over sampled next states exceeds a threshold, and Bellman backups are stabilized via pessimistic penalties based on Q-value-level dispersion across energy-guided samples. We formalize MC-ETM through a hybrid pessimistic MDP formulation and derive a conservative performance bound separating in-support evaluation error from truncation risk. Empirically, MC-ETM improves multi-step dynamics fidelity and yields higher normalized returns on standard offline control benchmarks, particularly under irregular dynamics and sparse data coverage.</li>
</ul>

<h3>Title: A Random Matrix Theory Perspective on the Consistency of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Binxu Wang, Jacob Zavatone-Veth, Cengiz Pehlevan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02908">https://arxiv.org/abs/2602.02908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02908">https://arxiv.org/pdf/2602.02908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02908]] A Random Matrix Theory Perspective on the Consistency of Diffusion Models(https://arxiv.org/abs/2602.02908)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models trained on different, non-overlapping subsets of a dataset often produce strikingly similar outputs when given the same noise seed. We trace this consistency to a simple linear effect: the shared Gaussian statistics across splits already predict much of the generated images. To formalize this, we develop a random matrix theory (RMT) framework that quantifies how finite datasets shape the expectation and variance of the learned denoiser and sampling map in the linear setting. For expectations, sampling variability acts as a renormalization of the noise level through a self-consistent relation $\sigma^2 \mapsto \kappa(\sigma^2)$, explaining why limited data overshrink low-variance directions and pull samples toward the dataset mean. For fluctuations, our variance formulas reveal three key factors behind cross-split disagreement: \textit{anisotropy} across eigenmodes, \textit{inhomogeneity} across inputs, and overall scaling with dataset size. Extending deterministic-equivalence tools to fractional matrix powers further allows us to analyze entire sampling trajectories. The theory sharply predicts the behavior of linear diffusion models, and we validate its predictions on UNet and DiT architectures in their non-memorization regime, identifying where and how samples deviates across training data split. This provides a principled baseline for reproducibility in diffusion training, linking spectral properties of data to the stability of generative outputs.</li>
</ul>

<h3>Title: Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels</h3>
<ul>
<li><strong>Authors: </strong>Yunsung Chung, Keum San Chun, Migyeong Gwak, Han Feng, Yingshuo Liu, Chanho Lim, Viswam Nathan, Nassir Marrouche, Sharanya Arcot Desai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02917">https://arxiv.org/abs/2602.02917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02917">https://arxiv.org/pdf/2602.02917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02917]] Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels(https://arxiv.org/abs/2602.02917)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Advances in wearable computing and AI have increased interest in leveraging PPG for health monitoring over the past decade. One of the biggest challenges in developing health algorithms based on such biosignals is the sparsity of clinical labels, which makes biosignals temporally distant from lab draws less reliable for supervision. To address this problem, we introduce a simple training strategy that learns a biomarker-specific decay of sample weight over the time gap between a segment and its ground truth label and uses this weight in the loss with a regularizer to prevent trivial solutions. On smartwatch PPG from 450 participants across 10 biomarkers, the approach improves over baselines. In the subject-wise setting, the proposed approach averages 0.715 AUPRC, compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a feature-based Random Forest. A comparison of four decay families shows that a simple linear decay function is most robust on average. Beyond accuracy, the learned decay rates summarize how quickly each biomarker's PPG evidence becomes stale, providing an interpretable view of temporal sensitivity.</li>
</ul>

<h3>Title: How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyuan Cheng, Wenxuan Yuan, Boyang Li, Yuanchao Xu, Yiming Yang, Hao Liang, Bei Peng, Robert Loftin, Zhuo Sun, Yukun Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02924">https://arxiv.org/abs/2602.02924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02924">https://arxiv.org/pdf/2602.02924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02924]] How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?(https://arxiv.org/abs/2602.02924)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.</li>
</ul>

<h3>Title: Refining Decision Boundaries In Anomaly Detection Using Similarity Search Within the Feature Space</h3>
<ul>
<li><strong>Authors: </strong>Sidahmed Benabderrahmane, Petko Valtchev, James Cheney, Talal Rahwan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02925">https://arxiv.org/abs/2602.02925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02925">https://arxiv.org/pdf/2602.02925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02925]] Refining Decision Boundaries In Anomaly Detection Using Similarity Search Within the Feature Space(https://arxiv.org/abs/2602.02925)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Detecting rare and diverse anomalies in highly imbalanced datasets-such as Advanced Persistent Threats (APTs) in cybersecurity-remains a fundamental challenge for machine learning systems. Active learning offers a promising direction by strategically querying an oracle to minimize labeling effort, yet conventional approaches often fail to exploit the intrinsic geometric structure of the feature space for model refinement. In this paper, we introduce SDA2E, a Sparse Dual Adversarial Attention-based AutoEncoder designed to learn compact and discriminative latent representations from imbalanced, high-dimensional data. We further propose a similarity-guided active learning framework that integrates three novel strategies to refine decision boundaries efficiently: mormal-like expansion, which enriches the training set with points similar to labeled normals to improve reconstruction fidelity; anomaly-like prioritization, which boosts ranking accuracy by focusing on points resembling known anomalies; and a hybrid strategy that combines both for balanced model refinement and ranking. A key component of our framework is a new similarity measure, Normalized Matching 1s (SIM_NM1), tailored for sparse binary embeddings. We evaluate SDA2E extensively across 52 imbalanced datasets, including multiple DARPA Transparent Computing scenarios, and benchmark it against 15 state-of-the-art anomaly detection methods. Results demonstrate that SDA2E consistently achieves superior ranking performance (nDCG up to 1.0 in several cases) while reducing the required labeled data by up to 80% compared to passive training. Statistical tests confirm the significance of these improvements. Our work establishes a robust, efficient, and statistically validated framework for anomaly detection that is particularly suited to cybersecurity applications such as APT detection.</li>
</ul>

<h3>Title: Distance Marching for Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zimo Wang, Ishit Mehta, Haolin Lu, Chung-En Sun, Ge Yan, Tsui-Wei Weng, Tzu-Mao Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02928">https://arxiv.org/abs/2602.02928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02928">https://arxiv.org/pdf/2602.02928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02928]] Distance Marching for Generative Modeling(https://arxiv.org/abs/2602.02928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Time-unconditional generative models learn time-independent denoising vector fields. But without time conditioning, the same noisy input may correspond to multiple noise levels and different denoising directions, which interferes with the supervision signal. Inspired by distance field modeling, we propose Distance Marching, a new time-unconditional approach with two principled inference methods. Crucially, we design losses that focus on closer targets. This yields denoising directions better directed toward the data manifold. Across architectures, Distance Marching consistently improves FID by 13.5% on CIFAR-10 and ImageNet over recent time-unconditional baselines. For class-conditional ImageNet generation, despite removing time input, Distance Marching surpasses flow matching using our losses and inference methods. It achieves lower FID than flow matching's final performance using 60% of the sampling steps and 13.6% lower FID on average across backbone sizes. Moreover, our distance prediction is also helpful for early stopping during sampling and for OOD detection. We hope distance field modeling can serve as a principled lens for generative modeling.</li>
</ul>

<h3>Title: RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Asif Tauhid, Sidahmed Benabderrahmane, Mohamad Altrabulsi, Ahamed Foisal, Talal Rahwan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02929">https://arxiv.org/abs/2602.02929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02929">https://arxiv.org/pdf/2602.02929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02929]] RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection(https://arxiv.org/abs/2602.02929)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Advanced Persistent Threats (APTs) are sophisticated, long-term cyberattacks that are difficult to detect because they operate stealthily and often blend into normal system behavior. This paper presents a neuro-symbolic anomaly detection framework that combines a Graph Autoencoder (GAE) with rare pattern mining to identify APT-like activities in system-level provenance data. Our approach first constructs a process behavioral graph using k-Nearest Neighbors based on feature similarity, then learns normal relational structure using a Graph Autoencoder. Anomaly candidates are identified through deviations between observed and reconstructed graph structure. To further improve detection, we integrate an rare pattern mining module that discovers infrequent behavioral co-occurrences and uses them to boost anomaly scores for processes exhibiting rare signatures. We evaluate the proposed method on the DARPA Transparent Computing datasets and show that rare-pattern boosting yields substantial gains in anomaly ranking quality over the baseline GAE. Compared with existing unsupervised approaches on the same benchmark, our single unified model consistently outperforms individual context-based detectors and achieves performance competitive with ensemble aggregation methods that require multiple separate detectors. These results highlight the value of coupling graph-based representation learning with classical pattern mining to improve both effectiveness and interpretability in provenance-based security anomaly detection.</li>
</ul>

<h3>Title: 3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wen, Lei Fan, Jianyi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02943">https://arxiv.org/abs/2602.02943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02943">https://arxiv.org/pdf/2602.02943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02943]] 3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning(https://arxiv.org/abs/2602.02943)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Predict-then-Optimize (PTO) pipelines are widely employed in computing and networked systems, where Machine Learning (ML) models are used to predict critical contextual information for downstream decision-making tasks such as cloud LLM serving, data center demand response, and edge workload scheduling. However, these ML predictors are often vulnerable to out-of-distribution (OOD) samples at test time, leading to significant decision performance degradation due to large prediction errors. To address the generalization challenges under OOD conditions, we present the framework of Distributionally Robust Decision-Focused Learning (DR-DFL), which trains ML models to optimize decision performance under the worst-case distribution. Instead of relying on classical Distributionally Robust Optimization (DRO) techniques, we propose Diffusion-Augmented Distributionally Robust Decision-Focused Learning (3D-Learning), which searches for the worst-case distribution within the parameterized space of a diffusion model. By leveraging the powerful distribution modeling capabilities of diffusion models, 3D-Learning identifies worst-case distributions that remain consistent with real data, achieving a favorable balance between average and worst-case scenarios. Empirical results on an LLM resource provisioning task demonstrate that 3D-Learning outperforms existing DRO and Data Augmentation methods in OOD generalization performance.</li>
</ul>

<h3>Title: Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization</h3>
<ul>
<li><strong>Authors: </strong>Haocheng Xi, Shuo Yang, Yilong Zhao, Muyang Li, Han Cai, Xingyang Li, Yujun Lin, Zhuoyang Zhang, Jintao Zhang, Xiuyu Li, Zhiying Xu, Jun Wu, Chenfeng Xu, Ion Stoica, Song Han, Kurt Keutzer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02958">https://arxiv.org/abs/2602.02958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02958">https://arxiv.org/pdf/2602.02958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02958]] Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization(https://arxiv.org/abs/2602.02958)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.</li>
</ul>

<h3>Title: From Zero to Hero: Advancing Zero-Shot Foundation Models for Tabular Outlier Detection</h3>
<ul>
<li><strong>Authors: </strong>Xueying Ding, Haomin Wen, Simon Klütterman, Leman Akoglu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03018">https://arxiv.org/abs/2602.03018</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03018">https://arxiv.org/pdf/2602.03018</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03018]] From Zero to Hero: Advancing Zero-Shot Foundation Models for Tabular Outlier Detection(https://arxiv.org/abs/2602.03018)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Outlier detection (OD) is widely used in practice; but its effective deployment on new tasks is hindered by lack of labeled outliers, which makes algorithm and hyperparameter selection notoriously hard. Foundation models (FMs) have transformed ML, and OD is no exception: Shen et. al. (2025) introduced FoMo-0D, the first FM for OD, achieving remarkable performance against numerous baselines. This work introduces OUTFORMER, which advances FoMo-0D with (1) a mixture of synthetic priors and (2) self-evolving curriculum training. OUTFORMER is pretrained solely on synthetic labeled datasets and infers test labels of a new task by using its training data as in-context input. Inference is fast and zero-shot, requiring merely forward pass and no labeled outliers. Thanks to in-context learning, it requires zero additional work-no OD model training or bespoke model selection-enabling truly plug-and-play deployment. OUTFORMER achieves state-of-the-art performance on the prominent AdBench, as well as two new large-scale OD benchmarks that we introduce, comprising over 1,500 datasets, while maintaining speedy inference.</li>
</ul>

<h3>Title: HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency</h3>
<ul>
<li><strong>Authors: </strong>Geonhui Son, Jeong Ryong Lee, Dosik Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03039">https://arxiv.org/abs/2602.03039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03039">https://arxiv.org/pdf/2602.03039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03039]] HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency(https://arxiv.org/abs/2602.03039)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) have made significant progress in enhancing the quality of image synthesis. Recent methods frequently leverage pretrained networks to calculate perceptual losses or utilize pretrained feature spaces. In this paper, we extend the capabilities of pretrained networks by incorporating innovative self-supervised learning techniques and enforcing consistency between discriminators during GAN training. Our proposed method, named HP-GAN, effectively exploits neural network priors through two primary strategies: FakeTwins and discriminator consistency. FakeTwins leverages pretrained networks as encoders to compute a self-supervised loss and applies this through the generated images to train the generator, thereby enabling the generation of more diverse and high quality images. Additionally, we introduce a consistency mechanism between discriminators that evaluate feature maps extracted from Convolutional Neural Network (CNN) and Vision Transformer (ViT) feature networks. Discriminator consistency promotes coherent learning among discriminators and enhances training robustness by aligning their assessments of image quality. Our extensive evaluation across seventeen datasets-including scenarios with large, small, and limited data, and covering a variety of image domains-demonstrates that HP-GAN consistently outperforms current state-of-the-art methods in terms of Fréchet Inception Distance (FID), achieving significant improvements in image diversity and quality. Code is available at: this https URL.</li>
</ul>

<h3>Title: A generalizable large-scale foundation model for musculoskeletal radiographs</h3>
<ul>
<li><strong>Authors: </strong>Shinn Kim, Soobin Lee, Kyoungseob Shin, Han-Soo Kim, Yongsung Kim, Minsu Kim, Juhong Nam, Somang Ko, Daeheon Kwon, Wook Huh, Ilkyu Han, Sunghoon Kwon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03076">https://arxiv.org/abs/2602.03076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03076">https://arxiv.org/pdf/2602.03076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03076]] A generalizable large-scale foundation model for musculoskeletal radiographs(https://arxiv.org/abs/2602.03076)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) has shown promise in detecting and characterizing musculoskeletal diseases from radiographs. However, most existing models remain task-specific, annotation-dependent, and limited in generalizability across diseases and anatomical regions. Although a generalizable foundation model trained on large-scale musculoskeletal radiographs is clinically needed, publicly available datasets remain limited in size and lack sufficient diversity to enable training across a wide range of musculoskeletal conditions and anatomical sites. Here, we present SKELEX, a large-scale foundation model for musculoskeletal radiographs, trained using self-supervised learning on 1.2 million diverse, condition-rich images. The model was evaluated on 12 downstream diagnostic tasks and generally outperformed baselines in fracture detection, osteoarthritis grading, and bone tumor classification. Furthermore, SKELEX demonstrated zero-shot abnormality localization, producing error maps that identified pathologic regions without task-specific training. Building on this capability, we developed an interpretable, region-guided model for predicting bone tumors, which maintained robust performance on independent external datasets and was deployed as a publicly accessible web application. Overall, SKELEX provides a scalable, label-efficient, and generalizable AI framework for musculoskeletal imaging, establishing a foundation for both clinical translation and data-efficient research in musculoskeletal radiology.</li>
</ul>

<h3>Title: Geometry-Preserving Neural Architectures on Manifolds with Boundary</h3>
<ul>
<li><strong>Authors: </strong>Karthik Elamvazhuthi, Shiba Biswal, Kian Rosenblum, Arushi Katyal, Tianli Qu, Grady Ma, Rishi Sonthalia</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03082">https://arxiv.org/abs/2602.03082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03082">https://arxiv.org/pdf/2602.03082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03082]] Geometry-Preserving Neural Architectures on Manifolds with Boundary(https://arxiv.org/abs/2602.03082)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Preserving geometric structure is important in learning. We propose a unified class of geometry-aware architectures that interleave geometric updates between layers, where both projection layers and intrinsic exponential map updates arise as discretizations of projected dynamical systems on manifolds (with or without boundary). Within this framework, we establish universal approximation results for constrained neural ODEs. We also analyze architectures that enforce geometry only at the output, proving a separate universal approximation property that enables direct comparison to interleaved designs. When the constraint set is unknown, we learn projections via small-time heat-kernel limits, showing diffusion/flow-matching can be used as data-based projections. Experiments on dynamics over S^2 and SO(3), and diffusion on S^{d-1}-valued features demonstrate exact feasibility for analytic updates and strong performance for learned projections</li>
</ul>

<h3>Title: Gromov Wasserstein Optimal Transport for Semantic Correspondences</h3>
<ul>
<li><strong>Authors: </strong>Francis Snelgar, Stephen Gould, Ming Xu, Liang Zheng, Akshay Asthana</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03105">https://arxiv.org/abs/2602.03105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03105">https://arxiv.org/pdf/2602.03105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03105]] Gromov Wasserstein Optimal Transport for Semantic Correspondences(https://arxiv.org/abs/2602.03105)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Establishing correspondences between image pairs is a long studied problem in computer vision. With recent large-scale foundation models showing strong zero-shot performance on downstream tasks including classification and segmentation, there has been interest in using the internal feature maps of these models for the semantic correspondence task. Recent works observe that features from DINOv2 and Stable Diffusion (SD) are complementary, the former producing accurate but sparse correspondences, while the latter produces spatially consistent correspondences. As a result, current state-of-the-art methods for semantic correspondence involve combining features from both models in an ensemble. While the performance of these methods is impressive, they are computationally expensive, requiring evaluating feature maps from large-scale foundation models. In this work we take a different approach, instead replacing SD features with a superior matching algorithm which is imbued with the desirable spatial consistency property. Specifically, we replace the standard nearest neighbours matching with an optimal transport algorithm that includes a Gromov Wasserstein spatial smoothness prior. We show that we can significantly boost the performance of the DINOv2 baseline, and be competitive and sometimes surpassing state-of-the-art methods using Stable Diffusion features, while being 5--10x more efficient. We make code available at this https URL .</li>
</ul>

<h3>Title: Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Judah Goldfeder, Shreyes Kaliyur, Vaibhav Sourirajan, Patrick Minwan Puma, Philippe Martin Wyder, Yuhang Hu, Jiong Lin, Hod Lipson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03123">https://arxiv.org/abs/2602.03123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03123">https://arxiv.org/pdf/2602.03123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03123]] Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models(https://arxiv.org/abs/2602.03123)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations. Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism. However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task. In this work, we present EvoAug, an automated augmentation learning pipeline, which leverages these generative models alongside an efficient evolutionary algorithm to learn optimal task-specific augmentations. Our pipeline introduces a novel approach to image augmentation that learns stochastic augmentation trees that hierarchically compose augmentations, enabling more structured and adaptive transformations. We demonstrate strong performance across fine-grained classification and few-shot learning tasks. Notably, our pipeline discovers augmentations that align with domain knowledge, even in low-data settings. These results highlight the potential of learned generative augmentations, unlocking new possibilities for robust model training.</li>
</ul>

<h3>Title: Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Francis Snelgar, Ming Xu, Stephen Gould, Liang Zheng, Akshay Asthana</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03126">https://arxiv.org/abs/2602.03126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03126">https://arxiv.org/pdf/2602.03126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03126]] Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models(https://arxiv.org/abs/2602.03126)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D human pose estimation from 2D images is a challenging problem due to depth ambiguity and occlusion. Because of these challenges the task is underdetermined, where there exists multiple -- possibly infinite -- poses that are plausible given the image. Despite this, many prior works assume the existence of a deterministic mapping and estimate a single pose given an image. Furthermore, methods based on machine learning require a large amount of paired 2D-3D data to train and suffer from generalization issues to unseen scenarios. To address both of these issues, we propose a framework for pose estimation using diffusion models, which enables sampling from a probability distribution over plausible poses which are consistent with a 2D image. Our approach falls under the guidance framework for conditional generation, and guides samples from an unconditional diffusion model, trained only on 3D data, using the gradients of the heatmaps from a 2D keypoint detector. We evaluate our method on the Human 3.6M dataset under best-of-$m$ multiple hypothesis evaluation, showing state-of-the-art performance among methods which do not require paired 2D-3D data for training. We additionally evaluate the generalization ability using the MPI-INF-3DHP and 3DPW datasets and demonstrate competitive performance. Finally, we demonstrate the flexibility of our framework by using it for novel tasks including pose generation and pose completion, without the need to train bespoke conditional models. We make code available at this https URL .</li>
</ul>

<h3>Title: FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chen-Bin Feng, Youyang Sha, Longfei Liu, Yongjun Yu, Chi Man Vong, Xuanlong Yu, Xi Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03137">https://arxiv.org/abs/2602.03137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03137">https://arxiv.org/pdf/2602.03137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03137]] FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion(https://arxiv.org/abs/2602.03137)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: this https URL.</li>
</ul>

<h3>Title: Human-in-the-loop Adaptation in Group Activity Feature Learning for Team Sports Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Chihiro Nakatani, Hiroaki Kawashima, Norimichi Ukita</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03157">https://arxiv.org/abs/2602.03157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03157">https://arxiv.org/pdf/2602.03157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03157]] Human-in-the-loop Adaptation in Group Activity Feature Learning for Team Sports Video Retrieval(https://arxiv.org/abs/2602.03157)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper proposes human-in-the-loop adaptation for Group Activity Feature Learning (GAFL) without group activity annotations. This human-in-the-loop adaptation is employed in a group-activity video retrieval framework to improve its retrieval performance. Our method initially pre-trains the GAF space based on the similarity of group activities in a self-supervised manner, unlike prior work that classifies videos into pre-defined group activity classes in a supervised learning manner. Our interactive fine-tuning process updates the GAF space to allow a user to better retrieve videos similar to query videos given by the user. In this fine-tuning, our proposed data-efficient video selection process provides several videos, which are selected from a video database, to the user in order to manually label these videos as positive or negative. These labeled videos are used to update (i.e., fine-tune) the GAF space, so that the positive and negative videos move closer to and farther away from the query videos through contrastive learning. Our comprehensive experimental results on two team sports datasets validate that our method significantly improves the retrieval performance. Ablation studies also demonstrate that several components in our human-in-the-loop adaptation contribute to the improvement of the retrieval performance. Code: this https URL.</li>
</ul>

<h3>Title: LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Tianxing Wu, Zheng Chen, Cirou Xu, Bowen Chai, Yong Guo, Yutong Liu, Linghe Kong, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03182">https://arxiv.org/abs/2602.03182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03182">https://arxiv.org/pdf/2602.03182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03182]] LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution(https://arxiv.org/abs/2602.03182)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>One-Step Diffusion Models have demonstrated promising capability and fast inference in video super-resolution (VSR) for real-world. Nevertheless, the substantial model size and high computational cost of Diffusion Transformers (DiTs) limit downstream applications. While low-bit quantization is a common approach for model compression, the effectiveness of quantized models is challenged by the high dynamic range of input latent and diverse layer behaviors. To deal with these challenges, we introduce LSGQuant, a layer-sensitivity guided quantizing approach for one-step diffusion-based real-world VSR. Our method incorporates a Dynamic Range Adaptive Quantizer (DRAQ) to fit video token activations. Furthermore, we estimate layer sensitivity and implement a Variance-Oriented Layer Training Strategy (VOLTS) by analyzing layer-wise statistics in calibration. We also introduce Quantization-Aware Optimization (QAO) to jointly refine the quantized branch and a retained high-precision branch. Extensive experiments demonstrate that our method has nearly performance to origin model with full-precision and significantly exceeds existing quantization techniques. Code is available at: this https URL.</li>
</ul>

<h3>Title: Hand3R: Online 4D Hand-Scene Reconstruction in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Wendi Hu, Haonan Zhou, Wenhao Hu, Gaoang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03200">https://arxiv.org/abs/2602.03200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03200">https://arxiv.org/pdf/2602.03200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03200]] Hand3R: Online 4D Hand-Scene Reconstruction in the Wild(https://arxiv.org/abs/2602.03200)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>For Embodied AI, jointly reconstructing dynamic hands and the dense scene context is crucial for understanding physical interaction. However, most existing methods recover isolated hands in local coordinates, overlooking the surrounding 3D environment. To address this, we present Hand3R, the first online framework for joint 4D hand-scene reconstruction from monocular video. Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism. By injecting high-fidelity hand priors into a persistent scene memory, our approach enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry in a single forward pass. Experiments demonstrate that Hand3R bypasses the reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning.</li>
</ul>

<h3>Title: Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyan Ye, Zhongjie Duan, Zhiwen Li, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03208">https://arxiv.org/abs/2602.03208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03208">https://arxiv.org/pdf/2602.03208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03208]] Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation(https://arxiv.org/abs/2602.03208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inference-time scaling offers a versatile paradigm for aligning visual generative models with downstream objectives without parameter updates. However, existing approaches that optimize the high-dimensional initial noise suffer from severe inefficiency, as many search directions exert negligible influence on the final generation. We show that this inefficiency is closely related to a spectral bias in generative dynamics: model sensitivity to initial perturbations diminishes rapidly as frequency increases. Building on this insight, we propose Spectral Evolution Search (SES), a plug-and-play framework for initial noise optimization that executes gradient-free evolutionary search within a low-frequency subspace. Theoretically, we derive the Spectral Scaling Prediction from perturbation propagation dynamics, which explains the systematic differences in the impact of perturbations across frequencies. Extensive experiments demonstrate that SES significantly advances the Pareto frontier of generation quality versus computational cost, consistently outperforming strong baselines under equivalent budgets.</li>
</ul>

<h3>Title: VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Li, Zhongjie Duan, Jinyan Ye, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03210">https://arxiv.org/abs/2602.03210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03210">https://arxiv.org/pdf/2602.03210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03210]] VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers(https://arxiv.org/abs/2602.03210)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>Replicating In-Context Learning (ICL) in computer vision remains challenging due to task heterogeneity. We propose \textbf{VIRAL}, a framework that elicits visual reasoning from a pre-trained image editing model by formulating ICL as conditional generation via visual analogy ($x_s : x_t :: x_q : y_q$). We adapt a frozen Diffusion Transformer (DiT) using role-aware multi-image conditioning and introduce a Mixture-of-Experts LoRA to mitigate gradient interference across diverse tasks. Additionally, to bridge the gaps in current visual context datasets, we curate a large-scale dataset spanning perception, restoration, and editing. Experiments demonstrate that VIRAL outperforms existing methods, validating that a unified V-ICL paradigm can handle the majority of visual tasks, including open-domain editing. Our code is available at this https URL</li>
</ul>

<h3>Title: Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yeongmin Kim, Donghyeok Shin, Byeonghu Na, Minsang Park, Richard Lee Kim, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03211">https://arxiv.org/abs/2602.03211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03211">https://arxiv.org/pdf/2602.03211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03211]] Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models(https://arxiv.org/abs/2602.03211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong generative performance; however, generated samples often fail to fully align with human intent. This paper studies a test-time scaling method that enables sampling from regions with higher human-aligned reward values. Existing gradient guidance methods approximate the expected future reward (EFR) at an intermediate particle $\mathbf{x}_t$ using a Taylor approximation, but this approximation at each time step incurs high computational cost due to sequential neural backpropagation. We show that the EFR at any $\mathbf{x}_t$ can be computed using only marginal samples from a pre-trained diffusion model. The proposed EFR formulation detaches the neural dependency between $\mathbf{x}_t$ and the EFR, enabling closed-form guidance computation without neural backpropagation. To further improve efficiency, we introduce lookahead sampling to collect marginal samples. For final sample generation, we use an accurate solver that guides particles toward high-reward lookahead samples. We refer to this sampling scheme as LiDAR sampling. LiDAR achieves substantial performance improvements using only three samples with a 3-step lookahead solver, exhibiting steep performance gains as lookahead accuracy and sample count increase; notably, it reaches the same GenEval performance as the latest gradient guidance method for SDXL with a 9.5x speedup.</li>
</ul>

<h3>Title: Topology Matters: A Cautionary Case Study of Graph SSL on Neuro-Inspired Benchmarks</h3>
<ul>
<li><strong>Authors: </strong>May Kristine Jonson Carlon, Su Myat Noe, Haojiong Wang, Yasuo Kuniyoshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03217">https://arxiv.org/abs/2602.03217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03217">https://arxiv.org/pdf/2602.03217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03217]] Topology Matters: A Cautionary Case Study of Graph SSL on Neuro-Inspired Benchmarks(https://arxiv.org/abs/2602.03217)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Understanding how local interactions give rise to global brain organization requires models that can represent information across multiple scales. We introduce a hierarchical self-supervised learning (SSL) framework that jointly learns node-, edge-, and graph-level embeddings, inspired by multimodal neuroimaging. We construct a controllable synthetic benchmark mimicking the topological properties of connectomes. Our four-stage evaluation protocol reveals a critical failure: the invariance-based SSL model is fundamentally misaligned with the benchmark's topological properties and is catastrophically outperformed by classical, topology-aware heuristics. Ablations confirm an objective mismatch: SSL objectives designed to be invariant to topological perturbations learn to ignore the very community structure that classical methods exploit. Our results expose a fundamental pitfall in applying generic graph SSL to connectome-like data. We present this framework as a cautionary case study, highlighting the need for new, topology-aware SSL objectives for neuro-AI research that explicitly reward the preservation of structure (e.g., modularity or motifs).</li>
</ul>

<h3>Title: PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingbang Tang (James)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03220">https://arxiv.org/abs/2602.03220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03220">https://arxiv.org/pdf/2602.03220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03220]] PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation(https://arxiv.org/abs/2602.03220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper studies reference-free style-conditioned character generation in text-to-image diffusion models, where high-quality synthesis requires both stable character structure and consistent, fine-grained style expression across diverse prompts. Existing approaches primarily rely on text-only prompting, which is often under-specified for visual style and tends to produce noticeable style drift and geometric inconsistency, or introduce reference-based adapters that depend on external images at inference time, increasing architectural complexity and limiting deployment this http URL propose PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly inside the diffusion decoder. By decoupling text and style conditioning at the attention level, our method enables effective reference-free stylized generation while keeping the pretrained diffusion backbone fully this http URL Attention trains only decoder cross-attention layers together with a compact style projection module, resulting in a parameter-efficient and plug-and-play control component that can be easily integrated into existing diffusion pipelines and transferred across different this http URL on a stylized character generation benchmark (Pokemon-style) demonstrate that our method consistently improves style fidelity, semantic alignment, and character shape consistency compared with representative adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.</li>
</ul>

<h3>Title: EventFlash: Towards Efficient MLLMs for Event-Based Vision</h3>
<ul>
<li><strong>Authors: </strong>Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang, Ming Li, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03230">https://arxiv.org/abs/2602.03230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03230">https://arxiv.org/pdf/2602.03230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03230]] EventFlash: Towards Efficient MLLMs for Event-Based Vision(https://arxiv.org/abs/2602.03230)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.</li>
</ul>

<h3>Title: GraDE: A Graph Diffusion Estimator for Frequent Subgraph Discovery in Neural Architectures</h3>
<ul>
<li><strong>Authors: </strong>Yikang Yang, Zhengxin Yang, Minghao Luo, Luzhou Peng, Hongxiao Li, Wanling Gao, Lei Wang, Jianfeng Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03257">https://arxiv.org/abs/2602.03257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03257">https://arxiv.org/pdf/2602.03257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03257]] GraDE: A Graph Diffusion Estimator for Frequent Subgraph Discovery in Neural Architectures(https://arxiv.org/abs/2602.03257)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Finding frequently occurring subgraph patterns or network motifs in neural architectures is crucial for optimizing efficiency, accelerating design, and uncovering structural insights. However, as the subgraph size increases, enumeration-based methods are perfectly accurate but computationally prohibitive, while sampling-based methods are computationally tractable but suffer from a severe decline in discovery capability. To address these challenges, this paper proposes GraDE, a diffusion-guided search framework that ensures both computational feasibility and discovery capability. The key innovation is the Graph Diffusion Estimator (GraDE), which is the first to introduce graph diffusion models to identify frequent subgraphs by scoring their typicality within the learned distribution. Comprehensive experiments demonstrate that the estimator achieves superior ranking accuracy, with up to 114\% improvement compared to sampling-based baselines. Benefiting from this, the proposed framework successfully discovers large-scale frequent patterns, achieving up to 30$\times$ higher median frequency than sampling-based methods.</li>
</ul>

<h3>Title: Anomaly Detection via Mean Shift Density Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Pritam Kar, Rahul Bordoloi, Olaf Wolkenhauer, Saptarshi Bej</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03293">https://arxiv.org/abs/2602.03293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03293">https://arxiv.org/pdf/2602.03293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03293]] Anomaly Detection via Mean Shift Density Enhancement(https://arxiv.org/abs/2602.03293)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection stands as an important problem in machine learning, with applications in financial fraud prevention, network security and medical diagnostics. Existing unsupervised anomaly detection algorithms rarely perform well across different anomaly types, often excelling only under specific structural assumptions. This lack of robustness also becomes particularly evident under noisy settings. We propose Mean Shift Density Enhancement (MSDE), a fully unsupervised framework that detects anomalies through their geometric response to density-driven manifold evolution. MSDE is based on the principle that normal samples, being well supported by local density, remain stable under iterative density enhancement, whereas anomalous samples undergo large cumulative displacements as they are attracted toward nearby density modes. To operationalize this idea, MSDE employs a weighted mean-shift procedure with adaptive, sample-specific density weights derived from a UMAP-based fuzzy neighborhood graph. Anomaly scores are defined by the total displacement accumulated across a small number of mean-shift iterations. We evaluate MSDE on the ADBench benchmark, comprising forty six real-world tabular datasets, four realistic anomaly generation mechanisms, and six noise levels. Compared to 13 established unsupervised baselines, MSDE achieves consistently strong, balanced and robust performance for AUC-ROC, AUC-PR, and Precision@n, at several noise levels and on average over several types of anomalies. These results demonstrate that displacement-based scoring provides a robust alternative to the existing state-of-the-art for unsupervised anomaly detection.</li>
</ul>

<h3>Title: R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Zhang, Tianyi Lin, Huanjin Yao, Xiang Lan, Shunyu Liu, Jiaxing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03300">https://arxiv.org/abs/2602.03300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03300">https://arxiv.org/pdf/2602.03300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03300]] R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?(https://arxiv.org/abs/2602.03300)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we aim to develop effective data synthesis techniques that autonomously synthesize multimodal training data for enhancing MLLMs in solving complex real-world tasks. To this end, we propose Collective Adversarial Data Synthesis (CADS), a novel and general approach to synthesize high-quality, diverse and challenging multimodal data for MLLMs. The core idea of CADS is to leverage collective intelligence to ensure high-quality and diverse generation, while exploring adversarial learning to synthesize challenging samples for effectively driving model improvement. Specifically, CADS operates with two cyclic phases, i.e., Collective Adversarial Data Generation (CAD-Generate) and Collective Adversarial Data Judgment (CAD-Judge). CAD-Generate leverages collective knowledge to jointly generate new and diverse multimodal data, while CAD-Judge collaboratively assesses the quality of synthesized data. In addition, CADS introduces an Adversarial Context Optimization mechanism to optimize the generation context to encourage challenging and high-value data generation. With CADS, we construct MMSynthetic-20K and train our model R1-SyntheticVL, which demonstrates superior performance on various benchmarks.</li>
</ul>

<h3>Title: Full end-to-end diagnostic workflow automation of 3D OCT via foundation model-driven AI for retinal diseases</h3>
<ul>
<li><strong>Authors: </strong>Jinze Zhang, Jian Zhong, Li Lin, Jiaxiong Li, Ke Ma, Naiyang Li, Meng Li, Yuan Pan, Zeyu Meng, Mengyun Zhou, Shang Huang, Shilong Yu, Zhengyu Duan, Sutong Li, Honghui Xia, Juping Liu, Dan Liang, Yantao Wei, Xiaoying Tang, Jin Yuan, Peng Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03302">https://arxiv.org/abs/2602.03302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03302">https://arxiv.org/pdf/2602.03302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03302]] Full end-to-end diagnostic workflow automation of 3D OCT via foundation model-driven AI for retinal diseases(https://arxiv.org/abs/2602.03302)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Optical coherence tomography (OCT) has revolutionized retinal disease diagnosis with its high-resolution and three-dimensional imaging nature, yet its full diagnostic automation in clinical practices remains constrained by multi-stage workflows and conventional single-slice single-task AI models. We present Full-process OCT-based Clinical Utility System (FOCUS), a foundation model-driven framework enabling end-to-end automation of 3D OCT retinal disease diagnosis. FOCUS sequentially performs image quality assessment with EfficientNetV2-S, followed by abnormality detection and multi-disease classification using a fine-tuned Vision Foundation Model. Crucially, FOCUS leverages a unified adaptive aggregation method to intelligently integrate 2D slices-level predictions into comprehensive 3D patient-level diagnosis. Trained and tested on 3,300 patients (40,672 slices), and externally validated on 1,345 patients (18,498 slices) across four different-tier centers and diverse OCT devices, FOCUS achieved high F1 scores for quality assessment (99.01%), abnormally detection (97.46%), and patient-level diagnosis (94.39%). Real-world validation across centers also showed stable performance (F1: 90.22%-95.24%). In human-machine comparisons, FOCUS matched expert performance in abnormality detection (F1: 95.47% vs 90.91%) and multi-disease diagnosis (F1: 93.49% vs 91.35%), while demonstrating better efficiency. FOCUS automates the image-to-diagnosis pipeline, representing a critical advance towards unmanned ophthalmology with a validated blueprint for autonomous screening to enhance population scale retinal care accessibility and efficiency.</li>
</ul>

<h3>Title: PQTNet: Pixel-wise Quantitative Thermography Neural Network for Estimating Defect Depth in Polylactic Acid Parts by Additive Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Lei Deng, Wenhao Huang, Chao Yang, Haoyuan Zheng, Yinbin Tian, Yue Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03314">https://arxiv.org/abs/2602.03314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03314">https://arxiv.org/pdf/2602.03314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03314]] PQTNet: Pixel-wise Quantitative Thermography Neural Network for Estimating Defect Depth in Polylactic Acid Parts by Additive Manufacturing(https://arxiv.org/abs/2602.03314)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Defect depth quantification in additively manufactured (AM) components remains a significant challenge for non-destructive testing (NDT). This study proposes a Pixel-wise Quantitative Thermography Neural Network (PQT-Net) to address this challenge for polylactic acid (PLA) parts. A key innovation is a novel data augmentation strategy that reconstructs thermal sequence data into two-dimensional stripe images, preserving the complete temporal evolution of heat diffusion for each pixel. The PQT-Net architecture incorporates a pre-trained EfficientNetV2-S backbone and a custom Residual Regression Head (RRH) with learnable parameters to refine outputs. Comparative experiments demonstrate the superiority of PQT-Net over other deep learning models, achieving a minimum Mean Absolute Error (MAE) of 0.0094 mm and a coefficient of determination (R) exceeding 99%. The high precision of PQT-Net underscores its potential for robust quantitative defect characterization in AM.</li>
</ul>

<h3>Title: Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ting Xiang, Jinhui Zhao, Changjian Chen, Zhuo Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03316">https://arxiv.org/abs/2602.03316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03316">https://arxiv.org/pdf/2602.03316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03316]] Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation(https://arxiv.org/abs/2602.03316)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of image generative models, generative data augmentation has become an effective way to enrich training images, especially when only small-scale datasets are available. At the same time, in practical applications, generative data augmentation can be vulnerable to clean-label backdoor attacks, which aim to bypass human inspection. However, based on theoretical analysis and preliminary experiments, we observe that directly applying existing pixel-level clean-label backdoor attack methods (e.g., COMBAT) to generated images results in low attack success rates. This motivates us to move beyond pixel-level triggers and focus instead on the latent feature level. To this end, we propose InvLBA, an invisible clean-label backdoor attack method for generative data augmentation by latent perturbation. We theoretically prove that the generalization of the clean accuracy and attack success rates of InvLBA can be guaranteed. Experiments on multiple datasets show that our method improves the attack success rate by 46.43% on average, with almost no reduction in clean accuracy and high robustness against SOTA defense methods.</li>
</ul>

<h3>Title: Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability</h3>
<ul>
<li><strong>Authors: </strong>Bingchen Zhao, Qiushan Guo, Ye Wang, Yixuan Huang, Zhonghua Zhai, Yu Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03339">https://arxiv.org/abs/2602.03339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03339">https://arxiv.org/pdf/2602.03339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03339]] Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability(https://arxiv.org/abs/2602.03339)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce CompTok, a training framework for learning visual tokenizers whose tokens are enhanced for compositionality. CompTok uses a token-conditioned diffusion decoder. By employing an InfoGAN-style objective, where we train a recognition model to predict the tokens used to condition the diffusion decoder using the decoded images, we enforce the decoder to not ignore any of the tokens. To promote compositional control, besides the original images, CompTok also trains on tokens formed by swapping token subsets between images, enabling more compositional control of the token over the decoder. As the swapped tokens between images do not have ground truth image targets, we apply a manifold constraint via an adversarial flow regularizer to keep unpaired swap generations on the natural-image distribution. The resulting tokenizer not only achieves state-of-the-art performance on image class-conditioned generation, but also demonstrates properties such as swapping tokens between images to achieve high level semantic editing of an image. Additionally, we propose two metrics that measures the landscape of the token space that can be useful to describe not only the compositionality of the tokens, but also how easy to learn the landscape is for a generator to be trained on this space. We show in experiments that CompTok can improve on both of the metrics as well as supporting state-of-the-art generators for class conditioned generation.</li>
</ul>

<h3>Title: Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Bryan Sangwoo Kim, Jonghyun Park, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03342">https://arxiv.org/abs/2602.03342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03342">https://arxiv.org/pdf/2602.03342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03342]] Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution(https://arxiv.org/abs/2602.03342)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.</li>
</ul>

<h3>Title: Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Takaya Kawakatsu, Ryo Ishiyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03370">https://arxiv.org/abs/2602.03370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03370">https://arxiv.org/pdf/2602.03370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03370]] Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition(https://arxiv.org/abs/2602.03370)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Handwritten Mathematical Expression Recognition (HMER) requires reasoning over diverse symbols and 2D structural layouts, yet autoregressive models struggle with exposure bias and syntactic inconsistency. We present a discrete diffusion framework that reformulates HMER as iterative symbolic refinement instead of sequential generation. Through multi-step remasking, the proposal progressively refines both symbols and structural relations, removing causal dependencies and improving structural consistency. A symbol-aware tokenization and Random-Masking Mutual Learning further enhance syntactic alignment and robustness to handwriting diversity. On the MathWriting benchmark, the proposal achieves 5.56\% CER and 60.42\% EM, outperforming strong Transformer and commercial baselines. Consistent gains on CROHME 2014--2023 demonstrate that discrete diffusion provides a new paradigm for structure-aware visual recognition beyond generative modeling.</li>
</ul>

<h3>Title: SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI</h3>
<ul>
<li><strong>Authors: </strong>Mario Pascual-González, Ariadna Jiménez-Partinen, R.M. Luque-Baena, Fátima Nagib-Raya, Ezequiel López-Rubio</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03372">https://arxiv.org/abs/2602.03372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03372">https://arxiv.org/pdf/2602.03372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03372]] SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI(https://arxiv.org/abs/2602.03372)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Focal cortical dysplasia (FCD) lesions in epilepsy FLAIR MRI are subtle and scarce, making joint image--mask generative modeling prone to instability and memorization. We propose SLIM-Diff, a compact joint diffusion model whose main contributions are (i) a single shared-bottleneck U-Net that enforces tight coupling between anatomy and lesion geometry from a 2-channel image+mask representation, and (ii) loss-geometry tuning via a tunable $L_p$ objective. As an internal baseline, we include the canonical DDPM-style objective ($\epsilon$-prediction with $L_2$ loss) and isolate the effect of prediction parameterization and $L_p$ geometry under a matched setup. Experiments show that $x_0$-prediction is consistently the strongest choice for joint synthesis, and that fractional sub-quadratic penalties ($L_{1.5}$) improve image fidelity while $L_2$ better preserves lesion mask morphology. Our code and model weights are available in this https URL</li>
</ul>

<h3>Title: UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Piotr Wójcik, Maksym Petrenko, Wojciech Gromski, Przemysław Spurek, Maciej Zieba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03410">https://arxiv.org/abs/2602.03410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03410">https://arxiv.org/pdf/2602.03410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03410]] UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning(https://arxiv.org/abs/2602.03410)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large-scale diffusion models have intensified concerns about their potential misuse, particularly in generating realistic yet harmful or socially disruptive content. This challenge has spurred growing interest in effective machine unlearning, the process of selectively removing specific knowledge or concepts from a model without compromising its overall generative capabilities. Among various approaches, Low-Rank Adaptation (LoRA) has emerged as an effective and efficient method for fine-tuning models toward targeted unlearning. However, LoRA-based methods often exhibit limited adaptability to concept semantics and struggle to balance removing closely related concepts with maintaining generalization across broader meanings. Moreover, these methods face scalability challenges when multiple concepts must be erased simultaneously. To address these limitations, we introduce UnHype, a framework that incorporates hypernetworks into single- and multi-concept LoRA training. The proposed architecture can be directly plugged into Stable Diffusion as well as modern flow-based text-to-image models, where it demonstrates stable training behavior and effective concept control. During inference, the hypernetwork dynamically generates adaptive LoRA weights based on the CLIP embedding, enabling more context-aware, scalable unlearning. We evaluate UnHype across several challenging tasks, including object erasure, celebrity erasure, and explicit content removal, demonstrating its effectiveness and versatility. Repository: this https URL.</li>
</ul>

<h3>Title: Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection</h3>
<ul>
<li><strong>Authors: </strong>Alexander Loth, Dominique Conceicao Rosario, Peter Ebinger, Martin Kappes, Marc-Oliver Pahl</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03423">https://arxiv.org/abs/2602.03423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03423">https://arxiv.org/pdf/2602.03423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03423]] Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection(https://arxiv.org/abs/2602.03423)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The proliferation of generative AI poses challenges for information integrity assurance, requiring systems that connect model governance with end-user verification. We present Origin Lens, a privacy-first mobile framework that targets visual disinformation through a layered verification architecture. Unlike server-side detection systems, Origin Lens performs cryptographic image provenance verification and AI detection locally on the device via a Rust/Flutter hybrid architecture. Our system integrates multiple signals - including cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification - to provide users with graded confidence indicators at the point of consumption. We discuss the framework's alignment with regulatory requirements (EU AI Act, DSA) and its role in verification infrastructure that complements platform-level mechanisms.</li>
</ul>

<h3>Title: Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yijia Xu, Zihao Wang, Jinshi Cui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03448">https://arxiv.org/abs/2602.03448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03448">https://arxiv.org/pdf/2602.03448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03448]] Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation(https://arxiv.org/abs/2602.03448)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.</li>
</ul>

<h3>Title: Inlier-Centric Post-Training Quantization for Object Detection Models</h3>
<ul>
<li><strong>Authors: </strong>Minsu Kim, Dongyeun Lee, Jaemyung Yu, Jiwan Hur, Giseop Kim, Junmo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03472">https://arxiv.org/abs/2602.03472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03472">https://arxiv.org/pdf/2602.03472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03472]] Inlier-Centric Post-Training Quantization for Object Detection Models(https://arxiv.org/abs/2602.03472)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Object detection is pivotal in computer vision, yet its immense computational demands make deployment slow and power-hungry, motivating quantization. However, task-irrelevant morphologies such as background clutter and sensor noise induce redundant activations (or anomalies). These anomalies expand activation ranges and skew activation distributions toward task-irrelevant responses, complicating bit allocation and weakening the preservation of informative features. Without a clear criterion to distinguish anomalies, suppressing them can inadvertently discard useful information. To address this, we present InlierQ, an inlier-centric post-training quantization approach that separates anomalies from informative inliers. InlierQ computes gradient-aware volume saliency scores, classifies each volume as an inlier or anomaly, and fits a posterior distribution over these scores using the Expectation-Maximization (EM) algorithm. This design suppresses anomalies while preserving informative features. InlierQ is label-free, drop-in, and requires only 64 calibration samples. Experiments on the COCO and nuScenes benchmarks show consistent reductions in quantization error for camera-based (2D and 3D) and LiDAR-based (3D) object detection.</li>
</ul>

<h3>Title: ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Wang, Cheng Chen, Gaoyang Jiang, Zijia Ren, Chuangxin Zhao, Lu Shi, Yanbiao Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03477">https://arxiv.org/abs/2602.03477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03477">https://arxiv.org/pdf/2602.03477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03477]] ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression(https://arxiv.org/abs/2602.03477)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Single-cell RNA-seq profiles are high-dimensional, sparse, and unordered, causing autoregressive generation to impose an artificial ordering bias and suffer from error accumulation. To address this, we propose scDiVa, a masked discrete diffusion foundation model that aligns generation with the dropout-like corruption process by defining a continuous-time forward masking mechanism in token space. ScDiVa features a bidirectional denoiser that jointly models discrete gene identities and continuous values, utilizing entropy-normalized serialization and a latent anchor token to maximize information efficiency and preserve global cell identity. The model is trained via depth-invariant time sampling and a dual denoising objective to simulate varying sparsity levels while ensuring precise recovery of both identity and magnitude. Pre-trained on 59 million cells, scDiVa achieves strong transfer performance across major benchmarks, including batch integration, cell type annotation, and perturbation response prediction. These results suggest that masked discrete diffusion serves as a biologically coherent and effective alternative to autoregression.</li>
</ul>

<h3>Title: A Minimal Task Reveals Emergent Path Integration and Object-Location Binding in a Predictive Sequence Model</h3>
<ul>
<li><strong>Authors: </strong>Linda Ariel Ventura, Victoria Bosch, Tim C Kietzmann, Sushrut Thorat</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03490">https://arxiv.org/abs/2602.03490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03490">https://arxiv.org/pdf/2602.03490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03490]] A Minimal Task Reveals Emergent Path Integration and Object-Location Binding in a Predictive Sequence Model(https://arxiv.org/abs/2602.03490)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Adaptive cognition requires structured internal models representing objects and their relations. Predictive neural networks are often proposed to form such "world models", yet their underlying mechanisms remain unclear. One hypothesis is that action-conditioned sequential prediction suffices for learning such world models. In this work, we investigate this possibility in a minimal in-silico setting. Sequentially sampling tokens from 2D continuous token scenes, a recurrent neural network is trained to predict the upcoming token from current input and a saccade-like displacement. On novel scenes, prediction accuracy improves across the sequence, indicating in-context learning. Decoding analyses reveal path integration and dynamic binding of token identity to position. Interventional analyses show that new bindings can be learned late in sequence and that out-of-distribution bindings can be learned. Together, these results demonstrate how structured representations that rely on flexible binding emerge to support prediction, offering a mechanistic account of sequential world modeling relevant to cognitive science.</li>
</ul>

<h3>Title: Lookahead Path Likelihood Optimization for Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuejie Liu, Yap Vit Chun, Yitao Liang, Anji Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03496">https://arxiv.org/abs/2602.03496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03496">https://arxiv.org/pdf/2602.03496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03496]] Lookahead Path Likelihood Optimization for Diffusion LLMs(https://arxiv.org/abs/2602.03496)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) support arbitrary-order generation, yet their inference performance critically depends on the unmasking order. Existing strategies rely on heuristics that greedily optimize local confidence, offering limited guidance for identifying unmasking paths that are globally consistent and accurate. To bridge this gap, we introduce path log-likelihood (Path LL), a trajectory-conditioned objective that strongly correlates with downstream accuracy and enables principled selection of unmasking paths. To optimize Path LL at inference time, we propose POKE, an efficient value estimator that predicts the expected future Path LL of a partial decoding trajectory. We then integrate this lookahead signal into POKE-SMC, a Sequential Monte Carlo-based search framework for dynamically identifying optimal unmasking paths. Extensive experiments across 6 reasoning tasks show that POKE-SMC consistently improves accuracy, achieving 2%--3% average gains over strong decoding-time scaling baselines at comparable inference overhead on LLaDA models and advancing the accuracy--compute Pareto frontier.</li>
</ul>

<h3>Title: Reparameterization Flow Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hai Zhong, Zhuoran Li, Xun Wang, Longbo Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03501">https://arxiv.org/abs/2602.03501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03501">https://arxiv.org/pdf/2602.03501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03501]] Reparameterization Flow Policy Optimization(https://arxiv.org/abs/2602.03501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\times$ the reward of the state-of-the-art baseline.</li>
</ul>

<h3>Title: Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Bozhou Li, Yushuo Guan, Haolin Li, Bohan Zeng, Yiyan Ji, Yue Ding, Pengfei Wan, Kun Gai, Yuanxing Zhang, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03510">https://arxiv.org/abs/2602.03510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03510">https://arxiv.org/pdf/2602.03510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03510]] Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers(https://arxiv.org/abs/2602.03510)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.</li>
</ul>

<h3>Title: Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xufei Zhang, Xinjiao Zhou, Ziling Deng, Dongdong Geng, Jianxiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03530">https://arxiv.org/abs/2602.03530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03530">https://arxiv.org/pdf/2602.03530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03530]] Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning(https://arxiv.org/abs/2602.03530)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Logical anomalies are violations of predefined constraints on object quantity, spatial layout, and compositional relationships in industrial images. While prior work largely treats anomaly detection as a binary decision, such formulations cannot indicate which logical rule is broken and therefore offer limited value for quality assurance. We introduce Logical Anomaly Classification (LAC), a task that unifies anomaly detection and fine-grained violation classification in a single inference step. To tackle LAC, we propose LogiCls, a vision-language framework that decomposes complex logical constraints into a sequence of verifiable subqueries. We further present a data-centric instruction synthesis pipeline that generates chain-of-thought (CoT) supervision for these subqueries, coupling precise grounding annotations with diverse image-text augmentations to adapt vision language models (VLMs) to logic-sensitive reasoning. Training is stabilized by a difficulty-aware resampling strategy that emphasizes challenging subqueries and long tail constraint types. Extensive experiments demonstrate that LogiCls delivers robust, interpretable, and accurate industrial logical anomaly classification, providing both the predicted violation categories and their evidence trails.</li>
</ul>

<h3>Title: PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yongwei Chen, Tianyi Wei, Yushi Lan, Zhaoyang Lyu, Shangchen Zhou, Xudong Xu, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03533">https://arxiv.org/abs/2602.03533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03533">https://arxiv.org/pdf/2602.03533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03533]] PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation(https://arxiv.org/abs/2602.03533)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The rapid progress of large multimodal models has inspired efforts toward unified frameworks that couple understanding and generation. While such paradigms have shown remarkable success in 2D, extending them to 3D remains largely underexplored. Existing attempts to unify 3D tasks under a single autoregressive (AR) paradigm lead to significant performance degradation due to forced signal quantization and prohibitive training cost. Our key insight is that the essential challenge lies not in enforcing a unified autoregressive paradigm, but in enabling effective information interaction between generation and understanding while minimally compromising their inherent capabilities and leveraging pretrained models to reduce training cost. Guided by this perspective, we present the first unified framework for 3D understanding and generation that combines autoregression with diffusion. Specifically, we adopt an autoregressive next-token prediction paradigm for 3D understanding, and a continuous diffusion paradigm for 3D generation. A lightweight transformer bridges the feature space of large language models and the conditional space of 3D diffusion models, enabling effective cross-modal information exchange while preserving the priors learned by standalone models. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across diverse 3D understanding and generation benchmarks, while also excelling in 3D editing tasks. These results highlight the potential of unified AR+diffusion models as a promising direction for building more general-purpose 3D intelligence.</li>
</ul>

<h3>Title: Can Large Language Models Generalize Procedures Across Representations?</h3>
<ul>
<li><strong>Authors: </strong>Fangru Lin, Valentin Hofmann, Xingchen Wan, Weixing Wang, Zifeng Ding, Anthony G. Cohn, Janet B. Pierrehumbert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03542">https://arxiv.org/abs/2602.03542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03542">https://arxiv.org/pdf/2602.03542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03542]] Can Large Language Models Generalize Procedures Across Representations?(https://arxiv.org/abs/2602.03542)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.</li>
</ul>

<h3>Title: SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Yuqin Dai, Ning Gao, Wei Zhang, Jie Wang, Zichen Luo, Jinpeng Wang, Yujie Wang, Ruiyuan Wu, Chaozheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03548">https://arxiv.org/abs/2602.03548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03548">https://arxiv.org/pdf/2602.03548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03548]] SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue(https://arxiv.org/abs/2602.03548)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: this https URL.</li>
</ul>

<h3>Title: ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Li, Zhiming Xu, Zhichao Zhang, Zhaolin Cai, Sijing Wu, Xiongkuo Min, Yitong Chen, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03558">https://arxiv.org/abs/2602.03558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03558">https://arxiv.org/pdf/2602.03558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03558]] ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images(https://arxiv.org/abs/2602.03558)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.</li>
</ul>

<h3>Title: ACL: Aligned Contrastive Learning Improves BERT and Multi-exit BERT Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03563">https://arxiv.org/abs/2602.03563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03563">https://arxiv.org/pdf/2602.03563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03563]] ACL: Aligned Contrastive Learning Improves BERT and Multi-exit BERT Fine-tuning(https://arxiv.org/abs/2602.03563)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Despite its success in self-supervised learning, contrastive learning is less studied in the supervised setting. In this work, we first use a set of pilot experiments to show that in the supervised setting, the cross-entropy loss objective (CE) and the contrastive learning objective often conflict with each other, thus hindering the applications of CL in supervised settings. To resolve this problem, we introduce a novel \underline{A}ligned \underline{C}ontrastive \underline{L}earning (ACL) framework. First, ACL-Embed regards label embeddings as extra augmented samples with different labels and employs contrastive learning to align the label embeddings with its samples' representations. Second, to facilitate the optimization of ACL-Embed objective combined with the CE loss, we propose ACL-Grad, which will discard the ACL-Embed term if the two objectives are in conflict. To further enhance the performances of intermediate exits of multi-exit BERT, we further propose cross-layer ACL (ACL-CL), which is to ask the teacher exit to guide the optimization of student shallow exits. Extensive experiments on the GLUE benchmark results in the following takeaways: (a) ACL-BRT outperforms or performs comparably with CE and CE+SCL on the GLUE tasks; (b) ACL, especially CL-ACL, significantly surpasses the baseline methods on the fine-tuning of multi-exit BERT, thus providing better quality-speed tradeoffs for low-latency applications.</li>
</ul>

<h3>Title: CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yaguo Liu, Mingyue Cheng, Daoyu Wang, Xiaoyu Tao, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03564">https://arxiv.org/abs/2602.03564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03564">https://arxiv.org/pdf/2602.03564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03564]] CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting(https://arxiv.org/abs/2602.03564)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Time series forecasting can be viewed as a generative problem that requires both semantic understanding over contextual conditions and stochastic modeling of continuous temporal dynamics. Existing approaches typically rely on either autoregressive large language models (LLMs) for semantic context modeling or diffusion-like models for continuous probabilistic generation. However, neither method alone can adequately model both aspects simultaneously. In this work, we propose CoGenCast, a hybrid generative framework that couples pre-trained LLMs with flow-matching mechanism for effective time series forecasting. Specifically, we reconfigure pre-trained decoder-only LLMs into a native forecasting encoder-decoder backbone by modifying only the attention topology, enabling bidirectional context encoding and causal representation generation. Building on this, a flow-matching mechanism is further integrated to model temporal evolution, capturing continuous stochastic dynamics conditioned on the autoregressively generated representation. Notably, CoGenCast naturally supports multimodal forecasting and cross-domain unified training. Extensive experiments on multiple benchmarks show that CoGenCast consistently outperforms previous compared baselines. Code is available at this https URL.</li>
</ul>

<h3>Title: Riemannian Neural Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Micheli, Yueqi Cao, Anthea Monod, Samir Bhatt</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03566">https://arxiv.org/abs/2602.03566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03566">https://arxiv.org/pdf/2602.03566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03566]] Riemannian Neural Optimal Transport(https://arxiv.org/abs/2602.03566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Computational optimal transport (OT) offers a principled framework for generative modeling. Neural OT methods, which use neural networks to learn an OT map (or potential) from data in an amortized way, can be evaluated out of sample after training, but existing approaches are tailored to Euclidean geometry. Extending neural OT to high-dimensional Riemannian manifolds remains an open challenge. In this paper, we prove that any method for OT on manifolds that produces discrete approximations of transport maps necessarily suffers from the curse of dimensionality: achieving a fixed accuracy requires a number of parameters that grows exponentially with the manifold dimension. Motivated by this limitation, we introduce Riemannian Neural OT (RNOT) maps, which are continuous neural-network parameterizations of OT maps on manifolds that avoid discretization and incorporate geometric structure by construction. Under mild regularity assumptions, we prove that RNOT maps approximate Riemannian OT maps with sub-exponential complexity in the dimension. Experiments on synthetic and real datasets demonstrate improved scalability and competitive performance relative to discretization-based baselines.</li>
</ul>

<h3>Title: CL-bench: A Benchmark for Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Shihan Dou, Ming Zhang, Zhangyue Yin, Chenhao Huang, Yujiong Shen, Junzhe Wang, Jiayi Chen, Yuchen Ni, Junjie Ye, Cheng Zhang, Huaibing Xie, Jianglu Hu, Shaolei Wang, Weichao Wang, Yanling Xiao, Yiting Liu, Zenan Xu, Zhen Guo, Pluto Zhou, Tao Gui, Zuxuan Wu, Xipeng Qiu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, Di Wang, Shunyu Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03587">https://arxiv.org/abs/2602.03587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03587">https://arxiv.org/pdf/2602.03587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03587]] CL-bench: A Benchmark for Context Learning(https://arxiv.org/abs/2602.03587)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.</li>
</ul>

<h3>Title: TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Alireza Salehi, Ehsan Karami, Sepehr Noey, Sahand Noey, Makoto Yamada, Reshad Hosseini, Mohammad Sabokrou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03594">https://arxiv.org/abs/2602.03594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03594">https://arxiv.org/pdf/2602.03594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03594]] TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection(https://arxiv.org/abs/2602.03594)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection identifies departures from expected behavior in safety-critical settings. When target-domain normal data are unavailable, zero-shot anomaly detection (ZSAD) leverages vision-language models (VLMs). However, CLIP's coarse image-text alignment limits both localization and detection due to (i) spatial misalignment and (ii) weak sensitivity to fine-grained anomalies; prior work compensates with complex auxiliary modules yet largely overlooks the choice of backbone. We revisit the backbone and use TIPS-a VLM trained with spatially aware objectives. While TIPS alleviates CLIP's issues, it exposes a distributional gap between global and local features. We address this with decoupled prompts-fixed for image-level detection and learnable for pixel-level localization-and by injecting local evidence into the global score. Without CLIP-specific tricks, our TIPS-based pipeline improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, delivering strong generalization with a lean architecture. Code is available at this http URL.</li>
</ul>

<h3>Title: SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network</h3>
<ul>
<li><strong>Authors: </strong>Cristian Manca, Christian Scano, Giorgio Piras, Fabio Brau, Maura Pintor, Battista Biggio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03596">https://arxiv.org/abs/2602.03596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03596">https://arxiv.org/pdf/2602.03596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03596]] SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network(https://arxiv.org/abs/2602.03596)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive this http URL this work, we study the problem of detecting 5G attacks \textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network this http URL then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.</li>
</ul>

<h3>Title: A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures</h3>
<ul>
<li><strong>Authors: </strong>Basile Terver, Randall Balestriero, Megi Dervishi, David Fan, Quentin Garrido, Tushar Nagarajan, Koustuv Sinha, Wancong Zhang, Mike Rabbat, Yann LeCun, Amir Bar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03604">https://arxiv.org/abs/2602.03604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03604">https://arxiv.org/pdf/2602.03604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03604]] A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures(https://arxiv.org/abs/2602.03604)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at this https URL.</li>
</ul>

<h3>Title: Controlling Output Rankings in Generative Engines for LLM-based Search</h3>
<ul>
<li><strong>Authors: </strong>Haibo Jin, Ruoxi Chen, Peiyan Zhang, Yifeng Luo, Huimin Zeng, Man Luo, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03608">https://arxiv.org/abs/2602.03608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03608">https://arxiv.org/pdf/2602.03608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03608]] Controlling Output Rankings in Generative Engines for LLM-based Search(https://arxiv.org/abs/2602.03608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility. In this work, we propose CORE, an optimization method that \textbf{C}ontrols \textbf{O}utput \textbf{R}ankings in g\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface. Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \textbf{91.4\% @Top-5}, \textbf{86.6\% @Top-3}, and \textbf{80.3\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.</li>
</ul>

<h3>Title: Ultra Fast PDE Solving via Physics Guided Few-step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Cindy Xiangrui Kong, Yueqi Wang, Haoyang Zheng, Weijian Luo, Guang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03627">https://arxiv.org/abs/2602.03627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03627">https://arxiv.org/pdf/2602.03627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03627]] Ultra Fast PDE Solving via Physics Guided Few-step Diffusion(https://arxiv.org/abs/2602.03627)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based models have demonstrated impressive accuracy and generalization in solving partial differential equations (PDEs). However, they still face significant limitations, such as high sampling costs and insufficient physical consistency, stemming from their many-step iterative sampling mechanism and lack of explicit physics constraints. To address these issues, we propose Phys-Instruct, a novel physics-guided distillation framework which not only (1) compresses a pre-trained diffusion PDE solver into a few-step generator via matching generator and prior diffusion distributions to enable rapid sampling, but also (2) enhances the physics consistency by explicitly injecting PDE knowledge through a PDE distillation guidance. Physic-Instruct is built upon a solid theoretical foundation, leading to a practical physics-constrained training objective that admits tractable gradients. Across five PDE benchmarks, Phys-Instruct achieves orders-of-magnitude faster inference while reducing PDE error by more than 8 times compared to state-of-the-art diffusion baselines. Moreover, the resulting unconditional student model functions as a compact prior, enabling efficient and physically consistent inference for various downstream conditional tasks. Our results indicate that Phys-Instruct is a novel, effective, and efficient framework for ultra-fast PDE solving powered by deep generative models.</li>
</ul>

<h3>Title: CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets</h3>
<ul>
<li><strong>Authors: </strong>Milosh Devic, Jordan Gierschendorf, David Garson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03641">https://arxiv.org/abs/2602.03641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03641">https://arxiv.org/pdf/2602.03641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03641]] CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets(https://arxiv.org/abs/2602.03641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. However, most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and bridging the gap for privacy for interpolation-based sampling methods and deep generative methods. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries such as healthcare, fraud detection, and predictive maintenance where even small gains in minority cases can be critical.</li>
</ul>

<h3>Title: RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish</h3>
<ul>
<li><strong>Authors: </strong>Süha Kağan Köse, Mehmet Can Baytekin, Burak Aktaş, Bilge Kaan Görür, Evren Ayberk Munis, Deniz Yılmaz, Muhammed Yusuf Kartal, Çağrı Toraman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03652">https://arxiv.org/abs/2602.03652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03652">https://arxiv.org/pdf/2602.03652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03652]] RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish(https://arxiv.org/abs/2602.03652)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.</li>
</ul>

<h3>Title: Reference-Free EM Validation Flow for Detecting Triggered Hardware Trojans</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Tahghigh, Hassan Salmani</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03666">https://arxiv.org/abs/2602.03666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03666">https://arxiv.org/pdf/2602.03666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03666]] Reference-Free EM Validation Flow for Detecting Triggered Hardware Trojans(https://arxiv.org/abs/2602.03666)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Hardware Trojans (HTs) threaten the trust and reliability of integrated circuits (ICs), particularly when triggered HTs remain dormant during standard testing and activate only under rare conditions. Existing electromagnetic (EM) side-channel-based detection techniques often rely on golden references or labeled data, which are infeasible in modern distributed manufacturing. This paper introduces a reference-free, design-agnostic framework for detecting triggered HTs directly from post-silicon EM emissions. The proposed flow converts each EM trace into a time-frequency scalogram using Continuous Wavelet Transform (CWT), extracts discriminative features through a convolutional neural network (CNN), reduces dimensionality with principal component analysis (PCA), and applies Bayesian Gaussian Mixture Modeling (BGMM) for unsupervised probabilistic clustering. The framework quantifies detection confidence using posterior-based metrics (alpha_{post}, beta_{post}), Bayesian information criterion (Delta BIC), and Mahalanobis cluster separation (D), enabling interpretable anomaly decisions without golden data. Experimental validation on AES-128 designs embedded with four different HTs demonstrates high separability between HT-free and HT-activated conditions and robustness to PCA variance thresholds. The results highlight the method's scalability, statistical interpretability, and potential for extension to runtime and in-field HT monitoring in trusted microelectronics.</li>
</ul>

<h3>Title: Referring Industrial Anomaly Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Yue, Xiaokang Jiang, Yilin Lu, Jianghang Lin, Shengchuan Zhang, Liujuan Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03673">https://arxiv.org/abs/2602.03673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03673">https://arxiv.org/pdf/2602.03673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03673]] Referring Industrial Anomaly Segmentation(https://arxiv.org/abs/2602.03673)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Industrial Anomaly Detection (IAD) is vital for manufacturing, yet traditional methods face significant challenges: unsupervised approaches yield rough localizations requiring manual thresholds, while supervised methods overfit due to scarce, imbalanced data. Both suffer from the "One Anomaly Class, One Model" limitation. To address this, we propose Referring Industrial Anomaly Segmentation (RIAS), a paradigm leveraging language to guide detection. RIAS generates precise masks from text descriptions without manual thresholds and uses universal prompts to detect diverse anomalies with a single model. We introduce the MVTec-Ref dataset to support this, designed with diverse referring expressions and focusing on anomaly patterns, notably with 95% small anomalies. We also propose the Dual Query Token with Mask Group Transformer (DQFormer) benchmark, enhanced by Language-Gated Multi-Level Aggregation (LMA) to improve multi-scale segmentation. Unlike traditional methods using redundant queries, DQFormer employs only "Anomaly" and "Background" tokens for efficient visual-textual integration. Experiments demonstrate RIAS's effectiveness in advancing IAD toward open-set capabilities. Code: this https URL.</li>
</ul>

<h3>Title: ContraLog: Log File Anomaly Detection with Contrastive Learning and Masked Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Simon Dietz, Kai Klede, An Nguyen, Bjoern M Eskofier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03678">https://arxiv.org/abs/2602.03678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03678">https://arxiv.org/pdf/2602.03678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03678]] ContraLog: Log File Anomaly Detection with Contrastive Learning and Masked Language Modeling(https://arxiv.org/abs/2602.03678)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Log files record computational events that reflect system state and behavior, making them a primary source of operational insights in modern computer systems. Automated anomaly detection on logs is therefore critical, yet most established methods rely on log parsers that collapse messages into discrete templates, discarding variable values and semantic content. We propose ContraLog, a parser-free and self-supervised method that reframes log anomaly detection as predicting continuous message embeddings rather than discrete template IDs. ContraLog combines a message encoder that produces rich embeddings for individual log messages with a sequence encoder to model temporal dependencies within sequences. The model is trained with a combination of masked language modeling and contrastive learning to predict masked message embeddings based on the surrounding context. Experiments on the HDFS, BGL, and Thunderbird benchmark datasets empirically demonstrate effectiveness on complex datasets with diverse log messages. Additionally, we find that message embeddings generated by ContraLog carry meaningful information and are predictive of anomalies even without sequence context. These results highlight embedding-level prediction as an approach for log anomaly detection, with potential applicability to other event sequences.</li>
</ul>

<h3>Title: Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization</h3>
<ul>
<li><strong>Authors: </strong>Henrik Schopmans, Christopher von Klitzing, Pascal Friederich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03729">https://arxiv.org/abs/2602.03729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03729">https://arxiv.org/pdf/2602.03729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03729]] Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization(https://arxiv.org/abs/2602.03729)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sampling from unnormalized probability densities is a central challenge in computational science. Boltzmann generators are generative models that enable independent sampling from the Boltzmann distribution of physical systems at a given temperature. However, their practical success depends on data-efficient training, as both simulation data and target energy evaluations are costly. To this end, we propose off-policy log-dispersion regularization (LDR), a novel regularization framework that builds on a generalization of the log-variance objective. We apply LDR in the off-policy setting in combination with standard data-based training objectives, without requiring additional on-policy samples. LDR acts as a shape regularizer of the energy landscape by leveraging additional information in the form of target energy labels. The proposed regularization framework is broadly applicable, supporting unbiased or biased simulation datasets as well as purely variational training without access to target samples. Across all benchmarks, LDR improves both final performance and data efficiency, with sample efficiency gains of up to one order of magnitude.</li>
</ul>

<h3>Title: LIVE: Long-horizon Interactive Video World Modeling</h3>
<ul>
<li><strong>Authors: </strong>Junchao Huang, Ziyang Ye, Xinting Hu, Tianyu He, Guiyu Zhang, Shaoshuai Shi, Jiang Bian, Li Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03747">https://arxiv.org/abs/2602.03747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03747">https://arxiv.org/pdf/2602.03747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03747]] LIVE: Long-horizon Interactive Video World Modeling(https://arxiv.org/abs/2602.03747)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.</li>
</ul>

<h3>Title: See-through: Single-image Layer Decomposition for Anime Characters</h3>
<ul>
<li><strong>Authors: </strong>Jian Lin, Chengze Li, Haoyun Qin, Kwun Wang Chan, Yanghua Jin, Hanyuan Liu, Stephen Chun Wang Choy, Xueting Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03749">https://arxiv.org/abs/2602.03749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03749">https://arxiv.org/pdf/2602.03749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03749]] See-through: Single-image Layer Decomposition for Anime Characters(https://arxiv.org/abs/2602.03749)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a framework that automates the transformation of static anime illustrations into manipulatable 2.5D models. Current professional workflows require tedious manual segmentation and the artistic ``hallucination'' of occluded regions to enable motion. Our approach overcomes this by decomposing a single image into fully inpainted, semantically distinct layers with inferred drawing orders. To address the scarcity of training data, we introduce a scalable engine that bootstraps high-quality supervision from commercial Live2D models, capturing pixel-perfect semantics and hidden geometry. Our methodology couples a diffusion-based Body Part Consistency Module, which enforces global geometric coherence, with a pixel-level pseudo-depth inference mechanism. This combination resolves the intricate stratification of anime characters, e.g., interleaving hair strands, allowing for dynamic layer reconstruction. We demonstrate that our approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications.</li>
</ul>

<h3>Title: Test-Time Conditioning with Representation-Aligned Visual Features</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Sereyjol-Garros, Ellington Kirby, Victor Letzelter, Victor Besnier, Nermin Samet</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03753">https://arxiv.org/abs/2602.03753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03753">https://arxiv.org/pdf/2602.03753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03753]] Test-Time Conditioning with Representation-Aligned Visual Features(https://arxiv.org/abs/2602.03753)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at this https URL.</li>
</ul>

<h3>Title: Reasoning with Latent Tokens in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Andre He, Sean Welleck, Daniel Fried</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03769">https://arxiv.org/abs/2602.03769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03769">https://arxiv.org/pdf/2602.03769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03769]] Reasoning with Latent Tokens in Diffusion Language Models(https://arxiv.org/abs/2602.03769)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have recently become competitive with autoregressive models for language modeling, even outperforming them on reasoning tasks requiring planning and global coherence, but they require more computation at inference time. We trace this trade-off to a key mechanism: diffusion models are trained to jointly predict a distribution over all unknown tokens, including those that will not actually be decoded in the current step. Ablating this joint prediction yields faster inference but degrades performance, revealing that accurate prediction at the decoded position relies on joint reasoning about the distribution of undecoded tokens. We interpret these as latent tokens and introduce a method for modulating their number, demonstrating empirically that this enables a smooth tradeoff between inference speed and sample quality. Furthermore, we demonstrate that latent tokens can be introduced into autoregressive models through an auxiliary multi-token prediction objective, yielding substantial improvements on the same reasoning tasks where they have traditionally struggled. Our results suggest that latent tokens, while arising naturally in diffusion, represent a general mechanism for improving performance on tasks requiring global coherence or lookahead.</li>
</ul>

<h3>Title: Efficient Estimation of Kernel Surrogate Models for Task Attribution</h3>
<ul>
<li><strong>Authors: </strong>Zhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03783">https://arxiv.org/abs/2602.03783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03783">https://arxiv.org/pdf/2602.03783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03783]] Efficient Estimation of Kernel Surrogate Models for Task Attribution(https://arxiv.org/abs/2602.03783)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.</li>
</ul>

<h3>Title: Inference-time Unlearning Using Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Somnath Basu Roy Chowdhury, Rahul Kidambi, Avinava Dubey, David Wang, Gokhan Mergen, Amr Ahmed, Aranyak Mehta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03787">https://arxiv.org/abs/2602.03787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03787">https://arxiv.org/pdf/2602.03787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03787]] Inference-time Unlearning Using Conformal Prediction(https://arxiv.org/abs/2602.03787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning is the process of efficiently removing specific information from a trained machine learning model without retraining from scratch. Existing unlearning methods, which often provide provable guarantees, typically involve retraining a subset of model parameters based on a forget set. While these approaches show promise in certain scenarios, their underlying assumptions are often challenged in real-world applications -- particularly when applied to generative models. Furthermore, updating parameters using these unlearning procedures often degrades the general-purpose capabilities the model acquired during pre-training. Motivated by these shortcomings, this paper considers the paradigm of inference time unlearning -- wherein, the generative model is equipped with an (approximately correct) verifier that judges whether the model's response satisfies appropriate unlearning guarantees. This paper introduces a framework that iteratively refines the quality of the generated responses using feedback from the verifier without updating the model parameters. The proposed framework leverages conformal prediction to reduce computational overhead and provide distribution-free unlearning guarantees. This paper's approach significantly outperforms existing state-of-the-art methods, reducing unlearning error by up to 93% across challenging unlearning benchmarks.</li>
</ul>

<h3>Title: Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Bogdan Kulynych, Theresa Stadler, Jean Louis Raisaro, Carmela Troncoso</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03791">https://arxiv.org/abs/2602.03791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03791">https://arxiv.org/pdf/2602.03791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03791]] Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation(https://arxiv.org/abs/2602.03791)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modelling have led many to see synthetic data as the go-to solution for a range of problems around data access, scarcity, and under-representation. In this paper, we study three prominent use cases: (1) Sharing synthetic data as a proxy for proprietary datasets to enable statistical analyses while protecting privacy, (2) Augmenting machine learning training sets with synthetic data to improve model performance, and (3) Augmenting datasets with synthetic data to reduce variance in statistical estimation. For each use case, we formalise the problem setting and study, through formal analysis and case studies, under which conditions synthetic data can achieve its intended objectives. We identify fundamental and practical limits that constrain when synthetic data can serve as an effective solution for a particular problem. Our analysis reveals that due to these limits many existing or envisioned use cases of synthetic data are a poor problem fit. Our formalisations and classification of synthetic data use cases enable decision makers to assess whether synthetic data is a suitable approach for their specific data availability problem.</li>
</ul>

<h3>Title: Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziru Chen, Dongdong Chen, Ruinan Jin, Yingbin Liang, Yujia Xie, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03806">https://arxiv.org/abs/2602.03806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03806">https://arxiv.org/pdf/2602.03806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03806]] Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation(https://arxiv.org/abs/2602.03806)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References</h3>
<ul>
<li><strong>Authors: </strong>Sahil Tripathi, Gautam Siddharth Kashyap, Mehwish Nasim, Jian Yang, Jiechao Gao, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03822">https://arxiv.org/abs/2602.03822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03822">https://arxiv.org/pdf/2602.03822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03822]] They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References(https://arxiv.org/abs/2602.03822)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.</li>
</ul>

<h3>Title: Continuous Control of Editing Models via Adaptive-Origin Guidance</h3>
<ul>
<li><strong>Authors: </strong>Alon Wolf, Chen Katzir, Kfir Aberman, Or Patashnik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03826">https://arxiv.org/abs/2602.03826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03826">https://arxiv.org/pdf/2602.03826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03826]] Continuous Control of Editing Models via Adaptive-Origin Guidance(https://arxiv.org/abs/2602.03826)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.</li>
</ul>

<h3>Title: PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Romain Cosentino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03846">https://arxiv.org/abs/2602.03846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03846">https://arxiv.org/pdf/2602.03846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03846]] PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning(https://arxiv.org/abs/2602.03846)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \textsc{PLATE} (\textbf{Pla}sticity-\textbf{T}unable \textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $\Delta W = B A Q^\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at this https URL.</li>
</ul>

<h3>Title: EventNeuS: 3D Mesh Reconstruction from a Single Event Camera</h3>
<ul>
<li><strong>Authors: </strong>Shreyas Sachan, Viktor Rudnev, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03847">https://arxiv.org/abs/2602.03847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03847">https://arxiv.org/pdf/2602.03847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03847]] EventNeuS: 3D Mesh Reconstruction from a Single Event Camera(https://arxiv.org/abs/2602.03847)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
