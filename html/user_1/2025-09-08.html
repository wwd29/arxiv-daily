<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-08</h1>
<h3>Title: Just-in-time and distributed task representations in language models</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Li, Declan Campbell, Stephanie C. Y. Chan, Andrew Kyle Lampinen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04466">https://arxiv.org/abs/2509.04466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04466">https://arxiv.org/pdf/2509.04466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04466]] Just-in-time and distributed task representations in language models(https://arxiv.org/abs/2509.04466)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Many of language models' impressive capabilities originate from their in-context learning: based on instructions or examples, they can infer and perform new tasks without weight updates. In this work, we investigate \emph{when} representations for new tasks are formed in language models, and \emph{how} these representations change over the course of context. We focus on ''transferrable'' task representations -- vector representations that can restore task context in another instance of the model, even without the full prompt. We show that these representations evolve in non-monotonic and sporadic ways, and are distinct from a more inert representation of high-level task categories that persists throughout the context. Specifically, models often condense multiple evidence into these transferrable task representations, which align well with the performance improvement based on more examples in the context. However, this accrual process exhibits strong locality along the sequence dimension, coming online only at certain tokens -- despite task identity being reliably decodable throughout the context. Moreover, these local but transferrable task representations tend to capture minimal ''task scopes'', such as a semantically-independent subtask, and models rely on more temporally-distributed representations to support longer and composite tasks. This two-fold locality (temporal and semantic) underscores a kind of just-in-time computational process underlying language models' ability to adapt to new evidence and learn new tasks on the fly.</li>
</ul>

<h3>Title: COCORELI: Cooperative, Compositional Reconstitution \& Execution of Language Instructions</h3>
<ul>
<li><strong>Authors: </strong>Swarnadeep Bhar, Omar Naim, Eleni Metheniti, Bastien Navarri, Lo√Øc Cabannes, Morteza Ezzabady, Nicholas Asher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04470">https://arxiv.org/abs/2509.04470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04470">https://arxiv.org/pdf/2509.04470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04470]] COCORELI: Cooperative, Compositional Reconstitution \& Execution of Language Instructions(https://arxiv.org/abs/2509.04470)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We present COCORELI, a hybrid agent framework designed to tackle the limitations of large language models (LLMs) in tasks requiring: following complex instructions, minimizing hallucination, and spatial reasoning. COCORELI integrates medium-sized LLM agents with novel abstraction mechanisms and a discourse module to parse instructions to in-context learn dynamic, high-level representations of the environment. Experiments on natural collaborative construction tasks show that COCORELI outperforms single-LLM CoT and agentic LLM systems, all using larger LLMs. It manages to largely avoid hallucinations, identify missing information, ask for clarifications, and update its learned objects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown in the ToolBench API completion task.</li>
</ul>

<h3>Title: DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs</h3>
<ul>
<li><strong>Authors: </strong>Minghui Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04483">https://arxiv.org/abs/2509.04483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04483">https://arxiv.org/pdf/2509.04483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04483]] DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs(https://arxiv.org/abs/2509.04483)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Claim decomposition plays a crucial role in the fact-checking process by breaking down complex claims into simpler atomic components and identifying their unfactual elements. Despite its importance, current research primarily focuses on generative methods for decomposition, with insufficient emphasis on evaluating the quality of these decomposed atomic claims. To bridge this gap, we introduce \textbf{DecMetrics}, which comprises three new metrics: \texttt{COMPLETENESS}, \texttt{CORRECTNESS}, and \texttt{SEMANTIC ENTROPY}, designed to automatically assess the quality of claims produced by decomposition models. Utilizing these metrics, we develop a lightweight claim decomposition model, optimizing its performance through the integration of these metrics as a reward function. Through automatic evaluation, our approach aims to set a benchmark for claim decomposition, enhancing both the reliability and effectiveness of fact-checking systems.</li>
</ul>

<h3>Title: DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence</h3>
<ul>
<li><strong>Authors: </strong>Pranav Narayanan Venkit, Philippe Laban, Yilun Zhou, Kung-Hsiang Huang, Yixin Mao, Chien-Sheng Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04499">https://arxiv.org/abs/2509.04499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04499">https://arxiv.org/pdf/2509.04499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04499]] DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence(https://arxiv.org/abs/2509.04499)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative search engines and deep research LLM agents promise trustworthy, source-grounded synthesis, yet users regularly encounter overconfidence, weak sourcing, and confusing citation practices. We introduce DeepTRACE, a novel sociotechnically grounded audit framework that turns prior community-identified failure cases into eight measurable dimensions spanning answer text, sources, and citations. DeepTRACE uses statement-level analysis (decomposition, confidence scoring) and builds citation and factual-support matrices to audit how systems reason with and attribute evidence end-to-end. Using automated extraction pipelines for popular public models (e.g., GPT-4.5/5, this http URL, Perplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to human raters, we evaluate both web-search engines and deep-research configurations. Our findings show that generative search engines and deep research agents frequently produce one-sided, highly confident responses on debate queries and include large fractions of statements unsupported by their own listed sources. Deep-research configurations reduce overconfidence and can attain high citation thoroughness, but they remain highly one-sided on debate queries and still exhibit large fractions of unsupported statements, with citation accuracy ranging from 40--80% across systems.</li>
</ul>

<h3>Title: Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations</h3>
<ul>
<li><strong>Authors: </strong>Martha O. Dimgba, Sharon Oba, Ameeta Agrawal, Philippe J. Giabbanelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04515">https://arxiv.org/abs/2509.04515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04515">https://arxiv.org/pdf/2509.04515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04515]] Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations(https://arxiv.org/abs/2509.04515)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Language models have been shown to propagate social bias through their output, particularly in the representation of gender and ethnicity. This paper investigates gender and ethnicity biases in AI-generated occupational stories. Representation biases are measured before and after applying our proposed mitigation strategy, Bias Analysis and Mitigation through Explanation (BAME), revealing improvements in demographic representation ranging from 2% to 20%. BAME leverages model-generated explanations to inform targeted prompt engineering, effectively reducing biases without modifying model parameters. By analyzing stories generated across 25 occupational groups, three large language models (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and multiple demographic dimensions, we identify persistent patterns of overrepresentation and underrepresentation linked to training data stereotypes. Our findings demonstrate that guiding models with their own internal reasoning mechanisms can significantly enhance demographic parity, thereby contributing to the development of more transparent generative AI systems.</li>
</ul>

<h3>Title: PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting</h3>
<ul>
<li><strong>Authors: </strong>Linqing Wang, Ximing Xing, Yiji Cheng, Zhiyuan Zhao, Jiale Tao, Qixun Wang, Ruihuang Li, Xin Li, Mingrui Wu, Xinchi Deng, Chunyu Wang, Qinglin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04545">https://arxiv.org/abs/2509.04545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04545">https://arxiv.org/pdf/2509.04545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04545]] PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting(https://arxiv.org/abs/2509.04545)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-image (T2I) diffusion models have demonstrated remarkable capabilities in generating high-fidelity images. However, these models often struggle to faithfully render complex user prompts, particularly in aspects like attribute binding, negation, and compositional relationships. This leads to a significant mismatch between user intent and the generated output. To address this challenge, we introduce PromptEnhancer, a novel and universal prompt rewriting framework that enhances any pretrained T2I model without requiring modifications to its weights. Unlike prior methods that rely on model-specific fine-tuning or implicit reward signals like image-reward scores, our framework decouples the rewriter from the generator. We achieve this by training a Chain-of-Thought (CoT) rewriter through reinforcement learning, guided by a dedicated reward model we term the AlignEvaluator. The AlignEvaluator is trained to provide explicit and fine-grained feedback based on a systematic taxonomy of 24 key points, which are derived from a comprehensive analysis of common T2I failure modes. By optimizing the CoT rewriter to maximize the reward from our AlignEvaluator, our framework learns to generate prompts that are more precisely interpreted by T2I models. Extensive experiments on the HunyuanImage 2.1 model demonstrate that PromptEnhancer significantly improves image-text alignment across a wide range of semantic and compositional challenges. Furthermore, we introduce a new, high-quality human preference benchmark to facilitate future research in this direction.</li>
</ul>

<h3>Title: Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Lu, Kai Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04582">https://arxiv.org/abs/2509.04582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04582">https://arxiv.org/pdf/2509.04582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04582]] Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping(https://arxiv.org/abs/2509.04582)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: this https URL</li>
</ul>

<h3>Title: DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jin Ma, Mohammed Aldeen, Christopher Salas, Feng Luo, Mashrur Chowdhury, Mert Pes√©, Long Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04597">https://arxiv.org/abs/2509.04597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04597">https://arxiv.org/pdf/2509.04597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04597]] DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models(https://arxiv.org/abs/2509.04597)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Object detection is fundamental to various real-world applications, such as security monitoring and surveillance video analysis. Despite their advancements, state-of-theart object detectors are still vulnerable to adversarial patch attacks, which can be easily applied to real-world objects to either conceal actual items or create non-existent ones, leading to severe consequences. Given the current diversity of adversarial patch attacks and potential unknown threats, an ideal defense method should be effective, generalizable, and robust against adaptive attacks. In this work, we introduce DISPATCH, the first diffusion-based defense framework for object detection. Unlike previous works that aim to "detect and remove" adversarial patches, DISPATCH adopts a "regenerate and rectify" strategy, leveraging generative models to disarm attack effects while preserving the integrity of the input image. Specifically, we utilize the in-distribution generative power of diffusion models to regenerate the entire image, aligning it with benign data. A rectification process is then employed to identify and replace adversarial regions with their regenerated benign counterparts. DISPATCH is attack-agnostic and requires no prior knowledge of the existing patches. Extensive experiments across multiple detectors and attacks demonstrate that DISPATCH consistently outperforms state-of-the-art defenses on both hiding attacks and creating attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and lowering the attack success rate to 24.8% on untargeted creating attacks. Moreover, it maintains strong robustness against adaptive attacks, making it a practical and reliable defense for object detection systems.</li>
</ul>

<h3>Title: Spoken in Jest, Detected in Earnest: A Systematic Review of Sarcasm Recognition -- Multimodal Fusion, Challenges, and Future Prospects</h3>
<ul>
<li><strong>Authors: </strong>Xiyuan Gao, Shekhar Nayak, Matt Coler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04605">https://arxiv.org/abs/2509.04605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04605">https://arxiv.org/pdf/2509.04605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04605]] Spoken in Jest, Detected in Earnest: A Systematic Review of Sarcasm Recognition -- Multimodal Fusion, Challenges, and Future Prospects(https://arxiv.org/abs/2509.04605)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sarcasm, a common feature of human communication, poses challenges in interpersonal interactions and human-machine interactions. Linguistic research has highlighted the importance of prosodic cues, such as variations in pitch, speaking rate, and intonation, in conveying sarcastic intent. Although previous work has focused on text-based sarcasm detection, the role of speech data in recognizing sarcasm has been underexplored. Recent advancements in speech technology emphasize the growing importance of leveraging speech data for automatic sarcasm recognition, which can enhance social interactions for individuals with neurodegenerative conditions and improve machine understanding of complex human language use, leading to more nuanced interactions. This systematic review is the first to focus on speech-based sarcasm recognition, charting the evolution from unimodal to multimodal approaches. It covers datasets, feature extraction, and classification methods, and aims to bridge gaps across diverse research domains. The findings include limitations in datasets for sarcasm recognition in speech, the evolution of feature extraction techniques from traditional acoustic features to deep learning-based representations, and the progression of classification methods from unimodal approaches to multimodal fusion techniques. In so doing, we identify the need for greater emphasis on cross-cultural and multilingual sarcasm recognition, as well as the importance of addressing sarcasm as a multimodal phenomenon, rather than a text-based challenge.</li>
</ul>

<h3>Title: Sample-efficient Integration of New Modalities into Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Osman Batur ƒ∞nce, Andr√© F. T. Martins, Oisin Mac Aodha, Edoardo M. Ponti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04606">https://arxiv.org/abs/2509.04606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04606">https://arxiv.org/pdf/2509.04606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04606]] Sample-efficient Integration of New Modalities into Large Language Models(https://arxiv.org/abs/2509.04606)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multimodal foundation models can process several modalities. However, since the space of possible modalities is large and evolving over time, training a model from scratch to encompass all modalities is unfeasible. Moreover, integrating a modality into a pre-existing foundation model currently requires a significant amount of paired data, which is often not available for low-resource modalities. In this paper, we introduce a method for sample-efficient modality integration (SEMI) into Large Language Models (LLMs). To this end, we devise a hypernetwork that can adapt a shared projector -- placed between modality-specific encoders and an LLM -- to any modality. The hypernetwork, trained on high-resource modalities (i.e., text, speech, audio, video), is conditioned on a few samples from any arbitrary modality at inference time to generate a suitable adapter. To increase the diversity of training modalities, we artificially multiply the number of encoders through isometric transformations. We find that SEMI achieves a significant boost in sample efficiency during few-shot integration of new modalities (i.e., satellite images, astronomical images, inertial measurements, and molecules) with encoders of arbitrary embedding dimensionality. For instance, to reach the same accuracy as 32-shot SEMI, training the projector from scratch needs 64$\times$ more data. As a result, SEMI holds promise to extend the modality coverage of foundation models.</li>
</ul>

<h3>Title: Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</h3>
<ul>
<li><strong>Authors: </strong>Jialin Wu, Shreya Saha, Yiqing Bo, Meenakshi Khosla</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04622">https://arxiv.org/abs/2509.04622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04622">https://arxiv.org/pdf/2509.04622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04622]] Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families(https://arxiv.org/abs/2509.04622)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Representational similarity metrics are fundamental tools in neuroscience and AI, yet we lack systematic comparisons of their discriminative power across model families. We introduce a quantitative framework to evaluate representational similarity measures based on their ability to separate model families-across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures-dprime from signal detection theory, silhouette coefficients and ROC-AUC, we systematically assess the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes, and soft matching. We show that separability systematically increases as metrics impose more stringent alignment constraints. Among mapping-based approaches, soft-matching achieves the highest separability, followed by Procrustes alignment and linear predictivity. Non-fitting methods such as RSA also yield strong separability across families. These results provide the first systematic comparison of similarity metrics through a separability lens, clarifying their relative sensitivity and guiding metric choice for large-scale model and brain comparisons.</li>
</ul>

<h3>Title: Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ayush Gupta, Ramneet Kaur, Anirban Roy, Adam D. Cobb, Rama Chellappa, Susmit Jha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04655">https://arxiv.org/abs/2509.04655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04655">https://arxiv.org/pdf/2509.04655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04655]] Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs(https://arxiv.org/abs/2509.04655)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We propose a novel inference-time out-of-domain (OOD) detection algorithm for specialized large language models (LLMs). Despite achieving state-of-the-art performance on in-domain tasks through fine-tuning, specialized LLMs remain vulnerable to incorrect or unreliable outputs when presented with OOD inputs, posing risks in critical applications. Our method leverages the Inductive Conformal Anomaly Detection (ICAD) framework, using a new non-conformity measure based on the model's dropout tolerance. Motivated by recent findings on polysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs exhibit higher dropout tolerance than OOD inputs. We aggregate dropout tolerance across multiple layers via a valid ensemble approach, improving detection while maintaining theoretical false alarm bounds from ICAD. Experiments with medical-specialized LLMs show that our approach detects OOD inputs better than baseline methods, with AUROC improvements of $2\%$ to $37\%$ when treating OOD datapoints as positives and in-domain test datapoints as negatives.</li>
</ul>

<h3>Title: AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs</h3>
<ul>
<li><strong>Authors: </strong>Aisha Alansari, Hamzah Luqman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04656">https://arxiv.org/abs/2509.04656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04656">https://arxiv.org/pdf/2509.04656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04656]] AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs(https://arxiv.org/abs/2509.04656)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, extensive research on the hallucination of the large language models (LLMs) has mainly focused on the English language. Despite the growing number of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination in the Arabic context remains relatively underexplored. The knowledge gap is particularly pressing given Arabic's widespread use across many regions and its importance in global communication and media. This paper presents the first comprehensive hallucination evaluation of Arabic and multilingual LLMs on two critical Arabic natural language generation tasks: generative question answering (GQA) and summarization. This study evaluates a total of 12 LLMs, including 4 Arabic pre-trained models, 4 multilingual models, and 4 reasoning-based models. To assess the factual consistency and faithfulness of LLMs' outputs, we developed a fine-grained hallucination evaluation framework consisting of 12 fine-grained hallucination indicators that represent the varying characteristics of each task. The results reveal that factual hallucinations are more prevalent than faithfulness errors across all models and tasks. Notably, the Arabic pre-trained model Allam consistently demonstrates lower hallucination rates than multilingual models and a comparative performance with reasoning-based models. The code is available at: \href{this https URL}{Github link}.</li>
</ul>

<h3>Title: Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization</h3>
<ul>
<li><strong>Authors: </strong>Dharsan Ravindran, Kevin Wang, Zhuoyuan Cao, Saleh Abdelrahman, Jeffery Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04735">https://arxiv.org/abs/2509.04735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04735">https://arxiv.org/pdf/2509.04735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04735]] Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization(https://arxiv.org/abs/2509.04735)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent advances in vision foundation models, such as the Segment Anything Model (SAM) and its successor SAM2, have achieved state-of-the-art performance on general image segmentation benchmarks. However, these models struggle in adverse weather conditions where visual ambiguity is high, largely due to their lack of uncertainty quantification. Inspired by progress in medical imaging, where uncertainty-aware training has improved reliability in ambiguous cases, we investigate two approaches to enhance segmentation robustness for autonomous driving. First, we introduce a multi-step finetuning procedure for SAM2 that incorporates uncertainty metrics directly into the loss function, improving overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter (UAT), originally designed for medical image segmentation, to driving contexts. We evaluate both methods on CamVid, BDD100K, and GTA driving datasets. Experiments show that UAT-SAM outperforms standard SAM in extreme weather, while SAM2 with uncertainty-aware loss achieves improved performance across diverse driving scenes. These findings underscore the value of explicit uncertainty modeling for safety-critical autonomous driving in challenging environments.</li>
</ul>

<h3>Title: Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects</h3>
<ul>
<li><strong>Authors: </strong>Gunmay Handa, Zekun Wu, Adriano Koshiyama, Philip Treleaven</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04794">https://arxiv.org/abs/2509.04794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04794">https://arxiv.org/pdf/2509.04794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04794]] Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects(https://arxiv.org/abs/2509.04794)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Personality manipulation in large language models (LLMs) is increasingly applied in customer service and agentic scenarios, yet its mechanisms and trade-offs remain unclear. We present a systematic study of personality control using the Big Five traits, comparing in-context learning (ICL), parameter-efficient fine-tuning (PEFT), and mechanistic steering (MS). Our contributions are fourfold. First, we construct a contrastive dataset with balanced high/low trait responses, enabling effective steering vector computation and fair cross-method evaluation. Second, we introduce a unified evaluation framework based on within-run $\Delta$ analysis that disentangles, reasoning capability, agent performance, and demographic bias across MMLU, GAIA, and BBQ benchmarks. Third, we develop trait purification techniques to separate openness from conscientiousness, addressing representational overlap in trait encoding. Fourth, we propose a three-level stability framework that quantifies method-, trait-, and combination-level robustness, offering practical guidance under deployment constraints. Experiments on Gemma-2-2B-IT and LLaMA-3-8B-Instruct reveal clear trade-offs: ICL achieves strong alignment with minimal capability loss, PEFT delivers the highest alignment at the cost of degraded task performance, and MS provides lightweight runtime control with competitive effectiveness. Trait-level analysis shows openness as uniquely challenging, agreeableness as most resistant to ICL, and personality encoding consolidating around intermediate layers. Taken together, these results establish personality manipulation as a multi-level probe into behavioral representation, linking surface conditioning, parameter encoding, and activation-level steering, and positioning mechanistic steering as a lightweight alternative to fine-tuning for both deployment and interpretability.</li>
</ul>

<h3>Title: Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training</h3>
<ul>
<li><strong>Authors: </strong>Figarri Keisha, Zekun Wu, Ze Wang, Adriano Koshiyama, Philip Treleaven</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04796">https://arxiv.org/abs/2509.04796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04796">https://arxiv.org/pdf/2509.04796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04796]] Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training(https://arxiv.org/abs/2509.04796)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models increasingly rely on synthetic data due to human-written content scarcity, yet recursive training on model-generated outputs leads to model collapse, a degenerative process threatening factual reliability. We define knowledge collapse as a distinct three-stage phenomenon where factual accuracy deteriorates while surface fluency persists, creating "confidently wrong" outputs that pose critical risks in accuracy-dependent domains. Through controlled experiments with recursive synthetic training, we demonstrate that collapse trajectory and timing depend critically on instruction format, distinguishing instruction-following collapse from traditional model collapse through its conditional, prompt-dependent nature. We propose domain-specific synthetic training as a targeted mitigation strategy that achieves substantial improvements in collapse resistance while maintaining computational efficiency. Our evaluation framework combines model-centric indicators with task-centric metrics to detect distinct degradation phases, enabling reproducible assessment of epistemic deterioration across different language models. These findings provide both theoretical insights into collapse dynamics and practical guidance for sustainable AI training in knowledge-intensive applications where accuracy is paramount.</li>
</ul>

<h3>Title: Using LLMs for Multilingual Clinical Entity Linking to ICD-10</h3>
<ul>
<li><strong>Authors: </strong>Sylvia Vassileva, Ivan Koychev, Svetla Boytcheva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04868">https://arxiv.org/abs/2509.04868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04868">https://arxiv.org/pdf/2509.04868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04868]] Using LLMs for Multilingual Clinical Entity Linking to ICD-10(https://arxiv.org/abs/2509.04868)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The linking of clinical entities is a crucial part of extracting structured information from clinical texts. It is the process of assigning a code from a medical ontology or classification to a phrase in the text. The International Classification of Diseases - 10th revision (ICD-10) is an international standard for classifying diseases for statistical and insurance purposes. Automatically assigning the correct ICD-10 code to terms in discharge summaries will simplify the work of healthcare professionals and ensure consistent coding in hospitals. Our paper proposes an approach for linking clinical terms to ICD-10 codes in different languages using Large Language Models (LLMs). The approach consists of a multistage pipeline that uses clinical dictionaries to match unambiguous terms in the text and then applies in-context learning with GPT-4.1 to predict the ICD-10 code for the terms that do not match the dictionary. Our system shows promising results in predicting ICD-10 codes on different benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on subcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC.</li>
</ul>

<h3>Title: Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series</h3>
<ul>
<li><strong>Authors: </strong>Yuki Takemoto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04921">https://arxiv.org/abs/2509.04921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04921">https://arxiv.org/pdf/2509.04921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04921]] Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series(https://arxiv.org/abs/2509.04921)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series forecasting plays a critical role in decision-making processes across diverse fields including meteorology, traffic, electricity, economics, finance, and so on. Especially, predicting returns on financial instruments is a challenging problem. Some researchers have proposed time series foundation models applicable to various forecasting tasks. Simultaneously, based on the recognition that real-world time series exhibit chaotic properties, methods have been developed to artificially generate synthetic chaotic time series, construct diverse datasets and train models. In this study, we propose a methodology for modeling financial time series by generating artificial chaotic time series and applying resampling techniques to simulate financial time series data, which we then use as training samples. Increasing the resampling interval to extend predictive horizons, we conducted large-scale pre-training using 10 billion training samples for each case. We subsequently created test datasets for multiple timeframes using actual Bitcoin trade data and performed zero-shot prediction without re-training the pre-trained model. The results of evaluating the profitability of a simple trading strategy based on these predictions demonstrated significant performance improvements over autocorrelation models. During the large-scale pre-training process, we observed a scaling law-like phenomenon that we can achieve predictive performance at a certain level with extended predictive horizons for chaotic time series by increasing the number of training samples exponentially. If this scaling law proves robust and holds true across various chaotic models, it suggests the potential to predict near-future events by investing substantial computational resources. Future research should focus on further large-scale training and verifying the applicability of this scaling law to diverse chaotic models.</li>
</ul>

<h3>Title: Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper</h3>
<ul>
<li><strong>Authors: </strong>Gehui Chen, Guan'an Wang, Xiaowen Huang, Jitao Sang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04957">https://arxiv.org/abs/2509.04957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04957">https://arxiv.org/pdf/2509.04957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04957]] Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper(https://arxiv.org/abs/2509.04957)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Recent Video-to-Audio (V2A) generation relies on extracting semantic and temporal features from video to condition generative models. Training these models from scratch is resource intensive. Consequently, leveraging foundation models (FMs) has gained traction due to their cross-modal knowledge transfer and generalization capabilities. One prior work has explored fine-tuning a lightweight mapper network to connect a pre-trained visual encoder with a text-to-audio generation model for V2A. Inspired by this, we introduce the Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper approach, MFM-Mapper benefits from richer semantic and temporal information by fusing features from dual visual encoders. Furthermore, by replacing a linear mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels between cross-modal features mapping and autoregressive translation tasks. Our MFM-Mapper exhibits remarkable training efficiency. It achieves better performance in semantic and temporal consistency with fewer training consuming, requiring only 16\% of the training scale compared to previous mapper-based work, yet achieves competitive performance with models trained on a much larger scale.</li>
</ul>

<h3>Title: Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts</h3>
<ul>
<li><strong>Authors: </strong>Julius Neumann, Robert Lange, Yuni Susanti, Michael F√§rber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04982">https://arxiv.org/abs/2509.04982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04982">https://arxiv.org/pdf/2509.04982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04982]] Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts(https://arxiv.org/abs/2509.04982)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sentiment classification in short text datasets faces significant challenges such as class imbalance, limited training samples, and the inherent subjectivity of sentiment labels -- issues that are further intensified by the limited context in short texts. These factors make it difficult to resolve ambiguity and exacerbate data sparsity, hindering effective learning. In this paper, we evaluate the effectiveness of small Transformer-based models (i.e., BERT and RoBERTa, with fewer than 1 billion parameters) for multi-label sentiment classification, with a particular focus on short-text settings. Specifically, we evaluated three key factors influencing model performance: (1) continued domain-specific pre-training, (2) data augmentation using automatically generated examples, specifically generative data augmentation, and (3) architectural variations of the classification head. Our experiment results show that data augmentation improves classification performance, while continued pre-training on augmented datasets can introduce noise rather than boost accuracy. Furthermore, we confirm that modifications to the classification head yield only marginal benefits. These findings provide practical guidance for optimizing BERT-based models in resource-constrained settings and refining strategies for sentiment classification in short-text datasets.</li>
</ul>

<h3>Title: Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Sidahmed Benabderrahmane, Talal Rahwan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.04999">https://arxiv.org/abs/2509.04999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.04999">https://arxiv.org/pdf/2509.04999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.04999]] Adversarial Augmentation and Active Sampling for Robust Cyber Anomaly Detection(https://arxiv.org/abs/2509.04999)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Advanced Persistent Threats (APTs) present a considerable challenge to cybersecurity due to their stealthy, long-duration nature. Traditional supervised learning methods typically require large amounts of labeled data, which is often scarce in real-world scenarios. This paper introduces a novel approach that combines AutoEncoders for anomaly detection with active learning to iteratively enhance APT detection. By selectively querying an oracle for labels on uncertain or ambiguous samples, our method reduces labeling costs while improving detection accuracy, enabling the model to effectively learn with minimal data and reduce reliance on extensive manual labeling. We present a comprehensive formulation of the Attention Adversarial Dual AutoEncoder-based anomaly detection framework and demonstrate how the active learning loop progressively enhances the model's performance. The framework is evaluated on real-world, imbalanced provenance trace data from the DARPA Transparent Computing program, where APT-like attacks account for just 0.004\% of the data. The datasets, which cover multiple operating systems including Android, Linux, BSD, and Windows, are tested in two attack scenarios. The results show substantial improvements in detection rates during active learning, outperforming existing methods.</li>
</ul>

<h3>Title: LUIVITON: Learned Universal Interoperable VIrtual Try-ON</h3>
<ul>
<li><strong>Authors: </strong>Cong Cao, Xianhang Cheng, Jingyuan Liu, Yujian Zheng, Zhenhui Lin, Meriem Chkir, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05030">https://arxiv.org/abs/2509.05030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05030">https://arxiv.org/pdf/2509.05030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05030]] LUIVITON: Learned Universal Interoperable VIrtual Try-ON(https://arxiv.org/abs/2509.05030)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>We present LUIVITON, an end-to-end system for fully automated virtual try-on, capable of draping complex, multi-layer clothing onto diverse and arbitrarily posed humanoid characters. To address the challenge of aligning complex garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy representation and separate the clothing-to-body draping problem into two correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence, where each has its unique challenges. While we address the clothing-to-SMPL fitting problem using a geometric learning-based approach for partial-to-complete shape correspondence prediction, we introduce a diffusion model-based approach for body-to-SMPL correspondence using multi-view consistent appearance features and a pre-trained 2D foundation model. Our method can handle complex geometries, non-manifold meshes, and generalizes effectively to a wide range of humanoid characters -- including humans, robots, cartoon subjects, creatures, and aliens, while maintaining computational efficiency for practical adoption. In addition to offering a fully automatic fitting solution, LUIVITON supports fast customization of clothing size, allowing users to adjust clothing sizes and material properties after they have been draped. We show that our system can produce high-quality 3D clothing fittings without any human labor, even when 2D clothing sewing patterns are not available.</li>
</ul>

<h3>Title: Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Wu, Hanxi Li, Lin Yuanbo Wu, Hao Chen, Deyin Liu, Peng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05034">https://arxiv.org/abs/2509.05034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05034">https://arxiv.org/pdf/2509.05034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05034]] Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization(https://arxiv.org/abs/2509.05034)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP = 96.1\% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC = 99.1\% on MVTec AD).</li>
</ul>

<h3>Title: Masked Diffusion Language Models with Frequency-Informed Training</h3>
<ul>
<li><strong>Authors: </strong>Despoina Kosmopoulou, Efthymios Georgiou, Vaggelis Dorovatas, Georgios Paraskevopoulos, Alexandros Potamianos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05056">https://arxiv.org/abs/2509.05056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05056">https://arxiv.org/pdf/2509.05056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05056]] Masked Diffusion Language Models with Frequency-Informed Training(https://arxiv.org/abs/2509.05056)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present a masked diffusion language modeling framework for data-efficient training for the BabyLM 2025 Challenge. Our approach applies diffusion training objectives to language modeling under strict data constraints, incorporating frequency-informed masking that prioritizes learning from rare tokens while maintaining theoretical validity. We explore multiple noise scheduling strategies, including two-mode approaches, and investigate different noise weighting schemes within the NELBO objective. We evaluate our method on the BabyLM benchmark suite, measuring linguistic competence, world knowledge, and human-likeness. Results show performance competitive to hybrid autoregressive-masked baselines, demonstrating that diffusion-based training offers a viable alternative for data-restricted language learning.</li>
</ul>

<h3>Title: ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions</h3>
<ul>
<li><strong>Authors: </strong>Matteo Bortoletto, Constantin Ruhdorfer, Andreas Bulling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05066">https://arxiv.org/abs/2509.05066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05066">https://arxiv.org/pdf/2509.05066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05066]] ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions(https://arxiv.org/abs/2509.05066)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Most existing Theory of Mind (ToM) benchmarks for foundation models rely on variations of the Sally-Anne test, offering only a very limited perspective on ToM and neglecting the complexity of human social interactions. To address this gap, we propose ToM-SSI: a new benchmark specifically designed to test ToM capabilities in environments rich with social interactions and spatial dynamics. While current ToM benchmarks are limited to text-only or dyadic interactions, ToM-SSI is multimodal and includes group interactions of up to four agents that communicate and move in situated environments. This unique design allows us to study, for the first time, mixed cooperative-obstructive settings and reasoning about multiple agents' mental state in parallel, thus capturing a wider range of social cognition than existing benchmarks. Our evaluations reveal that the current models' performance is still severely limited, especially in these new tasks, highlighting critical gaps for future research.</li>
</ul>

<h3>Title: Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction</h3>
<ul>
<li><strong>Authors: </strong>Mojtaba Safari, Zach Eidex, Richard L.J. Qiu, Matthew Goette, Tonghe Wang, Xiaofeng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05071">https://arxiv.org/abs/2509.05071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05071">https://arxiv.org/pdf/2509.05071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05071]] Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction(https://arxiv.org/abs/2509.05071)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Background: To systematically review and perform a meta-analysis of artificial intelligence (AI)-driven methods for detecting and correcting magnetic resonance imaging (MRI) motion artifacts, assessing current developments, effectiveness, challenges, and future research directions. Methods: A comprehensive systematic review and meta-analysis were conducted, focusing on deep learning (DL) approaches, particularly generative models, for the detection and correction of MRI motion artifacts. Quantitative data were extracted regarding utilized datasets, DL architectures, and performance metrics. Results: DL, particularly generative models, show promise for reducing motion artifacts and improving image quality; however, limited generalizability, reliance on paired training data, and risk of visual distortions remain key challenges that motivate standardized datasets and reporting. Conclusions: AI-driven methods, particularly DL generative models, show significant potential for improving MRI image quality by effectively addressing motion artifacts. However, critical challenges must be addressed, including the need for comprehensive public datasets, standardized reporting protocols for artifact levels, and more advanced, adaptable DL techniques to reduce reliance on extensive paired datasets. Addressing these aspects could substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and improve patient care outcomes.</li>
</ul>

<h3>Title: A Scalable Attention-Based Approach for Image-to-3D Texture Mapping</h3>
<ul>
<li><strong>Authors: </strong>Arianna Rampini, Kanika Madan, Bruno Roy, AmirHossein Zamani, Derek Cheung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05131">https://arxiv.org/abs/2509.05131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05131">https://arxiv.org/pdf/2509.05131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05131]] A Scalable Attention-Based Approach for Image-to-3D Texture Mapping(https://arxiv.org/abs/2509.05131)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-quality textures are critical for realistic 3D content creation, yet existing generative methods are slow, rely on UV maps, and often fail to remain faithful to a reference image. To address these challenges, we propose a transformer-based framework that predicts a 3D texture field directly from a single image and a mesh, eliminating the need for UV mapping and differentiable rendering, and enabling faster texture generation. Our method integrates a triplane representation with depth-based backprojection losses, enabling efficient training and faster inference. Once trained, it generates high-fidelity textures in a single forward pass, requiring only 0.2s per shape. Extensive qualitative, quantitative, and user preference evaluations demonstrate that our method outperforms state-of-the-art baselines on single-image texture reconstruction in terms of both fidelity to the input image and perceptual quality, highlighting its practicality for scalable, high-quality, and controllable 3D content creation.</li>
</ul>

<h3>Title: Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights</h3>
<ul>
<li><strong>Authors: </strong>Cosmin-Andrei Hatfaludi, Alex Serban</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05142">https://arxiv.org/abs/2509.05142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05142">https://arxiv.org/pdf/2509.05142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05142]] Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights(https://arxiv.org/abs/2509.05142)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Federated learning has the potential to unlock siloed data and distributed resources by enabling collaborative model training without sharing private data. As more complex foundational models gain widespread use, the need to expand training resources and integrate privately owned data grows as well. In this article, we explore the intersection of federated learning and foundational models, aiming to identify, categorize, and characterize technical methods that integrate the two paradigms. As a unified survey is currently unavailable, we present a literature survey structured around a novel taxonomy that follows the development life-cycle stages, along with a technical comparison of available methods. Additionally, we provide practical insights and guidelines for implementing and evolving these methods, with a specific focus on the healthcare domain as a case study, where the potential impact of federated learning and foundational models is considered significant. Our survey covers multiple intersecting topics, including but not limited to federated learning, self-supervised learning, fine-tuning, distillation, and transfer learning. Initially, we retrieved and reviewed a set of over 4,200 articles. This collection was narrowed to more than 250 thoroughly reviewed articles through inclusion criteria, featuring 42 unique methods. The methods were used to construct the taxonomy and enabled their comparison based on complexity, efficiency, and scalability. We present these results as a self-contained overview that not only summarizes the state of the field but also provides insights into the practical aspects of adopting, evolving, and integrating foundational models with federated learning.</li>
</ul>

<h3>Title: SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ariel Basso Madjoukeng, J√©r√¥me Fink, Pierre Poitier, Edith Belise Kenmogne, Benoit Frenay</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05188">https://arxiv.org/abs/2509.05188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05188">https://arxiv.org/pdf/2509.05188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05188]] SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition(https://arxiv.org/abs/2509.05188)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Sign language recognition (SLR) is a machine learning task aiming to identify signs in videos. Due to the scarcity of annotated data, unsupervised methods like contrastive learning have become promising in this field. They learn meaningful representations by pulling positive pairs (two augmented versions of the same instance) closer and pushing negative pairs (different from the positive pairs) apart. In SLR, in a sign video, only certain parts provide information that is truly useful for its recognition. Applying contrastive methods to SLR raises two issues: (i) contrastive learning methods treat all parts of a video in the same way, without taking into account the relevance of certain parts over others; (ii) shared movements between different signs make negative pairs highly similar, complicating sign discrimination. These issues lead to learning non-discriminative features for sign recognition and poor results in downstream tasks. In response, this paper proposes a self-supervised learning framework designed to learn meaningful representations for SLR. This framework consists of two key components designed to work together: (i) a new self-supervised approach with free-negative pairs; (ii) a new data augmentation technique. This approach shows a considerable gain in accuracy compared to several contrastive and self-supervised methods, across linear evaluation, semi-supervised learning, and transferability between sign languages.</li>
</ul>

<h3>Title: BEDTime: A Unified Benchmark for Automatically Describing Time Series</h3>
<ul>
<li><strong>Authors: </strong>Medhasweta Sen, Zachary Gottesman, Jiaxing Qiu, C. Bayan Bruss, Nam Nguyen, Tom Hartvigsen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05215">https://arxiv.org/abs/2509.05215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05215">https://arxiv.org/pdf/2509.05215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05215]] BEDTime: A Unified Benchmark for Automatically Describing Time Series(https://arxiv.org/abs/2509.05215)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Many recent studies have proposed general-purpose foundation models designed for a variety of time series analysis tasks. While several established datasets already exist for evaluating these models, previous works frequently introduce their models in conjunction with new datasets, limiting opportunities for direct, independent comparisons and obscuring insights into the relative strengths of different methods. Additionally, prior evaluations often cover numerous tasks simultaneously, assessing a broad range of model abilities without clearly pinpointing which capabilities contribute to overall performance. To address these gaps, we formalize and evaluate 3 tasks that test a model's ability to describe time series using generic natural language: (1) recognition (True/False question-answering), (2) differentiation (multiple choice question-answering), and (3) generation (open-ended natural language description). We then unify 4 recent datasets to enable head-to-head model comparisons on each task. Experimentally, in evaluating 13 state-of-the-art language, vision--language, and time series--language models, we find that (1) popular language-only methods largely underperform, indicating a need for time series-specific architectures, (2) VLMs are quite successful, as expected, identifying the value of vision models for these tasks and (3) pretrained multimodal time series--language models successfully outperform LLMs, but still have significant room for improvement. We also find that all approaches exhibit clear fragility in a range of robustness tests. Overall, our benchmark provides a standardized evaluation on a task necessary for time series reasoning systems.</li>
</ul>

<h3>Title: FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</h3>
<ul>
<li><strong>Authors: </strong>Matteo Poggi, Fabio Tosi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.05297">https://arxiv.org/abs/2509.05297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.05297">https://arxiv.org/pdf/2509.05297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.05297]] FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases(https://arxiv.org/abs/2509.05297)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present FlowSeek, a novel framework for optical flow requiring minimal hardware resources for training. FlowSeek marries the latest advances on the design space of optical flow networks with cutting-edge single-image depth foundation models and classical low-dimensional motion parametrization, implementing a compact, yet accurate architecture. FlowSeek is trained on a single consumer-grade GPU, a hardware budget about 8x lower compared to most recent methods, and still achieves superior cross-dataset generalization on Sintel Final and KITTI, with a relative improvement of 10 and 15% over the previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow datasets.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
