<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement. (arXiv:2309.07254v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07254">http://arxiv.org/abs/2309.07254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07254]] Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement(http://arxiv.org/abs/2309.07254)</code></li>
<li>Summary: <p>While diffusion models demonstrate a remarkable capability for generating
high-quality images, their tendency to `replicate' training data raises privacy
concerns. Although recent research suggests that this replication may stem from
the insufficient generalization of training data captions and duplication of
training images, effective mitigation strategies remain elusive. To address
this gap, our paper first introduces a generality score that measures the
caption generality and employ large language model (LLM) to generalize training
captions. Subsequently, we leverage generalized captions and propose a novel
dual fusion enhancement approach to mitigate the replication of diffusion
models. Our empirical results demonstrate that our proposed methods can
significantly reduce replication by 43.5% compared to the original diffusion
model while maintaining the diversity and quality of generations.
</p></li>
</ul>

<h3>Title: Unbiased Face Synthesis With Diffusion Models: Are We There Yet?. (arXiv:2309.07277v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07277">http://arxiv.org/abs/2309.07277</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07277]] Unbiased Face Synthesis With Diffusion Models: Are We There Yet?(http://arxiv.org/abs/2309.07277)</code></li>
<li>Summary: <p>Text-to-image diffusion models have achieved widespread popularity due to
their unprecedented image generation capability. In particular, their ability
to synthesize and modify human faces has spurred research into using generated
face images in both training data augmentation and model performance
assessments. In this paper, we study the efficacy and shortcomings of
generative models in the context of face generation. Utilizing a combination of
qualitative and quantitative measures, including embedding-based metrics and
user studies, we present a framework to audit the characteristics of generated
faces conditioned on a set of social attributes. We applied our framework on
faces generated through state-of-the-art text-to-image diffusion models. We
identify several limitations of face image generation that include faithfulness
to the text prompt, demographic disparities, and distributional shifts.
Furthermore, we present an analytical model that provides insights into how
training data selection contributes to the performance of generative models.
</p></li>
</ul>

<h3>Title: Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07398">http://arxiv.org/abs/2309.07398</a></li>
<li>Code URL: https://github.com/steven202/semantic_adv_via_dm</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07398]] Semantic Adversarial Attacks via Diffusion Models(http://arxiv.org/abs/2309.07398)</code></li>
<li>Summary: <p>Traditional adversarial attacks concentrate on manipulating clean examples in
the pixel space by adding adversarial perturbations. By contrast, semantic
adversarial attacks focus on changing semantic attributes of clean examples,
such as color, context, and features, which are more feasible in the real
world. In this paper, we propose a framework to quickly generate a semantic
adversarial attack by leveraging recent diffusion models since semantic
information is included in the latent space of well-trained diffusion models.
Then there are two variants of this framework: 1) the Semantic Transformation
(ST) approach fine-tunes the latent space of the generated image and/or the
diffusion model itself; 2) the Latent Masking (LM) approach masks the latent
space with another target image and local backpropagation-based interpretation
methods. Additionally, the ST approach can be applied in either white-box or
black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ
datasets, and our framework demonstrates great fidelity, generalizability, and
transferability compared to other baselines. Our approaches achieve
approximately 100% attack success rate in multiple settings with the best FID
as 36.61. Code is available at
https://github.com/steven202/semantic_adv_via_dm.
</p></li>
</ul>

<h3>Title: Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos. (arXiv:2309.07409v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07409">http://arxiv.org/abs/2309.07409</a></li>
<li>Code URL: https://github.com/ffzzy840304/masked-pdpp</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07409]] Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos(http://arxiv.org/abs/2309.07409)</code></li>
<li>Summary: <p>A key challenge with procedure planning in instructional videos lies in how
to handle a large decision space consisting of a multitude of action types that
belong to various tasks. To understand real-world video content, an AI agent
must proficiently discern these action types (e.g., pour milk, pour water, open
lid, close lid, etc.) based on brief visual observation. Moreover, it must
adeptly capture the intricate semantic relation of the action types and task
goals, along with the variable action sequences. Recently, notable progress has
been made via the integration of diffusion models and visual representation
learning to address the challenge. However, existing models employ rudimentary
mechanisms to utilize task information to manage the decision space. To
overcome this limitation, we introduce a simple yet effective enhancement - a
masked diffusion model. The introduced mask acts akin to a task-oriented
attention filter, enabling the diffusion/denoising process to concentrate on a
subset of action types. Furthermore, to bolster the accuracy of task
classification, we harness more potent visual representation learning
techniques. In particular, we learn a joint visual-text embedding, where a text
embedding is generated by prompting a pre-trained vision-language model to
focus on human actions. We evaluate the method on three public datasets and
achieve state-of-the-art performance on multiple metrics. Code is available at
https://github.com/ffzzy840304/Masked-PDPP.
</p></li>
</ul>

<h3>Title: DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks. (arXiv:2309.07509v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07509">http://arxiv.org/abs/2309.07509</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07509]] DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks(http://arxiv.org/abs/2309.07509)</code></li>
<li>Summary: <p>Generating realistic talking faces is a complex and widely discussed task
with numerous applications. In this paper, we present DiffTalker, a novel model
designed to generate lifelike talking faces through audio and landmark
co-driving. DiffTalker addresses the challenges associated with directly
applying diffusion models to audio control, which are traditionally trained on
text-image pairs. DiffTalker consists of two agent networks: a
transformer-based landmarks completion network for geometric accuracy and a
diffusion-based face generation network for texture details. Landmarks play a
pivotal role in establishing a seamless connection between the audio and image
domains, facilitating the incorporation of knowledge from pre-trained diffusion
models. This innovative approach efficiently produces articulate-speaking
faces. Experimental results showcase DiffTalker's superior performance in
producing clear and geometrically accurate talking faces, all without the need
for additional alignment between audio and image features.
</p></li>
</ul>

<h3>Title: Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07909">http://arxiv.org/abs/2309.07909</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07909]] Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch(http://arxiv.org/abs/2309.07909)</code></li>
<li>Summary: <p>Unsupervised contrastive learning methods have recently seen significant
improvements, particularly through data augmentation strategies that aim to
produce robust and generalizable representations. However, prevailing data
augmentation methods, whether hand designed or based on foundation models, tend
to rely heavily on prior knowledge or external data. This dependence often
compromises their effectiveness and efficiency. Furthermore, the applicability
of most existing data augmentation strategies is limited when transitioning to
other research domains, especially science-related data. This limitation stems
from the paucity of prior knowledge and labeled data available in these
domains. To address these challenges, we introduce DiffAug-a novel and
efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure
that the augmented and original data share a smoothed latent space, which is
achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug
first mines sufficient prior semantic knowledge about the neighborhood. This
provides a constraint to guide the diffusion steps, eliminating the need for
labels, external data/models, or prior knowledge. Designed as an
architecture-agnostic framework, DiffAug provides consistent improvements.
Specifically, it improves image classification and clustering accuracy by
1.6%~4.5%. When applied to biological data, DiffAug improves performance by up
to 10.1%, with an average improvement of 5.8%. DiffAug shows good performance
in both vision and biological domains.
</p></li>
</ul>

<h3>Title: Large-Vocabulary 3D Diffusion Model with Transformer. (arXiv:2309.07920v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07920">http://arxiv.org/abs/2309.07920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07920]] Large-Vocabulary 3D Diffusion Model with Transformer(http://arxiv.org/abs/2309.07920)</code></li>
<li>Summary: <p>Creating diverse and high-quality 3D assets with an automatic generative
model is highly desirable. Despite extensive efforts on 3D generation, most
existing works focus on the generation of a single category or a few
categories. In this paper, we introduce a diffusion-based feed-forward
framework for synthesizing massive categories of real-world 3D objects with a
single generative model. Notably, there are three major challenges for this
large-vocabulary 3D generation: a) the need for expressive yet efficient 3D
representation; b) large diversity in geometry and texture across categories;
c) complexity in the appearances of real-world objects. To this end, we propose
a novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for
handling challenges via three aspects. 1) Considering efficiency and
robustness, we adopt a revised triplane representation and improve the fitting
speed and accuracy. 2) To handle the drastic variations in geometry and
texture, we regard the features of all 3D objects as a combination of
generalized 3D knowledge and specialized 3D features. To extract generalized 3D
knowledge from diverse categories, we propose a novel 3D-aware transformer with
shared cross-plane attention. It learns the cross-plane relations across
different planes and aggregates the generalized 3D knowledge with specialized
3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance
the generalized 3D knowledge in the encoded triplanes for handling categories
with complex appearances. Extensive experiments on ShapeNet and OmniObject3D
(over 200 diverse real-world categories) convincingly demonstrate that a single
DiffTF model achieves state-of-the-art large-vocabulary 3D object generation
performance with large diversity, rich semantics, and high quality.
</p></li>
</ul>

<h3>Title: Beta quantile regression for robust estimation of uncertainty in the presence of outliers. (arXiv:2309.07374v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07374">http://arxiv.org/abs/2309.07374</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07374]] Beta quantile regression for robust estimation of uncertainty in the presence of outliers(http://arxiv.org/abs/2309.07374)</code></li>
<li>Summary: <p>Quantile Regression (QR) can be used to estimate aleatoric uncertainty in
deep neural networks and can generate prediction intervals. Quantifying
uncertainty is particularly important in critical applications such as clinical
diagnosis, where a realistic assessment of uncertainty is essential in
determining disease status and planning the appropriate treatment. The most
common application of quantile regression models is in cases where the
parametric likelihood cannot be specified. Although quantile regression is
quite robust to outlier response observations, it can be sensitive to outlier
covariate observations (features). Outlier features can compromise the
performance of deep learning regression problems such as style translation,
image reconstruction, and deep anomaly detection, potentially leading to
misleading conclusions. To address this problem, we propose a robust solution
for quantile regression that incorporates concepts from robust divergence. We
compare the performance of our proposed method with (i) least trimmed quantile
regression and (ii) robust regression based on the regularization of
case-specific parameters in a simple real dataset in the presence of outlier.
These methods have not been applied in a deep learning framework. We also
demonstrate the applicability of the proposed method by applying it to a
medical imaging translation task using diffusion models.
</p></li>
</ul>

<h3>Title: Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07867">http://arxiv.org/abs/2309.07867</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07867]] Beta Diffusion(http://arxiv.org/abs/2309.07867)</code></li>
<li>Summary: <p>We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Multi-Modal Hybrid Learning and Sequential Training for RGB-T Saliency Detection. (arXiv:2309.07297v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07297">http://arxiv.org/abs/2309.07297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07297]] Multi-Modal Hybrid Learning and Sequential Training for RGB-T Saliency Detection(http://arxiv.org/abs/2309.07297)</code></li>
<li>Summary: <p>RGB-T saliency detection has emerged as an important computer vision task,
identifying conspicuous objects in challenging scenes such as dark
environments. However, existing methods neglect the characteristics of
cross-modal features and rely solely on network structures to fuse RGB and
thermal features. To address this, we first propose a Multi-Modal Hybrid loss
(MMHL) that comprises supervised and self-supervised loss functions. The
supervised loss component of MMHL distinctly utilizes semantic features from
different modalities, while the self-supervised loss component reduces the
distance between RGB and thermal features. We further consider both spatial and
channel information during feature fusion and propose the Hybrid Fusion Module
to effectively fuse RGB and thermal features. Lastly, instead of jointly
training the network with cross-modal features, we implement a sequential
training strategy which performs training only on RGB images in the first stage
and then learns cross-modal features in the second stage. This training
strategy improves saliency detection performance without computational
overhead. Results from performance evaluation and ablation studies demonstrate
the superior performance achieved by the proposed method compared with the
existing state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Nucleus-aware Self-supervised Pretraining Using Unpaired Image-to-image Translation for Histopathology Images. (arXiv:2309.07394v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07394">http://arxiv.org/abs/2309.07394</a></li>
<li>Code URL: https://github.com/zhiyuns/UNITPathSSL</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07394]] Nucleus-aware Self-supervised Pretraining Using Unpaired Image-to-image Translation for Histopathology Images(http://arxiv.org/abs/2309.07394)</code></li>
<li>Summary: <p>Self-supervised pretraining attempts to enhance model performance by
obtaining effective features from unlabeled data, and has demonstrated its
effectiveness in the field of histopathology images. Despite its success, few
works concentrate on the extraction of nucleus-level information, which is
essential for pathologic analysis. In this work, we propose a novel
nucleus-aware self-supervised pretraining framework for histopathology images.
The framework aims to capture the nuclear morphology and distribution
information through unpaired image-to-image translation between histopathology
images and pseudo mask images. The generation process is modulated by both
conditional and stochastic style representations, ensuring the reality and
diversity of the generated histopathology images for pretraining. Further, an
instance segmentation guided strategy is employed to capture instance-level
information. The experiments on 7 datasets show that the proposed pretraining
method outperforms supervised ones on Kather classification, multiple instance
learning, and 5 dense-prediction tasks with the transfer learning protocol, and
yields superior results than other self-supervised approaches on 8
semi-supervised tasks. Our project is publicly available at
https://github.com/zhiyuns/UNITPathSSL.
</p></li>
</ul>

<h3>Title: CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders. (arXiv:2309.07707v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07707">http://arxiv.org/abs/2309.07707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07707]] CoLLD: Contrastive Layer-to-layer Distillation for Compressing Multilingual Pre-trained Speech Encoders(http://arxiv.org/abs/2309.07707)</code></li>
<li>Summary: <p>Large-scale self-supervised pre-trained speech encoders outperform
conventional approaches in speech recognition and translation tasks. Due to the
high cost of developing these large models, building new encoders for new tasks
and deploying them to on-device applications are infeasible. Prior studies
propose model compression methods to address this issue, but those works focus
on smaller models and less realistic tasks. Thus, we propose Contrastive
Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to
compress pre-trained speech encoders by leveraging masked prediction and
contrastive learning to train student models to copy the behavior of a large
teacher model. CoLLD outperforms prior methods and closes the gap between small
and large models on multilingual speech-to-text translation and recognition
benchmarks.
</p></li>
</ul>

<h3>Title: Hodge-Aware Contrastive Learning. (arXiv:2309.07364v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07364">http://arxiv.org/abs/2309.07364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07364]] Hodge-Aware Contrastive Learning(http://arxiv.org/abs/2309.07364)</code></li>
<li>Summary: <p>Simplicial complexes prove effective in modeling data with multiway
dependencies, such as data defined along the edges of networks or within other
higher-order structures. Their spectrum can be decomposed into three
interpretable subspaces via the Hodge decomposition, resulting foundational in
numerous applications. We leverage this decomposition to develop a contrastive
self-supervised learning approach for processing simplicial data and generating
embeddings that encapsulate specific spectral information.Specifically, we
encode the pertinent data invariances through simplicial neural networks and
devise augmentations that yield positive contrastive examples with suitable
spectral properties for downstream tasks. Additionally, we reweight the
significance of negative examples in the contrastive loss, considering the
similarity of their Hodge components to the anchor. By encouraging a stronger
separation among less similar instances, we obtain an embedding space that
reflects the spectral properties of the data. The numerical results on two
standard edge flow classification tasks show a superior performance even when
compared to supervised learning techniques. Our findings underscore the
importance of adopting a spectral perspective for contrastive learning with
higher-order data.
</p></li>
</ul>

<h3>Title: Learning Beyond Similarities: Incorporating Dissimilarities between Positive Pairs in Self-Supervised Time Series Learning. (arXiv:2309.07526v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07526">http://arxiv.org/abs/2309.07526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07526]] Learning Beyond Similarities: Incorporating Dissimilarities between Positive Pairs in Self-Supervised Time Series Learning(http://arxiv.org/abs/2309.07526)</code></li>
<li>Summary: <p>By identifying similarities between successive inputs, Self-Supervised
Learning (SSL) methods for time series analysis have demonstrated their
effectiveness in encoding the inherent static characteristics of temporal data.
However, an exclusive emphasis on similarities might result in representations
that overlook the dynamic attributes critical for modeling cardiovascular
diseases within a confined subject cohort. Introducing Distilled Encoding
Beyond Similarities (DEBS), this paper pioneers an SSL approach that transcends
mere similarities by integrating dissimilarities among positive pairs. The
framework is applied to electrocardiogram (ECG) signals, leading to a notable
enhancement of +10\% in the detection accuracy of Atrial Fibrillation (AFib)
across diverse subjects. DEBS underscores the potential of attaining a more
refined representation by encoding the dynamic characteristics of time series
data, tapping into dissimilarities during the optimization process. Broadly,
the strategy delineated in this study holds the promise of unearthing novel
avenues for advancing SSL methodologies tailored to temporal data.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning. (arXiv:2309.07911v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07911">http://arxiv.org/abs/2309.07911</a></li>
<li>Code URL: https://github.com/alibaba-mmai-research/dist</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07911]] Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning(http://arxiv.org/abs/2309.07911)</code></li>
<li>Summary: <p>Recently, large-scale pre-trained language-image models like CLIP have shown
extraordinary capabilities for understanding spatial contents, but naively
transferring such models to video recognition still suffers from unsatisfactory
temporal modeling capabilities. Existing methods insert tunable structures into
or in parallel with the pre-trained model, which either requires
back-propagation through the whole pre-trained model and is thus
resource-demanding, or is limited by the temporal reasoning capability of the
pre-trained structure. In this work, we present DiST, which disentangles the
learning of spatial and temporal aspects of videos. Specifically, DiST uses a
dual-encoder structure, where a pre-trained foundation model acts as the
spatial encoder, and a lightweight network is introduced as the temporal
encoder. An integration branch is inserted between the encoders to fuse
spatio-temporal information. The disentangled spatial and temporal learning in
DiST is highly efficient because it avoids the back-propagation of massive
pre-trained parameters. Meanwhile, we empirically show that disentangled
learning with an extra network for integration benefits both spatial and
temporal understanding. Extensive experiments on five benchmarks show that DiST
delivers better performance than existing state-of-the-art methods by
convincing gaps. When pre-training on the large-scale Kinetics-710, we achieve
89.7% on Kinetics-400 with a frozen ViT-L model, which verifies the scalability
of DiST. Codes and models can be found in
https://github.com/alibaba-mmai-research/DiST.
</p></li>
</ul>

<h3>Title: EarthPT: a foundation model for Earth Observation. (arXiv:2309.07207v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07207">http://arxiv.org/abs/2309.07207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07207]] EarthPT: a foundation model for Earth Observation(http://arxiv.org/abs/2309.07207)</code></li>
<li>Summary: <p>We introduce EarthPT -- an Earth Observation (EO) pretrained transformer.
EarthPT is a 700 million parameter decoding transformer foundation model
trained in an autoregressive self-supervised manner and developed specifically
with EO use-cases in mind. We demonstrate that EarthPT is an effective
forecaster that can accurately predict future pixel-level surface reflectances
across the 400-2300 nm range well into the future. For example, forecasts of
the evolution of the Normalised Difference Vegetation Index (NDVI) have a
typical error of approximately 0.05 (over a natural range of -1 -&gt; 1) at the
pixel level over a five month test set horizon, out-performing simple
phase-folded models based on historical averaging. We also demonstrate that
embeddings learnt by EarthPT hold semantically meaningful information and could
be exploited for downstream tasks such as highly granular, dynamic land use
classification. Excitingly, we note that the abundance of EO data provides us
with -- in theory -- quadrillions of training tokens. Therefore, if we assume
that EarthPT follows neural scaling laws akin to those derived for Large
Language Models (LLMs), there is currently no data-imposed limit to scaling
EarthPT and other similar `Large Observation Models.'
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: GAN-based Algorithm for Efficient Image Inpainting. (arXiv:2309.07293v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07293">http://arxiv.org/abs/2309.07293</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07293]] GAN-based Algorithm for Efficient Image Inpainting(http://arxiv.org/abs/2309.07293)</code></li>
<li>Summary: <p>Global pandemic due to the spread of COVID-19 has post challenges in a new
dimension on facial recognition, where people start to wear masks. Under such
condition, the authors consider utilizing machine learning in image inpainting
to tackle the problem, by complete the possible face that is originally covered
in mask. In particular, autoencoder has great potential on retaining important,
general features of the image as well as the generative power of the generative
adversarial network (GAN). The authors implement a combination of the two
models, context encoders and explain how it combines the power of the two
models and train the model with 50,000 images of influencers faces and yields a
solid result that still contains space for improvements. Furthermore, the
authors discuss some shortcomings with the model, their possible improvements,
as well as some area of study for future investigation for applicative
perspective, as well as directions to further enhance and refine the model.
</p></li>
</ul>

<h3>Title: SwitchGPT: Adapting Large Language Models for Non-Text Outputs. (arXiv:2309.07623v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07623">http://arxiv.org/abs/2309.07623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07623]] SwitchGPT: Adapting Large Language Models for Non-Text Outputs(http://arxiv.org/abs/2309.07623)</code></li>
<li>Summary: <p>Large Language Models (LLMs), primarily trained on text-based datasets,
exhibit exceptional proficiencies in understanding and executing complex
linguistic instructions via text outputs. However, they falter when requests to
generate non-text ones. Concurrently, modality conversion models, such as
text-to-image, despite generating high-quality images, suffer from a lack of
extensive textual pretraining. As a result, these models are only capable of
accommodating specific image descriptions rather than comprehending more
complex instructions. To bridge this gap, we propose a novel approach,
\methodname, from a modality conversion perspective that evolves a text-based
LLM into a multi-modal one. We specifically employ a minimal dataset to
instruct LLMs to recognize the intended output modality as directed by the
instructions. Consequently, the adapted LLM can effectively summon various
off-the-shelf modality conversion models from the model zoos to generate
non-text responses. This circumvents the necessity for complicated pretraining
that typically requires immense quantities of paired multi-modal data, while
simultaneously inheriting the extensive knowledge of LLMs and the ability of
high-quality generative models. To evaluate and compare the adapted multi-modal
LLM with its traditional counterparts, we have constructed a multi-modal
instruction benchmark that solicits diverse modality outputs. The experiment
results reveal that, with minimal training, LLMs can be conveniently adapted to
comprehend requests for non-text responses, thus achieving higher flexibility
in multi-modal scenarios. Code and data will be made available at
https://github.com/xinke-wang/SwitchGPT.
</p></li>
</ul>

<h3>Title: Dataset Condensation via Generative Model. (arXiv:2309.07698v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07698">http://arxiv.org/abs/2309.07698</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07698]] Dataset Condensation via Generative Model(http://arxiv.org/abs/2309.07698)</code></li>
<li>Summary: <p>Dataset condensation aims to condense a large dataset with a lot of training
samples into a small set. Previous methods usually condense the dataset into
the pixels format. However, it suffers from slow optimization speed and large
number of parameters to be optimized. When increasing image resolutions and
classes, the number of learnable parameters grows accordingly, prohibiting
condensation methods from scaling up to large datasets with diverse classes.
Moreover, the relations among condensed samples have been neglected and hence
the feature distribution of condensed samples is often not diverse. To solve
these problems, we propose to condense the dataset into another format, a
generative model. Such a novel format allows for the condensation of large
datasets because the size of the generative model remains relatively stable as
the number of classes or image resolution increases. Furthermore, an
intra-class and an inter-class loss are proposed to model the relation of
condensed samples. Intra-class loss aims to create more diverse samples for
each class by pushing each sample away from the others of the same class.
Meanwhile, inter-class loss increases the discriminability of samples by
widening the gap between the centers of different classes. Extensive
comparisons with state-of-the-art methods and our ablation studies confirm the
effectiveness of our method and its individual component. To our best
knowledge, we are the first to successfully conduct condensation on
ImageNet-1k.
</p></li>
</ul>

<h3>Title: Generative Image Dynamics. (arXiv:2309.07906v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07906">http://arxiv.org/abs/2309.07906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07906]] Generative Image Dynamics(http://arxiv.org/abs/2309.07906)</code></li>
<li>Summary: <p>We present an approach to modeling an image-space prior on scene dynamics.
Our prior is learned from a collection of motion trajectories extracted from
real video sequences containing natural, oscillating motion such as trees,
flowers, candles, and clothes blowing in the wind. Given a single image, our
trained model uses a frequency-coordinated diffusion sampling process to
predict a per-pixel long-term motion representation in the Fourier domain,
which we call a neural stochastic motion texture. This representation can be
converted into dense motion trajectories that span an entire video. Along with
an image-based rendering module, these trajectories can be used for a number of
downstream applications, such as turning still images into seamlessly looping
dynamic videos, or allowing users to realistically interact with objects in
real pictures.
</p></li>
</ul>

<h3>Title: Looking at words and points with attention: a benchmark for text-to-shape coherence. (arXiv:2309.07917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07917">http://arxiv.org/abs/2309.07917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07917]] Looking at words and points with attention: a benchmark for text-to-shape coherence(http://arxiv.org/abs/2309.07917)</code></li>
<li>Summary: <p>While text-conditional 3D object generation and manipulation have seen rapid
progress, the evaluation of coherence between generated 3D shapes and input
textual descriptions lacks a clear benchmark. The reason is twofold: a) the low
quality of the textual descriptions in the only publicly available dataset of
text-shape pairs; b) the limited effectiveness of the metrics used to
quantitatively assess such coherence. In this paper, we propose a comprehensive
solution that addresses both weaknesses. Firstly, we employ large language
models to automatically refine textual descriptions associated with shapes.
Secondly, we propose a quantitative metric to assess text-to-shape coherence,
through cross-attention mechanisms. To validate our approach, we conduct a user
study and compare quantitatively our metric with existing ones. The refined
dataset, the new metric and a set of text-shape pairs validated by the user
study comprise a novel, fine-grained benchmark that we publicly release to
foster research on text-to-shape coherence of text-conditioned 3D generative
models. Benchmark available at
https://cvlab-unibo.github.io/CrossCoherence-Web/.
</p></li>
</ul>

<h3>Title: Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text. (arXiv:2309.07689v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07689">http://arxiv.org/abs/2309.07689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07689]] Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text(http://arxiv.org/abs/2309.07689)</code></li>
<li>Summary: <p>While recent advancements in the capabilities and widespread accessibility of
generative language models, such as ChatGPT (OpenAI, 2022), have brought about
various benefits by generating fluent human-like text, the task of
distinguishing between human- and large language model (LLM) generated text has
emerged as a crucial problem. These models can potentially deceive by
generating artificial text that appears to be human-generated. This issue is
particularly significant in domains such as law, education, and science, where
ensuring the integrity of text is of the utmost importance. This survey
provides an overview of the current approaches employed to differentiate
between texts generated by humans and ChatGPT. We present an account of the
different datasets constructed for detecting ChatGPT-generated text, the
various methods utilized, what qualitative analyses into the characteristics of
human versus ChatGPT-generated text have been performed, and finally, summarize
our findings into general insights
</p></li>
</ul>

<h3>Title: Generative AI Text Classification using Ensemble LLM Approaches. (arXiv:2309.07755v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07755">http://arxiv.org/abs/2309.07755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07755]] Generative AI Text Classification using Ensemble LLM Approaches(http://arxiv.org/abs/2309.07755)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have shown impressive performance across a
variety of Artificial Intelligence (AI) and natural language processing tasks,
such as content creation, report generation, etc. However, unregulated malign
application of these models can create undesirable consequences such as
generation of fake news, plagiarism, etc. As a result, accurate detection of
AI-generated language can be crucial in responsible usage of LLMs. In this
work, we explore 1) whether a certain body of text is AI generated or written
by human, and 2) attribution of a specific language model in generating a body
of text. Texts in both English and Spanish are considered. The datasets used in
this study are provided as part of the Automated Text Identification
(AuTexTification) shared task. For each of the research objectives stated
above, we propose an ensemble neural model that generates probabilities from
different pre-trained LLMs which are used as features to a Traditional Machine
Learning (TML) classifier following it. For the first task of distinguishing
between AI and human generated text, our model ranked in fifth and thirteenth
place (with macro $F1$ scores of 0.733 and 0.649) for English and Spanish
texts, respectively. For the second task on model attribution, our model ranked
in first place with macro $F1$ scores of 0.625 and 0.653 for English and
Spanish texts, respectively.
</p></li>
</ul>

<h3>Title: Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context. (arXiv:2309.07708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07708">http://arxiv.org/abs/2309.07708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07708]] Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context(http://arxiv.org/abs/2309.07708)</code></li>
<li>Summary: <p>Financial simulators play an important role in enhancing forecasting
accuracy, managing risks, and fostering strategic financial decision-making.
Despite the development of financial market simulation methodologies, existing
frameworks often struggle with adapting to specialized simulation context. We
pinpoint the challenges as i) current financial datasets do not contain context
labels; ii) current techniques are not designed to generate financial data with
context as control, which demands greater precision compared to other
modalities; iii) the inherent difficulties in generating context-aligned,
high-fidelity data given the non-stationary, noisy nature of financial data. To
address these challenges, our contributions are: i) we proposed the Contextual
Market Dataset with market dynamics, stock ticker, and history state as
context, leveraging a market dynamics modeling method that combines linear
regression and Dynamic Time Warping clustering to extract market dynamics; ii)
we present Market-GAN, a novel architecture incorporating a Generative
Adversarial Networks (GAN) for the controllable generation with context, an
autoencoder for learning low-dimension features, and supervisors for knowledge
transfer; iii) we introduce a two-stage training scheme to ensure that
Market-GAN captures the intrinsic market distribution with multiple objectives.
In the pertaining stage, with the use of the autoencoder and supervisors, we
prepare the generator with a better initialization for the adversarial training
stage. We propose a set of holistic evaluation metrics that consider alignment,
fidelity, data usability on downstream tasks, and market facts. We evaluate
Market-GAN with the Dow Jones Industrial Average data from 2000 to 2023 and
showcase superior performance in comparison to 4 state-of-the-art time-series
generative models.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks. (arXiv:2309.07730v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07730">http://arxiv.org/abs/2309.07730</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07730]] AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks(http://arxiv.org/abs/2309.07730)</code></li>
<li>Summary: <p>Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used for
underwater environments and find applications in many areas. However, a lack of
security considerations, the unstable and challenging nature of the underwater
environment, and the resource-constrained nature of the sensor nodes used for
UW-ASNs (which makes them incapable of adopting security primitives) make the
UW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralised
Intrusion Detection and Prevention System called AIDPS for UW-ASNs. The
proposed AIDPS can improve the security of the UW-ASNs so that they can
efficiently detect underwater-related attacks (e.g., blackhole, grayhole and
flooding attacks). To determine the most effective configuration of the
proposed construction, we conduct a number of experiments using several
state-of-the-art machine learning algorithms (e.g., Adaptive Random Forest
(ARF), light gradient-boosting machine, and K-nearest neighbours) and concept
drift detection algorithms (e.g., ADWIN, kdqTree, and Page-Hinkley). Our
experimental results show that incremental ARF using ADWIN provides optimal
performance when implemented with One-class support vector machine (SVM)
anomaly-based detectors. Furthermore, our extensive evaluation results also
show that the proposed scheme outperforms state-of-the-art bench-marking
methods while providing a wider range of desirable features such as scalability
and complexity.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07915">http://arxiv.org/abs/2309.07915</a></li>
<li>Code URL: https://github.com/haozhezhao/mic</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07915]] MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning(http://arxiv.org/abs/2309.07915)</code></li>
<li>Summary: <p>Starting from the resurgence of deep learning, vision-language models (VLMs)
benefiting from large language models (LLMs) have never been so popular.
However, while LLMs can utilize extensive background knowledge and task
information with in-context learning, most VLMs still struggle with
understanding complex multi-modal prompts with multiple images. The issue can
traced back to the architectural design of VLMs or pre-training data.
Specifically, the current VLMs primarily emphasize utilizing multi-modal data
with a single image some, rather than multi-modal prompts with interleaved
multiple images and text. Even though some newly proposed VLMs could handle
user prompts with multiple images, pre-training data does not provide more
sophisticated multi-modal prompts than interleaved image and text crawled from
the web. We propose MMICL to address the issue by considering both the model
and data perspectives. We introduce a well-designed architecture capable of
seamlessly integrating visual and textual context in an interleaved manner and
MIC dataset to reduce the gap between the training data and the complex user
prompts in real-world applications, including: 1) multi-modal context with
interleaved images and text, 2) textual references for each image, and 3)
multi-image data with spatial, logical, or temporal relationships. Our
experiments confirm that MMICL achieves new stat-of-the-art zero-shot and
few-shot performance on a wide range of general vision-language tasks,
especially for complex reasoning benchmarks including MME and MMBench. Our
analysis demonstrates that MMICL effectively deals with the challenge of
complex multi-modal prompt understanding. The experiments on ScienceQA-IMG also
show that MMICL successfully alleviates the issue of language bias in VLMs,
which we believe is the reason behind the advanced performance of MMICL.
</p></li>
</ul>

<h3>Title: In-Contextual Bias Suppression for Large Language Models. (arXiv:2309.07251v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07251">http://arxiv.org/abs/2309.07251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07251]] In-Contextual Bias Suppression for Large Language Models(http://arxiv.org/abs/2309.07251)</code></li>
<li>Summary: <p>Despite their impressive performance in a wide range of NLP tasks, Large
Language Models (LLMs) have been reported to encode worrying-levels of gender
bias. Prior work has proposed debiasing methods that require human labelled
examples, data augmentation and fine-tuning of the LLMs, which are
computationally costly. Moreover, one might not even have access to the
internal parameters for performing debiasing such as in the case of
commercially available LLMs such as GPT-4. To address this challenge we propose
bias suppression, a novel alternative to debiasing that does not require access
to model parameters. We show that text-based preambles, generated from manually
designed templates covering counterfactual statements, can accurately suppress
gender biases in LLMs. Moreover, we find that descriptive sentences for
occupations can further suppress gender biases. Interestingly, we find that
bias suppression has a minimal adverse effect on downstream task performance,
while effectively mitigating the gender biases.
</p></li>
</ul>

<h3>Title: Ambiguity-Aware In-Context Learning with Large Language Models. (arXiv:2309.07900v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07900">http://arxiv.org/abs/2309.07900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07900]] Ambiguity-Aware In-Context Learning with Large Language Models(http://arxiv.org/abs/2309.07900)</code></li>
<li>Summary: <p>In-context learning (ICL) i.e. showing LLMs only a few task-specific
demonstrations has led to downstream gains with no task-specific fine-tuning
required. However, LLMs are sensitive to the choice of prompts, and therefore a
crucial research question is how to select good demonstrations for ICL. One
effective strategy is leveraging semantic similarity between the ICL
demonstrations and test inputs by using a text retriever, which however is
sub-optimal as that does not consider the LLM's existing knowledge about that
task. From prior work (Min et al., 2022), we already know that labels paired
with the demonstrations bias the model predictions. This leads us to our
hypothesis whether considering LLM's existing knowledge about the task,
especially with respect to the output label space can help in a better
demonstration selection strategy. Through extensive experimentation on three
text classification tasks, we find that it is beneficial to not only choose
semantically similar ICL demonstrations but also to choose those demonstrations
that help resolve the inherent label ambiguity surrounding the test example.
Interestingly, we find that including demonstrations that the LLM previously
mis-classified and also fall on the test example's decision boundary, brings
the most performance gain.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
