<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-15</h1>
<h3>Title: PRDP: Proximal Reward Difference Prediction for Large-Scale Reward  Finetuning of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08714">https://arxiv.org/abs/2402.08714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08714">https://arxiv.org/pdf/2402.08714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08714]] PRDP: Proximal Reward Difference Prediction for Large-Scale Reward  Finetuning of Diffusion Models(https://arxiv.org/abs/2402.08714)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text prompts from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen prompts whereas RL-based methods completely fail.</li>
</ul>

<h3>Title: Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs</h3>
<ul>
<li><strong>Authors: </strong>Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris J. Maddison</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08733">https://arxiv.org/abs/2402.08733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08733">https://arxiv.org/pdf/2402.08733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08733]] Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs(https://arxiv.org/abs/2402.08733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or "hallucinated" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to "cheat" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p(Y|X)$ and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques.</li>
</ul>

<h3>Title: Towards the Detection of AI-Synthesized Human Face Images</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Lu, Touradj Ebrahimi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08750">https://arxiv.org/abs/2402.08750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08750">https://arxiv.org/pdf/2402.08750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08750]] Towards the Detection of AI-Synthesized Human Face Images(https://arxiv.org/abs/2402.08750)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Over the past years, image generation and manipulation have achieved remarkable progress due to the rapid development of generative AI based on deep learning. Recent studies have devoted significant efforts to address the problem of face image manipulation caused by deepfake techniques. However, the problem of detecting purely synthesized face images has been explored to a lesser extent. In particular, the recent popular Diffusion Models (DMs) have shown remarkable success in image synthesis. Existing detectors struggle to generalize between synthesized images created by different generative models. In this work, a comprehensive benchmark including human face images produced by Generative Adversarial Networks (GANs) and a variety of DMs has been established to evaluate both the generalization ability and robustness of state-of-the-art detectors. Then, the forgery traces introduced by different generative models have been analyzed in the frequency domain to draw various insights. The paper further demonstrates that a detector trained with frequency representation can generalize well to other unseen generative models.</li>
</ul>

<h3>Title: Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal  Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Maurice Diesendruck, Jianzhe Lin, Shima Imani, Gayathri Mahalingam, Mingyang Xu, Jie Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08756">https://arxiv.org/abs/2402.08756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08756">https://arxiv.org/pdf/2402.08756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08756]] Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal  Foundation Models(https://arxiv.org/abs/2402.08756)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model, in-context</a></li>
<li><strong>Abstract: </strong>When LLMs perform zero-shot inference, they typically use a prompt with a task specification, and generate a completion. However, there is no work to explore the possibility of the reverse - going from completion to task specification. In this paper, we employ both directions to perform cycle-supervised learning entirely in-context. Our goal is to create a forward map f : X -> Y (e.g. image -> generated caption), coupled with a backward map g : Y -> X (e.g. caption -> generated image) to construct a cycle-consistency "loss" (formulated as an update to the prompt) to enforce g(f(X)) ~= X. The technique, called CyclePrompt, uses cycle-consistency as a free supervisory signal to iteratively craft the prompt. Importantly, CyclePrompt reinforces model performance without expensive fine-tuning, without training data, and without the complexity of external environments (e.g. compilers, APIs). We demonstrate CyclePrompt in two domains: code generation and image captioning. Our results on the HumanEval coding benchmark put us in first place on the leaderboard among models that do not rely on extra training data or usage of external environments, and third overall. Compared to the GPT4 baseline, we improve accuracy from 80.5% to 87.2%. In the vision-language space, we generate detailed image captions which outperform baseline zero-shot GPT4V captions, when tested against natural (VQAv2) and diagrammatic (FigureQA) visual question-answering benchmarks. To the best of our knowledge, this is the first use of self-supervised learning for prompting.</li>
</ul>

<h3>Title: LDTrack: Dynamic People Tracking by Service Robots using Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Angus Fung, Beno Benhabib, Goldie Nejat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08774">https://arxiv.org/abs/2402.08774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08774">https://arxiv.org/pdf/2402.08774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08774]] LDTrack: Dynamic People Tracking by Service Robots using Diffusion  Models(https://arxiv.org/abs/2402.08774)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Tracking of dynamic people in cluttered and crowded human-centered environments is a challenging robotics problem due to the presence of intraclass variations including occlusions, pose deformations, and lighting variations. This paper introduces a novel deep learning architecture, using conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for tracking multiple dynamic people under intraclass variations. By uniquely utilizing conditional latent diffusion models to capture temporal person embeddings, our architecture can adapt to appearance changes of people over time. We incorporated a latent feature encoder network which enables the diffusion process to operate within a high-dimensional latent space to allow for the extraction and spatial-temporal refinement of such rich features as person appearance, motion, location, identity, and contextual information. Extensive experiments demonstrate the effectiveness of LDTrack over other state-of-the-art tracking methods in cluttered and crowded human-centered environments under intraclass variations. Namely, the results show our method outperforms existing deep learning robotic people tracking methods in both tracking accuracy and tracking precision with statistical significance.</li>
</ul>

<h3>Title: Rethinking Machine Unlearning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08787">https://arxiv.org/abs/2402.08787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08787">https://arxiv.org/pdf/2402.08787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08787]] Rethinking Machine Unlearning for Large Language Models(https://arxiv.org/abs/2402.08787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.</li>
</ul>

<h3>Title: Improving Molecule Generation and Drug Discovery with a  Knowledge-enhanced Generative Model</h3>
<ul>
<li><strong>Authors: </strong>Aditya Malusare, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08790">https://arxiv.org/abs/2402.08790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08790">https://arxiv.org/pdf/2402.08790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08790]] Improving Molecule Generation and Drug Discovery with a  Knowledge-enhanced Generative Model(https://arxiv.org/abs/2402.08790)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative models have established state-of-the-art benchmarks in generating molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called K-DReAM. We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while ensuring validity and synthesizability. K-DReAM outperforms state-of-the-art generative models on both unconditional and targeted generation tasks.</li>
</ul>

<h3>Title: Learning to Generate Context-Sensitive Backchannel Smiles for Embodied  AI Agents with Applications in Mental Health Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Maneesh Bilalpur, Mert Inan, Dorsa Zeinali, Jeffrey F. Cohn, Malihe Alikhani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08837">https://arxiv.org/abs/2402.08837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08837">https://arxiv.org/pdf/2402.08837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08837]] Learning to Generate Context-Sensitive Backchannel Smiles for Embodied  AI Agents with Applications in Mental Health Dialogues(https://arxiv.org/abs/2402.08837)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Addressing the critical shortage of mental health resources for effective screening, diagnosis, and treatment remains a significant challenge. This scarcity underscores the need for innovative solutions, particularly in enhancing the accessibility and efficacy of therapeutic support. Embodied agents with advanced interactive capabilities emerge as a promising and cost-effective supplement to traditional caregiving methods. Crucial to these agents' effectiveness is their ability to simulate non-verbal behaviors, like backchannels, that are pivotal in establishing rapport and understanding in therapeutic contexts but remain under-explored. To improve the rapport-building capabilities of embodied agents we annotated backchannel smiles in videos of intimate face-to-face conversations over topics such as mental health, illness, and relationships. We hypothesized that both speaker and listener behaviors affect the duration and intensity of backchannel smiles. Using cues from speech prosody and language along with the demographics of the speaker and listener, we found them to contain significant predictors of the intensity of backchannel smiles. Based on our findings, we introduce backchannel smile production in embodied agents as a generation problem. Our attention-based generative model suggests that listener information offers performance improvements over the baseline speaker-centric generation approach. Conditioned generation using the significant predictors of smile intensity provides statistically significant improvements in empirical measures of generation quality. Our user study by transferring generated smiles to an embodied agent suggests that agent with backchannel smiles is perceived to be more human-like and is an attractive alternative for non-personal conversations over agent without backchannel smiles.</li>
</ul>

<h3>Title: TikTokActions: A TikTok-Derived Video Dataset for Human Action  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter Washington</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08875">https://arxiv.org/abs/2402.08875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08875">https://arxiv.org/pdf/2402.08875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08875]] TikTokActions: A TikTok-Derived Video Dataset for Human Action  Recognition(https://arxiv.org/abs/2402.08875)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The increasing variety and quantity of tagged multimedia content on platforms such as TikTok provides an opportunity to advance computer vision modeling. We have curated a distinctive dataset of 283,582 unique video clips categorized under 386 hashtags relating to modern human actions. We release this dataset as a valuable resource for building domain-specific foundation models for human movement modeling tasks such as action recognition. To validate this dataset, which we name TikTokActions, we perform two sets of experiments. First, we pretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on TikTokActions subset, and then fine-tune and evaluate on popular datasets such as UCF101 and the HMDB51. We find that the performance of the model pre-trained using our Tik-Tok dataset is comparable to models trained on larger action recognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, our investigation into the relationship between pre-training dataset size and fine-tuning performance reveals that beyond a certain threshold, the incremental benefit of larger training sets diminishes. This work introduces a useful TikTok video dataset that is available for public use and provides insights into the marginal benefit of increasing pre-training dataset sizes for video-based foundation models.</li>
</ul>

<h3>Title: Graph Inference Acceleration by Learning MLPs on Graphs without  Supervision</h3>
<ul>
<li><strong>Authors: </strong>Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08918">https://arxiv.org/abs/2402.08918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08918">https://arxiv.org/pdf/2402.08918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08918]] Graph Inference Acceleration by Learning MLPs on Graphs without  Supervision(https://arxiv.org/abs/2402.08918)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, we comprehensively analyze \textsc{SimMLP} to demonstrate its equivalence to GNNs in the optimal case and its generalization capability. Empirically, \textsc{SimMLP} outperforms state-of-the-art baselines, especially in settings with unseen nodes. In particular, it obtains significant performance gains {\bf (7$\sim$26\%)} over MLPs and inference acceleration over GNNs {\bf (90$\sim$126$\times$)} on large-scale graph datasets. Our codes are available at: \url{https://github.com/Zehong-Wang/SimMLP}.</li>
</ul>

<h3>Title: The Mirrored Influence Hypothesis: Efficient Data Influence Estimation  by Harnessing Forward Passes</h3>
<ul>
<li><strong>Authors: </strong>Myeongseob Ko, Feiyang Kang, Weiyan Shi, Ming Jin, Zhou Yu, Ruoxi Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08922">https://arxiv.org/abs/2402.08922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08922">https://arxiv.org/pdf/2402.08922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08922]] The Mirrored Influence Hypothesis: Efficient Data Influence Estimation  by Harnessing Forward Passes(https://arxiv.org/abs/2402.08922)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models. In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches. We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analysis of memorization, mislabeled data detection, and tracing behavior in language models. Our code will be made available at https://github.com/ruoxi-jia-group/Forward-INF.</li>
</ul>

<h3>Title: Towards Next-Level Post-Training Quantization of Hyper-Scale  Transformers</h3>
<ul>
<li><strong>Authors: </strong>Junhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon Jeon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08958">https://arxiv.org/abs/2402.08958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08958">https://arxiv.org/pdf/2402.08958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08958]] Towards Next-Level Post-Training Quantization of Hyper-Scale  Transformers(https://arxiv.org/abs/2402.08958)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive experiments on various language models and complexity analysis, we demonstrate that aespa is accurate and efficient in quantizing Transformer models.</li>
</ul>

<h3>Title: DUEL: Duplicate Elimination on Active Memory for Self-Supervised  Class-Imbalanced Learning</h3>
<ul>
<li><strong>Authors: </strong>Won-Seok Choi, Hyundo Lee, Dong-Sig Han, Junseok Park, Heeyeon Koo, Byoung-Tak Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08963">https://arxiv.org/abs/2402.08963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08963">https://arxiv.org/pdf/2402.08963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08963]] DUEL: Duplicate Elimination on Active Memory for Self-Supervised  Class-Imbalanced Learning(https://arxiv.org/abs/2402.08963)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources. On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information. To address class imbalances cost-efficiently, we propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory. The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances. We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its robustness and providing reliable results in downstream tasks. We also analyze the role of the DUEL policy in the training process through various metrics and visualizations.</li>
</ul>

<h3>Title: Research and application of Transformer based anomaly detection model: A  literature review</h3>
<ul>
<li><strong>Authors: </strong>Mingrui Ma, Lansheng Han, Chunjie Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.08975">https://arxiv.org/abs/2402.08975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.08975">https://arxiv.org/pdf/2402.08975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.08975]] Research and application of Transformer based anomaly detection model: A  literature review(https://arxiv.org/abs/2402.08975)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Transformer, as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of anomaly detection. To inspire research on Transformer-based anomaly detection, this review offers a fresh perspective on the concept of anomaly detection. We explore the current challenges of anomaly detection and provide detailed insights into the operating principles of Transformer and its variants in anomaly detection tasks. Additionally, we delineate various application scenarios for Transformer-based anomaly detection models and discuss the datasets and evaluation metrics employed. Furthermore, this review highlights the key challenges in Transformer-based anomaly detection research and conducts a comprehensive analysis of future research trends in this domain. The review includes an extensive compilation of over 100 core references related to Transformer-based anomaly detection. To the best of our knowledge, this is the first comprehensive review that focuses on the research related to Transformer in the context of anomaly detection. We hope that this paper can provide detailed technical information to researchers interested in Transformer-based anomaly detection tasks.</li>
</ul>

<h3>Title: Review-Incorporated Model-Agnostic Profile Injection Attacks on  Recommender Systems</h3>
<ul>
<li><strong>Authors: </strong>Shiyi Yang, Lina Yao, Chen Wang, Xiwei Xu, Liming Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09023">https://arxiv.org/abs/2402.09023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09023">https://arxiv.org/pdf/2402.09023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09023]] Review-Incorporated Model-Agnostic Profile Injection Attacks on  Recommender Systems(https://arxiv.org/abs/2402.09023)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks. Understanding attack tactics helps improve the robustness of RSs. We intend to develop efficient attack methods that use limited resources to generate high-quality fake user profiles to achieve 1) transferability among black-box RSs 2) and imperceptibility among detectors. In order to achieve these goals, we introduce textual reviews of products to enhance the generation quality of the profiles. Specifically, we propose a novel attack framework named R-Trojan, which formulates the attack objectives as an optimization problem and adopts a tailored transformer-based generative adversarial network (GAN) to solve it so that high-quality attack profiles can be produced. Comprehensive experiments on real-world datasets demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods on various victim RSs under black-box settings and show its good imperceptibility.</li>
</ul>

<h3>Title: Can Text-to-image Model Assist Multi-modal Learning for Visual  Recognition with Visual Modality Missing?</h3>
<ul>
<li><strong>Authors: </strong>Tiantian Feng, Daniel Yang, Digbalay Bose, Shrikanth Narayanan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09036">https://arxiv.org/abs/2402.09036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09036">https://arxiv.org/pdf/2402.09036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09036]] Can Text-to-image Model Assist Multi-modal Learning for Visual  Recognition with Visual Modality Missing?(https://arxiv.org/abs/2402.09036)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multi-modal learning has emerged as an increasingly promising avenue in vision recognition, driving innovations across diverse domains ranging from media and education to healthcare and transportation. Despite its success, the robustness of multi-modal learning for visual recognition is often challenged by the unavailability of a subset of modalities, especially the visual modality. Conventional approaches to mitigate missing modalities in multi-modal learning rely heavily on algorithms and modality fusion schemes. In contrast, this paper explores the use of text-to-image models to assist multi-modal learning. Specifically, we propose a simple but effective multi-modal learning framework GTI-MM to enhance the data efficiency and model robustness against missing visual modality by imputing the missing data with generative transformers. Using multiple multi-modal datasets with visual recognition tasks, we present a comprehensive analysis of diverse conditions involving missing visual modality in data, including model training. Our findings reveal that synthetic images benefit training data efficiency with visual data missing in training and improve model robustness with visual data missing involving training and testing. Moreover, we demonstrate GTI-MM is effective with lower generation quantity and simple prompt techniques.</li>
</ul>

<h3>Title: Affine transformation estimation improves visual self-supervised  learning</h3>
<ul>
<li><strong>Authors: </strong>David Torpey, Richard Klein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09071">https://arxiv.org/abs/2402.09071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09071">https://arxiv.org/pdf/2402.09071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09071]] Affine transformation estimation improves visual self-supervised  learning(https://arxiv.org/abs/2402.09071)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The standard approach to modern self-supervised learning is to generate random views through data augmentations and minimise a loss computed from the representations of these views. This inherently encourages invariance to the transformations that comprise the data augmentation function. In this work, we show that adding a module to constrain the representations to be predictive of an affine transformation improves the performance and efficiency of the learning process. The module is agnostic to the base self-supervised model and manifests in the form of an additional loss term that encourages an aggregation of the encoder representations to be predictive of an affine transformation applied to the input images. We perform experiments in various modern self-supervised models and see a performance improvement in all cases. Further, we perform an ablation study on the components of the affine transformation to understand which of them is affecting performance the most, as well as on key architectural design decisions.</li>
</ul>

<h3>Title: Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?</h3>
<ul>
<li><strong>Authors: </strong>Tommaso Puccetti, Andrea Ceccarelli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09082">https://arxiv.org/abs/2402.09082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09082">https://arxiv.org/pdf/2402.09082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09082]] Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?(https://arxiv.org/abs/2402.09082)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The ever-evolving landscape of attacks, coupled with the growing complexity of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error detectors (ED) a difficult task: they must accurately detect attacks, and they should promptly perform detections. Although improving and comparing the detection capability is the focus of most research works, the timeliness of the detection is less considered and often insufficiently evaluated or discussed. In this paper, we argue the relevance of measuring the temporal latency of attacks and errors, and we propose an evaluation approach for detectors to ensure a pragmatic trade-off between correct and in-time detection. Briefly, the approach relates the false positive rate with the temporal latency of attacks and errors, and this ultimately leads to guidelines for configuring a detector. We apply our approach by evaluating different ED and ID solutions in two industrial cases: i) an embedded railway on-board system that optimizes public mobility, and ii) an edge device for the Industrial Internet of Things. Our results show that considering latency in addition to traditional metrics like the false positive rate, precision, and coverage gives an additional fundamental perspective on the actual performance of the detector and should be considered when assessing and configuring anomaly detectors.</li>
</ul>

<h3>Title: Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Ghorbani Lohesara, Karen Egiazarian, Sebastian Knorr</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09100">https://arxiv.org/abs/2402.09100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09100">https://arxiv.org/pdf/2402.09100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09100]] Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs(https://arxiv.org/abs/2402.09100)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup. This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions. In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence. Our study introduces a network designed for expression-based video inpainting, employing generative adversarial networks (GANs) to handle static and moving occlusions across all frames. By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user's identity consistently across frames. We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs. Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results.</li>
</ul>

<h3>Title: Learning Interpretable Concepts: Unifying Causal Representation Learning  and Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Schölkopf, Pradeep Ravikumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09236">https://arxiv.org/abs/2402.09236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09236">https://arxiv.org/pdf/2402.09236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09236]] Learning Interpretable Concepts: Unifying Causal Representation Learning  and Foundation Models(https://arxiv.org/abs/2402.09236)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.</li>
</ul>

<h3>Title: Weatherproofing Retrieval for Localization with Generative AI and  Geometric Consistency</h3>
<ul>
<li><strong>Authors: </strong>Yannis Kalantidis, Mert Bülent Sarıyıldız, Rafael S. Rezende, Philippe Weinzaepfel, Diane Larlus, Gabriela Csurka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09237">https://arxiv.org/abs/2402.09237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09237">https://arxiv.org/pdf/2402.09237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09237]] Weatherproofing Retrieval for Localization with Generative AI and  Geometric Consistency(https://arxiv.org/abs/2402.09237)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we improve this retrieval step and tailor it to the final localization task. Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from generative text-to-image models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization. After expanding the training set, we propose a training approach that leverages the specificities and the underlying geometry of this mix of real and synthetic images. We experimentally show that those changes translate into large improvements for the most challenging visual localization datasets. Project page: https://europe.naverlabs.com/ret4loc</li>
</ul>

<h3>Title: Switch EMA: A Free Lunch for Better Flatness and Sharpness</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Li, Zicheng Liu, Juanxi Tian, Ge Wang, Zedong Wang, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09240">https://arxiv.org/abs/2402.09240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09240">https://arxiv.org/pdf/2402.09240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09240]] Switch EMA: A Free Lunch for Better Flatness and Sharpness(https://arxiv.org/abs/2402.09240)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.</li>
</ul>

<h3>Title: Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food  Detection</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Zhou, Weiqing Min, Jiajun Song, Yang Zhang, Shuqiang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09242">https://arxiv.org/abs/2402.09242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09242">https://arxiv.org/pdf/2402.09242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09242]] Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food  Detection(https://arxiv.org/abs/2402.09242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health. As a fundamental task in food computing, food detection needs Zero-Shot Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants. Therefore, we first benchmark the task of Zero-Shot Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations. Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable. The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories. To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes. Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source graphs to provide prior knowledge for distinguishing fine-grained features. Within ZSFDet, Knowledge-Enhanced Feature Synthesizer (KEFS) learns knowledge representation from multiple sources (e.g., ingredients correlation from knowledge graph) via the multi-source graph fusion. Conditioned on the fusion of semantic knowledge representation, the region feature diffusion model in KEFS can generate fine-grained features for training the effective zero-shot detector. Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS. Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic knowledge can also improve the performance on general ZSD. Code and dataset are available at https://github.com/LanceZPF/KEFS.</li>
</ul>

<h3>Title: Embracing the black box: Heading towards foundation models for causal  discovery from time series data</h3>
<ul>
<li><strong>Authors: </strong>Gideon Stein, Maha Shadaydeh, Joachim Denzler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09305">https://arxiv.org/abs/2402.09305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09305">https://arxiv.org/pdf/2402.09305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09305]] Embracing the black box: Heading towards foundation models for causal  discovery from time series data(https://arxiv.org/abs/2402.09305)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner. Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We argue that this hints at the possibility of a foundation model for causal discovery.</li>
</ul>

<h3>Title: ICDPO: Effectively Borrowing Alignment Capability of Others via  In-context Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Feifan Song, Yuxuan Fan, Xin Zhang, Peiyi Wang, Houfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09320">https://arxiv.org/abs/2402.09320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09320">https://arxiv.org/pdf/2402.09320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09320]] ICDPO: Effectively Borrowing Alignment Capability of Others via  In-context Direct Preference Optimization(https://arxiv.org/abs/2402.09320)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL). Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA. We also conduct detailed analyses to offer comprehensive insights into ICDPO.</li>
</ul>

<h3>Title: HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM  Inference</h3>
<ul>
<li><strong>Authors: </strong>Yashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09360">https://arxiv.org/abs/2402.09360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09360">https://arxiv.org/pdf/2402.09360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09360]] HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM  Inference(https://arxiv.org/abs/2402.09360)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top-$k$ rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate top-$k$ operator. We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by $1.47\times$ on a single TPUv5e device.</li>
</ul>

<h3>Title: Magic-Me: Identity-Specific Video Customized Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09368">https://arxiv.org/abs/2402.09368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09368">https://arxiv.org/pdf/2402.09368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09368]] Magic-Me: Identity-Specific Video Customized Diffusion(https://arxiv.org/abs/2402.09368)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution. Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with finetuned text-to-image models available publically, further improving its usability. The codes are available at https://github.com/Zhen-Dong/Magic-Me.</li>
</ul>

<h3>Title: GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in  Metagenomic Assembly</h3>
<ul>
<li><strong>Authors: </strong>Ali Azizpour, Advait Balaji, Todd J. Treangen, Santiago Segarra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09381">https://arxiv.org/abs/2402.09381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09381">https://arxiv.org/pdf/2402.09381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09381]] GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in  Metagenomic Assembly(https://arxiv.org/abs/2402.09381)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment. This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities. Detecting repeats is a crucial first step in overcoming these challenges. To address this issue, we propose GraSSRep, a novel approach that leverages the assembly graph's structure through graph neural networks (GNNs) within a self-supervised learning framework to classify DNA sequences into repetitive and non-repetitive categories. Specifically, we frame this problem as a node classification task within a metagenomic assembly graph. In a self-supervised fashion, we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the nodes. We then use those pseudo-labels to train a GNN embedding and a random forest classifier to propagate the labels to the remaining nodes. In this way, GraSSRep combines sequencing features with pre-defined and learned graph features to achieve state-of-the-art performance in repeat detection. We evaluate our method using simulated and synthetic metagenomic datasets. The results on the simulated data highlight our GraSSRep's robustness to repeat attributes, demonstrating its effectiveness in handling the complexity of repeated sequences. Additionally, our experiments with synthetic metagenomic datasets reveal that incorporating the graph structure and the GNN enhances our detection performance. Finally, in comparative analyses, GraSSRep outperforms existing repeat detection tools with respect to precision and recall.</li>
</ul>

<h3>Title: Long-form evaluation of model editing</h3>
<ul>
<li><strong>Authors: </strong>Domenic Rosati, Robie Gonzales, Jinkun Chen, Xuemin Yu, Melis Erkan, Yahya Kayani, Satya Deepika Chavatapalli, Frank Rudzicz, Hassan Sajjad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09394">https://arxiv.org/abs/2402.09394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09394">https://arxiv.org/pdf/2402.09394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09394]] Long-form evaluation of model editing(https://arxiv.org/abs/2402.09394)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluations of model editing currently only use the `next few token' completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (\textbf{\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods. Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues.</li>
</ul>

<h3>Title: Reinforcement Learning from Human Feedback with Active Queries</h3>
<ul>
<li><strong>Authors: </strong>Kaixuan Ji, Jiafan He, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09401">https://arxiv.org/abs/2402.09401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09401">https://arxiv.org/pdf/2402.09401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09401]] Reinforcement Learning from Human Feedback with Active Queries(https://arxiv.org/abs/2402.09401)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
