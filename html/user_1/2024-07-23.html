<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-23</h1>
<h3>Title: A Foundation Model for Soccer</h3>
<ul>
<li><strong>Authors: </strong>Ethan Baron, Daniel Hocevar, Zach Salehe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14558">https://arxiv.org/abs/2407.14558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14558">https://arxiv.org/pdf/2407.14558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14558]] A Foundation Model for Soccer(https://arxiv.org/abs/2407.14558)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We propose a foundation model for soccer, which is able to predict subsequent actions in a soccer match from a given input sequence of actions. As a proof of concept, we train a transformer architecture on three seasons of data from a professional soccer league. We quantitatively and qualitatively compare the performance of this transformer architecture to two baseline models: a Markov model and a multi-layer perceptron. Additionally, we discuss potential applications of our model. We provide an open-source implementation of our methods at this https URL.</li>
</ul>

<h3>Title: NNsight and NDIF: Democratizing Access to Foundation Model Internals</h3>
<ul>
<li><strong>Authors: </strong>Jaden Fiotto-Kaufman, Alexander R Loftus, Eric Todd, Jannik Brinkmann, Caden Juang, Koyena Pal, Can Rager, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Michael Ripa, Adam Belfki, Nikhil Prakash, Sumeet Multani, Carla Brodley, Arjun Guha, Jonathan Bell, Byron Wallace, David Bau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14561">https://arxiv.org/abs/2407.14561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14561">https://arxiv.org/pdf/2407.14561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14561]] NNsight and NDIF: Democratizing Access to Foundation Model Internals(https://arxiv.org/abs/2407.14561)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The enormous scale of state-of-the-art foundation models has limited their accessibility to scientists, because customized experiments at large model sizes require costly hardware and complex engineering that is impractical for most researchers. To alleviate these problems, we introduce NNsight, an open-source Python package with a simple, flexible API that can express interventions on any PyTorch model by building computation graphs. We also introduce NDIF, a collaborative research platform providing researchers access to foundation-scale LLMs via the NNsight API. Code, documentation, and tutorials are available at this https URL.</li>
</ul>

<h3>Title: Learning Visual Grounding from Generative Vision and Language Model</h3>
<ul>
<li><strong>Authors: </strong>Shijie Wang, Dahun Kim, Ali Taalimi, Chen Sun, Weicheng Kuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14563">https://arxiv.org/abs/2407.14563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14563">https://arxiv.org/pdf/2407.14563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14563]] Learning Visual Grounding from Generative Vision and Language Model(https://arxiv.org/abs/2407.14563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual grounding tasks aim to localize image regions based on natural language references. In this work, we explore whether generative VLMs predominantly trained on image-text data could be leveraged to scale up the text annotation of visual grounding data. We find that grounding knowledge already exists in generative VLM and can be elicited by proper prompting. We thus prompt a VLM to generate object-level descriptions by feeding it object regions from existing object detection datasets. We further propose attribute modeling to explicitly capture the important object attributes, and spatial relation modeling to capture inter-object relationship, both of which are common linguistic pattern in referring expression. Our constructed dataset (500K images, 1M objects, 16M referring expressions) is one of the largest grounding datasets to date, and the first grounding dataset with purely model-generated queries and human-annotated objects. To verify the quality of this data, we conduct zero-shot transfer experiments to the popular RefCOCO benchmarks for both referring expression comprehension (REC) and segmentation (RES) tasks. On both tasks, our model significantly outperform the state-of-the-art approaches without using human annotated visual grounding data. Our results demonstrate the promise of generative VLM to scale up visual grounding in the real world. Code and models will be released.</li>
</ul>

<h3>Title: Trading Devil Final: Backdoor attack via Stock market and Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Orson Mengara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, q-fin.CP, q-fin.PR, q-fin.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14573">https://arxiv.org/abs/2407.14573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14573">https://arxiv.org/pdf/2407.14573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14573]] Trading Devil Final: Backdoor attack via Stock market and Bayesian Optimization(https://arxiv.org/abs/2407.14573)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Since the advent of generative artificial intelligence, every company and researcher has been rushing to develop their own generative models, whether commercial or not. Given the large number of users of these powerful new tools, there is currently no intrinsically verifiable way to explain from the ground up what happens when LLMs (large language models) learn. For example, those based on automatic speech recognition systems, which have to rely on huge and astronomical amounts of data collected from all over the web to produce fast and efficient results, In this article, we develop a backdoor attack called MarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is mainly based on modern stock market models. In order to show the possible vulnerabilities of speech-based transformers that may rely on LLMs.</li>
</ul>

<h3>Title: Advancing Melanoma Diagnosis with Self-Supervised Neural Networks: Evaluating the Effectiveness of Different Techniques</h3>
<ul>
<li><strong>Authors: </strong>Srivishnu Vusirikala, Suraj Rajendran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14628">https://arxiv.org/abs/2407.14628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14628">https://arxiv.org/pdf/2407.14628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14628]] Advancing Melanoma Diagnosis with Self-Supervised Neural Networks: Evaluating the Effectiveness of Different Techniques(https://arxiv.org/abs/2407.14628)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We investigate the potential of self-supervision in improving the accuracy of deep learning models trained to classify melanoma patches. Various self-supervision techniques such as rotation prediction, missing patch prediction, and corruption removal were implemented and assessed for their impact on the convolutional neural network's performance. Preliminary results suggest a positive influence of self-supervision methods on the model's accuracy. The study notably demonstrates the efficacy of the corruption removal method in enhancing model performance. Despite observable improvements, we conclude that the self-supervised models have considerable potential for further enhancement, achievable through training over more epochs or expanding the dataset. We suggest exploring other self-supervision methods like Bootstrap Your Own Latent (BYOL) and contrastive learning in future research, emphasizing the cost-benefit trade-off due to their resource-intensive nature. The findings underline the promise of self-supervision in augmenting melanoma detection capabilities of deep learning models.</li>
</ul>

<h3>Title: CVE-LLM : Automatic vulnerability evaluation in medical device industry using large language models</h3>
<ul>
<li><strong>Authors: </strong>Rikhiya Ghosh, Oladimeji Farri, Hans-Martin von Stockhausen, Martin Schmitt, George Marica Vasile</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14640">https://arxiv.org/abs/2407.14640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14640">https://arxiv.org/pdf/2407.14640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14640]] CVE-LLM : Automatic vulnerability evaluation in medical device industry using large language models(https://arxiv.org/abs/2407.14640)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The healthcare industry is currently experiencing an unprecedented wave of cybersecurity attacks, impacting millions of individuals. With the discovery of thousands of vulnerabilities each month, there is a pressing need to drive the automation of vulnerability assessment processes for medical devices, facilitating rapid mitigation efforts. Generative AI systems have revolutionized various industries, offering unparalleled opportunities for automation and increased efficiency. This paper presents a solution leveraging Large Language Models (LLMs) to learn from historical evaluations of vulnerabilities for the automatic assessment of vulnerabilities in the medical devices industry. This approach is applied within the portfolio of a single manufacturer, taking into account device characteristics, including existing security posture and controls. The primary contributions of this paper are threefold. Firstly, it provides a detailed examination of the best practices for training a vulnerability Language Model (LM) in an industrial context. Secondly, it presents a comprehensive comparison and insightful analysis of the effectiveness of Language Models in vulnerability assessment. Finally, it proposes a new human-in-the-loop framework to expedite vulnerability evaluation processes.</li>
</ul>

<h3>Title: OASIS: Conditional Distribution Shaping for Offline Safe Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yihang Yao, Zhepeng Cen, Wenhao Ding, Haohong Lin, Shiqi Liu, Tingnan Zhang, Wenhao Yu, Ding Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14653">https://arxiv.org/abs/2407.14653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14653">https://arxiv.org/pdf/2407.14653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14653]] OASIS: Conditional Distribution Shaping for Offline Safe Reinforcement Learning(https://arxiv.org/abs/2407.14653)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Offline safe reinforcement learning (RL) aims to train a policy that satisfies constraints using a pre-collected dataset. Most current methods struggle with the mismatch between imperfect demonstrations and the desired safe and rewarding performance. In this paper, we introduce OASIS (cOnditionAl diStributIon Shaping), a new paradigm in offline safe RL designed to overcome these critical limitations. OASIS utilizes a conditional diffusion model to synthesize offline datasets, thus shaping the data distribution toward a beneficial target domain. Our approach makes compliance with safety constraints through effective data utilization and regularization techniques to benefit offline safe RL training. Comprehensive evaluations on public benchmarks and varying datasets showcase OASIS's superiority in benefiting offline safe RL agents to achieve high-reward behavior while satisfying the safety constraints, outperforming established baselines. Furthermore, OASIS exhibits high data efficiency and robustness, making it suitable for real-world applications, particularly in tasks where safety is imperative and high-quality demonstrations are scarce.</li>
</ul>

<h3>Title: On Learning Discriminative Features from Synthesized Data for Self-Supervised Fine-Grained Visual Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zihu Wang, Lingqiao Liu, Scott Ricardo Figueroa Weston, Samuel Tian, Peng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14676">https://arxiv.org/abs/2407.14676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14676">https://arxiv.org/pdf/2407.14676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14676]] On Learning Discriminative Features from Synthesized Data for Self-Supervised Fine-Grained Visual Recognition(https://arxiv.org/abs/2407.14676)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-Supervised Learning (SSL) has become a prominent approach for acquiring visual representations across various tasks, yet its application in fine-grained visual recognition (FGVR) is challenged by the intricate task of distinguishing subtle differences between categories. To overcome this, we introduce an novel strategy that boosts SSL's ability to extract critical discriminative features vital for FGVR. This approach creates synthesized data pairs to guide the model to focus on discriminative features critical for FGVR during SSL. We start by identifying non-discriminative features using two main criteria: features with low variance that fail to effectively separate data and those deemed less important by Grad-CAM induced from the SSL loss. We then introduce perturbations to these non-discriminative features while preserving discriminative ones. A decoder is employed to reconstruct images from both perturbed and original feature vectors to create data pairs. An encoder is trained on such generated data pairs to become invariant to variations in non-discriminative dimensions while focusing on discriminative features, thereby improving the model's performance in FGVR tasks. We demonstrate the promising FGVR performance of the proposed approach through extensive evaluation on a wide variety of datasets.</li>
</ul>

<h3>Title: $\infty$-Brush: Controllable Large Image Synthesis with Diffusion Models in Infinite Dimensions</h3>
<ul>
<li><strong>Authors: </strong>Minh-Quan Le, Alexandros Graikos, Srikar Yellapragada, Rajarsi Gupta, Joel Saltz, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14709">https://arxiv.org/abs/2407.14709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14709">https://arxiv.org/pdf/2407.14709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14709]] $\infty$-Brush: Controllable Large Image Synthesis with Diffusion Models in Infinite Dimensions(https://arxiv.org/abs/2407.14709)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Synthesizing high-resolution images from intricate, domain-specific information remains a significant challenge in generative modeling, particularly for applications in large-image domains such as digital histopathology and remote sensing. Existing methods face critical limitations: conditional diffusion models in pixel or latent space cannot exceed the resolution on which they were trained without losing fidelity, and computational demands increase significantly for larger image sizes. Patch-based methods offer computational efficiency but fail to capture long-range spatial relationships due to their overreliance on local information. In this paper, we introduce a novel conditional diffusion model in infinite dimensions, $\infty$-Brush for controllable large image synthesis. We propose a cross-attention neural operator to enable conditioning in function space. Our model overcomes the constraints of traditional finite-dimensional diffusion models and patch-based methods, offering scalability and superior capability in preserving global image structures while maintaining fine details. To our best knowledge, $\infty$-Brush is the first conditional diffusion model in function space, that can controllably synthesize images at arbitrary resolutions of up to $4096\times4096$ pixels. The code is available at this https URL.</li>
</ul>

<h3>Title: Differential Privacy of Cross-Attention with Provable Guarantee</h3>
<ul>
<li><strong>Authors: </strong>Jiuxiang Gu, Yingyu Liang, Zhenmei Shi, Zhao Song, Yufa Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14717">https://arxiv.org/abs/2407.14717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14717">https://arxiv.org/pdf/2407.14717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14717]] Differential Privacy of Cross-Attention with Provable Guarantee(https://arxiv.org/abs/2407.14717)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Cross-attention has become a fundamental module nowadays in many important artificial intelligence applications, e.g., retrieval-augmented generation (RAG), system prompt, guided stable diffusion, and many so on. Ensuring cross-attention privacy is crucial and urgently needed because its key and value matrices may contain sensitive information about companies and their users, many of which profit solely from their system prompts or RAG data. In this work, we design a novel differential privacy (DP) data structure to address the privacy security of cross-attention with a theoretical guarantee. In detail, let $n$ be the input token length of system prompt/RAG data, $d$ be the feature dimension, $0 < \alpha \le 1$ be the relative error parameter, $R$ be the maximum value of the query and key matrices, $R_w$ be the maximum value of the value matrix, and $r,s,\epsilon_s$ be parameters of polynomial kernel methods. Then, our data structure requires $\widetilde{O}(ndr^2)$ memory consumption with $\widetilde{O}(nr^2)$ initialization time complexity and $\widetilde{O}(\alpha^{-1} r^2)$ query time complexity for a single token query. In addition, our data structure can guarantee that the user query is $(\epsilon, \delta)$-DP with $\widetilde{O}(n^{-1} \epsilon^{-1} \alpha^{-1/2} R^{2s} R_w r^2)$ additive error and $n^{-1} (\alpha + \epsilon_s)$ relative error between our output and the true answer. Furthermore, our result is robust to adaptive queries in which users can intentionally attack the cross-attention system. To our knowledge, this is the first work to provide DP for cross-attention. We believe it can inspire more privacy algorithm design in large generative models (LGMs).</li>
</ul>

<h3>Title: FedDM: Enhancing Communication Efficiency and Handling Data Heterogeneity in Federated Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jayneel Vora, Nader Bouacida, Aditya Krishnan, Prasant Mohapatra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14730">https://arxiv.org/abs/2407.14730</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14730">https://arxiv.org/pdf/2407.14730</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14730]] FedDM: Enhancing Communication Efficiency and Handling Data Heterogeneity in Federated Diffusion Models(https://arxiv.org/abs/2407.14730)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce FedDM, a novel training framework designed for the federated training of diffusion models. Our theoretical analysis establishes the convergence of diffusion models when trained in a federated setting, presenting the specific conditions under which this convergence is guaranteed. We propose a suite of training algorithms that leverage the U-Net architecture as the backbone for our diffusion models. These include a basic Federated Averaging variant, FedDM-vanilla, FedDM-prox to handle data heterogeneity among clients, and FedDM-quant, which incorporates a quantization module to reduce the model update size, thereby enhancing communication efficiency across the federated network. We evaluate our algorithms on FashionMNIST (28x28 resolution), CIFAR-10 (32x32 resolution), and CelebA (64x64 resolution) for DDPMs, as well as LSUN Church Outdoors (256x256 resolution) for LDMs, focusing exclusively on the imaging modality. Our evaluation results demonstrate that FedDM algorithms maintain high generation quality across image resolutions. At the same time, the use of quantized updates and proximal terms in the local training objective significantly enhances communication efficiency (up to 4x) and model convergence, particularly in non-IID data settings, at the cost of increased FID scores (up to 1.75x).</li>
</ul>

<h3>Title: Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL</h3>
<ul>
<li><strong>Authors: </strong>Yunseon Choi, Sangmin Bae, Seonghyun Ban, Minchan Jeong, Chuheng Zhang, Lei Song, Li Zhao, Jiang Bian, Kee-Eung Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14733">https://arxiv.org/abs/2407.14733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14733">https://arxiv.org/pdf/2407.14733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14733]] Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL(https://arxiv.org/abs/2407.14733)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>With the advent of foundation models, prompt tuning has positioned itself as an important technique for directing model behaviors and eliciting desired responses. Prompt tuning regards selecting appropriate keywords included into the input, thereby adapting to the downstream task without adjusting or fine-tuning the model parameters. There is a wide range of work in prompt tuning, from approaches that directly harness the backpropagated gradient signals from the model, to those employing black-box optimization such as reinforcement learning (RL) methods. Our primary focus is on RLPrompt, which aims to find optimal prompt tokens leveraging soft Q-learning. While the results show promise, we have observed that the prompts frequently appear unnatural, which impedes their interpretability. We address this limitation by using sparse Tsallis entropy regularization, a principled approach to filtering out unlikely tokens from consideration. We extensively evaluate our approach across various tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion from images. The results indicate a notable improvement over baselines, highlighting the efficacy of our approach in addressing the challenges of prompt tuning. Moreover, we show that the prompts discovered using our method are more natural and interpretable compared to those from other baselines.</li>
</ul>

<h3>Title: Difflare: Removing Image Lens Flare with Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Tianwen Zhou, Qihao Duan, Zitong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14746">https://arxiv.org/abs/2407.14746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14746">https://arxiv.org/pdf/2407.14746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14746]] Difflare: Removing Image Lens Flare with Latent Diffusion Model(https://arxiv.org/abs/2407.14746)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The recovery of high-quality images from images corrupted by lens flare presents a significant challenge in low-level vision. Contemporary deep learning methods frequently entail training a lens flare removing model from scratch. However, these methods, despite their noticeable success, fail to utilize the generative prior learned by pre-trained models, resulting in unsatisfactory performance in lens flare removal. Furthermore, there are only few works considering the physical priors relevant to flare removal. To address these issues, we introduce Difflare, a novel approach designed for lens flare removal. To leverage the generative prior learned by Pre-Trained Diffusion Models (PTDM), we introduce a trainable Structural Guidance Injection Module (SGIM) aimed at guiding the restoration process with PTDM. Towards more efficient training, we employ Difflare in the latent space. To address information loss resulting from latent compression and the stochastic sampling process of PTDM, we introduce an Adaptive Feature Fusion Module (AFFM), which incorporates the Luminance Gradient Prior (LGP) of lens flare to dynamically regulate feature extraction. Extensive experiments demonstrate that our proposed Difflare achieves state-of-the-art performance in real-world lens flare removal, restoring images corrupted by flare with improved fidelity and perceptual quality. The codes will be released soon.</li>
</ul>

<h3>Title: Data Augmentation in Graph Neural Networks: The Role of Generated Synthetic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sumeyye Bas, Kiymet Kaya, Resul Tugay, Sule Gunduz Oguducu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14765">https://arxiv.org/abs/2407.14765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14765">https://arxiv.org/pdf/2407.14765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14765]] Data Augmentation in Graph Neural Networks: The Role of Generated Synthetic Graphs(https://arxiv.org/abs/2407.14765)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graphs are crucial for representing interrelated data and aiding predictive modeling by capturing complex relationships. Achieving high-quality graph representation is important for identifying linked patterns, leading to improvements in Graph Neural Networks (GNNs) to better capture data structures. However, challenges such as data scarcity, high collection costs, and ethical concerns limit progress. As a result, generative models and data augmentation have become more and more popular. This study explores using generated graphs for data augmentation, comparing the performance of combining generated graphs with real graphs, and examining the effect of different quantities of generated graphs on graph classification tasks. The experiments show that balancing scalability and quality requires different generators based on graph size. Our results introduce a new approach to graph data augmentation, ensuring consistent labels and enhancing classification performance.</li>
</ul>

<h3>Title: Implementing Fairness: the view from a FairDream</h3>
<ul>
<li><strong>Authors: </strong>Thomas Souverain, Johnathan Nguyen, Nicolas Meric, Paul Égré</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14766">https://arxiv.org/abs/2407.14766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14766">https://arxiv.org/pdf/2407.14766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14766]] Implementing Fairness: the view from a FairDream(https://arxiv.org/abs/2407.14766)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an experimental investigation of the problem of AI fairness in classification. We train an AI model and develop our own fairness package FairDream to detect inequalities and then to correct for them, using income prediction as a case study. Our experiments show that it is a property of FairDream to fulfill fairness objectives which are conditional on the ground truth (Equalized Odds), even when the algorithm is set the task of equalizing positives across groups (Demographic Parity). While this may be seen as an anomaly, we explain this property by comparing our approach with a closely related fairness method (GridSearch), which can enforce Demographic Parity at the expense of Equalized Odds. We grant that a fairness metric conditioned on true labels does not give a sufficient criterion to reach fairness, but we argue that it gives us at least a necessary condition to implement Demographic Parity cautiously. We also explain why neither Equal Calibration nor Equal Precision stand as relevant fairness criteria in classification. Addressing their limitations to warn the decision-maker for any disadvantaging rate, Equalized Odds avoids the peril of strict conservatism, while keeping away the utopia of a whole redistribution of resources through algorithms.</li>
</ul>

<h3>Title: Blind Image Deconvolution by Generative-based Kernel Prior and Initializer via Latent Encoding</h3>
<ul>
<li><strong>Authors: </strong>Jiangtao Zhang, Zongsheng Yue, Hui Wang, Qian Zhao, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14816">https://arxiv.org/abs/2407.14816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14816">https://arxiv.org/pdf/2407.14816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14816]] Blind Image Deconvolution by Generative-based Kernel Prior and Initializer via Latent Encoding(https://arxiv.org/abs/2407.14816)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Blind image deconvolution (BID) is a classic yet challenging problem in the field of image processing. Recent advances in deep image prior (DIP) have motivated a series of DIP-based approaches, demonstrating remarkable success in BID. However, due to the high non-convexity of the inherent optimization process, these methods are notorious for their sensitivity to the initialized kernel. To alleviate this issue and further improve their performance, we propose a new framework for BID that better considers the prior modeling and the initialization for blur kernels, leveraging a deep generative model. The proposed approach pre-trains a generative adversarial network-based kernel generator that aptly characterizes the kernel priors and a kernel initializer that facilitates a well-informed initialization for the blur kernel through latent space encoding. With the pre-trained kernel generator and initializer, one can obtain a high-quality initialization of the blur kernel, and enable optimization within a compact latent kernel manifold. Such a framework results in an evident performance improvement over existing DIP-based BID methods. Extensive experiments on different datasets demonstrate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: Text-based Talking Video Editing with Cascaded Conditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Bo Han, Heqing Zou, Haoyang Li, Guangcong Wang, Chng Eng Siong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14841">https://arxiv.org/abs/2407.14841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14841">https://arxiv.org/pdf/2407.14841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14841]] Text-based Talking Video Editing with Cascaded Conditional Diffusion(https://arxiv.org/abs/2407.14841)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>Text-based talking-head video editing aims to efficiently insert, delete, and substitute segments of talking videos through a user-friendly text editing approach. It is challenging because of \textbf{1)} generalizable talking-face representation, \textbf{2)} seamless audio-visual transitions, and \textbf{3)} identity-preserved talking faces. Previous works either require minutes of talking-face video training data and expensive test-time optimization for customized talking video editing or directly generate a video sequence without considering in-context information, leading to a poor generalizable representation, or incoherent transitions, or even inconsistent identity. In this paper, we propose an efficient cascaded conditional diffusion-based framework, which consists of two stages: audio to dense-landmark motion and motion to video. \textit{\textbf{In the first stage}}, we first propose a dynamic weighted in-context diffusion module to synthesize dense-landmark motions given an edited audio. \textit{\textbf{In the second stage}}, we introduce a warping-guided conditional diffusion module. The module first interpolates between the start and end frames of the editing interval to generate smooth intermediate frames. Then, with the help of the audio-to-dense motion images, these intermediate frames are warped to obtain coarse intermediate frames. Conditioned on the warped intermedia frames, a diffusion model is adopted to generate detailed and high-resolution target frames, which guarantees coherent and identity-preserved transitions. The cascaded conditional diffusion model decomposes the complex talking editing task into two flexible generation tasks, which provides a generalizable talking-face representation, seamless audio-visual transitions, and identity-preserved faces on a small dataset. Experiments show the effectiveness and superiority of the proposed method.</li>
</ul>

<h3>Title: Reduced Effectiveness of Kolmogorov-Arnold Networks on Functions with Noise</h3>
<ul>
<li><strong>Authors: </strong>Haoran Shen, Chen Zeng, Jiahui Wang, Qiao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14882">https://arxiv.org/abs/2407.14882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14882">https://arxiv.org/pdf/2407.14882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14882]] Reduced Effectiveness of Kolmogorov-Arnold Networks on Functions with Noise(https://arxiv.org/abs/2407.14882)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>It has been observed that even a small amount of noise introduced into the dataset can significantly degrade the performance of KAN. In this brief note, we aim to quantitatively evaluate the performance when noise is added to the dataset. We propose an oversampling technique combined with denoising to alleviate the impact of noise. Specifically, we employ kernel filtering based on diffusion maps for pre-filtering the noisy data for training KAN network. Our experiments show that while adding i.i.d. noise with any fixed SNR, when we increase the amount of training data by a factor of $r$, the test-loss (RMSE) of KANs will exhibit a performance trend like $\text{test-loss} \sim \mathcal{O}(r^{-\frac{1}{2}})$ as $r\to +\infty$. We conclude that applying both oversampling and filtering strategies can reduce the detrimental effects of noise. Nevertheless, determining the optimal variance for the kernel filtering process is challenging, and enhancing the volume of training data substantially increases the associated costs, because the training dataset needs to be expanded multiple times in comparison to the initial clean data. As a result, the noise present in the data ultimately diminishes the effectiveness of Kolmogorov-Arnold networks.</li>
</ul>

<h3>Title: Falcon2-11B Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Quentin Malartic, Nilabhra Roy Chowdhury, Ruxandra Cojocaru, Mugariya Farooq, Giulia Campesan, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Ankit Singh, Maksim Velikanov, Basma El Amel Boussaha, Mohammed Al-Yafeai, Hamza Alobeidli, Leen Al Qadi, Mohamed El Amine Seddik, Kirill Fedyanin, Reda Alami, Hakim Hacid</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14885">https://arxiv.org/abs/2407.14885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14885">https://arxiv.org/pdf/2407.14885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14885]] Falcon2-11B Technical Report(https://arxiv.org/abs/2407.14885)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce Falcon2-11B, a foundation model trained on over five trillion tokens, and its multimodal counterpart, Falcon2-11B-vlm, which is a vision-to-text model. We report our findings during the training of the Falcon2-11B which follows a multi-stage approach where the early stages are distinguished by their context length and a final stage where we use a curated, high-quality dataset. Additionally, we report the effect of doubling the batch size mid-training and how training loss spikes are affected by the learning rate. The downstream performance of the foundation model is evaluated on established benchmarks, including multilingual and code datasets. The foundation model shows strong generalization across all the tasks which makes it suitable for downstream finetuning use cases. For the vision language model, we report the performance on several benchmarks and show that our model achieves a higher average score compared to open-source models of similar size. The model weights and code of both Falcon2-11B and Falcon2-11B-vlm are made available under a permissive license.</li>
</ul>

<h3>Title: Latent Pollution Model: The Hidden Carbon Footprint in 3D Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Marvin Seyfarth, Salman Ul Hassan Dar, Sandy Engelhardt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14892">https://arxiv.org/abs/2407.14892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14892">https://arxiv.org/pdf/2407.14892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14892]] Latent Pollution Model: The Hidden Carbon Footprint in 3D Image Synthesis(https://arxiv.org/abs/2407.14892)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Contemporary developments in generative AI are rapidly transforming the field of medical AI. These developments have been predominantly driven by the availability of large datasets and high computing power, which have facilitated a significant increase in model capacity. Despite their considerable potential, these models demand substantially high power, leading to high carbon dioxide (CO2) emissions. Given the harm such models are causing to the environment, there has been little focus on the carbon footprints of such models. This study analyzes carbon emissions from 2D and 3D latent diffusion models (LDMs) during training and data generation phases, revealing a surprising finding: the synthesis of large images contributes most significantly to these emissions. We assess different scenarios including model sizes, image dimensions, distributed training, and data generation steps. Our findings reveal substantial carbon emissions from these models, with training 2D and 3D models comparable to driving a car for 10 km and 90 km, respectively. The process of data generation is even more significant, with CO2 emissions equivalent to driving 160 km for 2D models and driving for up to 3345 km for 3D synthesis. Additionally, we found that the location of the experiment can increase carbon emissions by up to 94 times, and even the time of year can influence emissions by up to 50%. These figures are alarming, considering they represent only a single training and data generation phase for each model. Our results emphasize the urgent need for developing environmentally sustainable strategies in generative AI.</li>
</ul>

<h3>Title: AGLLDiff: Guiding Diffusion Models Towards Unsupervised Training-free Real-world Low-light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Lin, Tian Ye, Sixiang Chen, Zhenqi Fu, Yingying Wang, Wenhao Chai, Zhaohu Xing, Lei Zhu, Xinghao Ding</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14900">https://arxiv.org/abs/2407.14900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14900">https://arxiv.org/pdf/2407.14900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14900]] AGLLDiff: Guiding Diffusion Models Towards Unsupervised Training-free Real-world Low-light Image Enhancement(https://arxiv.org/abs/2407.14900)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing low-light image enhancement (LIE) methods have achieved noteworthy success in solving synthetic distortions, yet they often fall short in practical applications. The limitations arise from two inherent challenges in real-world LIE: 1) the collection of distorted/clean image pairs is often impractical and sometimes even unavailable, and 2) accurately modeling complex degradations presents a non-trivial problem. To overcome them, we propose the Attribute Guidance Diffusion framework (AGLLDiff), a training-free method for effective real-world LIE. Instead of specifically defining the degradation process, AGLLDiff shifts the paradigm and models the desired attributes, such as image exposure, structure and color of normal-light images. These attributes are readily available and impose no assumptions about the degradation process, which guides the diffusion sampling process to a reliable high-quality solution space. Extensive experiments demonstrate that our approach outperforms the current leading unsupervised LIE methods across benchmarks in terms of distortion-based and perceptual-based metrics, and it performs well even in sophisticated wild degradation.</li>
</ul>

<h3>Title: Self-supervised transformer-based pre-training method with General Plant Infection dataset</h3>
<ul>
<li><strong>Authors: </strong>Zhengle Wang, Ruifeng Wang, Minjuan Wang, Tianyun Lai, Man Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14911">https://arxiv.org/abs/2407.14911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14911">https://arxiv.org/pdf/2407.14911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14911]] Self-supervised transformer-based pre-training method with General Plant Infection dataset(https://arxiv.org/abs/2407.14911)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Pest and disease classification is a challenging issue in agriculture. The performance of deep learning models is intricately linked to training data diversity and quantity, posing issues for plant pest and disease datasets that remain underdeveloped. This study addresses these challenges by constructing a comprehensive dataset and proposing an advanced network architecture that combines Contrastive Learning and Masked Image Modeling (MIM). The dataset comprises diverse plant species and pest categories, making it one of the largest and most varied in the field. The proposed network architecture demonstrates effectiveness in addressing plant pest and disease recognition tasks, achieving notable detection accuracy. This approach offers a viable solution for rapid, efficient, and cost-effective plant pest and disease detection, thereby reducing agricultural production costs. Our code and dataset will be publicly available to advance research in plant pest and disease recognition the GitHub repository at this https URL</li>
</ul>

<h3>Title: Automatic Generation of Fashion Images using Prompting in Generative Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Georgia Argyrou, Angeliki Dimitriou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14944">https://arxiv.org/abs/2407.14944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14944">https://arxiv.org/pdf/2407.14944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14944]] Automatic Generation of Fashion Images using Prompting in Generative Machine Learning Models(https://arxiv.org/abs/2407.14944)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The advent of artificial intelligence has contributed in a groundbreaking transformation of the fashion industry, redefining creativity and innovation in unprecedented ways. This work investigates methodologies for generating tailored fashion descriptions using two distinct Large Language Models and a Stable Diffusion model for fashion image creation. Emphasizing adaptability in AI-driven fashion creativity, we depart from traditional approaches and focus on prompting techniques, such as zero-shot and few-shot learning, as well as Chain-of-Thought (CoT), which results in a variety of colors and textures, enhancing the diversity of the outputs. Central to our methodology is Retrieval-Augmented Generation (RAG), enriching models with insights from fashion sources to ensure contemporary representations. Evaluation combines quantitative metrics such as CLIPscore with qualitative human judgment, highlighting strengths in creativity, coherence, and aesthetic appeal across diverse styles. Among the participants, RAG and few-shot learning techniques are preferred for their ability to produce more relevant and appealing fashion descriptions. Our code is provided at this https URL.</li>
</ul>

<h3>Title: Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Desta Haileselassie Hagos, Rick Battle, Danda B. Rawat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14962">https://arxiv.org/abs/2407.14962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14962">https://arxiv.org/pdf/2407.14962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14962]] Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives(https://arxiv.org/abs/2407.14962)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The emergence of Generative Artificial Intelligence (AI) and Large Language Models (LLMs) has marked a new era of Natural Language Processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This paper explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our paper contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of Generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.</li>
</ul>

<h3>Title: RGB2Point: 3D Point Cloud Generation from Single RGB Images</h3>
<ul>
<li><strong>Authors: </strong>Jae Joong Lee, Bedrich Benes</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14979">https://arxiv.org/abs/2407.14979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14979">https://arxiv.org/pdf/2407.14979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14979]] RGB2Point: 3D Point Cloud Generation from Single RGB Images(https://arxiv.org/abs/2407.14979)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce RGB2Point, an unposed single-view RGB image to a 3D point cloud generation based on Transformer. RGB2Point takes an input image of an object and generates a dense 3D point cloud. Contrary to prior works based on CNN layers and diffusion denoising approaches, we use pre-trained Transformer layers that are fast and generate high-quality point clouds with consistent quality over available categories. Our generated point clouds demonstrate high quality on a real-world dataset, as evidenced by improved Chamfer distance (51.15%) and Earth Mover's distance (45.96%) metrics compared to the current state-of-the-art. Additionally, our approach shows a better quality on a synthetic dataset, achieving better Chamfer distance (39.26%), Earth Mover's distance (26.95%), and F-score (47.16%). Moreover, our method produces 63.1% more consistent high-quality results across various object categories compared to prior works. Furthermore, RGB2Point is computationally efficient, requiring only 2.3GB of VRAM to reconstruct a 3D point cloud from a single RGB image, and our implementation generates the results 15,133x faster than a SOTA diffusion-based model.</li>
</ul>

<h3>Title: GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingzhi Gong, Sisi Li, Giordano d'Aloisio, Zishuo Ding, Yulong Ye, William B. Langdon, Federica Sarro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.14982">https://arxiv.org/abs/2407.14982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.14982">https://arxiv.org/pdf/2407.14982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.14982]] GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation(https://arxiv.org/abs/2407.14982)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Tuning the parameters and prompts for improving AI-based text-to-image generation has remained a substantial yet unaddressed challenge. Hence we introduce GreenStableYolo, which improves the parameters and prompts for Stable Diffusion to both reduce GPU inference time and increase image generation quality using NSGA-II and Yolo. Our experiments show that despite a relatively slight trade-off (18%) in image quality compared to StableYolo (which only considers image quality), GreenStableYolo achieves a substantial reduction in inference time (266% less) and a 526% higher hypervolume, thereby advancing the state-of-the-art for text-to-image generation.</li>
</ul>

<h3>Title: Requiem for a drone: a machine-learning based framework for stealthy attacks against unmanned autonomous vehicles</h3>
<ul>
<li><strong>Authors: </strong>Kyo Hyun Kim, Denizhan Kara, Vineetha Paruchuri, Sibin Mohan, Greg Kimberly, Jae Kim, Josh Eckhardt</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15003">https://arxiv.org/abs/2407.15003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15003">https://arxiv.org/pdf/2407.15003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15003]] Requiem for a drone: a machine-learning based framework for stealthy attacks against unmanned autonomous vehicles(https://arxiv.org/abs/2407.15003)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>There is a space of uncertainty in the modeling of vehicular dynamics of autonomous systems due to noise in sensor readings, environmental factors or modeling errors. We present Requiem, a software-only, blackbox approach that exploits this space in a stealthy manner causing target systems, e.g., unmanned aerial vehicles (UAVs), to significantly deviate from their mission parameters. Our system achieves this by modifying sensor values, all while avoiding detection by onboard anomaly detectors (hence, "stealthy"). The Requiem framework uses a combination of multiple deep learning models (that we refer to as "surrogates" and "spoofers") coupled with extensive, realistic simulations on a software-in-the-loop quadrotor UAV system. Requiem makes no assumptions about either the (types of) sensors or the onboard state estimation algorithm(s) -- it works so long as the latter is "learnable". We demonstrate the effectiveness of our system using various attacks across multiple missions as well as multiple sets of statistical analyses. We show that Requiem successfully exploits the modeling errors (i.e., causes significant deviations from planned mission parameters) while remaining stealthy (no detection even after {tens of meters of deviations}) and are generalizable (Requiem has potential to work across different attacks and sensor types).</li>
</ul>

<h3>Title: Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Hu, Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15085">https://arxiv.org/abs/2407.15085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15085">https://arxiv.org/pdf/2407.15085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15085]] Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization(https://arxiv.org/abs/2407.15085)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Domain generalization (DG) aims to avoid the performance degradation of the model when the distribution shift between the limited training data and unseen test data occurs. Recently, foundation models with enormous parameters have been pre-trained with huge datasets, demonstrating strong generalization ability and showing promising direction for solving the DG problem. However, fully Fine-Tuning (FT) the foundation models results in unsatisfactory out-of-distribution accuracy due to the destroyed pre-trained generalized features. Recently, Parameter-Efficient Fine-Tuning (PEFT) alleviates the above problem by fine-tuning a small portion of the model parameters while keeping the rest frozen, which achieves better generalization performance compared to FT. Nevertheless, PEFT still suffers from the issue of overfitting to the training domains. To address the above issue, we propose Parameter-Efficient Group with Orthogonal regularization (PEGO) for vision transformers, which effectively preserves the generalization ability of the pre-trained network and learns more diverse knowledge compared with conventional PEFT. Specifically, we inject a group of trainable Low-Rank Adaptation (LoRA) modules into the pre-trained model and propose an orthogonal regularization loss to enhance the generalization ability of the model. Our framework achieves SOTA performance on five DG benchmarks, while only requiring training a small number of parameters without adding additional testing cost.</li>
</ul>

<h3>Title: A General Framework for Data-Use Auditing of ML Models</h3>
<ul>
<li><strong>Authors: </strong>Zonghao Huang, Neil Zhenqiang Gong, Michael K. Reiter</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15100">https://arxiv.org/abs/2407.15100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15100">https://arxiv.org/pdf/2407.15100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15100]] A General Framework for Data-Use Auditing of ML Models(https://arxiv.org/abs/2407.15100)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Auditing the use of data in training machine-learning (ML) models is an increasingly pressing challenge, as myriad ML practitioners routinely leverage the effort of content creators to train models without their permission. In this paper, we propose a general method to audit an ML model for the use of a data-owner's data in training, without prior knowledge of the ML task for which the data might be used. Our method leverages any existing black-box membership inference method, together with a sequential hypothesis test of our own design, to detect data use with a quantifiable, tunable false-detection rate. We show the effectiveness of our proposed framework by applying it to audit data use in two types of ML models, namely image classifiers and foundation models.</li>
</ul>

<h3>Title: D$^4$-VTON: Dynamic Semantics Disentangling for Differential Diffusion based Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Zhaotong Yang, Zicheng Jiang, Xinzhe Li, Huiyu Zhou, Junyu Dong, Huaidong Zhang, Yong Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15111">https://arxiv.org/abs/2407.15111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15111">https://arxiv.org/pdf/2407.15111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15111]] D$^4$-VTON: Dynamic Semantics Disentangling for Differential Diffusion based Virtual Try-On(https://arxiv.org/abs/2407.15111)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce D$^4$-VTON, an innovative solution for image-based virtual try-on. We address challenges from previous studies, such as semantic inconsistencies before and after garment warping, and reliance on static, annotation-driven clothing parsers. Additionally, we tackle the complexities in diffusion-based VTON models when handling simultaneous tasks like inpainting and denoising. Our approach utilizes two key technologies: Firstly, Dynamic Semantics Disentangling Modules (DSDMs) extract abstract semantic information from garments to create distinct local flows, improving precise garment warping in a self-discovered manner. Secondly, by integrating a Differential Information Tracking Path (DITP), we establish a novel diffusion-based VTON paradigm. This path captures differential information between incomplete try-on inputs and their complete versions, enabling the network to handle multiple degradations independently, thereby minimizing learning ambiguities and achieving realistic results with minimal overhead. Extensive experiments demonstrate that D$^4$-VTON significantly outperforms existing methods in both quantitative metrics and qualitative evaluations, demonstrating its capability in generating realistic images and ensuring semantic consistency.</li>
</ul>

<h3>Title: D$^4$M: Dataset Distillation via Disentangled Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Duo Su, Junjie Hou, Weizhi Gao, Yingjie Tian, Bowen Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15138">https://arxiv.org/abs/2407.15138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15138">https://arxiv.org/pdf/2407.15138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15138]] D$^4$M: Dataset Distillation via Disentangled Diffusion Model(https://arxiv.org/abs/2407.15138)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Dataset distillation offers a lightweight synthetic dataset for fast network training with promising test accuracy. To imitate the performance of the original dataset, most approaches employ bi-level optimization and the distillation space relies on the matching architecture. Nevertheless, these approaches either suffer significant computational costs on large-scale datasets or experience performance decline on cross-architectures. We advocate for designing an economical dataset distillation framework that is independent of the matching architectures. With empirical observations, we argue that constraining the consistency of the real and synthetic image spaces will enhance the cross-architecture generalization. Motivated by this, we introduce Dataset Distillation via Disentangled Diffusion Model (D$^4$M), an efficient framework for dataset distillation. Compared to architecture-dependent methods, D$^4$M employs latent diffusion model to guarantee consistency and incorporates label information into category prototypes. The distilled datasets are versatile, eliminating the need for repeated generation of distinct datasets for various architectures. Through comprehensive experiments, D$^4$M demonstrates superior performance and robust generalization, surpassing the SOTA methods across most aspects.</li>
</ul>

<h3>Title: Anchored Diffusion for Video Face Reenactment</h3>
<ul>
<li><strong>Authors: </strong>Idan Kligvasser, Regev Cohen, George Leifman, Ehud Rivlin, Michael Elad</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15153">https://arxiv.org/abs/2407.15153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15153">https://arxiv.org/pdf/2407.15153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15153]] Anchored Diffusion for Video Face Reenactment(https://arxiv.org/abs/2407.15153)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video generation has drawn significant interest recently, pushing the development of large-scale models capable of producing realistic videos with coherent motion. Due to memory constraints, these models typically generate short video segments that are then combined into long videos. The merging process poses a significant challenge, as it requires ensuring smooth transitions and overall consistency. In this paper, we introduce Anchored Diffusion, a novel method for synthesizing relatively long and seamless videos. We extend Diffusion Transformers (DiTs) to incorporate temporal information, creating our sequence-DiT (sDiT) model for generating short video segments. Unlike previous works, we train our model on video sequences with random non-uniform temporal spacing and incorporate temporal information via external guidance, increasing flexibility and allowing it to capture both short and long-term relationships. Furthermore, during inference, we leverage the transformer architecture to modify the diffusion process, generating a batch of non-uniform sequences anchored to a common frame, ensuring consistency regardless of temporal distance. To demonstrate our method, we focus on face reenactment, the task of creating a video from a source image that replicates the facial expressions and movements from a driving video. Through comprehensive experiments, we show our approach outperforms current techniques in producing longer consistent high-quality videos while offering editing capabilities.</li>
</ul>

<h3>Title: Distilling Vision-Language Foundation Models: A Data-Free Approach via Prompt Diversification</h3>
<ul>
<li><strong>Authors: </strong>Yunyi Xuan, Weijie Chen, Shicai Yang, Di Xie, Luojun Lin, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15155">https://arxiv.org/abs/2407.15155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15155">https://arxiv.org/pdf/2407.15155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15155]] Distilling Vision-Language Foundation Models: A Data-Free Approach via Prompt Diversification(https://arxiv.org/abs/2407.15155)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Data-Free Knowledge Distillation (DFKD) has shown great potential in creating a compact student model while alleviating the dependency on real training data by synthesizing surrogate data. However, prior arts are seldom discussed under distribution shifts, which may be vulnerable in real-world applications. Recent Vision-Language Foundation Models, e.g., CLIP, have demonstrated remarkable performance in zero-shot out-of-distribution generalization, yet consuming heavy computation resources. In this paper, we discuss the extension of DFKD to Vision-Language Foundation Models without access to the billion-level image-text datasets. The objective is to customize a student model for distribution-agnostic downstream tasks with given category concepts, inheriting the out-of-distribution generalization capability from the pre-trained foundation models. In order to avoid generalization degradation, the primary challenge of this task lies in synthesizing diverse surrogate images driven by text prompts. Since not only category concepts but also style information are encoded in text prompts, we propose three novel Prompt Diversification methods to encourage image synthesis with diverse styles, namely Mix-Prompt, Random-Prompt, and Contrastive-Prompt. Experiments on out-of-distribution generalization datasets demonstrate the effectiveness of the proposed methods, with Contrastive-Prompt performing the best.</li>
</ul>

<h3>Title: Assessing Sample Quality via the Latent Space of Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Xu, Hieu Le, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15171">https://arxiv.org/abs/2407.15171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15171">https://arxiv.org/pdf/2407.15171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15171]] Assessing Sample Quality via the Latent Space of Generative Models(https://arxiv.org/abs/2407.15171)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Advances in generative models increase the need for sample quality assessment. To do so, previous methods rely on a pre-trained feature extractor to embed the generated samples and real samples into a common space for comparison. However, different feature extractors might lead to inconsistent assessment outcomes. Moreover, these methods are not applicable for domains where a robust, universal feature extractor does not yet exist, such as medical images or 3D assets. In this paper, we propose to directly examine the latent space of the trained generative model to infer generated sample quality. This is feasible because the quality a generated sample directly relates to the amount of training data resembling it, and we can infer this information by examining the density of the latent space. Accordingly, we use a latent density score function to quantify sample quality. We show that the proposed score correlates highly with the sample quality for various generative models including VAEs, GANs and Latent Diffusion Models. Compared with previous quality assessment methods, our method has the following advantages: 1) pre-generation quality estimation with reduced computational cost, 2) generalizability to various domains and modalities, and 3) applicability to latent-based image editing and generation methods. Extensive experiments demonstrate that our proposed methods can benefit downstream tasks such as few-shot image classification and latent face image editing. Code is available at this https URL.</li>
</ul>

<h3>Title: HoloDreamer: Holistic 3D Panoramic World Generation from Text Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Haiyang Zhou, Xinhua Cheng, Wangbo Yu, Yonghong Tian, Li Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15187">https://arxiv.org/abs/2407.15187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15187">https://arxiv.org/pdf/2407.15187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15187]] HoloDreamer: Holistic 3D Panoramic World Generation from Text Descriptions(https://arxiv.org/abs/2407.15187)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>3D scene generation is in high demand across various domains, including virtual reality, gaming, and the film industry. Owing to the powerful generative capabilities of text-to-image diffusion models that provide reliable priors, the creation of 3D scenes using only text prompts has become viable, thereby significantly advancing researches in text-driven 3D scene generation. In order to obtain multiple-view supervision from 2D diffusion models, prevailing methods typically employ the diffusion model to generate an initial local image, followed by iteratively outpainting the local image using diffusion models to gradually generate scenes. Nevertheless, these outpainting-based approaches prone to produce global inconsistent scene generation results without high degree of completeness, restricting their broader applications. To tackle these problems, we introduce HoloDreamer, a framework that first generates high-definition panorama as a holistic initialization of the full 3D scene, then leverage 3D Gaussian Splatting (3D-GS) to quickly reconstruct the 3D scene, thereby facilitating the creation of view-consistent and fully enclosed 3D scenes. Specifically, we propose Stylized Equirectangular Panorama Generation, a pipeline that combines multiple diffusion models to enable stylized and detailed equirectangular panorama generation from complex text prompts. Subsequently, Enhanced Two-Stage Panorama Reconstruction is introduced, conducting a two-stage optimization of 3D-GS to inpaint the missing region and enhance the integrity of the scene. Comprehensive experiments demonstrated that our method outperforms prior works in terms of overall visual consistency and harmony as well as reconstruction quality and rendering robustness when generating fully enclosed scenes.</li>
</ul>

<h3>Title: Mask Guided Gated Convolution for Amodal Content Completion</h3>
<ul>
<li><strong>Authors: </strong>Kaziwa Saleh, Sándor Szénási, Zoltán Vámossy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15203">https://arxiv.org/abs/2407.15203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15203">https://arxiv.org/pdf/2407.15203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15203]] Mask Guided Gated Convolution for Amodal Content Completion(https://arxiv.org/abs/2407.15203)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We present a model to reconstruct partially visible objects. The model takes a mask as an input, which we call weighted mask. The mask is utilized by gated convolutions to assign more weight to the visible pixels of the occluded instance compared to the background, while ignoring the features of the invisible pixels. By drawing more attention from the visible region, our model can predict the invisible patch more effectively than the baseline models, especially in instances with uniform texture. The model is trained on COCOA dataset and two subsets of it in a self-supervised manner. The results demonstrate that our model generates higher quality and more texture-rich outputs compared to baseline models. Code is available at: this https URL.</li>
</ul>

<h3>Title: CGB-DM: Content and Graphic Balance Layout Generation with Transformer-based Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yu Li, Yifan Chen, Gongye Liu, Jie Wu, Yujiu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15233">https://arxiv.org/abs/2407.15233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15233">https://arxiv.org/pdf/2407.15233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15233]] CGB-DM: Content and Graphic Balance Layout Generation with Transformer-based Diffusion Model(https://arxiv.org/abs/2407.15233)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Layout generation is the foundation task of intelligent design, which requires the integration of visual aesthetics and harmonious expression of content delivery. However, existing methods still face challenges in generating precise and visually appealing layouts, including blocking, overlap, or spatial misalignment between layouts, which are closely related to the spatial structure of graphic layouts. We find that these methods overly focus on content information and lack constraints on layout spatial structure, resulting in an imbalance of learning content-aware and graphic-aware features. To tackle this issue, we propose Content and Graphic Balance Layout Generation with Transformer-based Diffusion Model (CGB-DM). Specifically, we first design a regulator that balances the predicted content and graphic weight, overcoming the tendency of paying more attention to the content on canvas. Secondly, we introduce a graphic constraint of saliency bounding box to further enhance the alignment of geometric features between layout representations and images. In addition, we adapt a transformer-based diffusion model as the backbone, whose powerful generation capability ensures the quality in layout generation. Extensive experimental results indicate that our method has achieved state-of-the-art performance in both quantitative and qualitative evaluations. Our model framework can also be expanded to other graphic design fields.</li>
</ul>

<h3>Title: Variational Potential Flow: A Novel Probabilistic Framework for Energy-Based Generative Modelling</h3>
<ul>
<li><strong>Authors: </strong>Junn Yong Loo, Michelle Adeline, Arghya Pal, Vishnu Monn Baskaran, Chee-Ming Ting, Raphael C.-W. Phan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15238">https://arxiv.org/abs/2407.15238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15238">https://arxiv.org/pdf/2407.15238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15238]] Variational Potential Flow: A Novel Probabilistic Framework for Energy-Based Generative Modelling(https://arxiv.org/abs/2407.15238)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Energy based models (EBMs) are appealing for their generality and simplicity in data likelihood modeling, but have conventionally been difficult to train due to the unstable and time-consuming implicit MCMC sampling during contrastive divergence training. In this paper, we present a novel energy-based generative framework, Variational Potential Flow (VAPO), that entirely dispenses with implicit MCMC sampling and does not rely on complementary latent models or cooperative training. The VAPO framework aims to learn a potential energy function whose gradient (flow) guides the prior samples, so that their density evolution closely follows an approximate data likelihood homotopy. An energy loss function is then formulated to minimize the Kullback-Leibler divergence between density evolution of the flow-driven prior and the data likelihood homotopy. Images can be generated after training the potential energy, by initializing the samples from Gaussian prior and solving the ODE governing the potential flow on a fixed time interval using generic ODE solvers. Experiment results show that the proposed VAPO framework is capable of generating realistic images on various image datasets. In particular, our proposed framework achieves competitive FID scores for unconditional image generation on the CIFAR-10 and CelebA datasets.</li>
</ul>

<h3>Title: BIGbench: A Unified Benchmark for Social Bias in Text-to-Image Generative Models Based on Multi-modal LLM</h3>
<ul>
<li><strong>Authors: </strong>Hanjun Luo, Haoyu Huang, Ziye Deng, Xuecheng Liu, Ruizhe Chen, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15240">https://arxiv.org/abs/2407.15240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15240">https://arxiv.org/pdf/2407.15240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15240]] BIGbench: A Unified Benchmark for Social Bias in Text-to-Image Generative Models Based on Multi-modal LLM(https://arxiv.org/abs/2407.15240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) generative models are becoming more crucial in terms of their ability to generate complex and high-quality images, which also raises concerns about the social biases in their outputs, especially in human generation. Sociological research has established systematic classifications of bias; however, existing research of T2I models often conflates different types of bias, hindering the progress of these methods. In this paper, we introduce BIGbench, a unified benchmark for Biases of Image Generation with a well-designed dataset. In contrast to existing benchmarks, BIGbench classifies and evaluates complex biases into four dimensions: manifestation of bias, visibility of bias, acquired attributes, and protected attributes. Additionally, BIGbench applies advanced multi-modal large language models (MLLM), achieving fully automated evaluation while maintaining high accuracy. We apply BIGbench to evaluate eight recent general T2I models and three debiased methods. We also conduct human evaluation, whose results demonstrated the effectiveness of BIGbench in aligning images and identifying various biases. Besides, our study also revealed new research directions about biases, including the side-effect of irrelevant protected attributes and distillation. Our dataset and benchmark is openly accessible to the research community to ensure the reproducibility.</li>
</ul>

<h3>Title: TimeInf: Time Series Data Contribution via Influence Functions</h3>
<ul>
<li><strong>Authors: </strong>Yizi Zhang, Jingyan Shen, Xiaoxue Xiong, Yongchan Kwon</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15247">https://arxiv.org/abs/2407.15247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15247">https://arxiv.org/pdf/2407.15247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15247]] TimeInf: Time Series Data Contribution via Influence Functions(https://arxiv.org/abs/2407.15247)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Evaluating the contribution of individual data points to a model's prediction is critical for interpreting model predictions and improving model performance. Existing data contribution methods have been applied to various data types, including tabular data, images, and texts; however, their primary focus has been on i.i.d. settings. Despite the pressing need for principled approaches tailored to time series datasets, the problem of estimating data contribution in such settings remains unexplored, possibly due to challenges associated with handling inherent temporal dependencies. This paper introduces TimeInf, a data contribution estimation method for time-series datasets. TimeInf uses influence functions to attribute model predictions to individual time points while preserving temporal structures. Our extensive empirical results demonstrate that TimeInf outperforms state-of-the-art methods in identifying harmful anomalies and helpful time points for forecasting. Additionally, TimeInf offers intuitive and interpretable attributions of data values, allowing us to easily distinguish diverse anomaly patterns through visualizations.</li>
</ul>

<h3>Title: Weakly SSM : On the Viability of Weakly Supervised Segmentations for Statistical Shape Modeling</h3>
<ul>
<li><strong>Authors: </strong>Janmesh Ukey, Tushar Kataria, Shireen Y. Elhabian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15260">https://arxiv.org/abs/2407.15260</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15260">https://arxiv.org/pdf/2407.15260</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15260]] Weakly SSM : On the Viability of Weakly Supervised Segmentations for Statistical Shape Modeling(https://arxiv.org/abs/2407.15260)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Statistical Shape Models (SSMs) excel at identifying population level anatomical variations, which is at the core of various clinical and biomedical applications, including morphology-based diagnostics and surgical planning. However, the effectiveness of SSM is often constrained by the necessity for expert-driven manual segmentation, a process that is both time-intensive and expensive, thereby restricting their broader application and utility. Recent deep learning approaches enable the direct estimation of Statistical Shape Models (SSMs) from unsegmented images. While these models can predict SSMs without segmentation during deployment, they do not address the challenge of acquiring the manual annotations needed for training, particularly in resource-limited settings. Semi-supervised and foundation models for anatomy segmentation can mitigate the annotation burden. Yet, despite the abundance of available approaches, there are no established guidelines to inform end-users on their effectiveness for the downstream task of constructing SSMs. In this study, we systematically evaluate the potential of weakly supervised methods as viable alternatives to manual segmentation's for building SSMs. We establish a new performance benchmark by employing various semi-supervised and foundational model methods for anatomy segmentation under low annotation settings, utilizing the predicted segmentation's for the task of SSM. We compare the modes of shape variation and use quantitative metrics to compare against a shape model derived from a manually annotated dataset. Our results indicate that some methods produce noisy segmentation, which is very unfavorable for SSM tasks, while others can capture the correct modes of variations in the population cohort with 60-80\% reduction in required manual annotation.</li>
</ul>

<h3>Title: Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Liwen Sun, James Zhao, Megan Han, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15268">https://arxiv.org/abs/2407.15268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15268">https://arxiv.org/pdf/2407.15268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15268]] Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation(https://arxiv.org/abs/2407.15268)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report generation. Experiments on two benchmark datasets show that our multimodal retriever outperforms state-of-the-art retrievers on both language generation and radiology-specific metrics, up to 6.5% and 2% score in F1CheXbert and F1RadGraph. Further analysis indicates that employing our factually-informed training strategy imposes an effective supervision signal, without relying on explicit diagnostic label guidance, and successfully propagates fact-aware capabilities from the multimodal retriever to the multimodal foundation model in radiology report generation.</li>
</ul>

<h3>Title: MIBench: Evaluating Multimodal Large Language Models over Multiple Images</h3>
<ul>
<li><strong>Authors: </strong>Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya Jiang, Ming Yan, Ji Zhang, Fei Huang, Chunfeng Yuan, Bing Li, Weiming Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15272">https://arxiv.org/abs/2407.15272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15272">https://arxiv.org/pdf/2407.15272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15272]] MIBench: Evaluating Multimodal Large Language Models over Multiple Images(https://arxiv.org/abs/2407.15272)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Built on the power of LLMs, numerous multimodal large language models (MLLMs) have recently achieved remarkable performance on various vision-language tasks across multiple benchmarks. However, most existing MLLMs and benchmarks primarily focus on single-image input scenarios, leaving the performance of MLLMs when handling realistic multiple images remain underexplored. Although a few benchmarks consider multiple images, their evaluation dimensions and samples are very limited. Therefore, in this paper, we propose a new benchmark MIBench, to comprehensively evaluate fine-grained abilities of MLLMs in multi-image scenarios. Specifically, MIBench categorizes the multi-image abilities into three scenarios: multi-image instruction (MII), multimodal knowledge-seeking (MKS) and multimodal in-context learning (MIC), and constructs 13 tasks with a total of 13K annotated samples. During data construction, for MII and MKS, we extract correct options from manual annotations and create challenging distractors to obtain multiple-choice questions. For MIC, to enable an in-depth evaluation, we set four sub-tasks and transform the original datasets into in-context learning formats. We evaluate several open-source MLLMs and close-source MLLMs on the proposed MIBench. The results reveal that although current models excel in single-image tasks, they exhibit significant shortcomings when faced with multi-image inputs, such as confused fine-grained perception, limited multi-image reasoning, and unstable in-context learning. The annotated data in MIBench is available at this https URL.</li>
</ul>

<h3>Title: Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Kwanyong Park, Kuniaki Saito, Donghyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15296">https://arxiv.org/abs/2407.15296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15296">https://arxiv.org/pdf/2407.15296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15296]] Weak-to-Strong Compositional Learning from Generative Models for Language-based Object Detection(https://arxiv.org/abs/2407.15296)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision-language (VL) models often exhibit a limited understanding of complex expressions of visual objects (e.g., attributes, shapes, and their relations), given complex and diverse language queries. Traditional approaches attempt to improve VL models using hard negative synthetic text, but their effectiveness is limited. In this paper, we harness the exceptional compositional understanding capabilities of generative foundational models. We introduce a novel method for structured synthetic data generation aimed at enhancing the compositional understanding of VL models in language-based object detection. Our framework generates densely paired positive and negative triplets (image, text descriptions, and bounding boxes) in both image and text domains. By leveraging these synthetic triplets, we transform 'weaker' VL models into 'stronger' models in terms of compositional understanding, a process we call "Weak-to-Strong Compositional Learning" (WSCL). To achieve this, we propose a new compositional contrastive learning formulation that discovers semantics and structures in complex descriptions from synthetic triplets. As a result, VL models trained with our synthetic data generation exhibit a significant performance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark by +6.9AP upon existing baselines.</li>
</ul>

<h3>Title: Iterative Ensemble Training with Anti-Gradient Control for Mitigating Memorization in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Xiaoliu Guan, Yu Wu, Jiaxu Miao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15328">https://arxiv.org/abs/2407.15328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15328">https://arxiv.org/pdf/2407.15328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15328]] Iterative Ensemble Training with Anti-Gradient Control for Mitigating Memorization in Diffusion Models(https://arxiv.org/abs/2407.15328)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models, known for their tremendous ability to generate novel and high-quality samples, have recently raised concerns due to their data memorization behavior, which poses privacy risks. Recent approaches for memory mitigation either only focused on the text modality problem in cross-modal generation tasks or utilized data augmentation strategies. In this paper, we propose a novel training framework for diffusion models from the perspective of visual modality, which is more generic and fundamental for mitigating memorization. To facilitate ``forgetting'' of stored information in diffusion model parameters, we propose an iterative ensemble training strategy by splitting the data into multiple shards for training multiple models and intermittently aggregating these model parameters. Moreover, practical analysis of losses illustrates that the training loss for easily memorable images tends to be obviously lower. Thus, we propose an anti-gradient control method to exclude the sample with a lower loss value from the current mini-batch to avoid memorizing. Extensive experiments and analysis on \crnote{four} datasets are conducted to illustrate the effectiveness of our method, and results show that our method successfully reduces memory capacity while even improving the performance slightly. Moreover, to save the computing cost, we successfully apply our method to fine-tune the well-trained diffusion models by limited epochs, demonstrating the applicability of our method. Code is available in this https URL.</li>
</ul>

<h3>Title: ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis with Coarse-to-Fine In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Senbin Zhu, Hanjie Zhao, Xingren Wang, Shanhong Liu, Yuxiang Jia, Hongying Zan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15341">https://arxiv.org/abs/2407.15341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15341">https://arxiv.org/pdf/2407.15341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15341]] ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis with Coarse-to-Fine In-context Learning(https://arxiv.org/abs/2407.15341)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The DimABSA task requires fine-grained sentiment intensity prediction for restaurant reviews, including scores for Valence and Arousal dimensions for each Aspect Term. In this study, we propose a Coarse-to-Fine In-context Learning(CFICL) method based on the Baichuan2-7B model for the DimABSA task in the SIGHAN 2024 workshop. Our method improves prediction accuracy through a two-stage optimization process. In the first stage, we use fixed in-context examples and prompt templates to enhance the model's sentiment recognition capability and provide initial predictions for the test data. In the second stage, we encode the Opinion field using BERT and select the most similar training data as new in-context examples based on similarity. These examples include the Opinion field and its scores, as well as related opinion words and their average scores. By filtering for sentiment polarity, we ensure that the examples are consistent with the test data. Our method significantly improves prediction accuracy and consistency by effectively utilizing training data and optimizing in-context examples, as validated by experimental results.</li>
</ul>

<h3>Title: Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA</h3>
<ul>
<li><strong>Authors: </strong>Yuan Pu, Zhuolun He, Tairu Qiu, Haoyuan Wu, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15353">https://arxiv.org/abs/2407.15353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15353">https://arxiv.org/pdf/2407.15353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15353]] Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA(https://arxiv.org/abs/2407.15353)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Retrieval augmented generation (RAG) enhances the accuracy and reliability of generative AI models by sourcing factual information from external databases, which is extensively employed in document-grounded question-answering (QA) tasks. Off-the-shelf RAG flows are well pretrained on general-purpose documents, yet they encounter significant challenges when being applied to knowledge-intensive vertical domains, such as electronic design automation (EDA). This paper addresses such issue by proposing a customized RAG framework along with three domain-specific techniques for EDA tool documentation QA, including a contrastive learning scheme for text embedding model fine-tuning, a reranker distilled from proprietary LLM, and a generative LLM fine-tuned with high-quality domain corpus. Furthermore, we have developed and released a documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced RTL-to-GDSII design platform. Experimental results demonstrate that our proposed RAG flow and techniques have achieved superior performance on ORD-QA as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA benchmark and the training dataset for our customized RAG flow are open-source at this https URL.</li>
</ul>

<h3>Title: X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images</h3>
<ul>
<li><strong>Authors: </strong>Yunpeng Wang, Kang Wang, Yaoyao Zhuo, Weiya Shi, Fei Shan, Lei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15356">https://arxiv.org/abs/2407.15356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15356">https://arxiv.org/pdf/2407.15356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15356]] X-Recon: Learning-based Patient-specific High-Resolution CT Reconstruction from Orthogonal X-Ray Images(https://arxiv.org/abs/2407.15356)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and computed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is commonly used for initial localization of pneumothorax, while CT ensures accurate quantification. However, CT scans involve high radiation doses and can be costly. To achieve precise quantitative diagnosis while minimizing radiation exposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based on ortho-lateral chest X-ray images. X-Recon integrates generative adversarial networks (GANs), including a generator with a multi-scale fusion rendering module and a discriminator enhanced by 3D coordinate convolutional layers, designed to facilitate CT reconstruction. To improve precision, a projective spatial transformer is utilized to incorporate multi-angle projection loss. Additionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation algorithm, combining image processing techniques with deep-learning models for the segmentation of air-accumulated regions and lung structures. Experiments on a large-scale dataset demonstrate its superiority over existing approaches. X-Recon achieved a significantly higher reconstruction resolution with a higher average spatial resolution and a lower average slice thickness. The reconstruction metrics achieved state-of-the-art performance in terms of several metrics including peak signal-to-noise ratio. The zero-shot segmentation algorithm, PTX-Seg, also demonstrated high segmentation precision for the air-accumulated region, the left lung, and the right lung. Moreover, the consistency analysis for the pneumothorax chest occupancy ratio between reconstructed CT and original CT obtained a high correlation coefficient. Code will be available at: this https URL</li>
</ul>

<h3>Title: A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Yingxue Xu, Yihui Wang, Fengtao Zhou, Jiabo Ma, Shu Yang, Huangjing Lin, Xin Wang, Jiguang Wang, Li Liang, Anjia Han, Ronald Cheong Kin Chan, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15362">https://arxiv.org/abs/2407.15362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15362">https://arxiv.org/pdf/2407.15362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15362]] A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model(https://arxiv.org/abs/2407.15362)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Remarkable strides in computational pathology have been made in the task-agnostic foundation model that advances the performance of a wide array of downstream clinical tasks. Despite the promising performance, there are still several challenges. First, prior works have resorted to either vision-only or vision-captions data, disregarding invaluable pathology reports and gene expression profiles which respectively offer distinct knowledge for versatile clinical applications. Second, the current progress in pathology FMs predominantly concentrates on the patch level, where the restricted context of patch-level pretraining fails to capture whole-slide patterns. Here we curated the largest multimodal dataset consisting of H\&E diagnostic whole slide images and their associated pathology reports and RNA-Seq data, resulting in 26,169 slide-level modality pairs from 10,275 patients across 32 cancer types. To leverage these data for CPath, we propose a novel whole-slide pretraining paradigm which injects multimodal knowledge at the whole-slide context into the pathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed paradigm revolutionizes the workflow of pretraining for CPath, which enables the pathology FM to acquire the whole-slide context. To our knowledge, this is the first attempt to incorporate multimodal knowledge at the slide level for enhancing pathology FMs, expanding the modelling context from unimodal to multimodal knowledge and from patch-level to slide-level. To systematically evaluate the capabilities of mSTAR, extensive experiments including slide-level unimodal and multimodal applications, are conducted across 7 diverse types of tasks on 43 subtasks, resulting in the largest spectrum of downstream tasks. The average performance in various slide-level applications consistently demonstrates significant performance enhancements for mSTAR compared to SOTA FMs.</li>
</ul>

<h3>Title: A Solution toward Transparent and Practical AI Regulation: Privacy Nutrition Labels for Open-source Generative AI-based Applications</h3>
<ul>
<li><strong>Authors: </strong>Meixue Si, Shidong Pan, Dianshu Liao, Xiaoyu Sun, Zhen Tao, Wenchang Shi, Zhenchang Xing</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15407">https://arxiv.org/abs/2407.15407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15407">https://arxiv.org/pdf/2407.15407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15407]] A Solution toward Transparent and Practical AI Regulation: Privacy Nutrition Labels for Open-source Generative AI-based Applications(https://arxiv.org/abs/2407.15407)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid development and widespread adoption of Generative Artificial Intelligence-based (GAI) applications have greatly enriched our daily lives, benefiting people by enhancing creativity, personalizing experiences, improving accessibility, and fostering innovation and efficiency across various domains. However, along with the development of GAI applications, concerns have been raised about transparency in their privacy practices. Traditional privacy policies often fail to effectively communicate essential privacy information due to their complexity and length, and open-source community developers often neglect privacy practices even more. Only 12.2% of examined open-source GAI apps provide a privacy policy. To address this, we propose a regulation-driven GAI Privacy Label and introduce Repo2Label, a novel framework for automatically generating these labels based on code repositories. Our user study indicates a common endorsement of the proposed GAI privacy label format. Additionally, Repo2Label achieves a precision of 0.81, recall of 0.88, and F1-score of 0.84 based on the benchmark dataset, significantly outperforming the developer self-declared privacy notices. We also discuss the common regulatory (in)compliance of open-source GAI apps, comparison with other privacy notices, and broader impacts to different stakeholders. Our findings suggest that Repo2Label could serve as a significant tool for bolstering the privacy transparency of GAI apps and make them more practical and responsible.</li>
</ul>

<h3>Title: Bidirectional skip-frame prediction for video anomaly detection with intra-domain disparity-driven attention</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Lyu, Minghua Zhao, Jing Hu, Runtao Xi, Xuewen Huang, Shuangli Du, Cheng Shi, Tian Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15424">https://arxiv.org/abs/2407.15424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15424">https://arxiv.org/pdf/2407.15424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15424]] Bidirectional skip-frame prediction for video anomaly detection with intra-domain disparity-driven attention(https://arxiv.org/abs/2407.15424)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>With the widespread deployment of video surveillance devices and the demand for intelligent system development, video anomaly detection (VAD) has become an important part of constructing intelligent surveillance systems. Expanding the discriminative boundary between normal and abnormal events to enhance performance is the common goal and challenge of VAD. To address this problem, we propose a Bidirectional Skip-frame Prediction (BiSP) network based on a dual-stream autoencoder, from the perspective of learning the intra-domain disparity between different features. The BiSP skips frames in the training phase to achieve the forward and backward frame prediction respectively, and in the testing phase, it utilizes bidirectional consecutive frames to co-predict the same intermediate frames, thus expanding the degree of disparity between normal and abnormal events. The BiSP designs the variance channel attention and context spatial attention from the perspectives of movement patterns and object scales, respectively, thus ensuring the maximization of the disparity between normal and abnormal in the feature extraction and delivery with different dimensions. Extensive experiments from four benchmark datasets demonstrate the effectiveness of the proposed BiSP, which substantially outperforms state-of-the-art competing methods.</li>
</ul>

<h3>Title: Text2Place: Affordance-aware Text Guided Human Placement</h3>
<ul>
<li><strong>Authors: </strong>Rishubh Parihar, Harsh Gupta, Sachidanand VS, R. Venkatesh Babu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15446">https://arxiv.org/abs/2407.15446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15446">https://arxiv.org/pdf/2407.15446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15446]] Text2Place: Affordance-aware Text Guided Human Placement(https://arxiv.org/abs/2407.15446)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>For a given scene, humans can easily reason for the locations and pose to place objects. Designing a computational model to reason about these affordances poses a significant challenge, mirroring the intuitive reasoning abilities of humans. This work tackles the problem of realistic human insertion in a given background scene termed as \textbf{Semantic Human Placement}. This task is extremely challenging given the diverse backgrounds, scale, and pose of the generated person and, finally, the identity preservation of the person. We divide the problem into the following two stages \textbf{i)} learning \textit{semantic masks} using text guidance for localizing regions in the image to place humans and \textbf{ii)} subject-conditioned inpainting to place a given subject adhering to the scene affordance within the \textit{semantic masks}. For learning semantic masks, we leverage rich object-scene priors learned from the text-to-image generative models and optimize a novel parameterization of the semantic mask, eliminating the need for large-scale training. To the best of our knowledge, we are the first ones to provide an effective solution for realistic human placements in diverse real-world scenes. The proposed method can generate highly realistic scene compositions while preserving the background and subject identity. Further, we present results for several downstream tasks - scene hallucination from a single or multiple generated persons and text-based attribute editing. With extensive comparisons against strong baselines, we show the superiority of our method in realistic human placement.</li>
</ul>

<h3>Title: In-Context Learning Improves Compositional Understanding of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Matteo Nulli, Anesa Ibrahimi, Avik Pal, Hoshe Lee, Ivona Najdenkoska</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15487">https://arxiv.org/abs/2407.15487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15487">https://arxiv.org/pdf/2407.15487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15487]] In-Context Learning Improves Compositional Understanding of Vision-Language Models(https://arxiv.org/abs/2407.15487)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have shown remarkable capabilities in a large number of downstream tasks. Nonetheless, compositional image understanding remains a rather difficult task due to the object bias present in training data. In this work, we investigate the reasons for such a lack of capability by performing an extensive bench-marking of compositional understanding in VLMs. We compare contrastive models with generative ones and analyze their differences in architecture, pre-training data, and training tasks and losses. Furthermore, we leverage In-Context Learning (ICL) as a way to improve the ability of VLMs to perform more complex reasoning and understanding given an image. Our extensive experiments demonstrate that our proposed approach outperforms baseline models across multiple compositional understanding datasets.</li>
</ul>

<h3>Title: DiffX: Guide Your Layout to Cross-Modal Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Wang, Jingyu Lin, Yifei Qian, Yi Huang, Shicen Tian, Bosong Chai, Juncan Deng, Lan Du, Cunjian Chen, Yufei Guo, Kejie Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15488">https://arxiv.org/abs/2407.15488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15488">https://arxiv.org/pdf/2407.15488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15488]] DiffX: Guide Your Layout to Cross-Modal Generative Modeling(https://arxiv.org/abs/2407.15488)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have made significant strides in text-driven and layout-driven image generation. However, most diffusion models are limited to visible RGB image generation. In fact, human perception of the world is enriched by diverse viewpoints, including chromatic contrast, thermal illumination, and depth information. In this paper, we introduce a novel diffusion model for general layout-guided cross-modal "RGB+X" generation, called DiffX. We firstly construct the cross-modal image datasets with text descriptions using the LLaVA model for image captioning, supplemented by manual corrections. Notably, DiffX presents a simple yet effective cross-modal generative modeling pipeline, which conducts diffusion and denoising processes in the modality-shared latent space, facilitated by our Dual-Path Variational AutoEncoder (DP-VAE). Furthermore, we incorporate the gated cross-attention mechanism to connect the layout and text conditions, leveraging Long-CLIP for embedding long captions to enhance user guidance. Through extensive experiments, DiffX demonstrates robustness and flexibility in cross-modal generation across three RGB+X datasets: FLIR, MFNet, and COME15K, guided by various layout types. It also shows the potential for adaptive generation of "RGB+X+Y" or more diverse modalities. Our code and processed image captions are available at this https URL.</li>
</ul>

<h3>Title: TextureCrop: Enhancing Synthetic Image Detection through Texture-based Cropping</h3>
<ul>
<li><strong>Authors: </strong>Despina Konstantinidou, Christos Koutlis, Symeon Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15500">https://arxiv.org/abs/2407.15500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15500">https://arxiv.org/pdf/2407.15500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15500]] TextureCrop: Enhancing Synthetic Image Detection through Texture-based Cropping(https://arxiv.org/abs/2407.15500)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI technologies produce hyper-realistic imagery that can be used for nefarious purposes such as producing misleading or harmful content, among others. This makes Synthetic Image Detection (SID) an essential tool for defending against AI-generated harmful content. Current SID methods typically resize input images to a fixed resolution or perform center-cropping due to computational concerns, leading to challenges in effectively detecting artifacts in high-resolution images. To this end, we propose TextureCrop, a novel image pre-processing technique. By focusing on high-frequency image parts where generation artifacts are prevalent, TextureCrop effectively enhances SID accuracy while maintaining manageable memory requirements. Experimental results demonstrate a consistent improvement in AUC across various detectors by 5.7% compared to center cropping and by 14% compared to resizing, across high-resolution images from the Forensynths and Synthbuster datasets.</li>
</ul>

<h3>Title: WebRPG: Automatic Web Rendering Parameters Generation for Visual Presentation</h3>
<ul>
<li><strong>Authors: </strong>Zirui Shao, Feiyu Gao, Hangdi Xing, Zepeng Zhu, Zhi Yu, Jiajun Bu, Qi Zheng, Cong Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15502">https://arxiv.org/abs/2407.15502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15502">https://arxiv.org/pdf/2407.15502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15502]] WebRPG: Automatic Web Rendering Parameters Generation for Visual Presentation(https://arxiv.org/abs/2407.15502)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the era of content creation revolution propelled by advancements in generative models, the field of web design remains unexplored despite its critical role in modern digital communication. The web design process is complex and often time-consuming, especially for those with limited expertise. In this paper, we introduce Web Rendering Parameters Generation (WebRPG), a new task that aims at automating the generation for visual presentation of web pages based on their HTML code. WebRPG would contribute to a faster web development workflow. Since there is no existing benchmark available, we develop a new dataset for WebRPG through an automated pipeline. Moreover, we present baseline models, utilizing VAE to manage numerous elements and rendering parameters, along with custom HTML embedding for capturing essential semantic and hierarchical information from HTML. Extensive experiments, including customized quantitative evaluations for this specific task, are conducted to evaluate the quality of the generated results.</li>
</ul>

<h3>Title: SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over Time</h3>
<ul>
<li><strong>Authors: </strong>Stanislav Frolov, Brian B. Moser, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15507">https://arxiv.org/abs/2407.15507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15507">https://arxiv.org/pdf/2407.15507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15507]] SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over Time(https://arxiv.org/abs/2407.15507)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating high-resolution images with generative models has recently been made widely accessible by leveraging diffusion models pre-trained on large-scale datasets. Various techniques, such as MultiDiffusion and SyncDiffusion, have further pushed image generation beyond training resolutions, i.e., from square images to panorama, by merging multiple overlapping diffusion paths or employing gradient descent to maintain perceptual coherence. However, these methods suffer from significant computational inefficiencies due to generating and averaging numerous predictions, which is required in practice to produce high-quality and seamless images. This work addresses this limitation and presents a novel approach that eliminates the need to generate and average numerous overlapping denoising predictions. Our method shifts non-overlapping denoising windows over time, ensuring that seams in one timestep are corrected in the next. This results in coherent, high-resolution images with fewer overall steps. We demonstrate the effectiveness of our approach through qualitative and quantitative evaluations, comparing it with MultiDiffusion, SyncDiffusion, and StitchDiffusion. Our method offers several key benefits, including improved computational efficiency and faster inference times while producing comparable or better image quality.</li>
</ul>

<h3>Title: Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks</h3>
<ul>
<li><strong>Authors: </strong>Eugenio Lomurno, Matteo Matteucci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15526">https://arxiv.org/abs/2407.15526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15526">https://arxiv.org/pdf/2407.15526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15526]] Synthetic Image Learning: Preserving Performance and Preventing Membership Inference Attacks(https://arxiv.org/abs/2407.15526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence has transformed the generation of synthetic data, providing innovative solutions to challenges like data scarcity and privacy, which are particularly critical in fields such as medicine. However, the effective use of this synthetic data to train high-performance models remains a significant challenge. This paper addresses this issue by introducing Knowledge Recycling (KR), a pipeline designed to optimise the generation and use of synthetic data for training downstream classifiers. At the heart of this pipeline is Generative Knowledge Distillation (GKD), the proposed technique that significantly improves the quality and usefulness of the information provided to classifiers through a synthetic dataset regeneration and soft labelling mechanism. The KR pipeline has been tested on a variety of datasets, with a focus on six highly heterogeneous medical image datasets, ranging from retinal images to organ scans. The results show a significant reduction in the performance gap between models trained on real and synthetic data, with models based on synthetic data outperforming those trained on real data in some cases. Furthermore, the resulting models show almost complete immunity to Membership Inference Attacks, manifesting privacy properties missing in models trained with conventional techniques.</li>
</ul>

<h3>Title: An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Yuetong Zhao, Hongyu Cao, Xianyu Zhao, Zhijian Ou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15569">https://arxiv.org/abs/2407.15569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15569">https://arxiv.org/pdf/2407.15569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15569]] An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought(https://arxiv.org/abs/2407.15569)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become essential tools in daily life. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Augmented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.</li>
</ul>

<h3>Title: Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts</h3>
<ul>
<li><strong>Authors: </strong>Soojin Yoon, Sungho Ko, Tongyoung Kim, SeongKu Kang, Jinyoung Yeo, Dongha Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15588">https://arxiv.org/abs/2407.15588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15588">https://arxiv.org/pdf/2407.15588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15588]] Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of Entity and Relation Texts(https://arxiv.org/abs/2407.15588)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Cross-lingual entity alignment (EA) enables the integration of multiple knowledge graphs (KGs) across different languages, providing users with seamless access to diverse and comprehensive knowledge.Existing methods, mostly supervised, face challenges in obtaining labeled entity pairs. To address this, recent studies have shifted towards a self-supervised and unsupervised frameworks. Despite their effectiveness, these approaches have limitations: (1) they mainly focus on entity features, neglecting the semantic information of relations, (2) they assume isomorphism between source and target graphs, leading to noise and reduced alignment accuracy, and (3) they are susceptible to noise in the textual features, especially when encountering inconsistent translations or Out-Of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an unsupervised and robust cross-lingual EA framework that jointly performs Entity-level and Relation-level Alignment using semantic textual features of relations and entities. Its refinement process iteratively enhances results by fusing entity-level and relation-level alignments based on neighbor triple matching. The additional verification process examines the entities' neighbor triples as the linearized text. This \textit{Align-and-Verify} pipeline that rigorously assesses alignment results, achieving near-perfect alignment even in the presence of noisy textual features of entities. Our extensive experiments demonstrate that robustness and general applicability of \proposed improved the accuracy and effectiveness of EA tasks, contributing significantly to knowledge-oriented applications.</li>
</ul>

<h3>Title: Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Amir Mohammad Karimi Mamaghan, Samuele Papa, Karl Henrik Johansson, Stefan Bauer, Andrea Dittadi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15589">https://arxiv.org/abs/2407.15589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15589">https://arxiv.org/pdf/2407.15589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15589]] Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models(https://arxiv.org/abs/2407.15589)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Object-centric (OC) representations, which represent the state of a visual scene by modeling it as a composition of objects, have the potential to be used in various downstream tasks to achieve systematic compositional generalization and facilitate reasoning. However, these claims have not been thoroughly analyzed yet. Recently, foundation models have demonstrated unparalleled capabilities across diverse domains from language to computer vision, marking them as a potential cornerstone of future research for a multitude of computational tasks. In this paper, we conduct an extensive empirical study on representation learning for downstream Visual Question Answering (VQA), which requires an accurate compositional understanding of the scene. We thoroughly investigate the benefits and trade-offs of OC models and alternative approaches including large pre-trained foundation models on both synthetic and real-world data, and demonstrate a viable way to achieve the best of both worlds. The extensiveness of our study, encompassing over 800 downstream VQA models and 15 different types of upstream representations, also provides several additional insights that we believe will be of interest to the community at large.</li>
</ul>

<h3>Title: Learning Where to Look: Self-supervised Viewpoint Selection for Active Localization using Geometrical Information</h3>
<ul>
<li><strong>Authors: </strong>Luca Di Giammarino, Boyang Sun, Giorgio Grisetti, Marc Pollefeys, Hermann Blum, Daniel Barath</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15593">https://arxiv.org/abs/2407.15593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15593">https://arxiv.org/pdf/2407.15593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15593]] Learning Where to Look: Self-supervised Viewpoint Selection for Active Localization using Geometrical Information(https://arxiv.org/abs/2407.15593)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate localization in diverse environments is a fundamental challenge in computer vision and robotics. The task involves determining a sensor's precise position and orientation, typically a camera, within a given space. Traditional localization methods often rely on passive sensing, which may struggle in scenarios with limited features or dynamic environments. In response, this paper explores the domain of active localization, emphasizing the importance of viewpoint selection to enhance localization accuracy. Our contributions involve using a data-driven approach with a simple architecture designed for real-time operation, a self-supervised data training method, and the capability to consistently integrate our map into a planning framework tailored for real-world robotics applications. Our results demonstrate that our method performs better than the existing one, targeting similar problems and generalizing on synthetic and real data. We also release an open-source implementation to benefit the community.</li>
</ul>

<h3>Title: Discrete Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, Yaron Lipman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15595">https://arxiv.org/abs/2407.15595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15595">https://arxiv.org/pdf/2407.15595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15595]] Discrete Flow Matching(https://arxiv.org/abs/2407.15595)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: (i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser ($x$-prediction) and noise-prediction ($\epsilon$-prediction); (iii) practically, focusing on specific probability paths defined with different schedulers considerably improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models.</li>
</ul>

<h3>Title: Semi-Supervised Learning for Anomaly Detection in Blockchain-based Supply Chains</h3>
<ul>
<li><strong>Authors: </strong>Do Hai Son, Bui Duc Manh, Tran Viet Khoa, Nguyen Linh Trung, Dinh Thai Hoang, Hoang Trong Minh, Yibeltal Alem, Le Quang Minh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15603">https://arxiv.org/abs/2407.15603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15603">https://arxiv.org/pdf/2407.15603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15603]] Semi-Supervised Learning for Anomaly Detection in Blockchain-based Supply Chains(https://arxiv.org/abs/2407.15603)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Blockchain-based supply chain (BSC) systems have tremendously been developed recently and can play an important role in our society in the future. In this study, we develop an anomaly detection model for BSC systems. Our proposed model can detect cyber-attacks at various levels, including the network layer, consensus layer, and beyond, by analyzing only the traffic data at the network layer. To do this, we first build a BSC system at our laboratory to perform experiments and collect datasets. We then propose a novel semi-supervised DAE-MLP (Deep AutoEncoder-Multilayer Perceptron) that combines the advantages of supervised and unsupervised learning to detect anomalies in BSC systems. The experimental results demonstrate the effectiveness of our model for anomaly detection within BSCs, achieving a detection accuracy of 96.5%. Moreover, DAE-MLP can effectively detect new attacks by improving the F1-score up to 33.1% after updating the MLP component.</li>
</ul>

<h3>Title: Probing Fine-Grained Action Understanding and Cross-View Generalization of Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Thinesh Thiyakesan Ponbagavathi, Kunyu Peng, Alina Roitberg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15605">https://arxiv.org/abs/2407.15605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15605">https://arxiv.org/pdf/2407.15605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15605]] Probing Fine-Grained Action Understanding and Cross-View Generalization of Foundation Models(https://arxiv.org/abs/2407.15605)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) are large neural networks trained on broad datasets, excelling in downstream tasks with minimal fine-tuning. Human activity recognition in video has advanced with FMs, driven by competition among different architectures. However, high accuracies on standard benchmarks can draw an artificially rosy picture, as they often overlook real-world factors like changing camera perspectives. Popular benchmarks, mostly from YouTube or movies, offer diverse views but only coarse actions, which are insufficient for use-cases needing fine-grained, domain-specific actions. Domain-specific datasets (e.g., for industrial assembly) typically use data from limited static perspectives. This paper empirically evaluates how perspective changes affect different FMs in fine-grained human activity recognition. We compare multiple backbone architectures and design choices, including image- and video- based models, and various strategies for temporal information fusion, including commonly used score averaging and more novel attention-based temporal aggregation mechanisms. This is the first systematic study of different foundation models and specific design choices for human activity recognition from unknown views, conducted with the goal to provide guidance for backbone- and temporal- fusion scheme selection. Code and models will be made publicly available to the community.</li>
</ul>

<h3>Title: StylusAI: Stylistic Adaptation for Robust German Handwritten Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Nauman Riaz, Saifullah Saifullah, Stefan Agne, Andreas Dengel, Sheraz Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15608">https://arxiv.org/abs/2407.15608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15608">https://arxiv.org/pdf/2407.15608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15608]] StylusAI: Stylistic Adaptation for Robust German Handwritten Text Generation(https://arxiv.org/abs/2407.15608)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this study, we introduce StylusAI, a novel architecture leveraging diffusion models in the domain of handwriting style generation. StylusAI is specifically designed to adapt and integrate the stylistic nuances of one language's handwriting into another, particularly focusing on blending English handwriting styles into the context of the German writing system. This approach enables the generation of German text in English handwriting styles and German handwriting styles into English, enriching machine-generated handwriting diversity while ensuring that the generated text remains legible across both languages. To support the development and evaluation of StylusAI, we present the \lq{Deutscher Handschriften-Datensatz}\rq~(DHSD), a comprehensive dataset encompassing 37 distinct handwriting styles within the German language. This dataset provides a fundamental resource for training and benchmarking in the realm of handwritten text generation. Our results demonstrate that StylusAI not only introduces a new method for style adaptation in handwritten text generation but also surpasses existing models in generating handwriting samples that improve both text quality and stylistic fidelity, evidenced by its performance on the IAM database and our newly proposed DHSD. Thus, StylusAI represents a significant advancement in the field of handwriting style generation, offering promising avenues for future research and applications in cross-linguistic style adaptation for languages with similar scripts.</li>
</ul>

<h3>Title: Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Ma, Yaohui Wang, Gengyu Jia, Xinyuan Chen, Yuan-Fang Li, Cunjian Chen, Yu Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15642">https://arxiv.org/abs/2407.15642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15642">https://arxiv.org/pdf/2407.15642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15642]] Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models(https://arxiv.org/abs/2407.15642)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved great progress in image animation due to powerful generative capabilities. However, maintaining spatio-temporal consistency with detailed information from the input static image over time (e.g., style, background, and object of the input static image) and ensuring smoothness in animated video narratives guided by textual prompts still remains challenging. In this paper, we introduce Cinemo, a novel image animation approach towards achieving better motion controllability, as well as stronger temporal consistency and smoothness. In general, we propose three effective strategies at the training and inference stages of Cinemo to accomplish our goal. At the training stage, Cinemo focuses on learning the distribution of motion residuals, rather than directly predicting subsequent via a motion diffusion model. Additionally, a structural similarity index-based strategy is proposed to enable Cinemo to have better controllability of motion intensity. At the inference stage, a noise refinement technique based on discrete cosine transformation is introduced to mitigate sudden motion changes. Such three strategies enable Cinemo to produce highly consistent, smooth, and motion-controllable results. Compared to previous methods, Cinemo offers simpler and more precise user controllability. Extensive experiments against several state-of-the-art methods, including both commercial tools and research approaches, across multiple metrics, demonstrate the effectiveness and superiority of our proposed approach.</li>
</ul>

<h3>Title: TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly</h3>
<ul>
<li><strong>Authors: </strong>Mengqi Guo, Chen Li, Yuyang Zhao, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15648">https://arxiv.org/abs/2407.15648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15648">https://arxiv.org/pdf/2407.15648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15648]] TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly(https://arxiv.org/abs/2407.15648)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Inferring step-wise actions to assemble 3D objects with primitive bricks from images is a challenging task due to complex constraints and the vast number of possible combinations. Recent studies have demonstrated promising results on sequential LEGO brick assembly through the utilization of LEGO-Graph modeling to predict sequential actions. However, existing approaches are class-specific and require significant computational and 3D annotation resources. In this work, we first propose a computationally efficient breadth-first search (BFS) LEGO-Tree structure to model the sequential assembly actions by considering connections between consecutive layers. Based on the LEGO-Tree structure, we then design a class-agnostic tree-transformer framework to predict the sequential assembly actions from the input multi-view images. A major challenge of the sequential brick assembly task is that the step-wise action labels are costly and tedious to obtain in practice. We mitigate this problem by leveraging synthetic-to-real transfer learning. Specifically, our model is first pre-trained on synthetic data with full supervision from the available action labels. We then circumvent the requirement for action labels in the real data by proposing an action-to-silhouette projection that replaces action labels with input image silhouettes for self-supervision. Without any annotation on the real data, our model outperforms existing methods with 3D supervision by 7.8% and 11.3% in mIoU on the MNIST and ModelNet Construction datasets, respectively.</li>
</ul>

<h3>Title: DriveDiTFit: Fine-tuning Diffusion Transformers for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Jiahang Tu, Wei Ji, Hanbin Zhao, Chao Zhang, Roger Zimmermann, Hui Qian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15661">https://arxiv.org/abs/2407.15661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15661">https://arxiv.org/pdf/2407.15661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15661]] DriveDiTFit: Fine-tuning Diffusion Transformers for Autonomous Driving(https://arxiv.org/abs/2407.15661)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In autonomous driving, deep models have shown remarkable performance across various visual perception tasks with the demand of high-quality and huge-diversity training datasets. Such datasets are expected to cover various driving scenarios with adverse weather, lighting conditions and diverse moving objects. However, manually collecting these data presents huge challenges and expensive cost. With the rapid development of large generative models, we propose DriveDiTFit, a novel method for efficiently generating autonomous Driving data by Fine-tuning pre-trained Diffusion Transformers (DiTs). Specifically, DriveDiTFit utilizes a gap-driven modulation technique to carefully select and efficiently fine-tune a few parameters in DiTs according to the discrepancy between the pre-trained source data and the target driving data. Additionally, DriveDiTFit develops an effective weather and lighting condition embedding module to ensure diversity in the generated data, which is initialized by a nearest-semantic-similarity initialization approach. Through progressive tuning scheme to refined the process of detail generation in early diffusion process and enlarging the weights corresponding to small objects in training loss, DriveDiTFit ensures high-quality generation of small moving objects in the generated data. Extensive experiments conducted on driving datasets confirm that our method could efficiently produce diverse real driving data. The source codes will be available at this https URL.</li>
</ul>

<h3>Title: Enhancing Transferability of Targeted Adversarial Examples: A Self-Universal Perspective</h3>
<ul>
<li><strong>Authors: </strong>Bowen Peng, Li Liu, Tianpeng Liu, Zhen Liu, Yongxiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15683">https://arxiv.org/abs/2407.15683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15683">https://arxiv.org/pdf/2407.15683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15683]] Enhancing Transferability of Targeted Adversarial Examples: A Self-Universal Perspective(https://arxiv.org/abs/2407.15683)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Transfer-based targeted adversarial attacks against black-box deep neural networks (DNNs) have been proven to be significantly more challenging than untargeted ones. The impressive transferability of current SOTA, the generative methods, comes at the cost of requiring massive amounts of additional data and time-consuming training for each targeted label. This results in limited efficiency and flexibility, significantly hindering their deployment in practical applications. In this paper, we offer a self-universal perspective that unveils the great yet underexplored potential of input transformations in pursuing this goal. Specifically, transformations universalize gradient-based attacks with intrinsic but overlooked semantics inherent within individual images, exhibiting similar scalability and comparable results to time-consuming learning over massive additional data from diverse classes. We also contribute a surprising empirical insight that one of the most fundamental transformations, simple image scaling, is highly effective, scalable, sufficient, and necessary in enhancing targeted transferability. We further augment simple scaling with orthogonal transformations and block-wise applicability, resulting in the Simple, faSt, Self-universal yet Strong Scale Transformation (S$^4$ST) for self-universal TTA. On the ImageNet-Compatible benchmark dataset, our method achieves a 19.8% improvement in the average targeted transfer success rate against various challenging victim models over existing SOTA transformation methods while only consuming 36% time for attacking. It also outperforms resource-intensive attacks by a large margin in various challenging settings.</li>
</ul>

<h3>Title: Estimating Probability Densities with Transformer and Denoising Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Henry W. Leung, Jo Bovy, Joshua S. Speagle</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.IM, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15703">https://arxiv.org/abs/2407.15703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15703">https://arxiv.org/pdf/2407.15703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15703]] Estimating Probability Densities with Transformer and Denoising Diffusion(https://arxiv.org/abs/2407.15703)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Transformers are often the go-to architecture to build foundation models that ingest a large amount of training data. But these models do not estimate the probability density distribution when trained on regression problems, yet obtaining full probabilistic outputs is crucial to many fields of science, where the probability distribution of the answer can be non-Gaussian and multimodal. In this work, we demonstrate that training a probabilistic model using a denoising diffusion head on top of the Transformer provides reasonable probability density estimation even for high-dimensional inputs. The combined Transformer+Denoising Diffusion model allows conditioning the output probability density on arbitrary combinations of inputs and it is thus a highly flexible density function emulator of all possible input/output combinations. We illustrate our Transformer+Denoising Diffusion model by training it on a large dataset of astronomical observations and measured labels of stars within our Galaxy and we apply it to a variety of inference tasks to show that the model can infer labels accurately with reasonable distributions.</li>
</ul>

<h3>Title: Predicting the Best of N Visual Trackers</h3>
<ul>
<li><strong>Authors: </strong>Basit Alawode, Sajid Javed, Arif Mahmood, Jiri Matas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15707">https://arxiv.org/abs/2407.15707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15707">https://arxiv.org/pdf/2407.15707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15707]] Predicting the Best of N Visual Trackers(https://arxiv.org/abs/2407.15707)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We observe that the performance of SOTA visual trackers surprisingly strongly varies across different video attributes and datasets. No single tracker remains the best performer across all tracking attributes and datasets. To bridge this gap, for a given video sequence, we predict the "Best of the N Trackers", called the BofN meta-tracker. At its core, a Tracking Performance Prediction Network (TP2N) selects a predicted best performing visual tracker for the given video sequence using only a few initial frames. We also introduce a frame-level BofN meta-tracker which keeps predicting best performer after regular temporal intervals. The TP2N is based on self-supervised learning architectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with ViT-S as a backbone performs the best. The video-level BofN meta-tracker outperforms, by a large margin, existing SOTA trackers on nine standard benchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123, OTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN meta-tracker effectively handling variations in the tracking scenarios within long sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is 88.7% and 91.1% with video and frame-level settings respectively. The best performing tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average overlap is 67.88% and 70.98% with video and frame level settings, compared to the best performing ARTrack, 64.12%. This work also presents an extensive evaluation of competitive tracking methods on all commonly used benchmarks, following their protocols. The code, the trained models, and the results will soon be made publicly available on this https URL.</li>
</ul>

<h3>Title: GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI</h3>
<ul>
<li><strong>Authors: </strong>Zhaojie Fang, Shenghao Zhu, Yifei Chen, Binfeng Zou, Fan Jia, Linwei Qiu, Chang Liu, Yiyu Huang, Xiang Feng, Feiwei Qin, Changmiao Wang, Yeru Wang, Jin Fan, Changbiao Chu, Wan-Zhen Wu, Hu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15719">https://arxiv.org/abs/2407.15719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15719">https://arxiv.org/pdf/2407.15719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15719]] GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI(https://arxiv.org/abs/2407.15719)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder that often progresses from Mild Cognitive Impairment (MCI), leading to memory loss and significantly impacting patients' lives. Clinical trials indicate that early targeted interventions for MCI patients can potentially slow or halt the development and progression of AD. Previous research has shown that accurate medical classification requires the inclusion of extensive multimodal data, such as assessment scales and various neuroimaging techniques like Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). However, consistently tracking the diagnosis of the same individual over time and simultaneously collecting multimodal data poses significant challenges. To address this issue, we introduce GFE-Mamba, a classifier based on Generative Feature Extraction (GFE). This classifier effectively integrates data from assessment scales, MRI, and PET, enabling deeper multimodal fusion. It efficiently extracts both long and short sequence information and incorporates additional information beyond the pixel space. This approach not only improves classification accuracy but also enhances the interpretability and stability of the model. We constructed datasets of over 3000 samples based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) for a two-step training process. Our experimental results demonstrate that the GFE-Mamba model is effective in predicting the conversion from MCI to AD and outperforms several state-of-the-art methods. Our source code and ADNI dataset processing code are available at this https URL.</li>
</ul>

<h3>Title: Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyan Xu, Zhenmei Shi, Yingyu Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15720">https://arxiv.org/abs/2407.15720</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15720">https://arxiv.org/pdf/2407.15720</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15720]] Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability(https://arxiv.org/abs/2407.15720)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have emerged as powerful tools for many AI problems and exhibit remarkable in-context learning (ICL) capabilities. Compositional ability, solving unseen complex tasks that combine two or more simple tasks, is an essential reasoning ability for Artificial General Intelligence. Despite LLM's tremendous success, how they approach composite tasks, especially those not encountered during the pretraining phase, remains an open question and largely ununderstood. In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples. We develop a test suite of composite tasks that include linguistic and logical challenges and perform empirical studies across different LLM families. We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks that involving reasoning multiple steps, where each step represent one task, models typically underperform, and scaling up generally provide no improvements. We offer theoretical analysis in a simplified setting, explaining that models exhibit compositional capability when the task handles different input parts separately. We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale. Our dataset and code are available at {\url{this https URL}}.</li>
</ul>

<h3>Title: DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design</h3>
<ul>
<li><strong>Authors: </strong>Zhi Hao Luo, Luis Lara, Ge Ya Luo, Florian Golemo, Christopher Beckham, Christopher Pal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15723">https://arxiv.org/abs/2407.15723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15723">https://arxiv.org/pdf/2407.15723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15723]] DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design(https://arxiv.org/abs/2407.15723)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text conditioned generative models for images have yielded impressive results. Text conditioned floorplan generation as a special type of raster image generation task also received particular attention. However there are many use cases in floorpla generation where numerical properties of the generated result are more important than the aesthetics. For instance, one might want to specify sizes for certain rooms in a floorplan and compare the generated floorplan with given specifications Current approaches, datasets and commonly used evaluations do not support these kinds of constraints. As such, an attractive strategy is to generate an intermediate data structure that contains numerical properties of a floorplan which can be used to generate the final floorplan image. To explore this setting we (1) construct a new dataset for this data-structure to data-structure formulation of floorplan generation using two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and provide the tools to convert further procedurally generated ProcTHOR floorplan data into our format. (2) We explore the task of floorplan generation given a partial or complete set of constraints and we design a series of metrics and benchmarks to enable evaluating how well samples generated from models respect the constraints. (3) We create multiple baselines by finetuning a large language model (LLM), Llama3, and demonstrate the feasibility of using floorplan data structure conditioned LLMs for the problem of floorplan generation respecting numerical constraints. We hope that our new datasets and benchmarks will encourage further research on different ways to improve the performance of LLMs and other generative modelling techniques for generating designs where quantitative constraints are only partially specified, but must be respected.</li>
</ul>

<h3>Title: Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language Encoders</h3>
<ul>
<li><strong>Authors: </strong>Laura Niss, Kevin Vogt-Lowell, Theodoros Tsiligkaridis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15731">https://arxiv.org/abs/2407.15731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15731">https://arxiv.org/pdf/2407.15731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15731]] Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language Encoders(https://arxiv.org/abs/2407.15731)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Despite the proliferation of large vision-language foundation models, estimation of the learning and forgetting outcomes following fine-tuning of these models remains largely unexplored. Inspired by work highlighting the significance of the modality gap in contrastive dual-encoders, we propose the Inter-Intra Modal Measure (IIMM). Combining terms quantifying the similarity between image embeddings and the similarity between incorrect image and label embedding pairs, the IIMM functions as a strong predictor of performance changes with fine-tuning. Our extensive empirical analysis across four state-of-the-art vision-language models (CLIP, SigLIP, CoCa, EVA-02-CLIP) and five fine-tuning techniques (full fine-tuning, BitFit, attention-weight tuning, LoRA, CLIP-Adapter) demonstrates a strong, statistically significant linear relationship: fine-tuning on tasks with higher IIMM scores produces greater in-domain performance gains but also induces more severe out-of-domain performance degradation, with some parameter-efficient fine-tuning (PEFT) methods showing extreme forgetting. We compare our measure against transfer scores from state-of-the-art model selection methods and show that the IIMM is significantly more predictive of accuracy gains. With only a single forward pass of the target data, practitioners can leverage this key insight to heuristically evaluate the degree to which a model can be expected to improve following fine-tuning. Given additional knowledge about the model's performance on a few diverse tasks, this heuristic further evolves into a strong predictor of expected performance changes when training for new tasks.</li>
</ul>

<h3>Title: Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Silvio Galesso, Philipp Schröppel, Hssan Driss, Thomas Brox</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15739">https://arxiv.org/abs/2407.15739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15739">https://arxiv.org/pdf/2407.15739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15739]] Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond(https://arxiv.org/abs/2407.15739)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, research on out-of-distribution (OoD) detection for semantic segmentation has mainly focused on road scenes -- a domain with a constrained amount of semantic diversity. In this work, we challenge this constraint and extend the domain of this task to general natural images. To this end, we introduce: 1. the ADE-OoD benchmark, which is based on the ADE20k dataset and includes images from diverse domains with a high semantic diversity, and 2. a novel approach that uses Diffusion score matching for OoD detection (DOoD) and is robust to the increased semantic diversity. ADE-OoD features indoor and outdoor images, defines 150 semantic categories as in-distribution, and contains a variety of OoD objects. For DOoD, we train a diffusion model with an MLP architecture on semantic in-distribution embeddings and build on the score matching interpretation to compute pixel-wise OoD scores at inference time. On common road scene OoD benchmarks, DOoD performs on par or better than the state of the art, without using outliers for training or making assumptions about the data domain. On ADE-OoD, DOoD outperforms previous approaches, but leaves much room for future improvements.</li>
</ul>

<h3>Title: Towards Open-World Object-based Anomaly Detection via Self-Supervised Outlier Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Brian K. S. Isaac-Medina, Yona Falinie A. Gaus, Neelanjan Bhowmik, Toby P. Breckon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15763">https://arxiv.org/abs/2407.15763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15763">https://arxiv.org/pdf/2407.15763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15763]] Towards Open-World Object-based Anomaly Detection via Self-Supervised Outlier Synthesis(https://arxiv.org/abs/2407.15763)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Object detection is a pivotal task in computer vision that has received significant attention in previous years. Nonetheless, the capability of a detector to localise objects out of the training distribution remains unexplored. Whilst recent approaches in object-level out-of-distribution (OoD) detection heavily rely on class labels, such approaches contradict truly open-world scenarios where the class distribution is often unknown. In this context, anomaly detection focuses on detecting unseen instances rather than classifying detections as OoD. This work aims to bridge this gap by leveraging an open-world object detector and an OoD detector via virtual outlier synthesis. This is achieved by using the detector backbone features to first learn object pseudo-classes via self-supervision. These pseudo-classes serve as the basis for class-conditional virtual outlier sampling of anomalous features that are classified by an OoD head. Our approach empowers our overall object detector architecture to learn anomaly-aware feature representations without relying on class labels, hence enabling truly open-world object anomaly detection. Empirical validation of our approach demonstrates its effectiveness across diverse datasets encompassing various imaging modalities (visible, infrared, and X-ray). Moreover, our method establishes state-of-the-art performance on object-level anomaly detection, achieving an average recall score improvement of over 5.4% for natural images and 23.5% for a security X-ray dataset compared to the current approaches. In addition, our method detects anomalies in datasets where current approaches fail. Code available at this https URL.</li>
</ul>

<h3>Title: Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach</h3>
<ul>
<li><strong>Authors: </strong>Rian Dolphin, Joe Dursun, Jonathan Chow, Jarrett Blankenship, Katie Adams, Quinton Pike</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15788">https://arxiv.org/abs/2407.15788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15788">https://arxiv.org/pdf/2407.15788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15788]] Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach(https://arxiv.org/abs/2407.15788)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Financial news plays a crucial role in decision-making processes across the financial sector, yet the efficient processing of this information into a structured format remains challenging. This paper presents a novel approach to financial news processing that leverages Large Language Models (LLMs) to overcome limitations that previously prevented the extraction of structured data from unstructured financial news. We introduce a system that extracts relevant company tickers from raw news article content, performs sentiment analysis at the company level, and generates summaries, all without relying on pre-structured data feeds. Our methodology combines the generative capabilities of LLMs, and recent prompting techniques, with a robust validation framework that uses a tailored string similarity approach. Evaluation on a dataset of 5530 financial news articles demonstrates the effectiveness of our approach, with 90% of articles not missing any tickers compared with current data providers, and 22% of articles having additional relevant tickers. In addition to this paper, the methodology has been implemented at scale with the resulting processed data made available through a live API endpoint, which is updated in real-time with the latest news. To the best of our knowledge, we are the first data provider to offer granular, per-company sentiment analysis from news articles, enhancing the depth of information available to market participants. We also release the evaluation dataset of 5530 processed articles as a static file, which we hope will facilitate further research leveraging financial news.</li>
</ul>

<h3>Title: CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Emanuele Frascaroli, Aniello Panariello, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15793">https://arxiv.org/abs/2407.15793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15793">https://arxiv.org/pdf/2407.15793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15793]] CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning(https://arxiv.org/abs/2407.15793)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, large pre-trained models have become a common strategy to enhance performance in Continual Learning scenarios. This led to the development of numerous prompting strategies to effectively fine-tune transformer-based models without succumbing to catastrophic forgetting. However, these methods struggle to specialize the model on domains significantly deviating from the pre-training and preserving its zero-shot capabilities. In this work, we propose Continual Generative training for Incremental prompt-Learning, a novel approach to mitigate forgetting while adapting a VLM, which exploits generative replay to align prompts to tasks. We also introduce a new metric to evaluate zero-shot capabilities within CL benchmarks. Through extensive experiments on different domains, we demonstrate the effectiveness of our framework in adapting to new tasks while improving zero-shot capabilities. Further analysis reveals that our approach can bridge the gap with joint prompt tuning. The codebase is available at this https URL.</li>
</ul>

<h3>Title: AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yunkang Cao, Jiangning Zhang, Luca Frittoli, Yuqi Cheng, Weiming Shen, Giacomo Boracchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15795">https://arxiv.org/abs/2407.15795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15795">https://arxiv.org/pdf/2407.15795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15795]] AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection(https://arxiv.org/abs/2407.15795)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories. This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP. AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data. Two types of learnable prompts are proposed: static and dynamic. Static prompts are shared across all images, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic prompts are generated for each test image, providing CLIP with dynamic adaptation capabilities. The combination of static and dynamic prompts is referred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive experiments conducted across 14 real-world anomaly detection datasets from industrial and medical domains indicate that AdaCLIP outperforms other ZSAD methods and can generalize better to different categories and even domains. Finally, our analysis highlights the importance of diverse auxiliary data and optimized prompts for enhanced generalization capacity. Code is available at this https URL.</li>
</ul>

<h3>Title: MILAN: Milli-Annotations for Lidar Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Nermin Samet, Gilles Puy, Oriane Siméoni, Renaud Marlet</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15797">https://arxiv.org/abs/2407.15797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15797">https://arxiv.org/pdf/2407.15797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15797]] MILAN: Milli-Annotations for Lidar Semantic Segmentation(https://arxiv.org/abs/2407.15797)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Annotating lidar point clouds for autonomous driving is a notoriously expensive and time-consuming task. In this work, we show that the quality of recent self-supervised lidar scan representations allows a great reduction of the annotation cost. Our method has two main steps. First, we show that self-supervised representations allow a simple and direct selection of highly informative lidar scans to annotate: training a network on these selected scans leads to much better results than a random selection of scans and, more interestingly, to results on par with selections made by SOTA active learning methods. In a second step, we leverage the same self-supervised representations to cluster points in our selected scans. Asking the annotator to classify each cluster, with a single click per cluster, then permits us to close the gap with fully-annotated training sets, while only requiring one thousandth of the point labels.</li>
</ul>

<h3>Title: Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget</h3>
<ul>
<li><strong>Authors: </strong>Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15811">https://arxiv.org/abs/2407.15811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15811">https://arxiv.org/pdf/2407.15811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15811]] Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget(https://arxiv.org/abs/2407.15811)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only \$1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118$\times$ lower cost than stable diffusion models and 14$\times$ lower cost than the current state-of-the-art approach that costs \$28,400. We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets.</li>
</ul>

<h3>Title: dMel: Speech Tokenization made Simple</h3>
<ul>
<li><strong>Authors: </strong>He Bai, Tatiana Likhomanenko, Ruixiang Zhang, Zijin Gu, Zakaria Aldeneh, Navdeep Jaitly</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15835">https://arxiv.org/abs/2407.15835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15835">https://arxiv.org/pdf/2407.15835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15835]] dMel: Speech Tokenization made Simple(https://arxiv.org/abs/2407.15835)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated complicated speech tokenization methods to discretize continuous speech signals so that language modeling techniques can be applied to speech data. However, existing approaches either model semantic tokens, potentially losing acoustic information, or model acoustic tokens, risking the loss of semantic information. Having multiple token types also complicates the architecture and requires additional pretraining. Here we show that discretizing mel-filterbank channels into discrete intensity bins produces a simple representation (dMel), that performs better than other existing speech tokenization methods. Using a transformer decoder-only architecture for speech-text modeling, we comprehensively evaluate different speech tokenization methods on speech recognition (ASR), speech synthesis (TTS). Our results demonstrate the effectiveness of dMel in achieving high performance on both tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.</li>
</ul>

<h3>Title: Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Yibing Wei, Abhinav Gupta, Pedro Morgado</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15837">https://arxiv.org/abs/2407.15837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15837">https://arxiv.org/pdf/2407.15837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15837]] Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning(https://arxiv.org/abs/2407.15837)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Masked Image Modeling (MIM) has emerged as a promising method for deriving visual representations from unlabeled image data by predicting missing pixels from masked portions of images. It excels in region-aware learning and provides strong initializations for various tasks, but struggles to capture high-level semantics without further supervised fine-tuning, likely due to the low-level nature of its pixel reconstruction objective. A promising yet unrealized framework is learning representations through masked reconstruction in latent space, combining the locality of MIM with the high-level targets. However, this approach poses significant training challenges as the reconstruction targets are learned in conjunction with the model, potentially leading to trivial or suboptimal solutions.Our study is among the first to thoroughly analyze and address the challenges of such framework, which we refer to as Latent MIM. Through a series of carefully designed experiments and extensive analysis, we identify the source of these challenges, including representation collapsing for joint online/target optimization, learning objectives, the high region correlation in latent space and decoding conditioning. By sequentially addressing these issues, we demonstrate that Latent MIM can indeed learn high-level representations while retaining the benefits of MIM models.</li>
</ul>

<h3>Title: Artist: Aesthetically Controllable Text-Driven Stylization without Training</h3>
<ul>
<li><strong>Authors: </strong>Ruixiang Jiang, Changwen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.15842">https://arxiv.org/abs/2407.15842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.15842">https://arxiv.org/pdf/2407.15842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.15842]] Artist: Aesthetically Controllable Text-Driven Stylization without Training(https://arxiv.org/abs/2407.15842)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models entangle content and style generation during the denoising process, leading to undesired content modification when directly applied to stylization tasks. Existing methods struggle to effectively control the diffusion model to meet the aesthetic-level requirements for stylization. In this paper, we introduce \textbf{Artist}, a training-free approach that aesthetically controls the content and style generation of a pretrained diffusion model for text-driven stylization. Our key insight is to disentangle the denoising of content and style into separate diffusion processes while sharing information between them. We propose simple yet effective content and style control methods that suppress style-irrelevant content generation, resulting in harmonious stylization results. Extensive experiments demonstrate that our method excels at achieving aesthetic-level stylization requirements, preserving intricate details in the content image and aligning well with the style prompt. Furthermore, we showcase the highly controllability of the stylization strength from various perspectives. Code will be released, project home page: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
