<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation. (arXiv:2310.09458v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09458">http://arxiv.org/abs/2310.09458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09458]] PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation(http://arxiv.org/abs/2310.09458)</code></li>
<li>Summary: <p>Recent advances in zero-shot text-to-3D human generation, which employ the
human model prior (eg, SMPL) or Score Distillation Sampling (SDS) with
pre-trained text-to-image diffusion models, have been groundbreaking. However,
SDS may provide inaccurate gradient directions under the weak diffusion
guidance, as it tends to produce over-smoothed results and generate body
textures that are inconsistent with the detailed mesh geometry. Therefore,
directly leverage existing strategies for high-fidelity text-to-3D human
texturing is challenging. In this work, we propose a model called PaintHuman to
addresses the challenges from two aspects. We first propose a novel score
function, Denoised Score Distillation (DSD), which directly modifies the SDS by
introducing negative gradient components to iteratively correct the gradient
direction and generate high-quality textures. In addition, we use the depth map
as a geometric guidance to ensure the texture is semantically aligned to human
mesh surfaces. To guarantee the quality of rendered results, we employ
geometry-aware networks to predict surface materials and render realistic human
textures. Extensive experiments, benchmarked against state-of-the-art methods,
validate the efficacy of our approach.
</p></li>
</ul>

<h3>Title: Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner. (arXiv:2310.09469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09469">http://arxiv.org/abs/2310.09469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09469]] Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner(http://arxiv.org/abs/2310.09469)</code></li>
<li>Summary: <p>A diffusion model, which is formulated to produce an image using thousands of
denoising steps, usually suffers from a slow inference speed. Existing
acceleration algorithms simplify the sampling by skipping most steps yet
exhibit considerable performance degradation. By viewing the generation of
diffusion models as a discretized integrating process, we argue that the
quality drop is partly caused by applying an inaccurate integral direction to a
timestep interval. To rectify this issue, we propose a timestep aligner that
helps find a more accurate integral direction for a particular interval at the
minimum cost. Specifically, at each denoising step, we replace the original
parameterization by conditioning the network on a new timestep, which is
obtained by aligning the sampling distribution to the real distribution.
Extensive experiments show that our plug-in design can be trained efficiently
and boost the inference performance of various state-of-the-art acceleration
methods, especially when there are few denoising steps. For example, when using
10 denoising steps on the popular LSUN Bedroom dataset, we improve the FID of
DDIM from 9.65 to 6.07, simply by adopting our method for a more appropriate
set of timesteps. Code will be made publicly available.
</p></li>
</ul>

<h3>Title: Unified High-binding Watermark for Unconditional Image Generation Models. (arXiv:2310.09479v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09479">http://arxiv.org/abs/2310.09479</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09479]] Unified High-binding Watermark for Unconditional Image Generation Models(http://arxiv.org/abs/2310.09479)</code></li>
<li>Summary: <p>Deep learning techniques have implemented many unconditional image generation
(UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images
(also known as AI-Generated Content, AIGC for short) produced by these models
bring urgent needs for intellectual property protection such as data
traceability and copyright certification. An attacker can steal the output
images of the target model and use them as part of the training data to train a
private surrogate UIG model. The implementation mechanisms of UIG models are
diverse and complex, and there is no unified and effective protection and
verification method at present. To address these issues, we propose a two-stage
unified watermark verification mechanism with high-binding effects for such
models. In the first stage, we use an encoder to invisibly write the watermark
image into the output images of the original AIGC tool, and reversely extract
the watermark image through the corresponding decoder. In the second stage, we
design the decoder fine-tuning process, and the fine-tuned decoder can make
correct judgments on whether the suspicious model steals the original AIGC tool
data. Experiments demonstrate our method can complete the verification work
with almost zero false positive rate under the condition of only using the
model output images. Moreover, the proposed method can achieve data steal
verification across different types of UIG models, which further increases the
practicality of the method.
</p></li>
</ul>

<h3>Title: Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09484">http://arxiv.org/abs/2310.09484</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09484]] Exploring the Design Space of Diffusion Autoencoders for Face Morphing(http://arxiv.org/abs/2310.09484)</code></li>
<li>Summary: <p>Face morphs created by Diffusion Autoencoders are a recent innovation and the
design space of such an approach has not been well explored. We explore three
axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM
solver, and 3) partial sampling through small amounts of added noise.
</p></li>
</ul>

<h3>Title: Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09336">http://arxiv.org/abs/2310.09336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09336]] Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task(http://arxiv.org/abs/2310.09336)</code></li>
<li>Summary: <p>Modern generative models exhibit unprecedented capabilities to generate
extremely realistic data. However, given the inherent compositionality of the
real world, reliable use of these models in practical applications requires
that they exhibit the capability to compose a novel set of concepts to generate
outputs not seen in the training data set. Prior work demonstrates that recent
diffusion models do exhibit intriguing compositional generalization abilities,
but also fail unpredictably. Motivated by this, we perform a controlled study
for understanding compositional generalization in conditional diffusion models
in a synthetic setting, varying different attributes of the training data and
measuring the model's ability to generate samples out-of-distribution. Our
results show: (i) the order in which the ability to generate samples from a
concept and compose them emerges is governed by the structure of the underlying
data-generating process; (ii) performance on compositional tasks exhibits a
sudden ``emergence'' due to multiplicative reliance on the performance of
constituent tasks, partially explaining emergent phenomena seen in generative
models; and (iii) composing concepts with lower frequency in the training data
to generate out-of-distribution samples requires considerably more optimization
steps compared to generating in-distribution samples. Overall, our study lays a
foundation for understanding capabilities and compositionality in generative
models from a data-centric perspective.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance. (arXiv:2310.09507v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09507">http://arxiv.org/abs/2310.09507</a></li>
<li>Code URL: https://github.com/jlianglab/ark</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09507]] Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance(http://arxiv.org/abs/2310.09507)</code></li>
<li>Summary: <p>Deep learning nowadays offers expert-level and sometimes even
super-expert-level performance, but achieving such performance demands massive
annotated data for training (e.g., Google's proprietary CXR Foundation Model
(CXR-FM) was trained on 821,544 labeled and mostly private chest X-rays
(CXRs)). Numerous datasets are publicly available in medical imaging but
individually small and heterogeneous in expert labels. We envision a powerful
and robust foundation model that can be trained by aggregating numerous small
public datasets. To realize this vision, we have developed Ark, a framework
that accrues and reuses knowledge from heterogeneous expert annotations in
various datasets. As a proof of concept, we have trained two Ark models on
335,484 and 704,363 CXRs, respectively, by merging several datasets including
ChestX-ray14, CheXpert, MIMIC-II, and VinDr-CXR, evaluated them on a wide range
of imaging tasks covering both classification and segmentation via fine-tuning,
linear-probing, and gender-bias analysis, and demonstrated our Ark's superior
and robust performance over the SOTA fully/self-supervised baselines and
Google's proprietary CXR-FM. This enhanced performance is attributed to our
simple yet powerful observation that aggregating numerous public datasets
diversifies patient populations and accrues knowledge from diverse experts,
yielding unprecedented performance yet saving annotation cost. With all codes
and pretrained models released at GitHub.com/JLiangLab/Ark, we hope that Ark
exerts an important impact on open science, as accruing and reusing knowledge
from expert annotations in public datasets can potentially surpass the
performance of proprietary models trained on unusually large data, inspiring
many more researchers worldwide to share codes and datasets to build open
foundation models, accelerate open science, and democratize deep learning for
medical imaging.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: Efficient Apple Maturity and Damage Assessment: A Lightweight Detection Model with GAN and Attention Mechanism. (arXiv:2310.09347v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09347">http://arxiv.org/abs/2310.09347</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09347]] Efficient Apple Maturity and Damage Assessment: A Lightweight Detection Model with GAN and Attention Mechanism(http://arxiv.org/abs/2310.09347)</code></li>
<li>Summary: <p>This study proposes a method based on lightweight convolutional neural
networks (CNN) and generative adversarial networks (GAN) for apple ripeness and
damage level detection tasks. Initially, a lightweight CNN model is designed by
optimizing the model's depth and width, as well as employing advanced model
compression techniques, successfully reducing the model's parameter and
computational requirements, thus enhancing real-time performance in practical
applications. Simultaneously, attention mechanisms are introduced, dynamically
adjusting the importance of different feature layers to improve the performance
in object detection tasks. To address the issues of sample imbalance and
insufficient sample size, GANs are used to generate realistic apple images,
expanding the training dataset and enhancing the model's recognition capability
when faced with apples of varying ripeness and damage levels. Furthermore, by
applying the object detection network for damage location annotation on damaged
apples, the accuracy of damage level detection is improved, providing a more
precise basis for decision-making. Experimental results show that in apple
ripeness grading detection, the proposed model achieves 95.6\%, 93.8\%, 95.0\%,
and 56.5 in precision, recall, accuracy, and FPS, respectively. In apple damage
level detection, the proposed model reaches 95.3\%, 93.7\%, and 94.5\% in
precision, recall, and mAP, respectively. In both tasks, the proposed method
outperforms other mainstream models, demonstrating the excellent performance
and high practical value of the proposed method in apple ripeness and damage
level detection tasks.
</p></li>
</ul>

<h3>Title: A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09430">http://arxiv.org/abs/2310.09430</a></li>
<li>Code URL: https://github.com/strong-ai-lab/logical-and-abstract-reasoning</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09430]] A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks(http://arxiv.org/abs/2310.09430)</code></li>
<li>Summary: <p>Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly
advanced the performance of artificial systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness to perform logical reasoning remain under-evaluated. To probe this
ability, we propose three new logical reasoning datasets named "ReClor-plus",
"LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with
randomly shuffled options, the second with the correct choices replaced by
"none of the other options are correct", and a combination of the previous two
subsets. We carry out experiments on these datasets with both discriminative
and generative LLMs and show that these simple tricks greatly hinder the
performance of the language models. Despite their superior performance on the
original publicly available datasets, we find that all models struggle to
answer our newly constructed datasets. We show that introducing task variations
by perturbing a sizable training set can markedly improve the model's
generalisation and robustness in logical reasoning tasks. Moreover, applying
logic-driven data augmentation for fine-tuning, combined with prompting can
enhance the generalisation performance of both discriminative large language
models and generative large language models. These results offer insights into
assessing and improving the generalisation and robustness of large language
models for logical reasoning tasks. We make our source code and data publicly
available
\url{https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning}.
</p></li>
</ul>

<h3>Title: One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09499">http://arxiv.org/abs/2310.09499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09499]] One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models(http://arxiv.org/abs/2310.09499)</code></li>
<li>Summary: <p>Various Large Language Models(LLMs) from the Generative Pretrained
Transformer~(GPT) family have achieved outstanding performances in a wide range
of text generation tasks. However, the enormous model sizes have hindered their
practical use in real-world applications due to high inference latency.
Therefore, improving the efficiencies of LLMs through quantization, pruning,
and other means has been a key issue in LLM studies. In this work, we propose a
method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs
to at least 50\% sparsity without the need of any retraining. It allocates
sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced
error while maintaining the overall sparsity level. The advantages of the
proposed method exhibit even more when the sparsity is extremely high.
Furthermore, our method is compatible with quantization, enabling further
compression of LLMs.
</p></li>
</ul>

<h3>Title: Uncertainty Quantification using Generative Approach. (arXiv:2310.09338v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09338">http://arxiv.org/abs/2310.09338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09338]] Uncertainty Quantification using Generative Approach(http://arxiv.org/abs/2310.09338)</code></li>
<li>Summary: <p>We present the Incremental Generative Monte Carlo (IGMC) method, designed to
measure uncertainty in deep neural networks using deep generative approaches.
IGMC iteratively trains generative models, adding their output to the dataset,
to compute the posterior distribution of the expectation of a random variable.
We provide a theoretical guarantee of the convergence rate of IGMC relative to
the sample size and sampling depth. Due to its compatibility with deep
generative approaches, IGMC is adaptable to both neural network classification
and regression tasks. We empirically study the behavior of IGMC on the MNIST
digit classification task.
</p></li>
</ul>

<h2>anomaly</h2>
<h2>in-context</h2>
<h3>Title: SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation. (arXiv:2310.09424v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.09424">http://arxiv.org/abs/2310.09424</a></li>
<li>Code URL: https://github.com/NVIDIA/NeMo</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.09424]] SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation(http://arxiv.org/abs/2310.09424)</code></li>
<li>Summary: <p>We present a novel Speech Augmented Language Model (SALM) with {\em
multitask} and {\em in-context} learning capabilities. SALM comprises a frozen
text LLM, a audio encoder, a modality adapter module, and LoRA layers to
accommodate speech input and associated task instructions. The unified SALM not
only achieves performance on par with task-specific Conformer baselines for
Automatic Speech Recognition (ASR) and Speech Translation (AST), but also
exhibits zero-shot in-context learning capabilities, demonstrated through
keyword-boosting task for ASR and AST. Moreover, {\em speech supervised
in-context training} is proposed to bridge the gap between LLM training and
downstream speech tasks, which further boosts the in-context learning ability
of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
