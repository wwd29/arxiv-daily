<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: On Manipulating Scene Text in the Wild with Diffusion Models. (arXiv:2311.00734v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00734">http://arxiv.org/abs/2311.00734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00734]] On Manipulating Scene Text in the Wild with Diffusion Models(http://arxiv.org/abs/2311.00734)</code></li>
<li>Summary: <p>Diffusion models have gained attention for image editing yielding impressive
results in text-to-image tasks. On the downside, one might notice that
generated images of stable diffusion models suffer from deteriorated details.
This pitfall impacts image editing tasks that require information preservation
e.g., scene text editing. As a desired result, the model must show the
capability to replace the text on the source image to the target text while
preserving the details e.g., color, font size, and background. To leverage the
potential of diffusion models, in this work, we introduce Diffusion-BasEd Scene
Text manipulation Network so-called DBEST. Specifically, we design two
adaptation strategies, namely one-shot style adaptation and text-recognition
guidance. In experiments, we thoroughly assess and compare our proposed method
against state-of-the-arts on various scene text datasets, then provide
extensive ablation studies for each granularity to analyze our performance
gain. Also, we demonstrate the effectiveness of our proposed method to
synthesize scene text indicated by competitive Optical Character Recognition
(OCR) accuracy. Our method achieves 94.15% and 98.12% on COCO-text and
ICDAR2013 datasets for character-level evaluation.
</p></li>
</ul>

<h3>Title: Towards High-quality HDR Deghosting with Conditional Diffusion Models. (arXiv:2311.00932v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00932">http://arxiv.org/abs/2311.00932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00932]] Towards High-quality HDR Deghosting with Conditional Diffusion Models(http://arxiv.org/abs/2311.00932)</code></li>
<li>Summary: <p>High Dynamic Range (HDR) images can be recovered from several Low Dynamic
Range (LDR) images by existing Deep Neural Networks (DNNs) techniques. Despite
the remarkable progress, DNN-based methods still generate ghosting artifacts
when LDR images have saturation and large motion, which hinders potential
applications in real-world scenarios. To address this challenge, we formulate
the HDR deghosting problem as an image generation that leverages LDR features
as the diffusion model's condition, consisting of the feature condition
generator and the noise predictor. Feature condition generator employs
attention and Domain Feature Alignment (DFA) layer to transform the
intermediate features to avoid ghosting artifacts. With the learned features as
conditions, the noise predictor leverages a stochastic iterative denoising
process for diffusion models to generate an HDR image by steering the sampling
process. Furthermore, to mitigate semantic confusion caused by the saturation
problem of LDR images, we design a sliding window noise estimator to sample
smooth noise in a patch-based manner. In addition, an image space loss is
proposed to avoid the color distortion of the estimated HDR results. We
empirically evaluate our model on benchmark datasets for HDR imaging. The
results demonstrate that our approach achieves state-of-the-art performances
and well generalization to real-world images.
</p></li>
</ul>

<h3>Title: Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance. (arXiv:2311.00938v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00938">http://arxiv.org/abs/2311.00938</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00938]] Bridging the Gap: Addressing Discrepancies in Diffusion Model Training for Classifier-Free Guidance(http://arxiv.org/abs/2311.00938)</code></li>
<li>Summary: <p>Diffusion models have emerged as a pivotal advancement in generative models,
setting new standards to the quality of the generated instances. In the current
paper we aim to underscore a discrepancy between conventional training methods
and the desired conditional sampling behavior of these models. While the
prevalent classifier-free guidance technique works well, it's not without
flaws. At higher values for the guidance scale parameter $w$, we often get out
of distribution samples and mode collapse, whereas at lower values for $w$ we
may not get the desired specificity. To address these challenges, we introduce
an updated loss function that better aligns training objectives with sampling
behaviors. Experimental validation with FID scores on CIFAR-10 elucidates our
method's ability to produce higher quality samples with fewer sampling
timesteps, and be more robust to the choice of guidance scale $w$. We also
experiment with fine-tuning Stable Diffusion on the proposed loss, to provide
early evidence that large diffusion models may also benefit from this refined
loss function.
</p></li>
</ul>

<h3>Title: Gaussian Mixture Solvers for Diffusion Models. (arXiv:2311.00941v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00941">http://arxiv.org/abs/2311.00941</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00941]] Gaussian Mixture Solvers for Diffusion Models(http://arxiv.org/abs/2311.00941)</code></li>
<li>Summary: <p>Recently, diffusion models have achieved great success in generative tasks.
Sampling from diffusion models is equivalent to solving the reverse diffusion
stochastic differential equations (SDEs) or the corresponding probability flow
ordinary differential equations (ODEs). In comparison, SDE-based solvers can
generate samples of higher quality and are suited for image translation tasks
like stroke-based synthesis. During inference, however, existing SDE-based
solvers are severely constrained by the efficiency-effectiveness dilemma. Our
investigation suggests that this is because the Gaussian assumption in the
reverse transition kernel is frequently violated (even in the case of simple
mixture data) given a limited number of discretization steps. To overcome this
limitation, we introduce a novel class of SDE-based solvers called
\emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver
estimates the first three-order moments and optimizes the parameters of a
Gaussian mixture transition kernel using generalized methods of moments in each
step during sampling. Empirically, our solver outperforms numerous SDE-based
solvers in terms of sample quality in image generation and stroke-based
synthesis in various diffusion models, which validates the motivation and
effectiveness of GMS. Our code is available at
https://github.com/Guohanzhong/GMS.
</p></li>
</ul>

<h3>Title: Optimal Noise pursuit for Augmenting Text-to-Video Generation. (arXiv:2311.00949v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00949">http://arxiv.org/abs/2311.00949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00949]] Optimal Noise pursuit for Augmenting Text-to-Video Generation(http://arxiv.org/abs/2311.00949)</code></li>
<li>Summary: <p>Despite the remarkable progress in text-to-video generation, existing
diffusion-based models often exhibit instability in terms of noise during
inference. Specifically, when different noises are fed for the given text,
these models produce videos that differ significantly in terms of both frame
quality and temporal consistency. With this observation, we posit that there
exists an optimal noise matched to each textual input; however, the widely
adopted strategies of random noise sampling often fail to capture it. In this
paper, we argue that the optimal noise can be approached through inverting the
groundtruth video using the established noise-video mapping derived from the
diffusion model. Nevertheless, the groundtruth video for the text prompt is not
available during inference. To address this challenge, we propose to
approximate the optimal noise via a search and inversion pipeline. Given a text
prompt, we initially search for a video from a predefined candidate pool that
closely relates to the text prompt. Subsequently, we invert the searched video
into the noise space, which serves as an improved noise prompt for the textual
input. In addition to addressing noise, we also observe that the text prompt
with richer details often leads to higher-quality videos. Motivated by this, we
further design a semantic-preserving rewriter to enrich the text prompt, where
a reference-guided rewriting is devised for reasonable details compensation,
and a denoising with a hybrid semantics strategy is proposed to preserve the
semantic consistency. Extensive experiments on the WebVid-10M benchmark show
that our proposed method can improve the text-to-video models with a clear
margin, while introducing no optimization burden.
</p></li>
</ul>

<h3>Title: VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning. (arXiv:2311.00990v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00990">http://arxiv.org/abs/2311.00990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00990]] VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning(http://arxiv.org/abs/2311.00990)</code></li>
<li>Summary: <p>Customized text-to-video generation aims to generate text-guided videos with
customized user-given subjects, which has gained increasing attention recently.
However, existing works are primarily limited to generating videos for a single
subject, leaving the more challenging problem of customized multi-subject
text-to-video generation largely unexplored. In this paper, we fill this gap
and propose a novel VideoDreamer framework. VideoDreamer can generate
temporally consistent text-guided videos that faithfully preserve the visual
features of the given multiple subjects. Specifically, VideoDreamer leverages
the pretrained Stable Diffusion with latent-code motion dynamics and temporal
cross-frame attention as the base video generator. The video generator is
further customized for the given multiple subjects by the proposed Disen-Mix
Finetuning and Human-in-the-Loop Re-finetuning strategy, which can tackle the
attribute binding problem of multi-subject generation. We also introduce
MultiStudioBench, a benchmark for evaluating customized multi-subject
text-to-video generation models. Extensive experiments demonstrate the
remarkable ability of VideoDreamer to generate videos with new content such as
new events and backgrounds, tailored to the customized multiple subjects. Our
project page is available at https://videodreamer23.github.io/.
</p></li>
</ul>

<h3>Title: Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs. (arXiv:2311.01015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01015">http://arxiv.org/abs/2311.01015</a></li>
<li>Code URL: https://github.com/jpthu17/graphmotion</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01015]] Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs(http://arxiv.org/abs/2311.01015)</code></li>
<li>Summary: <p>Most text-driven human motion generation methods employ sequential modeling
approaches, e.g., transformer, to extract sentence-level text representations
automatically and implicitly for human motion synthesis. However, these compact
text representations may overemphasize the action names at the expense of other
important properties and lack fine-grained details to guide the synthesis of
subtly distinct motion. In this paper, we propose hierarchical semantic graphs
for fine-grained control over motion generation. Specifically, we disentangle
motion descriptions into hierarchical semantic graphs including three levels of
motions, actions, and specifics. Such global-to-local structures facilitate a
comprehensive understanding of motion description and fine-grained control of
motion generation. Correspondingly, to leverage the coarse-to-fine topology of
hierarchical semantic graphs, we decompose the text-to-motion diffusion process
into three semantic levels, which correspond to capturing the overall motion,
local actions, and action specifics. Extensive experiments on two benchmark
human motion datasets, including HumanML3D and KIT, with superior performances,
justify the efficacy of our method. More encouragingly, by modifying the edge
weights of hierarchical semantic graphs, our method can continuously refine the
generated motion, which may have a far-reaching impact on the community. Code
and pre-training weights are available at
https://github.com/jpthu17/GraphMotion.
</p></li>
</ul>

<h3>Title: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01017">http://arxiv.org/abs/2311.01017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01017]] Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion(http://arxiv.org/abs/2311.01017)</code></li>
<li>Summary: <p>Learning world models can teach an agent how the world works in an
unsupervised manner. Even though it can be viewed as a special case of sequence
modeling, progress for scaling world models on robotic applications such as
autonomous driving has been somewhat less rapid than scaling language models
with Generative Pre-trained Transformers (GPT). We identify two reasons as
major bottlenecks: dealing with complex and unstructured observation space, and
having a scalable generative model. Consequently, we propose a novel world
modeling approach that first tokenizes sensor observations with VQVAE, then
predicts the future via discrete diffusion. To efficiently decode and denoise
tokens in parallel, we recast Masked Generative Image Transformer into the
discrete diffusion framework with a few simple changes, resulting in notable
improvement. When applied to learning world models on point cloud observations,
our model reduces prior SOTA Chamfer distance by more than 65% for 1s
prediction, and more than 50% for 3s prediction, across NuScenes, KITTI
Odometry, and Argoverse2 datasets. Our results demonstrate that discrete
diffusion on tokenized agent experience can unlock the power of GPT-like
unsupervised learning for robotic agents.
</p></li>
</ul>

<h3>Title: Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning. (arXiv:2311.01018v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01018">http://arxiv.org/abs/2311.01018</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01018]] Expanding Expressiveness of Diffusion Models with Limited Data via Self-Distillation based Fine-Tuning(http://arxiv.org/abs/2311.01018)</code></li>
<li>Summary: <p>Training diffusion models on limited datasets poses challenges in terms of
limited generation capacity and expressiveness, leading to unsatisfactory
results in various downstream tasks utilizing pretrained diffusion models, such
as domain translation and text-guided image manipulation. In this paper, we
propose Self-Distillation for Fine-Tuning diffusion models (SDFT), a
methodology to address these challenges by leveraging diverse features from
diffusion models pretrained on large source datasets. SDFT distills more
general features (shape, colors, etc.) and less domain-specific features
(texture, fine details, etc) from the source model, allowing successful
knowledge transfer without disturbing the training process on target datasets.
The proposed method is not constrained by the specific architecture of the
model and thus can be generally adopted to existing frameworks. Experimental
results demonstrate that SDFT enhances the expressiveness of the diffusion
model with limited datasets, resulting in improved generation capabilities
across various downstream tasks.
</p></li>
</ul>

<h3>Title: Infusion: Internal Diffusion for Video Inpainting. (arXiv:2311.01090v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01090">http://arxiv.org/abs/2311.01090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01090]] Infusion: Internal Diffusion for Video Inpainting(http://arxiv.org/abs/2311.01090)</code></li>
<li>Summary: <p>Video inpainting is the task of filling a desired region in a video in a
visually convincing manner. It is a very challenging task due to the high
dimensionality of the signal and the temporal consistency required for
obtaining convincing results. Recently, diffusion models have shown impressive
results in modeling complex data distributions, including images and videos.
Diffusion models remain nonetheless very expensive to train and perform
inference with, which strongly restrict their application to video. We show
that in the case of video inpainting, thanks to the highly auto-similar nature
of videos, the training of a diffusion model can be restricted to the video to
inpaint and still produce very satisfying results. This leads us to adopt an
internal learning approch, which also allows for a greatly reduced network
size. We call our approach "Infusion": an internal learning algorithm for video
inpainting through diffusion. Due to our frugal network, we are able to propose
the first video inpainting approach based purely on diffusion. Other methods
require supporting elements such as optical flow estimation, which limits their
performance in the case of dynamic textures for example. We introduce a new
method for efficient training and inference of diffusion models in the context
of internal learning. We split the diffusion process into different learning
intervals which greatly simplifies the learning steps. We show qualititative
and quantitative results, demonstrating that our method reaches
state-of-the-art performance, in particular in the case of dynamic backgrounds
and textures.
</p></li>
</ul>

<h3>Title: Optimal Transport-Guided Conditional Score-Based Diffusion Models. (arXiv:2311.01226v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01226">http://arxiv.org/abs/2311.01226</a></li>
<li>Code URL: https://github.com/xjtu-xgu/otcs</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01226]] Optimal Transport-Guided Conditional Score-Based Diffusion Models(http://arxiv.org/abs/2311.01226)</code></li>
<li>Summary: <p>Conditional score-based diffusion model (SBDM) is for conditional generation
of target data with paired data as condition, and has achieved great success in
image translation. However, it requires the paired data as condition, and there
would be insufficient paired data provided in real-world applications. To
tackle the applications with partially paired or even unpaired dataset, we
propose a novel Optimal Transport-guided Conditional Score-based diffusion
model (OTCS) in this paper. We build the coupling relationship for the unpaired
or partially paired dataset based on $L_2$-regularized unsupervised or
semi-supervised optimal transport, respectively. Based on the coupling
relationship, we develop the objective for training the conditional score-based
model for unpaired or partially paired settings, which is based on a
reformulation and generalization of the conditional SBDM for paired setting.
With the estimated coupling relationship, we effectively train the conditional
score-based model by designing a ``resampling-by-compatibility'' strategy to
choose the sampled data with high compatibility as guidance. Extensive
experiments on unpaired super-resolution and semi-paired image-to-image
translation demonstrated the effectiveness of the proposed OTCS model. From the
viewpoint of optimal transport, OTCS provides an approach to transport data
across distributions, which is a challenge for OT on large-scale datasets. We
theoretically prove that OTCS realizes the data transport in OT with a
theoretical bound. Code is available at \url{https://github.com/XJTU-XGU/OTCS}.
</p></li>
</ul>

<h3>Title: DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning. (arXiv:2311.01295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01295">http://arxiv.org/abs/2311.01295</a></li>
<li>Code URL: https://github.com/wenxuan-bao/dp-mix</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01295]] DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning(http://arxiv.org/abs/2311.01295)</code></li>
<li>Summary: <p>Data augmentation techniques, such as simple image transformations and
combinations, are highly effective at improving the generalization of computer
vision models, especially when training data is limited. However, such
techniques are fundamentally incompatible with differentially private learning
approaches, due to the latter's built-in assumption that each training image's
contribution to the learned model is bounded. In this paper, we investigate why
naive applications of multi-sample data augmentation techniques, such as mixup,
fail to achieve good performance and propose two novel data augmentation
techniques specifically designed for the constraints of differentially private
learning. Our first technique, DP-Mix_Self, achieves SoTA classification
performance across a range of datasets and settings by performing mixup on
self-augmented data. Our second technique, DP-Mix_Diff, further improves
performance by incorporating synthetic data from a pre-trained diffusion model
into the mixup process. We open-source the code at
https://github.com/wenxuan-Bao/DP-Mix.
</p></li>
</ul>

<h3>Title: The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing. (arXiv:2311.01410v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01410">http://arxiv.org/abs/2311.01410</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01410]] The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing(http://arxiv.org/abs/2311.01410)</code></li>
<li>Summary: <p>We present a unified probabilistic formulation for diffusion-based image
editing, where a latent variable is edited in a task-specific manner and
generally deviates from the corresponding marginal distribution induced by the
original stochastic or ordinary differential equation (SDE or ODE). Instead, it
defines a corresponding SDE or ODE for editing. In the formulation, we prove
that the Kullback-Leibler divergence between the marginal distributions of the
two SDEs gradually decreases while that for the ODEs remains as the time
approaches zero, which shows the promise of SDE in image editing. Inspired by
it, we provide the SDE counterparts for widely used ODE baselines in various
tasks including inpainting and image-to-image translation, where SDE shows a
consistent and substantial improvement. Moreover, we propose SDE-Drag -- a
simple yet effective method built upon the SDE formulation for point-based
content dragging. We build a challenging benchmark (termed DragBench) with
open-set natural, art, and AI-generated images for evaluation. A user study on
DragBench indicates that SDE-Drag significantly outperforms our ODE baseline,
existing diffusion-based methods, and the renowned DragGAN. Our results
demonstrate the superiority and versatility of SDE in image editing and push
the boundary of diffusion-based editing methods.
</p></li>
</ul>

<h3>Title: Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00797">http://arxiv.org/abs/2311.00797</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00797]] Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling(http://arxiv.org/abs/2311.00797)</code></li>
<li>Summary: <p>We study the tipping point collective dynamics of an adaptive
susceptible-infected-susceptible (SIS) epidemiological network in a
data-driven, machine learning-assisted manner. We identify a
parameter-dependent effective stochastic differential equation (eSDE) in terms
of physically meaningful coarse mean-field variables through a deep-learning
ResNet architecture inspired by numerical stochastic integrators. We construct
an approximate effective bifurcation diagram based on the identified drift term
of the eSDE and contrast it with the mean-field SIS model bifurcation diagram.
We observe a subcritical Hopf bifurcation in the evolving network's effective
SIS dynamics, that causes the tipping point behavior; this takes the form of
large amplitude collective oscillations that spontaneously -- yet rarely --
arise from the neighborhood of a (noisy) stationary state. We study the
statistics of these rare events both through repeated brute force simulations
and by using established mathematical/computational tools exploiting the
right-hand-side of the identified SDE. We demonstrate that such a collective
SDE can also be identified (and the rare events computations also performed) in
terms of data-driven coarse observables, obtained here via manifold learning
techniques, in particular Diffusion Maps. The workflow of our study is
straightforwardly applicable to other complex dynamics problems exhibiting
tipping point dynamics.
</p></li>
</ul>

<h3>Title: Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction. (arXiv:2311.01033v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01033">http://arxiv.org/abs/2311.01033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01033]] Non-Autoregressive Diffusion-based Temporal Point Processes for Continuous-Time Long-Term Event Prediction(http://arxiv.org/abs/2311.01033)</code></li>
<li>Summary: <p>Continuous-time long-term event prediction plays an important role in many
application scenarios. Most existing works rely on autoregressive frameworks to
predict event sequences, which suffer from error accumulation, thus
compromising prediction quality. Inspired by the success of denoising diffusion
probabilistic models, we propose a diffusion-based non-autoregressive temporal
point process model for long-term event prediction in continuous time. Instead
of generating events one at a time in an autoregressive way, our model predicts
the future event sequence entirely as a whole. In order to perform diffusion
processes on event sequences, we develop a bidirectional map between target
event sequences and the Euclidean vector space. Furthermore, we design a novel
denoising network to capture both sequential and contextual features for better
sample quality. Extensive experiments are conducted to prove the superiority of
our proposed model over state-of-the-art methods on long-term event prediction
in continuous time. To the best of our knowledge, this is the first work to
apply diffusion methods to long-term event prediction problems.
</p></li>
</ul>

<h3>Title: Add and Thin: Diffusion for Temporal Point Processes. (arXiv:2311.01139v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01139">http://arxiv.org/abs/2311.01139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01139]] Add and Thin: Diffusion for Temporal Point Processes(http://arxiv.org/abs/2311.01139)</code></li>
<li>Summary: <p>Autoregressive neural networks within the temporal point process (TPP)
framework have become the standard for modeling continuous-time event data.
Even though these models can expressively capture event sequences in a
one-step-ahead fashion, they are inherently limited for long-term forecasting
applications due to the accumulation of errors caused by their sequential
nature. To overcome these limitations, we derive ADD-THIN, a principled
probabilistic denoising diffusion model for TPPs that operates on entire event
sequences. Unlike existing diffusion approaches, ADD-THIN naturally handles
data with discrete and continuous components. In experiments on synthetic and
real-world datasets, our model matches the state-of-the-art TPP models in
density estimation and strongly outperforms them in forecasting.
</p></li>
</ul>

<h3>Title: Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01223">http://arxiv.org/abs/2311.01223</a></li>
<li>Code URL: https://github.com/apexrl/diff4rlsurvey</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01223]] Diffusion Models for Reinforcement Learning: A Survey(http://arxiv.org/abs/2311.01223)</code></li>
<li>Summary: <p>Diffusion models have emerged as a prominent class of generative models,
surpassing previous methods regarding sample quality and training stability.
Recent works have shown the advantages of diffusion models in improving
reinforcement learning (RL) solutions, including as trajectory planners,
expressive policy classes, data synthesizers, etc. This survey aims to provide
an overview of the advancements in this emerging field and hopes to inspire new
avenues of research. First, we examine several challenges encountered by
current RL algorithms. Then, we present a taxonomy of existing methods based on
the roles played by diffusion models in RL and explore how the existing
challenges are addressed. We further outline successful applications of
diffusion models in various RL-related tasks while discussing the limitations
of current approaches. Finally, we conclude the survey and offer insights into
future research directions, focusing on enhancing model performance and
applying diffusion models to broader tasks. We are actively maintaining a
GitHub repository for papers and other related resources in applying diffusion
models in RL: https://github.com/apexrl/Diff4RLSurvey .
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Are These the Same Apple? Comparing Images Based on Object Intrinsics. (arXiv:2311.00750v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00750">http://arxiv.org/abs/2311.00750</a></li>
<li>Code URL: https://github.com/s-tian/cute</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00750]] Are These the Same Apple? Comparing Images Based on Object Intrinsics(http://arxiv.org/abs/2311.00750)</code></li>
<li>Summary: <p>The human visual system can effortlessly recognize an object under different
extrinsic factors such as lighting, object poses, and background, yet current
computer vision systems often struggle with these variations. An important step
to understanding and improving artificial vision systems is to measure image
similarity purely based on intrinsic object properties that define object
identity. This problem has been studied in the computer vision literature as
re-identification, though mostly restricted to specific object categories such
as people and cars. We propose to extend it to general object categories,
exploring an image similarity metric based on object intrinsics. To benchmark
such measurements, we collect the Common paired objects Under differenT
Extrinsics (CUTE) dataset of $18,000$ images of $180$ objects under different
extrinsic factors such as lighting, poses, and imaging conditions. While
existing methods such as LPIPS and CLIP scores do not measure object intrinsics
well, we find that combining deep features learned from contrastive
self-supervised learning with foreground filtering is a simple yet effective
approach to approximating the similarity. We conduct an extensive survey of
pre-trained features and foreground extraction methods to arrive at a strong
baseline that best measures intrinsic object-centric image similarity among
current methods. Finally, we demonstrate that our approach can aid in
downstream applications such as acting as an analog for human subjects and
improving generalizable re-identification. Please see our project website at
https://s-tian.github.io/projects/cute/ for visualizations of the data and
demos of our metric.
</p></li>
</ul>

<h3>Title: Concatenated Masked Autoencoders as Spatial-Temporal Learner. (arXiv:2311.00961v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00961">http://arxiv.org/abs/2311.00961</a></li>
<li>Code URL: https://github.com/minhoooo1/catmae</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00961]] Concatenated Masked Autoencoders as Spatial-Temporal Learner(http://arxiv.org/abs/2311.00961)</code></li>
<li>Summary: <p>Learning representations from videos requires understanding continuous motion
and visual correspondences between frames. In this paper, we introduce the
Concatenated Masked Autoencoders (CatMAE) as a spatial-temporal learner for
self-supervised video representation learning. For the input sequence of video
frames, CatMAE keeps the initial frame unchanged while applying substantial
masking (95%) to subsequent frames. The encoder in CatMAE is responsible for
encoding visible patches for each frame individually; subsequently, for each
masked frame, the decoder leverages visible patches from both previous and
current frames to reconstruct the original image. Our proposed method enables
the model to estimate the motion information between visible patches, match the
correspondences between preceding and succeeding frames, and ultimately learn
the evolution of scenes. Furthermore, we propose a new data augmentation
strategy, Video-Reverse (ViRe), which uses reversed video frames as the model's
reconstruction targets. This further encourages the model to utilize continuous
motion details and correspondences to complete the reconstruction, thereby
enhancing the model's capabilities. Compared to the most advanced pre-training
methods, CatMAE achieves a leading level in video segmentation tasks and action
recognition tasks.
</p></li>
</ul>

<h3>Title: Terrain-Informed Self-Supervised Learning: Enhancing Building Footprint Extraction from LiDAR Data with Limited Annotations. (arXiv:2311.01188v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01188">http://arxiv.org/abs/2311.01188</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01188]] Terrain-Informed Self-Supervised Learning: Enhancing Building Footprint Extraction from LiDAR Data with Limited Annotations(http://arxiv.org/abs/2311.01188)</code></li>
<li>Summary: <p>Estimating building footprint maps from geospatial data is of paramount
importance in urban planning, development, disaster management, and various
other applications. Deep learning methodologies have gained prominence in
building segmentation maps, offering the promise of precise footprint
extraction without extensive post-processing. However, these methods face
challenges in generalization and label efficiency, particularly in remote
sensing, where obtaining accurate labels can be both expensive and
time-consuming. To address these challenges, we propose terrain-aware
self-supervised learning, tailored to remote sensing, using digital elevation
models from LiDAR data. We propose to learn a model to differentiate between
bare Earth and superimposed structures enabling the network to implicitly learn
domain-relevant features without the need for extensive pixel-level
annotations. We test the effectiveness of our approach by evaluating building
segmentation performance on test datasets with varying label fractions.
Remarkably, with only 1% of the labels (equivalent to 25 labeled examples), our
method improves over ImageNet pre-training, showing the advantage of leveraging
unlabeled data for feature extraction in the domain of remote sensing. The
performance improvement is more pronounced in few-shot scenarios and gradually
closes the gap with ImageNet pre-training as the label fraction increases. We
test on a dataset characterized by substantial distribution shifts and labeling
errors to demonstrate the generalizability of our approach. When compared to
other baselines, including ImageNet pretraining and more complex architectures,
our approach consistently performs better, demonstrating the efficiency and
effectiveness of self-supervised terrain-aware feature learning.
</p></li>
</ul>

<h3>Title: Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00768">http://arxiv.org/abs/2311.00768</a></li>
<li>Code URL: https://github.com/yuroeth/icu_benchmarks</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00768]] Language Model Training Paradigms for Clinical Feature Embeddings(http://arxiv.org/abs/2311.00768)</code></li>
<li>Summary: <p>In research areas with scarce data, representation learning plays a
significant role. This work aims to enhance representation learning for
clinical time series by deriving universal embeddings for clinical features,
such as heart rate and blood pressure. We use self-supervised training
paradigms for language models to learn high-quality clinical feature
embeddings, achieving a finer granularity than existing time-step and
patient-level representation learning. We visualize the learnt embeddings via
unsupervised dimension reduction techniques and observe a high degree of
consistency with prior clinical knowledge. We also evaluate the model
performance on the MIMIC-III benchmark and demonstrate the effectiveness of
using clinical feature embeddings. We publish our code online for replication.
</p></li>
</ul>

<h3>Title: COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning. (arXiv:2311.00886v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00886">http://arxiv.org/abs/2311.00886</a></li>
<li>Code URL: https://github.com/google-research/google-research</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00886]] COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning(http://arxiv.org/abs/2311.00886)</code></li>
<li>Summary: <p>Estimation of temporal counterfactual outcomes from observed history is
crucial for decision-making in many domains such as healthcare and e-commerce,
particularly when randomized controlled trials (RCTs) suffer from high cost or
impracticality. For real-world datasets, modeling time-dependent confounders is
challenging due to complex dynamics, long-range dependencies and both past
treatments and covariates affecting the future outcomes. In this paper, we
introduce COunterfactual Self-supervised TrAnsformeR (COSTAR), a novel approach
that integrates self-supervised learning for improved historical
representations. The proposed framework combines temporal and feature-wise
attention with a component-wise contrastive loss tailored for temporal
treatment outcome observations, yielding superior performance in estimation
accuracy and generalization to out-of-distribution data compared to existing
models, as validated by empirical results on both synthetic and real-world
datasets.
</p></li>
</ul>

<h3>Title: VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01191">http://arxiv.org/abs/2311.01191</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01191]] VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification(http://arxiv.org/abs/2311.01191)</code></li>
<li>Summary: <p>Class imbalance in graph data poses significant challenges for node
classification. Existing methods, represented by SMOTE-based approaches,
partially alleviate this issue but still exhibit limitations during imbalanced
scenario construction. Self-supervised learning (SSL) offers a promising
solution by synthesizing minority nodes from the data itself, yet its potential
remains unexplored. In this paper, we analyze the limitations of SMOTE-based
approaches and introduce VIGraph, a novel SSL model based on the
self-supervised Variational Graph Auto-Encoder (VGAE) that leverages
Variational Inference (VI) to generate minority nodes. Specifically, VIGraph
strictly adheres to the concept of imbalance when constructing imbalanced
graphs and utilizes the generative VGAE to generate minority nodes. Moreover,
VIGraph introduces a novel Siamese contrastive strategy at the decoding phase
to improve the overall quality of generated nodes. VIGraph can generate
high-quality nodes without reintegrating them into the original graph,
eliminating the "Generating, Reintegrating, and Retraining" process found in
SMOTE-based methods. Experiments on multiple real-world datasets demonstrate
that VIGraph achieves promising results for class-imbalanced node
classification tasks.
</p></li>
</ul>

<h3>Title: Combating Bilateral Edge Noise for Robust Link Prediction. (arXiv:2311.01196v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01196">http://arxiv.org/abs/2311.01196</a></li>
<li>Code URL: https://github.com/tmlr-group/rgib</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01196]] Combating Bilateral Edge Noise for Robust Link Prediction(http://arxiv.org/abs/2311.01196)</code></li>
<li>Summary: <p>Although link prediction on graphs has achieved great success with the
development of graph neural networks (GNNs), the potential robustness under the
edge noise is still less investigated. To close this gap, we first conduct an
empirical study to disclose that the edge noise bilaterally perturbs both input
topology and target label, yielding severe performance degradation and
representation collapse. To address this dilemma, we propose an
information-theory-guided principle, Robust Graph Information Bottleneck
(RGIB), to extract reliable supervision signals and avoid representation
collapse. Different from the basic information bottleneck, RGIB further
decouples and balances the mutual dependence among graph topology, target
labels, and representation, building new learning objectives for robust
representation against the bilateral noise. Two instantiations, RGIB-SSL and
RGIB-REP, are explored to leverage the merits of different methodologies, i.e.,
self-supervised learning and data reparameterization, for implicit and explicit
data denoising, respectively. Extensive experiments on six datasets and three
GNNs with diverse noisy scenarios verify the effectiveness of our RGIB
instantiations. The code is publicly available at:
https://github.com/tmlr-group/RGIB.
</p></li>
</ul>

<h3>Title: Unreading Race: Purging Protected Features from Chest X-ray Embeddings. (arXiv:2311.01349v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01349">http://arxiv.org/abs/2311.01349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01349]] Unreading Race: Purging Protected Features from Chest X-ray Embeddings(http://arxiv.org/abs/2311.01349)</code></li>
<li>Summary: <p>Purpose: To analyze and remove protected feature effects in chest radiograph
embeddings of deep learning models.
</p>
<p>Materials and Methods: An orthogonalization is utilized to remove the
influence of protected features (e.g., age, sex, race) in chest radiograph
embeddings, ensuring feature-independent results. To validate the efficacy of
the approach, we retrospectively study the MIMIC and CheXpert datasets using
three pre-trained models, namely a supervised contrastive, a self-supervised
contrastive, and a baseline classifier model. Our statistical analysis involves
comparing the original versus the orthogonalized embeddings by estimating
protected feature influences and evaluating the ability to predict race, age,
or sex using the two types of embeddings.
</p>
<p>Results: Our experiments reveal a significant influence of protected features
on predictions of pathologies. Applying orthogonalization removes these feature
effects. Apart from removing any influence on pathology classification, while
maintaining competitive predictive performance, orthogonalized embeddings
further make it infeasible to directly predict protected attributes and
mitigate subgroup disparities.
</p>
<p>Conclusion: The presented work demonstrates the successful application and
evaluation of the orthogonalization technique in the domain of chest X-ray
classification.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images. (arXiv:2311.01064v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01064">http://arxiv.org/abs/2311.01064</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01064]] Multimodal Foundation Models for Zero-shot Animal Species Recognition in Camera Trap Images(http://arxiv.org/abs/2311.01064)</code></li>
<li>Summary: <p>Due to deteriorating environmental conditions and increasing human activity,
conservation efforts directed towards wildlife is crucial. Motion-activated
camera traps constitute an efficient tool for tracking and monitoring wildlife
populations across the globe. Supervised learning techniques have been
successfully deployed to analyze such imagery, however training such techniques
requires annotations from experts. Reducing the reliance on costly labelled
data therefore has immense potential in developing large-scale wildlife
tracking solutions with markedly less human labor. In this work we propose
WildMatch, a novel zero-shot species classification framework that leverages
multimodal foundation models. In particular, we instruction tune
vision-language models to generate detailed visual descriptions of camera trap
images using similar terminology to experts. Then, we match the generated
caption to an external knowledge base of descriptions in order to determine the
species in a zero-shot manner. We investigate techniques to build instruction
tuning datasets for detailed animal description generation and propose a novel
knowledge augmentation technique to enhance caption quality. We demonstrate the
performance of WildMatch on a new camera trap dataset collected in the
Magdalena Medio region of Colombia.
</p></li>
</ul>

<h3>Title: Recognize Any Regions. (arXiv:2311.01373v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01373">http://arxiv.org/abs/2311.01373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01373]] Recognize Any Regions(http://arxiv.org/abs/2311.01373)</code></li>
<li>Summary: <p>Understanding the semantics of individual regions or patches within
unconstrained images, such as in open-world object detection, represents a
critical yet challenging task in computer vision. Building on the success of
powerful image-level vision-language (ViL) foundation models like CLIP, recent
efforts have sought to harness their capabilities by either training a
contrastive model from scratch with an extensive collection of region-label
pairs or aligning the outputs of a detection model with image-level
representations of region proposals. Despite notable progress, these approaches
are plagued by computationally intensive training requirements, susceptibility
to data noise, and deficiency in contextual information. To address these
limitations, we explore the synergistic potential of off-the-shelf foundation
models, leveraging their respective strengths in localization and semantics. We
introduce a novel, generic, and efficient region recognition architecture,
named RegionSpot, designed to integrate position-aware localization knowledge
from a localization foundation model (e.g., SAM) with semantic information
extracted from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge
while minimizing training overhead, we keep both foundation models frozen,
focusing optimization efforts solely on a lightweight attention-based knowledge
integration module. Through extensive experiments in the context of open-world
object recognition, our RegionSpot demonstrates significant performance
improvements over prior alternatives, while also providing substantial
computational savings. For instance, training our model with 3 million data in
a single day using 8 V100 GPUs. Our model outperforms GLIP by 6.5 % in mean
average precision (mAP), with an even larger margin by 14.8 % for more
challenging and rare categories.
</p></li>
</ul>

<h3>Title: Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models. (arXiv:2311.01441v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01441">http://arxiv.org/abs/2311.01441</a></li>
<li>Code URL: https://github.com/andyz245/discreteadversarialdistillation</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01441]] Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models(http://arxiv.org/abs/2311.01441)</code></li>
<li>Summary: <p>We propose a conceptually simple and lightweight framework for improving the
robustness of vision models through the combination of knowledge distillation
and data augmentation. We address the conjecture that larger models do not make
for better teachers by showing strong gains in out-of-distribution robustness
when distilling from pretrained foundation models. Following this finding, we
propose Discrete Adversarial Distillation (DAD), which leverages a robust
teacher to generate adversarial examples and a VQGAN to discretize them,
creating more informative samples than standard data augmentation techniques.
We provide a theoretical framework for the use of a robust teacher in the
knowledge distillation with data augmentation setting and demonstrate strong
gains in out-of-distribution robustness and clean accuracy across different
student architectures. Notably, our method adds minor computational overhead
compared to similar techniques and can be easily combined with other data
augmentations for further improvements.
</p></li>
</ul>

<h3>Title: Generating QM1B with PySCF$_{\text{IPU}}$. (arXiv:2311.01135v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01135">http://arxiv.org/abs/2311.01135</a></li>
<li>Code URL: https://github.com/graphcore-research/pyscf-ipu</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01135]] Generating QM1B with PySCF$_{\text{IPU}}$(http://arxiv.org/abs/2311.01135)</code></li>
<li>Summary: <p>The emergence of foundation models in Computer Vision and Natural Language
Processing have resulted in immense progress on downstream tasks. This progress
was enabled by datasets with billions of training examples. Similar benefits
are yet to be unlocked for quantum chemistry, where the potential of deep
learning is constrained by comparatively small datasets with 100k to 20M
training examples. These datasets are limited in size because the labels are
computed using the accurate (but computationally demanding) predictions of
Density Functional Theory (DFT). Notably, prior DFT datasets were created using
CPU supercomputers without leveraging hardware acceleration. In this paper, we
take a first step towards utilising hardware accelerators by introducing the
data generator PySCF$_{\text{IPU}}$ using Intelligence Processing Units (IPUs).
This allowed us to create the dataset QM1B with one billion training examples
containing 9-11 heavy atoms. We demonstrate that a simple baseline neural
network (SchNet 9M) improves its performance by simply increasing the amount of
training data without additional inductive biases. To encourage future
researchers to use QM1B responsibly, we highlight several limitations of QM1B
and emphasise the low-resolution of our DFT options, which also serves as
motivation for even larger, more accurate datasets. Code and dataset are
available on Github: <a href="http://github.com/graphcore-research/pyscf-ipu">this http URL</a>
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network. (arXiv:2311.00735v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00735">http://arxiv.org/abs/2311.00735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00735]] PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network(http://arxiv.org/abs/2311.00735)</code></li>
<li>Summary: <p>Positron emission tomography (PET), as an imaging technique with high
biochemical sensitivity, has been widely used in diagnosis of encephalopathy
and brain science research used in brain disease diagnosis and brain science
research. Since different tracers present different effects on the same focal
area, the choice of tracers is getting more significant for PET imaging.
Nowadays, with the wide application of PET imaging in neuropsychiatric
treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine (DOPA) has been found to
be more effective than 18F-labeled fluorine-2-deoxyglucose (FDG) in this field.
However, due to the complexity of its preparation and other limitations, DOPA
is far less widely used than FDG. To address this issue, a tracer conversion
invertible neural network (TC-INN) for image projection is developed to map FDG
images to DOPA images through deep learning. More diagnostic information is
obtained by generating PET images from FDG to DOPA. Specifically, the proposed
TC-INN consists of two separate phases, one for training the traceable data,
the other for re-building the new data. The reference DOPA PET image is used as
the learning target for the corresponding network during the training process
of tracer conversion. Mean-while, the invertible network iteratively estimates
the resultant DOPA PET data and compares it to the reference DOPA PET data.
Notably, the reversible model employed variable enhancement techniques to
achieve better power generation. Moreover, image registration needs to be
performed before training due to the angular deviation of the acquired FDG and
DOPA data information. Experimental results show generative ability in mapping
be-tween FDG images and DOPA images. It demonstrates great potential for PET
image conversion in the case of limited tracer applications.
</p></li>
</ul>

<h3>Title: Detecting Generated Images by Real Images Only. (arXiv:2311.00962v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00962">http://arxiv.org/abs/2311.00962</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00962]] Detecting Generated Images by Real Images Only(http://arxiv.org/abs/2311.00962)</code></li>
<li>Summary: <p>As deep learning technology continues to evolve, the images yielded by
generative models are becoming more and more realistic, triggering people to
question the authenticity of images. Existing generated image detection methods
detect visual artifacts in generated images or learn discriminative features
from both real and generated images by massive training. This learning paradigm
will result in efficiency and generalization issues, making detection methods
always lag behind generation methods. This paper approaches the generated image
detection problem from a new perspective: Start from real images. By finding
the commonality of real images and mapping them to a dense subspace in feature
space, the goal is that generated images, regardless of their generative model,
are then projected outside the subspace. As a result, images from different
generative models can be detected, solving some long-existing problems in the
field. Experimental results show that although our method was trained only by
real images and uses 99.9\% less training data than other deep learning-based
methods, it can compete with state-of-the-art methods and shows excellent
performance in detecting emerging generative models with high inference
efficiency. Moreover, the proposed method shows robustness against various
post-processing. These advantages allow the method to be used in real-world
scenarios.
</p></li>
</ul>

<h3>Title: A Chronological Survey of Theoretical Advancements in Generative Adversarial Networks for Computer Vision. (arXiv:2311.00995v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00995">http://arxiv.org/abs/2311.00995</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00995]] A Chronological Survey of Theoretical Advancements in Generative Adversarial Networks for Computer Vision(http://arxiv.org/abs/2311.00995)</code></li>
<li>Summary: <p>Generative Adversarial Networks (GANs) have been workhorse generative models
for last many years, especially in the research field of computer vision.
Accordingly, there have been many significant advancements in the theory and
application of GAN models, which are notoriously hard to train, but produce
good results if trained well. There have been many a surveys on GANs,
organizing the vast GAN literature from various focus and perspectives.
However, none of the surveys brings out the important chronological aspect: how
the multiple challenges of employing GAN models were solved one-by-one over
time, across multiple landmark research works. This survey intends to bridge
that gap and present some of the landmark research works on the theory and
application of GANs, in chronological order.
</p></li>
</ul>

<h3>Title: Novel View Synthesis from a Single RGBD Image for Indoor Scenes. (arXiv:2311.01065v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01065">http://arxiv.org/abs/2311.01065</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01065]] Novel View Synthesis from a Single RGBD Image for Indoor Scenes(http://arxiv.org/abs/2311.01065)</code></li>
<li>Summary: <p>In this paper, we propose an approach for synthesizing novel view images from
a single RGBD (Red Green Blue-Depth) input. Novel view synthesis (NVS) is an
interesting computer vision task with extensive applications. Methods using
multiple images has been well-studied, exemplary ones include training
scene-specific Neural Radiance Fields (NeRF), or leveraging multi-view stereo
(MVS) and 3D rendering pipelines. However, both are either computationally
intensive or non-generalizable across different scenes, limiting their
practical value. Conversely, the depth information embedded in RGBD images
unlocks 3D potential from a singular view, simplifying NVS. The widespread
availability of compact, affordable stereo cameras, and even LiDARs in
contemporary devices like smartphones, makes capturing RGBD images more
accessible than ever. In our method, we convert an RGBD image into a point
cloud and render it from a different viewpoint, then formulate the NVS task
into an image translation problem. We leveraged generative adversarial networks
to style-transfer the rendered image, achieving a result similar to a
photograph taken from the new perspective. We explore both unsupervised
learning using CycleGAN and supervised learning with Pix2Pix, and demonstrate
the qualitative results. Our method circumvents the limitations of traditional
multi-image techniques, holding significant promise for practical, real-time
applications in NVS.
</p></li>
</ul>

<h3>Title: Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network. (arXiv:2311.01192v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01192">http://arxiv.org/abs/2311.01192</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01192]] Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network(http://arxiv.org/abs/2311.01192)</code></li>
<li>Summary: <p>Along with generative AI, interest in scene graph generation (SGG), which
comprehensively captures the relationships and interactions between objects in
an image and creates a structured graph-based representation, has significantly
increased in recent years. However, relying on object-centric and dichotomous
relationships, existing SGG methods have a limited ability to accurately
predict detailed relationships. To solve these problems, a new approach to the
modeling multiobject relationships, called edge dual scene graph generation
(EdgeSGG), is proposed herein. EdgeSGG is based on a edge dual scene graph and
Dual Message Passing Neural Network (DualMPNN), which can capture rich
contextual interactions between unconstrained objects. To facilitate the
learning of edge dual scene graphs with a symmetric graph structure, the
proposed DualMPNN learns both object- and relation-centric features for more
accurately predicting relation-aware contexts and allows fine-grained
relational updates between objects. A comparative experiment with
state-of-the-art (SoTA) methods was conducted using two public datasets for SGG
operations and six metrics for three subtasks. Compared with SoTA approaches,
the proposed model exhibited substantial performance improvements across all
SGG subtasks. Furthermore, experiment on long-tail distributions revealed that
incorporating the relationships between objects effectively mitigates existing
long-tail problems.
</p></li>
</ul>

<h3>Title: Robust Identity Perceptual Watermark Against Deepfake Face Swapping. (arXiv:2311.01357v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01357">http://arxiv.org/abs/2311.01357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01357]] Robust Identity Perceptual Watermark Against Deepfake Face Swapping(http://arxiv.org/abs/2311.01357)</code></li>
<li>Summary: <p>Notwithstanding offering convenience and entertainment to society, Deepfake
face swapping has caused critical privacy issues with the rapid development of
deep generative models. Due to imperceptible artifacts in high-quality
synthetic images, passive detection models against face swapping in recent
years usually suffer performance damping regarding the generalizability issue.
Therefore, several studies have been attempted to proactively protect the
original images against malicious manipulations by inserting invisible signals
in advance. However, the existing proactive defense approaches demonstrate
unsatisfactory results with respect to visual quality, detection accuracy, and
source tracing ability. In this study, we propose the first robust identity
perceptual watermarking framework that concurrently performs detection and
source tracing against Deepfake face swapping proactively. We assign identity
semantics regarding the image contents to the watermarks and devise an
unpredictable and unreversible chaotic encryption system to ensure watermark
confidentiality. The watermarks are encoded and recovered by jointly training
an encoder-decoder framework along with adversarial image manipulations.
Extensive experiments demonstrate state-of-the-art performance against Deepfake
face swapping under both cross-dataset and cross-manipulation settings.
</p></li>
</ul>

<h3>Title: Multi-dimensional data refining strategy for effective fine-tuning LLMs. (arXiv:2311.01049v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01049">http://arxiv.org/abs/2311.01049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01049]] Multi-dimensional data refining strategy for effective fine-tuning LLMs(http://arxiv.org/abs/2311.01049)</code></li>
<li>Summary: <p>Data is a cornerstone for fine-tuning large language models, yet acquiring
suitable data remains challenging. Challenges encompassed data scarcity,
linguistic diversity, and domain-specific content. This paper presents lessons
learned while crawling and refining data tailored for fine-tuning Vietnamese
language models. Crafting such a dataset, while accounting for linguistic
intricacies and striking a balance between inclusivity and accuracy, demands
meticulous planning. Our paper presents a multidimensional strategy including
leveraging existing datasets in the English language and developing customized
data-crawling scripts with the assistance of generative AI tools. A fine-tuned
LLM model for the Vietnamese language, which was produced using resultant
datasets, demonstrated good performance while generating Vietnamese news
articles from prompts. The study offers practical solutions and guidance for
future fine-tuning models in languages like Vietnamese.
</p></li>
</ul>

<h3>Title: Generative Input: Towards Next-Generation Input Methods Paradigm. (arXiv:2311.01166v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01166">http://arxiv.org/abs/2311.01166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01166]] Generative Input: Towards Next-Generation Input Methods Paradigm(http://arxiv.org/abs/2311.01166)</code></li>
<li>Summary: <p>Since the release of ChatGPT, generative models have achieved tremendous
success and become the de facto approach for various NLP tasks. However, its
application in the field of input methods remains under-explored. Many neural
network approaches have been applied to the construction of Chinese input
method engines(IMEs).Previous research often assumed that the input pinyin was
correct and focused on Pinyin-to-character(P2C) task, which significantly falls
short of meeting users' demands. Moreover, previous research could not leverage
user feedback to optimize the model and provide personalized results. In this
study, we propose a novel Generative Input paradigm named GeneInput. It uses
prompts to handle all input scenarios and other intelligent auxiliary input
functions, optimizing the model with user feedback to deliver personalized
results. The results demonstrate that we have achieved state-of-the-art
performance for the first time in the Full-mode Key-sequence to
Characters(FK2C) task. We propose a novel reward model training method that
eliminates the need for additional manual annotations and the performance
surpasses GPT-4 in tasks involving intelligent association and conversational
assistance. Compared to traditional paradigms, GeneInput not only demonstrates
superior performance but also exhibits enhanced robustness, scalability, and
online learning capabilities.
</p></li>
</ul>

<h3>Title: People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection. (arXiv:2311.01270v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01270">http://arxiv.org/abs/2311.01270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01270]] People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection(http://arxiv.org/abs/2311.01270)</code></li>
<li>Summary: <p>NLP models are used in a variety of critical social computing tasks, such as
detecting sexist, racist, or otherwise hateful content. Therefore, it is
imperative that these models are robust to spurious features. Past work has
attempted to tackle such spurious features using training data augmentation,
including Counterfactually Augmented Data (CADs). CADs introduce minimal
changes to existing training data points and flip their labels; training on
them may reduce model dependency on spurious features. However, manually
generating CADs can be time-consuming and expensive. Hence in this work, we
assess if this task can be automated using generative NLP models. We
automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate
their usefulness in improving model robustness compared to manually-generated
CADs. By testing both model performance on multiple out-of-domain test sets and
individual data point efficacy, our results show that while manual CADs are
still the most effective, CADs generated by ChatGPT come a close second. One
key reason for the lower performance of automated methods is that the changes
they introduce are often insufficient to flip the original label.
</p></li>
</ul>

<h3>Title: Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information. (arXiv:2311.01326v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01326">http://arxiv.org/abs/2311.01326</a></li>
<li>Code URL: https://github.com/screemix/kgc-t5-with-neighbors</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01326]] Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information(http://arxiv.org/abs/2311.01326)</code></li>
<li>Summary: <p>Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which
limits their potential performance. Knowledge Graph Completion (KGC) techniques
aim to address this issue. However, traditional KGC methods are computationally
intensive and impractical for large-scale KGs, necessitating the learning of
dense node embeddings and computing pairwise distances. Generative
transformer-based language models (e.g., T5 and recent KGT5) offer a promising
solution as they can predict the tail nodes directly. In this study, we propose
to include node neighborhoods as additional information to improve KGC methods
based on language models. We examine the effects of this imputation and show
that, on both inductive and transductive Wikidata subsets, our method
outperforms KGT5 and conventional KGC approaches. We also provide an extensive
analysis of the impact of neighborhood on model prediction and show its
importance. Furthermore, we point the way to significantly improve KGC through
more effective neighborhood selection.
</p></li>
</ul>

<h3>Title: Monotone Generative Modeling via a Gromov-Monge Embedding. (arXiv:2311.01375v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01375">http://arxiv.org/abs/2311.01375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01375]] Monotone Generative Modeling via a Gromov-Monge Embedding(http://arxiv.org/abs/2311.01375)</code></li>
<li>Summary: <p>Generative Adversarial Networks (GANs) are powerful tools for creating new
content, but they face challenges such as sensitivity to starting conditions
and mode collapse. To address these issues, we propose a deep generative model
that utilizes the Gromov-Monge embedding (GME). It helps identify the
low-dimensional structure of the underlying measure of the data and then maps
it, while preserving its geometry, into a measure in a low-dimensional latent
space, which is then optimally transported to the reference measure. We
guarantee the preservation of the underlying geometry by the GME and
$c$-cyclical monotonicity of the generative map, where $c$ is an intrinsic
embedding cost employed by the GME. The latter property is a first step in
guaranteeing better robustness to initialization of parameters and mode
collapse. Numerical experiments demonstrate the effectiveness of our approach
in generating high-quality images, avoiding mode collapse, and exhibiting
robustness to different starting conditions.
</p></li>
</ul>

<h3>Title: Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods. (arXiv:2311.01428v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01428">http://arxiv.org/abs/2311.01428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01428]] Identifying Alzheimer Disease Dementia Levels Using Machine Learning Methods(http://arxiv.org/abs/2311.01428)</code></li>
<li>Summary: <p>Dementia, a prevalent neurodegenerative condition, is a major manifestation
of Alzheimer's disease (AD). As the condition progresses from mild to severe,
it significantly impairs the individual's ability to perform daily tasks
independently, necessitating the need for timely and accurate AD
classification. Machine learning or deep learning models have emerged as
effective tools for this purpose. In this study, we suggested an approach for
classifying the four stages of dementia using RF, SVM, and CNN algorithms,
augmented with watershed segmentation for feature extraction from MRI images.
Our results reveal that SVM with watershed features achieves an impressive
accuracy of 96.25%, surpassing other classification methods. The ADNI dataset
is utilized to evaluate the effectiveness of our method, and we observed that
the inclusion of watershed segmentation contributes to the enhanced performance
of the models.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Cheating Depth: Enhancing 3D Surface Anomaly Detection via Depth Simulation. (arXiv:2311.01117v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01117">http://arxiv.org/abs/2311.01117</a></li>
<li>Code URL: https://github.com/vitjanz/3dsr</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01117]] Cheating Depth: Enhancing 3D Surface Anomaly Detection via Depth Simulation(http://arxiv.org/abs/2311.01117)</code></li>
<li>Summary: <p>RGB-based surface anomaly detection methods have advanced significantly.
However, certain surface anomalies remain practically invisible in RGB alone,
necessitating the incorporation of 3D information. Existing approaches that
employ point-cloud backbones suffer from suboptimal representations and reduced
applicability due to slow processing. Re-training RGB backbones, designed for
faster dense input processing, on industrial depth datasets is hindered by the
limited availability of sufficiently large datasets. We make several
contributions to address these challenges. (i) We propose a novel Depth-Aware
Discrete Autoencoder (DADA) architecture, that enables learning a general
discrete latent space that jointly models RGB and 3D data for 3D surface
anomaly detection. (ii) We tackle the lack of diverse industrial depth datasets
by introducing a simulation process for learning informative depth features in
the depth encoder. (iii) We propose a new surface anomaly detection method
3DSR, which outperforms all existing state-of-the-art on the challenging
MVTec3D anomaly detection benchmark, both in terms of accuracy and processing
speed. The experimental results validate the effectiveness and efficiency of
our approach, highlighting the potential of utilizing depth information for
improved surface anomaly detection.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models. (arXiv:2311.00871v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.00871">http://arxiv.org/abs/2311.00871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.00871]] Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models(http://arxiv.org/abs/2311.00871)</code></li>
<li>Summary: <p>Transformer models, notably large language models (LLMs), have the remarkable
ability to perform in-context learning (ICL) -- to perform new tasks when
prompted with unseen input-output examples without any explicit model training.
In this work, we study how effectively transformers can bridge between their
pretraining data mixture, comprised of multiple distinct task families, to
identify and learn new tasks in-context which are both inside and outside the
pretraining distribution. Building on previous work, we investigate this
question in a controlled setting, where we study transformer models trained on
sequences of $(x, f(x))$ pairs rather than natural language. Our empirical
results show transformers demonstrate near-optimal unsupervised model selection
capabilities, in their ability to first in-context identify different task
families and in-context learn within them when the task families are
well-represented in their pretraining data. However when presented with tasks
or functions which are out-of-domain of their pretraining data, we demonstrate
various failure modes of transformers and degradation of their generalization
for even simple extrapolation tasks. Together our results highlight that the
impressive ICL abilities of high-capacity sequence models may be more closely
tied to the coverage of their pretraining data mixtures than inductive biases
that create fundamental generalization capabilities.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
