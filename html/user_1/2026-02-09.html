<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-09</h1>
<h3>Title: Multi-Agent-Driven Cognitive Secure Communications in Satellite-Terrestrial Networks</h3>
<ul>
<li><strong>Authors: </strong>Yujie Ling, Zan Li, Lei Guan, Zheng Zhang, Shengyu Zhang, Tony Q.S. Quek</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06048">https://arxiv.org/abs/2602.06048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06048">https://arxiv.org/pdf/2602.06048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06048]] Multi-Agent-Driven Cognitive Secure Communications in Satellite-Terrestrial Networks(https://arxiv.org/abs/2602.06048)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Satellite-terrestrial networks (STNs) have emerged as a promising architecture for providing seamless wireless coverage and connectivity for multiple users. However, potential malicious eavesdroppers pose a serious threat to the private information via STNs due to their non-cooperative behavior and ability to launch intelligent attacks. To address this challenge, we propose a cognitive secure communication framework driven by multiple agents that coordinates spectrum scheduling and protection through real-time sensing, thereby disrupting the judgment of eavesdroppers while preserving reliable data transmission. On this basis, we formulate an optimization problem to maximize the secrecy probability of legitimate users, subject to a reliable transmission probability threshold. To tackle this problem, we propose a two-layer coordinated defense system. First, we develop a foundation layer based on multi-agent coordination schedule to determine the satellite operation matrix and the frequency slot occupation matrices, aiming to mitigate spectrum congestion and enhance transmission reliability. Then, we exploit generative adversarial networks to produce adversarial matrices, and employ learning-aided power control to set real and adversarial signal powers for protection layer, which actively degrades the inference capability of eavesdroppers. Simulation results demonstrate that the proposed method outperforms benchmark methods in terms of enhancing security performance and reducing power overhead for STNs in the cognitive secure communication scenario.</li>
</ul>

<h3>Title: From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors</h3>
<ul>
<li><strong>Authors: </strong>Ding-Jiun Huang, Yuanhao Wang, Shao-Ji Yuan, Albert Mosella-Montoro, Francisco Vicente Carrasco, Cheng Zhang, Fernando De la Torre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06122">https://arxiv.org/abs/2602.06122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06122">https://arxiv.org/pdf/2602.06122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06122]] From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors(https://arxiv.org/abs/2602.06122)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.</li>
</ul>

<h3>Title: Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction</h3>
<ul>
<li><strong>Authors: </strong>Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06129">https://arxiv.org/abs/2602.06129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06129">https://arxiv.org/pdf/2602.06129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06129]] Urban Spatio-Temporal Foundation Models for Climate-Resilient Housing: Scaling Diffusion Transformers for Disaster Risk Prediction(https://arxiv.org/abs/2602.06129)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Climate hazards increasingly disrupt urban transportation and emergency-response operations by damaging housing stock, degrading infrastructure, and reducing network accessibility. This paper presents Skjold-DiT, a diffusion-transformer framework that integrates heterogeneous spatio-temporal urban data to forecast building-level climate-risk indicators while explicitly incorporating transportation-network structure and accessibility signals relevant to intelligent vehicles (e.g., emergency reachability and evacuation-route constraints). Concretely, Skjold-DiT enables hazard-conditioned routing constraints by producing calibrated, uncertainty-aware accessibility layers (reachability, travel-time inflation, and route redundancy) that can be consumed by intelligent-vehicle routing and emergency dispatch systems. Skjold-DiT combines: (1) Fjell-Prompt, a prompt-based conditioning interface designed to support cross-city transfer; (2) Norrland-Fusion, a cross-modal attention mechanism unifying hazard maps/imagery, building attributes, demographics, and transportation infrastructure into a shared latent representation; and (3) Valkyrie-Forecast, a counterfactual simulator for generating probabilistic risk trajectories under intervention prompts. We introduce the Baltic-Caspian Urban Resilience (BCUR) dataset with 847,392 building-level observations across six cities, including multi-hazard annotations (e.g., flood and heat indicators) and transportation accessibility features. Experiments evaluate prediction quality, cross-city generalization, calibration, and downstream transportation-relevant outcomes, including reachability and hazard-conditioned travel times under counterfactual interventions.</li>
</ul>

<h3>Title: Flow Matching for Offline Reinforcement Learning with Discrete Actions</h3>
<ul>
<li><strong>Authors: </strong>Fairoz Nower Khan, Nabuat Zaman Nahim, Ruiquan Huang, Haibo Yang, Peizhong Ju</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06138">https://arxiv.org/abs/2602.06138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06138">https://arxiv.org/pdf/2602.06138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06138]] Flow Matching for Offline Reinforcement Learning with Discrete Actions(https://arxiv.org/abs/2602.06138)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative policies based on diffusion models and flow matching have shown strong promise for offline reinforcement learning (RL), but their applicability remains largely confined to continuous action spaces. To address a broader range of offline RL settings, we extend flow matching to a general framework that supports discrete action spaces with multiple objectives. Specifically, we replace continuous flows with continuous-time Markov chains, trained using a Q-weighted flow matching objective. We then extend our design to multi-agent settings, mitigating the exponential growth of joint action spaces via a factorized conditional path. We theoretically show that, under idealized conditions, optimizing this objective recovers the optimal policy. Extensive experiments further demonstrate that our method performs robustly in practical scenarios, including high-dimensional control, multi-modal decision-making, and dynamically changing preferences over multiple objectives. Our discrete framework can also be applied to continuous-control problems through action quantization, providing a flexible trade-off between representational complexity and performance.</li>
</ul>

<h3>Title: Latent Structure Emergence in Diffusion Models via Confidence-Based Filtering</h3>
<ul>
<li><strong>Authors: </strong>Wei Wei, Yizhou Zeng, Kuntian Chen, Sophie Langer, Mariia Seleznova, Hung-Hsu Chou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06155">https://arxiv.org/abs/2602.06155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06155">https://arxiv.org/pdf/2602.06155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06155]] Latent Structure Emergence in Diffusion Models via Confidence-Based Filtering(https://arxiv.org/abs/2602.06155)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models rely on a high-dimensional latent space of initial noise seeds, yet it remains unclear whether this space contains sufficient structure to predict properties of the generated samples, such as their classes. In this work, we investigate the emergence of latent structure through the lens of confidence scores assigned by a pre-trained classifier to generated samples. We show that while the latent space appears largely unstructured when considering all noise realizations, restricting attention to initial noise seeds that produce high-confidence samples reveals pronounced class separability. By comparing class predictability across noise subsets of varying confidence and examining the class separability of the latent space, we find evidence of class-relevant latent structure that becomes observable only under confidence-based filtering. As a practical implication, we discuss how confidence-based filtering enables conditional generation as an alternative to guidance-based methods.</li>
</ul>

<h3>Title: Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Chen, Conglang Zhang, Chuanheng Fu, Zihao Yang, Kaixuan Zhou, Yizhi Zhang, Jianan He, Yanfeng Zhang, Mingwei Sun, Zengmao Wang, Zhen Dong, Xiaoxiao Long, Liqiu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06159">https://arxiv.org/abs/2602.06159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06159">https://arxiv.org/pdf/2602.06159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06159]] Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving(https://arxiv.org/abs/2602.06159)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by "baking in" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for "texture baking," while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: this https URL</li>
</ul>

<h3>Title: Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding</h3>
<ul>
<li><strong>Authors: </strong>Yanzheng Xiang, Lan Wei, Yizhen Yao, Qinglin Zhu, Hanqi Yan, Chen Jin, Philip Alexander Teare, Dandan Zhang, Lin Gui, Amrutha Saseendran, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06161">https://arxiv.org/abs/2602.06161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06161">https://arxiv.org/pdf/2602.06161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06161]] Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding(https://arxiv.org/abs/2602.06161)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.</li>
</ul>

<h3>Title: M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Bangji Yang, Ruihan Guo, Jiajun Fan, Chaoran Cheng, Ge Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06166">https://arxiv.org/abs/2602.06166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06166">https://arxiv.org/pdf/2602.06166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06166]] M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning(https://arxiv.org/abs/2602.06166)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Generative models have achieved impressive fidelity in text-to-image synthesis, yet struggle with complex compositional prompts involving multiple constraints. We introduce \textbf{M3 (Multi-Modal, Multi-Agent, Multi-Round)}, a training-free framework that systematically resolves these failures through iterative inference-time refinement. M3 orchestrates off-the-shelf foundation models in a robust multi-agent loop: a Planner decomposes prompts into verifiable checklists, while specialized Checker, Refiner, and Editor agents surgically correct constraints one at a time, with a Verifier ensuring monotonic improvement. Applied to open-source models, M3 achieves remarkable results on the challenging OneIG-EN benchmark, with our Qwen-Image+M3 surpassing commercial flagship systems including Imagen4 (0.515) and Seedream 3.0 (0.530), reaching state-of-the-art performance (0.532 overall). This demonstrates that intelligent multi-agent reasoning can elevate open-source models beyond proprietary alternatives. M3 also substantially improves GenEval compositional metrics, effectively doubling spatial reasoning performance on hardened test sets. As a plug-and-play module compatible with any pre-trained T2I model, M3 establishes a new paradigm for compositional generation without costly retraining.</li>
</ul>

<h3>Title: Unsupervised Anomaly Detection of Diseases in the Female Pelvis for Real-Time MR Imaging</h3>
<ul>
<li><strong>Authors: </strong>Anika Knupfer, Johanna P. MÃ¼ller, Jordina A. Verdera, Martin Fenske, Claudius S. Mathy, Smiti Tripathy, Sebastian Arndt, Matthias May, Michael Uder, Matthias W. Beckmann, Stefanie Burghaus, Jana Hutter</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06179">https://arxiv.org/abs/2602.06179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06179">https://arxiv.org/pdf/2602.06179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06179]] Unsupervised Anomaly Detection of Diseases in the Female Pelvis for Real-Time MR Imaging(https://arxiv.org/abs/2602.06179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Pelvic diseases in women of reproductive age represent a major global health burden, with diagnosis frequently delayed due to high anatomical variability, complicating MRI interpretation. Existing AI approaches are largely disease-specific and lack real-time compatibility, limiting generalizability and clinical integration. To address these challenges, we establish a benchmark framework for disease- and parameter-agnostic, real-time-compatible unsupervised anomaly detection in pelvic MRI. The method uses a residual variational autoencoder trained exclusively on healthy sagittal T2-weighted scans acquired across diverse imaging protocols to model normal pelvic anatomy. During inference, reconstruction error heatmaps indicate deviations from learned healthy structure, enabling detection of pathological regions without labeled abnormal data. The model is trained on 294 healthy scans and augmented with diffusion-generated synthetic data to improve robustness. Quantitative evaluation on the publicly available Uterine Myoma MRI Dataset yields an average area-under-the-curve (AUC) value of 0.736, with 0.828 sensitivity and 0.692 specificity. Additional inter-observer clinical evaluation extends analysis to endometrial cancer, endometriosis, and adenomyosis, revealing the influence of anatomical heterogeneity and inter-observer variability on performance interpretation. With a reconstruction time of approximately 92.6 frames per second, the proposed framework establishes a baseline for unsupervised anomaly detection in the female pelvis and supports future integration into real-time MRI. Code is available upon request (this https URL), prospective data sets are available for academic collaboration.</li>
</ul>

<h3>Title: DeDPO: Debiased Direct Preference Optimization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Khiem Pham, Quang Nguyen, Tung Nguyen, Jingsen Zhu, Michele Santacatterina, Dimitris Metaxas, Ramin Zabih</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06195">https://arxiv.org/abs/2602.06195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06195">https://arxiv.org/pdf/2602.06195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06195]] DeDPO: Debiased Direct Preference Optimization for Diffusion Models(https://arxiv.org/abs/2602.06195)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has emerged as a predominant alignment method for diffusion models, facilitating off-policy training without explicit reward modeling. However, its reliance on large-scale, high-quality human preference labels presents a severe cost and scalability bottleneck. To overcome this, We propose a semi-supervised framework augmenting limited human data with a large corpus of unlabeled pairs annotated via cost-effective synthetic AI feedback. Our paper introduces Debiased DPO (DeDPO), which uniquely integrates a debiased estimation technique from causal inference into the DPO objective. By explicitly identifying and correcting the systematic bias and noise inherent in synthetic annotators, DeDPO ensures robust learning from imperfect feedback sources, including self-training and Vision-Language Models (VLMs). Experiments demonstrate that DeDPO is robust to the variations in synthetic labeling methods, achieving performance that matches and occasionally exceeds the theoretical upper bound of models trained on fully human-labeled data. This establishes DeDPO as a scalable solution for human-AI alignment using inexpensive synthetic supervision.</li>
</ul>

<h3>Title: AnyThermal: Towards Learning Universal Representations for Thermal Perception</h3>
<ul>
<li><strong>Authors: </strong>Parv Maheshwari, Jay Karhade, Yogesh Chawla, Isaiah Adu, Florian Heisen, Andrew Porco, Andrew Jong, Yifei Liu, Santosh Pitla, Sebastian Scherer, Wenshan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06203">https://arxiv.org/abs/2602.06203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06203">https://arxiv.org/pdf/2602.06203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06203]] AnyThermal: Towards Learning Universal Representations for Thermal Perception(https://arxiv.org/abs/2602.06203)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present AnyThermal, a thermal backbone that captures robust task-agnostic thermal features suitable for a variety of tasks such as cross-modal place recognition, thermal segmentation, and monocular depth estimation using thermal images. Existing thermal backbones that follow task-specific training from small-scale data result in utility limited to a specific environment and task. Unlike prior methods, AnyThermal can be used for a wide range of environments (indoor, aerial, off-road, urban) and tasks, all without task-specific training. Our key insight is to distill the feature representations from visual foundation models such as DINOv2 into a thermal encoder using thermal data from these multiple environments. To bridge the diversity gap of the existing RGB-Thermal datasets, we introduce the TartanRGBT platform, the first open-source data collection platform with synced RGB-Thermal image acquisition. We use this payload to collect the TartanRGBT dataset - a diverse and balanced dataset collected in 4 environments. We demonstrate the efficacy of AnyThermal and TartanRGBT, achieving state-of-the-art results with improvements of up to 36% across diverse environments and downstream tasks on existing datasets.</li>
</ul>

<h3>Title: ASMa: Asymmetric Spatio-temporal Masking for Skeleton Action Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Aman Anand, Amir Eskandari, Elyas Rahsno, Farhana Zulkernine</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06251">https://arxiv.org/abs/2602.06251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06251">https://arxiv.org/pdf/2602.06251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06251]] ASMa: Asymmetric Spatio-temporal Masking for Skeleton Action Representation Learning(https://arxiv.org/abs/2602.06251)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has shown remarkable success in skeleton-based action recognition by leveraging data augmentations to learn meaningful representations. However, existing SSL methods rely on data augmentations that predominantly focus on masking high-motion frames and high-degree joints such as joints with degree 3 or 4. This results in biased and incomplete feature representations that struggle to generalize across varied motion patterns. To address this, we propose Asymmetric Spatio-temporal Masking (ASMa) for Skeleton Action Representation Learning, a novel combination of masking to learn a full spectrum of spatio-temporal dynamics inherent in human actions. ASMa employs two complementary masking strategies: one that selectively masks high-degree joints and low-motion, and another that masks low-degree joints and high-motion frames. These masking strategies ensure a more balanced and comprehensive skeleton representation learning. Furthermore, we introduce a learnable feature alignment module to effectively align the representations learned from both masked views. To facilitate deployment in resource-constrained settings and on low-resource devices, we compress the learned and aligned representation into a lightweight model using knowledge distillation. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our approach outperforms existing SSL methods with an average improvement of 2.7-4.4% in fine-tuning and up to 5.9% in transfer learning to noisy datasets and achieves competitive performance compared to fully supervised baselines. Our distilled model achieves 91.4% parameter reduction and 3x faster inference on edge devices while maintaining competitive accuracy, enabling practical deployment in resource-constrained scenarios.</li>
</ul>

<h3>Title: GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt</h3>
<ul>
<li><strong>Authors: </strong>Mark Russinovich, Yanan Cai, Keegan Hines, Giorgio Severi, Blake Bullwinkel, Ahmed Salem</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06258">https://arxiv.org/abs/2602.06258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06258">https://arxiv.org/pdf/2602.06258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06258]] GRP-Obliteration: Unaligning LLMs With a Single Unlabeled Prompt(https://arxiv.org/abs/2602.06258)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Safety alignment is only as robust as its weakest failure mode. Despite extensive work on safety post-training, it has been shown that models can be readily unaligned through post-deployment fine-tuning. However, these methods often require extensive data curation and degrade model utility. In this work, we extend the practical limits of unalignment by introducing GRP-Obliteration (GRP-Oblit), a method that uses Group Relative Policy Optimization (GRPO) to directly remove safety constraints from target models. We show that a single unlabeled prompt is sufficient to reliably unalign safety-aligned models while largely preserving their utility, and that GRP-Oblit achieves stronger unalignment on average than existing state-of-the-art techniques. Moreover, GRP-Oblit generalizes beyond language models and can also unalign diffusion-based image generation systems. We evaluate GRP-Oblit on six utility benchmarks and five safety benchmarks across fifteen 7-20B parameter models, spanning instruct and reasoning models, as well as dense and MoE architectures. The evaluated model families include GPT-OSS, distilled DeepSeek, Gemma, Llama, Ministral, and Qwen.</li>
</ul>

<h3>Title: MMEarth-Bench: Global Model Adaptation via Multimodal Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>Lucia Gordon, Serge Belongie, Christian Igel, Nico Lang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06285">https://arxiv.org/abs/2602.06285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06285">https://arxiv.org/pdf/2602.06285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06285]] MMEarth-Bench: Global Model Adaptation via Multimodal Test-Time Training(https://arxiv.org/abs/2602.06285)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent research in geospatial machine learning has demonstrated that models pretrained with self-supervised learning on Earth observation data can perform well on downstream tasks with limited training data. However, most of the existing geospatial benchmark datasets have few data modalities and poor global representation, limiting the ability to evaluate multimodal pretrained models at global scales. To fill this gap, we introduce MMEarth-Bench, a collection of five new multimodal environmental tasks with 12 modalities, globally distributed data, and both in- and out-of-distribution test splits. We benchmark a diverse set of pretrained models and find that while (multimodal) pretraining tends to improve model robustness in limited data settings, geographic generalization abilities remain poor. In order to facilitate model adaptation to new downstream tasks and geographic domains, we propose a model-agnostic method for test-time training with multimodal reconstruction (TTT-MMR) that uses all the modalities available at test time as auxiliary tasks, regardless of whether a pretrained model accepts them as input. Our method improves model performance on both the random and geographic test splits, and geographic batching leads to a good trade-off between regularization and specialization during TTT. Our dataset, code, and visualization tool are linked from the project page at this http URL.</li>
</ul>

<h3>Title: Toward generative machine learning for boosting ensembles of climate simulations</h3>
<ul>
<li><strong>Authors: </strong>Parsa Gooya, Reinel Sospedra-Alfonso, Johannes Exenberger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06287">https://arxiv.org/abs/2602.06287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06287">https://arxiv.org/pdf/2602.06287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06287]] Toward generative machine learning for boosting ensembles of climate simulations(https://arxiv.org/abs/2602.06287)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurately quantifying uncertainty in predictions and projections arising from irreducible internal climate variability is critical for informed decision making. Such uncertainty is typically assessed using ensembles produced with physics based climate models. However, computational constraints impose a trade off between generating the large ensembles required for robust uncertainty estimation and increasing model resolution to better capture fine scale dynamics. Generative machine learning offers a promising pathway to alleviate these constraints. We develop a conditional Variational Autoencoder (cVAE) trained on a limited sample of climate simulations to generate arbitrary large ensembles. The approach is applied to output from monthly CMIP6 historical and future scenario experiments produced with the Canadian Centre for Climate Modelling and Analysis' (CCCma's) Earth system model CanESM5. We show that the cVAE model learns the underlying distribution of the data and generates physically consistent samples that reproduce realistic low and high moment statistics, including extremes. Compared with more sophisticated generative architectures, cVAEs offer a mathematically transparent, interpretable, and computationally efficient framework. Their simplicity lead to some limitations, such as overly smooth outputs, spectral bias, and underdispersion, that we discuss along with strategies to mitigate them. Specifically, we show that incorporating output noise improves the representation of climate relevant multiscale variability, and we propose a simple method to achieve this. Finally, we show that cVAE-enhanced ensembles capture realistic global teleconnection patterns, even under climate conditions absent from the training data.</li>
</ul>

<h3>Title: Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math</h3>
<ul>
<li><strong>Authors: </strong>Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, Hyunwoo Ko, Amit Agarwal, Sunghee Ahn, Kyong-Ha Lee, Youngjae Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06291">https://arxiv.org/abs/2602.06291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06291">https://arxiv.org/pdf/2602.06291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06291]] Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math(https://arxiv.org/abs/2602.06291)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.</li>
</ul>

<h3>Title: Halt the Hallucination: Decoupling Signal and Semantic OOD Detection Based on Cascaded Early Rejection</h3>
<ul>
<li><strong>Authors: </strong>Ningkang Peng, Chuanjie Cheng, Jingyang Mao, Xiaoqian Peng, Feng Xing, Bo Zhang, Chao Tan, Zhichao Zheng, Peiheng Li, Yanhui Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06330">https://arxiv.org/abs/2602.06330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06330">https://arxiv.org/pdf/2602.06330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06330]] Halt the Hallucination: Decoupling Signal and Semantic OOD Detection Based on Cascaded Early Rejection(https://arxiv.org/abs/2602.06330)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Efficient and robust Out-of-Distribution (OOD) detection is paramount for safety-critical this http URL, existing methods still execute full-scale inference on low-level statistical noise. This computational mismatch not only incurs resource waste but also induces semantic hallucination, where deep networks forcefully interpret physical anomalies as high-confidence semantic this http URL address this, we propose the Cascaded Early Rejection (CER) framework, which realizes hierarchical filtering for anomaly detection via a coarse-to-fine this http URL comprises two core modules: 1)Structural Energy Sieve (SES), which establishes a non-parametric barrier at the network entry using the Laplacian operator to efficiently intercept physical signal anomalies; and 2) the Semantically-aware Hyperspherical Energy (SHE) detector, which decouples feature magnitude from direction in intermediate layers to identify fine-grained semantic deviations. Experimental results demonstrate that CER not only reduces computational overhead by 32% but also achieves a significant performance leap on the CIFAR-100 benchmark:the average FPR95 drastically decreases from 33.58% to 22.84%, and AUROC improves to 93.97%. Crucially, in real-world scenarios simulating sensor failures, CER exhibits performance far exceeding state-of-the-art methods. As a universal plugin, CER can be seamlessly integrated into various SOTA models to provide performance gains.</li>
</ul>

<h3>Title: Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering</h3>
<ul>
<li><strong>Authors: </strong>Weiquan Wang, Feifei Shao, Lin Li, Zhen Wang, Jun Xiao, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06343">https://arxiv.org/abs/2602.06343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06343">https://arxiv.org/pdf/2602.06343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06343]] Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering(https://arxiv.org/abs/2602.06343)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.</li>
</ul>

<h3>Title: Di3PO -- Diptych Diffusion DPO for Targeted Improvements in Image</h3>
<ul>
<li><strong>Authors: </strong>Sanjana Reddy (1), Ishaan Malhi (2), Sally Ma (2), Praneet Dutta (2) ((1) Google, (2) Google DeepMind)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06355">https://arxiv.org/abs/2602.06355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06355">https://arxiv.org/pdf/2602.06355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06355]] Di3PO -- Diptych Diffusion DPO for Targeted Improvements in Image(https://arxiv.org/abs/2602.06355)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing methods for preference tuning of text-to-image (T2I) diffusion models often rely on computationally expensive generation steps to create positive and negative pairs of images. These approaches frequently yield training pairs that either lack meaningful differences, are expensive to sample and filter, or exhibit significant variance in irrelevant pixel regions, thereby degrading training efficiency. To address these limitations, we introduce "Di3PO", a novel method for constructing positive and negative pairs that isolates specific regions targeted for improvement during preference tuning, while keeping the surrounding context in the image stable. We demonstrate the efficacy of our approach by applying it to the challenging task of text rendering in diffusion models, showcasing improvements over baseline methods of SFT and DPO.</li>
</ul>

<h3>Title: SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass</h3>
<ul>
<li><strong>Authors: </strong>Yewei Liu, Xiyuan Wang, Yansheng Mao, Yoav Gelbery, Haggai Maron, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06358">https://arxiv.org/abs/2602.06358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06358">https://arxiv.org/pdf/2602.06358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06358]] SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass(https://arxiv.org/abs/2602.06358)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at this https URL</li>
</ul>

<h3>Title: Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production</h3>
<ul>
<li><strong>Authors: </strong>Alberto Andres Valdes Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06370">https://arxiv.org/abs/2602.06370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06370">https://arxiv.org/pdf/2602.06370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06370]] Cost-Aware Model Selection for Text Classification: Multi-Objective Trade-offs Between Fine-Tuned Encoders and LLM Prompting in Production(https://arxiv.org/abs/2602.06370)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) such as GPT-4o and Claude Sonnet 4.5 have demonstrated strong capabilities in open-ended reasoning and generative language tasks, leading to their widespread adoption across a broad range of NLP applications. However, for structured text classification problems with fixed label spaces, model selection is often driven by predictive performance alone, overlooking operational constraints encountered in production systems. In this work, we present a systematic comparison of two contrasting paradigms for text classification: zero- and few-shot prompt-based large language models, and fully fine-tuned encoder-only architectures. We evaluate these approaches across four canonical benchmarks (IMDB, SST-2, AG News, and DBPedia), measuring predictive quality (macro F1), inference latency, and monetary cost. We frame model evaluation as a multi-objective decision problem and analyze trade-offs using Pareto frontier projections and a parameterized utility function reflecting different deployment regimes. Our results show that fine-tuned encoder-based models from the BERT family achieve competitive, and often superior, classification performance while operating at one to two orders of magnitude lower cost and latency compared to zero- and few-shot LLM prompting. Overall, our findings suggest that indiscriminate use of large language models for standard text classification workloads can lead to suboptimal system-level outcomes. Instead, fine-tuned encoders emerge as robust and efficient components for structured NLP pipelines, while LLMs are better positioned as complementary elements within hybrid architectures. We release all code, datasets, and evaluation protocols to support reproducibility and cost-aware NLP system design.</li>
</ul>

<h3>Title: Generating High-quality Privacy-preserving Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>David Yavo, Richard Khoury, Christophe Pere, Sadoune Ait Kaci Azzou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06390">https://arxiv.org/abs/2602.06390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06390">https://arxiv.org/pdf/2602.06390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06390]] Generating High-quality Privacy-preserving Synthetic Data(https://arxiv.org/abs/2602.06390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data enables sharing and analysis of sensitive records, but its practical deployment requires balancing distributional fidelity, downstream utility, and privacy protection. We study a simple, model agnostic post processing framework that can be applied on top of any synthetic data generator to improve this trade off. First, a mode patching step repairs categories that are missing or severely underrepresented in the synthetic data, while largely preserving learned dependencies. Second, a k nearest neighbor filter replaces synthetic records that lie too close to real data points, enforcing a minimum distance between real and synthetic samples. We instantiate this framework for two neural generative models for tabular data, a feed forward generator and a variational autoencoder, and evaluate it on three public datasets covering credit card transactions, cardiovascular health, and census based income. We assess marginal and joint distributional similarity, the performance of models trained on synthetic data and evaluated on real data, and several empirical privacy indicators, including nearest neighbor distances and attribute inference attacks. With moderate thresholds between 0.2 and 0.35, the post processing reduces divergence between real and synthetic categorical distributions by up to 36 percent and improves a combined measure of pairwise dependence preservation by 10 to 14 percent, while keeping downstream predictive performance within about 1 percent of the unprocessed baseline. At the same time, distance based privacy indicators improve and the success rate of attribute inference attacks remains largely unchanged. These results provide practical guidance for selecting thresholds and applying post hoc repairs to improve the quality and empirical privacy of synthetic tabular data, while complementing approaches that provide formal differential privacy guarantees.</li>
</ul>

<h3>Title: A neuromorphic model of the insect visual system for natural image processing</h3>
<ul>
<li><strong>Authors: </strong>Adam D. Hines, Karin NordstrÃ¶m, Andrew B. Barron</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06405">https://arxiv.org/abs/2602.06405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06405">https://arxiv.org/pdf/2602.06405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06405]] A neuromorphic model of the insect visual system for natural image processing(https://arxiv.org/abs/2602.06405)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.</li>
</ul>

<h3>Title: Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding</h3>
<ul>
<li><strong>Authors: </strong>Daisuke Oba, Danushka Bollegala, Masahiro Kaneko, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06412">https://arxiv.org/abs/2602.06412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06412">https://arxiv.org/pdf/2602.06412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06412]] Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding(https://arxiv.org/abs/2602.06412)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at this https URL .</li>
</ul>

<h3>Title: Adaptive Protein Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Rohit Dilip, Ayush Varshney, David Van Valen</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06418">https://arxiv.org/abs/2602.06418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06418">https://arxiv.org/pdf/2602.06418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06418]] Adaptive Protein Tokenization(https://arxiv.org/abs/2602.06418)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Tokenization is a promising path to multi-modal models capable of jointly understanding protein sequences, structure, and function. Existing protein structure tokenizers create tokens by pooling information from local neighborhoods, an approach that limits their performance on generative and representation tasks. In this work, we present a method for global tokenization of protein structures in which successive tokens contribute increasing levels of detail to a global representation. This change resolves several issues with generative models based on local protein tokenization: it mitigates error accumulation, provides embeddings without sequence-reduction operations, and allows task-specific adaptation of a tokenized sequence's information content. We validate our method on reconstruction, generative, and representation tasks and demonstrate that it matches or outperforms existing models based on local protein structure tokenizers. We show how adaptive tokens enable inference criteria based on information content, which boosts designability. We validate representations generated from our tokenizer on CATH classification tasks and demonstrate that non-linear probing on our tokenized sequences outperforms equivalent probing on representations from other tokenizers. Finally, we demonstrate how our method supports zero-shot protein shrinking and affinity maturation.</li>
</ul>

<h3>Title: Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors</h3>
<ul>
<li><strong>Authors: </strong>Soham Pahari, Sandeep C. Kumain</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06419">https://arxiv.org/abs/2602.06419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06419">https://arxiv.org/pdf/2602.06419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06419]] Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors(https://arxiv.org/abs/2602.06419)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human visual attention on three-dimensional objects emerges from the interplay between bottom-up geometric processing and top-down semantic recognition. Existing 3D saliency methods rely on hand-crafted geometric features or learning-based approaches that lack semantic awareness, failing to explain why humans fixate on semantically meaningful but geometrically unremarkable regions. We introduce SemGeo-AttentionNet, a dual-stream architecture that explicitly formalizes this dichotomy through asymmetric cross-modal fusion, leveraging diffusion-based semantic priors from geometry-conditioned multi-view rendering and point cloud transformers for geometric processing. Cross-attention ensures geometric features query semantic content, enabling bottom-up distinctiveness to guide top-down retrieval. We extend our framework to temporal scanpath generation through reinforcement learning, introducing the first formulation respecting 3D mesh topology with inhibition-of-return dynamics. Evaluation on SAL3D, NUS3D and 3DVA datasets demonstrates substantial improvements, validating how cognitively motivated architectures effectively model human visual attention on three-dimensional surfaces.</li>
</ul>

<h3>Title: TrajAD: Trajectory Anomaly Detection for Trustworthy LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Yibing Liu, Chong Zhang, Zhongyi Han, Hansong Liu, Yong Wang, Yang Yu, Xiaoyan Wang, Yilong Yin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06443">https://arxiv.org/abs/2602.06443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06443">https://arxiv.org/pdf/2602.06443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06443]] TrajAD: Trajectory Anomaly Detection for Trustworthy LLM Agents(https://arxiv.org/abs/2602.06443)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We address the problem of runtime trajectory anomaly detection, a critical capability for enabling trustworthy LLM agents. Current safety measures predominantly focus on static input/output filtering. However, we argue that ensuring LLM agents reliability requires auditing the intermediate execution process. In this work, we formulate the task of Trajectory Anomaly Detection. The goal is not merely detection, but precise error localization. This capability is essential for enabling efficient rollback-and-retry. To achieve this, we construct TrajBench, a dataset synthesized via a perturb-and-complete strategy to cover diverse procedural anomalies. Using this benchmark, we investigate the capability of models in process supervision. We observe that general-purpose LLMs, even with zero-shot prompting, struggle to identify and localize these anomalies. This reveals that generalized capabilities do not automatically translate to process reliability. To address this, we propose TrajAD, a specialized verifier trained with fine-grained process supervision. Our approach outperforms baselines, demonstrating that specialized supervision is essential for building trustworthy agents.</li>
</ul>

<h3>Title: Principle-Evolvable Scientific Discovery via Uncertainty Minimization</h3>
<ul>
<li><strong>Authors: </strong>Yingming Pu, Tao Lin, Hongyu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06448">https://arxiv.org/abs/2602.06448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06448">https://arxiv.org/pdf/2602.06448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06448]] Principle-Evolvable Scientific Discovery via Uncertainty Minimization(https://arxiv.org/abs/2602.06448)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM)-based scientific agents have accelerated scientific discovery, yet they often suffer from significant inefficiencies due to adherence to fixed initial priors. Existing approaches predominantly operate within a static hypothesis space, which restricts the discovery of novel phenomena, resulting in computational waste when baseline theories fail. To address this, we propose shifting the focus from searching hypotheses to evolving the underlying scientific principles. We present PiEvo, a principle-evolvable framework that treats scientific discovery as Bayesian optimization over an expanding principle space. By integrating Information-Directed Hypothesis Selection via Gaussian Process and an anomaly-driven augmentation mechanism, PiEvo enables agents to autonomously refine their theoretical worldview. Evaluation across four benchmarks demonstrates that PiEvo (1) achieves an average solution quality of up to 90.81%~93.15%, representing a 29.7%~31.1% improvement over the state-of-the-art, (2) attains an 83.3% speedup in convergence step via significantly reduced sample complexity by optimizing the compact principle space, and (3) maintains robust performance across diverse scientific domains and LLM backbones.</li>
</ul>

<h3>Title: Exploring Specular Reflection Inconsistency for Generalizable Face Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Hongyan Fei, Zexi Jia, Chuanwei Huang, Jinchao Zhang, Jie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06452">https://arxiv.org/abs/2602.06452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06452">https://arxiv.org/pdf/2602.06452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06452]] Exploring Specular Reflection Inconsistency for Generalizable Face Forgery Detection(https://arxiv.org/abs/2602.06452)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Detecting deepfakes has become increasingly challenging as forgery faces synthesized by AI-generated methods, particularly diffusion models, achieve unprecedented quality and resolution. Existing forgery detection approaches relying on spatial and frequency features demonstrate limited efficacy against high-quality, entirely synthesized forgeries. In this paper, we propose a novel detection method grounded in the observation that facial attributes governed by complex physical laws and multiple parameters are inherently difficult to replicate. Specifically, we focus on illumination, particularly the specular reflection component in the Phong illumination model, which poses the greatest replication challenge due to its parametric complexity and nonlinear formulation. We introduce a fast and accurate face texture estimation method based on Retinex theory to enable precise specular reflection separation. Furthermore, drawing from the mathematical formulation of specular reflection, we posit that forgery evidence manifests not only in the specular reflection itself but also in its relationship with corresponding face texture and direct light. To address this issue, we design the Specular-Reflection-Inconsistency-Network (SRI-Net), incorporating a two-stage cross-attention mechanism to capture these correlations and integrate specular reflection related features with image features for robust forgery detection. Experimental results demonstrate that our method achieves superior performance on both traditional deepfake datasets and generative deepfake datasets, particularly those containing diffusion-generated forgery faces.</li>
</ul>

<h3>Title: Diffusion-State Policy Optimization for Masked Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Daisuke Oba, Hiroki Furuta, Naoaki Okazaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06462">https://arxiv.org/abs/2602.06462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06462">https://arxiv.org/pdf/2602.06462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06462]] Diffusion-State Policy Optimization for Masked Diffusion Language Models(https://arxiv.org/abs/2602.06462)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at this https URL .</li>
</ul>

<h3>Title: DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation</h3>
<ul>
<li><strong>Authors: </strong>Lulu Chen, Yijiang Hu, Yuanqing Liu, Yulong Li, Yue Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06494">https://arxiv.org/abs/2602.06494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06494">https://arxiv.org/pdf/2602.06494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06494]] DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation(https://arxiv.org/abs/2602.06494)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In modern interior design, the generation of personalized spaces frequently necessitates a delicate balance between rigid architectural structural constraints and specific stylistic preferences. However, existing multi-condition generative frameworks often struggle to harmonize these inputs, leading to "condition conflicts" where stylistic attributes inadvertently compromise the geometric precision of the layout. To address this challenge, we present DreamHome-Pano, a controllable panoramic generation framework designed for high-fidelity interior synthesis. Our approach introduces a Prompt-LLM that serves as a semantic bridge, effectively translating layout constraints and style references into professional descriptive prompts to achieve precise cross-modal alignment. To safeguard architectural integrity during the generative process, we develop a Conflict-Free Control architecture that incorporates structural-aware geometric priors and a multi-condition decoupling strategy, effectively suppressing stylistic interference from eroding the spatial layout. Furthermore, we establish a comprehensive panoramic interior benchmark alongside a multi-stage training pipeline, encompassing progressive Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Experimental results demonstrate that DreamHome-Pano achieves a superior balance between aesthetic quality and structural consistency, offering a robust and professional-grade solution for panoramic interior visualization.</li>
</ul>

<h3>Title: AlertBERT: A noise-robust alert grouping framework for simultaneous cyber attacks</h3>
<ul>
<li><strong>Authors: </strong>Lukas Karner, Max Landauer, Markus Wurzenberger, Florian Skopik</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06534">https://arxiv.org/abs/2602.06534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06534">https://arxiv.org/pdf/2602.06534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06534]] AlertBERT: A noise-robust alert grouping framework for simultaneous cyber attacks(https://arxiv.org/abs/2602.06534)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Automated detection of cyber attacks is a critical capability to counteract the growing volume and sophistication of cyber attacks. However, the high numbers of security alerts issued by intrusion detection systems lead to alert fatigue among analysts working in security operations centres (SOC), which in turn causes slow reaction time and incorrect decision making. Alert grouping, which refers to clustering of security alerts according to their underlying causes, can significantly reduce the number of distinct items analysts have to consider. Unfortunately, conventional time-based alert grouping solutions are unsuitable for large scale computer networks characterised by high levels of false positive alerts and simultaneously occurring attacks. To address these limitations, we propose AlertBERT, a self-supervised framework designed to group alerts from isolated or concurrent attacks in noisy environments. Thereby, our open-source implementation of AlertBERT leverages masked-language-models and density-based clustering to support both real-time or forensic operation. To evaluate our framework, we further introduce a novel data augmentation method that enables flexible control over noise levels and simulates concurrent attack occurrences. Based on the data sets generated through this method, we demonstrate that AlertBERT consistently outperforms conventional time-based grouping techniques, achieving superior accuracy in identifying correct alert groups.</li>
</ul>

<h3>Title: Live Knowledge Tracing: Real-Time Adaptation using Tabular Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Mounir Lbath, Alexandre Paresy, Abdelkayoum Kaddouri, Alan AndrÃ©, Alexandre Ittah, Jill-JÃªnn Vie (SODA)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06542">https://arxiv.org/abs/2602.06542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06542">https://arxiv.org/pdf/2602.06542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06542]] Live Knowledge Tracing: Real-Time Adaptation using Tabular Foundation Models(https://arxiv.org/abs/2602.06542)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Deep knowledge tracing models have achieved significant breakthroughs in modeling student learning trajectories. However, these architectures require substantial training time and are prone to overfitting on datasets with short sequences. In this paper, we explore a new paradigm for knowledge tracing by leveraging tabular foundation models (TFMs). Unlike traditional methods that require offline training on a fixed training set, our approach performs real-time ''live'' knowledge tracing in an online way. The core of our method lies in a two-way attention mechanism: while attention knowledge tracing models only attend across earlier time steps, TFMs simultaneously attend across both time steps and interactions of other students in the training set. They align testing sequences with relevant training sequences at inference time, therefore skipping the training step entirely. We demonstrate, using several datasets of increasing size, that our method achieves competitive predictive performance with up to 273x speedups, in a setting where more student interactions are observed over time.</li>
</ul>

<h3>Title: Refining the Information Bottleneck via Adversarial Information Separation</h3>
<ul>
<li><strong>Authors: </strong>Shuai Ning, Zhenpeng Wang, Lin Wang, Bing Chen, Shuangrong Liu, Xu Wu, Jin Zhou, Bo Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06549">https://arxiv.org/abs/2602.06549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06549">https://arxiv.org/pdf/2602.06549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06549]] Refining the Information Bottleneck via Adversarial Information Separation(https://arxiv.org/abs/2602.06549)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Generalizing from limited data is particularly critical for models in domains such as material science, where task-relevant features in experimental datasets are often heavily confounded by measurement noise and experimental artifacts. Standard regularization techniques fail to precisely separate meaningful features from noise, while existing adversarial adaptation methods are limited by their reliance on explicit separation labels. To address this challenge, we propose the Adversarial Information Separation Framework (AdverISF), which isolates task-relevant features from noise without requiring explicit supervision. AdverISF introduces a self-supervised adversarial mechanism to enforce statistical independence between task-relevant features and noise representations. It further employs a multi-layer separation architecture that progressively recycles noise information across feature hierarchies to recover features inadvertently discarded as noise, thereby enabling finer-grained feature extraction. Extensive experiments demonstrate that AdverISF outperforms state-of-the-art methods in data-scarce scenarios. In addition, evaluations on real-world material design tasks show that it achieves superior generalization performance.</li>
</ul>

<h3>Title: Transformer-based Parameter Fitting of Models derived from Bloch-McConnell Equations for CEST MRI Analysis</h3>
<ul>
<li><strong>Authors: </strong>Christof Duhme, Chris Lippe, Verena Hoerr, Xiaoyi Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06574">https://arxiv.org/abs/2602.06574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06574">https://arxiv.org/pdf/2602.06574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06574]] Transformer-based Parameter Fitting of Models derived from Bloch-McConnell Equations for CEST MRI Analysis(https://arxiv.org/abs/2602.06574)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Chemical exchange saturation transfer (CEST) MRI is a non-invasive imaging modality for detecting metabolites. It offers higher resolution and sensitivity compared to conventional magnetic resonance spectroscopy (MRS). However, quantification of CEST data is challenging because the measured signal results from a complex interplay of many physiological variables. Here, we introduce a transformer-based neural network to fit parameters such as metabolite concentrations, exchange and relaxation rates of a physical model derived from Bloch-McConnell equations to in-vitro CEST spectra. We show that our self-supervised trained neural network clearly outperforms the solution of classical gradient-based solver.</li>
</ul>

<h3>Title: Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Deqian Kong, Minglu Zhao, Aoyang Qin, Bo Pang, Chenxin Tao, David Hartmann, Edouardo Honig, Dehong Xu, Amit Kumar, Matt Sarte, Chuan Li, Jianwen Xie, Ying Nian Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06584">https://arxiv.org/abs/2602.06584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06584">https://arxiv.org/pdf/2602.06584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06584]] Inference-Time Rethinking with Latent Thought Vectors for Math Reasoning(https://arxiv.org/abs/2602.06584)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Standard chain-of-thought reasoning generates a solution in a single forward pass, committing irrevocably to each token and lacking a mechanism to recover from early errors. We introduce Inference-Time Rethinking, a generative framework that enables iterative self-correction by decoupling declarative latent thought vectors from procedural generation. We factorize reasoning into a continuous latent thought vector (what to reason about) and a decoder that verbalizes the trace conditioned on this vector (how to reason). Beyond serving as a declarative buffer, latent thought vectors compress the reasoning structure into a continuous representation that abstracts away surface-level token variability, making gradient-based optimization over reasoning strategies well-posed. Our prior model maps unstructured noise to a learned manifold of valid reasoning patterns, and at test time we employ a Gibbs-style procedure that alternates between generating a candidate trace and optimizing the latent vector to better explain that trace, effectively navigating the latent manifold to refine the reasoning strategy. Training a 0.2B-parameter model from scratch on GSM8K, our method with 30 rethinking iterations surpasses baselines with 10 to 15 times more parameters, including a 3B counterpart. This result demonstrates that effective mathematical reasoning can emerge from sophisticated inference-time computation rather than solely from massive parameter counts.</li>
</ul>

<h3>Title: Target noise: A pre-training based neural network initialization for efficient high resolution learning</h3>
<ul>
<li><strong>Authors: </strong>Shaowen Wang, Tariq Alkhalifah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06585">https://arxiv.org/abs/2602.06585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06585">https://arxiv.org/pdf/2602.06585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06585]] Target noise: A pre-training based neural network initialization for efficient high resolution learning(https://arxiv.org/abs/2602.06585)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Weight initialization plays a crucial role in the optimization behavior and convergence efficiency of neural networks. Most existing initialization methods, such as Xavier and Kaiming initializations, rely on random sampling and do not exploit information from the optimization process itself. We propose a simple, yet effective, initialization strategy based on self-supervised pre-training using random noise as the target. Instead of directly training the network from random weights, we first pre-train it to fit random noise, which leads to a structured and non-random parameter configuration. We show that this noise-driven pre-training significantly improves convergence speed in subsequent tasks, without requiring additional data or changes to the network architecture. The proposed method is particularly effective for implicit neural representations (INRs) and Deep Image Prior (DIP)-style networks, which are known to exhibit a strong low-frequency bias during optimization. After noise-based pre-training, the network is able to capture high-frequency components much earlier in training, leading to faster and more stable convergence. Although random noise contains no semantic information, it serves as an effective self-supervised signal (considering its white spectrum nature) for shaping the initialization of neural networks. Overall, this work demonstrates that noise-based pre-training offers a lightweight and general alternative to traditional random initialization, enabling more efficient optimization of deep neural networks.</li>
</ul>

<h3>Title: DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters</h3>
<ul>
<li><strong>Authors: </strong>Haoran Zhang, Haixuan Liu, Yong Liu, Yunzhong Qiu, Yuxuan Wang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06597">https://arxiv.org/abs/2602.06597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06597">https://arxiv.org/pdf/2602.06597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06597]] DiTS: Multimodal Diffusion Transformers Are Time Series Forecasters(https://arxiv.org/abs/2602.06597)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>While generative modeling on time series facilitates more capable and flexible probabilistic forecasting, existing generative time series models do not address the multi-dimensional properties of time series data well. The prevalent architecture of Diffusion Transformers (DiT), which relies on simplistic conditioning controls and a single-stream Transformer backbone, tends to underutilize cross-variate dependencies in covariate-aware forecasting. Inspired by Multimodal Diffusion Transformers that integrate textual guidance into video generation, we propose Diffusion Transformers for Time Series (DiTS), a general-purpose architecture that frames endogenous and exogenous variates as distinct modalities. To better capture both inter-variate and intra-variate dependencies, we design a dual-stream Transformer block tailored for time-series data, comprising a Time Attention module for autoregressive modeling along the temporal dimension and a Variate Attention module for cross-variate modeling. Unlike the common approach for images, which flattens 2D token grids into 1D sequences, our design leverages the low-rank property inherent in multivariate dependencies, thereby reducing computational costs. Experiments show that DiTS achieves state-of-the-art performance across benchmarks, regardless of the presence of future exogenous variate observations, demonstrating unique generative forecasting strengths over traditional deterministic deep forecasting models.</li>
</ul>

<h3>Title: Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention</h3>
<ul>
<li><strong>Authors: </strong>Himanshu Singh, Ziwei Xu, A. V. Subramanyam, Mohan Kankanhalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06623">https://arxiv.org/abs/2602.06623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06623">https://arxiv.org/pdf/2602.06623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06623]] Do Prompts Guarantee Safety? Mitigating Toxicity from LLM Generations through Subspace Intervention(https://arxiv.org/abs/2602.06623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are powerful text generators, yet they can produce toxic or harmful content even when given seemingly harmless prompts. This presents a serious safety challenge and can cause real-world harm. Toxicity is often subtle and context-dependent, making it difficult to detect at the token level or through coarse sentence-level signals. Moreover, efforts to mitigate toxicity often face a trade-off between safety and the coherence, or fluency of the generated text. In this work, we present a targeted subspace intervention strategy for identifying and suppressing hidden toxic patterns from underlying model representations, while preserving overall ability to generate safe fluent content. On the RealToxicityPrompts, our method achieves strong mitigation performance compared to existing baselines, with minimal impact on inference complexity. Across multiple LLMs, our approach reduces toxicity of state-of-the-art detoxification systems by 8-20%, while maintaining comparable fluency. Through extensive quantitative and qualitative analyses, we show that our approach achieves effective toxicity reduction without impairing generative performance, consistently outperforming existing baselines.</li>
</ul>

<h3>Title: Memory-Conditioned Flow-Matching for Stable Autoregressive PDE Rollouts</h3>
<ul>
<li><strong>Authors: </strong>Victor Armegioiu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06689">https://arxiv.org/abs/2602.06689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06689">https://arxiv.org/pdf/2602.06689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06689]] Memory-Conditioned Flow-Matching for Stable Autoregressive PDE Rollouts(https://arxiv.org/abs/2602.06689)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive generative PDE solvers can be accurate one step ahead yet drift over long rollouts, especially in coarse-to-fine regimes where each step must regenerate unresolved fine scales. This is the regime of diffusion and flow-matching generators: although their internal dynamics are Markovian, rollout stability is governed by per-step \emph{conditional law} errors. Using the Mori--Zwanzig projection formalism, we show that eliminating unresolved variables yields an exact resolved evolution with a Markov term, a memory term, and an orthogonal forcing, exposing a structural limitation of memoryless closures. Motivated by this, we introduce memory-conditioned diffusion/flow-matching with a compact online state injected into denoising via latent features. Via disintegration, memory induces a structured conditional tail prior for unresolved scales and reduces the transport needed to populate missing frequencies. We prove Wasserstein stability of the resulting conditional kernel. We then derive discrete GrÃ¶nwall rollout bounds that separate memory approximation from conditional generation error. Experiments on compressible flows with shocks and multiscale mixing show improved accuracy and markedly more stable long-horizon rollouts, with better fine-scale spectral and statistical fidelity.</li>
</ul>

<h3>Title: SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shentong Mo, Lanqing Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06706">https://arxiv.org/abs/2602.06706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06706">https://arxiv.org/pdf/2602.06706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06706]] SaDiT: Efficient Protein Backbone Design via Latent Structural Tokenization and Diffusion Transformers(https://arxiv.org/abs/2602.06706)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models for de novo protein backbone design have achieved remarkable success in creating novel protein structures. However, these diffusion-based approaches remain computationally intensive and slower than desired for large-scale structural exploration. While recent efforts like Proteina have introduced flow-matching to improve sampling efficiency, the potential of tokenization for structural compression and acceleration remains largely unexplored in the protein domain. In this work, we present SaDiT, a novel framework that accelerates protein backbone generation by integrating SaProt Tokenization with a Diffusion Transformer (DiT) architecture. SaDiT leverages a discrete latent space to represent protein geometry, significantly reducing the complexity of the generation process while maintaining theoretical SE(3) equivalence. To further enhance efficiency, we introduce an IPA Token Cache mechanism that optimizes the Invariant Point Attention (IPA) layers by reusing computed token states during iterative sampling. Experimental results demonstrate that SaDiT outperforms state-of-the-art models, including RFDiffusion and Proteina, in both computational speed and structural viability. We evaluate our model across unconditional backbone generation and fold-class conditional generation tasks, where SaDiT shows superior ability to capture complex topological features with high designability.</li>
</ul>

<h3>Title: Disentanglement by means of action-induced representations</h3>
<ul>
<li><strong>Authors: </strong>Gorka MuÃ±oz-Gil, Hendrik Poulsen Nautrup, Arunava Majumder, Paulin de Schoulepnikoff, Florian FÃ¼rrutter, Marius Krumm, Hans J. Briegel</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06741">https://arxiv.org/abs/2602.06741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06741">https://arxiv.org/pdf/2602.06741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06741]] Disentanglement by means of action-induced representations(https://arxiv.org/abs/2602.06741)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning interpretable representations with variational autoencoders (VAEs) is a major goal of representation learning. The main challenge lies in obtaining disentangled representations, where each latent dimension corresponds to a distinct generative factor. This difficulty is fundamentally tied to the inability to perform nonlinear independent component analysis. Here, we introduce the framework of action-induced representations (AIRs) which models representations of physical systems given experiments (or actions) that can be performed on them. We show that, in this framework, we can provably disentangle degrees of freedom w.r.t. their action dependence. We further introduce a variational AIR architecture (VAIR) that can extract AIRs and therefore achieve provable disentanglement where standard VAEs fail. Beyond state representation, VAIR also captures the action dependence of the underlying generative factors, directly linking experiments to the degrees of freedom they influence.</li>
</ul>

<h3>Title: Gold Exploration using Representations from a Multispectral Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Argyro Tsandalidou, Konstantinos Dogeas, Eleftheria Tetoula Tsonga, Elisavet Parselia, Georgios Tsimiklis, George Arvanitakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06748">https://arxiv.org/abs/2602.06748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06748">https://arxiv.org/pdf/2602.06748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06748]] Gold Exploration using Representations from a Multispectral Autoencoder(https://arxiv.org/abs/2602.06748)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 imagery to identify gold-bearing regions from space. An autoencoder foundation model, called Isometric, which is pretrained on the large-scale FalconSpace-S2 v1.0 dataset, produces information-dense spectral-spatial representations that serve as inputs to a lightweight XGBoost classifier. We compare this representation-based approach with a raw spectral input baseline using a dataset of 63 Sentinel-2 images from known gold and non-gold locations. The proposed method improves patch-level accuracy from 0.51 to 0.68 and image-level accuracy from 0.55 to 0.73, demonstrating that generative embeddings capture transferable mineralogical patterns even with limited labeled data. These results highlight the potential of foundation-model representations to make mineral exploration more efficient, scalable, and globally applicable.</li>
</ul>

<h3>Title: R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging</h3>
<ul>
<li><strong>Authors: </strong>Yanlin Lai, Mitt Huang, Hangyu Guo, Xiangfeng Wang, Haodong Li, Shaoxiong Zhan, Liang Zhao, Chengyuan Yao, Yinmin Zhang, Qi Han, Chun Yuan, Zheng Ge, Xiangyu Zhang, Daxin Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06763">https://arxiv.org/abs/2602.06763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06763">https://arxiv.org/pdf/2602.06763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06763]] R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging(https://arxiv.org/abs/2602.06763)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.</li>
</ul>

<h3>Title: AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Fengpeng Li, Kemou Li, Qizhou Wang, Bo Han, Jiantao Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06771">https://arxiv.org/abs/2602.06771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06771">https://arxiv.org/pdf/2602.06771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06771]] AEGIS: Adversarial Target-Guided Retention-Data-Free Robust Concept Erasure from Diffusion Models(https://arxiv.org/abs/2602.06771)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Concept erasure helps stop diffusion models (DMs) from generating harmful content; but current methods face robustness retention trade off. Robustness means the model fine-tuned by concept erasure methods resists reactivation of erased concepts, even under semantically related prompts. Retention means unrelated concepts are preserved so the model's overall utility stays intact. Both are critical for concept erasure in practice, yet addressing them simultaneously is challenging, as existing works typically improve one factor while sacrificing the other. Prior work typically strengthens one while degrading the other, e.g., mapping a single erased prompt to a fixed safe target leaves class level remnants exploitable by prompt attacks, whereas retention-oriented schemes underperform against adaptive adversaries. This paper introduces Adversarial Erasure with Gradient Informed Synergy (AEGIS), a retention-data-free framework that advances both robustness and retention.</li>
</ul>

<h3>Title: Calibrating Generative AI to Produce Realistic Essays for Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Edward W. Wolfe, Justin O. Barber</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06772">https://arxiv.org/abs/2602.06772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06772">https://arxiv.org/pdf/2602.06772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06772]] Calibrating Generative AI to Produce Realistic Essays for Data Augmentation(https://arxiv.org/abs/2602.06772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data augmentation can mitigate limited training data in machine-learning automated scoring engines for constructed response items. This study seeks to determine how well three approaches to large language model prompting produce essays that preserve the writing quality of the original essays and produce realistic text for augmenting ASE training datasets. We created simulated versions of student essays, and human raters assigned scores to them and rated the realism of the generated text. The results of the study indicate that the predict next prompting strategy produces the highest level of agreement between human raters regarding simulated essay scores, predict next and sentence strategies best preserve the rated quality of the original essay in the simulated essays, and predict next and 25 examples strategies produce the most realistic text as judged by human raters.</li>
</ul>

<h3>Title: Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs</h3>
<ul>
<li><strong>Authors: </strong>Yassine Chagna, Antal Goldschmidt</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06777">https://arxiv.org/abs/2602.06777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06777">https://arxiv.org/pdf/2602.06777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06777]] Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs(https://arxiv.org/abs/2602.06777)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.</li>
</ul>

<h3>Title: FlowDA: Accurate, Low-Latency Weather Data Assimilation via Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Ran Cheng, Lailai Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06800">https://arxiv.org/abs/2602.06800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06800">https://arxiv.org/pdf/2602.06800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06800]] FlowDA: Accurate, Low-Latency Weather Data Assimilation via Flow Matching(https://arxiv.org/abs/2602.06800)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Data assimilation (DA) is a fundamental component of modern weather prediction, yet it remains a major computational bottleneck in machine learning (ML)-based forecasting pipelines due to reliance on traditional variational methods. Recent generative ML-based DA methods offer a promising alternative but typically require many sampling steps and suffer from error accumulation under long-horizon auto-regressive rollouts with cycling assimilation. We propose FlowDA, a low-latency weather-scale generative DA framework based on flow matching. FlowDA conditions on observations through a SetConv-based embedding and fine-tunes the Aurora foundation model to deliver accurate, efficient, and robust analyses. Experiments across observation rates decreasing from $3.9\%$ to $0.1\%$ demonstrate superior performance of FlowDA over strong baselines with similar tunable-parameter size. FlowDA further shows robustness to observational noise and stable performance in long-horizon auto-regressive cycling DA. Overall, FlowDA points to an efficient and scalable direction for data-driven DA.</li>
</ul>

<h3>Title: RAIGen: Rare Attribute Identification in Text-to-Image Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Silpa Vadakkeeveetil Sreelatha, Dan Wang, Serge Belongie, Muhammad Awais, Anjan Dutta</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06806">https://arxiv.org/abs/2602.06806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06806">https://arxiv.org/pdf/2602.06806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06806]] RAIGen: Rare Attribute Identification in Text-to-Image Generative Models(https://arxiv.org/abs/2602.06806)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.</li>
</ul>

<h3>Title: Calibrating Tabular Anomaly Detection via Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Hangting Ye, He Zhao.Wei Fan, Xiaozhuang Song, Dandan Guo, Yi Chang, Hongyuan Zha</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06810">https://arxiv.org/abs/2602.06810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06810">https://arxiv.org/pdf/2602.06810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06810]] Calibrating Tabular Anomaly Detection via Optimal Transport(https://arxiv.org/abs/2602.06810)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Tabular anomaly detection (TAD) remains challenging due to the heterogeneity of tabular data: features lack natural relationships, vary widely in distribution and scale, and exhibit diverse types. Consequently, each TAD method makes implicit assumptions about anomaly patterns that work well on some datasets but fail on others, and no method consistently outperforms across diverse scenarios. We present CTAD (Calibrating Tabular Anomaly Detection), a model-agnostic post-processing framework that enhances any existing TAD detector through sample-specific calibration. Our approach characterizes normal data via two complementary distributions, i.e., an empirical distribution from random sampling and a structural distribution from K-means centroids, and measures how adding a test sample disrupts their compatibility using Optimal Transport (OT) distance. Normal samples maintain low disruption while anomalies cause high disruption, providing a calibration signal to amplify detection. We prove that OT distance has a lower bound proportional to the test sample's distance from centroids, and establish that anomalies systematically receive higher calibration scores than normals in expectation, explaining why the method generalizes across datasets. Extensive experiments on 34 diverse tabular datasets with 7 representative detectors spanning all major TAD categories (density estimation, classification, reconstruction, and isolation-based methods) demonstrate that CTAD consistently improves performance with statistical significance. Remarkably, CTAD enhances even state-of-the-art deep learning methods and shows robust performance across diverse hyperparameter settings, requiring no additional tuning for practical deployment.</li>
</ul>

<h3>Title: AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yuming Li, Qingyu Li, Chengyu Bai, Xiangyang Luo, Zeyue Xue, Wenyu Qin, Meng Wang, Yikai Wang, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06825">https://arxiv.org/abs/2602.06825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06825">https://arxiv.org/pdf/2602.06825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06825]] AEGPO: Adaptive Entropy-Guided Policy Optimization for Diffusion Models(https://arxiv.org/abs/2602.06825)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement learning from human feedback (RLHF) shows promise for aligning diffusion and flow models, yet policy optimization methods such as GRPO suffer from inefficient and static sampling strategies. These methods treat all prompts and denoising steps uniformly, ignoring substantial variations in sample learning value as well as the dynamic nature of critical exploration moments. To address this issue, we conduct a detailed analysis of the internal attention dynamics during GRPO training and uncover a key insight: attention entropy can serve as a powerful dual-signal proxy. First, across different samples, the relative change in attention entropy ({\Delta}Entropy), which reflects the divergence between the current policy and the base policy, acts as a robust indicator of sample learning value. Second, during the denoising process, the peaks of absolute attention entropy (Entropy(t)), which quantify attention dispersion, effectively identify critical timesteps where high-value exploration occurs. Building on this observation, we propose Adaptive Entropy-Guided Policy Optimization (AEGPO), a novel dual-signal, dual-level adaptive optimization strategy. At the global level, AEGPO uses {\Delta}Entropy to dynamically allocate rollout budgets, prioritizing prompts with higher learning value. At the local level, it exploits the peaks of Entropy(t) to guide exploration selectively at critical high-dispersion timesteps rather than uniformly across all denoising steps. By focusing computation on the most informative samples and the most critical moments, AEGPO enables more efficient and effective policy optimization. Experiments on text-to-image generation tasks demonstrate that AEGPO significantly accelerates convergence and achieves superior alignment performance compared to standard GRPO variants.</li>
</ul>

<h3>Title: Improved Sampling Schedules for Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Alberto Foresti, Mustapha Bounoua, Giulio Franzese, Luca Ambrogioni, Pietro Michiardi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06849">https://arxiv.org/abs/2602.06849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06849">https://arxiv.org/pdf/2602.06849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06849]] Improved Sampling Schedules for Discrete Diffusion Models(https://arxiv.org/abs/2602.06849)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have emerged as a powerful paradigm for generative modeling on sequence data; however, the information-theoretic principles governing their reverse processes remain significantly less understood than those of their continuous counterparts. In this work, we bridge this gap by analyzing the reverse process dynamics through the lens of thermodynamic entropy production. We propose the entropy production rate as a rigorous proxy for quantifying information generation, deriving as a byproduct a bound on the Wasserstein distance between intermediate states and the data distribution. Leveraging these insights, we introduce two novel sampling schedules that are uniformly spaced with respect to their corresponding physics-inspired metrics: the Entropic Discrete Schedule (EDS), which is defined by maintaining a constant rate of information gain, and the Wasserstein Discrete Schedule (WDS), which is defined by taking equal steps in terms of the Wasserstein distance. We empirically demonstrate that our proposed schedules significantly outperform state-of-the-art strategies across diverse application domains, including synthetic data, music notation, vision and language modeling, consistently achieving superior performance at a lower computational budget.</li>
</ul>

<h3>Title: Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping</h3>
<ul>
<li><strong>Authors: </strong>Chao Zhou, Tianyi Wei, Yiling Chen, Wenbo Zhou, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06850">https://arxiv.org/abs/2602.06850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06850">https://arxiv.org/pdf/2602.06850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06850]] Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping(https://arxiv.org/abs/2602.06850)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\times$ inference speedup and a 5.1$\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.</li>
</ul>

<h3>Title: Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhao, Qingyun Sun, Jiayi Luo, Xingcheng Fu, Jianxin Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06859">https://arxiv.org/abs/2602.06859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06859">https://arxiv.org/pdf/2602.06859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06859]] Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts(https://arxiv.org/abs/2602.06859)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Graph Anomaly Detection (GAD) aims to identify irregular patterns in graph data, and recent works have explored zero-shot generalist GAD to enable generalization to unseen graph datasets. However, existing zero-shot GAD methods largely ignore intrinsic geometric differences across diverse anomaly patterns, substantially limiting their cross-domain generalization. In this work, we reveal that anomaly detectability is highly dependent on the underlying geometric properties and that embedding graphs from different domains into a single static curvature space can distort the structural signatures of anomalies. To address the challenge that a single curvature space cannot capture geometry-dependent graph anomaly patterns, we propose GAD-MoRE, a novel framework for zero-shot Generalizable Graph Anomaly Detection with a Mixture of Riemannian Experts architecture. Specifically, to ensure that each anomaly pattern is modeled in the Riemannian space where it is most detectable, GAD-MoRE employs a set of specialized Riemannian expert networks, each operating in a distinct curvature space. To align raw node features with curvature-specific anomaly characteristics, we introduce an anomaly-aware multi-curvature feature alignment module that projects inputs into parallel Riemannian spaces, enabling the capture of diverse geometric characteristics. Finally, to facilitate better generalization beyond seen patterns, we design a memory-based dynamic router that adaptively assigns each input to the most compatible expert based on historical reconstruction performance on similar anomalies. Extensive experiments in the zero-shot setting demonstrate that GAD-MoRE significantly outperforms state-of-the-art generalist GAD baselines, and even surpasses strong competitors that are few-shot fine-tuned with labeled data from the target domain.</li>
</ul>

<h3>Title: RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Salehi, Mehdi Noroozi, Luca Morreale, Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Ramos, Abhinav Mehrotra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06871">https://arxiv.org/abs/2602.06871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06871">https://arxiv.org/pdf/2602.06871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06871]] RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing(https://arxiv.org/abs/2602.06871)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: this https URL</li>
</ul>

<h3>Title: NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices</h3>
<ul>
<li><strong>Authors: </strong>Ruchika Chavhan, Malcolm Chadwick, Alberto Gil Couto Pimentel Ramos, Luca Morreale, Mehdi Noroozi, Abhinav Mehrotra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06879">https://arxiv.org/abs/2602.06879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06879">https://arxiv.org/pdf/2602.06879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06879]] NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices(https://arxiv.org/abs/2602.06879)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.</li>
</ul>

<h3>Title: Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Yao, Yuxuan Chen, Hui Li, Kaihui Cheng, Qipeng Guo, Yuwei Sun, Zilong Dong, Jingdong Wang, Siyu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06886">https://arxiv.org/abs/2602.06886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06886">https://arxiv.org/pdf/2602.06886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06886]] Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers(https://arxiv.org/abs/2602.06886)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multimodal Diffusion Transformers (MMDiTs) for text-to-image generation maintain separate text and image branches, with bidirectional information flow between text tokens and visual latents throughout denoising. In this setting, we observe a prompt forgetting phenomenon: the semantics of the prompt representation in the text branch is progressively forgotten as depth increases. We further verify this effect on three representative MMDiTs--SD3, SD3.5, and FLUX.1 by probing linguistic attributes of the representations over the layers in the text branch. Motivated by these findings, we introduce a training-free approach, prompt reinjection, which reinjects prompt representations from early layers into later layers to alleviate this forgetting. Experiments on GenEval, DPG, and T2I-CompBench++ show consistent gains in instruction-following capability, along with improvements on metrics capturing preference, aesthetics, and overall text--image generation quality.</li>
</ul>

<h3>Title: Revisiting the Generic Transformer: Deconstructing a Strong Baseline for Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yunshi Wen, Wesley M. Gifford, Chandra Reddy, Lam M. Nguyen, Jayant Kalagnanam, Anak Agung Julius</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06909">https://arxiv.org/abs/2602.06909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06909">https://arxiv.org/pdf/2602.06909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06909]] Revisiting the Generic Transformer: Deconstructing a Strong Baseline for Time Series Foundation Models(https://arxiv.org/abs/2602.06909)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The recent surge in Time Series Foundation Models has rapidly advanced the field, yet the heterogeneous training setups across studies make it difficult to attribute improvements to architectural innovations versus data engineering. In this work, we investigate the potential of a standard patch Transformer, demonstrating that this generic architecture achieves state-of-the-art zero-shot forecasting performance using a straightforward training protocol. We conduct a comprehensive ablation study that covers model scaling, data composition, and training techniques to isolate the essential ingredients for high performance. Our findings identify the key drivers of performance, while confirming that the generic architecture itself demonstrates excellent scalability. By strictly controlling these variables, we provide comprehensive empirical results on model scaling across multiple dimensions. We release our open-source model and detailed findings to establish a transparent, reproducible baseline for future research.</li>
</ul>

<h3>Title: PANC: Prior-Aware Normalized Cut for Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Juan GutiÃ©rrez, Victor GutiÃ©rrez-Garcia, JosÃ© Luis Blanco-Murillo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06912">https://arxiv.org/abs/2602.06912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06912">https://arxiv.org/pdf/2602.06912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06912]] PANC: Prior-Aware Normalized Cut for Object Segmentation(https://arxiv.org/abs/2602.06912)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics. We propose PANC, a weakly supervised spectral segmentation framework that uses a minimal set of annotated visual tokens to produce stable, controllable, and reproducible object masks. From the TokenCut approach, we augment the token-token affinity graph with a handful of priors coupled to anchor nodes. By manipulating the graph topology, we bias the spectral eigenspace toward partitions that are consistent with the annotations. Our approach preserves the global grouping enforced by dense self-supervised visual features, trading annotated tokens for significant gains in reproducibility, user control, and segmentation quality. Using 5 to 30 annotations per dataset, our training-free method achieves state-of-the-art performance among weakly and unsupervised approaches on standard benchmarks (e.g., DUTS-TE, ECSSD, MS COCO). Contrarily, it excels in domains where dense labels are costly or intra-class differences are subtle. We report strong and reliable results on homogeneous, fine-grained, and texture-limited domains, achieving 96.8% (+14.43% over SotA), 78.0% (+0.2%), and 78.8% (+0.37%) average mean intersection-over-union (mIoU) on CrackForest (CFD), CUB-200-2011, and HAM10000 datasets, respectively. For multi-object benchmarks, the framework showcases explicit, user-controllable semantic segmentation.</li>
</ul>

<h3>Title: Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Samir Abdaljalil, Parichit Sharma, Erchin Serpedin, Hasan Kurban</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06920">https://arxiv.org/abs/2602.06920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06920">https://arxiv.org/pdf/2602.06920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06920]] Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs(https://arxiv.org/abs/2602.06920)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{this https URL}.</li>
</ul>

<h3>Title: Continuous-time reinforcement learning: ellipticity enables model-free value function approximation</h3>
<ul>
<li><strong>Authors: </strong>Wenlong Mou</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06930">https://arxiv.org/abs/2602.06930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06930">https://arxiv.org/pdf/2602.06930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06930]] Continuous-time reinforcement learning: ellipticity enables model-free value function approximation(https://arxiv.org/abs/2602.06930)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study off-policy reinforcement learning for controlling continuous-time Markov diffusion processes with discrete-time observations and actions. We consider model-free algorithms with function approximation that learn value and advantage functions directly from data, without unrealistic structural assumptions on the dynamics. Leveraging the ellipticity of the diffusions, we establish a new class of Hilbert-space positive definiteness and boundedness properties for the Bellman operators. Based on these properties, we propose the Sobolev-prox fitted $q$-learning algorithm, which learns value and advantage functions by iteratively solving least-squares regression problems. We derive oracle inequalities for the estimation error, governed by (i) the best approximation error of the function classes, (ii) their localized complexity, (iii) exponentially decaying optimization error, and (iv) numerical discretization error. These results identify ellipticity as a key structural property that renders reinforcement learning with function approximation for Markov diffusions no harder than supervised learning.</li>
</ul>

<h3>Title: Reliable Mislabel Detection for Video Capsule Endoscopy Data</h3>
<ul>
<li><strong>Authors: </strong>Julia Werner, Julius Oexle, Oliver Bause, Maxime Le Floch, Franz Brinkmann, Hannah Tolle, Jochen Hampe, Oliver Bringmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06938">https://arxiv.org/abs/2602.06938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06938">https://arxiv.org/pdf/2602.06938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06938]] Reliable Mislabel Detection for Video Capsule Endoscopy Data(https://arxiv.org/abs/2602.06938)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.</li>
</ul>

<h3>Title: DAWN: Dependency-Aware Fast Inference for Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lizhuo Luo, Zhuoran Shi, Jiajun Luo, Zhi Wang, Shen Ren, Wenya Wang, Tianwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06953">https://arxiv.org/abs/2602.06953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06953">https://arxiv.org/pdf/2602.06953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06953]] DAWN: Dependency-Aware Fast Inference for Diffusion LLMs(https://arxiv.org/abs/2602.06953)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at this https URL.</li>
</ul>

<h3>Title: Learning a Generative Meta-Model of LLM Activations</h3>
<ul>
<li><strong>Authors: </strong>Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06964">https://arxiv.org/abs/2602.06964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06964">https://arxiv.org/pdf/2602.06964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06964]] Learning a Generative Meta-Model of LLM Activations(https://arxiv.org/abs/2602.06964)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating "meta-models" that learn the distribution of a network's internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model's learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model's neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: this https URL.</li>
</ul>

<h3>Title: MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Ankan Deria, Komal Kumar, Adinath Madhavrao Dukre, Eran Segal, Salman Khan, Imran Razzak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.06965">https://arxiv.org/abs/2602.06965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.06965">https://arxiv.org/pdf/2602.06965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.06965]] MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images(https://arxiv.org/abs/2602.06965)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
