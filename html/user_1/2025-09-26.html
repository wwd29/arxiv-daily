<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-26</h1>
<h3>Title: Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text</h3>
<ul>
<li><strong>Authors: </strong>Sharanya Parimanoharan, Ruwan D. Nawarathna</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20375">https://arxiv.org/abs/2509.20375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20375">https://arxiv.org/pdf/2509.20375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20375]] Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text(https://arxiv.org/abs/2509.20375)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid adoption of large language models (LLMs) such as ChatGPT has blurred the line between human and AI-generated texts, raising urgent questions about academic integrity, intellectual property, and the spread of misinformation. Thus, reliable AI-text detection is needed for fair assessment to safeguard human authenticity and cultivate trust in digital communication. In this study, we investigate how well current machine learning (ML) approaches can distinguish ChatGPT-3.5-generated texts from human-written texts employing a labeled data set of 250 pairs of abstracts from a wide range of research topics. We test and compare both classical (Logistic Regression armed with classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier, and LSTM-based N-gram models) ML detection techniques. As we aim to assess each model's performance in detecting AI-generated research texts, we also aim to test whether an ensemble of these models can outperform any single detector. Results show DistilBERT achieves the overall best performance, while Logistic Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and BERT-N-gram approaches lag. The max voting ensemble of the three best models fails to surpass DistilBERT itself, highlighting the primacy of a single transformer-based representation over mere model diversity. By comprehensively assessing the strengths and weaknesses of these AI-text detection approaches, this work lays a foundation for more robust transformer frameworks with larger, richer datasets to keep pace with ever-improving generative AI models.</li>
</ul>

<h3>Title: A Theory of Multi-Agent Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Leo Maxime Brunswic, Haozhi Wang, Shuang Luo, Jianye Hao, Amir Rasouli, Yinchuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20408">https://arxiv.org/abs/2509.20408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20408">https://arxiv.org/pdf/2509.20408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20408]] A Theory of Multi-Agent Generative Flow Networks(https://arxiv.org/abs/2509.20408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative flow networks utilize a flow-matching loss to learn a stochastic policy for generating objects from a sequence of actions, such that the probability of generating a pattern can be proportional to the corresponding given reward. However, a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose the theory framework of MA-GFlowNets, which can be applied to multiple agents to generate objects collaboratively through a series of joint actions. We further propose four algorithms: a centralized flow network for centralized training of MA-GFlowNets, an independent flow network for decentralized execution, a joint flow network for achieving centralized training with decentralized execution, and its updated conditional version. Joint Flow training is based on a local-global principle allowing to train a collection of (local) GFN as a unique (global) GFN. This principle provides a loss of reasonable complexity and allows to leverage usual results on GFN to provide theoretical guarantees that the independent policies generate samples with probability proportional to the reward function. Experimental results demonstrate the superiority of the proposed framework compared to reinforcement learning and MCMC-based methods.</li>
</ul>

<h3>Title: Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Tharcisse Ndayipfukamiye, Jianguo Ding, Doreen Sebastian Sarwatt, Adamu Gaston Philipo, Huansheng Ning</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20411">https://arxiv.org/abs/2509.20411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20411">https://arxiv.org/pdf/2509.20411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20411]] Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation(https://arxiv.org/abs/2509.20411)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning-based cybersecurity systems are highly vulnerable to adversarial attacks, while Generative Adversarial Networks (GANs) act as both powerful attack enablers and promising defenses. This survey systematically reviews GAN-based adversarial defenses in cybersecurity (2021--August 31, 2025), consolidating recent progress, identifying gaps, and outlining future directions. Using a PRISMA-compliant systematic literature review protocol, we searched five major digital libraries. From 829 initial records, 185 peer-reviewed studies were retained and synthesized through quantitative trend analysis and thematic taxonomy development. We introduce a four-dimensional taxonomy spanning defensive function, GAN architecture, cybersecurity domain, and adversarial threat model. GANs improve detection accuracy, robustness, and data utility across network intrusion detection, malware analysis, and IoT security. Notable advances include WGAN-GP for stable training, CGANs for targeted synthesis, and hybrid GAN models for improved resilience. Yet, persistent challenges remain such as instability in training, lack of standardized benchmarks, high computational cost, and limited explainability. GAN-based defenses demonstrate strong potential but require advances in stable architectures, benchmarking, transparency, and deployment. We propose a roadmap emphasizing hybrid models, unified evaluation, real-world integration, and defenses against emerging threats such as LLM-driven cyberattacks. This survey establishes the foundation for scalable, trustworthy, and adaptive GAN-powered defenses.</li>
</ul>

<h3>Title: Seedream 4.0: Toward Next-generation Multimodal Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, Xiaowen Jian, Huafeng Kuang, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yanzuo Lu, Zhengxiong Luo, Tongtong Ou, Guang Shi, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Wenxu Wu, Yonghui Wu, Xin Xia, Xuefeng Xiao, Shuang Xu, Xin Yan, Ceyuan Yang, Jianchao Yang, Zhonghua Zhai, Chenlin Zhang, Heng Zhang, Qi Zhang, Xinyu Zhang, Yuwei Zhang, Shijia Zhao, Wenliang Zhao, Wenjia Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20427">https://arxiv.org/abs/2509.20427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20427">https://arxiv.org/pdf/2509.20427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20427]] Seedream 4.0: Toward Next-generation Multimodal Image Generation(https://arxiv.org/abs/2509.20427)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, in-context</a></li>
<li><strong>Abstract: </strong>We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on this https URL.</li>
</ul>

<h3>Title: Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data</h3>
<ul>
<li><strong>Authors: </strong>Simon Baeuerle, Pratik Khanna, Nils Friederich, Angelo Jovin Yamachui Sitcheu, Damir Shakirov, Andreas Steimer, Ralf Mikut</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20479">https://arxiv.org/abs/2509.20479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20479">https://arxiv.org/pdf/2509.20479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20479]] Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data(https://arxiv.org/abs/2509.20479)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation Models (FMs) have shown impressive performance on various text and image processing tasks. They can generalize across domains and datasets in a zero-shot setting. This could make them suitable for automated quality inspection during series manufacturing, where various types of images are being evaluated for many different products. Replacing tedious labeling tasks with a simple text prompt to describe anomalies and utilizing the same models across many products would save significant efforts during model setup and implementation. This is a strong advantage over supervised Artificial Intelligence (AI) models, which are trained for individual applications and require labeled training data. We test multiple recent FMs on both custom real-world industrial image data and public image data. We show that all of those models fail on our real-world data, while the very same models perform well on public benchmark datasets.</li>
</ul>

<h3>Title: A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Oscar Leong, Yann Traonmilin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20511">https://arxiv.org/abs/2509.20511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20511">https://arxiv.org/pdf/2509.20511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20511]] A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm(https://arxiv.org/abs/2509.20511)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recovering high-dimensional signals from corrupted measurements is a central challenge in inverse problems. Recent advances in generative diffusion models have shown remarkable empirical success in providing strong data-driven priors, but rigorous recovery guarantees remain limited. In this work, we develop a theoretical framework for analyzing deterministic diffusion-based algorithms for inverse problems, focusing on a deterministic version of the algorithm proposed by Kadkhodaie \& Simoncelli \cite{kadkhodaie2021stochastic}. First, we show that when the underlying data distribution concentrates on a low-dimensional model set, the associated noise-convolved scores can be interpreted as time-varying projections onto such a set. This leads to interpreting previous algorithms using diffusion priors for inverse problems as generalized projected gradient descent methods with varying projections. When the sensing matrix satisfies a restricted isometry property over the model set, we can derive quantitative convergence rates that depend explicitly on the noise schedule. We apply our framework to two instructive data distributions: uniform distributions over low-dimensional compact, convex sets and low-rank Gaussian mixture models. In the latter setting, we can establish global convergence guarantees despite the nonconvexity of the underlying model set.</li>
</ul>

<h3>Title: PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mingze Yuan, Pengfei Jin, Na Li, Quanzheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20570">https://arxiv.org/abs/2509.20570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20570">https://arxiv.org/pdf/2509.20570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20570]] PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models(https://arxiv.org/abs/2509.20570)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.</li>
</ul>

<h3>Title: Large Pre-Trained Models for Bimanual Manipulation in 3D</h3>
<ul>
<li><strong>Authors: </strong>Hanna Yurchyk, Wei-Di Chang, Gregory Dudek, David Meger</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20579">https://arxiv.org/abs/2509.20579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20579">https://arxiv.org/pdf/2509.20579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20579]] Large Pre-Trained Models for Bimanual Manipulation in 3D(https://arxiv.org/abs/2509.20579)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We investigate the integration of attention maps from a pre-trained Vision Transformer into voxel representations to enhance bimanual robotic manipulation. Specifically, we extract attention maps from DINOv2, a self-supervised ViT model, and interpret them as pixel-level saliency scores over RGB images. These maps are lifted into a 3D voxel grid, resulting in voxel-level semantic cues that are incorporated into a behavior cloning policy. When integrated into a state-of-the-art voxel-based policy, our attention-guided featurization yields an average absolute improvement of 8.2% and a relative gain of 21.9% across all tasks in the RLBench bimanual benchmark.</li>
</ul>

<h3>Title: MMG: Mutual Information Estimation via the MMSE Gap in Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Longxuan Yu, Xing Shi, Xianghao Kong, Tong Jia, Greg Ver Steeg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20609">https://arxiv.org/abs/2509.20609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20609">https://arxiv.org/pdf/2509.20609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20609]] MMG: Mutual Information Estimation via the MMSE Gap in Diffusion(https://arxiv.org/abs/2509.20609)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Mutual information (MI) is one of the most general ways to measure relationships between random variables, but estimating this quantity for complex systems is challenging. Denoising diffusion models have recently set a new bar for density estimation, so it is natural to consider whether these methods could also be used to improve MI estimation. Using the recently introduced information-theoretic formulation of denoising diffusion models, we show the diffusion models can be used in a straightforward way to estimate MI. In particular, the MI corresponds to half the gap in the Minimum Mean Square Error (MMSE) between conditional and unconditional diffusion, integrated over all Signal-to-Noise-Ratios (SNRs) in the noising process. Our approach not only passes self-consistency tests but also outperforms traditional and score-based diffusion MI estimators. Furthermore, our method leverages adaptive importance sampling to achieve scalable MI estimation, while maintaining strong performance even when the MI is high.</li>
</ul>

<h3>Title: FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Amin Karimi Monsefi, Nikhil Bhendawade, Manuel Rafael Ciosici, Dominic Culver, Yizhe Zhang, Irina Belousova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20624">https://arxiv.org/abs/2509.20624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20624">https://arxiv.org/pdf/2509.20624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20624]] FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models(https://arxiv.org/abs/2509.20624)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive language models (ARMs) deliver strong likelihoods, but are inherently serial: they generate one token per forward pass, which limits throughput and inflates latency for long sequences. Diffusion Language Models (DLMs) parallelize across positions and thus appear promising for language generation, yet standard discrete diffusion typically needs hundreds to thousands of model evaluations to reach high quality, trading serial depth for iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A discrete flow-matching model designed for speed without sacrificing quality. The core idea is simple: make the number of sampling steps an explicit parameter and train the model to be consistent across step budgets, so one big move lands where many small moves would. We pair this with a reliable update rule that moves probability in the right direction without overshooting, and with strong teacher guidance distilled from long-run trajectories. Together, these choices make few-step sampling stable, accurate, and easy to control. On language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens using a similar-size model, delivering up to 128 times faster sampling and corresponding latency/throughput gains.</li>
</ul>

<h3>Title: Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Pan, Zhe Liu, Hesheng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20648">https://arxiv.org/abs/2509.20648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20648">https://arxiv.org/pdf/2509.20648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20648]] Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration(https://arxiv.org/abs/2509.20648)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Autonomous exploration in complex multi-agent reinforcement learning (MARL) with sparse rewards critically depends on providing agents with effective intrinsic motivation. While artificial curiosity offers a powerful self-supervised signal, it often confuses environmental stochasticity with meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform novelty bias, treating all unexpected observations equally. However, peer behavior novelty, which encode latent task dynamics, are often overlooked, resulting in suboptimal exploration in decentralized, communication-free MARL settings. To this end, inspired by how human children adaptively calibrate their own exploratory behaviors via observing peers, we propose a novel approach to enhance multi-agent exploration. We introduce CERMIC, a principled framework that empowers agents to robustly filter noisy surprise signals and guide exploration by dynamically calibrating their intrinsic curiosity with inferred multi-agent context. Additionally, CERMIC generates theoretically-grounded intrinsic rewards, encouraging agents to explore state transitions with high information gain. We evaluate CERMIC on benchmark suites including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that exploration with CERMIC significantly outperforms SoTA algorithms in sparse-reward environments.</li>
</ul>

<h3>Title: Enhancing Molecular Property Prediction with Knowledge from Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Peng Zhou, Lai Hou Tim, Zhixiang Cheng, Kun Xie, Chaoyi Li, Wei Liu, Xiangxiang Zeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20664">https://arxiv.org/abs/2509.20664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20664">https://arxiv.org/pdf/2509.20664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20664]] Enhancing Molecular Property Prediction with Knowledge from Large Language Models(https://arxiv.org/abs/2509.20664)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Predicting molecular properties is a critical component of drug discovery. Recent advances in deep learning, particularly Graph Neural Networks (GNNs), have enabled end-to-end learning from molecular structures, reducing reliance on manual feature engineering. However, while GNNs and self-supervised learning approaches have advanced molecular property prediction (MPP), the integration of human prior knowledge remains indispensable, as evidenced by recent methods that leverage large language models (LLMs) for knowledge extraction. Despite their strengths, LLMs are constrained by knowledge gaps and hallucinations, particularly for less-studied molecular properties. In this work, we propose a novel framework that, for the first time, integrates knowledge extracted from LLMs with structural features derived from pre-trained molecular models to enhance MPP. Our approach prompts LLMs to generate both domain-relevant knowledge and executable code for molecular vectorization, producing knowledge-based features that are subsequently fused with structural representations. We employ three state-of-the-art LLMs, GPT-4o, GPT-4.1, and DeepSeek-R1, for knowledge extraction. Extensive experiments demonstrate that our integrated method outperforms existing approaches, confirming that the combination of LLM-derived knowledge and structural information provides a robust and effective solution for MPP.</li>
</ul>

<h3>Title: Theoretical Bounds for Stable In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Tongxi Wang, Zhuoyang Xia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20677">https://arxiv.org/abs/2509.20677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20677">https://arxiv.org/pdf/2509.20677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20677]] Theoretical Bounds for Stable In-Context Learning(https://arxiv.org/abs/2509.20677)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is flexible but its reliability is highly sensitive to prompt length. This paper establishes a non-asymptotic lower bound that links the minimal number of demonstrations to ICL stability under fixed high-dimensional sub-Gaussian representations. The bound gives explicit sufficient conditions in terms of spectral properties of the covariance, providing a computable criterion for practice. Building on this analysis, we propose a two-stage observable estimator with a one-shot calibration that produces practitioner-ready prompt-length estimates without distributional priors. Experiments across diverse datasets, encoders, and generators show close alignment between the predicted thresholds and empirical knee-points, with the theory acting as a conservative but reliable upper bound; the calibrated variant further tightens this gap. These results connect spectral coverage to stable ICL, bridge theory and deployment, and improve the interpretability and reliability of large-scale prompting in realistic finite-sample regimes.</li>
</ul>

<h3>Title: Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yu Guo, Shengfeng He, Yuxu Lu, Haonan An, Yihang Tao, Huilin Zhu, Jingxian Liu, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20745">https://arxiv.org/abs/2509.20745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20745">https://arxiv.org/pdf/2509.20745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20745]] Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection(https://arxiv.org/abs/2509.20745)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Maritime object detection is essential for navigation safety, surveillance, and autonomous operations, yet constrained by two key challenges: the scarcity of annotated maritime data and poor generalization across various maritime attributes (e.g., object category, viewpoint, location, and imaging environment). % In particular, models trained on existing datasets often underperform in underrepresented scenarios such as open-sea environments. To address these challenges, we propose Neptune-X, a data-centric generative-selection framework that enhances training effectiveness by leveraging synthetic data generation with task-aware sample selection. From the generation perspective, we develop X-to-Maritime, a multi-modality-conditioned generative model that synthesizes diverse and realistic maritime scenes. A key component is the Bidirectional Object-Water Attention module, which captures boundary interactions between objects and their aquatic surroundings to improve visual fidelity. To further improve downstream tasking performance, we propose Attribute-correlated Active Sampling, which dynamically selects synthetic samples based on their task relevance. To support robust benchmarking, we construct the Maritime Generation Dataset, the first dataset tailored for generative maritime learning, encompassing a wide range of semantic conditions. Extensive experiments demonstrate that our approach sets a new benchmark in maritime scene synthesis, significantly improving detection accuracy, particularly in challenging and previously underrepresented this http URL code is available at this https URL.</li>
</ul>

<h3>Title: FreeInsert: Personalized Object Insertion with Geometric and Style Control</h3>
<ul>
<li><strong>Authors: </strong>Yuhong Zhang, Han Wang, Yiwen Wang, Rong Xie, Li Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20756">https://arxiv.org/abs/2509.20756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20756">https://arxiv.org/pdf/2509.20756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20756]] FreeInsert: Personalized Object Insertion with Geometric and Style Control(https://arxiv.org/abs/2509.20756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have made significant progress in image generation, allowing for effortless customized generation. However, existing image editing methods still face certain limitations when dealing with personalized image composition tasks. First, there is the issue of lack of geometric control over the inserted objects. Current methods are confined to 2D space and typically rely on textual instructions, making it challenging to maintain precise geometric control over the objects. Second, there is the challenge of style consistency. Existing methods often overlook the style consistency between the inserted object and the background, resulting in a lack of realism. In addition, the challenge of inserting objects into images without extensive training remains significant. To address these issues, we propose \textit{FreeInsert}, a novel training-free framework that customizes object insertion into arbitrary scenes by leveraging 3D geometric information. Benefiting from the advances in existing 3D generation models, we first convert the 2D object into 3D, perform interactive editing at the 3D level, and then re-render it into a 2D image from a specified view. This process introduces geometric controls such as shape or view. The rendered image, serving as geometric control, is combined with style and content control achieved through diffusion adapters, ultimately producing geometrically controlled, style-consistent edited images via the diffusion model.</li>
</ul>

<h3>Title: Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Maria F. Davila R, Azizjon Turaev, Wolfram Wingerath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20768">https://arxiv.org/abs/2509.20768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20768">https://arxiv.org/pdf/2509.20768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20768]] Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis(https://arxiv.org/abs/2509.20768)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Synthetic tabular data is used for privacy-preserving data sharing and data-driven model development. Its effectiveness, however, depends heavily on the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that Transformer-based models outperform other state-of-the-art models such as Generative Adversarial Networks (GANs) and Diffusion models in terms of data quality. However, Transformer-based models also come with high computational costs, making them sometimes unfeasible for end users with prosumer hardware. This study presents a sensitivity assessment on how the choice of hyperparameters, such as number of layers or hidden dimension affects the quality of the resultant synthetic data and the computational performance. It is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model setups that vary in architecture type and depth. We assess the sensitivity on three dimensions: runtime, machine learning (ML) utility, and similarity to real data distributions. Experiments were conducted on four real-world datasets. Our findings reveal that runtime is proportional to the number of hyperparameters, with shallower configurations completing faster. GReaT consistently achieves lower runtimes than REaLTabFormer, and only on the largest dataset they have comparable runtime. For small datasets, both tools achieve synthetic data with high utility and optimal similarity, but on larger datasets only REaLTabFormer sustains strong utility and similarity. As a result, REaLTabFormer with lightweight LLMs provides the best balance, since it preserves data quality while reducing computational requirements. Nonetheless, its runtime remains higher than that of GReaT and other TDS tools, suggesting that efficiency gains are possible but only up to a certain level.</li>
</ul>

<h3>Title: CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion</h3>
<ul>
<li><strong>Authors: </strong>Maoye Ren, Praneetha Vaddamanu, Jianjin Xu, Fernando De la Torre Frade</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20775">https://arxiv.org/abs/2509.20775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20775">https://arxiv.org/pdf/2509.20775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20775]] CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion(https://arxiv.org/abs/2509.20775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently remarkable progress has been made in synthesizing realistic human photos using text-to-image diffusion models. However, current approaches face degraded scenes, insufficient control, and suboptimal perceptual identity. We introduce CustomEnhancer, a novel framework to augment existing identity customization models. CustomEnhancer is a zero-shot enhancement pipeline that leverages face swapping techniques, pretrained diffusion model, to obtain additional representations in a zeroshot manner for encoding into personalized models. Through our proposed triple-flow fused PerGeneration approach, which identifies and combines two compatible counter-directional latent spaces to manipulate a pivotal space of personalized model, we unify the generation and reconstruction processes, realizing generation from three flows. Our pipeline also enables comprehensive training-free control over the generation process of personalized models, offering precise controlled personalization for them and eliminating the need for controller retraining for per-model. Besides, to address the high time complexity of null-text inversion (NTI), we introduce ResInversion, a novel inversion method that performs noise rectification via a pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity fidelity, training-free controls, while also showing the efficiency of our ResInversion over NTI. The code will be made publicly available upon paper acceptance.</li>
</ul>

<h3>Title: Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Jincai Song, Haipeng Chen, Jun Qin, Na Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20785">https://arxiv.org/abs/2509.20785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20785">https://arxiv.org/pdf/2509.20785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20785]] Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization(https://arxiv.org/abs/2509.20785)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Semi-supervised domain generalization (SSDG) in medical image segmentation offers a promising solution for generalizing to unseen domains during testing, addressing domain shift challenges and minimizing annotation costs. However, conventional SSDG methods assume labeled and unlabeled data are available for each source domain in the training set, a condition that is not always met in practice. The coexistence of limited annotation and domain shift in the training set is a prevalent issue. Thus, this paper explores a more practical and challenging scenario, cross-domain semi-supervised domain generalization (CD-SSDG), where domain shifts occur between labeled and unlabeled training data, in addition to shifts between training and testing sets. Existing SSDG methods exhibit sub-optimal performance under such domain shifts because of inaccurate pseudolabels. To address this issue, we propose a novel dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG. Building upon the co-training paradigm with two sub-models offering cross pseudo supervision, our DAC framework integrates extra feature-level supervision and asymmetric auxiliary tasks for each sub-model. This feature-level supervision serves to address inaccurate pseudo supervision caused by domain shifts between labeled and unlabeled data, utilizing complementary supervision from the rich feature space. Additionally, two distinct auxiliary self-supervised tasks are integrated into each sub-model to enhance domain-invariant discriminative feature learning and prevent model collapse. Extensive experiments on real-world medical image segmentation datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust generalizability of the proposed DAC framework.</li>
</ul>

<h3>Title: Federated Domain Generalization with Domain-specific Soft Prompts Generation</h3>
<ul>
<li><strong>Authors: </strong>Jianhan Wu, Xiaoyang Qu, Zhangcheng Huang, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20807">https://arxiv.org/abs/2509.20807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20807">https://arxiv.org/pdf/2509.20807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20807]] Federated Domain Generalization with Domain-specific Soft Prompts Generation(https://arxiv.org/abs/2509.20807)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prompt learning has become an efficient paradigm for adapting CLIP to downstream tasks. Compared with traditional fine-tuning, prompt learning optimizes a few parameters yet yields highly competitive results, especially appealing in federated learning for computational efficiency. engendering domain shift among clients and posing a formidable challenge for downstream-task adaptation. Existing federated domain generalization (FDG) methods based on prompt learning typically learn soft prompts from training samples, replacing manually designed prompts to enhance the generalization ability of federated models. However, these learned prompts exhibit limited diversity and tend to ignore information from unknown domains. We propose a novel and effective method from a generative perspective for handling FDG tasks, namely federated domain generalization with domain-specific soft prompts generation (FedDSPG). Specifically, during training, we introduce domain-specific soft prompts (DSPs) for each domain and integrate content and domain knowledge into the generative model among clients. In the inference phase, the generator is utilized to obtain DSPs for unseen target domains, thus guiding downstream tasks in unknown domains. Comprehensive evaluations across several public datasets confirm that our method outperforms existing strong baselines in FDG, achieving state-of-the-art results.</li>
</ul>

<h3>Title: Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection</h3>
<ul>
<li><strong>Authors: </strong>Taehee Park, Heejin Do, Gary Geunbae Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20811">https://arxiv.org/abs/2509.20811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20811">https://arxiv.org/pdf/2509.20811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20811]] Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection(https://arxiv.org/abs/2509.20811)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Robust supervised fine-tuned small Language Models (sLMs) often show high reliability but tend to undercorrect. They achieve high precision at the cost of low recall. Conversely, Large Language Models (LLMs) often show the opposite tendency, making excessive overcorrection, leading to low precision. To effectively harness the strengths of LLMs to address the recall challenges in sLMs, we propose Post-Correction via Overcorrection (PoCO), a novel approach that strategically balances recall and precision. PoCO first intentionally triggers overcorrection via LLM to maximize recall by allowing comprehensive revisions, then applies a targeted post-correction step via fine-tuning smaller models to identify and refine erroneous outputs. We aim to harmonize both aspects by leveraging the generative power of LLMs while preserving the reliability of smaller supervised models. Our extensive experiments demonstrate that PoCO effectively balances GEC performance by increasing recall with competitive precision, ultimately improving the overall quality of grammatical error correction.</li>
</ul>

<h3>Title: Distilling Many-Shot In-Context Learning into a Cheat Sheet</h3>
<ul>
<li><strong>Authors: </strong>Ukyo Honda, Soichiro Murakami, Peinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20820">https://arxiv.org/abs/2509.20820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20820">https://arxiv.org/pdf/2509.20820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20820]] Distilling Many-Shot In-Context Learning into a Cheat Sheet(https://arxiv.org/abs/2509.20820)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) enable effective in-context learning (ICL) with many-shot examples, but at the cost of high computational demand due to longer input tokens. To address this, we propose cheat-sheet ICL, which distills the information from many-shot ICL into a concise textual summary (cheat sheet) used as the context at inference time. Experiments on challenging reasoning tasks show that cheat-sheet ICL achieves comparable or better performance than many-shot ICL with far fewer tokens, and matches retrieval-based ICL without requiring test-time retrieval. These findings demonstrate that cheat-sheet ICL is a practical alternative for leveraging LLMs in downstream tasks.</li>
</ul>

<h3>Title: T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Hwa Hui Tew, Junn Yong Loo, Yee-Fan Tan, Xinyu Tang, Hernando Ombao, Fuad Noman, Raphael C.-W. Phan, Chee-Ming Ting</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20822">https://arxiv.org/abs/2509.20822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20822">https://arxiv.org/pdf/2509.20822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20822]] T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models(https://arxiv.org/abs/2509.20822)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging method that enables in-depth analysis of brain activity by measuring dynamic changes in the blood oxygenation level-dependent (BOLD) signals. However, the resource-intensive nature of fMRI data acquisition limits the availability of high-fidelity samples required for data-driven brain analysis models. While modern generative models can synthesize fMRI data, they often underperform because they overlook the complex non-stationarity and nonlinear BOLD dynamics. To address these challenges, we introduce T2I-Diff, an fMRI generation framework that leverages time-frequency representation of BOLD signals and classifier-free denoising diffusion. Specifically, our framework first converts BOLD signals into windowed spectrograms via a time-dependent Fourier transform, capturing both the underlying temporal dynamics and spectral evolution. Subsequently, a classifier-free diffusion model is trained to generate class-conditioned frequency spectrograms, which are then reverted to BOLD signals via inverse Fourier transforms. Finally, we validate the efficacy of our approach by demonstrating improved accuracy and generalization in downstream fMRI-based brain network classification.</li>
</ul>

<h3>Title: CaTS-Bench: Can Language Models Describe Numeric Time Series?</h3>
<ul>
<li><strong>Authors: </strong>Luca Zhou, Pratham Yashwante, Marshall Fisher, Alessio Sampieri, Zihao Zhou, Fabio Galasso, Rose Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20823">https://arxiv.org/abs/2509.20823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20823">https://arxiv.org/pdf/2509.20823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20823]] CaTS-Bench: Can Language Models Describe Numeric Time Series?(https://arxiv.org/abs/2509.20823)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series captioning, the task of describing numeric time series in natural language, requires numerical reasoning, trend interpretation, and contextual understanding. Existing benchmarks, however, often rely on synthetic data or overly simplistic captions, and typically neglect metadata and visual representations. To close this gap, we introduce CaTS-Bench, the first large-scale, real-world benchmark for Context-aware Time Series captioning. CaTS-Bench is derived from 11 diverse datasets reframed as captioning and Q&A tasks, comprising roughly 465k training and 105k test timestamps. Each sample includes a numeric series segment, contextual metadata, a line-chart image, and a caption. A key contribution of this work is the scalable pipeline used to generate reference captions: while most references are produced by an oracle LLM and verified through factual checks, human indistinguishability studies, and diversity analyses, we also provide a human-revisited subset of 579 test captions, refined from LLM outputs to ensure accuracy and human-like style. Beyond captioning, CaTS-Bench offers 460 multiple-choice questions targeting deeper aspects of time series reasoning. We further propose new tailored evaluation metrics and benchmark leading VLMs, highlighting both their strengths and persistent limitations. Together, these contributions establish CaTS-Bench and its captioning pipeline as a reliable and extensible foundation for future research at the intersection of time series analysis and foundation models.</li>
</ul>

<h3>Title: Causal Time Series Generation via Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yutong Xia, Chang Xu, Yuxuan Liang, Qingsong Wen, Roger Zimmermann, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20846">https://arxiv.org/abs/2509.20846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20846">https://arxiv.org/pdf/2509.20846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20846]] Causal Time Series Generation via Diffusion Models(https://arxiv.org/abs/2509.20846)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Time series generation (TSG) synthesizes realistic sequences and has achieved remarkable success. Among TSG, conditional models generate sequences given observed covariates, however, such models learn observational correlations without considering unobserved confounding. In this work, we propose a causal perspective on conditional TSG and introduce causal time series generation as a new TSG task family, formalized within Pearl's causal ladder, extending beyond observational generation to include interventional and counterfactual settings. To instantiate these tasks, we develop CaTSG, a unified diffusion-based framework with backdoor-adjusted guidance that causally steers sampling toward desired interventions and individual counterfactuals while preserving observational fidelity. Specifically, our method derives causal score functions via backdoor adjustment and the abduction-action-prediction procedure, thus enabling principled support for all three levels of TSG. Extensive experiments on both synthetic and real-world datasets show that CaTSG achieves superior fidelity and also supporting interventional and counterfactual generation that existing baselines cannot handle. Overall, we propose the causal TSG family and instantiate it with CaTSG, providing an initial proof-of-concept and opening a promising direction toward more reliable simulation under interventions and counterfactual generation.</li>
</ul>

<h3>Title: FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Kjersti Engan, Neel Kanwal, Anita Yeconia, Ladislaus Blacy, Yuda Munyaw, Estomih Mduma, Hege Ersdal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20852">https://arxiv.org/abs/2509.20852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20852">https://arxiv.org/pdf/2509.20852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20852]] FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting(https://arxiv.org/abs/2509.20852)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Approximately 10\% of newborns require assistance to initiate breathing at birth, and around 5\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.</li>
</ul>

<h3>Title: TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting</h3>
<ul>
<li><strong>Authors: </strong>Xiaonan Hu, Xuebing Li, Jinyu Xu, Abdulkadir Duran Adan, Letian Zhou, Xuhui Zhu, Yanan Li, Wei Guo, Shouyang Liu, Wenzhong Liu, Hao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20857">https://arxiv.org/abs/2509.20857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20857">https://arxiv.org/pdf/2509.20857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20857]] TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting(https://arxiv.org/abs/2509.20857)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Accurate plant counting provides valuable information for agriculture such as crop yield prediction, plant density assessment, and phenotype quantification. Vision-based approaches are currently the mainstream solution. Prior art typically uses a detection or a regression model to count a specific plant. However, plants have biodiversity, and new cultivars are increasingly bred each year. It is almost impossible to exhaust and build all species-dependent counting models. Inspired by class-agnostic counting (CAC) in computer vision, we argue that it is time to rethink the problem formulation of plant counting, from what plants to count to how to count plants. In contrast to most daily objects with spatial and temporal invariance, plants are dynamic, changing with time and space. Their non-rigid structure often leads to worse performance than counting rigid instances like heads and cars such that current CAC and open-world detection models are suboptimal to count plants. In this work, we inherit the vein of the TasselNet plant counting model and introduce a new extension, TasselNetV4, shifting from species-specific counting to cross-species counting. TasselNetV4 marries the local counting idea of TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain vision transformer and incorporates novel multi-branch box-aware local counters used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC models show that TasselNetV4 achieves not only superior counting performance but also high this http URL results indicate that TasselNetV4 emerges to be a vision foundation model for cross-scene, cross-scale, and cross-species plant counting.</li>
</ul>

<h3>Title: WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs</h3>
<ul>
<li><strong>Authors: </strong>Guowei Xu, Wenxin Xu, Jiawang Zhao, Kaisheng Ma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20863">https://arxiv.org/abs/2509.20863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20863">https://arxiv.org/pdf/2509.20863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20863]] WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs(https://arxiv.org/abs/2509.20863)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently shown strong potential in language modeling, offering faster generation compared to traditional autoregressive approaches. However, applying supervised fine-tuning (SFT) to diffusion models remains challenging, as they lack precise probability estimates at each denoising step. While the diffusion mechanism enables the model to reason over entire sequences, it also makes the generation process less predictable and often inconsistent. This highlights the importance of controlling key tokens that guide the direction of generation. To address this issue, we propose WeFT, a weighted SFT method for diffusion language models, where tokens are assigned different weights based on their entropy. Derived from diffusion theory, WeFT delivers substantial gains: training on s1K, s1K-1.1, and 3k samples from open-r1, it achieves relative improvements of 39%, 64%, and 83% over standard SFT on four widely used reasoning benchmarks (Sudoku, Countdown, GSM8K, and MATH-500). The code and models will be made publicly available.</li>
</ul>

<h3>Title: Nuclear Diffusion Models for Low-Rank Background Suppression in Videos</h3>
<ul>
<li><strong>Authors: </strong>Tristan S.W. Stevens, Oisn Nolan, Jean-Luc Robert, Ruud J.G. van Sloun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20886">https://arxiv.org/abs/2509.20886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20886">https://arxiv.org/pdf/2509.20886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20886]] Nuclear Diffusion Models for Low-Rank Background Suppression in Videos(https://arxiv.org/abs/2509.20886)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Video sequences often contain structured noise and background artifacts that obscure dynamic content, posing challenges for accurate analysis and restoration. Robust principal component methods address this by decomposing data into low-rank and sparse components. Still, the sparsity assumption often fails to capture the rich variability present in real video data. To overcome this limitation, a hybrid framework that integrates low-rank temporal modeling with diffusion posterior sampling is proposed. The proposed method, Nuclear Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac ultrasound dehazing, and demonstrates improved dehazing performance compared to traditional RPCA concerning contrast enhancement (gCNR) and signal preservation (KS statistic). These results highlight the potential of combining model-based temporal models with deep generative priors for high-fidelity video restoration.</li>
</ul>

<h3>Title: FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies</h3>
<ul>
<li><strong>Authors: </strong>Shuqiao Liang, Jian Liu, Renzhang Chen, Quanlong Guan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20890">https://arxiv.org/abs/2509.20890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20890">https://arxiv.org/pdf/2509.20890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20890]] FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies(https://arxiv.org/abs/2509.20890)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing realism of synthetic images generated by advanced models such as VAEs, GANs, and LDMs poses significant challenges for synthetic image detection. To address this issue, we explore two artifact types introduced during the generation process: (1) latent distribution deviations and (2) decoding-induced smoothing effects, which manifest as inconsistencies in local textures, edges, and color transitions. Leveraging local pixel dependencies (LPD) properties rooted in Markov Random Fields, we reconstruct synthetic images using neighboring pixel information to expose disruptions in texture continuity and edge coherence. Building upon LPD, we propose FerretNet, a lightweight neural network with only 1.1M parameters that delivers efficient and robust synthetic image detection. Extensive experiments demonstrate that FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an average accuracy of 97.1% on an open-world benchmark comprising across 22 generative models, surpassing state-of-the-art methods by 10.6%.</li>
</ul>

<h3>Title: Deterministic Discrete Denoising</h3>
<ul>
<li><strong>Authors: </strong>Hideyuki Suzuki, Hiroshi Yamashita</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.CD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20896">https://arxiv.org/abs/2509.20896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20896">https://arxiv.org/pdf/2509.20896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20896]] Deterministic Discrete Denoising(https://arxiv.org/abs/2509.20896)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose a deterministic denoising algorithm for discrete-state diffusion models based on Markov chains. The generative reverse process is derandomized by introducing a variant of the herding algorithm with weakly chaotic dynamics, which induces deterministic discrete state transitions. Our approach is a direct replacement for the stochastic denoising process, requiring neither retraining nor continuous state embeddings. We demonstrate consistent improvements in both efficiency and sample quality on text and image generation tasks. Thus, this simple derandomization approach is expected to enhance the significance of discrete diffusion in generative modeling. Furthermore, our results reveal that deterministic reverse processes, well established in continuous diffusion, can also be effective in discrete state spaces.</li>
</ul>

<h3>Title: Revisiting Data Challenges of Computational Pathology: A Pack-based Multiple Instance Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Tang, Heng Fang, Ge Wu, Xiang Li, Ming-Ming Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20923">https://arxiv.org/abs/2509.20923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20923">https://arxiv.org/pdf/2509.20923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20923]] Revisiting Data Challenges of Computational Pathology: A Pack-based Multiple Instance Learning Framework(https://arxiv.org/abs/2509.20923)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Computational pathology (CPath) digitizes pathology slides into whole slide images (WSIs), enabling analysis for critical healthcare tasks such as cancer diagnosis and prognosis. However, WSIs possess extremely long sequence lengths (up to 200K), significant length variations (from 200 to 200K), and limited supervision. These extreme variations in sequence length lead to high data heterogeneity and redundancy. Conventional methods often compromise on training efficiency and optimization to preserve such heterogeneity under limited supervision. To comprehensively address these challenges, we propose a pack-based MIL framework. It packs multiple sampled, variable-length feature sequences into fixed-length ones, enabling batched training while preserving data heterogeneity. Moreover, we introduce a residual branch that composes discarded features from multiple slides into a hyperslide which is trained with tailored labels. It offers multi-slide supervision while mitigating feature loss from sampling. Meanwhile, an attention-driven downsampler is introduced to compress features in both branches to reduce redundancy. By alleviating these challenges, our approach achieves an accuracy improvement of up to 8% while using only 12% of the training time in the PANDA(UNI). Extensive experiments demonstrate that focusing data challenges in CPath holds significant potential in the era of foundation models. The code is this https URL</li>
</ul>

<h3>Title: SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Akihisa Watanabe, Jiawei Ren, Li Siyao, Yichen Peng, Erwin Wu, Edgar Simo-Serra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20927">https://arxiv.org/abs/2509.20927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20927">https://arxiv.org/pdf/2509.20927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20927]] SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation(https://arxiv.org/abs/2509.20927)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating physically plausible human motion is crucial for applications such as character animation and virtual reality. Existing approaches often incorporate a simulator-based motion projection layer to the diffusion process to enforce physical plausibility. However, such methods are computationally expensive due to the sequential nature of the simulator, which prevents parallelization. We show that simulator-based motion projection can be interpreted as a form of guidance, either classifier-based or classifier-free, within the diffusion process. Building on this insight, we propose SimDiff, a Simulator-constrained Diffusion Model that integrates environment parameters (e.g., gravity, wind) directly into the denoising process. By conditioning on these parameters, SimDiff generates physically plausible motions efficiently, without repeated simulator calls at inference, and also provides fine-grained control over different physical coefficients. Moreover, SimDiff successfully generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.</li>
</ul>

<h3>Title: GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Sarah Seifi, Anass Ibrahimi, Tobias Sukianto, Cecilia Carbonelli, Lorenzo Servadei, Robert Wille</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20936">https://arxiv.org/abs/2509.20936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20936">https://arxiv.org/pdf/2509.20936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20936]] GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series(https://arxiv.org/abs/2509.20936)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations aim to enhance model transparency by showing how inputs can be minimally altered to change predictions. For multivariate time series, existing methods often generate counterfactuals that are invalid, implausible, or unintuitive. We introduce GenFacts, a generative framework based on a class-discriminative variational autoencoder. It integrates contrastive and classification-consistency objectives, prototype-based initialization, and realism-constrained optimization. We evaluate GenFacts on radar gesture data as an industrial use case and handwritten letter trajectories as an intuitive benchmark. Across both datasets, GenFacts outperforms state-of-the-art baselines in plausibility (+18.7%) and achieves the highest interpretability scores in a human study. These results highlight that plausibility and user-centered interpretability, rather than sparsity alone, are key to actionable counterfactuals in time series data.</li>
</ul>

<h3>Title: Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery</h3>
<ul>
<li><strong>Authors: </strong>Angelo Henriques, Korab Hoxha, Daniel Zapp, Peter C. Issa, Nassir Navab, M. Ali Nasseri</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20941">https://arxiv.org/abs/2509.20941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20941">https://arxiv.org/pdf/2509.20941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20941]] Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery(https://arxiv.org/abs/2509.20941)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Scene graphs (SGs) provide structured relational representations crucial for decoding complex, dynamic surgical environments. This PRISMA-ScR-guided scoping review systematically maps the evolving landscape of SG research in surgery, charting its applications, methodological advancements, and future directions. Our analysis reveals rapid growth, yet uncovers a critical 'data divide': internal-view research (e.g., triplet recognition) almost exclusively uses real-world 2D video, while external-view 4D modeling relies heavily on simulated data, exposing a key translational research gap. Methodologically, the field has advanced from foundational graph neural networks to specialized foundation models that now significantly outperform generalist large vision-language models in surgical contexts. This progress has established SGs as a cornerstone technology for both analysis, such as workflow recognition and automated safety monitoring, and generative tasks like controllable surgical simulation. Although challenges in data annotation and real-time implementation persist, they are actively being addressed through emerging techniques. Surgical SGs are maturing into an essential semantic bridge, enabling a new generation of intelligent systems to improve surgical safety, efficiency, and training.</li>
</ul>

<h3>Title: A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongqi Zheng, Wenjin Fu, Guangzong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20946">https://arxiv.org/abs/2509.20946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20946">https://arxiv.org/pdf/2509.20946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20946]] A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning(https://arxiv.org/abs/2509.20946)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We present an automated vision-based system for defect detection and classification of laser power meter sensor coatings. Our approach addresses the critical challenge of identifying coating defects such as thermal damage and scratches that can compromise laser energy measurement accuracy in medical and industrial applications. The system employs an unsupervised anomaly detection framework that trains exclusively on ``good'' sensor images to learn normal coating distribution patterns, enabling detection of both known and novel defect types without requiring extensive labeled defect datasets. Our methodology consists of three key components: (1) a robust preprocessing pipeline using Laplacian edge detection and K-means clustering to segment the area of interest, (2) synthetic data augmentation via StyleGAN2, and (3) a UFlow-based neural network architecture for multi-scale feature extraction and anomaly map generation. Experimental evaluation on 366 real sensor images demonstrates $93.8\%$ accuracy on defective samples and $89.3\%$ accuracy on good samples, with image-level AUROC of 0.957 and pixel-level AUROC of 0.961. The system provides potential annual cost savings through automated quality control and processing times of 0.5 seconds per image in on-device implementation.</li>
</ul>

<h3>Title: Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy</h3>
<ul>
<li><strong>Authors: </strong>Weili Zeng, Yichao Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20952">https://arxiv.org/abs/2509.20952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20952">https://arxiv.org/pdf/2509.20952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20952]] Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy(https://arxiv.org/abs/2509.20952)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow matching has recently emerged as a powerful alternative to diffusion models, providing a continuous-time formulation for generative modeling and representation learning. Yet, we show that this framework suffers from a fundamental instability in the low-noise regime. As noise levels approach zero, arbitrarily small perturbations in the input can induce large variations in the velocity target, causing the condition number of the learning problem to diverge. This ill-conditioning not only slows optimization but also forces the encoder to reallocate its limited Jacobian capacity toward noise directions, thereby degrading semantic representations. We provide the first theoretical analysis of this phenomenon, which we term the low-noise pathology, establishing its intrinsic link to the structure of the flow matching objective. Building on these insights, we propose Local Contrastive Flow (LCF), a hybrid training protocol that replaces direct velocity regression with contrastive feature alignment at small noise levels, while retaining standard flow matching at moderate and high noise. Empirically, LCF not only improves convergence speed but also stabilizes representation quality. Our findings highlight the critical importance of addressing low-noise pathologies to unlock the full potential of flow matching for both generation and representation learning.</li>
</ul>

<h3>Title: Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhengyuan Shi, Jingxin Wang, Wentao Jiang, Chengyu Ma, Ziyang Zheng, Zhufei Chu, Weikang Qian, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20968">https://arxiv.org/abs/2509.20968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20968">https://arxiv.org/pdf/2509.20968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20968]] Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning(https://arxiv.org/abs/2509.20968)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Multiview learning on Boolean circuits holds immense promise, as different graph-based representations offer complementary structural and semantic information. However, the vast structural heterogeneity between views, such as an And-Inverter Graph (AIG) versus an XOR-Majority Graph (XMG), poses a critical barrier to effective fusion, especially for self-supervised techniques like masked modeling. Naively applying such methods fails, as the cross-view context is perceived as noise. Our key insight is that functional alignment is a necessary precondition to unlock the power of multiview self-supervision. We introduce MixGate, a framework built on a principled training curriculum that first teaches the model a shared, function-aware representation space via an Equivalence Alignment Loss. Only then do we introduce a multiview masked modeling objective, which can now leverage the aligned views as a rich, complementary signal. Extensive experiments, including a crucial ablation study, demonstrate that our alignment-first strategy transforms masked modeling from an ineffective technique into a powerful performance driver.</li>
</ul>

<h3>Title: FracAug: Fractional Augmentation boost Graph-level Anomaly Detection under Limited Supervision</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Dong, Xingyi Zhang, Sibo Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20978">https://arxiv.org/abs/2509.20978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20978">https://arxiv.org/pdf/2509.20978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20978]] FracAug: Fractional Augmentation boost Graph-level Anomaly Detection under Limited Supervision(https://arxiv.org/abs/2509.20978)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Graph-level anomaly detection (GAD) is critical in diverse domains such as drug discovery, yet high labeling costs and dataset imbalance hamper the performance of Graph Neural Networks (GNNs). To address these issues, we propose FracAug, an innovative plug-in augmentation framework that enhances GNNs by generating semantically consistent graph variants and pseudo-labeling with mutual verification. Unlike previous heuristic methods, FracAug learns semantics within given graphs and synthesizes fractional variants, guided by a novel weighted distance-aware margin loss. This captures multi-scale topology to generate diverse, semantic-preserving graphs unaffected by data imbalance. Then, FracAug utilizes predictions from both original and augmented graphs to pseudo-label unlabeled data, iteratively expanding the training set. As a model-agnostic module compatible with various GNNs, FracAug demonstrates remarkable universality and efficacy: experiments across 14 GNNs on 12 real-world datasets show consistent gains, boosting average AUROC, AUPRC, and F1-score by up to 5.72%, 7.23%, and 4.18%, respectively.</li>
</ul>

<h3>Title: SiNGER: A Clearer Voice Distills Vision Transformers Further</h3>
<ul>
<li><strong>Authors: </strong>Geunhyeok Yu, Sunjae Jeong, Yoonyoung Choi, Jaeseung Kim, Hyoseok Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20986">https://arxiv.org/abs/2509.20986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20986">https://arxiv.org/pdf/2509.20986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20986]] SiNGER: A Clearer Voice Distills Vision Transformers Further(https://arxiv.org/abs/2509.20986)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.</li>
</ul>

<h3>Title: Binary Autoencoder for Mechanistic Interpretability of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hakaze Cho, Haolin Yang, Brian M. Kurkoski, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20997">https://arxiv.org/abs/2509.20997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20997">https://arxiv.org/pdf/2509.20997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20997]] Binary Autoencoder for Mechanistic Interpretability of Large Language Models(https://arxiv.org/abs/2509.20997)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e., $L_1$ normalization, top-k function, etc.), without an explicit guarantee of global sparsity among instances, causing a large amount of dense (simultaneously inactive) features, harming the feature sparsity and atomization. In this paper, we propose a novel autoencoder variant that enforces minimal entropy on minibatches of hidden activations, thereby promoting feature independence and sparsity across instances. For efficient entropy calculation, we discretize the hidden activations to 1-bit via a step function and apply gradient estimation to enable backpropagation, so that we term it as Binary Autoencoder (BAE) and empirically demonstrate two major applications: (1) Feature set entropy calculation. Entropy can be reliably estimated on binary hidden activations, which we empirically evaluate and leverage to characterize the inference dynamics of LLMs and In-context Learning. (2) Feature untangling. Similar to typical methods, BAE can extract atomized features from LLM's hidden states. To robustly evaluate such feature extraction capability, we refine traditional feature-interpretation methods to avoid unreliable handling of numerical tokens, and show that BAE avoids dense features while producing the largest number of interpretable ones among baselines, which confirms the effectiveness of BAE serving as a feature extractor.</li>
</ul>

<h3>Title: Lossless Compression: A New Benchmark for Time Series Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Meng Wan, Benxi Tian, Jue Wang, Cui Hui, Ningming Nie, Tiantian Liu, Zongguo Wang, Cao Rongqiang, Peng Shi, Yangang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21002">https://arxiv.org/abs/2509.21002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21002">https://arxiv.org/pdf/2509.21002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21002]] Lossless Compression: A New Benchmark for Time Series Model Evaluation(https://arxiv.org/abs/2509.21002)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>The evaluation of time series models has traditionally focused on four canonical tasks: forecasting, imputation, anomaly detection, and classification. While these tasks have driven significant progress, they primarily assess task-specific performance and do not rigorously measure whether a model captures the full generative distribution of the data. We introduce lossless compression as a new paradigm for evaluating time series models, grounded in Shannon's source coding theorem. This perspective establishes a direct equivalence between optimal compression length and the negative log-likelihood, providing a strict and unified information-theoretic criterion for modeling capacity. Then We define a standardized evaluation protocol and metrics. We further propose and open-source a comprehensive evaluation framework TSCom-Bench, which enables the rapid adaptation of time series models as backbones for lossless compression. Experiments across diverse datasets on state-of-the-art models, including TimeXer, iTransformer, and PatchTST, demonstrate that compression reveals distributional weaknesses overlooked by classic benchmarks. These findings position lossless compression as a principled task that complements and extends existing evaluation for time series modeling.</li>
</ul>

<h3>Title: A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Qinqin He, Jiaqi Weng, Jialing Tao, Hui Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21008">https://arxiv.org/abs/2509.21008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21008">https://arxiv.org/pdf/2509.21008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21008]] A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models(https://arxiv.org/abs/2509.21008)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image models exhibit remarkable capabilities in image generation. However, they also pose safety risks of generating harmful content. A key challenge of existing concept erasure methods is the precise removal of target concepts while minimizing degradation of image quality. In this paper, we propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can precisely prevent harmful content generation by manipulating only a single neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text embeddings into a sparse, disentangled latent space, where individual neurons align tightly with atomic semantic concepts. To accurately locate neurons responsible for harmful concepts, we design a novel neuron identification method based on the modulated frequency scoring of activation patterns. By suppressing activations of the harmful concept-specific neuron, SNCE achieves surgical precision in concept erasure with minimal disruption to image quality. Experiments on various benchmarks demonstrate that SNCE achieves state-of-the-art results in target concept erasure, while preserving the model's generation capabilities for non-target concepts. Additionally, our method exhibits strong robustness against adversarial attacks, significantly outperforming existing methods.</li>
</ul>

<h3>Title: ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Haotian Guo, Hui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21010">https://arxiv.org/abs/2509.21010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21010">https://arxiv.org/pdf/2509.21010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21010]] ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning(https://arxiv.org/abs/2509.21010)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generation of high-quality candidate molecules remains a central challenge in AI-driven drug design. Current phenotype-based and target-based strategies each suffer limitations, either incurring high experimental costs or overlook system-level cellular responses. To bridge this gap, we propose ExMoIRL, a novel generative framework that synergistically integrates phenotypic and target-specific cues for de novo molecular generation. The phenotype-guided generator is first pretrained on expansive drug-induced transcriptional profiles and subsequently fine-tuned via multi-objective reinforcement learning (RL). Crucially, the reward function fuses docking affinity and drug-likeness scores, augmented with ranking loss, prior-likelihood regularization, and entropy maximization. The multi-objective RL steers the model toward chemotypes that are simultaneously potent, diverse, and aligned with the specified phenotypic effects. Extensive experiments demonstrate ExMoIRL's superior performance over state-of-the-art phenotype-based and target-based models across multiple well-characterized targets. Our generated molecules exhibit favorable drug-like properties, high target affinity, and inhibitory potency (IC50) against cancer cells. This unified framework showcases the synergistic potential of combining phenotype-guided and target-aware strategies, offering a more effective solution for de novo drug discovery.</li>
</ul>

<h3>Title: Mechanism of Task-oriented Information Removal in In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Hakaze Cho, Haolin Yang, Gouki Minegishi, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21012">https://arxiv.org/abs/2509.21012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21012">https://arxiv.org/pdf/2509.21012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21012]] Mechanism of Task-oriented Information Removal in In-context Learning(https://arxiv.org/abs/2509.21012)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.</li>
</ul>

<h3>Title: Actor-Critic without Actor</h3>
<ul>
<li><strong>Authors: </strong>Donghyeon Ki, Hee-Jun Ahn, Kyungyoon Kim, Byung-Jun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21022">https://arxiv.org/abs/2509.21022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21022">https://arxiv.org/pdf/2509.21022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21022]] Actor-Critic without Actor(https://arxiv.org/abs/2509.21022)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Actor-critic methods constitute a central paradigm in reinforcement learning (RL), coupling policy evaluation with policy improvement. While effective across many domains, these methods rely on separate actor and critic networks, which makes training vulnerable to architectural decisions and hyperparameter tuning. Such complexity limits their scalability in settings that require large function approximators. Recently, diffusion models have recently been proposed as expressive policies that capture multi-modal behaviors and improve exploration, but they introduce additional design choices and computational burdens, hindering efficient deployment. We introduce Actor-Critic without Actor (ACA), a lightweight framework that eliminates the explicit actor network and instead generates actions directly from the gradient field of a noise-level critic. This design removes the algorithmic and computational overhead of actor training while keeping policy improvement tightly aligned with the critic's latest value estimates. Moreover, ACA retains the ability to capture diverse, multi-modal behaviors without relying on diffusion-based actors, combining simplicity with expressiveness. Through extensive experiments on standard online RL benchmarks,ACA achieves more favorable learning curves and competitive performance compared to both standard actor-critic and state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.</li>
</ul>

<h3>Title: Generative AI for FFRDCs</h3>
<ul>
<li><strong>Authors: </strong>Arun S. Maiya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21040">https://arxiv.org/abs/2509.21040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21040">https://arxiv.org/pdf/2509.21040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21040]] Generative AI for FFRDCs(https://arxiv.org/abs/2509.21040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Federally funded research and development centers (FFRDCs) face text-heavy workloads, from policy documents to scientific and engineering papers, that are slow to analyze manually. We show how large language models can accelerate summarization, classification, extraction, and sense-making with only a few input-output examples. To enable use in sensitive government contexts, we apply OnPrem$.$LLM, an open-source framework for secure and flexible application of generative AI. Case studies on defense policy documents and scientific corpora, including the National Defense Authorization Act (NDAA) and National Science Foundation (NSF) Awards, demonstrate how this approach enhances oversight and strategic analysis while maintaining auditability and data sovereignty.</li>
</ul>

<h3>Title: Physics of Learning: A Lagrangian perspective to different learning paradigms</h3>
<ul>
<li><strong>Authors: </strong>Siyuan Guo, Bernhard Schlkopf</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21049">https://arxiv.org/abs/2509.21049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21049">https://arxiv.org/pdf/2509.21049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21049]] Physics of Learning: A Lagrangian perspective to different learning paradigms(https://arxiv.org/abs/2509.21049)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the problem of building an efficient learning system. Efficient learning processes information in the least time, i.e., building a system that reaches a desired error threshold with the least number of observations. Building upon least action principles from physics, we derive classic learning algorithms, Bellman's optimality equation in reinforcement learning, and the Adam optimizer in generative models from first principles, i.e., the Learning $\textit{Lagrangian}$. We postulate that learning searches for stationary paths in the Lagrangian, and learning algorithms are derivable by seeking the stationary trajectories.</li>
</ul>

<h3>Title: SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Sedjro Salomon Hotegni, Sebastian Peitz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21058">https://arxiv.org/abs/2509.21058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21058">https://arxiv.org/pdf/2509.21058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21058]] SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion(https://arxiv.org/abs/2509.21058)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Developing efficient multi-objective optimization methods to compute the Pareto set of optimal compromises between conflicting objectives remains a key challenge, especially for large-scale and expensive problems. To bridge this gap, we introduce SPREAD, a generative framework based on Denoising Diffusion Probabilistic Models (DDPMs). SPREAD first learns a conditional diffusion process over points sampled from the decision space and then, at each reverse diffusion step, refines candidates via a sampling scheme that uses an adaptive multiple gradient descent-inspired update for fast convergence alongside a Gaussian RBF-based repulsion term for diversity. Empirical results on multi-objective optimization benchmarks, including offline and Bayesian surrogate-based settings, show that SPREAD matches or exceeds leading baselines in efficiency, scalability, and Pareto front coverage.</li>
</ul>

<h3>Title: SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials</h3>
<ul>
<li><strong>Authors: </strong>Qixin Wan, Zilong Wang, Jingwen Zhou, Wanting Wang, Ziheng Geng, Jiachen Liu, Ran Cao, Minghui Cheng, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21079">https://arxiv.org/abs/2509.21079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21079">https://arxiv.org/pdf/2509.21079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21079]] SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials(https://arxiv.org/abs/2509.21079)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have shown remarkable capabilities in various domains, but their performance on complex, multimodal engineering problems remains largely unexplored. We introduce SoM-1K, the first large-scale multimodal benchmark dataset dedicated to evaluating foundation models on problems in the strength of materials (SoM). The dataset, which contains 1,065 annotated SoM problems, mirrors real-world engineering tasks by including both textual problem statements and schematic diagrams. Due to the limited capabilities of current foundation models in understanding complicated visual information, we propose a novel prompting strategy called Descriptions of Images (DoI), which provides rigorous expert-generated text descriptions of the visual diagrams as the context. We evaluate eight representative foundation models, including both large language models (LLMs) and vision language models (VLMs). Our results show that current foundation models struggle significantly with these engineering problems, with the best-performing model achieving only 56.6% accuracy. Interestingly, we found that LLMs, when provided with DoI, often outperform VLMs provided with visual diagrams. A detailed error analysis reveals that DoI plays a crucial role in mitigating visual misinterpretation errors, suggesting that accurate text-based descriptions can be more effective than direct image input for current foundation models. This work establishes a rigorous benchmark for engineering AI and highlights a critical need for developing more robust multimodal reasoning capabilities in foundation models, particularly in scientific and engineering contexts.</li>
</ul>

<h3>Title: Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yixin Wan, Xingrun Chen, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21080">https://arxiv.org/abs/2509.21080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21080">https://arxiv.org/pdf/2509.21080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21080]] Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and Agentic Mitigation in LLMs(https://arxiv.org/abs/2509.21080)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have unlocked a wide range of downstream generative applications. However, we found that they also risk perpetuating subtle fairness issues tied to culture, positioning their generations from the perspectives of the mainstream US culture while demonstrating salient externality towards non-mainstream ones. In this work, we identify and systematically investigate this novel culture positioning bias, in which an LLM's default generative stance aligns with a mainstream view and treats other cultures as outsiders. We propose the CultureLens benchmark with 4000 generation prompts and 3 evaluation metrics for quantifying this bias through the lens of a culturally situated interview script generation task, in which an LLM is positioned as an onsite reporter interviewing local people across 10 diverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a stark pattern: while models adopt insider tones in over 88 percent of US-contexted scripts on average, they disproportionately adopt mainly outsider stances for less dominant cultures. To resolve these biases, we propose 2 inference-time mitigation methods: a baseline prompt-based Fairness Intervention Pillars (FIP) method, and a structured Mitigation via Fairness Agents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent) introduces a self-reflection and rewriting loop based on fairness guidelines. (2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized agents: a Planner Agent(initial script generation), a Critique Agent (evaluates initial script against fairness pillars), and a Refinement Agent (incorporates feedback to produce a polished, unbiased script). Empirical results showcase the effectiveness of agent-based methods as a promising direction for mitigating biases in generative LLMs.</li>
</ul>

<h3>Title: UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Guojun Lei, Rong Zhang, Chi Wang, Tianhang Liu, Hong Li, Zhiyuan Ma, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21086">https://arxiv.org/abs/2509.21086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21086">https://arxiv.org/pdf/2509.21086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21086]] UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition(https://arxiv.org/abs/2509.21086)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>We propose a novel architecture UniTransfer, which introduces both spatial and diffusion timestep decomposition in a progressive paradigm, achieving precise and controllable video concept transfer. Specifically, in terms of spatial decomposition, we decouple videos into three key components: the foreground subject, the background, and the motion flow. Building upon this decomposed formulation, we further introduce a dual-to-single-stream DiT-based architecture for supporting fine-grained control over different components in the videos. We also introduce a self-supervised pretraining strategy based on random masking to enhance the decomposed representation learning from large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning paradigm, we further revisit the denoising diffusion process and propose a Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We decompose the denoising process into three stages of different granularity and leverage large language models (LLMs) for stage-specific instructions to guide the generation progressively. We also curate an animal-centric video dataset called OpenAnimal to facilitate the advancement and benchmarking of research in video concept transfer. Extensive experiments demonstrate that our method achieves high-quality and controllable video concept transfer across diverse reference images and scenes, surpassing existing baselines in both visual fidelity and editability. Web Page: this https URL</li>
</ul>

<h3>Title: GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization</h3>
<ul>
<li><strong>Authors: </strong>Louis Van Langendonck, Guillermo Bernrdez, Nina Miolane, Pere Barlet-Ros</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21097">https://arxiv.org/abs/2509.21097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21097">https://arxiv.org/pdf/2509.21097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21097]] GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization(https://arxiv.org/abs/2509.21097)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>A fundamental challenge in graph learning is understanding how models generalize to new, unseen graphs. While synthetic benchmarks offer controlled settings for analysis, existing approaches are confined to single-graph, transductive settings where models train and test on the same graph structure. Addressing this gap, we introduce GraphUniverse, a framework for generating entire families of graphs to enable the first systematic evaluation of inductive generalization at scale. Our core innovation is the generation of graphs with persistent semantic communities, ensuring conceptual consistency while allowing fine-grained control over structural properties like homophily and degree distributions. This enables crucial but underexplored robustness tests, such as performance under controlled distribution shifts. Benchmarking a wide range of architectures -- from GNNs to graph transformers and topological architectures -- reveals that strong transductive performance is a poor predictor of inductive generalization. Furthermore, we find that robustness to distribution shift is highly sensitive not only to model architecture choice but also to the initial graph regime (e.g., high vs. low homophily). Beyond benchmarking, GraphUniverse's flexibility and scalability can facilitate the development of robust and truly generalizable architectures -- including next-generation graph foundation models. An interactive demo is available at this https URL.</li>
</ul>

<h3>Title: MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Guojun Lei, Chi Wang, Yikai Wang, Hong Li, Ying Song, Weiwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21119">https://arxiv.org/abs/2509.21119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21119">https://arxiv.org/pdf/2509.21119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21119]] MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation(https://arxiv.org/abs/2509.21119)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating videos guided by camera trajectories poses significant challenges in achieving consistency and generalizability, particularly when both camera and object motions are present. Existing approaches often attempt to learn these motions separately, which may lead to confusion regarding the relative motion between the camera and the objects. To address this challenge, we propose a novel approach that integrates both camera and object motions by converting them into the motion of corresponding pixels. Utilizing a stable diffusion network, we effectively learn reference motion maps in relation to the specified camera trajectory. These maps, along with an extracted semantic object prior, are then fed into an image-to-video network to generate the desired video that can accurately follow the designated camera trajectory while maintaining consistent object motions. Extensive experiments verify that our model outperforms SOTA methods by a large margin.</li>
</ul>

<h3>Title: The Unwinnable Arms Race of AI Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Till Aczel, Lorenzo Vettor, Andreas Plesner, Roger Wattenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21135">https://arxiv.org/abs/2509.21135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21135">https://arxiv.org/pdf/2509.21135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21135]] The Unwinnable Arms Race of AI Image Detection(https://arxiv.org/abs/2509.21135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid progress of image generative AI has blurred the boundary between synthetic and real images, fueling an arms race between generators and discriminators. This paper investigates the conditions under which discriminators are most disadvantaged in this competition. We analyze two key factors: data dimensionality and data complexity. While increased dimensionality often strengthens the discriminators ability to detect subtle inconsistencies, complexity introduces a more nuanced effect. Using Kolmogorov complexity as a measure of intrinsic dataset structure, we show that both very simple and highly complex datasets reduce the detectability of synthetic images; generators can learn simple datasets almost perfectly, whereas extreme diversity masks imperfections. In contrast, intermediate-complexity datasets create the most favorable conditions for detection, as generators fail to fully capture the distribution and their errors remain visible.</li>
</ul>

<h3>Title: A Unified Framework for Diffusion Model Unlearning with f-Divergence</h3>
<ul>
<li><strong>Authors: </strong>Nicola Novello, Federico Fontana, Luigi Cinque, Deniz Gunduz, Andrea M. Tonello</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21167">https://arxiv.org/abs/2509.21167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21167">https://arxiv.org/pdf/2509.21167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21167]] A Unified Framework for Diffusion Model Unlearning with f-Divergence(https://arxiv.org/abs/2509.21167)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to remove specific knowledge from a trained model. While diffusion models (DMs) have shown remarkable generative capabilities, existing unlearning methods for text-to-image (T2I) models often rely on minimizing the mean squared error (MSE) between the output distribution of a target and an anchor concept. We show that this MSE-based approach is a special case of a unified $f$-divergence-based framework, in which any $f$-divergence can be utilized. We analyze the benefits of using different $f$-divergences, that mainly impact the convergence properties of the algorithm and the quality of unlearning. The proposed unified framework offers a flexible paradigm that allows to select the optimal divergence for a specific application, balancing different trade-offs between aggressive unlearning and concept preservation.</li>
</ul>

<h3>Title: Who's Laughing Now? An Overview of Computational Humour Generation and Explanation</h3>
<ul>
<li><strong>Authors: </strong>Tyler Loakman, William Thorne, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21175">https://arxiv.org/abs/2509.21175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21175">https://arxiv.org/pdf/2509.21175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21175]] Who's Laughing Now? An Overview of Computational Humour Generation and Explanation(https://arxiv.org/abs/2509.21175)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The creation and perception of humour is a fundamental human trait, positioning its computational understanding as one of the most challenging tasks in natural language processing (NLP). As an abstract, creative, and frequently context-dependent construct, humour requires extensive reasoning to understand and create, making it a pertinent task for assessing the common-sense knowledge and reasoning abilities of modern large language models (LLMs). In this work, we survey the landscape of computational humour as it pertains to the generative tasks of creation and explanation. We observe that, despite the task of understanding humour bearing all the hallmarks of a foundational NLP task, work on generating and explaining humour beyond puns remains sparse, while state-of-the-art models continue to fall short of human capabilities. We bookend our literature survey by motivating the importance of computational humour processing as a subdiscipline of NLP and presenting an extensive discussion of future directions for research in the area that takes into account the subjective and ethically ambiguous nature of humour.</li>
</ul>

<h3>Title: Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy</h3>
<ul>
<li><strong>Authors: </strong>Tian Lan, Hao Duong Le, Jinbo Li, Wenjun He, Meng Wang, Chenghao Liu, Chen Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21190">https://arxiv.org/abs/2509.21190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21190">https://arxiv.org/pdf/2509.21190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21190]] Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy(https://arxiv.org/abs/2509.21190)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) is a critical task, but developing models that generalize to unseen data in a zero-shot manner remains a major challenge. Prevailing foundation models for TSAD predominantly rely on reconstruction-based objectives, which suffer from a fundamental objective mismatch: they struggle to identify subtle anomalies while often misinterpreting complex normal patterns, leading to high rates of false negatives and positives. To overcome these limitations, we introduce \texttt{TimeRCD}, a novel foundation model for TSAD built upon a new pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning to reconstruct inputs, \texttt{TimeRCD} is explicitly trained to identify anomalies by detecting significant discrepancies between adjacent time windows. This relational approach, implemented with a standard Transformer architecture, enables the model to capture contextual shifts indicative of anomalies that reconstruction-based methods often miss. To facilitate this paradigm, we develop a large-scale, diverse synthetic corpus with token-level anomaly labels, providing the rich supervisory signal necessary for effective pre-training. Extensive experiments demonstrate that \texttt{TimeRCD} significantly outperforms existing general-purpose and anomaly-specific foundation models in zero-shot TSAD across diverse datasets. Our results validate the superiority of the RCD paradigm and establish a new, effective path toward building robust and generalizable foundation models for time series anomaly detection.</li>
</ul>

<h3>Title: Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets</h3>
<ul>
<li><strong>Authors: </strong>Team Hunyuan3D: Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Linus, Penghao Wang, Qingxiang Lin, Sicong Liu, Xianghui Yang, Yixuan Tang, Yunfei Zhao, Zeqiang Lai, Zhihao Liang, Zibo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21245">https://arxiv.org/abs/2509.21245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21245">https://arxiv.org/pdf/2509.21245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21245]] Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets(https://arxiv.org/abs/2509.21245)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.</li>
</ul>

<h3>Title: Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations</h3>
<ul>
<li><strong>Authors: </strong>Zhijian Yang, Noel DSouza, Istvan Megyeri, Xiaojian Xu, Amin Honarmandi Shandiz, Farzin Haddadpour, Krisztian Koos, Laszlo Rusko, Emanuele Valeriano, Bharadwaj Swaninathan, Lei Wu, Parminder Bhatia, Taha Kass-Hout, Erhan Bas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21249">https://arxiv.org/abs/2509.21249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21249">https://arxiv.org/pdf/2509.21249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21249]] Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations(https://arxiv.org/abs/2509.21249)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in clinical diagnosis and research, yet its complexity and heterogeneity pose challenges for automated analysis, particularly in scalable and generalizable machine learning applications. While foundation models have revolutionized natural language and vision tasks, their application to MRI remains limited due to data scarcity and narrow anatomical focus. In this work, we present Decipher-MR, a 3D MRI-specific vision-language foundation model trained on a large-scale dataset comprising 200,000 MRI series from over 22,000 studies spanning diverse anatomical regions, sequences, and pathologies. Decipher-MR integrates self-supervised vision learning with report-guided text supervision to build robust, generalizable representations, enabling effective adaptation across broad applications. To enable robust and diverse clinical tasks with minimal computational overhead, Decipher-MR supports a modular design that enables tuning of lightweight, task-specific decoders attached to a frozen pretrained encoder. Following this setting, we evaluate Decipher-MR across diverse benchmarks including disease classification, demographic prediction, anatomical localization, and cross-modal retrieval, demonstrating consistent performance gains over existing foundation models and task-specific approaches. Our results establish Decipher-MR as a scalable and versatile foundation for MRI-based AI, facilitating efficient development across clinical and research domains.</li>
</ul>

<h3>Title: Federated Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zifan Wang, Anqi Dong, Mahmoud Selim, Michael M. Zavlanos, Karl H. Johansson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21250">https://arxiv.org/abs/2509.21250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21250">https://arxiv.org/pdf/2509.21250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21250]] Federated Flow Matching(https://arxiv.org/abs/2509.21250)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data today is decentralized, generated and stored across devices and institutions where privacy, ownership, and regulation prevent centralization. This motivates the need to train generative models directly from distributed data locally without central aggregation. In this paper, we introduce Federated Flow Matching (FFM), a framework for training flow matching models under privacy constraints. Specifically, we first examine FFM-vanilla, where each client trains locally with independent source and target couplings, preserving privacy but yielding curved flows that slow inference. We then develop FFM-LOT, which employs local optimal transport couplings to improve straightness within each client but lacks global consistency under heterogeneous data. Finally, we propose FFM-GOT, a federated strategy based on the semi-dual formulation of optimal transport, where a shared global potential function coordinates couplings across clients. Experiments on synthetic and image datasets show that FFM enables privacy-preserving training while enhancing both the flow straightness and sample quality in federated settings, with performance comparable to the centralized baseline.</li>
</ul>

<h3>Title: Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Seyed Amir Kasaei, Mohammad Hossein Rohban</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21257">https://arxiv.org/abs/2509.21257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21257">https://arxiv.org/pdf/2509.21257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21257]] Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation(https://arxiv.org/abs/2509.21257)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In language and vision-language models, hallucination is broadly understood as content generated from a model's prior knowledge or biases rather than from the given input. While this phenomenon has been studied in those domains, it has not been clearly framed for text-to-image (T2I) generative models. Existing evaluations mainly focus on alignment, checking whether prompt-specified elements appear, but overlook what the model generates beyond the prompt. We argue for defining hallucination in T2I as bias-driven deviations and propose a taxonomy with three categories: attribute, relation, and object hallucinations. This framing introduces an upper bound for evaluation and surfaces hidden biases, providing a foundation for richer assessment of T2I models.</li>
</ul>

<h3>Title: Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication</h3>
<ul>
<li><strong>Authors: </strong>Evgeny Kaskov, Elizaveta Petrova, Petr Surovtsev, Anna Kostikova, Ilya Mistiurin, Alexander Kapitanov, Alexander Nagaev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21262">https://arxiv.org/abs/2509.21262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21262">https://arxiv.org/pdf/2509.21262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21262]] Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication(https://arxiv.org/abs/2509.21262)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Homonyms are words with identical spelling but distinct meanings, which pose challenges for many generative models. When a homonym appears in a prompt, diffusion models may generate multiple senses of the word simultaneously, which is known as homonym duplication. This issue is further complicated by an Anglocentric bias, which includes an additional translation step before the text-to-image model pipeline. As a result, even words that are not homonymous in the original language may become homonyms and lose their meaning after translation into English. In this paper, we introduce a method for measuring duplication rates and conduct evaluations of different diffusion models using both automatic evaluation utilizing Vision-Language Models (VLM) and human evaluation. Additionally, we investigate methods to mitigate the homonym duplication problem through prompt expansion, demonstrating that this approach also effectively reduces duplication related to Anglocentric bias. The code for the automatic evaluation pipeline is publicly available.</li>
</ul>

<h3>Title: Dense Semantic Matching with VGGT Prior</h3>
<ul>
<li><strong>Authors: </strong>Songlin Yang, Tianyi Wei, Yushi Lan, Zeqi Xiao, Anyi Rao, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21263">https://arxiv.org/abs/2509.21263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21263">https://arxiv.org/pdf/2509.21263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21263]] Dense Semantic Matching with VGGT Prior(https://arxiv.org/abs/2509.21263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Semantic matching aims to establish pixel-level correspondences between instances of the same category and represents a fundamental task in computer vision. Existing approaches suffer from two limitations: (i) Geometric Ambiguity: Their reliance on 2D foundation model features (e.g., Stable Diffusion, DINO) often fails to disambiguate symmetric structures, requiring extra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their pixel-wise matching ignores cross-image invisibility and neglects manifold preservation. These challenges call for geometry-aware pixel descriptors and holistic dense correspondence mechanisms. Inspired by recent advances in 3D geometric foundation models, we turn to VGGT, which provides geometry-grounded features and holistic dense matching capabilities well aligned with these needs. However, directly transferring VGGT is challenging, as it was originally designed for geometry matching within cross views of a single instance, misaligned with cross-instance semantic matching, and further hindered by the scarcity of dense semantic annotations. To address this, we propose an approach that (i) retains VGGT's intrinsic strengths by reusing early feature stages, fine-tuning later ones, and adding a semantic head for bidirectional correspondences; and (ii) adapts VGGT to the semantic matching scenario under data scarcity through cycle-consistent training strategy, synthetic data augmentation, and progressive training recipe with aliasing artifact mitigation. Extensive experiments demonstrate that our approach achieves superior geometry awareness, matching reliability, and manifold preservation, outperforming previous baselines.</li>
</ul>

<h3>Title: A Sentinel-3 foundation model for ocean colour</h3>
<ul>
<li><strong>Authors: </strong>Geoffrey Dawson, Remy Vandaele, Andrew Taylor, David Moffat, Helen Tamura-Wicks, Sarah Jackson, Rosie Lickorish, Paolo Fraccaro, Hywel Williams, Chunbo Luo, Anne Jones</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21273">https://arxiv.org/abs/2509.21273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21273">https://arxiv.org/pdf/2509.21273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21273]] A Sentinel-3 foundation model for ocean colour(https://arxiv.org/abs/2509.21273)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive unlabelled datasets, have the potential to drastically change AI applications in ocean science, where labelled data are often sparse and expensive to collect. In this work, we describe a new foundation model using the Prithvi-EO Vision Transformer architecture which has been pre-trained to reconstruct data from the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the model by fine-tuning on two downstream marine earth observation tasks. We first assess model performance compared to current baseline models used to quantify chlorophyll concentration. We then evaluate the FMs ability to refine remote sensing-based estimates of ocean primary production. Our results demonstrate the utility of self-trained FMs for marine monitoring, in particular for making use of small amounts of high quality labelled data and in capturing detailed spatial patterns of ocean colour whilst matching point observations. We conclude that this new generation of geospatial AI models has the potential to provide more robust, data-driven insights into ocean ecosystems and their role in global climate processes.</li>
</ul>

<h3>Title: Does FLUX Already Know How to Perform Physically Plausible Image Composition?</h3>
<ul>
<li><strong>Authors: </strong>Shilin Lu, Zhuming Lian, Zihan Zhou, Shaocong Zhang, Chen Zhao, Adams Wai-Kin Kong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21278">https://arxiv.org/abs/2509.21278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21278">https://arxiv.org/pdf/2509.21278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21278]] Does FLUX Already Know How to Perform Physically Plausible Image Composition?(https://arxiv.org/abs/2509.21278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.</li>
</ul>

<h3>Title: DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding</h3>
<ul>
<li><strong>Authors: </strong>Kin Ian Lo, Hala Hawashin, Mina Abbaszadeh, Tilen Limback-Stokin, Hadi Wazni, Mehrnoosh Sadrzadeh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21287">https://arxiv.org/abs/2509.21287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21287">https://arxiv.org/pdf/2509.21287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21287]] DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding(https://arxiv.org/abs/2509.21287)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent vision-language models excel at large-scale image-text alignment but often neglect the compositional structure of language, leading to failures on tasks that hinge on word order and predicate-argument structure. We introduce DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer with a novel tensor network text encoder that explicitly encodes syntactic structure. Sentences are parsed with a Combinatory Categorial Grammar parser to yield distributional word tensors whose contractions mirror the sentence's grammatical derivation. To keep the model efficient, high-order tensors are factorized with tensor decompositions, reducing parameter count from tens of millions to under one million. Trained end-to-end with a self-supervised contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%, boosts ARO attribution and relation scores by over 9% and 4%, and achieves 93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that embedding explicit linguistic structure via tensor networks yields interpretable, parameter-efficient representations that substantially improve compositional reasoning in vision-language tasks.</li>
</ul>

<h3>Title: The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages</h3>
<ul>
<li><strong>Authors: </strong>Pranjal A. Chitale, Varun Gumma, Sanchit Ahuja, Prashant Kodali, Manan Uppadhyay, Deepthi Sudharsan, Sunayana Sitaram</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21294">https://arxiv.org/abs/2509.21294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21294">https://arxiv.org/pdf/2509.21294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21294]] The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages(https://arxiv.org/abs/2509.21294)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing AI systems that operate effectively across languages while remaining culturally grounded is a long-standing challenge, particularly in low-resource settings. Synthetic data provides a promising avenue, yet its effectiveness in multilingual and multicultural contexts remains underexplored. We investigate the creation and impact of synthetic, culturally contextualized datasets for Indian languages through a bottom-up generation strategy that prompts large open-source LLMs (>= 235B parameters) to ground data generation in language-specific Wikipedia content. This approach complements the dominant top-down paradigm of translating synthetic datasets from high-resource languages such as English. We introduce Updesh, a high-quality large-scale synthetic instruction-following dataset comprising 9.5M data points across 13 Indian languages, encompassing diverse reasoning and generative tasks with an emphasis on long-context, multi-turn capabilities, and alignment with Indian cultural contexts. A comprehensive evaluation incorporating both automated metrics and human annotation across 10k assessments indicates that generated data is high quality; though, human evaluation highlights areas for further improvement. Additionally, we perform downstream evaluations by fine-tuning models on our dataset and assessing the performance across 15 diverse multilingual datasets. Models trained on Updesh consistently achieve significant gains on generative tasks and remain competitive on multiple-choice style NLU tasks. Notably, relative improvements are most pronounced in low and medium-resource languages, narrowing their gap with high-resource languages. These findings provide empirical evidence that effective multilingual AI requires multi-faceted data curation and generation strategies that incorporate context-aware, culturally grounded methodologies.</li>
</ul>

<h3>Title: SD3.5-Flash: Distribution-Guided Distillation of Generative Flows</h3>
<ul>
<li><strong>Authors: </strong>Hmrishav Bandyopadhyay, Rahim Entezari, Jim Scott, Reshinth Adithyan, Yi-Zhe Song, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21318">https://arxiv.org/abs/2509.21318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21318">https://arxiv.org/pdf/2509.21318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21318]] SD3.5-Flash: Distribution-Guided Distillation of Generative Flows(https://arxiv.org/abs/2509.21318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: "timestep sharing" to reduce gradient noise and "split-timestep fine-tuning" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.</li>
</ul>

<h3>Title: SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Wang, Chen Tang, Han Deng, Jiabei Xiao, Jiaqi Liu, Jianyu Wu, Jun Yao, Pengze Li, Encheng Su, Lintao Wang, Guohang Zhuang, Yuchen Ren, Ben Fei, Ming Hu, Xin Chen, Dongzhan Zhou, Junjun He, Xiangyu Yue, Zhenfei Yin, Jiamin Wu, Qihao Zheng, Yuhao Zhou, Huihui Xu, Chenglong Ma, Yan Lu, Wenlong Zhang, Chunfeng Song, Philip Torr, Shixiang Tang, Xinzhu Ma, Wanli Ouyang, Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.21320">https://arxiv.org/abs/2509.21320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.21320">https://arxiv.org/pdf/2509.21320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.21320]] SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines(https://arxiv.org/abs/2509.21320)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at this https URL and this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
