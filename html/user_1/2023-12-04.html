<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Unsupervised Keypoints from Pretrained Diffusion Models. (arXiv:2312.00065v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00065">http://arxiv.org/abs/2312.00065</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00065]] Unsupervised Keypoints from Pretrained Diffusion Models(http://arxiv.org/abs/2312.00065)</code></li>
<li>Summary: <p>Unsupervised learning of keypoints and landmarks has seen significant
progress with the help of modern neural network architectures, but performance
is yet to match the supervised counterpart, making their practicability
questionable. We leverage the emergent knowledge within text-to-image diffusion
models, towards more robust unsupervised keypoints. Our core idea is to find
text embeddings that would cause the generative model to consistently attend to
compact regions in images (i.e. keypoints). To do so, we simply optimize the
text embedding such that the cross-attention maps within the denoising network
are localized as Gaussians with small standard deviations. We validate our
performance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD,
DeepFashion, and Human3.6m datasets. We achieve significantly improved
accuracy, sometimes even outperforming supervised ones, particularly for data
that is non-aligned and less curated. Our code is publicly available and can be
found through our project page: https://ubc-vision.github.io/StableKeypoints/
</p></li>
</ul>

<h3>Title: HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models. (arXiv:2312.00079v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00079">http://arxiv.org/abs/2312.00079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00079]] HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models(http://arxiv.org/abs/2312.00079)</code></li>
<li>Summary: <p>This paper explores advancements in high-fidelity personalized image
generation through the utilization of pre-trained text-to-image diffusion
models. While previous approaches have made significant strides in generating
versatile scenes based on text descriptions and a few input images, challenges
persist in maintaining the subject fidelity within the generated images. In
this work, we introduce an innovative algorithm named HiFi Tuner to enhance the
appearance preservation of objects during personalized image generation. Our
proposed method employs a parameter-efficient fine-tuning framework, comprising
a denoising process and a pivotal inversion process. Key enhancements include
the utilization of mask guidance, a novel parameter regularization technique,
and the incorporation of step-wise subject representations to elevate the
sample fidelity. Additionally, we propose a reference-guided generation
approach that leverages the pivotal inversion of a reference image to mitigate
unwanted subject variations and artifacts. We further extend our method to a
novel image editing task: substituting the subject in an image through textual
manipulations. Experimental evaluations conducted on the DreamBooth dataset
using the Stable Diffusion model showcase promising results. Fine-tuning solely
on textual embeddings improves CLIP-T score by 3.6 points and improves DINO
score by 9.6 points over Textual Inversion. When fine-tuning all parameters,
HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2
points over DreamBooth, establishing a new state of the art.
</p></li>
</ul>

<h3>Title: Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?. (arXiv:2312.00084v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00084">http://arxiv.org/abs/2312.00084</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00084]] Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?(http://arxiv.org/abs/2312.00084)</code></li>
<li>Summary: <p>Stable Diffusion has established itself as a foundation model in generative
AI artistic applications, receiving widespread research and application. Some
recent fine-tuning methods have made it feasible for individuals to implant
personalized concepts onto the basic Stable Diffusion model with minimal
computational costs on small datasets. However, these innovations have also
given rise to issues like facial privacy forgery and artistic copyright
infringement. In recent studies, researchers have explored the addition of
imperceptible adversarial perturbations to images to prevent potential
unauthorized exploitation and infringements when personal data is used for
fine-tuning Stable Diffusion. Although these studies have demonstrated the
ability to protect images, it is essential to consider that these methods may
not be entirely applicable in real-world scenarios. In this paper, we
systematically evaluate the use of perturbations to protect images within a
practical threat model. The results suggest that these approaches may not be
sufficient to safeguard image privacy and copyright effectively. Furthermore,
we introduce a purification method capable of removing protected perturbations
while preserving the original image structure to the greatest extent possible.
Experiments reveal that Stable Diffusion can effectively learn from purified
images over all protective methods.
</p></li>
</ul>

<h3>Title: X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation. (arXiv:2312.00085v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00085">http://arxiv.org/abs/2312.00085</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00085]] X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation(http://arxiv.org/abs/2312.00085)</code></li>
<li>Summary: <p>In recent times, automatic text-to-3D content creation has made significant
progress, driven by the development of pretrained 2D diffusion models. Existing
text-to-3D methods typically optimize the 3D representation to ensure that the
rendered image aligns well with the given text, as evaluated by the pretrained
2D diffusion model. Nevertheless, a substantial domain gap exists between 2D
images and 3D assets, primarily attributed to variations in camera-related
attributes and the exclusive presence of foreground objects. Consequently,
employing 2D diffusion models directly for optimizing 3D representations may
lead to suboptimal outcomes. To address this issue, we present X-Dreamer, a
novel approach for high-quality text-to-3D content creation that effectively
bridges the gap between text-to-2D and text-to-3D synthesis. The key components
of X-Dreamer are two innovative designs: Camera-Guided Low-Rank Adaptation
(CG-LoRA) and Attention-Mask Alignment (AMA) Loss. CG-LoRA dynamically
incorporates camera information into the pretrained diffusion models by
employing camera-dependent generation for trainable parameters. This
integration enhances the alignment between the generated 3D assets and the
camera's perspective. AMA loss guides the attention map of the pretrained
diffusion model using the binary mask of the 3D object, prioritizing the
creation of the foreground object. This module ensures that the model focuses
on generating accurate and detailed foreground objects. Extensive evaluations
demonstrate the effectiveness of our proposed method compared to existing
text-to-3D approaches. Our project webpage:
https://xmuxiaoma666.github.io/Projects/X-Dreamer .
</p></li>
</ul>

<h3>Title: GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs. (arXiv:2312.00093v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00093">http://arxiv.org/abs/2312.00093</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00093]] GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs(http://arxiv.org/abs/2312.00093)</code></li>
<li>Summary: <p>As pretrained text-to-image diffusion models become increasingly powerful,
recent efforts have been made to distill knowledge from these text-to-image
pretrained models for optimizing a text-guided 3D model. Most of the existing
methods generate a holistic 3D model from a plain text input. This can be
problematic when the text describes a complex scene with multiple objects,
because the vectorized text embeddings are inherently unable to capture a
complex description with multiple entities and relationships. Holistic 3D
modeling of the entire scene further prevents accurate grounding of text
entities and concepts. To address this limitation, we propose GraphDreamer, a
novel framework to generate compositional 3D scenes from scene graphs, where
objects are represented as nodes and their interactions as edges. By exploiting
node and edge information in scene graphs, our method makes better use of the
pretrained text-to-image diffusion model and is able to fully disentangle
different objects without image-level supervision. To facilitate modeling of
object-wise relationships, we use signed distance fields as representation and
impose a constraint to avoid inter-penetration of objects. To avoid manual
scene graph creation, we design a text prompt for ChatGPT to generate scene
graphs based on text inputs. We conduct both qualitative and quantitative
experiments to validate the effectiveness of GraphDreamer in generating
high-fidelity compositional 3D scenes with disentangled object entities.
</p></li>
</ul>

<h3>Title: Fast ODE-based Sampling for Diffusion Models in Around 5 Steps. (arXiv:2312.00094v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00094">http://arxiv.org/abs/2312.00094</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00094]] Fast ODE-based Sampling for Diffusion Models in Around 5 Steps(http://arxiv.org/abs/2312.00094)</code></li>
<li>Summary: <p>Sampling from diffusion models can be treated as solving the corresponding
ordinary differential equations (ODEs), with the aim of obtaining an accurate
solution with as few number of function evaluations (NFE) as possible.
Recently, various fast samplers utilizing higher-order ODE solvers have emerged
and achieved better performance than the initial first-order one. However,
these numerical methods inherently result in certain approximation errors,
which significantly degrades sample quality with extremely small NFE (e.g.,
around 5). In contrast, based on the geometric observation that each sampling
trajectory almost lies in a two-dimensional subspace embedded in the ambient
space, we propose Approximate MEan-Direction Solver (AMED-Solver) that
eliminates truncation errors by directly learning the mean direction for fast
diffusion sampling. Besides, our method can be easily used as a plugin to
further improve existing ODE-based samplers. Extensive experiments on image
synthesis with the resolution ranging from 32 to 256 demonstrate the
effectiveness of our method. With only 5 NFE, we achieve 7.14 FID on CIFAR-10,
13.75 FID on ImageNet 64$\times$64, and 12.79 FID on LSUN Bedroom. Our code is
available at https://github.com/zhyzhouu/amed-solver.
</p></li>
</ul>

<h3>Title: S2ST: Image-to-Image Translation in the Seed Space of Latent Diffusion. (arXiv:2312.00116v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00116">http://arxiv.org/abs/2312.00116</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00116]] S2ST: Image-to-Image Translation in the Seed Space of Latent Diffusion(http://arxiv.org/abs/2312.00116)</code></li>
<li>Summary: <p>Image-to-image translation (I2IT) refers to the process of transforming
images from a source domain to a target domain while maintaining a fundamental
connection in terms of image content. In the past few years, remarkable
advancements in I2IT were achieved by Generative Adversarial Networks (GANs),
which nevertheless struggle with translations requiring high precision.
Recently, Diffusion Models have established themselves as the engine of choice
for image generation. In this paper we introduce S2ST, a novel framework
designed to accomplish global I2IT in complex photorealistic images, such as
day-to-night or clear-to-rain translations of automotive scenes. S2ST operates
within the seed space of a Latent Diffusion Model, thereby leveraging the
powerful image priors learned by the latter. We show that S2ST surpasses
state-of-the-art GAN-based I2IT methods, as well as diffusion-based approaches,
for complex automotive scenes, improving fidelity while respecting the target
domain's appearance across a variety of domains. Notably, S2ST obviates the
necessity for training domain-specific translation networks.
</p></li>
</ul>

<h3>Title: DREAM: Diffusion Rectification and Estimation-Adaptive Models. (arXiv:2312.00210v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00210">http://arxiv.org/abs/2312.00210</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00210]] DREAM: Diffusion Rectification and Estimation-Adaptive Models(http://arxiv.org/abs/2312.00210)</code></li>
<li>Summary: <p>We present DREAM, a novel training framework representing Diffusion
Rectification and Estimation-Adaptive Models, requiring minimal code changes
(just three lines) yet significantly enhancing the alignment of training with
sampling in diffusion models. DREAM features two components: diffusion
rectification, which adjusts training to reflect the sampling process, and
estimation adaptation, which balances perception against distortion. When
applied to image super-resolution (SR), DREAM adeptly navigates the tradeoff
between minimizing distortion and preserving high image quality. Experiments
demonstrate DREAM's superiority over standard diffusion-based SR methods,
showing a $2$ to $3\times $ faster training convergence and a $10$ to
$20\times$ reduction in necessary sampling steps to achieve comparable or
superior results. We hope DREAM will inspire a rethinking of diffusion model
training paradigms.
</p></li>
</ul>

<h3>Title: Text-Guided 3D Face Synthesis -- From Generation to Editing. (arXiv:2312.00375v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00375">http://arxiv.org/abs/2312.00375</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00375]] Text-Guided 3D Face Synthesis -- From Generation to Editing(http://arxiv.org/abs/2312.00375)</code></li>
<li>Summary: <p>Text-guided 3D face synthesis has achieved remarkable results by leveraging
text-to-image (T2I) diffusion models. However, most existing works focus solely
on the direct generation, ignoring the editing, restricting them from
synthesizing customized 3D faces through iterative adjustments. In this paper,
we propose a unified text-guided framework from face generation to editing. In
the generation stage, we propose a geometry-texture decoupled generation to
mitigate the loss of geometric details caused by coupling. Besides, decoupling
enables us to utilize the generated geometry as a condition for texture
generation, yielding highly geometry-texture aligned results. We further employ
a fine-tuned texture diffusion model to enhance texture quality in both RGB and
YUV space. In the editing stage, we first employ a pre-trained diffusion model
to update facial geometry or texture based on the texts. To enable sequential
editing, we introduce a UV domain consistency preservation regularization,
preventing unintentional changes to irrelevant facial attributes. Besides, we
propose a self-guided consistency weight strategy to improve editing efficacy
while preserving consistency. Through comprehensive experiments, we showcase
our method's superiority in face synthesis. Project page:
https://faceg2e.github.io/.
</p></li>
</ul>

<h3>Title: SynFundus: Generating a synthetic fundus images dataset with millions of samples and multi-disease annotations. (arXiv:2312.00377v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00377">http://arxiv.org/abs/2312.00377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00377]] SynFundus: Generating a synthetic fundus images dataset with millions of samples and multi-disease annotations(http://arxiv.org/abs/2312.00377)</code></li>
<li>Summary: <p>In the field of medical imaging, the scarcity of large-scale datasets due to
privacy restrictions stands as a significant barrier to develop large models
for medical. To address this issue, we introduce SynFundus-1M, a high-quality
synthetic dataset with over 1 million retinal fundus images and extensive
disease and pathologies annotations, which is generated by a Denoising
Diffusion Probabilistic Model. The SynFundus-Generator and SynFundus-1M achieve
superior Frechet Inception Distance (FID) scores compared to existing methods
on main-stream public real datasets. Furthermore, the ophthalmologists
evaluation validate the difficulty in discerning these synthetic images from
real ones, confirming the SynFundus-1M's authenticity. Through extensive
experiments, we demonstrate that both CNN and ViT can benifit from SynFundus-1M
by pretraining or training directly. Compared to datasets like ImageNet or
EyePACS, models train on SynFundus-1M not only achieve better performance but
also faster convergence on various downstream tasks.
</p></li>
</ul>

<h3>Title: LucidDreaming: Controllable Object-Centric 3D Generation. (arXiv:2312.00588v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00588">http://arxiv.org/abs/2312.00588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00588]] LucidDreaming: Controllable Object-Centric 3D Generation(http://arxiv.org/abs/2312.00588)</code></li>
<li>Summary: <p>With the recent development of generative models, Text-to-3D generations have
also seen significant growth. Nonetheless, achieving precise control over 3D
generation continues to be an arduous task, as using text to control often
leads to missing objects and imprecise locations. Contemporary strategies for
enhancing controllability in 3D generation often entail the introduction of
additional parameters, such as customized diffusion models. This often induces
hardness in adapting to different diffusion models or creating distinct
objects.
</p>
<p>In this paper, we present LucidDreaming as an effective pipeline capable of
fine-grained control over 3D generation. It requires only minimal input of 3D
bounding boxes, which can be deduced from a simple text prompt using a Large
Language Model. Specifically, we propose clipped ray sampling to separately
render and optimize objects with user specifications. We also introduce
object-centric density blob bias, fostering the separation of generated
objects. With individual rendering and optimizing of objects, our method excels
not only in controlled content generation from scratch but also within the
pre-trained NeRF scenes. In such scenarios, existing generative approaches
often disrupt the integrity of the original scene, and current editing methods
struggle to synthesize new content in empty spaces. We show that our method
exhibits remarkable adaptability across a spectrum of mainstream Score
Distillation Sampling-based 3D generation frameworks, and achieves superior
alignment of 3D content when compared to baseline approaches. We also provide a
dataset of prompts with 3D bounding boxes, benchmarking 3D spatial
controllability.
</p></li>
</ul>

<h3>Title: TrackDiffusion: Multi-object Tracking Data Generation via Diffusion Models. (arXiv:2312.00651v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00651">http://arxiv.org/abs/2312.00651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00651]] TrackDiffusion: Multi-object Tracking Data Generation via Diffusion Models(http://arxiv.org/abs/2312.00651)</code></li>
<li>Summary: <p>Diffusion models have gained prominence in generating data for perception
tasks such as image classification and object detection. However, the potential
in generating high-quality tracking sequences, a crucial aspect in the field of
video perception, has not been fully investigated. To address this gap, we
propose TrackDiffusion, a novel architecture designed to generate continuous
video sequences from the tracklets. TrackDiffusion represents a significant
departure from the traditional layout-to-image (L2I) generation and copy-paste
synthesis focusing on static image elements like bounding boxes by empowering
image diffusion models to encompass dynamic and continuous tracking
trajectories, thereby capturing complex motion nuances and ensuring instance
consistency among video frames. For the first time, we demonstrate that the
generated video sequences can be utilized for training multi-object tracking
(MOT) systems, leading to significant improvement in tracker performance.
Experimental results show that our model significantly enhances instance
consistency in generated video sequences, leading to improved perceptual
metrics. Our approach achieves an improvement of 8.7 in TrackAP and 11.8 in
TrackAP$_{50}$ on the YTVIS dataset, underscoring its potential to redefine the
standards of video data generation for MOT tasks and beyond.
</p></li>
</ul>

<h3>Title: Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift. (arXiv:2312.00050v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00050">http://arxiv.org/abs/2312.00050</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00050]] Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift(http://arxiv.org/abs/2312.00050)</code></li>
<li>Summary: <p>Diffusion models (DM) have become state-of-the-art generative models because
of their capability to generate high-quality images from noises without
adversarial training. However, they are vulnerable to backdoor attacks as
reported by recent studies. When a data input (e.g., some Gaussian noise) is
stamped with a trigger (e.g., a white patch), the backdoored model always
generates the target image (e.g., an improper photo). However, effective
defense strategies to mitigate backdoors from DMs are underexplored. To bridge
this gap, we propose the first backdoor detection and removal framework for
DMs. We evaluate our framework Elijah on hundreds of DMs of 3 types including
DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks.
Extensive experiments show that our approach can have close to 100% detection
accuracy and reduce the backdoor effects to close to zero without significantly
sacrificing the model utility.
</p></li>
</ul>

<h3>Title: Resource-constrained knowledge diffusion processes inspired by human peer learning. (arXiv:2312.00660v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00660">http://arxiv.org/abs/2312.00660</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00660]] Resource-constrained knowledge diffusion processes inspired by human peer learning(http://arxiv.org/abs/2312.00660)</code></li>
<li>Summary: <p>We consider a setting where a population of artificial learners is given, and
the objective is to optimize aggregate measures of performance, under
constraints on training resources. The problem is motivated by the study of
peer learning in human educational systems. In this context, we study natural
knowledge diffusion processes in networks of interacting artificial learners.
By `natural', we mean processes that reflect human peer learning where the
students' internal state and learning process is mostly opaque, and the main
degree of freedom lies in the formation of peer learning groups by a
coordinator who can potentially evaluate the learners before assigning them to
peer groups. Among else, we empirically show that such processes indeed make
effective use of the training resources, and enable the design of modular
neural models that have the capacity to generalize without being prone to
overfitting noisy labels.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Towards Unsupervised Representation Learning: Learning, Evaluating and Transferring Visual Representations. (arXiv:2312.00101v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00101">http://arxiv.org/abs/2312.00101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00101]] Towards Unsupervised Representation Learning: Learning, Evaluating and Transferring Visual Representations(http://arxiv.org/abs/2312.00101)</code></li>
<li>Summary: <p>Unsupervised representation learning aims at finding methods that learn
representations from data without annotation-based signals. Abstaining from
annotations not only leads to economic benefits but may - and to some extent
already does - result in advantages regarding the representation's structure,
robustness, and generalizability to different tasks. In the long run,
unsupervised methods are expected to surpass their supervised counterparts due
to the reduction of human intervention and the inherently more general setup
that does not bias the optimization towards an objective originating from
specific annotation-based signals. While major advantages of unsupervised
representation learning have been recently observed in natural language
processing, supervised methods still dominate in vision domains for most tasks.
In this dissertation, we contribute to the field of unsupervised (visual)
representation learning from three perspectives: (i) Learning representations:
We design unsupervised, backpropagation-free Convolutional Self-Organizing
Neural Networks (CSNNs) that utilize self-organization- and Hebbian-based
learning rules to learn convolutional kernels and masks to achieve deeper
backpropagation-free models. (ii) Evaluating representations: We build upon the
widely used (non-)linear evaluation protocol to define pretext- and
target-objective-independent metrics for measuring and investigating the
objective function mismatch between various unsupervised pretext tasks and
target tasks. (iii) Transferring representations: We contribute CARLANE, the
first 3-way sim-to-real domain adaptation benchmark for 2D lane detection, and
a method based on prototypical self-supervised learning. Finally, we contribute
a content-consistent unpaired image-to-image translation method that utilizes
masks, global and local discriminators, and similarity sampling to mitigate
content inconsistencies.
</p></li>
</ul>

<h3>Title: DNS SLAM: Dense Neural Semantic-Informed SLAM. (arXiv:2312.00204v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00204">http://arxiv.org/abs/2312.00204</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00204]] DNS SLAM: Dense Neural Semantic-Informed SLAM(http://arxiv.org/abs/2312.00204)</code></li>
<li>Summary: <p>In recent years, coordinate-based neural implicit representations have shown
promising results for the task of Simultaneous Localization and Mapping (SLAM).
While achieving impressive performance on small synthetic scenes, these methods
often suffer from oversmoothed reconstructions, especially for complex
real-world scenes. In this work, we introduce DNS SLAM, a novel neural RGB-D
semantic SLAM approach featuring a hybrid representation. Relying only on 2D
semantic priors, we propose the first semantic neural SLAM method that trains
class-wise scene representations while providing stable camera tracking at the
same time. Our method integrates multi-view geometry constraints with
image-based feature extraction to improve appearance details and to output
color, density, and semantic class information, enabling many downstream
applications. To further enable real-time tracking, we introduce a lightweight
coarse scene representation which is trained in a self-supervised manner in
latent space. Our experimental results achieve state-of-the-art performance on
both synthetic data and real-world data tracking while maintaining a
commendable operational speed on off-the-shelf hardware. Further, our method
outputs class-wise decomposed reconstructions with better texture capturing
appearance and geometric details.
</p></li>
</ul>

<h3>Title: Learning Anatomically Consistent Embedding for Chest Radiography. (arXiv:2312.00335v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00335">http://arxiv.org/abs/2312.00335</a></li>
<li>Code URL: https://github.com/jlianglab/peac</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00335]] Learning Anatomically Consistent Embedding for Chest Radiography(http://arxiv.org/abs/2312.00335)</code></li>
<li>Summary: <p>Self-supervised learning (SSL) approaches have recently shown substantial
success in learning visual representations from unannotated images. Compared
with photographic images, medical images acquired with the same imaging
protocol exhibit high consistency in anatomy. To exploit this anatomical
consistency, this paper introduces a novel SSL approach, called PEAC (patch
embedding of anatomical consistency), for medical image analysis. Specifically,
in this paper, we propose to learn global and local consistencies via stable
grid-based matching, transfer pre-trained PEAC models to diverse downstream
tasks, and extensively demonstrate that (1) PEAC achieves significantly better
performance than the existing state-of-the-art fully/self-supervised methods,
and (2) PEAC captures the anatomical structure consistency across views of the
same patient and across patients of different genders, weights, and healthy
statuses, which enhances the interpretability of our method for medical image
analysis.
</p></li>
</ul>

<h3>Title: On the Out-Of-Distribution Robustness of Self-Supervised Representation Learning for Phonocardiogram Signals. (arXiv:2312.00502v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00502">http://arxiv.org/abs/2312.00502</a></li>
<li>Code URL: https://github.com/aristotelisballas/listen2yourheart</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00502]] On the Out-Of-Distribution Robustness of Self-Supervised Representation Learning for Phonocardiogram Signals(http://arxiv.org/abs/2312.00502)</code></li>
<li>Summary: <p>Objective: Despite the recent increase in research activity, deep-learning
models have not yet been widely accepted in medicine. The shortage of
high-quality annotated data often hinders the development of robust and
generalizable models, which do not suffer from degraded effectiveness when
presented with newly-collected, out-of-distribution (OOD) datasets. Methods:
Contrastive Self-Supervised Learning (SSL) offers a potential solution to the
scarcity of labeled data as it takes advantage of unlabeled data to increase
model effectiveness and robustness. In this research, we propose applying
contrastive SSL for detecting abnormalities in phonocardiogram (PCG) samples by
learning a generalized representation of the signal. Specifically, we perform
an extensive comparative evaluation of a wide range of audio-based
augmentations and evaluate trained classifiers on multiple datasets across
different downstream tasks. Results: We experimentally demonstrate that,
depending on its training distribution, the effectiveness of a fully-supervised
model can degrade up to 32% when evaluated on unseen data, while SSL models
only lose up to 10% or even improve in some cases. Conclusions: Contrastive SSL
pretraining can assist in providing robust classifiers which can generalize to
unseen, OOD data, without relying on time- and labor-intensive annotation
processes by medical experts. Furthermore, the proposed extensive evaluation
protocol sheds light on the most promising and appropriate augmentations for
robust PCG signal processing. Significance: We provide researchers and
practitioners with a roadmap towards producing robust models for PCG
classification, in addition to an open-source codebase for developing novel
approaches.
</p></li>
</ul>

<h3>Title: Spatio-Temporal-Decoupled Masked Pre-training for Traffic Forecasting. (arXiv:2312.00516v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00516">http://arxiv.org/abs/2312.00516</a></li>
<li>Code URL: https://github.com/jimmy-7664/std_mae</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00516]] Spatio-Temporal-Decoupled Masked Pre-training for Traffic Forecasting(http://arxiv.org/abs/2312.00516)</code></li>
<li>Summary: <p>Accurate forecasting of multivariate traffic flow time series remains
challenging due to substantial spatio-temporal heterogeneity and complex
long-range correlative patterns. To address this, we propose
Spatio-Temporal-Decoupled Masked Pre-training (STD-MAE), a novel framework that
employs masked autoencoders to learn and encode complex spatio-temporal
dependencies via pre-training. Specifically, we use two decoupled masked
autoencoders to reconstruct the traffic data along spatial and temporal axes
using a self-supervised pre-training approach. These mask reconstruction
mechanisms capture the long-range correlations in space and time separately.
The learned hidden representations are then used to augment the downstream
spatio-temporal traffic predictor. A series of quantitative and qualitative
evaluations on four widely-used traffic benchmarks (PEMS03, PEMS04, PEMS07, and
PEMS08) are conducted to verify the state-of-the-art performance, with STD-MAE
explicitly enhancing the downstream spatio-temporal models' ability to capture
long-range intricate spatial and temporal patterns. Codes are available at
https://github.com/Jimmy-7664/STD_MAE.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: CLIP-QDA: An Explainable Concept Bottleneck Model. (arXiv:2312.00110v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00110">http://arxiv.org/abs/2312.00110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00110]] CLIP-QDA: An Explainable Concept Bottleneck Model(http://arxiv.org/abs/2312.00110)</code></li>
<li>Summary: <p>In this paper, we introduce an explainable algorithm designed from a
multi-modal foundation model, that performs fast and explainable image
classification. Drawing inspiration from CLIP-based Concept Bottleneck Models
(CBMs), our method creates a latent space where each neuron is linked to a
specific word. Observing that this latent space can be modeled with simple
distributions, we use a Mixture of Gaussians (MoG) formalism to enhance the
interpretability of this latent space. Then, we introduce CLIP-QDA, a
classifier that only uses statistical values to infer labels from the concepts.
In addition, this formalism allows for both local and global explanations.
These explanations come from the inner design of our architecture, our work is
part of a new family of greybox models, combining performances of opaque
foundation models and the interpretability of transparent models. Our empirical
findings show that in instances where the MoG assumption holds, CLIP-QDA
achieves similar accuracy with state-of-the-art methods CBMs. Our explanations
compete with existing XAI methods while being faster to compute.
</p></li>
</ul>

<h3>Title: Segment Anything Model-guided Collaborative Learning Network for Scribble-supervised Polyp Segmentation. (arXiv:2312.00312v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00312">http://arxiv.org/abs/2312.00312</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00312]] Segment Anything Model-guided Collaborative Learning Network for Scribble-supervised Polyp Segmentation(http://arxiv.org/abs/2312.00312)</code></li>
<li>Summary: <p>Polyp segmentation plays a vital role in accurately locating polyps at an
early stage, which holds significant clinical importance for the prevention of
colorectal cancer. Various polyp segmentation methods have been developed using
fully-supervised deep learning techniques. However, pixel-wise annotation for
polyp images by physicians during the diagnosis is both time-consuming and
expensive. Moreover, visual foundation models such as the Segment Anything
Model (SAM) have shown remarkable performance. Nevertheless, directly applying
SAM to medical segmentation may not produce satisfactory results due to the
inherent absence of medical knowledge. In this paper, we propose a novel
SAM-guided Collaborative Learning Network (SAM-CLNet) for scribble-supervised
polyp segmentation, enabling a collaborative learning process between our
segmentation network and SAM to boost the model performance. Specifically, we
first propose a Cross-level Enhancement and Aggregation Network (CEA-Net) for
weakly-supervised polyp segmentation. Within CEA-Net, we propose a Cross-level
Enhancement Module (CEM) that integrates the adjacent features to enhance the
representation capabilities of different resolution features. Additionally, a
Feature Aggregation Module (FAM) is employed to capture richer features across
multiple levels. Moreover, we present a box-augmentation strategy that combines
the segmentation maps generated by CEA-Net with scribble annotations to create
more precise prompts. These prompts are then fed into SAM, generating
segmentation SAM-guided masks, which can provide additional supervision to
train CEA-Net effectively. Furthermore, we present an Image-level Filtering
Mechanism to filter out unreliable SAM-guided masks. Extensive experimental
results show that our SAM-CLNet outperforms state-of-the-art weakly-supervised
segmentation methods.
</p></li>
</ul>

<h3>Title: Towards A Foundation Model For Trajectory Intelligence. (arXiv:2312.00076v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00076">http://arxiv.org/abs/2312.00076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00076]] Towards A Foundation Model For Trajectory Intelligence(http://arxiv.org/abs/2312.00076)</code></li>
<li>Summary: <p>We present the results of training a large trajectory model using real-world
user check-in data. Our approach follows a pre-train and fine-tune paradigm,
where a base model is pre-trained via masked trajectory modeling and then
adapted through fine-tuning for various downstream tasks. To address challenges
posed by noisy data and large spatial vocabularies, we propose a novel spatial
tokenization block. Our empirical analysis utilizes a comprehensive dataset of
over 2 billion check-ins generated by more than 6 million users. Through
fine-tuning on 3 downstream tasks we demonstrate that our base model has
effectively learned valuable underlying patterns in raw data, enabling its
application in meaningful trajectory intelligence tasks. Despite some
limitations, we believe this work represents an important step forward in the
realization of a foundation model for trajectory intelligence.
</p></li>
</ul>

<h3>Title: Multimodal Learning for Crystalline Materials. (arXiv:2312.00111v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00111">http://arxiv.org/abs/2312.00111</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00111]] Multimodal Learning for Crystalline Materials(http://arxiv.org/abs/2312.00111)</code></li>
<li>Summary: <p>Artificial intelligence (AI) has revolutionized the field of materials
science by improving the prediction of properties and accelerating the
discovery of novel materials. In recent years, publicly available material data
repositories containing data for various material properties have grown
rapidly. In this work, we introduce Multimodal Learning for Crystalline
Materials (MLCM), a new method for training a foundation model for crystalline
materials via multimodal alignment, where high-dimensional material properties
(i.e. modalities) are connected in a shared latent space to produce highly
useful material representations. We show the utility of MLCM on multiple axes:
(i) MLCM achieves state-of-the-art performance for material property prediction
on the challenging Materials Project database; (ii) MLCM enables a novel,
highly accurate method for inverse design, allowing one to screen for stable
material with desired properties; and (iii) MLCM allows the extraction of
interpretable emergent features that may provide insight to material
scientists. Further, we explore several novel methods for aligning an arbitrary
number of modalities, improving upon prior art in multimodal learning that
focuses on bimodal alignment. Our work brings innovations from the ongoing AI
revolution into the domain of materials science and identifies materials as a
testbed for the next generation of AI.
</p></li>
</ul>

<h3>Title: PEFTDebias : Capturing debiasing information using PEFTs. (arXiv:2312.00434v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00434">http://arxiv.org/abs/2312.00434</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00434]] PEFTDebias : Capturing debiasing information using PEFTs(http://arxiv.org/abs/2312.00434)</code></li>
<li>Summary: <p>The increasing use of foundation models highlights the urgent need to address
and eliminate implicit biases present in them that arise during pretraining. In
this paper, we introduce PEFTDebias, a novel approach that employs
parameter-efficient fine-tuning (PEFT) to mitigate the biases within foundation
models. PEFTDebias consists of two main phases: an upstream phase for acquiring
debiasing parameters along a specific bias axis, and a downstream phase where
these parameters are incorporated into the model and frozen during the
fine-tuning process. By evaluating on four datasets across two bias axes namely
gender and race, we find that downstream biases can be effectively reduced with
PEFTs. In addition, we show that these parameters possess axis-specific
debiasing characteristics, enabling their effective transferability in
mitigating biases in various downstream tasks. To ensure reproducibility, we
release the code to do our experiments.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Probabilistic Copyright Protection Can Fail for Text-to-Image Generative Models. (arXiv:2312.00057v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00057">http://arxiv.org/abs/2312.00057</a></li>
<li>Code URL: https://github.com/south7x/va3</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00057]] Probabilistic Copyright Protection Can Fail for Text-to-Image Generative Models(http://arxiv.org/abs/2312.00057)</code></li>
<li>Summary: <p>The booming use of text-to-image generative models has raised concerns about
their high risk of producing copyright-infringing content. While probabilistic
copyright protection methods provide a probabilistic guarantee against such
infringement, in this paper, we introduce Virtually Assured Amplification
Attack (VA3), a novel online attack framework that exposes the vulnerabilities
of these protection mechanisms. The proposed framework significantly amplifies
the probability of generating infringing content on the sustained interactions
with generative models and a lower-bounded success probability of each
engagement. Our theoretical and experimental results demonstrate the
effectiveness of our approach and highlight the potential risk of implementing
probabilistic copyright protection in practical applications of text-to-image
generative models. Code is available at https://github.com/South7X/VA3.
</p></li>
</ul>

<h3>Title: MoMask: Generative Masked Modeling of 3D Human Motions. (arXiv:2312.00063v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00063">http://arxiv.org/abs/2312.00063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00063]] MoMask: Generative Masked Modeling of 3D Human Motions(http://arxiv.org/abs/2312.00063)</code></li>
<li>Summary: <p>We introduce MoMask, a novel masked modeling framework for text-driven 3D
human motion generation. In MoMask, a hierarchical quantization scheme is
employed to represent human motion as multi-layer discrete motion tokens with
high-fidelity details. Starting at the base layer, with a sequence of motion
tokens obtained by vector quantization, the residual tokens of increasing
orders are derived and stored at the subsequent layers of the hierarchy. This
is consequently followed by two distinct bidirectional transformers. For the
base-layer motion tokens, a Masked Transformer is designated to predict
randomly masked motion tokens conditioned on text input at training stage.
During generation (i.e. inference) stage, starting from an empty sequence, our
Masked Transformer iteratively fills up the missing tokens; Subsequently, a
Residual Transformer learns to progressively predict the next-layer tokens
based on the results from current layer. Extensive experiments demonstrate that
MoMask outperforms the state-of-art methods on the text-to-motion generation
task, with an FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the HumanML3D dataset,
and 0.228 (vs 0.514) on KIT-ML, respectively. MoMask can also be seamlessly
applied in related tasks without further model fine-tuning, such as text-guided
temporal inpainting.
</p></li>
</ul>

<h3>Title: Mixture of Gaussian-distributed Prototypes with Generative Modelling for Interpretable Image Classification. (arXiv:2312.00092v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00092">http://arxiv.org/abs/2312.00092</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00092]] Mixture of Gaussian-distributed Prototypes with Generative Modelling for Interpretable Image Classification(http://arxiv.org/abs/2312.00092)</code></li>
<li>Summary: <p>Prototypical-part interpretable methods, e.g., ProtoPNet, enhance
interpretability by connecting classification predictions to class-specific
training prototypes, thereby offering an intuitive insight into their
decision-making. Current methods rely on a discriminative classifier trained
with point-based learning techniques that provide specific values for
prototypes. Such prototypes have relatively low representation power due to
their sparsity and potential redundancy, with each prototype containing no
variability measure. In this paper, we present a new generative learning of
prototype distributions, named Mixture of Gaussian-distributed Prototypes
(MGProto), which are represented by Gaussian mixture models (GMM). Such an
approach enables the learning of more powerful prototype representations since
each learned prototype will own a measure of variability, which naturally
reduces the sparsity given the spread of the distribution around each
prototype, and we also integrate a prototype diversity objective function into
the GMM optimisation to reduce redundancy. Incidentally, the generative nature
of MGProto offers a new and effective way for detecting out-of-distribution
samples. To improve the compactness of MGProto, we further propose to prune
Gaussian-distributed prototypes with a low prior. Experiments on CUB-200-2011,
Stanford Cars, Stanford Dogs, and Oxford-IIIT Pets datasets show that MGProto
achieves state-of-the-art classification and OoD detection performances with
encouraging interpretability results.
</p></li>
</ul>

<h3>Title: Raising the Bar of AI-generated Image Detection with CLIP. (arXiv:2312.00195v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00195">http://arxiv.org/abs/2312.00195</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00195]] Raising the Bar of AI-generated Image Detection with CLIP(http://arxiv.org/abs/2312.00195)</code></li>
<li>Summary: <p>Aim of this work is to explore the potential of pre-trained vision-language
models (VLMs) for universal detection of AI-generated images. We develop a
lightweight detection strategy based on CLIP features and study its performance
in a wide variety of challenging scenarios. We find that, unlike previous
belief, it is neither necessary nor convenient to use a large domain-specific
dataset for training. On the contrary, by using only a handful of example
images from a single generative model, a CLIP-based detector exhibits a
surprising generalization ability and high robustness across several different
architectures, including recent commercial tools such as Dalle-3, Midjourney
v5, and Firefly. We match the SoTA on in-distribution data, and improve largely
above it in terms of generalization to out-of-distribution data (+6% in terms
of AUC) and robustness to impaired/laundered data (+13%). Our project is
available at https://grip-unina.github.io/ClipBased-SyntheticImageDetection/
</p></li>
</ul>

<h3>Title: SparseGS: Real-Time 360{\deg} Sparse View Synthesis using Gaussian Splatting. (arXiv:2312.00206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00206">http://arxiv.org/abs/2312.00206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00206]] SparseGS: Real-Time 360{\deg} Sparse View Synthesis using Gaussian Splatting(http://arxiv.org/abs/2312.00206)</code></li>
<li>Summary: <p>The problem of novel view synthesis has grown significantly in popularity
recently with the introduction of Neural Radiance Fields (NeRFs) and other
implicit scene representation methods. A recent advance, 3D Gaussian Splatting
(3DGS), leverages an explicit representation to achieve real-time rendering
with high-quality results. However, 3DGS still requires an abundance of
training views to generate a coherent scene representation. In few shot
settings, similar to NeRF, 3DGS tends to overfit to training views, causing
background collapse and excessive floaters, especially as the number of
training views are reduced. We propose a method to enable training coherent
3DGS-based radiance fields of 360 scenes from sparse training views. We find
that using naive depth priors is not sufficient and integrate depth priors with
generative and explicit constraints to reduce background collapse, remove
floaters, and enhance consistency from unseen viewpoints. Experiments show that
our method outperforms base 3DGS by up to 30.5% and NeRF-based methods by up to
15.6% in LPIPS on the MipNeRF-360 dataset with substantially less training and
inference cost.
</p></li>
</ul>

<h3>Title: DeepDR: Deep Structure-Aware RGB-D Inpainting for Diminished Reality. (arXiv:2312.00532v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00532">http://arxiv.org/abs/2312.00532</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00532]] DeepDR: Deep Structure-Aware RGB-D Inpainting for Diminished Reality(http://arxiv.org/abs/2312.00532)</code></li>
<li>Summary: <p>Diminished reality (DR) refers to the removal of real objects from the
environment by virtually replacing them with their background. Modern DR
frameworks use inpainting to hallucinate unobserved regions. While recent deep
learning-based inpainting is promising, the DR use case is complicated by the
need to generate coherent structure and 3D geometry (i.e., depth), in
particular for advanced applications, such as 3D scene editing. In this paper,
we propose DeepDR, a first RGB-D inpainting framework fulfilling all
requirements of DR: Plausible image and geometry inpainting with coherent
structure, running at real-time frame rates, with minimal temporal artifacts.
Our structure-aware generative network allows us to explicitly condition color
and depth outputs on the scene semantics, overcoming the difficulty of
reconstructing sharp and consistent boundaries in regions with complex
backgrounds. Experimental results show that the proposed framework can
outperform related work qualitatively and quantitatively.
</p></li>
</ul>

<h3>Title: Generative models for visualising abstract social processes: Guiding streetview image synthesis of StyleGAN2 with indices of deprivation. (arXiv:2312.00570v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00570">http://arxiv.org/abs/2312.00570</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00570]] Generative models for visualising abstract social processes: Guiding streetview image synthesis of StyleGAN2 with indices of deprivation(http://arxiv.org/abs/2312.00570)</code></li>
<li>Summary: <p>This paper presents a novel application of Generative Adverserial Networks
(GANs) to study visual aspects of social processes. I train a a StyleGAN2-model
on a custom dataset of 14,564 images of London, sourced from Google Streetview
taken in London. After training, I invert the images in the training set,
finding points in the model's latent space that correspond to them, and compare
results from three inversion techniques. I connect each data point with
metadata from the Indices of Multiple Deprivation, describing income, health
and environmental quality in the area where the photographs were taken. It is
then possible to map which parts of the model's latent space encode visual
features that are distinctive for health, income and environmental quality, and
condition the synthesis of new images based on these factors. The synthetic
images created reflect visual features of social processes that were previously
unknown and difficult to study, describing recurring visual differences between
deprived and privileged areas in London. GANs are known for their capability to
produce a continuous range of images that exhibit visual differences. The paper
tests how to exploit this ability through visual comparisons in still images as
well as through an interactive website where users can guide image synthesis
with sliders. Though conditioned synthesis has its limitations and the results
are difficult to validate, the paper points to the potential for generative
models to be repurposed to be parts of social scientific methods.
</p></li>
</ul>

<h3>Title: MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes. (arXiv:2312.00583v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00583">http://arxiv.org/abs/2312.00583</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00583]] MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes(http://arxiv.org/abs/2312.00583)</code></li>
<li>Summary: <p>Accurate 3D tracking in highly deformable scenes with occlusions and shadows
can facilitate new applications in robotics, augmented reality, and generative
AI. However, tracking under these conditions is extremely challenging due to
the ambiguity that arises with large deformations, shadows, and occlusions. We
introduce MD-Splatting, an approach for simultaneous 3D tracking and novel view
synthesis, using video captures of a dynamic scene from various camera poses.
MD-Splatting builds on recent advances in Gaussian splatting, a method that
learns the properties of a large number of Gaussians for state-of-the-art and
fast novel view synthesis. MD-Splatting learns a deformation function to
project a set of Gaussians with non-metric, thus canonical, properties into
metric space. The deformation function uses a neural-voxel encoding and a
multilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow
scalar. We enforce physics-inspired regularization terms based on local
rigidity, conservation of momentum, and isometry, which leads to trajectories
with smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking
on highly deformable scenes with shadows and occlusions. Compared to
state-of-the-art, we improve 3D tracking by an average of 23.9 %, while
simultaneously achieving high-quality novel view synthesis. With sufficient
texture such as in scene 6, MD-Splatting achieves a median tracking error of
3.39 mm on a cloth of 1 x 1 meters in size. Project website:
https://md-splatting.github.io/.
</p></li>
</ul>

<h3>Title: EvE: Exploiting Generative Priors for Radiance Field Enrichment. (arXiv:2312.00639v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00639">http://arxiv.org/abs/2312.00639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00639]] EvE: Exploiting Generative Priors for Radiance Field Enrichment(http://arxiv.org/abs/2312.00639)</code></li>
<li>Summary: <p>Modeling large-scale scenes from unconstrained image collections in-the-wild
has proven to be a major challenge in computer vision. Existing methods
tackling in-the-wild neural rendering operate in a closed-world setting, where
knowledge is limited to a scene's captured images within a training set. We
propose EvE, which is, to the best of our knowledge, the first method
leveraging generative priors to improve in-the-wild scene modeling. We employ
pre-trained generative networks to enrich K-Planes representations with
extrinsic knowledge. To this end, we define an alternating training procedure
to conduct optimization guidance of K-Planes trained on the training set. We
carry out extensive experiments and verify the merit of our method on synthetic
data as well as real tourism photo collections. EvE enhances rendered scenes
with richer details and outperforms the state of the art on the task of novel
view synthesis in-the-wild. Our project page can be found at
https://eve-nvs.github.io .
</p></li>
</ul>

<h3>Title: GIFT: Generative Interpretable Fine-Tuning Transformers. (arXiv:2312.00700v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00700">http://arxiv.org/abs/2312.00700</a></li>
<li>Code URL: https://github.com/savadikarc/gift</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00700]] GIFT: Generative Interpretable Fine-Tuning Transformers(http://arxiv.org/abs/2312.00700)</code></li>
<li>Summary: <p>We present GIFT (Generative Interpretable Fine-tuning Transformers) for
fine-tuning pretrained (often large) Transformer models at downstream tasks in
a parameter-efficient way with built-in interpretability. Our GIFT is a deep
parameter-residual learning method, which addresses two problems in fine-tuning
a pretrained Transformer model: Where to apply the parameter-efficient
fine-tuning (PEFT) to be extremely lightweight yet sufficiently expressive, and
How to learn the PEFT to better exploit the knowledge of the pretrained model
in a direct way? For the former, we select the final projection (linear) layer
in the multi-head self-attention of a Transformer model, and verify its
effectiveness. For the latter, in contrast to the prior art that directly
introduce new model parameters (often in low-rank approximation form) to be
learned in fine-tuning with downstream data, we propose a method for learning
to generate the fine-tuning parameters. Our GIFT is a hyper-Transformer which
take as input the pretrained parameters of the projection layer to generate its
fine-tuning parameters using a proposed Parameter-to-Cluster Attention (PaCa).
The PaCa results in a simple clustering-based forward explainer that plays the
role of semantic segmentation in testing. In experiments, our proposed GIFT is
tested on the VTAB benchmark and the fine-grained visual classification (FGVC)
benchmark. It obtains significantly better performance than the prior art. Our
code is available at https://github.com/savadikarc/gift
</p></li>
</ul>

<h3>Title: Adversarial Score Distillation: When score distillation meets GAN. (arXiv:2312.00739v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00739">http://arxiv.org/abs/2312.00739</a></li>
<li>Code URL: https://github.com/2y7c3/asd</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00739]] Adversarial Score Distillation: When score distillation meets GAN(http://arxiv.org/abs/2312.00739)</code></li>
<li>Summary: <p>Existing score distillation methods are sensitive to classifier-free guidance
(CFG) scale: manifested as over-smoothness or instability at small CFG scales,
while over-saturation at large ones. To explain and analyze these issues, we
revisit the derivation of Score Distillation Sampling (SDS) and decipher
existing score distillation with the Wasserstein Generative Adversarial Network
(WGAN) paradigm. With the WGAN paradigm, we find that existing score
distillation either employs a fixed sub-optimal discriminator or conducts
incomplete discriminator optimization, resulting in the scale-sensitive issue.
We propose the Adversarial Score Distillation (ASD), which maintains an
optimizable discriminator and updates it using the complete optimization
objective. Experiments show that the proposed ASD performs favorably in 2D
distillation and text-to-3D tasks against existing methods. Furthermore, to
explore the generalization ability of our WGAN paradigm, we extend ASD to the
image editing task, which achieves competitive results. The project page and
code are at https://github.com/2y7c3/ASD.
</p></li>
</ul>

<h3>Title: Risk-Aware and Explainable Framework for Ensuring Guaranteed Coverage in Evolving Hardware Trojan Detection. (arXiv:2312.00009v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00009">http://arxiv.org/abs/2312.00009</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00009]] Risk-Aware and Explainable Framework for Ensuring Guaranteed Coverage in Evolving Hardware Trojan Detection(http://arxiv.org/abs/2312.00009)</code></li>
<li>Summary: <p>As the semiconductor industry has shifted to a fabless paradigm, the risk of
hardware Trojans being inserted at various stages of production has also
increased. Recently, there has been a growing trend toward the use of machine
learning solutions to detect hardware Trojans more effectively, with a focus on
the accuracy of the model as an evaluation metric. However, in a high-risk and
sensitive domain, we cannot accept even a small misclassification.
Additionally, it is unrealistic to expect an ideal model, especially when
Trojans evolve over time. Therefore, we need metrics to assess the
trustworthiness of detected Trojans and a mechanism to simulate unseen ones. In
this paper, we generate evolving hardware Trojans using our proposed novel
conformalized generative adversarial networks and offer an efficient approach
to detecting them based on a non-invasive algorithm-agnostic statistical
inference framework that leverages the Mondrian conformal predictor. The method
acts like a wrapper over any of the machine learning models and produces set
predictions along with uncertainty quantification for each new detected Trojan
for more robust decision-making. In the case of a NULL set, a novel method to
reject the decision by providing a calibrated explainability is discussed. The
proposed approach has been validated on both synthetic and real chip-level
benchmarks and proven to pave the way for researchers looking to find informed
machine learning solutions to hardware security problems.
</p></li>
</ul>

<h3>Title: Tokenized Model: A Blockchain-Empowered Decentralized Model Ownership Verification Platform. (arXiv:2312.00048v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00048">http://arxiv.org/abs/2312.00048</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00048]] Tokenized Model: A Blockchain-Empowered Decentralized Model Ownership Verification Platform(http://arxiv.org/abs/2312.00048)</code></li>
<li>Summary: <p>With the development of practical deep learning models like generative AI,
their excellent performance has brought huge economic value. For instance,
ChatGPT has attracted more than 100 million users in three months. Since the
model training requires a lot of data and computing power, a well-performing
deep learning model is behind a huge effort and cost. Facing various model
attacks, unauthorized use and abuse from the network that threaten the
interests of model owners, in addition to considering legal and other
administrative measures, it is equally important to protect the model's
copyright from the technical means. By using the model watermarking technology,
we point out the possibility of building a unified platform for model ownership
verification. Given the application history of blockchain in copyright
verification and the drawbacks of a centralized third-party, this paper
considers combining model watermarking technology and blockchain to build a
unified model copyright protection platform. By a new solution we called
Tokenized Model, it protects the model's copyright by reliable ownership record
and verification mechanism. It also promotes the financial value of model by
constructing the model's transaction process and contribution shares of a
model. In the typical case study, we also study the various performance under
usual scenario to verify the effectiveness of this platform.
</p></li>
</ul>

<h3>Title: Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation. (arXiv:2312.00645v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00645">http://arxiv.org/abs/2312.00645</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00645]] Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation(http://arxiv.org/abs/2312.00645)</code></li>
<li>Summary: <p>There is a growing need to gain insight into language model capabilities that
relate to sensitive topics, such as bioterrorism or cyberwarfare. However,
traditional open source benchmarks are not fit for the task, due to the
associated practice of publishing the correct answers in human-readable form.
At the same time, enforcing mandatory closed-quarters evaluations might stifle
development and erode trust. In this context, we propose hashmarking, a
protocol for evaluating language models in the open without having to disclose
the correct answers. In its simplest form, a hashmark is a benchmark whose
reference solutions have been cryptographically hashed prior to publication.
Following an overview of the proposed evaluation protocol, we go on to assess
its resilience against traditional attack vectors (e.g. rainbow table attacks),
as well as against failure modes unique to increasingly capable generative
models.
</p></li>
</ul>

<h3>Title: GFN-SR: Symbolic Regression with Generative Flow Networks. (arXiv:2312.00396v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00396">http://arxiv.org/abs/2312.00396</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00396]] GFN-SR: Symbolic Regression with Generative Flow Networks(http://arxiv.org/abs/2312.00396)</code></li>
<li>Summary: <p>Symbolic regression (SR) is an area of interpretable machine learning that
aims to identify mathematical expressions, often composed of simple functions,
that best fit in a given set of covariates $X$ and response $y$. In recent
years, deep symbolic regression (DSR) has emerged as a popular method in the
field by leveraging deep reinforcement learning to solve the complicated
combinatorial search problem. In this work, we propose an alternative framework
(GFN-SR) to approach SR with deep learning. We model the construction of an
expression tree as traversing through a directed acyclic graph (DAG) so that
GFlowNet can learn a stochastic policy to generate such trees sequentially.
Enhanced with an adaptive reward baseline, our method is capable of generating
a diverse set of best-fitting expressions. Notably, we observe that GFN-SR
outperforms other SR algorithms in noisy data regimes, owing to its ability to
learn a distribution of rewards over a space of candidate solutions.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Unsupervised textile defect detection using convolutional neural networks. (arXiv:2312.00224v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00224">http://arxiv.org/abs/2312.00224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00224]] Unsupervised textile defect detection using convolutional neural networks(http://arxiv.org/abs/2312.00224)</code></li>
<li>Summary: <p>In this study, we propose a novel motif-based approach for unsupervised
textile anomaly detection that combines the benefits of traditional
convolutional neural networks with those of an unsupervised learning paradigm.
It consists of five main steps: preprocessing, automatic pattern period
extraction, patch extraction, features selection and anomaly detection. This
proposed approach uses a new dynamic and heuristic method for feature selection
which avoids the drawbacks of initialization of the number of filters (neurons)
and their weights, and those of the backpropagation mechanism such as the
vanishing gradients, which are common practice in the state-of-the-art methods.
The design and training of the network are performed in a dynamic and input
domain-based manner and, thus, no ad-hoc configurations are required. Before
building the model, only the number of layers and the stride are defined. We do
not initialize the weights randomly nor do we define the filter size or number
of filters as conventionally done in CNN-based approaches. This reduces effort
and time spent on hyperparameter initialization and fine-tuning. Only one
defect-free sample is required for training and no further labeled data is
needed. The trained network is then used to detect anomalies on defective
fabric samples. We demonstrate the effectiveness of our approach on the
Patterned Fabrics benchmark dataset. Our algorithm yields reliable and
competitive results (on recall, precision, accuracy and f1- measure) compared
to state-of-the-art unsupervised approaches, in less time, with efficient
training in a single epoch and a lower computational cost.
</p></li>
</ul>

<h3>Title: Hypergraph Topological Features for Autoencoder-Based Intrusion Detection for Cybersecurity Data. (arXiv:2312.00023v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00023">http://arxiv.org/abs/2312.00023</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00023]] Hypergraph Topological Features for Autoencoder-Based Intrusion Detection for Cybersecurity Data(http://arxiv.org/abs/2312.00023)</code></li>
<li>Summary: <p>In this position paper, we argue that when hypergraphs are used to capture
multi-way local relations of data, their resulting topological features
describe global behaviour. Consequently, these features capture complex
correlations that can then serve as high fidelity inputs to autoencoder-driven
anomaly detection pipelines. We propose two such potential pipelines for
cybersecurity data, one that uses an autoencoder directly to determine network
intrusions, and one that de-noises input data for a persistent homology system,
PHANTOM. We provide heuristic justification for the use of the methods
described therein for an intrusion detection pipeline for cyber data. We
conclude by showing a small example over synthetic cyber attack data.
</p></li>
</ul>

<h3>Title: Anomaly Detection via Learning-Based Sequential Controlled Sensing. (arXiv:2312.00088v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00088">http://arxiv.org/abs/2312.00088</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00088]] Anomaly Detection via Learning-Based Sequential Controlled Sensing(http://arxiv.org/abs/2312.00088)</code></li>
<li>Summary: <p>In this paper, we address the problem of detecting anomalies among a given
set of binary processes via learning-based controlled sensing. Each process is
parameterized by a binary random variable indicating whether the process is
anomalous. To identify the anomalies, the decision-making agent is allowed to
observe a subset of the processes at each time instant. Also, probing each
process has an associated cost. Our objective is to design a sequential
selection policy that dynamically determines which processes to observe at each
time with the goal to minimize the delay in making the decision and the total
sensing cost. We cast this problem as a sequential hypothesis testing problem
within the framework of Markov decision processes. This formulation utilizes
both a Bayesian log-likelihood ratio-based reward and an entropy-based reward.
The problem is then solved using two approaches: 1) a deep reinforcement
learning-based approach where we design both deep Q-learning and policy
gradient actor-critic algorithms; and 2) a deep active inference-based
approach. Using numerical experiments, we demonstrate the efficacy of our
algorithms and show that our algorithms adapt to any unknown statistical
dependence pattern of the processes.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Manipulating the Label Space for In-Context Classification. (arXiv:2312.00351v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00351">http://arxiv.org/abs/2312.00351</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00351]] Manipulating the Label Space for In-Context Classification(http://arxiv.org/abs/2312.00351)</code></li>
<li>Summary: <p>After pre-training by generating the next word conditional on previous words,
the Language Model (LM) acquires the ability of In-Context Learning (ICL) that
can learn a new task conditional on the context of the given in-context
examples (ICEs). Similarly, visually-conditioned Language Modelling is also
used to train Vision-Language Models (VLMs) with ICL ability. However, such
VLMs typically exhibit weaker classification abilities compared to contrastive
learning-based models like CLIP, since the Language Modelling objective does
not directly contrast whether an object is paired with a text. To improve the
ICL of classification, using more ICEs to provide more knowledge is a
straightforward way. However, this may largely increase the selection time, and
more importantly, the inclusion of additional in-context images tends to extend
the length of the in-context sequence beyond the processing capacity of a VLM.
To alleviate these limitations, we propose to manipulate the label space of
each ICE to increase its knowledge density, allowing for fewer ICEs to convey
as much information as a larger set would. Specifically, we propose two
strategies which are Label Distribution Enhancement and Visual Descriptions
Enhancement to improve In-context classification performance on diverse
datasets, including the classic ImageNet and more fine-grained datasets like
CUB-200. Specifically, using our approach on ImageNet, we increase accuracy
from 74.70\% in a 4-shot setting to 76.21\% with just 2 shots. surpassing CLIP
by 0.67\%. On CUB-200, our method raises 1-shot accuracy from 48.86\% to
69.05\%, 12.15\% higher than CLIP. The code is given in
https://anonymous.4open.science/r/MLS_ICC.
</p></li>
</ul>

<h3>Title: Dolphins: Multimodal Language Model for Driving. (arXiv:2312.00438v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00438">http://arxiv.org/abs/2312.00438</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00438]] Dolphins: Multimodal Language Model for Driving(http://arxiv.org/abs/2312.00438)</code></li>
<li>Summary: <p>The quest for fully autonomous vehicles (AVs) capable of navigating complex
real-world scenarios with human-like understanding and responsiveness. In this
paper, we introduce Dolphins, a novel vision-language model architected to
imbibe human-like abilities as a conversational driving assistant. Dolphins is
adept at processing multimodal inputs comprising video (or image) data, text
instructions, and historical control signals to generate informed outputs
corresponding to the provided instructions. Building upon the open-sourced
pretrained Vision-Language Model, OpenFlamingo, we first enhance Dolphins's
reasoning capabilities through an innovative Grounded Chain of Thought (GCoT)
process. Then we tailored Dolphins to the driving domain by constructing
driving-specific instruction data and conducting instruction tuning. Through
the utilization of the BDD-X dataset, we designed and consolidated four
distinct AV tasks into Dolphins to foster a holistic understanding of intricate
driving scenarios. As a result, the distinctive features of Dolphins are
characterized into two dimensions: (1) the ability to provide a comprehensive
understanding of complex and long-tailed open-world driving scenarios and solve
a spectrum of AV tasks, and (2) the emergence of human-like capabilities
including gradient-free instant adaptation via in-context learning and error
recovery via reflection.
</p></li>
</ul>

<h3>Title: Automating Continual Learning. (arXiv:2312.00276v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.00276">http://arxiv.org/abs/2312.00276</a></li>
<li>Code URL: https://github.com/idsia/automated-cl</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.00276]] Automating Continual Learning(http://arxiv.org/abs/2312.00276)</code></li>
<li>Summary: <p>General-purpose learning systems should improve themselves in open-ended
fashion in ever-changing environments. Conventional learning algorithms for
neural networks, however, suffer from catastrophic forgetting (CF) --
previously acquired skills are forgotten when a new task is learned. Instead of
hand-crafting new algorithms for avoiding CF, we propose Automated Continual
Learning (ACL) to train self-referential neural networks to meta-learn their
own in-context continual (meta-)learning algorithms. ACL encodes all desiderata
-- good performance on both old and new tasks -- into its meta-learning
objectives. Our experiments demonstrate that ACL effectively solves "in-context
catastrophic forgetting"; our ACL-learned algorithms outperform hand-crafted
ones, e.g., on the Split-MNIST benchmark in the replay-free setting, and
enables continual learning of diverse tasks consisting of multiple few-shot and
standard image classification datasets.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
