<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-01</h1>
<h3>Title: Learning Low-Rank Feature for Thorax Disease Classification</h3>
<ul>
<li><strong>Authors: </strong>Rajeev Goel, Utkarsh Nath, Yancheng Wang, Alvin C. Silva, Teresa Wu, Yingzhen Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18933">https://arxiv.org/abs/2404.18933</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18933">https://arxiv.org/pdf/2404.18933</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18933]] Learning Low-Rank Feature for Thorax Disease Classification(https://arxiv.org/abs/2404.18933)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Deep neural networks, including Convolutional Neural Networks (CNNs) and Visual Transformers (ViT), have achieved stunning success in medical image domain. We study thorax disease classification in this paper. Effective extraction of features for the disease areas is crucial for disease classification on radiographic images. While various neural architectures and training techniques, such as self-supervised learning with contrastive/restorative learning, have been employed for disease classification on radiographic images, there are no principled methods which can effectively reduce the adverse effect of noise and background, or non-disease areas, on the radiographic images for disease classification. To address this challenge, we propose a novel Low-Rank Feature Learning (LRFL) method in this paper, which is universally applicable to the training of all neural networks. The LRFL method is both empirically motivated by the low frequency property observed on all the medical datasets in this paper, and theoretically motivated by our sharp generalization bound for neural networks with low-rank features. In the empirical study, using a neural network such as a ViT or a CNN pre-trained on unlabeled chest X-rays by Masked Autoencoders (MAE), our novel LRFL method is applied on the pre-trained neural network and demonstrate better classification results in terms of both multiclass area under the receiver operating curve (mAUC) and classification accuracy.</li>
</ul>

<h3>Title: Sub-Adjacent Transformer: Improving Time Series Anomaly Detection with  Reconstruction Error from Sub-Adjacent Neighborhoods</h3>
<ul>
<li><strong>Authors: </strong>Wenzhen Yue, Xianghua Ying, Ruohao Guo, DongDong Chen, Ji Shi, Bowei Xing, Yuqing Zhu, Taiyan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18948">https://arxiv.org/abs/2404.18948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18948">https://arxiv.org/pdf/2404.18948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18948]] Sub-Adjacent Transformer: Improving Time Series Anomaly Detection with  Reconstruction Error from Sub-Adjacent Neighborhoods(https://arxiv.org/abs/2404.18948)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In this paper, we present the Sub-Adjacent Transformer with a novel attention mechanism for unsupervised time series anomaly detection. Unlike previous approaches that rely on all the points within some neighborhood for time point reconstruction, our method restricts the attention to regions not immediately adjacent to the target points, termed sub-adjacent neighborhoods. Our key observation is that owing to the rarity of anomalies, they typically exhibit more pronounced differences from their sub-adjacent neighborhoods than from their immediate vicinities. By focusing the attention on the sub-adjacent areas, we make the reconstruction of anomalies more challenging, thereby enhancing their detectability. Technically, our approach concentrates attention on the non-diagonal areas of the attention matrix by enlarging the corresponding elements in the training stage. To facilitate the implementation of the desired attention matrix pattern, we adopt linear attention because of its flexibility and adaptability. Moreover, a learnable mapping function is proposed to improve the performance of linear attention. Empirically, the Sub-Adjacent Transformer achieves state-of-the-art performance across six real-world anomaly detection benchmarks, covering diverse fields such as server monitoring, space exploration, and water treatment.</li>
</ul>

<h3>Title: Unleashing the Power of Multi-Task Learning: A Comprehensive Survey  Spanning Traditional, Deep, and Pretrained Foundation Model Eras</h3>
<ul>
<li><strong>Authors: </strong>Jun Yu, Yutong Dai, Xiaokang Liu, Jin Huang, Yishan Shen, Ke Zhang, Rong Zhou, Eashan Adhikarla, Wenxuan Ye, Yixin Liu, Zhaoming Kong, Kai Zhang, Yilong Yin, Vinod Namboodiri, Brian D. Davison, Jason H. Moore, Yong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18961">https://arxiv.org/abs/2404.18961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18961">https://arxiv.org/pdf/2404.18961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18961]] Unleashing the Power of Multi-Task Learning: A Comprehensive Survey  Spanning Traditional, Deep, and Pretrained Foundation Model Eras(https://arxiv.org/abs/2404.18961)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>MTL is a learning paradigm that effectively leverages both task-specific and shared information to address multiple related tasks simultaneously. In contrast to STL, MTL offers a suite of benefits that enhance both the training process and the inference efficiency. MTL's key advantages encompass streamlined model architecture, performance enhancement, and cross-domain generalizability. Over the past twenty years, MTL has become widely recognized as a flexible and effective approach in various fields, including CV, NLP, recommendation systems, disease prognosis and diagnosis, and robotics. This survey provides a comprehensive overview of the evolution of MTL, encompassing the technical aspects of cutting-edge methods from traditional approaches to deep learning and the latest trend of pretrained foundation models. Our survey methodically categorizes MTL techniques into five key areas: regularization, relationship learning, feature propagation, optimization, and pre-training. This categorization not only chronologically outlines the development of MTL but also dives into various specialized strategies within each category. Furthermore, the survey reveals how the MTL evolves from handling a fixed set of tasks to embracing a more flexible approach free from task or modality constraints. It explores the concepts of task-promptable and -agnostic training, along with the capacity for ZSL, which unleashes the untapped potential of this historically coveted learning paradigm. Overall, we hope this survey provides the research community with a comprehensive overview of the advancements in MTL from its inception in 1997 to the present in 2023. We address present challenges and look ahead to future possibilities, shedding light on the opportunities and potential avenues for MTL research in a broad manner. This project is publicly available at https://github.com/junfish/Awesome-Multitask-Learning.</li>
</ul>

<h3>Title: Foundations of Multisensory Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Paul Pu Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.18976">https://arxiv.org/abs/2404.18976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.18976">https://arxiv.org/pdf/2404.18976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.18976]] Foundations of Multisensory Artificial Intelligence(https://arxiv.org/abs/2404.18976)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Building multisensory AI systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many scientific areas with practical benefits, such as in supporting human health and well-being, enabling multimedia content processing, and enhancing real-world autonomous agents. By synthesizing a range of theoretical frameworks and application domains, this thesis aims to advance the machine learning foundations of multisensory AI. In the first part, we present a theoretical framework formalizing how modalities interact with each other to give rise to new information for a task. These interactions are the basic building blocks in all multimodal problems, and their quantification enables users to understand their multimodal datasets, design principled approaches to learn these interactions, and analyze whether their model has succeeded in learning. In the second part, we study the design of practical multimodal foundation models that generalize over many modalities and tasks, which presents a step toward grounding large language models to real-world sensory modalities. We introduce MultiBench, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas, followed by the cross-modal attention and multimodal transformer architectures that now underpin many of today's multimodal foundation models. Scaling these architectures on MultiBench enables the creation of general-purpose multisensory AI systems, and we discuss our collaborative efforts in applying these models for real-world impact in affective computing, mental health, cancer prognosis, and robotics. Finally, we conclude this thesis by discussing how future work can leverage these ideas toward more general, interactive, and safe multisensory AI.</li>
</ul>

<h3>Title: Embedded Representation Learning Network for Animating Styled Video  Portrait</h3>
<ul>
<li><strong>Authors: </strong>Tianyong Wang, Xiangyu Liang, Wangguandong Zheng, Dan Niu, Haifeng Xia, Siyu Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19038">https://arxiv.org/abs/2404.19038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19038">https://arxiv.org/pdf/2404.19038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19038]] Embedded Representation Learning Network for Animating Styled Video  Portrait(https://arxiv.org/abs/2404.19038)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The talking head generation recently attracted considerable attention due to its widespread application prospects, especially for digital avatars and 3D animation design. Inspired by this practical demand, several works explored Neural Radiance Fields (NeRF) to synthesize the talking heads. However, these methods based on NeRF face two challenges: (1) Difficulty in generating style-controllable talking heads. (2) Displacement artifacts around the neck in rendered images. To overcome these two challenges, we propose a novel generative paradigm \textit{Embedded Representation Learning Network} (ERLNet) with two learning stages. First, the \textit{ audio-driven FLAME} (ADF) module is constructed to produce facial expression and head pose sequences synchronized with content audio and style video. Second, given the sequence deduced by the ADF, one novel \textit{dual-branch fusion NeRF} (DBF-NeRF) explores these contents to render the final images. Extensive empirical studies demonstrate that the collaboration of these two stages effectively facilitates our method to render a more realistic talking head than the existing algorithms.</li>
</ul>

<h3>Title: In-Context Symbolic Regression: Leveraging Language Models for Function  Discovery</h3>
<ul>
<li><strong>Authors: </strong>Matteo Merler, Nicola Dainese, Katsiaryna Haitsiukevich</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19094">https://arxiv.org/abs/2404.19094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19094">https://arxiv.org/pdf/2404.19094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19094]] In-Context Symbolic Regression: Leveraging Language Models for Function  Discovery(https://arxiv.org/abs/2404.19094)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Symbolic Regression (SR) is a task which aims to extract the mathematical expression underlying a set of empirical observations. Transformer-based methods trained on SR datasets detain the current state-of-the-art in this task, while the application of Large Language Models (LLMs) to SR remains unexplored. This work investigates the integration of pre-trained LLMs into the SR pipeline, utilizing an approach that iteratively refines a functional form based on the prediction error it achieves on the observation set, until it reaches convergence. Our method leverages LLMs to propose an initial set of possible functions based on the observations, exploiting their strong pre-training prior. These functions are then iteratively refined by the model itself and by an external optimizer for their coefficients. The process is repeated until the results are satisfactory. We then analyze Vision-Language Models in this context, exploring the inclusion of plots as visual inputs to aid the optimization process. Our findings reveal that LLMs are able to successfully recover good symbolic equations that fit the given data, outperforming SR baselines based on Genetic Programming, with the addition of images in the input showing promising results for the most complex benchmarks.</li>
</ul>

<h3>Title: Espresso: Robust Concept Filtering in Text-to-Image Models</h3>
<ul>
<li><strong>Authors: </strong>Anudeep Das, Vasisht Duddu, Rui Zhang, N. Asokan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19227">https://arxiv.org/abs/2404.19227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19227">https://arxiv.org/pdf/2404.19227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19227]] Espresso: Robust Concept Filtering in Text-to-Image Models(https://arxiv.org/abs/2404.19227)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image (T2I) models generate high-fidelity images for given textual prompts. They are trained on large datasets scraped from the Internet, potentially containing unacceptable concepts (e.g., copyright infringing or unsafe). Retraining T2I models after filtering out unacceptable concepts in the training data is inefficient and degrades utility. Hence, there is a need for concept removal techniques (CRTs) which are effective in removing unacceptable concepts, utility-preserving on acceptable concepts, and robust against evasion with adversarial prompts. None of the prior filtering and fine-tuning CRTs satisfy all these requirements simultaneously. We introduce Espresso, the first robust concept filter based on Contrastive Language-Image Pre-Training (CLIP). It identifies unacceptable concepts by projecting the generated image's embedding onto the vector connecting unacceptable and acceptable concepts in the joint text-image embedding space. This ensures robustness by restricting the adversary to adding noise only along this vector, in the direction of the acceptable concept. Further fine-tuning Espresso to separate embeddings of acceptable and unacceptable concepts, while preserving their pairing with image embeddings, ensures both effectiveness and utility. We evaluate Espresso on eleven concepts to show that it is effective (~5% CLIP accuracy on unacceptable concepts), utility-preserving (~93% normalized CLIP score on acceptable concepts), and robust (~4% CLIP accuracy on adversarial prompts for unacceptable concepts). Finally, we present theoretical bounds for the certified robustness of Espresso against adversarial prompts, and an empirical analysis.</li>
</ul>

<h3>Title: Improved AutoEncoder with LSTM module and KL divergence</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Bingyang Zhang, Kaituo Zhang, Hua Gao, Rongchun Wan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19247">https://arxiv.org/abs/2404.19247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19247">https://arxiv.org/pdf/2404.19247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19247]] Improved AutoEncoder with LSTM module and KL divergence(https://arxiv.org/abs/2404.19247)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The task of anomaly detection is to separate anomalous data from normal data in the dataset. Models such as deep convolutional autoencoder (CAE) network and deep supporting vector data description (SVDD) model have been universally employed and have demonstrated significant success in detecting anomalies. However, the over-reconstruction ability of CAE network for anomalous data can easily lead to high false negative rate in detecting anomalous data. On the other hand, the deep SVDD model has the drawback of feature collapse, which leads to a decrease of detection accuracy for anomalies. To address these problems, we propose the Improved AutoEncoder with LSTM module and Kullback-Leibler divergence (IAE-LSTM-KL) model in this paper. An LSTM network is added after the encoder to memorize feature representations of normal data. In the meanwhile, the phenomenon of feature collapse can also be mitigated by penalizing the featured input to SVDD module via KL divergence. The efficacy of the IAE-LSTM-KL model is validated through experiments on both synthetic and real-world datasets. Experimental results show that IAE-LSTM-KL model yields higher detection accuracy for anomalies. In addition, it is also found that the IAE-LSTM-KL model demonstrates enhanced robustness to contaminated outliers in the dataset.</li>
</ul>

<h3>Title: Mapping New Realities: Ground Truth Image Creation with Pix2Pix  Image-to-Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Zhenglin Li, Bo Guan, Yuanzhou Wei, Yiming Zhou, Jingyu Zhang, Jinxin Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19265">https://arxiv.org/abs/2404.19265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19265">https://arxiv.org/pdf/2404.19265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19265]] Mapping New Realities: Ground Truth Image Creation with Pix2Pix  Image-to-Image Translation(https://arxiv.org/abs/2404.19265)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) have significantly advanced image processing, with Pix2Pix being a notable framework for image-to-image translation. This paper explores a novel application of Pix2Pix to transform abstract map images into realistic ground truth images, addressing the scarcity of such images crucial for domains like urban planning and autonomous vehicle training. We detail the Pix2Pix model's utilization for generating high-fidelity datasets, supported by a dataset of paired map and aerial images, and enhanced by a tailored training regimen. The results demonstrate the model's capability to accurately render complex urban features, establishing its efficacy and potential for broad real-world applications.</li>
</ul>

<h3>Title: Bridge to Non-Barrier Communication: Gloss-Prompted Fine-grained Cued  Speech Gesture Generation with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Wentao Lei, Li Liu, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19277">https://arxiv.org/abs/2404.19277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19277">https://arxiv.org/pdf/2404.19277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19277]] Bridge to Non-Barrier Communication: Gloss-Prompted Fine-grained Cued  Speech Gesture Generation with Diffusion Model(https://arxiv.org/abs/2404.19277)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Cued Speech (CS) is an advanced visual phonetic encoding system that integrates lip reading with hand codings, enabling people with hearing impairments to communicate efficiently. CS video generation aims to produce specific lip and gesture movements of CS from audio or text inputs. The main challenge is that given limited CS data, we strive to simultaneously generate fine-grained hand and finger movements, as well as lip movements, meanwhile the two kinds of movements need to be asynchronously aligned. Existing CS generation methods are fragile and prone to poor performance due to template-based statistical models and careful hand-crafted pre-processing to fit the models. Therefore, we propose a novel Gloss-prompted Diffusion-based CS Gesture generation framework (called GlossDiff). Specifically, to integrate additional linguistic rules knowledge into the model. we first introduce a bridging instruction called \textbf{Gloss}, which is an automatically generated descriptive text to establish a direct and more delicate semantic connection between spoken language and CS gestures. Moreover, we first suggest rhythm is an important paralinguistic feature for CS to improve the communication efficacy. Therefore, we propose a novel Audio-driven Rhythmic Module (ARM) to learn rhythm that matches audio speech. Moreover, in this work, we design, record, and publish the first Chinese CS dataset with four CS cuers. Extensive experiments demonstrate that our method quantitatively and qualitatively outperforms current state-of-the-art (SOTA) methods. We release the code and data at https://glossdiff.github.io/.</li>
</ul>

<h3>Title: Soft Prompt Generation for Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Shuanghao Bai, Yuedi Zhang, Wanqi Zhou, Zhirong Luan, Badong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19286">https://arxiv.org/abs/2404.19286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19286">https://arxiv.org/pdf/2404.19286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19286]] Soft Prompt Generation for Domain Generalization(https://arxiv.org/abs/2404.19286)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large pre-trained vision language models (VLMs) have shown impressive zero-shot ability on downstream tasks with manually designed prompt, which are not optimal for specific domains. To further adapt VLMs to downstream tasks, soft prompt is proposed to replace manually designed prompt, which acts as a learning vector that undergoes fine-tuning based on specific domain data. Prior prompt learning methods primarily learn a fixed prompt and residuled prompt from training samples. However, the learned prompts lack diversity and ignore information about unseen domains, potentially compromising the transferability of the prompts. In this paper, we reframe the prompt learning framework from a generative perspective and propose a simple yet efficient method for the Domain Generalization (DG) task, namely \textbf{S}oft \textbf{P}rompt \textbf{G}eneration (SPG). To the best of our knowledge, we are the first to introduce the generative model into prompt learning in VLMs and explore its potential for producing soft prompts by relying solely on the generative model, ensuring the diversity of prompts. Specifically, SPG consists of a two-stage training phase and an inference phase. During the training phase, we introduce soft prompt labels for each domain, aiming to incorporate the generative model domain knowledge. During the inference phase, the generator of the generative model is employed to obtain instance-specific soft prompts for the unseen target domain. Extensive experiments on five domain generalization benchmarks of three DG tasks demonstrate that our proposed SPG achieves state-of-the-art performance. The code will be available soon.</li>
</ul>

<h3>Title: On Improving the Algorithm-, Model-, and Data- Efficiency of  Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Yun-Hao Cao, Jianxin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19289">https://arxiv.org/abs/2404.19289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19289">https://arxiv.org/pdf/2404.19289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19289]] On Improving the Algorithm-, Model-, and Data- Efficiency of  Self-Supervised Learning(https://arxiv.org/abs/2404.19289)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has developed rapidly in recent years. However, most of the mainstream methods are computationally expensive and rely on two (or more) augmentations for each image to construct positive pairs. Moreover, they mainly focus on large models and large-scale datasets, which lack flexibility and feasibility in many practical applications. In this paper, we propose an efficient single-branch SSL method based on non-parametric instance discrimination, aiming to improve the algorithm, model, and data efficiency of SSL. By analyzing the gradient formula, we correct the update rule of the memory bank with improved performance. We further propose a novel self-distillation loss that minimizes the KL divergence between the probability distribution and its square root version. We show that this alleviates the infrequent updating problem in instance discrimination and greatly accelerates convergence. We systematically compare the training overhead and performance of different methods in different scales of data, and under different backbones. Experimental results show that our method outperforms various baselines with significantly less overhead, and is especially effective for limited amounts of data and small models.</li>
</ul>

<h3>Title: A Light-weight Transformer-based Self-supervised Matching Network for  Heterogeneous Images</h3>
<ul>
<li><strong>Authors: </strong>Wang Zhang, Tingting Li, Yuntian Zhang, Gensheng Pei, Xiruo Jiang, Yazhou Yao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19311">https://arxiv.org/abs/2404.19311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19311">https://arxiv.org/pdf/2404.19311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19311]] A Light-weight Transformer-based Self-supervised Matching Network for  Heterogeneous Images(https://arxiv.org/abs/2404.19311)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Matching visible and near-infrared (NIR) images remains a significant challenge in remote sensing image fusion. The nonlinear radiometric differences between heterogeneous remote sensing images make the image matching task even more difficult. Deep learning has gained substantial attention in computer vision tasks in recent years. However, many methods rely on supervised learning and necessitate large amounts of annotated data. Nevertheless, annotated data is frequently limited in the field of remote sensing image matching. To address this challenge, this paper proposes a novel keypoint descriptor approach that obtains robust feature descriptors via a self-supervised matching network. A light-weight transformer network, termed as LTFormer, is designed to generate deep-level feature descriptors. Furthermore, we implement an innovative triplet loss function, LT Loss, to enhance the matching performance further. Our approach outperforms conventional hand-crafted local feature descriptors and proves equally competitive compared to state-of-the-art deep learning-based methods, even amidst the shortage of annotated data.</li>
</ul>

<h3>Title: Probing Unlearned Diffusion Models: A Transferable Adversarial Attack  Perspective</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxuan Han, Songlin Yang, Wei Wang, Yang Li, Jing Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19382">https://arxiv.org/abs/2404.19382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19382">https://arxiv.org/pdf/2404.19382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19382]] Probing Unlearned Diffusion Models: A Transferable Adversarial Attack  Perspective(https://arxiv.org/abs/2404.19382)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Advanced text-to-image diffusion models raise safety concerns regarding identity privacy violation, copyright infringement, and Not Safe For Work content generation. Towards this, unlearning methods have been developed to erase these involved concepts from diffusion models. However, these unlearning methods only shift the text-to-image mapping and preserve the visual content within the generative space of diffusion models, leaving a fatal flaw for restoring these erased concepts. This erasure trustworthiness problem needs probe, but previous methods are sub-optimal from two perspectives: (1) Lack of transferability: Some methods operate within a white-box setting, requiring access to the unlearned model. And the learned adversarial input often fails to transfer to other unlearned models for concept restoration; (2) Limited attack: The prompt-level methods struggle to restore narrow concepts from unlearned models, such as celebrity identity. Therefore, this paper aims to leverage the transferability of the adversarial attack to probe the unlearning robustness under a black-box setting. This challenging scenario assumes that the unlearning method is unknown and the unlearned model is inaccessible for optimization, requiring the attack to be capable of transferring across different unlearned models. Specifically, we employ an adversarial search strategy to search for the adversarial embedding which can transfer across different unlearned models. This strategy adopts the original Stable Diffusion model as a surrogate model to iteratively erase and search for embeddings, enabling it to find the embedding that can restore the target concept for different unlearning methods. Extensive experiments demonstrate the transferability of the searched adversarial embedding across several state-of-the-art unlearning methods and its effectiveness for different levels of concepts.</li>
</ul>

<h3>Title: Which Nigerian-Pidgin does Generative AI speak?: Issues about  Representativeness and Bias for Multilingual and Low Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>David Ifeoluwa Adelani, A. Seza Doğruöz, Iyanuoluwa Shode, Anuoluwapo Aremu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19442">https://arxiv.org/abs/2404.19442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19442">https://arxiv.org/pdf/2404.19442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19442]] Which Nigerian-Pidgin does Generative AI speak?: Issues about  Representativeness and Bias for Multilingual and Low Resource Languages(https://arxiv.org/abs/2404.19442)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Naija is the Nigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed language (e.g., English, Portuguese and Indigenous languages). Although it has mainly been a spoken language until recently, there are currently two written genres (BBC and Wikipedia) in Naija. Through statistical analyses and Machine Translation experiments, we prove that these two genres do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on Naija written in the BBC genre. In other words, Naija written in Wikipedia genre is not represented in Generative AI.</li>
</ul>

<h3>Title: AnomalyXFusion: Multi-modal Anomaly Synthesis with Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jie Hu, Yawen Huang, Yilin Lu, Guoyang Xie, Guannan Jiang, Yefeng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19444">https://arxiv.org/abs/2404.19444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19444">https://arxiv.org/pdf/2404.19444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19444]] AnomalyXFusion: Multi-modal Anomaly Synthesis with Diffusion(https://arxiv.org/abs/2404.19444)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly synthesis is one of the effective methods to augment abnormal samples for training. However, current anomaly synthesis methods predominantly rely on texture information as input, which limits the fidelity of synthesized abnormal samples. Because texture information is insufficient to correctly depict the pattern of anomalies, especially for logical anomalies. To surmount this obstacle, we present the AnomalyXFusion framework, designed to harness multi-modality information to enhance the quality of synthesized abnormal samples. The AnomalyXFusion framework comprises two distinct yet synergistic modules: the Multi-modal In-Fusion (MIF) module and the Dynamic Dif-Fusion (DDF) module. The MIF module refines modality alignment by aggregating and integrating various modality features into a unified embedding space, termed X-embedding, which includes image, text, and mask features. Concurrently, the DDF module facilitates controlled generation through an adaptive adjustment of X-embedding conditioned on the diffusion steps. In addition, to reveal the multi-modality representational power of AnomalyXFusion, we propose a new dataset, called MVTec Caption. More precisely, MVTec Caption extends 2.2k accurate image-mask-text annotations for the MVTec AD and LOCO datasets. Comprehensive evaluations demonstrate the effectiveness of AnomalyXFusion, especially regarding the fidelity and diversity for logical anomalies. Project page: http:github.com/hujiecpp/MVTec-Caption</li>
</ul>

<h3>Title: TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image  Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Teng Zhou, Yongchuan Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19475">https://arxiv.org/abs/2404.19475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19475">https://arxiv.org/pdf/2404.19475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19475]] TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image  Generation with Diffusion Models(https://arxiv.org/abs/2404.19475)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as effective tools for generating diverse and high-quality content. However, their capability in high-resolution image generation, particularly for panoramic images, still faces challenges such as visible seams and incoherent transitions. In this paper, we propose TwinDiffusion, an optimized framework designed to address these challenges through two key innovations: Crop Fusion for quality enhancement and Cross Sampling for efficiency optimization. We introduce a training-free optimizing stage to refine the similarity of the adjacent image areas, as well as an interleaving sampling strategy to yield dynamic patches during the cropping process. A comprehensive evaluation is conducted to compare TwinDiffusion with the existing methods, considering factors including coherence, fidelity, compatibility, and efficiency. The results demonstrate the superior performance of our approach in generating seamless and coherent panoramas, setting a new standard in quality and efficiency for panoramic image generation.</li>
</ul>

<h3>Title: MicroDreamer: Zero-shot 3D Generation in $\sim$20 Seconds by Score-based  Iterative Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Luxi Chen, Zhengyi Wang, Chongxuan Li, Tingting Gao, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19525">https://arxiv.org/abs/2404.19525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19525">https://arxiv.org/pdf/2404.19525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19525]] MicroDreamer: Zero-shot 3D Generation in $\sim$20 Seconds by Score-based  Iterative Reconstruction(https://arxiv.org/abs/2404.19525)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Optimization-based approaches, such as score distillation sampling (SDS), show promise in zero-shot 3D generation but suffer from low efficiency, primarily due to the high number of function evaluations (NFEs) required for each sample. In this paper, we introduce score-based iterative reconstruction (SIR), an efficient and general algorithm for 3D generation with a multi-view score-based diffusion model. Given the images produced by the diffusion model, SIR reduces NFEs by repeatedly optimizing 3D parameters, unlike the single optimization in SDS, mimicking the 3D reconstruction process. With other improvements including optimization in the pixel space, we present an efficient approach called MicroDreamer that generally applies to various 3D representations and 3D generation tasks. In particular, retaining a comparable performance, MicroDreamer is 5-20 times faster than SDS in generating neural radiance field and takes about 20 seconds to generate meshes from 3D Gaussian splitting on a single A100 GPU, halving the time of the fastest zero-shot baseline, DreamGaussian. Our code is available at https://github.com/ML-GSAI/MicroDreamer.</li>
</ul>

<h3>Title: MoST: Multi-modality Scene Tokenization for Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Norman Mu, Jingwei Ji, Zhenpei Yang, Nate Harada, Haotian Tang, Kan Chen, Charles R. Qi, Runzhou Ge, Kratarth Goel, Zoey Yang, Scott Ettinger, Rami Al-Rfou, Dragomir Anguelov, Yin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19531">https://arxiv.org/abs/2404.19531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19531">https://arxiv.org/pdf/2404.19531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19531]] MoST: Multi-modality Scene Tokenization for Motion Prediction(https://arxiv.org/abs/2404.19531)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Many existing motion prediction approaches rely on symbolic perception outputs to generate agent trajectories, such as bounding boxes, road graph information and traffic lights. This symbolic representation is a high-level abstraction of the real world, which may render the motion prediction model vulnerable to perception errors (e.g., failures in detecting open-vocabulary obstacles) while missing salient information from the scene context (e.g., poor road conditions). An alternative paradigm is end-to-end learning from raw sensors. However, this approach suffers from the lack of interpretability and requires significantly more training resources. In this work, we propose tokenizing the visual world into a compact set of scene elements and then leveraging pre-trained image foundation models and LiDAR neural networks to encode all the scene elements in an open-vocabulary manner. The image foundation model enables our scene tokens to encode the general knowledge of the open world while the LiDAR neural network encodes geometry information. Our proposed representation can efficiently encode the multi-frame multi-modality observations with a few hundred tokens and is compatible with most transformer-based architectures. To evaluate our method, we have augmented Waymo Open Motion Dataset with camera embeddings. Experiments over Waymo Open Motion Dataset show that our approach leads to significant performance improvements over the state-of-the-art.</li>
</ul>

<h3>Title: Perceptual Constancy Constrained Single Opinion Score Calibration for  Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Lei Wang, Desen Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19595">https://arxiv.org/abs/2404.19595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19595">https://arxiv.org/pdf/2404.19595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19595]] Perceptual Constancy Constrained Single Opinion Score Calibration for  Image Quality Assessment(https://arxiv.org/abs/2404.19595)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a highly efficient method to estimate an image's mean opinion score (MOS) from a single opinion score (SOS). Assuming that each SOS is the observed sample of a normal distribution and the MOS is its unknown expectation, the MOS inference is formulated as a maximum likelihood estimation problem, where the perceptual correlation of pairwise images is considered in modeling the likelihood of SOS. More specifically, by means of the quality-aware representations learned from the self-supervised backbone, we introduce a learnable relative quality measure to predict the MOS difference between two images. Then, the current image's maximum likelihood estimation towards MOS is represented by the sum of another reference image's estimated MOS and their relative quality. Ideally, no matter which image is selected as the reference, the MOS of the current image should remain unchanged, which is termed perceptual cons tancy constrained calibration (PC3). Finally, we alternatively optimize the relative quality measure's parameter and the current image's estimated MOS via backpropagation and Newton's method respectively. Experiments show that the proposed method is efficient in calibrating the biased SOS and significantly improves IQA model learning when only SOSs are available.</li>
</ul>

<h3>Title: Seeing Through the Clouds: Cloud Gap Imputation with Prithvi Foundation  Model</h3>
<ul>
<li><strong>Authors: </strong>Denys Godwin, Hanxi Li, Michael Cecil, Hamed Alemohammad</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19609">https://arxiv.org/abs/2404.19609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19609">https://arxiv.org/pdf/2404.19609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19609]] Seeing Through the Clouds: Cloud Gap Imputation with Prithvi Foundation  Model(https://arxiv.org/abs/2404.19609)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Filling cloudy pixels in multispectral satellite imagery is essential for accurate data analysis and downstream applications, especially for tasks which require time series data. To address this issue, we compare the performance of a foundational Vision Transformer (ViT) model with a baseline Conditional Generative Adversarial Network (CGAN) model for missing value imputation in time series of multispectral satellite imagery. We randomly mask time series of satellite images using real-world cloud masks and train each model to reconstruct the missing pixels. The ViT model is fine-tuned from a pretrained model, while the CGAN is trained from scratch. Using quantitative evaluation metrics such as structural similarity index and mean absolute error as well as qualitative visual analysis, we assess imputation accuracy and contextual preservation.</li>
</ul>

<h3>Title: SemiPL: A Semi-supervised Method for Event Sound Source Localization</h3>
<ul>
<li><strong>Authors: </strong>Yue Li, Baiqiao Yin, Jinfu Liu, Jiajun Wen, Jiaying Lin, Mengyuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19615">https://arxiv.org/abs/2404.19615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19615">https://arxiv.org/pdf/2404.19615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19615]] SemiPL: A Semi-supervised Method for Event Sound Source Localization(https://arxiv.org/abs/2404.19615)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In recent years, Event Sound Source Localization has been widely applied in various fields. Recent works typically relying on the contrastive learning framework show impressive performance. However, all work is based on large relatively simple datasets. It's also crucial to understand and analyze human behaviors (actions and interactions of people), voices, and sounds in chaotic events in many applications, e.g., crowd management, and emergency response services. In this paper, we apply the existing model to a more complex dataset, explore the influence of parameters on the model, and propose a semi-supervised improvement method SemiPL. With the increase in data quantity and the influence of label quality, self-supervised learning will be an unstoppable trend. The experiment shows that the parameter adjustment will positively affect the existing model. In particular, SSPL achieved an improvement of 12.2% cIoU and 0.56% AUC in Chaotic World compared to the results provided. The code is available at: https://github.com/ly245422/SSPL</li>
</ul>

<h3>Title: On Training a Neural Network to Explain Binaries</h3>
<ul>
<li><strong>Authors: </strong>Alexander Interrante-Grant, Andy Davis, Heather Preslier, Tim Leek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19631">https://arxiv.org/abs/2404.19631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19631">https://arxiv.org/pdf/2404.19631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19631]] On Training a Neural Network to Explain Binaries(https://arxiv.org/abs/2404.19631)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we begin to investigate the possibility of training a deep neural network on the task of binary code understanding. Specifically, the network would take, as input, features derived directly from binaries and output English descriptions of functionality to aid a reverse engineer in investigating the capabilities of a piece of closed-source software, be it malicious or benign. Given recent success in applying large language models (generative AI) to the task of source code summarization, this seems a promising direction. However, in our initial survey of the available datasets, we found nothing of sufficiently high quality and volume to train these complex models. Instead, we build our own dataset derived from a capture of Stack Overflow containing 1.1M entries. A major result of our work is a novel dataset evaluation method using the correlation between two distances on sample pairs: one distance in the embedding space of inputs and the other in the embedding space of outputs. Intuitively, if two samples have inputs close in the input embedding space, their outputs should also be close in the output embedding space. We found this Embedding Distance Correlation (EDC) test to be highly diagnostic, indicating that our collected dataset and several existing open-source datasets are of low quality as the distances are not well correlated. We proceed to explore the general applicability of EDC, applying it to a number of qualitatively known good datasets and a number of synthetically known bad ones and found it to be a reliable indicator of dataset value.</li>
</ul>

<h3>Title: MetaCoCo: A New Few-Shot Classification Benchmark with Spurious  Correlation</h3>
<ul>
<li><strong>Authors: </strong>Min Zhang, Haoxuan Li, Fei Wu, Kun Kuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19644">https://arxiv.org/abs/2404.19644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19644">https://arxiv.org/pdf/2404.19644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19644]] MetaCoCo: A New Few-Shot Classification Benchmark with Spurious  Correlation(https://arxiv.org/abs/2404.19644)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications. Recent studies suggest that the OOD problems in FSC mainly including: (a) cross-domain few-shot classification (CD-FSC) and (b) spurious-correlation few-shot classification (SC-FSC). Specifically, CD-FSC occurs when a classifier learns transferring knowledge from base classes drawn from seen training distributions but recognizes novel classes sampled from unseen testing distributions. In contrast, SC-FSC arises when a classifier relies on non-causal features (or contexts) that happen to be correlated with the labels (or concepts) in base classes but such relationships no longer hold during the model deployment. Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks. To this end, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios. Moreover, to quantify the extent of spurious-correlation shifts of the presented MetaCoCo, we further propose a metric by using CLIP as a pre-trained vision-language model. Extensive experiments on the proposed benchmark are performed to evaluate the state-of-the-art methods in FSC, cross-domain shifts, and self-supervised learning. The experimental results show that the performance of the existing methods degrades significantly in the presence of spurious-correlation shifts. We open-source all codes of our benchmark and hope that the proposed MetaCoCo can facilitate future research on spurious-correlation shifts problems in FSC. The code is available at: https://github.com/remiMZ/MetaCoCo-ICLR24.</li>
</ul>

<h3>Title: Landmark Alternating Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Sing-Yuan Yeh, Hau-Tieng Wu, Ronen Talmon, Mao-Pei Tsui</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, physics.data-an, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19649">https://arxiv.org/abs/2404.19649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19649">https://arxiv.org/pdf/2404.19649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19649]] Landmark Alternating Diffusion(https://arxiv.org/abs/2404.19649)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Alternating Diffusion (AD) is a commonly applied diffusion-based sensor fusion algorithm. While it has been successfully applied to various problems, its computational burden remains a limitation. Inspired by the landmark diffusion idea considered in the Robust and Scalable Embedding via Landmark Diffusion (ROSELAND), we propose a variation of AD, called Landmark AD (LAD), which captures the essence of AD while offering superior computational efficiency. We provide a series of theoretical analyses of LAD under the manifold setup and apply it to the automatic sleep stage annotation problem with two electroencephalogram channels to demonstrate its application.</li>
</ul>

<h3>Title: Masked Multi-Query Slot Attention for Unsupervised Object Discovery</h3>
<ul>
<li><strong>Authors: </strong>Rishav Pramanik, José-Fabian Villa-Vásquez, Marco Pedersoli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19654">https://arxiv.org/abs/2404.19654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19654">https://arxiv.org/pdf/2404.19654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19654]] Masked Multi-Query Slot Attention for Unsupervised Object Discovery(https://arxiv.org/abs/2404.19654)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Unsupervised object discovery is becoming an essential line of research for tackling recognition problems that require decomposing an image into entities, such as semantic segmentation and object detection. Recently, object-centric methods that leverage self-supervision have gained popularity, due to their simplicity and adaptability to different settings and conditions. However, those methods do not exploit effective techniques already employed in modern self-supervised approaches. In this work, we consider an object-centric approach in which DINO ViT features are reconstructed via a set of queried representations called slots. Based on that, we propose a masking scheme on input features that selectively disregards the background regions, inducing our model to focus more on salient objects during the reconstruction phase. Moreover, we extend the slot attention to a multi-query approach, allowing the model to learn multiple sets of slots, producing more stable masks. During training, these multiple sets of slots are learned independently while, at test time, these sets are merged through Hungarian matching to obtain the final slots. Our experimental results and ablations on the PASCAL-VOC 2012 dataset show the importance of each component and highlight how their combination consistently improves object localization. Our source code is available at: https://github.com/rishavpramanik/maskedmultiqueryslot</li>
</ul>

<h3>Title: Better & Faster Large Language Models via Multi-token Prediction</h3>
<ul>
<li><strong>Authors: </strong>Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, Gabriel Synnaeve</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19737">https://arxiv.org/abs/2404.19737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19737">https://arxiv.org/pdf/2404.19737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19737]] Better & Faster Large Language Models via Multi-token Prediction(https://arxiv.org/abs/2404.19737)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.</li>
</ul>

<h3>Title: Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Paul Engstler, Andrea Vedaldi, Iro Laina, Christian Rupprecht</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19758">https://arxiv.org/abs/2404.19758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19758">https://arxiv.org/pdf/2404.19758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19758]] Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting(https://arxiv.org/abs/2404.19758)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>3D scene generation has quickly become a challenging new research direction, fueled by consistent improvements of 2D generative diffusion models. Most prior work in this area generates scenes by iteratively stitching newly generated frames with existing geometry. These works often depend on pre-trained monocular depth estimators to lift the generated images into 3D, fusing them with the existing scene representation. These approaches are then often evaluated via a text metric, measuring the similarity between the generated images and a given text prompt. In this work, we make two fundamental contributions to the field of 3D scene generation. First, we note that lifting images to 3D with a monocular depth estimation model is suboptimal as it ignores the geometry of the existing scene. We thus introduce a novel depth completion model, trained via teacher distillation and self-training to learn the 3D fusion process, resulting in improved geometric coherence of the scene. Second, we introduce a new benchmarking scheme for scene generation methods that is based on ground truth geometry, and thus measures the quality of the structure of the scene.</li>
</ul>

<h3>Title: MotionLCM: Real-time Controllable Motion Generation via Latent  Consistency Model</h3>
<ul>
<li><strong>Authors: </strong>Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, Yansong Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.19759">https://arxiv.org/abs/2404.19759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.19759">https://arxiv.org/pdf/2404.19759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.19759]] MotionLCM: Real-time Controllable Motion Generation via Latent  Consistency Model(https://arxiv.org/abs/2404.19759)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work introduces MotionLCM, extending controllable motion generation to a real-time level. Existing methods for spatial control in text-conditioned motion generation suffer from significant runtime inefficiency. To address this issue, we first propose the motion latent consistency model (MotionLCM) for motion generation, building upon the latent diffusion model (MLD). By employing one-step (or few-step) inference, we further improve the runtime efficiency of the motion latent diffusion model for motion generation. To ensure effective controllability, we incorporate a motion ControlNet within the latent space of MotionLCM and enable explicit control signals (e.g., pelvis trajectory) in the vanilla motion space to control the generation process directly, similar to controlling other latent-free diffusion models for motion generation. By employing these techniques, our approach can generate human motions with text and control signals in real-time. Experimental results demonstrate the remarkable generation and controlling capabilities of MotionLCM while maintaining real-time runtime efficiency.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
