<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-26</h1>
<h3>Title: Towards Latent Diffusion Suitable For Text</h3>
<ul>
<li><strong>Authors: </strong>Nesta Midavaine, Christian A. Naesseth, Grigory Bartosh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16220">https://arxiv.org/abs/2601.16220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16220">https://arxiv.org/pdf/2601.16220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16220]] Towards Latent Diffusion Suitable For Text(https://arxiv.org/abs/2601.16220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Language diffusion models aim to improve sampling speed and coherence over autoregressive LLMs. We introduce Neural Flow Diffusion Models for language generation, an extension of NFDM that enables the straightforward application of continuous diffusion models to discrete state spaces. NFDM learns a multivariate forward process from the data, ensuring that the forward process and generative trajectory are a good fit for language modeling. Our model substantially reduces the likelihood gap with autoregressive models of the same size, while achieving sample quality comparable to that of previous latent diffusion models.</li>
</ul>

<h3>Title: GR3EN: Generative Relighting for 3D Environments</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyan Xing, Philipp Henzler, Junhwa Hur, Runze Li, Jonathan T. Barron, Pratul P. Srinivasan, Dor Verbin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16272">https://arxiv.org/abs/2601.16272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16272">https://arxiv.org/pdf/2601.16272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16272]] GR3EN: Generative Relighting for 3D Environments(https://arxiv.org/abs/2601.16272)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.</li>
</ul>

<h3>Title: Better as Generators Than Classifiers: Leveraging LLMs and Synthetic Data for Low-Resource Multilingual Classification</h3>
<ul>
<li><strong>Authors: </strong>Branislav Pecher, Jan Cegin, Robert Belanec, Ivan Srba, Jakub Simko, Maria Bielikova</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16278">https://arxiv.org/abs/2601.16278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16278">https://arxiv.org/pdf/2601.16278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16278]] Better as Generators Than Classifiers: Leveraging LLMs and Synthetic Data for Low-Resource Multilingual Classification(https://arxiv.org/abs/2601.16278)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable multilingual capabilities, making them promising tools in both high- and low-resource languages. One particularly valuable use case is generating synthetic samples that can be used to train smaller models in low-resource scenarios where human-labelled data is scarce. In this work, we investigate whether these synthetic data generation capabilities can serve as a form of distillation, producing smaller models that perform on par with or even better than massive LLMs across languages and tasks. To this end, we use a state-of-the-art multilingual LLM to generate synthetic datasets covering 11 languages and 4 classification tasks. These datasets are then used to train smaller models via fine-tuning or instruction tuning, or as synthetic in-context examples for compact LLMs. Our experiments show that even small amounts of synthetic data enable smaller models to outperform the large generator itself, particularly in low-resource languages. Overall, the results suggest that LLMs are best utilised as generators (teachers) rather than classifiers, producing data that empowers smaller and more efficient multilingual models.</li>
</ul>

<h3>Title: Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory</h3>
<ul>
<li><strong>Authors: </strong>Dohun Lee, Chun-Hao Paul Huang, Xuelin Chen, Jong Chul Ye, Duygu Ceylan, Hyeonho Jeong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16296">https://arxiv.org/abs/2601.16296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16296">https://arxiv.org/pdf/2601.16296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16296]] Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory(https://arxiv.org/abs/2601.16296)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: this https URL</li>
</ul>

<h3>Title: Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments</h3>
<ul>
<li><strong>Authors: </strong>Aditya K Surikuchi, Raquel Fernández, Sandro Pezzelle</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16333">https://arxiv.org/abs/2601.16333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16333">https://arxiv.org/pdf/2601.16333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16333]] Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments(https://arxiv.org/abs/2601.16333)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models are used for many real-world applications involving language generation from temporally-ordered multimodal events. In this work, we study the ability of models to identify the most important sub-events in a video, which is a fundamental prerequisite for narrating or summarizing multimodal events. Specifically, we focus on football games and evaluate models on their ability to distinguish between important and non-important sub-events in a game. To this end, we construct a new dataset by leveraging human preferences for importance implicit in football game highlight reels, without any additional annotation costs. Using our dataset, which we will publicly release to the community, we compare several state-of-the-art multimodal models and show that they are not far from chance level performance. Analyses of models beyond standard evaluation metrics reveal their tendency to rely on a single dominant modality and their ineffectiveness in synthesizing necessary information from multiple sources. Our findings underline the importance of modular architectures that can handle sample-level heterogeneity in multimodal data and the need for complementary training procedures that can maximize cross-modal synergy.</li>
</ul>

<h3>Title: VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Jiang, Yunkang Cao, Yuqi Cheng, Yiheng Zhang, Weiming Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16381">https://arxiv.org/abs/2601.16381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16381">https://arxiv.org/pdf/2601.16381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16381]] VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection(https://arxiv.org/abs/2601.16381)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.</li>
</ul>

<h3>Title: Jacobian Scopes: token-level causal attributions in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Toni J.B. Liu, Baran Zadeoğlu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16407">https://arxiv.org/abs/2601.16407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16407">https://arxiv.org/pdf/2601.16407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16407]] Jacobian Scopes: token-level causal attributions in LLMs(https://arxiv.org/abs/2601.16407)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) make next-token predictions based on clues present in their context, such as semantic descriptions and in-context examples. Yet, elucidating which prior tokens most strongly influence a given prediction remains challenging due to the proliferation of layers and attention heads in modern architectures. We propose Jacobian Scopes, a suite of gradient-based, token-level causal attribution methods for interpreting LLM predictions. By analyzing the linearized relations of final hidden state with respect to inputs, Jacobian Scopes quantify how input tokens influence a model's prediction. We introduce three variants - Semantic, Fisher, and Temperature Scopes - which respectively target sensitivity of specific logits, the full predictive distribution, and model confidence (inverse temperature). Through case studies spanning instruction understanding, translation and in-context learning (ICL), we uncover interesting findings, such as when Jacobian Scopes point to implicit political biases. We believe that our proposed methods also shed light on recently debated mechanisms underlying in-context time-series forecasting. Our code and interactive demonstrations are publicly available at this https URL.</li>
</ul>

<h3>Title: AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose</h3>
<ul>
<li><strong>Authors: </strong>Jongmin Yu, Hyeontaek Oh, Zhongtian Sun, Angelica I Aviles-Rivero, Moongu Jeon, Jinhong Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16429">https://arxiv.org/abs/2601.16429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16429">https://arxiv.org/pdf/2601.16429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16429]] AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose(https://arxiv.org/abs/2601.16429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `this https URL.</li>
</ul>

<h3>Title: VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology</h3>
<ul>
<li><strong>Authors: </strong>Peixian Liang, Songhao Li, Shunsuke Koga, Yutong Li, Zahra Alipour, Yucheng Tang, Daguang Xu, Zhi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16451">https://arxiv.org/abs/2601.16451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16451">https://arxiv.org/pdf/2601.16451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16451]] VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology(https://arxiv.org/abs/2601.16451)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at this https URL.</li>
</ul>

<h3>Title: A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study</h3>
<ul>
<li><strong>Authors: </strong>Maxwell Reynolds, Chaitanya Srinivasan, Vijay Cherupally, Michael Leone, Ke Yu, Li Sun, Tigmanshu Chaudhary, Andreas Pfenning, Kayhan Batmanghelich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16467">https://arxiv.org/abs/2601.16467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16467">https://arxiv.org/pdf/2601.16467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16467]] A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study(https://arxiv.org/abs/2601.16467)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Discovery of sensitive and biologically grounded biomarkers is essential for early detection and monitoring of Alzheimer's disease (AD). Structural MRI is widely available but typically relies on hand-crafted features such as cortical thickness or volume. We ask whether self-supervised learning (SSL) can uncover more powerful biomarkers from the same data. Existing SSL methods underperform FreeSurfer-derived features in disease classification, conversion prediction, and amyloid status prediction. We introduce Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information. R-NCE outperforms traditional features and existing SSL methods across multiple benchmarks, including AD conversion prediction. To assess biological relevance, we derive Brain Age Gap (BAG) measures and perform genome-wide association studies. R-NCE-BAG shows high heritability and associations with MAPT and IRAG1, with enrichment in astrocytes and oligodendrocytes, indicating sensitivity to neurodegenerative and cerebrovascular processes.</li>
</ul>

<h3>Title: Secure Intellicise Wireless Network: Agentic AI for Coverless Semantic Steganography Communication</h3>
<ul>
<li><strong>Authors: </strong>Rui Meng, Song Gao, Bingxuan Xu, Xiaodong Xu, Jianqiao Chen, Nan Ma, Pei Xiao, Ping Zhang, Rahim Tafazolli</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16472">https://arxiv.org/abs/2601.16472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16472">https://arxiv.org/pdf/2601.16472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16472]] Secure Intellicise Wireless Network: Agentic AI for Coverless Semantic Steganography Communication(https://arxiv.org/abs/2601.16472)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Semantic Communication (SemCom), leveraging its significant advantages in transmission efficiency and reliability, has emerged as a core technology for constructing future intellicise (intelligent and concise) wireless networks. However, intelligent attacks represented by semantic eavesdropping pose severe challenges to the security of SemCom. To address this challenge, Semantic Steganographic Communication (SemSteCom) achieves ``invisible'' encryption by implicitly embedding private semantic information into cover modality carriers. The state-of-the-art study has further introduced generative diffusion models to directly generate stega images without relying on original cover images, effectively enhancing steganographic capacity. Nevertheless, the recovery process of private images is highly dependent on the guidance of private semantic keys, which may be inferred by intelligent eavesdroppers, thereby introducing new security threats. To address this issue, we propose an Agentic AI-driven SemSteCom (AgentSemSteCom) scheme, which includes semantic extraction, digital token controlled reference image generation, coverless steganography, semantic codec, and optional task-oriented enhancement modules. The proposed AgentSemSteCom scheme obviates the need for both cover images and private semantic keys, thereby boosting steganographic capacity while reinforcing transmission security. The simulation results on open-source datasets verify that, AgentSemSteCom achieves better transmission quality and higher security levels than the baseline scheme.</li>
</ul>

<h3>Title: SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Tongcheng Fang, Hanling Zhang, Ruiqi Xie, Zhuo Han, Xin Tao, Tianchen Zhao, Pengfei Wan, Wenbo Ding, Wanli Ouyang, Xuefei Ning, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16515">https://arxiv.org/abs/2601.16515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16515">https://arxiv.org/pdf/2601.16515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16515]] SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer(https://arxiv.org/abs/2601.16515)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.</li>
</ul>

<h3>Title: Rethinking Large Language Models For Irregular Time Series Classification In Critical Care</h3>
<ul>
<li><strong>Authors: </strong>Feixiang Zheng, Yu Wu, Cecilia Mascolo, Ting Dang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16516">https://arxiv.org/abs/2601.16516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16516">https://arxiv.org/pdf/2601.16516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16516]] Rethinking Large Language Models For Irregular Time Series Classification In Critical Care(https://arxiv.org/abs/2601.16516)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Time series data from the Intensive Care Unit (ICU) provides critical information for patient monitoring. While recent advancements in applying Large Language Models (LLMs) to time series modeling (TSM) have shown great promise, their effectiveness on the irregular ICU data, characterized by particularly high rates of missing values, remains largely unexplored. This work investigates two key components underlying the success of LLMs for TSM: the time series encoder and the multimodal alignment strategy. To this end, we establish a systematic testbed to evaluate their impact across various state-of-the-art LLM-based methods on benchmark ICU datasets against strong supervised and self-supervised baselines. Results reveal that the encoder design is more critical than the alignment strategy. Encoders that explicitly model irregularity achieve substantial performance gains, yielding an average AUPRC increase of $12.8\%$ over the vanilla Transformer. While less impactful, the alignment strategy is also noteworthy, with the best-performing semantically rich, fusion-based strategy achieving a modest $2.9\%$ improvement over cross-attention. However, LLM-based methods require at least 10$\times$ longer training than the best-performing irregular supervised models, while delivering only comparable performance. They also underperform in data-scarce few-shot learning settings. These findings highlight both the promise and current limitations of LLMs for irregular ICU time series. The code is available at this https URL.</li>
</ul>

<h3>Title: AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding</h3>
<ul>
<li><strong>Authors: </strong>Runmao Yao, Junsheng Zhou, Zhen Dong, Yu-Shen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16532">https://arxiv.org/abs/2601.16532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16532">https://arxiv.org/pdf/2601.16532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16532]] AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding(https://arxiv.org/abs/2601.16532)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation.</li>
</ul>

<h3>Title: Semi-Supervised Hierarchical Open-Set Classification</h3>
<ul>
<li><strong>Authors: </strong>Erik Wallin, Fredrik Kahl, Lars Hammarstrand</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16541">https://arxiv.org/abs/2601.16541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16541">https://arxiv.org/pdf/2601.16541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16541]] Semi-Supervised Hierarchical Open-Set Classification(https://arxiv.org/abs/2601.16541)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Hierarchical open-set classification handles previously unseen classes by assigning them to the most appropriate high-level category in a class taxonomy. We extend this paradigm to the semi-supervised setting, enabling the use of large-scale, uncurated datasets containing a mixture of known and unknown classes to improve the hierarchical open-set performance. To this end, we propose a teacher-student framework based on pseudo-labeling. Two key components are introduced: 1) subtree pseudo-labels, which provide reliable supervision in the presence of unknown data, and 2) age-gating, a mechanism that mitigates overconfidence in pseudo-labels. Experiments show that our framework outperforms self-supervised pretraining followed by supervised adaptation, and even matches the fully supervised counterpart when using only 20 labeled samples per class on the iNaturalist19 benchmark. Our code is available at this https URL.</li>
</ul>

<h3>Title: Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Abdurahman Maarouf, Alket Bakiaj, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16568">https://arxiv.org/abs/2601.16568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16568">https://arxiv.org/pdf/2601.16568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16568]] Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach(https://arxiv.org/abs/2601.16568)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.</li>
</ul>

<h3>Title: A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jingsong Xia, Siqi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16608">https://arxiv.org/abs/2601.16608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16608">https://arxiv.org/pdf/2601.16608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16608]] A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling(https://arxiv.org/abs/2601.16608)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Intelligent medical image analysis is essential for clinical decision support but is often limited by scarce annotations, constrained computational resources, and suboptimal model generalization. To address these challenges, we propose a lightweight medical image classification framework that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. MobileNetV2 is employed as a compact backbone and pretrained using a SimCLR-style self-supervised paradigm on unlabeled images. A lightweight parameterized quantum circuit (PQC) is embedded as a quantum feature enhancement module, forming a hybrid classical-quantum architecture, which is subsequently fine-tuned on limited labeled data. Experimental results demonstrate that, with only approximately 2-3 million parameters and low computational cost, the proposed method consistently outperforms classical baselines without self-supervised learning or quantum enhancement in terms of Accuracy, AUC, and F1-score. Feature visualization further indicates improved discriminability and representation stability. Overall, this work provides a practical and forward-looking solution for high-performance medical artificial intelligence under resource-constrained settings.</li>
</ul>

<h3>Title: MultiLexNorm++: A Unified Benchmark and a Generative Model for Lexical Normalization for Asian Languages</h3>
<ul>
<li><strong>Authors: </strong>Weerayut Buaphet, Thanh-Nhi Nguyen, Risa Kondo, Tomoyuki Kajiwara, Yumin Kim, Jimin Lee, Hwanhee Lee, Holy Lovenia, Peerat Limkonchotiwat, Sarana Nutanong, Rob Van der Goot</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16623">https://arxiv.org/abs/2601.16623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16623">https://arxiv.org/pdf/2601.16623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16623]] MultiLexNorm++: A Unified Benchmark and a Generative Model for Lexical Normalization for Asian Languages(https://arxiv.org/abs/2601.16623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Social media data has been of interest to Natural Language Processing (NLP) practitioners for over a decade, because of its richness in information, but also challenges for automatic processing. Since language use is more informal, spontaneous, and adheres to many different sociolects, the performance of NLP models often deteriorates. One solution to this problem is to transform data to a standard variant before processing it, which is also called lexical normalization. There has been a wide variety of benchmarks and models proposed for this task. The MultiLexNorm benchmark proposed to unify these efforts, but it consists almost solely of languages from the Indo-European language family in the Latin script. Hence, we propose an extension to MultiLexNorm, which covers 5 Asian languages from different language families in 4 different scripts. We show that the previous state-of-the-art model performs worse on the new languages and propose a new architecture based on Large Language Models (LLMs), which shows more robust performance. Finally, we analyze remaining errors, revealing future directions for this task.</li>
</ul>

<h3>Title: SCHIGAND: A Synthetic Facial Generation Mode Pipeline</h3>
<ul>
<li><strong>Authors: </strong>Ananya Kadali, Sunnie Jehan-Morrison, Orasiki Wellington, Barney Evans, Precious Durojaiye, Richard Guest</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16627">https://arxiv.org/abs/2601.16627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16627">https://arxiv.org/pdf/2601.16627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16627]] SCHIGAND: A Synthetic Facial Generation Mode Pipeline(https://arxiv.org/abs/2601.16627)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The growing demand for diverse and high-quality facial datasets for training and testing biometric systems is challenged by privacy regulations, data scarcity, and ethical concerns. Synthetic facial images offer a potential solution, yet existing generative models often struggle to balance realism, diversity, and identity preservation. This paper presents SCHIGAND, a novel synthetic face generation pipeline integrating StyleCLIP, HyperStyle, InterfaceGAN, and Diffusion models to produce highly realistic and controllable facial datasets. SCHIGAND enhances identity preservation while generating realistic intra-class variations and maintaining inter-class distinctiveness, making it suitable for biometric testing. The generated datasets were evaluated using ArcFace, a leading facial verification model, to assess their effectiveness in comparison to real-world facial datasets. Experimental results demonstrate that SCHIGAND achieves a balance between image quality and diversity, addressing key limitations of prior generative models. This research highlights the potential of SCHIGAND to supplement and, in some cases, replace real data for facial biometric applications, paving the way for privacy-compliant and scalable solutions in synthetic dataset generation.</li>
</ul>

<h3>Title: Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss</h3>
<ul>
<li><strong>Authors: </strong>Minsu Gong, Nuri Ryu, Jungseul Ok, Sunghyun Cho</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16645">https://arxiv.org/abs/2601.16645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16645">https://arxiv.org/pdf/2601.16645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16645]] Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss(https://arxiv.org/abs/2601.16645)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at this https URL.</li>
</ul>

<h3>Title: EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Xinze Li, Ziyue Zhu, Siyuan Liu, Yubo Ma, Yuhang Zang, Yixin Cao, Aixin Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16690">https://arxiv.org/abs/2601.16690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16690">https://arxiv.org/pdf/2601.16690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16690]] EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents(https://arxiv.org/abs/2601.16690)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We introduce EMemBench, a programmatic benchmark for evaluating long-term memory of agents through interactive games. Rather than using a fixed set of questions, EMemBench generates questions from each agent's own trajectory, covering both text and visual game environments. Each template computes verifiable ground truth from underlying game signals, with controlled answerability and balanced coverage over memory skills: single/multi-hop recall, induction, temporal, spatial, logical, and adversarial. We evaluate memory agents with strong LMs/VLMs as backbones, using in-context prompting as baselines. Across 15 text games and multiple visual seeds, results are far from saturated: induction and spatial reasoning are persistent bottlenecks, especially in visual setting. Persistent memory yields clear gains for open backbones on text games, but improvements are less consistent for VLM agents, suggesting that visually grounded episodic memory remains an open challenge. A human study further confirms the difficulty of EMemBench.</li>
</ul>

<h3>Title: Flow Matching for Probabilistic Monocular 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Cuong Le, Pavló Melnyk, Bastian Wandt, Mårten Wadenbäck</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16763">https://arxiv.org/abs/2601.16763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16763">https://arxiv.org/pdf/2601.16763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16763]] Flow Matching for Probabilistic Monocular 3D Human Pose Estimation(https://arxiv.org/abs/2601.16763)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recovering 3D human poses from a monocular camera view is a highly ill-posed problem due to the depth ambiguity. Earlier studies on 3D human pose lifting from 2D often contain incorrect-yet-overconfident 3D estimations. To mitigate the problem, emerging probabilistic approaches treat the 3D estimations as a distribution, taking into account the uncertainty measurement of the poses. Falling in a similar category, we proposed FMPose, a probabilistic 3D human pose estimation method based on the flow matching generative approach. Conditioned on the 2D cues, the flow matching scheme learns the optimal transport from a simple source distribution to the plausible 3D human pose distribution via continuous normalizing flows. The 2D lifting condition is modeled via graph convolutional networks, leveraging the learnable connections between human body joints as the graph structure for feature aggregation. Compared to diffusion-based methods, the FMPose with optimal transport produces faster and more accurate 3D pose generations. Experimental results show major improvements of our FMPose over current state-of-the-art methods on three common benchmarks for 3D human pose estimation, namely Human3.6M, MPI-INF-3DHP and 3DPW.</li>
</ul>

<h3>Title: AutoRegressive Generation with B-rep Holistic Token Sequence Representation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Li, Yunpeng Bai, Yongkang Dai, Hao Guo, Hongping Gan, Yilei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16771">https://arxiv.org/abs/2601.16771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16771">https://arxiv.org/pdf/2601.16771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16771]] AutoRegressive Generation with B-rep Holistic Token Sequence Representation(https://arxiv.org/abs/2601.16771)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Previous representation and generation approaches for the B-rep relied on graph-based representations that disentangle geometric and topological features through decoupled computational pipelines, thereby precluding the application of sequence-based generative frameworks, such as transformer architectures that have demonstrated remarkable performance. In this paper, we propose BrepARG, the first attempt to encode B-rep's geometry and topology into a holistic token sequence representation, enabling sequence-based B-rep generation with an autoregressive architecture. Specifically, BrepARG encodes B-rep into 3 types of tokens: geometry and position tokens representing geometric features, and face index tokens representing topology. Then the holistic token sequence is constructed hierarchically, starting with constructing the geometry blocks (i.e., faces and edges) using the above tokens, followed by geometry block sequencing. Finally, we assemble the holistic sequence representation for the entire B-rep. We also construct a transformer-based autoregressive model that learns the distribution over holistic token sequences via next-token prediction, using a multi-layer decoder-only architecture with causal masking. Experiments demonstrate that BrepARG achieves state-of-the-art (SOTA) performance. BrepARG validates the feasibility of representing B-rep as holistic token sequences, opening new directions for B-rep generation.</li>
</ul>

<h3>Title: Persuasion Tokens for Editing Factual Knowledge in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Paul Youssef, Jörg Schlötterer, Christin Seifert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16781">https://arxiv.org/abs/2601.16781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16781">https://arxiv.org/pdf/2601.16781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16781]] Persuasion Tokens for Editing Factual Knowledge in LLMs(https://arxiv.org/abs/2601.16781)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context knowledge editing (IKE) is a promising technique for updating Large Language Models (LLMs) with new information. However, IKE relies on lengthy, fact-specific demonstrations which are costly to create and consume significant context window space. In this paper, we introduce persuasion tokens (P-Tokens) -- special tokens trained to replicate the effect of IKE demonstrations, enabling efficient knowledge editing without requiring fact-specific demonstrations. We evaluate P-Tokens across two editing datasets and three LLMs, demonstrating performance comparable to, and often exceeding, IKE. We further find that editing performance is robust to distractors with small negative effects to neighboring facts, and that increasing the number of P-Tokens improves performance. Our work addresses key limitations of IKE and provides a more practical and scalable alternative for editing LLMs.</li>
</ul>

<h3>Title: Calibrated Probabilistic Interpolation for GEDI Biomass</h3>
<ul>
<li><strong>Authors: </strong>Robin Young, Srinivasan Keshav</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16834">https://arxiv.org/abs/2601.16834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16834">https://arxiv.org/pdf/2601.16834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16834]] Calibrated Probabilistic Interpolation for GEDI Biomass(https://arxiv.org/abs/2601.16834)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of heterogeneous landscapes. We demonstrate these approaches generally fail to produce calibrated prediction intervals. We identify that this stems from conflating ensemble variance with aleatoric uncertainty and ignoring local spatial context. To resolve this, we introduce Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. Unlike static ensembles, ANPs learn a flexible spatial covariance function, allowing uncertainty estimates to expand in complex landscapes and contract in homogeneous areas. We validate this approach across five distinct biomes ranging from Tropical Amazonian forests to Boreal and Alpine ecosystems, demonstrating that ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration. We demonstrate the operational utility of the method through few-shot adaptation, where the model recovers most of the performance gap in cross-region transfer using minimal local data. This work provides a scalable, theoretically rigorous alternative to ensemble variance for continental scale earth observation.</li>
</ul>

<h3>Title: No Validation, No Problem: Predicting Model Performance from a Single Gradient</h3>
<ul>
<li><strong>Authors: </strong>Fangzheng Wu, Brian Summa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16874">https://arxiv.org/abs/2601.16874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16874">https://arxiv.org/pdf/2601.16874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16874]] No Validation, No Problem: Predicting Model Performance from a Single Gradient(https://arxiv.org/abs/2601.16874)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a validation-free checkpointing signal from a single forward-backward pass: the Frobenius norm of the classifier-head gradient on one detached-feature batch, ||g||_F = ||dL/dW||_F. Across ImageNet-1k CNNs and Transformers, this proxy is strongly negative with Top-1 and positive with loss. Selecting the checkpoint with the minimum head gradient in a short tail window closes most of the gap to the oracle (4.24% +/- 2.00% with a universal setup, about 1.12% with light per-family tuning). For practical deployment, a head-scale normalization is more stable within classic CNN families (e.g., ResNets), while a feature-scale normalization works well for Transformers and modern CNNs. The same one-batch probe also predicts COCO detection/segmentation mAP. In diffusion (UNet/DDPM on CIFAR-10), it tracks progress and enables near-oracle tail-window selection; it is positively correlated with same-distribution probe MSE and negatively with FID (lower is better), so it can be used as a lightweight, label-free monitor. Validation labels are never used beyond reporting. The probe adds much less than 0.1% of an epoch and works as a drop-in for validation-free checkpoint selection and early stopping.</li>
</ul>

<h3>Title: GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss</h3>
<ul>
<li><strong>Authors: </strong>Yangfan Xu, Lilian Zhang, Xiaofeng He, Pengdong Wu, Wenqi Wu, Jun Mao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16885">https://arxiv.org/abs/2601.16885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16885">https://arxiv.org/pdf/2601.16885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16885]] GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss(https://arxiv.org/abs/2601.16885)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at this https URL.</li>
</ul>

<h3>Title: LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems</h3>
<ul>
<li><strong>Authors: </strong>João A. Leite, Olesya Razuvayevskaya, Kalina Bontcheva, Carolina Scarton</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16890">https://arxiv.org/abs/2601.16890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16890">https://arxiv.org/pdf/2601.16890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16890]] LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems(https://arxiv.org/abs/2601.16890)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a novel class of persuasive adversarial attacks on AFCs by employing a generative LLM to rephrase claims using persuasion techniques. Considering 15 techniques grouped into 6 categories, we study the effects of persuasion on both claim verification and evidence retrieval using a decoupled evaluation strategy. Experiments on the FEVER and FEVEROUS benchmarks show that persuasion attacks can substantially degrade both verification performance and evidence retrieval. Our analysis identifies persuasion techniques as a potent class of adversarial attacks, highlighting the need for more robust AFC systems.</li>
</ul>

<h3>Title: Embedding -based Crop Type Classification in the Groundnut Basin of Senegal</h3>
<ul>
<li><strong>Authors: </strong>Madeline C. Lisaius, Srinivasan Keshav, Andrew Blake, Clement Atzberger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16900">https://arxiv.org/abs/2601.16900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16900">https://arxiv.org/pdf/2601.16900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16900]] Embedding -based Crop Type Classification in the Groundnut Basin of Senegal(https://arxiv.org/abs/2601.16900)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) transferability and 4) accessibility and evaluate geospatial foundation model (FM) embeddings -based approaches using TESSERA and AlphaEarth against current baseline methods for a region in the groundnut basin of Senegal. We find that the TESSERA -based approach to land cover and crop type mapping fulfills the selection criteria best, and in one temporal transfer example shows 28% higher accuracy compared to the next best method. These results indicate that TESSERA embeddings are an effective approach for crop type classification and mapping tasks in Senegal.</li>
</ul>

<h3>Title: LoL: Longer than Longer, Scaling Video Generation to Hour</h3>
<ul>
<li><strong>Authors: </strong>Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16914">https://arxiv.org/abs/2601.16914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16914">https://arxiv.org/pdf/2601.16914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16914]] LoL: Longer than Longer, Scaling Video Generation to Hour(https://arxiv.org/abs/2601.16914)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink frame, resulting in abrupt scene resets and cyclic motion patterns. Our analysis reveals that sink-collapse originates from an inherent conflict between the periodic structure of Rotary Position Embedding (RoPE) and the multi-head attention mechanisms prevalent in current generative models. To address it, we propose a lightweight, training-free approach that effectively suppresses this behavior by introducing multi-head RoPE jitter that breaks inter-head attention homogenization and mitigates long-horizon collapse. Extensive experiments show that our method successfully alleviates sink-collapse while preserving generation quality. To the best of our knowledge, this work achieves the first demonstration of real-time, streaming, and infinite-length video generation with little quality decay. As an illustration of this robustness, we generate continuous videos up to 12 hours in length, which, to our knowledge, is among the longest publicly demonstrated results in streaming video generation.</li>
</ul>

<h3>Title: Strategies for Span Labeling with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Danil Semin, Ondřej Dušek, Zdeněk Kasner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16946">https://arxiv.org/abs/2601.16946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16946">https://arxiv.org/pdf/2601.16946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16946]] Strategies for Span Labeling with Large Language Models(https://arxiv.org/abs/2601.16946)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used for text analysis tasks, such as named entity recognition or error detection. Unlike encoder-based models, however, generative architectures lack an explicit mechanism to refer to specific parts of their input. This leads to a variety of ad-hoc prompting strategies for span labeling, often with inconsistent results. In this paper, we categorize these strategies into three families: tagging the input text, indexing numerical positions of spans, and matching span content. To address the limitations of content matching, we introduce LogitMatch, a new constrained decoding method that forces the model's output to align with valid input spans. We evaluate all methods across four diverse tasks. We find that while tagging remains a robust baseline, LogitMatch improves upon competitive matching-based methods by eliminating span matching issues and outperforms other strategies in some setups.</li>
</ul>

<h3>Title: 3D Molecule Generation from Rigid Motifs via SE(3) Flows</h3>
<ul>
<li><strong>Authors: </strong>Roman Poletukhin, Marcel Kollovieh, Eike Eberhard, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16955">https://arxiv.org/abs/2601.16955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16955">https://arxiv.org/pdf/2601.16955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16955]] 3D Molecule Generation from Rigid Motifs via SE(3) Flows(https://arxiv.org/abs/2601.16955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.</li>
</ul>

<h3>Title: Auto-Regressive Masked Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Karami, Ali Ghodsi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16971">https://arxiv.org/abs/2601.16971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16971">https://arxiv.org/pdf/2601.16971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16971]] Auto-Regressive Masked Diffusion Models(https://arxiv.org/abs/2601.16971)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.</li>
</ul>

<h3>Title: Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Estela Sánchez-Carballo, Francisco M. Melgarejo-Meseguer, José Luis Rojo-Álvarez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16976">https://arxiv.org/abs/2601.16976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16976">https://arxiv.org/pdf/2601.16976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16976]] Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection(https://arxiv.org/abs/2601.16976)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques or generative models that struggle to simultaneously achieve high sample fidelity, diversity, and computational efficiency. To address these limitations, we propose the use of a Latent Diffusion Model (LDM) for attack data augmentation in IoT intrusion detection and provide a comprehensive comparison against state-of-the-art baselines. Experiments were conducted on three representative IoT attack types, specifically Distributed Denial-of-Service (DDoS), Mirai, and Man-in-the-Middle, evaluating both downstream IDS performance and intrinsic generative quality using distributional, dependency-based, and diversity metrics. Results show that balancing the training data with LDM-generated samples substantially improves IDS performance, achieving F1-scores of up to 0.99 for DDoS and Mirai attacks and consistently outperforming competing methods. Additionally, quantitative and qualitative analyses demonstrate that LDMs effectively preserve feature dependencies while generating diverse samples and reduce sampling time by approximately 25\% compared to diffusion models operating directly in data space. These findings highlight latent diffusion as an effective and scalable solution for synthetic IoT attack data generation, substantially mitigating the impact of class imbalance in ML-based IDSs for IoT scenarios.</li>
</ul>

<h3>Title: SyncLight: Controllable and Consistent Multi-View Relighting</h3>
<ul>
<li><strong>Authors: </strong>David Serrano-Lozano, Anand Bhattad, Luis Herranz, Jean-François Lalonde, Javier Vazquez-Corral</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16981">https://arxiv.org/abs/2601.16981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16981">https://arxiv.org/pdf/2601.16981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16981]] SyncLight: Controllable and Consistent Multi-View Relighting(https://arxiv.org/abs/2601.16981)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.</li>
</ul>

<h3>Title: AnyView: Synthesizing Any Novel View in Dynamic Scenes</h3>
<ul>
<li><strong>Authors: </strong>Basile Van Hoorick, Dian Chen, Shun Iwase, Pavel Tokmakov, Muhammad Zubair Irshad, Igor Vasiljevic, Swati Gupta, Fangzhou Cheng, Sergey Zakharov, Vitor Campagnolo Guizilini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.16982">https://arxiv.org/abs/2601.16982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.16982">https://arxiv.org/pdf/2601.16982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.16982]] AnyView: Synthesizing Any Novel View in Dynamic Scenes(https://arxiv.org/abs/2601.16982)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \textbf{AnyViewBench}, a challenging new benchmark tailored towards \emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \emph{any} viewpoint. Results, data, code, and models can be viewed at: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
