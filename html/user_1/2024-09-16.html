<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-16</h1>
<h3>Title: Scores as Actions: a framework of fine-tuning diffusion models by continuous-time reinforcement learning</h3>
<ul>
<li><strong>Authors: </strong>Hanyang Zhao, Haoxian Chen, Ji Zhang, David D. Yao, Wenpin Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08400">https://arxiv.org/abs/2409.08400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08400">https://arxiv.org/pdf/2409.08400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08400]] Scores as Actions: a framework of fine-tuning diffusion models by continuous-time reinforcement learning(https://arxiv.org/abs/2409.08400)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from human feedback (RLHF) has been shown a promising direction for aligning generative models with human intent and has also been explored in recent works for alignment of diffusion generative models. In this work, we provide a rigorous treatment by formulating the task of fine-tuning diffusion models, with reward functions learned from human feedback, as an exploratory continuous-time stochastic control problem. Our key idea lies in treating the score-matching functions as controls/actions, and upon this, we develop a unified framework from a continuous-time perspective, to employ reinforcement learning (RL) algorithms in terms of improving the generation quality of diffusion models. We also develop the corresponding continuous-time RL theory for policy optimization and regularization under assumptions of stochastic different equations driven environment. Experiments on the text-to-image (T2I) generation will be reported in the accompanied paper.</li>
</ul>

<h3>Title: Integrating Neural Operators with Diffusion Models Improves Spectral Representation in Turbulence Modeling</h3>
<ul>
<li><strong>Authors: </strong>Vivek Oommen, Aniruddha Bora, Zhen Zhang, George Em Karniadakis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08477">https://arxiv.org/abs/2409.08477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08477">https://arxiv.org/pdf/2409.08477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08477]] Integrating Neural Operators with Diffusion Models Improves Spectral Representation in Turbulence Modeling(https://arxiv.org/abs/2409.08477)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We integrate neural operators with diffusion models to address the spectral limitations of neural operators in surrogate modeling of turbulent flows. While neural operators offer computational efficiency, they exhibit deficiencies in capturing high-frequency flow dynamics, resulting in overly smooth approximations. To overcome this, we condition diffusion models on neural operators to enhance the resolution of turbulent structures. Our approach is validated for different neural operators on diverse datasets, including a high Reynolds number jet flow simulation and experimental Schlieren velocimetry. The proposed method significantly improves the alignment of predicted energy spectra with true distributions compared to neural operators alone. Additionally, proper orthogonal decomposition analysis demonstrates enhanced spectral fidelity in space-time. This work establishes a new paradigm for combining generative models with neural operators to advance surrogate modeling of turbulent systems, and it can be used in other scientific applications that involve microstructure and high-frequency content. See our project page: this http URL</li>
</ul>

<h3>Title: Risks When Sharing LoRA Fine-Tuned Diffusion Model Weights</h3>
<ul>
<li><strong>Authors: </strong>Dixi Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08482">https://arxiv.org/abs/2409.08482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08482">https://arxiv.org/pdf/2409.08482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08482]] Risks When Sharing LoRA Fine-Tuned Diffusion Model Weights(https://arxiv.org/abs/2409.08482)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>With the emerging trend in generative models and convenient public access to diffusion models pre-trained on large datasets, users can fine-tune these models to generate images of personal faces or items in new contexts described by natural language. Parameter efficient fine-tuning (PEFT) such as Low Rank Adaptation (LoRA) has become the most common way to save memory and computation usage on the user end during fine-tuning. However, a natural question is whether the private images used for fine-tuning will be leaked to adversaries when sharing model weights. In this paper, we study the issue of privacy leakage of a fine-tuned diffusion model in a practical setting, where adversaries only have access to model weights, rather than prompts or images used for fine-tuning. We design and build a variational network autoencoder that takes model weights as input and outputs the reconstruction of private images. To improve the efficiency of training such an autoencoder, we propose a training paradigm with the help of timestep embedding. The results give a surprising answer to this research question: an adversary can generate images containing the same identities as the private images. Furthermore, we demonstrate that no existing defense method, including differential privacy-based methods, can preserve the privacy of private data used for fine-tuning a diffusion model without compromising the utility of a fine-tuned model.</li>
</ul>

<h3>Title: Sub-graph Based Diffusion Model for Link Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hang Li, Wei Jin, Geri Skenderi, Harry Shomer, Wenzhuo Tang, Wenqi Fan, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08487">https://arxiv.org/abs/2409.08487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08487">https://arxiv.org/pdf/2409.08487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08487]] Sub-graph Based Diffusion Model for Link Prediction(https://arxiv.org/abs/2409.08487)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Denoising Diffusion Probabilistic Models (DDPMs) represent a contemporary class of generative models with exceptional qualities in both synthesis and maximizing the data likelihood. These models work by traversing a forward Markov Chain where data is perturbed, followed by a reverse process where a neural network learns to undo the perturbations and recover the original data. There have been increasing efforts exploring the applications of DDPMs in the graph domain. However, most of them have focused on the generative perspective. In this paper, we aim to build a novel generative model for link prediction. In particular, we treat link prediction between a pair of nodes as a conditional likelihood estimation of its enclosing sub-graph. With a dedicated design to decompose the likelihood estimation process via the Bayesian formula, we are able to separate the estimation of sub-graph structure and its node features. Such designs allow our model to simultaneously enjoy the advantages of inductive learning and the strong generalization capability. Remarkably, comprehensive experiments across various datasets validate that our proposed method presents numerous advantages: (1) transferability across datasets without retraining, (2) promising generalization on limited training data, and (3) robustness against graph adversarial attacks.</li>
</ul>

<h3>Title: Enhancing Privacy in ControlNet and Stable Diffusion via Split Learning</h3>
<ul>
<li><strong>Authors: </strong>Dixi Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08503">https://arxiv.org/abs/2409.08503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08503">https://arxiv.org/pdf/2409.08503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08503]] Enhancing Privacy in ControlNet and Stable Diffusion via Split Learning(https://arxiv.org/abs/2409.08503)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>With the emerging trend of large generative models, ControlNet is introduced to enable users to fine-tune pre-trained models with their own data for various use cases. A natural question arises: how can we train ControlNet models while ensuring users' data privacy across distributed devices? Exploring different distributed training schemes, we find conventional federated learning and split learning unsuitable. Instead, we propose a new distributed learning structure that eliminates the need for the server to send gradients back. Through a comprehensive evaluation of existing threats, we discover that in the context of training ControlNet with split learning, most existing attacks are ineffective, except for two mentioned in previous literature. To counter these threats, we leverage the properties of diffusion models and design a new timestep sampling policy during forward processes. We further propose a privacy-preserving activation function and a method to prevent private text prompts from leaving clients, tailored for image generation with diffusion models. Our experimental results demonstrate that our algorithms and systems greatly enhance the efficiency of distributed training for ControlNet while ensuring users' data privacy without compromising image generation quality.</li>
</ul>

<h3>Title: Exploiting Supervised Poison Vulnerability to Strengthen Self-Supervised Defense</h3>
<ul>
<li><strong>Authors: </strong>Jeremy Styborski, Mingzhi Lyu, Yi Huang, Adams Kong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08509">https://arxiv.org/abs/2409.08509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08509">https://arxiv.org/pdf/2409.08509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08509]] Exploiting Supervised Poison Vulnerability to Strengthen Self-Supervised Defense(https://arxiv.org/abs/2409.08509)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Availability poisons exploit supervised learning (SL) algorithms by introducing class-related shortcut features in images such that models trained on poisoned data are useless for real-world datasets. Self-supervised learning (SSL), which utilizes augmentations to learn instance discrimination, is regarded as a strong defense against poisoned data. However, by extending the study of SSL across multiple poisons on the CIFAR-10 and ImageNet-100 datasets, we demonstrate that it often performs poorly, far below that of training on clean data. Leveraging the vulnerability of SL to poison attacks, we introduce adversarial training (AT) on SL to obfuscate poison features and guide robust feature learning for SSL. Our proposed defense, designated VESPR (Vulnerability Exploitation of Supervised Poisoning for Robust SSL), surpasses the performance of six previous defenses across seven popular availability poisons. VESPR displays superior performance over all previous defenses, boosting the minimum and average ImageNet-100 test accuracies of poisoned models by 16% and 9%, respectively. Through analysis and ablation studies, we elucidate the mechanisms by which VESPR learns robust class features.</li>
</ul>

<h3>Title: DiffFAS: Face Anti-Spoofing via Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Xinxu Ge, Xin Liu, Zitong Yu, Jingang Shi, Chun Qi, Jie Li, Heikki Kälviäinen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08572">https://arxiv.org/abs/2409.08572</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08572">https://arxiv.org/pdf/2409.08572</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08572]] DiffFAS: Face Anti-Spoofing via Generative Diffusion Models(https://arxiv.org/abs/2409.08572)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Face anti-spoofing (FAS) plays a vital role in preventing face recognition (FR) systems from presentation attacks. Nowadays, FAS systems face the challenge of domain shift, impacting the generalization performance of existing FAS methods. In this paper, we rethink about the inherence of domain shift and deconstruct it into two factors: image style and image quality. Quality influences the purity of the presentation of spoof information, while style affects the manner in which spoof information is presented. Based on our analysis, we propose DiffFAS framework, which quantifies quality as prior information input into the network to counter image quality shift, and performs diffusion-based high-fidelity cross-domain and cross-attack types generation to counter image style shift. DiffFAS transforms easily collectible live faces into high-fidelity attack faces with precise labels while maintaining consistency between live and spoof face identities, which can also alleviate the scarcity of labeled data with novel type attacks faced by nowadays FAS system. We demonstrate the effectiveness of our framework on challenging cross-domain and cross-attack FAS datasets, achieving the state-of-the-art performance. Available at this https URL.</li>
</ul>

<h3>Title: Autoregressive Sequence Modeling for 3D Medical Image Representation</h3>
<ul>
<li><strong>Authors: </strong>Siwen Wang, Churan Wang, Fei Gao, Lixian Su, Fandong Zhang, Yizhou Wang, Yizhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08691">https://arxiv.org/abs/2409.08691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08691">https://arxiv.org/pdf/2409.08691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08691]] Autoregressive Sequence Modeling for 3D Medical Image Representation(https://arxiv.org/abs/2409.08691)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Three-dimensional (3D) medical images, such as Computed Tomography (CT) and Magnetic Resonance Imaging (MRI), are essential for clinical applications. However, the need for diverse and comprehensive representations is particularly pronounced when considering the variability across different organs, diagnostic tasks, and imaging modalities. How to effectively interpret the intricate contextual information and extract meaningful insights from these images remains an open challenge to the community. While current self-supervised learning methods have shown potential, they often consider an image as a whole thereby overlooking the extensive, complex relationships among local regions from one or multiple images. In this work, we introduce a pioneering method for learning 3D medical image representations through an autoregressive pre-training framework. Our approach sequences various 3D medical images based on spatial, contrast, and semantic correlations, treating them as interconnected visual tokens within a token sequence. By employing an autoregressive sequence modeling task, we predict the next visual token in the sequence, which allows our model to deeply understand and integrate the contextual information inherent in 3D medical images. Additionally, we implement a random startup strategy to avoid overestimating token relationships and to enhance the robustness of learning. The effectiveness of our approach is demonstrated by the superior performance over others on nine downstream tasks in public datasets.</li>
</ul>

<h3>Title: Distilling Monolingual and Crosslingual Word-in-Context Representations</h3>
<ul>
<li><strong>Authors: </strong>Yuki Arase, Tomoyuki Kajiwara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08719">https://arxiv.org/abs/2409.08719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08719">https://arxiv.org/pdf/2409.08719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08719]] Distilling Monolingual and Crosslingual Word-in-Context Representations(https://arxiv.org/abs/2409.08719)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In this study, we propose a method that distils representations of word meaning in context from a pre-trained masked language model in both monolingual and crosslingual settings. Word representations are the basis for context-aware lexical semantics and unsupervised semantic textual similarity (STS) estimation. Different from existing approaches, our method does not require human-annotated corpora nor updates of the parameters of the pre-trained model. The latter feature is appealing for practical scenarios where the off-the-shelf pre-trained model is a common asset among different applications. Specifically, our method learns to combine the outputs of different hidden layers of the pre-trained model using self-attention. Our auto-encoder based training only requires an automatically generated corpus. To evaluate the performance of the proposed approach, we performed extensive experiments using various benchmark tasks. The results on the monolingual tasks confirmed that our representations exhibited a competitive performance compared to that of the previous study for the context-aware lexical semantic tasks and outperformed it for STS estimation. The results of the crosslingual tasks revealed that the proposed method largely improved crosslingual word representations of multilingual pre-trained models.</li>
</ul>

<h3>Title: Uncertainty and Generalizability in Foundation Models for Earth Observation</h3>
<ul>
<li><strong>Authors: </strong>Raul Ramos-Pollan, Freddie Kalaitzis, Karthick Panner Selvam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08744">https://arxiv.org/abs/2409.08744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08744">https://arxiv.org/pdf/2409.08744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08744]] Uncertainty and Generalizability in Foundation Models for Earth Observation(https://arxiv.org/abs/2409.08744)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We take the perspective in which we want to design a downstream task (such as estimating vegetation coverage) on a certain area of interest (AOI) with a limited labeling budget. By leveraging an existing Foundation Model (FM) we must decide whether we train a downstream model on a different but label-rich AOI hoping it generalizes to our AOI, or we split labels in our AOI for training and validating. In either case, we face choices concerning what FM to use, how to sample our AOI for labeling, etc. which affect both the performance and uncertainty of the results. In this work, we perform a large ablative study using eight existing FMs on either Sentinel 1 or Sentinel 2 as input data, and the classes from the ESA World Cover product as downstream tasks across eleven AOIs. We do repeated sampling and training, resulting in an ablation of some 500K simple linear regression models. Our results show both the limits of spatial generalizability across AOIs and the power of FMs where we are able to get over 0.9 correlation coefficient between predictions and targets on different chip level predictive tasks. And still, performance and uncertainty vary greatly across AOIs, tasks and FMs. We believe this is a key issue in practice, because there are many design decisions behind each FM and downstream task (input modalities, sampling, architectures, pretraining, etc.) and usually a downstream task designer is aware of and can decide upon a few of them. Through this work, we advocate for the usage of the methodology herein described (large ablations on reference global labels and simple probes), both when publishing new FMs, and to make informed decisions when designing downstream tasks to use them.</li>
</ul>

<h3>Title: Electrocardiogram Report Generation and Question Answering via Retrieval-Augmented Self-Supervised Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jialu Tang, Tong Xia, Yuan Lu, Cecilia Mascolo, Aaqib Saeed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08788">https://arxiv.org/abs/2409.08788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08788">https://arxiv.org/pdf/2409.08788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08788]] Electrocardiogram Report Generation and Question Answering via Retrieval-Augmented Self-Supervised Modeling(https://arxiv.org/abs/2409.08788)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Interpreting electrocardiograms (ECGs) and generating comprehensive reports remain challenging tasks in cardiology, often requiring specialized expertise and significant time investment. To address these critical issues, we propose ECG-ReGen, a retrieval-based approach for ECG-to-text report generation and question answering. Our method leverages a self-supervised learning for the ECG encoder, enabling efficient similarity searches and report retrieval. By combining pre-training with dynamic retrieval and Large Language Model (LLM)-based refinement, ECG-ReGen effectively analyzes ECG data and answers related queries, with the potential of improving patient care. Experiments conducted on the PTB-XL and MIMIC-IV-ECG datasets demonstrate superior performance in both in-domain and cross-domain scenarios for report generation. Furthermore, our approach exhibits competitive performance on ECG-QA dataset compared to fully supervised methods when utilizing off-the-shelf LLMs for zero-shot question answering. This approach, effectively combining self-supervised encoder and LLMs, offers a scalable and efficient solution for accurate ECG interpretation, holding significant potential to enhance clinical decision-making.</li>
</ul>

<h3>Title: Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Cui, Yifan Yang, Jiajun Deng, Jiawen Kang, Shujie Hu, Tianzi Wang, Zhaoqing Li, Shiliang Zhang, Xie Chen, Xunying Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08797">https://arxiv.org/abs/2409.08797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08797">https://arxiv.org/pdf/2409.08797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08797]] Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR(https://arxiv.org/abs/2409.08797)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) based discrete speech representations are highly compact and domain adaptable. In this paper, SSL discrete speech features extracted from WavLM models are used as additional cross-utterance acoustic context features in Zipformer-Transducer ASR systems. The efficacy of replacing Fbank features with discrete token features for modelling either cross-utterance contexts (from preceding and future segments), or current utterance's internal contexts alone, or both at the same time, are demonstrated thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer system using discrete tokens based cross-utterance context features outperforms the baseline using utterance internal context only with statistically significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78% to 3.54% relative) on the dev and test data. The lowest published WER of 11.15% and 11.14% were obtained on the dev and test sets. Our work is open-source and publicly available at this https URL\_ASR.</li>
</ul>

<h3>Title: Exploring SSL Discrete Tokens for Multilingual ASR</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Cui, Daxin Tan, Yifan Yang, Dingdong Wang, Huimeng Wang, Xiao Chen, Xie Chen, Xunying Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08805">https://arxiv.org/abs/2409.08805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08805">https://arxiv.org/pdf/2409.08805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08805]] Exploring SSL Discrete Tokens for Multilingual ASR(https://arxiv.org/abs/2409.08805)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>With the advancement of Self-supervised Learning (SSL) in speech-related tasks, there has been growing interest in utilizing discrete tokens generated by SSL for automatic speech recognition (ASR), as they offer faster processing techniques. However, previous studies primarily focused on multilingual ASR with Fbank features or English ASR with discrete tokens, leaving a gap in adapting discrete tokens for multilingual ASR scenarios. This study presents a comprehensive comparison of discrete tokens generated by various leading SSL models across multiple language domains. We aim to explore the performance and efficiency of speech discrete tokens across multiple language domains for both monolingual and multilingual ASR scenarios. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on Fbank features in ASR tasks across seven language domains with an average word error rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70% relative) on dev and test sets respectively, with particularly WER reduction of 6.82% absolute (41.48% relative) on the Polish test set.</li>
</ul>

<h3>Title: DeCLIP: Decoding CLIP representations for deepfake localization</h3>
<ul>
<li><strong>Authors: </strong>Stefan Smeu, Elisabeta Oneata, Dan Oneata</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08849">https://arxiv.org/abs/2409.08849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08849">https://arxiv.org/pdf/2409.08849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08849]] DeCLIP: Decoding CLIP representations for deepfake localization(https://arxiv.org/abs/2409.08849)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Generative models can create entirely new images, but they can also partially modify real images in ways that are undetectable to the human eye. In this paper, we address the challenge of automatically detecting such local manipulations. One of the most pressing problems in deepfake detection remains the ability of models to generalize to different classes of generators. In the case of fully manipulated images, representations extracted from large self-supervised models (such as CLIP) provide a promising direction towards more robust detectors. Here, we introduce DeCLIP, a first attempt to leverage such large pretrained features for detecting local manipulations. We show that, when combined with a reasonably large convolutional decoder, pretrained self-supervised representations are able to perform localization and improve generalization capabilities over existing methods. Unlike previous work, our approach is able to perform localization on the challenging case of latent diffusion models, where the entire image is affected by the fingerprint of the generator. Moreover, we observe that this type of data, which combines local semantic information with a global fingerprint, provides more stable generalization than other categories of generative methods.</li>
</ul>

<h3>Title: InstantDrag: Improving Interactivity in Drag-based Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Joonghyuk Shin, Daehyeon Choi, Jaesik Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08857">https://arxiv.org/abs/2409.08857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08857">https://arxiv.org/pdf/2409.08857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08857]] InstantDrag: Improving Interactivity in Drag-based Image Editing(https://arxiv.org/abs/2409.08857)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Drag-based image editing has recently gained popularity for its interactivity and precision. However, despite the ability of text-to-image models to generate samples within a second, drag editing still lags behind due to the challenge of accurately reflecting user interaction while maintaining image content. Some existing approaches rely on computationally intensive per-image optimization or intricate guidance-based methods, requiring additional inputs such as masks for movable regions and text prompts, thereby compromising the interactivity of the editing process. We introduce InstantDrag, an optimization-free pipeline that enhances interactivity and speed, requiring only an image and a drag instruction as input. InstantDrag consists of two carefully designed networks: a drag-conditioned optical flow generator (FlowGen) and an optical flow-conditioned diffusion model (FlowDiffusion). InstantDrag learns motion dynamics for drag-based image editing in real-world video datasets by decomposing the task into motion generation and motion-conditioned image generation. We demonstrate InstantDrag's capability to perform fast, photo-realistic edits without masks or text prompts through experiments on facial video datasets and general scenes. These results highlight the efficiency of our approach in handling drag-based image editing, making it a promising solution for interactive, real-time applications.</li>
</ul>

<h3>Title: Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control</h3>
<ul>
<li><strong>Authors: </strong>Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, Ricky T. Q. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08861">https://arxiv.org/abs/2409.08861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08861">https://arxiv.org/pdf/2409.08861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08861]] Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control(https://arxiv.org/abs/2409.08861)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there has not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.</li>
</ul>

<h3>Title: Exploring the Impact of Data Quantity on ASR in Extremely Low-resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Yao-Fei Cheng, Li-Wei Chen, Hung-Shin Lee, Hsin-Min Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08872">https://arxiv.org/abs/2409.08872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08872">https://arxiv.org/pdf/2409.08872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08872]] Exploring the Impact of Data Quantity on ASR in Extremely Low-resource Languages(https://arxiv.org/abs/2409.08872)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This study investigates the efficacy of data augmentation techniques for low-resource automatic speech recognition (ASR), focusing on two endangered Austronesian languages, Amis and Seediq. Recognizing the potential of self-supervised learning (SSL) in low-resource settings, we explore the impact of data volume on the continued pre-training of SSL models. We propose a novel data-selection scheme leveraging a multilingual corpus to augment the limited target language data. This scheme utilizes a language classifier to extract utterance embeddings and employs one-class classifiers to identify utterances phonetically and phonologically proximate to the target languages. Utterances are ranked and selected based on their decision scores, ensuring the inclusion of highly relevant data in the SSL-ASR pipeline. Our experimental results demonstrate the effectiveness of this approach, yielding substantial improvements in ASR performance for both Amis and Seediq. These findings underscore the feasibility and promise of data augmentation through cross-lingual transfer learning for low-resource language ASR.</li>
</ul>

<h3>Title: Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Minh-Duc Vu, Zuheng Ming, Fangchen Feng, Bissmella Bahaduri, Anissa Mokraoui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08885">https://arxiv.org/abs/2409.08885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08885">https://arxiv.org/pdf/2409.08885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08885]] Interactive Masked Image Modeling for Multimodal Object Detection in Remote Sensing(https://arxiv.org/abs/2409.08885)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Object detection in remote sensing imagery plays a vital role in various Earth observation applications. However, unlike object detection in natural scene images, this task is particularly challenging due to the abundance of small, often barely visible objects across diverse terrains. To address these challenges, multimodal learning can be used to integrate features from different data modalities, thereby improving detection accuracy. Nonetheless, the performance of multimodal learning is often constrained by the limited size of labeled datasets. In this paper, we propose to use Masked Image Modeling (MIM) as a pre-training technique, leveraging self-supervised learning on unlabeled data to enhance detection performance. However, conventional MIM such as MAE which uses masked tokens without any contextual information, struggles to capture the fine-grained details due to a lack of interactions with other parts of image. To address this, we propose a new interactive MIM method that can establish interactions between different tokens, which is particularly beneficial for object detection in remote sensing. The extensive ablation studies and evluation demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation</h3>
<ul>
<li><strong>Authors: </strong>Guojun Liang, Najmeh Abiri, Atiye Sadat Hashemi, Jens Lundström, Stefan Byttner, Prayag Tiwari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08917">https://arxiv.org/abs/2409.08917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08917">https://arxiv.org/pdf/2409.08917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08917]] Latent Space Score-based Diffusion Model for Probabilistic Multivariate Time Series Imputation(https://arxiv.org/abs/2409.08917)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Accurate imputation is essential for the reliability and success of downstream tasks. Recently, diffusion models have attracted great attention in this field. However, these models neglect the latent distribution in a lower-dimensional space derived from the observed data, which limits the generative capacity of the diffusion model. Additionally, dealing with the original missing data without labels becomes particularly problematic. To address these issues, we propose the Latent Space Score-Based Diffusion Model (LSSDM) for probabilistic multivariate time series imputation. Observed values are projected onto low-dimensional latent space and coarse values of the missing data are reconstructed without knowing their ground truth values by this unsupervised learning approach. Finally, the reconstructed values are fed into a conditional diffusion model to obtain the precise imputed values of the time series. In this way, LSSDM not only possesses the power to identify the latent distribution but also seamlessly integrates the diffusion model to obtain the high-fidelity imputed values and assess the uncertainty of the dataset. Experimental results demonstrate that LSSDM achieves superior imputation performance while also providing a better explanation and uncertainty analysis of the imputation mechanism. The website of the code is \textit{this https URL\_imputation}.</li>
</ul>

<h3>Title: A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yohan Poirier-Ginter, Alban Gauthier, Julien Phillip, Jean-Francois Lalonde, George Drettakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.08947">https://arxiv.org/abs/2409.08947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.08947">https://arxiv.org/pdf/2409.08947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.08947]] A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis(https://arxiv.org/abs/2409.08947)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Relighting radiance fields is severely underconstrained for multi-view data, which is most often captured under a single illumination condition; It is especially hard for full scenes containing multiple objects. We introduce a method to create relightable radiance fields using such single-illumination data by exploiting priors extracted from 2D image diffusion models. We first fine-tune a 2D diffusion model on a multi-illumination dataset conditioned by light direction, allowing us to augment a single-illumination capture into a realistic -- but possibly inconsistent -- multi-illumination dataset from directly defined light directions. We use this augmented data to create a relightable radiance field represented by 3D Gaussian splats. To allow direct control of light direction for low-frequency lighting, we represent appearance with a multi-layer perceptron parameterized on light direction. To enforce multi-view consistency and overcome inaccuracies we optimize a per-image auxiliary feature vector. We show results on synthetic and real multi-view data under single illumination, demonstrating that our method successfully exploits 2D diffusion model priors to allow realistic 3D relighting for complete scenes. Project site this https URL</li>
</ul>

<h3>Title: Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach</h3>
<ul>
<li><strong>Authors: </strong>Siqi Li, Danni Liu, Jan Niehues</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.09009">https://arxiv.org/abs/2409.09009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.09009">https://arxiv.org/pdf/2409.09009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.09009]] Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach(https://arxiv.org/abs/2409.09009)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Direct speech translation (ST) models often struggle with rare words. Incorrect translation of these words can have severe consequences, impacting translation quality and user trust. While rare word translation is inherently challenging for neural models due to sparse learning signals, real-world scenarios often allow access to translations of past recordings on similar topics. To leverage these valuable resources, we propose a retrieval-and-demonstration approach to enhance rare word translation accuracy in direct ST models. First, we adapt existing ST models to incorporate retrieved examples for rare word translation, which allows the model to benefit from prepended examples, similar to in-context learning. We then develop a cross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to locate suitable examples. We demonstrate that standard ST models can be effectively adapted to leverage examples for rare word translation, improving rare word translation accuracy over the baseline by 17.6% with gold examples and 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval approach outperforms other modalities and exhibits higher robustness to unseen speakers. Our code is publicly available (this https URL).</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
