<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-15</h1>
<h3>Title: AI versus AI in Financial Crimes and Detection: GenAI Crime Waves to Co-Evolutionary AI</h3>
<ul>
<li><strong>Authors: </strong>Eren Kurshan, Dhagash Mehta, Bayan Bruss, Tucker Balch</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09066">https://arxiv.org/abs/2410.09066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09066">https://arxiv.org/pdf/2410.09066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09066]] AI versus AI in Financial Crimes and Detection: GenAI Crime Waves to Co-Evolutionary AI(https://arxiv.org/abs/2410.09066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Adoption of AI by criminal entities across traditional and emerging financial crime paradigms has been a disturbing recent trend. Particularly concerning is the proliferation of generative AI, which has empowered criminal activities ranging from sophisticated phishing schemes to the creation of hard-to-detect deep fakes, and to advanced spoofing attacks to biometric authentication systems. The exploitation of AI by criminal purposes continues to escalate, presenting an unprecedented challenge. AI adoption causes an increasingly complex landscape of fraud typologies intertwined with cybersecurity vulnerabilities. Overall, GenAI has a transformative effect on financial crimes and fraud. According to some estimates, GenAI will quadruple the fraud losses by 2027 with a staggering annual growth rate of over 30% [27]. As crime patterns become more intricate, personalized, and elusive, deploying effective defensive AI strategies becomes indispensable. However, several challenges hinder the necessary progress of AI-based fincrime detection systems. This paper examines the latest trends in AI/ML-driven financial crimes and detection systems. It underscores the urgent need for developing agile AI defenses that can effectively counteract the rapidly emerging threats. It also aims to highlight the need for cooperation across the financial services industry to tackle the GenAI induced crime waves.</li>
</ul>

<h3>Title: Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Andrey Anurin, Jonathan Ng, Kibo Schaffer, Ziyue Wang, Jason Schreiber, Esben Kran</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09114">https://arxiv.org/abs/2410.09114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09114">https://arxiv.org/pdf/2410.09114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09114]] Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities(https://arxiv.org/abs/2410.09114)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>LLM agents have the potential to revolutionize defensive cyber operations, but their offensive capabilities are not yet fully understood. To prepare for emerging threats, model developers and governments are evaluating the cyber capabilities of foundation models. However, these assessments often lack transparency and a comprehensive focus on offensive capabilities. In response, we introduce the Catastrophic Cyber Capabilities Benchmark (3CB), a novel framework designed to rigorously assess the real-world offensive capabilities of LLM agents. Our evaluation of modern LLMs on 3CB reveals that frontier models, such as GPT-4o and Claude 3.5 Sonnet, can perform offensive tasks such as reconnaissance and exploitation across domains ranging from binary analysis to web technologies. Conversely, smaller open-source models exhibit limited offensive capabilities. Our software solution and the corresponding benchmark provides a critical tool to reduce the gap between rapidly improving capabilities and robustness of cyber offense evaluations, aiding in the safer deployment and regulation of these powerful technologies.</li>
</ul>

<h3>Title: On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Bokun Wang, Yunwen Lei, Yiming Ying, Tianbao Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09156">https://arxiv.org/abs/2410.09156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09156">https://arxiv.org/pdf/2410.09156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09156]] On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning(https://arxiv.org/abs/2410.09156)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We study the discriminative probabilistic modeling problem on a continuous domain for (multimodal) self-supervised representation learning. To address the challenge of computing the integral in the partition function for each anchor data, we leverage the multiple importance sampling (MIS) technique for robust Monte Carlo integration, which can recover InfoNCE-based contrastive loss as a special case. Within this probabilistic modeling framework, we conduct generalization error analysis to reveal the limitation of current InfoNCE-based contrastive loss for self-supervised representation learning and derive insights for developing better approaches by reducing the error of Monte Carlo integration. To this end, we propose a novel non-parametric method for approximating the sum of conditional densities required by MIS through convex optimization, yielding a new contrastive objective for self-supervised representation learning. Moreover, we design an efficient algorithm for solving the proposed objective. We empirically compare our algorithm to representative baselines on the contrastive image-language pretraining task. Experimental results on the CC3M and CC12M datasets demonstrate the superior overall performance of our algorithm.</li>
</ul>

<h3>Title: An Efficient Contrastive Unimodal Pretraining Method for EHR Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Ryan King, Shivesh Kodali, Conrad Krueger, Tianbao Yang, Bobak J. Mortazavi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09199">https://arxiv.org/abs/2410.09199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09199">https://arxiv.org/pdf/2410.09199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09199]] An Efficient Contrastive Unimodal Pretraining Method for EHR Time Series Data(https://arxiv.org/abs/2410.09199)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Machine learning has revolutionized the modeling of clinical timeseries data. Using machine learning, a Deep Neural Network (DNN) can be automatically trained to learn a complex mapping of its input features for a desired task. This is particularly valuable in Electronic Health Record (EHR) databases, where patients often spend extended periods in intensive care units (ICUs). Machine learning serves as an efficient method for extract meaningful information. However, many state-of-the-art (SOTA) methods for training DNNs demand substantial volumes of labeled data, posing significant challenges for clinics in terms of cost and time. Self-supervised learning offers an alternative by allowing practitioners to extract valuable insights from data without the need for costly labels. Yet, current SOTA methods often necessitate large data batches to achieve optimal performance, increasing computational demands. This presents a challenge when working with long clinical timeseries data. To address this, we propose an efficient method of contrastive pretraining tailored for long clinical timeseries data. Our approach utilizes an estimator for negative pair comparison, enabling effective feature extraction. We assess the efficacy of our pretraining using standard self-supervised tasks such as linear evaluation and semi-supervised learning. Additionally, our model demonstrates the ability to impute missing measurements, providing clinicians with deeper insights into patient conditions. We demonstrate that our pretraining is capable of achieving better performance as both the size of the model and the size of the measurement vocabulary scale. Finally, we externally validate our model, trained on the MIMIC-III dataset, using the eICU dataset. We demonstrate that our model is capable of learning robust clinical information that is transferable to other clinics.</li>
</ul>

<h3>Title: Foundation Model-Powered 3D Few-Shot Class Incremental Learning via Training-free Adaptor</h3>
<ul>
<li><strong>Authors: </strong>Sahar Ahmadi, Ali Cheraghian, Morteza Saberi, Md.Towsif Abir, Hamidreza Dastmalchi, Farookh Hussain, Shafin Rahman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09237">https://arxiv.org/abs/2410.09237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09237">https://arxiv.org/pdf/2410.09237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09237]] Foundation Model-Powered 3D Few-Shot Class Incremental Learning via Training-free Adaptor(https://arxiv.org/abs/2410.09237)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning for processing point clouds hold increased interest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision. This paper introduces a new method to tackle the Few-Shot Continual Incremental Learning (FSCIL) problem in 3D point cloud environments. We leverage a foundational 3D model trained extensively on point cloud data. Drawing from recent improvements in foundation models, known for their ability to work well across different tasks, we propose a novel strategy that does not require additional training to adapt to new tasks. Our approach uses a dual cache system: first, it uses previous test samples based on how confident the model was in its predictions to prevent forgetting, and second, it includes a small number of new task samples to prevent overfitting. This dynamic adaptation ensures strong performance across different learning tasks without needing lots of fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet, ScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and demonstrating its effectiveness and versatility. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder</h3>
<ul>
<li><strong>Authors: </strong>Maksim Kuznetsov, Airat Valiev, Alex Aliper, Daniil Polykovskiy, Elena Tutubalina, Rim Shayakhmetov, Zulfat Miftahutdinov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09240">https://arxiv.org/abs/2410.09240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09240">https://arxiv.org/pdf/2410.09240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09240]] nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder(https://arxiv.org/abs/2410.09240)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements have integrated Language Models (LMs) into a drug discovery pipeline. However, existing models mostly work with SMILES and SELFIES chemical string representations, which lack spatial features vital for drug discovery. Additionally, attempts to translate chemical 3D structures into text format encounter issues such as excessive length and insufficient atom connectivity information. To address these issues, we introduce nach0-pc, a model combining domain-specific encoder and textual representation to handle spatial arrangement of atoms effectively. Our approach utilizes a molecular point cloud encoder for concise and order-invariant structure representation. We introduce a novel pre-training scheme for molecular point clouds to distillate the knowledge from spatial molecular structures datasets. After fine-tuning within both single-task and multi-task frameworks, nach0-pc demonstrates performance comparable with other diffusion models in terms of generated samples quality across several established spatial molecular generation tasks. Notably, our model is a multi-task approach, in contrast to diffusion models being limited to single tasks. Additionally, it is capable of processing point cloud-related data, which language models are not capable of handling due to memory limitations. These lead to our model having reduced training and inference time while maintaining on par performance.</li>
</ul>

<h3>Title: DFM: Interpolant-free Dual Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09246">https://arxiv.org/abs/2410.09246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09246">https://arxiv.org/pdf/2410.09246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09246]] DFM: Interpolant-free Dual Flow Matching(https://arxiv.org/abs/2410.09246)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Continuous normalizing flows (CNFs) can model data distributions with expressive infinite-length architectures. But this modeling involves computationally expensive process of solving an ordinary differential equation (ODE) during maximum likelihood training. Recently proposed flow matching (FM) framework allows to substantially simplify the training phase using a regression objective with the interpolated forward vector field. In this paper, we propose an interpolant-free dual flow matching (DFM) approach without explicit assumptions about the modeled vector field. DFM optimizes the forward and, additionally, a reverse vector field model using a novel objective that facilitates bijectivity of the forward and reverse transformations. Our experiments with the SMAP unsupervised anomaly detection show advantages of DFM when compared to the CNF trained with either maximum likelihood or FM objectives with the state-of-the-art performance metrics.</li>
</ul>

<h3>Title: Enhanced Federated Anomaly Detection Through Autoencoders Using Summary Statistics-Based Thresholding</h3>
<ul>
<li><strong>Authors: </strong>Sofiane Laridi, Gregory Palmer, Kam-Ming Mark Tam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09284">https://arxiv.org/abs/2410.09284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09284">https://arxiv.org/pdf/2410.09284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09284]] Enhanced Federated Anomaly Detection Through Autoencoders Using Summary Statistics-Based Thresholding(https://arxiv.org/abs/2410.09284)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In Federated Learning (FL), anomaly detection (AD) is a challenging task due to the decentralized nature of data and the presence of non-IID data distributions. This study introduces a novel federated threshold calculation method that leverages summary statistics from both normal and anomalous data to improve the accuracy and robustness of anomaly detection using autoencoders (AE) in a federated setting. Our approach aggregates local summary statistics across clients to compute a global threshold that optimally separates anomalies from normal data while ensuring privacy preservation. We conducted extensive experiments using publicly available datasets, including Credit Card Fraud Detection, Shuttle, and Covertype, under various data distribution scenarios. The results demonstrate that our method consistently outperforms existing federated and local threshold calculation techniques, particularly in handling non-IID data distributions. This study also explores the impact of different data distribution scenarios and the number of clients on the performance of federated anomaly detection. Our findings highlight the potential of using summary statistics for threshold calculation in improving the scalability and accuracy of federated anomaly detection systems.</li>
</ul>

<h3>Title: DeepOSets: Non-Autoregressive In-Context Learning of Supervised Learning Operators</h3>
<ul>
<li><strong>Authors: </strong>Shao-Ting Chiu, Junyuan Hong, Ulisses Braga-Neto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09298">https://arxiv.org/abs/2410.09298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09298">https://arxiv.org/pdf/2410.09298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09298]] DeepOSets: Non-Autoregressive In-Context Learning of Supervised Learning Operators(https://arxiv.org/abs/2410.09298)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We introduce DeepSets Operator Networks (DeepOSets), an efficient, non-autoregressive neural network architecture for in-context operator learning. In-context learning allows a trained machine learning model to learn from a user prompt without further training. DeepOSets adds in-context learning capabilities to Deep Operator Networks (DeepONets) by combining it with the DeepSets architecture. As the first non-autoregressive model for in-context operator learning, DeepOSets allow the user prompt to be processed in parallel, leading to significant computational savings. Here, we present the application of DeepOSets in the problem of learning supervised learning algorithms, which are operators mapping a finite-dimensional space of labeled data into an infinite-dimensional hypothesis space of prediction functions. In an empirical comparison with a popular autoregressive (transformer-based) model for in-context learning of the least-squares linear regression algorithm, DeepOSets reduced the number of model weights by several orders of magnitude and required a fraction of training and inference time. Furthermore, DeepOSets proved to be less sensitive to noise, outperforming the transformer model in noisy settings.</li>
</ul>

<h3>Title: TD-Paint: Faster Diffusion Inpainting Through Time Aware Pixel Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Tsiry Mayet, Pourya Shamsolmoali, Simon Bernard, Eric Granger, Romain Hérault, Clement Chatelain</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09306">https://arxiv.org/abs/2410.09306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09306">https://arxiv.org/pdf/2410.09306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09306]] TD-Paint: Faster Diffusion Inpainting Through Time Aware Pixel Conditioning(https://arxiv.org/abs/2410.09306)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as highly effective techniques for inpainting, however, they remain constrained by slow sampling rates. While recent advances have enhanced generation quality, they have also increased sampling time, thereby limiting scalability in real-world applications. We investigate the generative sampling process of diffusion-based inpainting models and observe that these models make minimal use of the input condition during the initial sampling steps. As a result, the sampling trajectory deviates from the data manifold, requiring complex synchronization mechanisms to realign the generation process. To address this, we propose Time-aware Diffusion Paint (TD-Paint), a novel approach that adapts the diffusion process by modeling variable noise levels at the pixel level. This technique allows the model to efficiently use known pixel values from the start, guiding the generation process toward the target manifold. By embedding this information early in the diffusion process, TD-Paint significantly accelerates sampling without compromising image quality. Unlike conventional diffusion-based inpainting models, which require a dedicated architecture or an expensive generation loop, TD-Paint achieves faster sampling times without architectural modifications. Experimental results across three datasets show that TD-Paint outperforms state-of-the-art diffusion models while maintaining lower complexity.</li>
</ul>

<h3>Title: LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Chong Li, Xinyi Chen, Shuo Wang, Yu Chao, Zhili Li, Haoyu Wang, Rongqiao An, Qi Shi, Zhixing Tan, Xu Han, Xiaodong Shi, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09342">https://arxiv.org/abs/2410.09342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09342">https://arxiv.org/pdf/2410.09342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09342]] LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models(https://arxiv.org/abs/2410.09342)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLM$\times$MapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLM$\times$MapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.</li>
</ul>

<h3>Title: ELICIT: LLM Augmentation via External In-Context Capability</h3>
<ul>
<li><strong>Authors: </strong>Futing Wang, Jianhao Yan, Yue Zhang, Tao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09343">https://arxiv.org/abs/2410.09343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09343">https://arxiv.org/pdf/2410.09343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09343]] ELICIT: LLM Augmentation via External In-Context Capability(https://arxiv.org/abs/2410.09343)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application. Traditional fine-tuning methods require substantial data and computational resources, especially for enhancing specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage. Inspired by the expression of in-context learned capabilities through task vectors and the concept of modularization, we propose \alg, a framework consisting of two modules designed to effectively store and reuse task vectors to elicit the diverse capabilities of models without additional training or inference tokens. Our comprehensive experiments and analysis demonstrate that our pipeline is highly transferable across different input formats, tasks, and model architectures. ELICIT serves as a plug-and-play performance booster to enable adaptive elicitation of model capabilities. By externally storing and reusing vectors that represent in-context learned capabilities, \alg not only demonstrates the potential to operate modular capabilities but also significantly enhances the performance, versatility, adaptability, and scalability of large language models. Our code will be publicly available at this https URL.</li>
</ul>

<h3>Title: Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment</h3>
<ul>
<li><strong>Authors: </strong>Huayu Chen, Hang Su, Peize Sun, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09347">https://arxiv.org/abs/2410.09347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09347">https://arxiv.org/pdf/2410.09347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09347]] Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment(https://arxiv.org/abs/2410.09347)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose \textit{Condition Contrastive Alignment} (CCA) to facilitate guidance-free AR visual generation with high performance and analyze its theoretical connection with guided sampling methods. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning ($\sim$ 1\% of pretraining epochs) on the pretraining dataset, on par with guided sampling methods. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. Code and model weights: this https URL.</li>
</ul>

<h3>Title: Inference and Verbalization Functions During In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Junyi Tao, Xiaoyin Chen, Nelson F. Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09349">https://arxiv.org/abs/2410.09349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09349">https://arxiv.org/pdf/2410.09349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09349]] Inference and Verbalization Functions During In-Context Learning(https://arxiv.org/abs/2410.09349)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LMs) are capable of in-context learning from a few demonstrations (example-label pairs) to solve new tasks during inference. Despite the intuitive importance of high-quality demonstrations, previous work has observed that, in some settings, ICL performance is minimally affected by irrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with irrelevant labels via two sequential processes: an inference function that solves the task, followed by a verbalization function that maps the inferred answer to the label space. Importantly, we hypothesize that the inference function is invariant to remappings of the label space (e.g., "true"/"false" to "cat"/"dog"), enabling LMs to share the same inference function across settings with different label words. We empirically validate this hypothesis with controlled layer-wise interchange intervention experiments. Our findings confirm the hypotheses on multiple datasets and tasks (natural language inference, sentiment analysis, and topic classification) and further suggest that the two functions can be localized in specific layers across various open-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and LLAMA-3.1-70B.</li>
</ul>

<h3>Title: Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyoung Park, Minseok Joo, Joo-Kyung Kim, Hyunwoo J. Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09350">https://arxiv.org/abs/2410.09350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09350">https://arxiv.org/pdf/2410.09350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09350]] Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation(https://arxiv.org/abs/2410.09350)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Knowledge graph-grounded dialog generation requires retrieving a dialog-relevant subgraph from the given knowledge base graph and integrating it with the dialog history. Previous works typically represent the graph using an external encoder, such as graph neural networks, and retrieve relevant triplets based on the similarity between single-vector representations of triplets and the dialog history. However, these external encoders fail to leverage the rich knowledge of pretrained language models, and the retrieval process is also suboptimal due to the information bottleneck caused by the single-vector abstraction of the dialog history. In this work, we propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant knowledge subgraphs by directly generating their token sequences on top of language models. For effective generative subgraph retrieval, we introduce two key methods: (i) structure-aware knowledge graph linearization with self-supervised graph-specific tokens and (ii) graph-constrained decoding utilizing graph structural proximity-based entity informativeness scores for valid and relevant generative retrieval. DialogGSR achieves state-of-the-art performance in knowledge graph-grounded dialog generation, as demonstrated on OpenDialKG and KOMODIS datasets.</li>
</ul>

<h3>Title: On Divergence Measures for Training GFlowNets</h3>
<ul>
<li><strong>Authors: </strong>Tiago da Silva, Eliezer de Souza da Silva, Diego Mesquita</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09355">https://arxiv.org/abs/2410.09355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09355">https://arxiv.org/pdf/2410.09355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09355]] On Divergence Measures for Training GFlowNets(https://arxiv.org/abs/2410.09355)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets) are amortized inference models designed to sample from unnormalized distributions over composable objects, with applications in generative modeling for tasks in fields such as causal discovery, NLP, and drug discovery. Traditionally, the training procedure for GFlowNets seeks to minimize the expected log-squared difference between a proposal (forward policy) and a target (backward policy) distribution, which enforces certain flow-matching conditions. While this training procedure is closely related to variational inference (VI), directly attempting standard Kullback-Leibler (KL) divergence minimization can lead to proven biased and potentially high-variance estimators. Therefore, we first review four divergence measures, namely, Renyi-$\alpha$'s, Tsallis-$\alpha$'s, reverse and forward KL's, and design statistically efficient estimators for their stochastic gradients in the context of training GFlowNets. Then, we verify that properly minimizing these divergences yields a provably correct and empirically effective training scheme, often leading to significantly faster convergence than previously proposed optimization. To achieve this, we design control variates based on the REINFORCE leave-one-out and score-matching estimators to reduce the variance of the learning objectives' gradients. Our work contributes by narrowing the gap between GFlowNets training and generalized variational approximations, paving the way for algorithmic ideas informed by the divergence minimization viewpoint.</li>
</ul>

<h3>Title: Multi-granularity Contrastive Cross-modal Collaborative Generation for End-to-End Long-term Video Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Ting Yu, Kunhao Fu, Jian Zhang, Qingming Huang, Jun Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09379">https://arxiv.org/abs/2410.09379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09379">https://arxiv.org/pdf/2410.09379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09379]] Multi-granularity Contrastive Cross-modal Collaborative Generation for End-to-End Long-term Video Question Answering(https://arxiv.org/abs/2410.09379)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Long-term Video Question Answering (VideoQA) is a challenging vision-and-language bridging task focusing on semantic understanding of untrimmed long-term videos and diverse free-form questions, simultaneously emphasizing comprehensive cross-modal reasoning to yield precise answers. The canonical approaches often rely on off-the-shelf feature extractors to detour the expensive computation overhead, but often result in domain-independent modality-unrelated representations. Furthermore, the inherent gradient blocking between unimodal comprehension and cross-modal interaction hinders reliable answer generation. In contrast, recent emerging successful video-language pre-training models enable cost-effective end-to-end modeling but fall short in domain-specific ratiocination and exhibit disparities in task formulation. Toward this end, we present an entirely end-to-end solution for long-term VideoQA: Multi-granularity Contrastive cross-modal collaborative Generation (MCG) model. To derive discriminative representations possessing high visual concepts, we introduce Joint Unimodal Modeling (JUM) on a clip-bone architecture and leverage Multi-granularity Contrastive Learning (MCL) to harness the intrinsically or explicitly exhibited semantic correspondences. To alleviate the task formulation discrepancy problem, we propose a Cross-modal Collaborative Generation (CCG) module to reformulate VideoQA as a generative task instead of the conventional classification scheme, empowering the model with the capability for cross-modal high-semantic fusion and generation so as to rationalize and answer. Extensive experiments conducted on six publicly available VideoQA datasets underscore the superiority of our proposed method.</li>
</ul>

<h3>Title: Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Ting Yu, Kunhao Fu, Shuhui Wang, Qingming Huang, Jun Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09380">https://arxiv.org/abs/2410.09380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09380">https://arxiv.org/pdf/2410.09380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09380]] Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering(https://arxiv.org/abs/2410.09380)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Video Question Answering (VideoQA) represents a crucial intersection between video understanding and language processing, requiring both discriminative unimodal comprehension and sophisticated cross-modal interaction for accurate inference. Despite advancements in multi-modal pre-trained models and video-language foundation models, these systems often struggle with domain-specific VideoQA due to their generalized pre-training objectives. Addressing this gap necessitates bridging the divide between broad cross-modal knowledge and the specific inference demands of VideoQA tasks. To this end, we introduce HeurVidQA, a framework that leverages domain-specific entity-action heuristics to refine pre-trained video-language foundation models. Our approach treats these models as implicit knowledge engines, employing domain-specific entity-action prompters to direct the model's focus toward precise cues that enhance reasoning. By delivering fine-grained heuristics, we improve the model's ability to identify and interpret key entities and actions, thereby enhancing its reasoning capabilities. Extensive evaluations across multiple VideoQA datasets demonstrate that our method significantly outperforms existing models, underscoring the importance of integrating domain-specific knowledge into video-language models for more accurate and context-aware VideoQA.</li>
</ul>

<h3>Title: Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models</h3>
<ul>
<li><strong>Authors: </strong>Sathya Kamesh Bhethanabhotla, Omar Swelam, Julien Siems, David Salinas, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09385">https://arxiv.org/abs/2410.09385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09385">https://arxiv.org/pdf/2410.09385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09385]] Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models(https://arxiv.org/abs/2410.09385)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks without the need for dataset specific fine-tuning. Mamba4Cast's key innovation lies in its ability to achieve strong zero-shot performance on real-world datasets while having much lower inference times than time series foundation models based on the transformer architecture. Trained solely on synthetic data, the model generates forecasts for entire horizons in a single pass, outpacing traditional auto-regressive approaches. Our experiments show that Mamba4Cast performs competitively against other state-of-the-art foundation models in various data sets while scaling significantly better with the prediction length. The source code can be accessed at this https URL.</li>
</ul>

<h3>Title: CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Xu, Zhenliang He, Shiguang Shan, Xilin Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09400">https://arxiv.org/abs/2410.09400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09400">https://arxiv.org/pdf/2410.09400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09400]] CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation(https://arxiv.org/abs/2410.09400)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, large-scale diffusion models have made impressive progress in text-to-image (T2I) generation. To further equip these T2I models with fine-grained spatial control, approaches like ControlNet introduce an extra network that learns to follow a condition image. However, for every single condition type, ControlNet requires independent training on millions of data pairs with hundreds of GPU hours, which is quite expensive and makes it challenging for ordinary users to explore and develop new types of conditions. To address this problem, we propose the CtrLoRA framework, which trains a Base ControlNet to learn the common knowledge of image-to-image generation from multiple base conditions, along with condition-specific LoRAs to capture distinct characteristics of each condition. Utilizing our pretrained Base ControlNet, users can easily adapt it to new conditions, requiring as few as 1,000 data pairs and less than one hour of single-GPU training to obtain satisfactory results in most scenarios. Moreover, our CtrLoRA reduces the learnable parameters by 90% compared to ControlNet, significantly lowering the threshold to distribute and deploy the model weights. Extensive experiments on various types of conditions demonstrate the efficiency and effectiveness of our method. Codes and model weights will be released at this https URL.</li>
</ul>

<h3>Title: Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study</h3>
<ul>
<li><strong>Authors: </strong>Pengfei He, Yingqian Cui, Han Xu, Hui Liu, Makoto Yamada, Jiliang Tang, Yue Xing</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09411">https://arxiv.org/abs/2410.09411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09411">https://arxiv.org/pdf/2410.09411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09411]] Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study(https://arxiv.org/abs/2410.09411)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has emerged as a powerful capability for large language models (LLMs) to adapt to downstream tasks by leveraging a few (demonstration) examples. Despite its effectiveness, the mechanism behind ICL remains underexplored. To better understand how ICL integrates the examples with the knowledge learned by the LLM during pre-training (i.e., pre-training knowledge) and how the examples impact ICL, this paper conducts a theoretical study in binary classification tasks. In particular, we introduce a probabilistic model extending from the Gaussian mixture model to exactly quantify the impact of pre-training knowledge, label frequency, and label noise on the prediction accuracy. Based on our analysis, when the pre-training knowledge contradicts the knowledge in the examples, whether ICL prediction relies more on the pre-training knowledge or the examples depends on the number of examples. In addition, the label frequency and label noise of the examples both affect the accuracy of the ICL prediction, where the minor class has a lower accuracy, and how the label noise impacts the accuracy is determined by the specific noise level of the two classes. Extensive simulations are conducted to verify the correctness of the theoretical results, and real-data experiments also align with the theoretical insights. Our work reveals the role of pre-training knowledge and examples in ICL, offering a deeper understanding of LLMs' behaviors in classification tasks.</li>
</ul>

<h3>Title: Power-Softmax: Towards Secure LLM Inference over Encrypted Data</h3>
<ul>
<li><strong>Authors: </strong>Itamar Zimerman, Allon Adir, Ehud Aharoni, Matan Avitan, Moran Baruch, Nir Drucker, Jenny Lerner, Ramy Masalha, Reut Meiri, Omri Soceanu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09457">https://arxiv.org/abs/2410.09457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09457">https://arxiv.org/pdf/2410.09457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09457]] Power-Softmax: Towards Secure LLM Inference over Encrypted Data(https://arxiv.org/abs/2410.09457)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Modern cryptographic methods for implementing privacy-preserving LLMs such as Homomorphic Encryption (HE) require the LLMs to have a polynomial form. Forming such a representation is challenging because Transformers include non-polynomial components, such as Softmax and layer normalization. Previous approaches have either directly approximated pre-trained models with large-degree polynomials, which are less efficient over HE, or replaced non-polynomial components with easier-to-approximate primitives before training, e.g., Softmax with pointwise attention. The latter approach might introduce scalability challenges. We present a new HE-friendly variant of self-attention that offers a stable form for training and is easy to approximate with polynomials for secure inference. Our work introduces the first polynomial LLMs with 32 layers and over a billion parameters, exceeding the size of previous models by more than tenfold. The resulting models demonstrate reasoning and in-context learning (ICL) capabilities comparable to standard transformers of the same size, representing a breakthrough in the field. Finally, we provide a detailed latency breakdown for each computation over encrypted data, paving the way for further optimization, and explore the differences in inductive bias between transformers relying on our HE-friendly variant and standard transformers. Our code is attached as a supplement.</li>
</ul>

<h3>Title: Enhancing Single Image to 3D Generation using Gaussian Splatting and Hybrid Diffusion Priors</h3>
<ul>
<li><strong>Authors: </strong>Hritam Basak, Hadi Tabatabaee, Shreekant Gayaka, Ming-Feng Li, Xin Yang, Cheng-Hao Kuo, Arnie Sen, Min Sun, Zhaozheng Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09467">https://arxiv.org/abs/2410.09467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09467">https://arxiv.org/pdf/2410.09467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09467]] Enhancing Single Image to 3D Generation using Gaussian Splatting and Hybrid Diffusion Priors(https://arxiv.org/abs/2410.09467)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D object generation from a single image involves estimating the full 3D geometry and texture of unseen views from an unposed RGB image captured in the wild. Accurately reconstructing an object's complete 3D structure and texture has numerous applications in real-world scenarios, including robotic manipulation, grasping, 3D scene understanding, and AR/VR. Recent advancements in 3D object generation have introduced techniques that reconstruct an object's 3D shape and texture by optimizing the efficient representation of Gaussian Splatting, guided by pre-trained 2D or 3D diffusion models. However, a notable disparity exists between the training datasets of these models, leading to distinct differences in their outputs. While 2D models generate highly detailed visuals, they lack cross-view consistency in geometry and texture. In contrast, 3D models ensure consistency across different views but often result in overly smooth textures. We propose bridging the gap between 2D and 3D diffusion models to address this limitation by integrating a two-stage frequency-based distillation loss with Gaussian Splatting. Specifically, we leverage geometric priors in the low-frequency spectrum from a 3D diffusion model to maintain consistent geometry and use a 2D diffusion model to refine the fidelity and texture in the high-frequency spectrum of the generated 3D structure, resulting in more detailed and fine-grained outcomes. Our approach enhances geometric consistency and visual quality, outperforming the current SOTA. Additionally, we demonstrate the easy adaptability of our method for efficient object pose estimation and tracking.</li>
</ul>

<h3>Title: Pic@Point: Cross-Modal Learning by Local and Global Point-Picture Correspondence</h3>
<ul>
<li><strong>Authors: </strong>Vencia Herzog, Stefan Suwelack</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09519">https://arxiv.org/abs/2410.09519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09519">https://arxiv.org/pdf/2410.09519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09519]] Pic@Point: Cross-Modal Learning by Local and Global Point-Picture Correspondence(https://arxiv.org/abs/2410.09519)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised pre-training has achieved remarkable success in NLP and 2D vision. However, these advances have yet to translate to 3D data. Techniques like masked reconstruction face inherent challenges on unstructured point clouds, while many contrastive learning tasks lack in complexity and informative value. In this paper, we present Pic@Point, an effective contrastive learning method based on structural 2D-3D correspondences. We leverage image cues rich in semantic and contextual knowledge to provide a guiding signal for point cloud representations at various abstraction levels. Our lightweight approach outperforms state-of-the-art pre-training methods on several 3D benchmarks.</li>
</ul>

<h3>Title: LexSumm and LexT5: Benchmarking and Modeling Legal Summarization Tasks in English</h3>
<ul>
<li><strong>Authors: </strong>T.Y.S.S. Santosh, Cornelius Weiss, Matthias Grabmair</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09527">https://arxiv.org/abs/2410.09527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09527">https://arxiv.org/pdf/2410.09527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09527]] LexSumm and LexT5: Benchmarking and Modeling Legal Summarization Tasks in English(https://arxiv.org/abs/2410.09527)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the evolving NLP landscape, benchmarks serve as yardsticks for gauging progress. However, existing Legal NLP benchmarks only focus on predictive tasks, overlooking generative tasks. This work curates LexSumm, a benchmark designed for evaluating legal summarization tasks in English. It comprises eight English legal summarization datasets, from diverse jurisdictions, such as the US, UK, EU and India. Additionally, we release LexT5, legal oriented sequence-to-sequence model, addressing the limitation of the existing BERT-style encoder-only models in the legal domain. We assess its capabilities through zero-shot probing on LegalLAMA and fine-tuning on LexSumm. Our analysis reveals abstraction and faithfulness errors even in summaries generated by zero-shot LLMs, indicating opportunities for further improvements. LexSumm benchmark and LexT5 model are available at this https URL.</li>
</ul>

<h3>Title: DiffuTraj: A Stochastic Vessel Trajectory Prediction Approach via Guided Diffusion Process</h3>
<ul>
<li><strong>Authors: </strong>Changlin Li, Yanglei Gan, Tian Lan, Yuxiang Cai, Xueyi Liu, Run Lin, Qiao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09550">https://arxiv.org/abs/2410.09550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09550">https://arxiv.org/pdf/2410.09550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09550]] DiffuTraj: A Stochastic Vessel Trajectory Prediction Approach via Guided Diffusion Process(https://arxiv.org/abs/2410.09550)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Maritime vessel maneuvers, characterized by their inherent complexity and indeterminacy, requires vessel trajectory prediction system capable of modeling the multi-modality nature of future motion states. Conventional stochastic trajectory prediction methods utilize latent variables to represent the multi-modality of vessel motion, however, tends to overlook the complexity and dynamics inherent in maritime behavior. In contrast, we explicitly simulate the transition of vessel motion from uncertainty towards a state of certainty, effectively handling future indeterminacy in dynamic scenes. In this paper, we present a novel framework (\textit{DiffuTraj}) to conceptualize the trajectory prediction task as a guided reverse process of motion pattern uncertainty diffusion, in which we progressively remove uncertainty from maritime regions to delineate the intended trajectory. Specifically, we encode the previous states of the target vessel, vessel-vessel interactions, and the environment context as guiding factors for trajectory generation. Subsequently, we devise a transformer-based conditional denoiser to capture spatio-temporal dependencies, enabling the generation of trajectories better aligned for particular maritime environment. Comprehensive experiments on vessel trajectory prediction benchmarks demonstrate the superiority of our method.</li>
</ul>

<h3>Title: Timeseria: an object-oriented time series processing library</h3>
<ul>
<li><strong>Authors: </strong>Stefano Alberto Russo, Giuliano Taffonia, Luca Bortolussi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09567">https://arxiv.org/abs/2410.09567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09567">https://arxiv.org/pdf/2410.09567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09567]] Timeseria: an object-oriented time series processing library(https://arxiv.org/abs/2410.09567)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Timeseria is an object-oriented time series processing library implemented in Python, which aims at making it easier to manipulate time series data and to build statistical and machine learning models on top of it. Unlike common data analysis frameworks, it builds up from well defined and reusable logical units (objects), which can be easily combined together in order to ensure a high level of consistency. Thanks to this approach, Timeseria can address by design several non-trivial issues often underestimated, such as handling data losses, non-uniform sampling rates, differences between aggregated data and punctual observations, time zones, daylight saving times, and more. Timeseria comes with a comprehensive set of base data structures, common data manipulation operations, and extensible models for data reconstruction, forecasting and anomaly detection. It also integrates a powerful plotting engine capable of handling even millions of data points.</li>
</ul>

<h3>Title: The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Subhankar Maity, Aniket Deroy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09576">https://arxiv.org/abs/2410.09576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09576">https://arxiv.org/pdf/2410.09576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09576]] The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models(https://arxiv.org/abs/2410.09576)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.</li>
</ul>

<h3>Title: Structure of Artificial Neural Networks -- Empirical Investigations</h3>
<ul>
<li><strong>Authors: </strong>Julian Stier</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09579">https://arxiv.org/abs/2410.09579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09579">https://arxiv.org/pdf/2410.09579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09579]] Structure of Artificial Neural Networks -- Empirical Investigations(https://arxiv.org/abs/2410.09579)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Within one decade, Deep Learning overtook the dominating solution methods of countless problems of artificial intelligence. ``Deep'' refers to the deep architectures with operations in manifolds of which there are no immediate observations. For these deep architectures some kind of structure is pre-defined -- but what is this structure? With a formal definition for structures of neural networks, neural architecture search problems and solution methods can be formulated under a common framework. Both practical and theoretical questions arise from closing the gap between applied neural architecture search and learning theory. Does structure make a difference or can it be chosen arbitrarily? This work is concerned with deep structures of artificial neural networks and examines automatic construction methods under empirical principles to shed light on to the so called ``black-box models''. Our contributions include a formulation of graph-induced neural networks that is used to pose optimisation problems for neural architecture. We analyse structural properties for different neural network objectives such as correctness, robustness or energy consumption and discuss how structure affects them. Selected automation methods for neural architecture optimisation problems are discussed and empirically analysed. With the insights gained from formalising graph-induced neural networks, analysing structural properties and comparing the applicability of neural architecture search methods qualitatively and quantitatively we advance these methods in two ways. First, new predictive models are presented for replacing computationally expensive evaluation schemes, and second, new generative models for informed sampling during neural architecture search are analysed and discussed.</li>
</ul>

<h3>Title: DuoDiff: Accelerating Diffusion Models with a Dual-Backbone Approach</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gallo Fernández, Rǎzvan-Andrei Matişan, Alejandro Monroy Muñoz, Ana-Maria Vasilcoiu, Janusz Partyka, Tin Hadži Veljković, Metod Jazbec</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09633">https://arxiv.org/abs/2410.09633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09633">https://arxiv.org/pdf/2410.09633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09633]] DuoDiff: Accelerating Diffusion Models with a Dual-Backbone Approach(https://arxiv.org/abs/2410.09633)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved unprecedented performance in image generation, yet they suffer from slow inference due to their iterative sampling process. To address this, early-exiting has recently been proposed, where the depth of the denoising network is made adaptive based on the (estimated) difficulty of each sampling step. Here, we discover an interesting "phase transition" in the sampling process of current adaptive diffusion models: the denoising network consistently exits early during the initial sampling steps, until it suddenly switches to utilizing the full network. Based on this, we propose accelerating generation by employing a shallower denoising network in the initial sampling steps and a deeper network in the later steps. We demonstrate empirically that our dual-backbone approach, DuoDiff, outperforms existing early-exit diffusion methods in both inference speed and generation quality. Importantly, DuoDiff is easy to implement and complementary to existing approaches for accelerating diffusion.</li>
</ul>

<h3>Title: Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health</h3>
<ul>
<li><strong>Authors: </strong>Abdullah Mamun, Lawrence D. Devoe, Mark I. Evans, David W. Britt, Judith Klein-Seetharaman, Hassan Ghasemzadeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09635">https://arxiv.org/abs/2410.09635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09635">https://arxiv.org/pdf/2410.09635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09635]] Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health(https://arxiv.org/abs/2410.09635)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Early detection of intrapartum risk enables interventions to potentially prevent or mitigate adverse labor outcomes such as cerebral palsy. Currently, there is no accurate automated system to predict such events to assist with clinical decision-making. To fill this gap, we propose "Artificial Intelligence (AI) for Modeling and Explaining Neonatal Health" (AIMEN), a deep learning framework that not only predicts adverse labor outcomes from maternal, fetal, obstetrical, and intrapartum risk factors but also provides the model's reasoning behind the predictions made. The latter can provide insights into what modifications in the input variables of the model could have changed the predicted outcome. We address the challenges of imbalance and small datasets by synthesizing additional training data using Adaptive Synthetic Sampling (ADASYN) and Conditional Tabular Generative Adversarial Networks (CTGAN). AIMEN uses an ensemble of fully-connected neural networks as the backbone for its classification with the data augmentation supported by either ADASYN or CTGAN. AIMEN, supported by CTGAN, outperforms AIMEN supported by ADASYN in classification. AIMEN can predict a high risk for adverse labor outcomes with an average F1 score of 0.784. It also provides counterfactual explanations that can be achieved by changing 2 to 3 attributes on average. Resources available: this https URL.</li>
</ul>

<h3>Title: Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?</h3>
<ul>
<li><strong>Authors: </strong>HyoJung Han, Akiko Eriguchi, Haoran Xu, Hieu Hoang, Marine Carpuat, Huda Khayrallah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09644">https://arxiv.org/abs/2410.09644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09644">https://arxiv.org/pdf/2410.09644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09644]] Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?(https://arxiv.org/abs/2410.09644)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vocabulary adaptation, which integrates new vocabulary into pre-trained language models (LMs), enables expansion to new languages and mitigates token over-fragmentation. However, existing approaches are limited by their reliance on heuristic or external embeddings. We propose VocADT, a novel method for vocabulary adaptation using adapter modules that are trained to learn the optimal linear combination of existing embeddings while keeping the model's weights fixed. VocADT offers a flexible and scalable solution without requiring external resources or language constraints. Across 11 languages-with various scripts, resource availability, and fragmentation-we demonstrate that VocADT outperforms the original Mistral model and other baselines across various multilingual tasks. We find that Latin-script languages and highly fragmented languages benefit the most from vocabulary adaptation. We further fine-tune the adapted model on the generative task of machine translation and find that vocabulary adaptation is still beneficial after fine-tuning and that VocADT is the most effective method.</li>
</ul>

<h3>Title: EquiJump: Protein Dynamics Simulation via SO(3)-Equivariant Stochastic Interpolants</h3>
<ul>
<li><strong>Authors: </strong>Allan dos Santos Costa, Ilan Mitnikov, Franco Pellegrini, Ameya Daigavane, Mario Geiger, Zhonglin Cao, Karsten Kreis, Tess Smidt, Emine Kucukbenli, Joseph Jacobson</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09667">https://arxiv.org/abs/2410.09667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09667">https://arxiv.org/pdf/2410.09667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09667]] EquiJump: Protein Dynamics Simulation via SO(3)-Equivariant Stochastic Interpolants(https://arxiv.org/abs/2410.09667)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Mapping the conformational dynamics of proteins is crucial for elucidating their functional mechanisms. While Molecular Dynamics (MD) simulation enables detailed time evolution of protein motion, its computational toll hinders its use in practice. To address this challenge, multiple deep learning models for reproducing and accelerating MD have been proposed drawing on transport-based generative methods. However, existing work focuses on generation through transport of samples from prior distributions, that can often be distant from the data manifold. The recently proposed framework of stochastic interpolants, instead, enables transport between arbitrary distribution endpoints. Building upon this work, we introduce EquiJump, a transferable SO(3)-equivariant model that bridges all-atom protein dynamics simulation time steps directly. Our approach unifies diverse sampling methods and is benchmarked against existing models on trajectory data of fast folding proteins. EquiJump achieves state-of-the-art results on dynamics simulation with a transferable model on all of the fast folding proteins.</li>
</ul>

<h3>Title: Robust 3D Point Clouds Classification based on Declarative Defenders</h3>
<ul>
<li><strong>Authors: </strong>Kaidong Li, Tianxiao Zhang, Chuncong Zhong, Ziming Zhang, Guanghui Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09691">https://arxiv.org/abs/2410.09691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09691">https://arxiv.org/pdf/2410.09691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09691]] Robust 3D Point Clouds Classification based on Declarative Defenders(https://arxiv.org/abs/2410.09691)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>3D point cloud classification requires distinct models from 2D image classification due to the divergent characteristics of the respective input data. While 3D point clouds are unstructured and sparse, 2D images are structured and dense. Bridging the domain gap between these two data types is a non-trivial challenge to enable model interchangeability. Recent research using Lattice Point Classifier (LPC) highlights the feasibility of cross-domain applicability. However, the lattice projection operation in LPC generates 2D images with disconnected projected pixels. In this paper, we explore three distinct algorithms for mapping 3D point clouds into 2D images. Through extensive experiments, we thoroughly examine and analyze their performance and defense mechanisms. Leveraging current large foundation models, we scrutinize the feature disparities between regular 2D images and projected 2D images. The proposed approaches demonstrate superior accuracy and robustness against adversarial attacks. The generative model-based mapping algorithms yield regular 2D images, further minimizing the domain gap from regular 2D classification tasks. The source code is available at this https URL.</li>
</ul>

<h3>Title: Can In-context Learning Really Generalize to Out-of-distribution Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Qixun Wang, Yifei Wang, Yisen Wang, Xianghua Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09695">https://arxiv.org/abs/2410.09695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09695">https://arxiv.org/pdf/2410.09695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09695]] Can In-context Learning Really Generalize to Out-of-distribution Tasks?(https://arxiv.org/abs/2410.09695)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In this work, we explore the mechanism of in-context learning (ICL) on out-of-distribution (OOD) tasks that were not encountered during training. To achieve this, we conduct synthetic experiments where the objective is to learn OOD mathematical functions through ICL using a GPT-2 model. We reveal that Transformers may struggle to learn OOD task functions through ICL. Specifically, ICL performance resembles implementing a function within the pretraining hypothesis space and optimizing it with gradient descent based on the in-context examples. Additionally, we investigate ICL's well-documented ability to learn unseen abstract labels in context. We demonstrate that such ability only manifests in the scenarios without distributional shifts and, therefore, may not serve as evidence of new-task-learning ability. Furthermore, we assess ICL's performance on OOD tasks when the model is pretrained on multiple tasks. Both empirical and theoretical analyses demonstrate the existence of the \textbf{low-test-error preference} of ICL, where it tends to implement the pretraining function that yields low test error in the testing context. We validate this through numerical experiments. This new theoretical result, combined with our empirical findings, elucidates the mechanism of ICL in addressing OOD tasks.</li>
</ul>

<h3>Title: EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Milos Vukadinovic, Xiu Tang, Neal Yuan, Paul Cheng, Debiao Li, Susan Cheng, Bryan He, David Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09704">https://arxiv.org/abs/2410.09704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09704">https://arxiv.org/pdf/2410.09704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09704]] EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation(https://arxiv.org/abs/2410.09704)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocardiography AI models are single-view, single-task systems that do not synthesize complementary information from multiple views captured during a full exam, and thus lead to limited performance and scope of applications. To address this problem, we introduce EchoPrime, a multi-view, view-informed, video-based vision-language foundation model trained on over 12 million video-report pairs. EchoPrime uses contrastive learning to train a unified embedding model for all standard views in a comprehensive echocardiogram study with representation of both rare and common diseases and diagnoses. EchoPrime then utilizes view-classification and a view-informed anatomic attention model to weight video-specific interpretations that accurately maps the relationship between echocardiographic views and anatomical structures. With retrieval-augmented interpretation, EchoPrime integrates information from all echocardiogram videos in a comprehensive study and performs holistic comprehensive clinical echocardiography interpretation. In datasets from two independent healthcare systems, EchoPrime achieves state-of-the art performance on 23 diverse benchmarks of cardiac form and function, surpassing the performance of both task-specific approaches and prior foundation models. Following rigorous clinical evaluation, EchoPrime can assist physicians in the automated preliminary assessment of comprehensive echocardiography.</li>
</ul>

<h3>Title: LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Junyan Ye, Baichuan Zhou, Zilong Huang, Junan Zhang, Tianyi Bai, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, Zhizheng Wu, Yiping Chen, Dahua Lin, Conghui He, Weijia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09732">https://arxiv.org/abs/2410.09732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09732">https://arxiv.org/pdf/2410.09732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09732]] LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models(https://arxiv.org/abs/2410.09732)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at this https URL</li>
</ul>

<h3>Title: Data Adaptive Few-shot Multi Label Segmentation with Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Gurunath Reddy, Dattesh Shanbhag, Deepa Anand</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09759">https://arxiv.org/abs/2410.09759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09759">https://arxiv.org/pdf/2410.09759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09759]] Data Adaptive Few-shot Multi Label Segmentation with Foundation Model(https://arxiv.org/abs/2410.09759)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The high cost of obtaining accurate annotations for image segmentation and localization makes the use of one and few shot algorithms attractive. Several state-of-the-art methods for few-shot segmentation have emerged, including text-based prompting for the task but suffer from sub-optimal performance for medical images. Leveraging sub-pixel level features of existing Vision Transformer (ViT) based foundation models for identifying similar region of interest (RoI) based on a single template image have been shown to be very effective for one shot segmentation and localization in medical images across modalities. However, such methods rely on assumption that template image and test image are well matched and simple correlation is sufficient to obtain correspondences. In practice, however such an approach can fail to generalize in clinical data due to patient pose changes, inter-protocol variations even within a single modality or extend to 3D data using single template image. Moreover, for multi-label tasks, the RoI identification has to be performed sequentially. In this work, we propose foundation model (FM) based adapters for single label, multi-label localization and segmentation to address these concerns. We demonstrate the efficacy of the proposed method for multiple segmentation and localization tasks for both 2D and 3D data as we well as clinical data with different poses and evaluate against the state of the art few shot segmentation methods.</li>
</ul>

<h3>Title: Compressing Scene Dynamics: A Generative Approach</h3>
<ul>
<li><strong>Authors: </strong>Shanzhi Yin, Zihan Zhang, Bolin Chen, Shiqi Wang, Yan Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09768">https://arxiv.org/abs/2410.09768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09768">https://arxiv.org/pdf/2410.09768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09768]] Compressing Scene Dynamics: A Generative Approach(https://arxiv.org/abs/2410.09768)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper proposes to learn generative priors from the motion patterns instead of video contents for generative video compression. The priors are derived from small motion dynamics in common scenes such as swinging trees in the wind and floating boat on the sea. Utilizing such compact motion priors, a novel generative scene dynamics compression framework is built to realize ultra-low bit-rate communication and high-quality reconstruction for diverse scene contents. At the encoder side, motion priors are characterized into compact representations in a dense-to-sparse manner. At the decoder side, the decoded motion priors serve as the trajectory hints for scene dynamics reconstruction via a diffusion-based flow-driven generator. The experimental results illustrate that the proposed method can achieve superior rate-distortion performance and outperform the state-of-the-art conventional video codec Versatile Video Coding (VVC) on scene dynamics sequences. The project page can be found at this https URL.</li>
</ul>

<h3>Title: Intermediate Representations for Enhanced Text-To-Image Generation Using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ran Galun, Sagie Benaim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09792">https://arxiv.org/abs/2410.09792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09792">https://arxiv.org/pdf/2410.09792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09792]] Intermediate Representations for Enhanced Text-To-Image Generation Using Diffusion Models(https://arxiv.org/abs/2410.09792)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have demonstrated an impressive ability to produce high-quality outputs. However, they often struggle to accurately follow fine-grained spatial information in an input text. To this end, we propose a compositional approach for text-to-image generation based on two stages. In the first stage, we design a diffusion-based generative model to produce one or more aligned intermediate representations (such as depth or segmentation maps) conditioned on text. In the second stage, we map these representations, together with the text, to the final output image using a separate diffusion-based generative model. Our findings indicate that such compositional approach can improve image generation, resulting in a notable improvement in FID score and a comparable CLIP score, when compared to the standard non-compositional baseline.</li>
</ul>

<h3>Title: EBDM: Exemplar-guided Image Translation with Brownian-bridge Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Eungbean Lee, Somi Jeong, Kwanghoon Sohn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09802">https://arxiv.org/abs/2410.09802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09802">https://arxiv.org/pdf/2410.09802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09802]] EBDM: Exemplar-guided Image Translation with Brownian-bridge Diffusion Models(https://arxiv.org/abs/2410.09802)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Exemplar-guided image translation, synthesizing photo-realistic images that conform to both structural control and style exemplars, is attracting attention due to its ability to enhance user control over style manipulation. Previous methodologies have predominantly depended on establishing dense correspondences across cross-domain inputs. Despite these efforts, they incur quadratic memory and computational costs for establishing dense correspondence, resulting in limited versatility and performance degradation. In this paper, we propose a novel approach termed Exemplar-guided Image Translation with Brownian-Bridge Diffusion Models (EBDM). Our method formulates the task as a stochastic Brownian bridge process, a diffusion process with a fixed initial point as structure control and translates into the corresponding photo-realistic image while being conditioned solely on the given exemplar image. To efficiently guide the diffusion process toward the style of exemplar, we delineate three pivotal components: the Global Encoder, the Exemplar Network, and the Exemplar Attention Module to incorporate global and detailed texture information from exemplar images. Leveraging Bridge diffusion, the network can translate images from structure control while exclusively conditioned on the exemplar style, leading to more robust training and inference processes. We illustrate the superiority of our method over competing approaches through comprehensive benchmark evaluations and visual results.</li>
</ul>

<h3>Title: DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Kecen Li, Bingquan Dai, Jingjing Fu, Xinwen Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09821">https://arxiv.org/abs/2410.09821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09821">https://arxiv.org/pdf/2410.09821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09821]] DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection(https://arxiv.org/abs/2410.09821)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Synthesizing anomaly samples has proven to be an effective strategy for self-supervised 2D industrial anomaly detection. However, this approach has been rarely explored in multi-modality anomaly detection, particularly involving 3D and RGB images. In this paper, we propose a novel dual-modality augmentation method for 3D anomaly synthesis, which is simple and capable of mimicking the characteristics of 3D defects. Incorporating with our anomaly synthesis method, we introduce a reconstruction-based discriminative anomaly detection network, in which a dual-modal discriminator is employed to fuse the original and reconstructed embedding of two modalities for anomaly detection. Additionally, we design an augmentation dropout mechanism to enhance the generalizability of the discriminator. Extensive experiments show that our method outperforms the state-of-the-art methods on detection precision and achieves competitive segmentation performance on both MVTec 3D-AD and Eyescandies datasets.</li>
</ul>

<h3>Title: Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for Fine-Tuning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fei Wang, Li Shen, Liang Ding, Chao Xue, Ye Liu, Changxing Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09823">https://arxiv.org/abs/2410.09823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09823">https://arxiv.org/pdf/2410.09823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09823]] Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for Fine-Tuning Large Language Models(https://arxiv.org/abs/2410.09823)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning is powerful for adapting large language models to downstream tasks, but it often results in huge memory usages. A promising approach to mitigate this is using Zeroth-Order (ZO) optimization, which estimates gradients to replace First-Order (FO) gradient calculations, albeit with longer training time due to its stochastic nature. By revisiting the Memory-efficient ZO (MeZO) optimizer, we discover that the full-parameter perturbation and updating processes consume over 50% of its overall fine-tuning time cost. Based on these observations, we introduce a novel layer-wise sparse computation and memory efficient ZO optimizer, named LeZO. LeZO treats layers as fundamental units for sparsification and dynamically perturbs different parameter subsets in each step to achieve full-parameter fine-tuning. LeZO incorporates layer-wise parameter sparsity in the process of simultaneous perturbation stochastic approximation (SPSA) and ZO stochastic gradient descent (ZO-SGD). It achieves accelerated computation during perturbation and updating processes without additional memory overhead. We conduct extensive experiments with the OPT model family on the SuperGLUE benchmark and two generative tasks. The experiments show that LeZO accelerates training without compromising the performance of ZO optimization. Specifically, it achieves over 3x speedup compared to MeZO on the SST-2, BoolQ, and Copa tasks.</li>
</ul>

<h3>Title: LoLI-Street: Benchmarking Low-Light Image Enhancement and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Md Tanvir Islam, Inzamamul Alam, Simon S. Woo, Saeed Anwar, IK Hyun Lee, Khan Muhammad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09831">https://arxiv.org/abs/2410.09831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09831">https://arxiv.org/pdf/2410.09831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09831]] LoLI-Street: Benchmarking Low-Light Image Enhancement and Beyond(https://arxiv.org/abs/2410.09831)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement (LLIE) is essential for numerous computer vision tasks, including object detection, tracking, segmentation, and scene understanding. Despite substantial research on improving low-quality images captured in underexposed conditions, clear vision remains critical for autonomous vehicles, which often struggle with low-light scenarios, signifying the need for continuous research. However, paired datasets for LLIE are scarce, particularly for street scenes, limiting the development of robust LLIE methods. Despite using advanced transformers and/or diffusion-based models, current LLIE methods struggle in real-world low-light conditions and lack training on street-scene datasets, limiting their effectiveness for autonomous vehicles. To bridge these gaps, we introduce a new dataset LoLI-Street (Low-Light Images of Streets) with 33k paired low-light and well-exposed images from street scenes in developed cities, covering 19k object classes for object detection. LoLI-Street dataset also features 1,000 real low-light test images for testing LLIE models under real-life conditions. Furthermore, we propose a transformer and diffusion-based LLIE model named "TriFuse". Leveraging the LoLI-Street dataset, we train and evaluate our TriFuse and SOTA models to benchmark on our dataset. Comparing various models, our dataset's generalization feasibility is evident in testing across different mainstream datasets by significantly enhancing images and object detection for practical applications in autonomous driving and surveillance systems. The complete code and dataset is available on this https URL.</li>
</ul>

<h3>Title: Toward Defining an Efficient and Expandable File Format for AI-Generated Contents</h3>
<ul>
<li><strong>Authors: </strong>Yixin Gao, Runsen Feng, Xin Li, Weiping Li, Zhibo Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09834">https://arxiv.org/abs/2410.09834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09834">https://arxiv.org/pdf/2410.09834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09834]] Toward Defining an Efficient and Expandable File Format for AI-Generated Contents(https://arxiv.org/abs/2410.09834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, AI-generated content (AIGC) has gained significant traction due to its powerful creation capability. However, the storage and transmission of large amounts of high-quality AIGC images inevitably pose new challenges for recent file formats. To overcome this, we define a new file format for AIGC images, named AIGIF, enabling ultra-low bitrate coding of AIGC images. Unlike compressing AIGC images intuitively with pixel-wise space as existing file formats, AIGIF instead compresses the generation syntax. This raises a crucial question: Which generation syntax elements, e.g., text prompt, device configuration, etc, are necessary for compression/transmission? To answer this question, we systematically investigate the effects of three essential factors: platform, generative model, and data configuration. We experimentally find that a well-designed composable bitstream structure incorporating the above three factors can achieve an impressive compression ratio of even up to 1/10,000 while still ensuring high fidelity. We also introduce an expandable syntax in AIGIF to support the extension of the most advanced generation models to be developed in the future.</li>
</ul>

<h3>Title: AuthFace: Towards Authentic Blind Face Restoration with Face-oriented Generative Diffusion Prior</h3>
<ul>
<li><strong>Authors: </strong>Guoqiang Liang, Qingnan Fan, Bingtao Fu, Jinwei Chen, Hong Gu, Lin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09864">https://arxiv.org/abs/2410.09864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09864">https://arxiv.org/pdf/2410.09864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09864]] AuthFace: Towards Authentic Blind Face Restoration with Face-oriented Generative Diffusion Prior(https://arxiv.org/abs/2410.09864)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Blind face restoration (BFR) is a fundamental and challenging problem in computer vision. To faithfully restore high-quality (HQ) photos from poor-quality ones, recent research endeavors predominantly rely on facial image priors from the powerful pretrained text-to-image (T2I) diffusion models. However, such priors often lead to the incorrect generation of non-facial features and insufficient facial details, thus rendering them less practical for real-world applications. In this paper, we propose a novel framework, namely AuthFace that achieves highly authentic face restoration results by exploring a face-oriented generative diffusion prior. To learn such a prior, we first collect a dataset of 1.5K high-quality images, with resolutions exceeding 8K, captured by professional photographers. Based on the dataset, we then introduce a novel face-oriented restoration-tuning pipeline that fine-tunes a pretrained T2I model. Identifying key criteria of quality-first and photography-guided annotation, we involve the retouching and reviewing process under the guidance of photographers for high-quality images that show rich facial features. The photography-guided annotation system fully explores the potential of these high-quality photographic images. In this way, the potent natural image priors from pretrained T2I diffusion models can be subtly harnessed, specifically enhancing their capability in facial detail restoration. Moreover, to minimize artifacts in critical facial areas, such as eyes and mouth, we propose a time-aware latent facial feature loss to learn the authentic face restoration process. Extensive experiments on the synthetic and real-world BFR datasets demonstrate the superiority of our approach.</li>
</ul>

<h3>Title: SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Xilin He, Cheng Luo, Xiaole Xian, Bing Li, Siyang Song, Muhammad Haris Khan, Weicheng Xie, Linlin Shen, Zongyuan Ge</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09865">https://arxiv.org/abs/2410.09865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09865">https://arxiv.org/pdf/2410.09865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09865]] SynFER: Towards Boosting Facial Expression Recognition with Synthetic Data(https://arxiv.org/abs/2410.09865)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Facial expression datasets remain limited in scale due to privacy concerns, the subjectivity of annotations, and the labor-intensive nature of data collection. This limitation poses a significant challenge for developing modern deep learning-based facial expression analysis models, particularly foundation models, that rely on large-scale data for optimal performance. To tackle the overarching and complex challenge, we introduce SynFER (Synthesis of Facial Expressions with Refined Control), a novel framework for synthesizing facial expression image data based on high-level textual descriptions as well as more fine-grained and precise control through facial action units. To ensure the quality and reliability of the synthetic data, we propose a semantic guidance technique to steer the generation process and a pseudo-label generator to help rectify the facial expression labels for the synthetic images. To demonstrate the generation fidelity and the effectiveness of the synthetic data from SynFER, we conduct extensive experiments on representation learning using both synthetic data and real-world data. Experiment results validate the efficacy of the proposed approach and the synthetic data. Notably, our approach achieves a 67.23% classification accuracy on AffectNet when training solely with synthetic data equivalent to the AffectNet training set size, which increases to 69.84% when scaling up to five times the original size. Our code will be made publicly available.</li>
</ul>

<h3>Title: Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy</h3>
<ul>
<li><strong>Authors: </strong>Hancheng Ye, Jiakang Yuan, Renqiu Xia, Xiangchao Yan, Tao Chen, Junchi Yan, Botian Shi, Bo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09873">https://arxiv.org/abs/2410.09873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09873">https://arxiv.org/pdf/2410.09873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09873]] Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy(https://arxiv.org/abs/2410.09873)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently achieved great success in the synthesis of high-quality images and videos. However, the existing denoising techniques in diffusion models are commonly based on step-by-step noise predictions, which suffers from high computation cost, resulting in a prohibitive latency for interactive applications. In this paper, we propose AdaptiveDiffusion to relieve this bottleneck by adaptively reducing the noise prediction steps during the denoising process. Our method considers the potential of skipping as many noise prediction steps as possible while keeping the final denoised results identical to the original full-step ones. Specifically, the skipping strategy is guided by the third-order latent difference that indicates the stability between timesteps during the denoising process, which benefits the reusing of previous noise prediction results. Extensive experiments on image and video diffusion models demonstrate that our method can significantly speed up the denoising process while generating identical results to the original process, achieving up to an average 2~5x speedup without quality degradation.</li>
</ul>

<h3>Title: Block-to-Scene Pre-training for Point Cloud Hybrid-Domain Masked Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Yaohua Zha, Tao Dai, Yanzi Wang, Hang Guo, Taolin Zhang, Zhihao Ouyang, Chunlin Fan, Bin Chen, Ke Chen, Shu-Tao Xia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09886">https://arxiv.org/abs/2410.09886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09886">https://arxiv.org/pdf/2410.09886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09886]] Block-to-Scene Pre-training for Point Cloud Hybrid-Domain Masked Autoencoders(https://arxiv.org/abs/2410.09886)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Point clouds, as a primary representation of 3D data, can be categorized into scene domain point clouds and object domain point clouds based on the modeled content. Masked autoencoders (MAE) have become the mainstream paradigm in point clouds self-supervised learning. However, existing MAE-based methods are domain-specific, limiting the model's generalization. In this paper, we propose to pre-train a general Point cloud Hybrid-Domain Masked AutoEncoder (PointHDMAE) via a block-to-scene pre-training strategy. We first propose a hybrid-domain masked autoencoder consisting of an encoder and decoder belonging to the scene domain and object domain, respectively. The object domain encoder specializes in handling object point clouds and multiple shared object encoders assist the scene domain encoder in analyzing the scene point clouds. Furthermore, we propose a block-to-scene strategy to pre-train our hybrid-domain model. Specifically, we first randomly select point blocks within a scene and apply a set of transformations to convert each point block coordinates from the scene space to the object space. Then, we employ an object-level mask and reconstruction pipeline to recover the masked points of each block, enabling the object encoder to learn a universal object representation. Finally, we introduce a scene-level block position regression pipeline, which utilizes the blocks' features in the object space to regress these blocks' initial positions within the scene space, facilitating the learning of scene representations. Extensive experiments across different datasets and tasks demonstrate the generalization and superiority of our hybrid-domain model.</li>
</ul>

<h3>Title: RMB: Comprehensively Benchmarking Reward Models in LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09893">https://arxiv.org/abs/2410.09893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09893">https://arxiv.org/pdf/2410.09893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09893]] RMB: Comprehensively Benchmarking Reward Models in LLM Alignment(https://arxiv.org/abs/2410.09893)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited distribution of evaluation data and evaluation methods that are not closely related to alignment objectives. To address these limitations, we propose RMB, a comprehensive RM benchmark that covers over 49 real-world scenarios and includes both pairwise and Best-of-N (BoN) evaluations to better reflect the effectiveness of RMs in guiding alignment optimization. We demonstrate a positive correlation between our benchmark and the downstream alignment task performance. Based on our benchmark, we conduct extensive analysis on the state-of-the-art RMs, revealing their generalization defects that were not discovered by previous benchmarks, and highlighting the potential of generative RMs. Furthermore, we delve into open questions in reward models, specifically examining the effectiveness of majority voting for the evaluation of reward models and analyzing the impact factors of generative RMs, including the influence of evaluation criteria and instructing methods. Our evaluation code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble for Zero-shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Jin, Peng Shu, Sekeun Kim, Qing Xiao, Sifan Song, Cheng Chen, Tianming Liu, Xiang Li, Quanzheng Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09908">https://arxiv.org/abs/2410.09908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09908">https://arxiv.org/pdf/2410.09908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09908]] Retrieval Instead of Fine-tuning: A Retrieval-based Parameter Ensemble for Zero-shot Learning(https://arxiv.org/abs/2410.09908)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have become a cornerstone in deep learning, with techniques like Low-Rank Adaptation (LoRA) offering efficient fine-tuning of large models. Similarly, methods such as Retrieval-Augmented Generation (RAG), which leverage vectorized databases, have further improved model performance by grounding outputs in external information. While these approaches have demonstrated notable success, they often require extensive training or labeled data, which can limit their adaptability in resource-constrained environments. To address these challenges, we introduce Retrieval-based Parameter Ensemble (RPE), a new method that creates a vectorized database of LoRAs, enabling efficient retrieval and application of model adaptations to new tasks. RPE minimizes the need for extensive training and eliminates the requirement for labeled data, making it particularly effective for zero-shot learning. Additionally, RPE is well-suited for privacy-sensitive domains like healthcare, as it modifies model parameters without accessing raw data. When applied to tasks such as medical report generation and image segmentation, RPE not only proved effective but also surpassed supervised fine-tuning methods in certain cases, highlighting its potential to enhance both computational efficiency and privacy in deep learning applications.</li>
</ul>

<h3>Title: UnSeg: One Universal Unlearnable Example Generator is Enough against All Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ye Sun, Hao Zhang, Tiehua Zhang, Xingjun Ma, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09909">https://arxiv.org/abs/2410.09909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09909">https://arxiv.org/pdf/2410.09909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09909]] UnSeg: One Universal Unlearnable Example Generator is Enough against All Image Segmentation(https://arxiv.org/abs/2410.09909)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Image segmentation is a crucial vision task that groups pixels within an image into semantically meaningful segments, which is pivotal in obtaining a fine-grained understanding of real-world scenes. However, an increasing privacy concern exists regarding training large-scale image segmentation models on unauthorized private data. In this work, we exploit the concept of unlearnable examples to make images unusable to model training by generating and adding unlearnable noise into the original images. Particularly, we propose a novel Unlearnable Segmentation (UnSeg) framework to train a universal unlearnable noise generator that is capable of transforming any downstream images into their unlearnable version. The unlearnable noise generator is finetuned from the Segment Anything Model (SAM) via bilevel optimization on an interactive segmentation dataset towards minimizing the training error of a surrogate model that shares the same architecture with SAM but is trained from scratch. We empirically verify the effectiveness of UnSeg across 6 mainstream image segmentation tasks, 10 widely used datasets, and 7 different network architectures, and show that the unlearnable images can reduce the segmentation performance by a large margin. Our work provides useful insights into how to leverage foundation models in a data-efficient and computationally affordable manner to protect images against image segmentation models.</li>
</ul>

<h3>Title: Combining Generative and Geometry Priors for Wide-Angle Portrait Correction</h3>
<ul>
<li><strong>Authors: </strong>Lan Yao, Chaofeng Chen, Xiaoming Li, Zifei Yan, Wangmeng Zuo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09911">https://arxiv.org/abs/2410.09911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09911">https://arxiv.org/pdf/2410.09911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09911]] Combining Generative and Geometry Priors for Wide-Angle Portrait Correction(https://arxiv.org/abs/2410.09911)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Wide-angle lens distortion in portrait photography presents a significant challenge for capturing photo-realistic and aesthetically pleasing images. Such distortions are especially noticeable in facial regions. In this work, we propose encapsulating the generative face prior as a guided natural manifold to facilitate the correction of facial regions. Moreover, a notable central symmetry relationship exists in the non-face background, yet it has not been explored in the correction process. This geometry prior motivates us to introduce a novel constraint to explicitly enforce symmetry throughout the correction process, thereby contributing to a more visually appealing and natural correction in the non-face region. Experiments demonstrate that our approach outperforms previous methods by a large margin, excelling not only in quantitative measures such as line straightness and shape consistency metrics but also in terms of perceptual visual quality. All the code and models are available at this https URL.</li>
</ul>

<h3>Title: MisinfoEval: Generative AI in the Era of "Alternative Facts"</h3>
<ul>
<li><strong>Authors: </strong>Saadia Gabriel, Liang Lyu, James Siderius, Marzyeh Ghassemi, Jacob Andreas, Asu Ozdaglar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09949">https://arxiv.org/abs/2410.09949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09949">https://arxiv.org/pdf/2410.09949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09949]] MisinfoEval: Generative AI in the Era of "Alternative Facts"(https://arxiv.org/abs/2410.09949)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The spread of misinformation on social media platforms threatens democratic processes, contributes to massive economic losses, and endangers public health. Many efforts to address misinformation focus on a knowledge deficit model and propose interventions for improving users' critical thinking through access to facts. Such efforts are often hampered by challenges with scalability, and by platform users' personal biases. The emergence of generative AI presents promising opportunities for countering misinformation at scale across ideological barriers. In this paper, we introduce a framework (MisinfoEval) for generating and comprehensively evaluating large language model (LLM) based misinformation interventions. We present (1) an experiment with a simulated social media environment to measure effectiveness of misinformation interventions, and (2) a second experiment with personalized explanations tailored to the demographics and beliefs of users with the goal of countering misinformation by appealing to their pre-existing values. Our findings confirm that LLM-based interventions are highly effective at correcting user behavior (improving overall user accuracy at reliability labeling by up to 41.72%). Furthermore, we find that users favor more personalized interventions when making decisions about news reliability and users shown personalized interventions have significantly higher accuracy at identifying misinformation.</li>
</ul>

<h3>Title: LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Han Qiu, Jiaxing Huang, Peng Gao, Qin Qi, Xiaoqin Zhang, Ling Shao, Shijian Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09962">https://arxiv.org/abs/2410.09962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09962">https://arxiv.org/pdf/2410.09962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09962]] LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models(https://arxiv.org/abs/2410.09962)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Hallucination, a phenomenon where multimodal large language models~(MLLMs) tend to generate textual responses that are plausible but unaligned with the image, has become one major hurdle in various MLLM-related applications. Several benchmarks have been created to gauge the hallucination levels of MLLMs, by either raising discriminative questions about the existence of objects or introducing LLM evaluators to score the generated text from MLLMs. However, the discriminative data largely involve simple questions that are not aligned with real-world text, while the generative data involve LLM evaluators that are computationally intensive and unstable due to their inherent randomness. We propose LongHalQA, an LLM-free hallucination benchmark that comprises 6K long and complex hallucination text. LongHalQA is featured by GPT4V-generated hallucinatory data that are well aligned with real-world scenarios, including object/image descriptions and multi-round conversations with 14/130 words and 189 words, respectively, on average. It introduces two new tasks, hallucination discrimination and hallucination completion, unifying both discriminative and generative evaluations in a single multiple-choice-question form and leading to more reliable and efficient evaluations without the need for LLM evaluators. Further, we propose an advanced pipeline that greatly facilitates the construction of future hallucination benchmarks with long and complex questions and descriptions. Extensive experiments over multiple recent MLLMs reveal various new challenges when they are handling hallucinations with long and complex textual data. Dataset and evaluation code are available at this https URL.</li>
</ul>

<h3>Title: Make the Pertinent Salient: Task-Relevant Reconstruction for Visual Control with Distractions</h3>
<ul>
<li><strong>Authors: </strong>Kyungmin Kim, JB Lanier, Pierre Baldi, Charless Fowlkes, Roy Fox</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09972">https://arxiv.org/abs/2410.09972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09972">https://arxiv.org/pdf/2410.09972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09972]] Make the Pertinent Salient: Task-Relevant Reconstruction for Visual Control with Distractions(https://arxiv.org/abs/2410.09972)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Model-Based Reinforcement Learning (MBRL) have made it a powerful tool for visual control tasks. Despite improved data efficiency, it remains challenging to train MBRL agents with generalizable perception. Training in the presence of visual distractions is particularly difficult due to the high variation they introduce to representation learning. Building on DREAMER, a popular MBRL method, we propose a simple yet effective auxiliary task to facilitate representation learning in distracting environments. Under the assumption that task-relevant components of image observations are straightforward to identify with prior knowledge in a given task, we use a segmentation mask on image observations to only reconstruct task-relevant components. In doing so, we greatly reduce the complexity of representation learning by removing the need to encode task-irrelevant objects in the latent representation. Our method, Segmentation Dreamer (SD), can be used either with ground-truth masks easily accessible in simulation or by leveraging potentially imperfect segmentation foundation models. The latter is further improved by selectively applying the reconstruction loss to avoid providing misleading learning signals due to mask prediction errors. In modified DeepMind Control suite (DMC) and Meta-World tasks with added visual distractions, SD achieves significantly better sample efficiency and greater final performance than prior work. We find that SD is especially helpful in sparse reward tasks otherwise unsolvable by prior work, enabling the training of visually robust agents without the need for extensive reward engineering.</li>
</ul>

<h3>Title: InterMask: 3D Human Interaction Generation via Collaborative Masked Modelling</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Gohar Javed, Chuan Guo, Li Cheng, Xingyu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10010">https://arxiv.org/abs/2410.10010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10010">https://arxiv.org/pdf/2410.10010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10010]] InterMask: 3D Human Interaction Generation via Collaborative Masked Modelling(https://arxiv.org/abs/2410.10010)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating realistic 3D human-human interactions from textual descriptions remains a challenging task. Existing approaches, typically based on diffusion models, often generate unnatural and unrealistic results. In this work, we introduce InterMask, a novel framework for generating human interactions using collaborative masked modeling in discrete space. InterMask first employs a VQ-VAE to transform each motion sequence into a 2D discrete motion token map. Unlike traditional 1D VQ token maps, it better preserves fine-grained spatio-temporal details and promotes spatial awareness within each token. Building on this representation, InterMask utilizes a generative masked modeling framework to collaboratively model the tokens of two interacting individuals. This is achieved by employing a transformer architecture specifically designed to capture complex spatio-temporal interdependencies. During training, it randomly masks the motion tokens of both individuals and learns to predict them. In inference, starting from fully masked sequences, it progressively fills in the tokens for both individuals. With its enhanced motion representation, dedicated architecture, and effective learning strategy, InterMask achieves state-of-the-art results, producing high-fidelity and diverse human interactions. It outperforms previous methods, achieving an FID of $5.154$ (vs $5.535$ for in2IN) on the InterHuman dataset and $0.399$ (vs $5.207$ for InterGen) on the InterX dataset. Additionally, InterMask seamlessly supports reaction generation without the need for model redesign or fine-tuning.</li>
</ul>

<h3>Title: GALA: Geometry-Aware Local Adaptive Grids for Detailed 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Dingdong Yang, Yizhi Wang, Konrad Schindler, Ali Mahdavi Amiri, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10037">https://arxiv.org/abs/2410.10037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10037">https://arxiv.org/pdf/2410.10037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10037]] GALA: Geometry-Aware Local Adaptive Grids for Detailed 3D Generation(https://arxiv.org/abs/2410.10037)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose GALA, a novel representation of 3D shapes that (i) excels at capturing and reproducing complex geometry and surface details, (ii) is computationally efficient, and (iii) lends itself to 3D generative modelling with modern, diffusion-based schemes. The key idea of GALA is to exploit both the global sparsity of surfaces within a 3D volume and their local surface properties. Sparsity is promoted by covering only the 3D object boundaries, not empty space, with an ensemble of tree root voxels. Each voxel contains an octree to further limit storage and compute to regions that contain surfaces. Adaptivity is achieved by fitting one local and geometry-aware coordinate frame in each non-empty leaf node. Adjusting the orientation of the local grid, as well as the anisotropic scales of its axes, to the local surface shape greatly increases the amount of detail that can be stored in a given amount of memory, which in turn allows for quantization without loss of quality. With our optimized C++/CUDA implementation, GALA can be fitted to an object in less than 10 seconds. Moreover, the representation can efficiently be flattened and manipulated with transformer networks. We provide a cascaded generation pipeline capable of generating 3D shapes with great geometric detail.</li>
</ul>

<h3>Title: DINTR: Tracking via Diffusion-based Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Pha Nguyen, Ngan Le, Jackson Cothren, Alper Yilmaz, Khoa Luu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10053">https://arxiv.org/abs/2410.10053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10053">https://arxiv.org/pdf/2410.10053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10053]] DINTR: Tracking via Diffusion-based Interpolation(https://arxiv.org/abs/2410.10053)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Object tracking is a fundamental task in computer vision, requiring the localization of objects of interest across video frames. Diffusion models have shown remarkable capabilities in visual generation, making them well-suited for addressing several requirements of the tracking problem. This work proposes a novel diffusion-based methodology to formulate the tracking task. Firstly, their conditional process allows for injecting indications of the target object into the generation process. Secondly, diffusion mechanics can be developed to inherently model temporal correspondences, enabling the reconstruction of actual frames in video. However, existing diffusion models rely on extensive and unnecessary mapping to a Gaussian noise domain, which can be replaced by a more efficient and stable interpolation process. Our proposed interpolation mechanism draws inspiration from classic image-processing techniques, offering a more interpretable, stable, and faster approach tailored specifically for the object tracking task. By leveraging the strengths of diffusion models while circumventing their limitations, our Diffusion-based INterpolation TrackeR (DINTR) presents a promising new paradigm and achieves a superior multiplicity on seven benchmarks across five indicator representations.</li>
</ul>

<h3>Title: Learning to Customize Text-to-Image Diffusion In Diverse Context</h3>
<ul>
<li><strong>Authors: </strong>Taewook Kim, Wei Chen, Qiang Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10058">https://arxiv.org/abs/2410.10058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10058">https://arxiv.org/pdf/2410.10058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10058]] Learning to Customize Text-to-Image Diffusion In Diverse Context(https://arxiv.org/abs/2410.10058)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Most text-to-image customization techniques fine-tune models on a small set of \emph{personal concept} images captured in minimal contexts. This often results in the model becoming overfitted to these training images and unable to generalize to new contexts in future text prompts. Existing customization methods are built on the success of effectively representing personal concepts as textual embeddings. Thus, in this work, we resort to diversifying the context of these personal concepts \emph{solely} within the textual space by simply creating a contextually rich set of text prompts, together with a widely used self-supervised learning objective. Surprisingly, this straightforward and cost-effective method significantly improves semantic alignment in the textual space, and this effect further extends to the image space, resulting in higher prompt fidelity for generated images. Additionally, our approach does not require any architectural modifications, making it highly compatible with existing text-to-image customization methods. We demonstrate the broad applicability of our approach by combining it with four different baseline methods, achieving notable CLIP score improvements.</li>
</ul>

<h3>Title: Divide, Reweight, and Conquer: A Logit Arithmetic Approach for In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Chengsong Huang, Langlin Huang, Jiaxin Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10074">https://arxiv.org/abs/2410.10074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10074">https://arxiv.org/pdf/2410.10074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10074]] Divide, Reweight, and Conquer: A Logit Arithmetic Approach for In-Context Learning(https://arxiv.org/abs/2410.10074)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) emerges as a key feature for Large Language Models (LLMs), allowing them to adapt to new tasks by leveraging task-specific examples without updating model parameters. However, ICL faces challenges with increasing numbers of examples due to performance degradation and quadratic computational costs. In this paper, we propose Logit Arithmetic Reweighting Approach (LARA), a novel framework that enhances ICL by using logit-based ensembling of multiple demonstrations. Our approach divides long input demonstrations into parallelizable shorter inputs to significantly reduce memory requirements, and then effectively aggregate the information by reweighting logits of each group via a non-gradient optimization approach. We further introduce Binary LARA (B-LARA), a variant that constrains weights to binary values to simplify the search space and reduces memory usage by filtering out less informative demonstration groups. Experiments on BBH and MMLU demonstrate that LARA and B-LARA outperform all baseline methods in both accuracy and memory efficiency. We also conduct extensive analysis to show that LARA generalizes well to scenarios of varying numbers of examples from limited to many-shot demonstrations.</li>
</ul>

<h3>Title: High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity</h3>
<ul>
<li><strong>Authors: </strong>Qian Yu, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Lihe Zhang, Huchuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10105">https://arxiv.org/abs/2410.10105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10105">https://arxiv.org/pdf/2410.10105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10105]] High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity(https://arxiv.org/abs/2410.10105)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the realm of high-resolution (HR), fine-grained image segmentation, the primary challenge is balancing broad contextual awareness with the precision required for detailed object delineation, capturing intricate details and the finest edges of objects. Diffusion models, trained on vast datasets comprising billions of image-text pairs, such as SD V2.1, have revolutionized text-to-image synthesis by delivering exceptional quality, fine detail resolution, and strong contextual awareness, making them an attractive solution for high-resolution image segmentation. To this end, we propose DiffDIS, a diffusion-driven segmentation model that taps into the potential of the pre-trained U-Net within diffusion models, specifically designed for high-resolution, fine-grained object segmentation. By leveraging the robust generalization capabilities and rich, versatile image representation prior of the SD models, coupled with a task-specific stable one-step denoising approach, we significantly reduce the inference time while preserving high-fidelity, detailed generation. Additionally, we introduce an auxiliary edge generation task to not only enhance the preservation of fine details of the object boundaries, but reconcile the probabilistic nature of diffusion with the deterministic demands of segmentation. With these refined strategies in place, DiffDIS serves as a rapid object mask generation model, specifically optimized for generating detailed binary maps at high resolutions, while demonstrating impressive accuracy and swift processing. Experiments on the DIS5K dataset demonstrate the superiority of DiffDIS, achieving state-of-the-art results through a streamlined inference process. Our code will be made publicly available.</li>
</ul>

<h3>Title: TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control</h3>
<ul>
<li><strong>Authors: </strong>Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10133">https://arxiv.org/abs/2410.10133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10133">https://arxiv.org/pdf/2410.10133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10133]] TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control(https://arxiv.org/abs/2410.10133)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Centred on content modification and style preservation, Scene Text Editing (STE) remains a challenging task despite considerable progress in text-to-image synthesis and text-driven image manipulation recently. GAN-based STE methods generally encounter a common issue of model generalization, while Diffusion-based STE methods suffer from undesired style deviations. To address these problems, we propose TextCtrl, a diffusion-based method that edits text with prior guidance control. Our method consists of two key components: (i) By constructing fine-grained text style disentanglement and robust text glyph structure representation, TextCtrl explicitly incorporates Style-Structure guidance into model design and network training, significantly improving text style consistency and rendering accuracy. (ii) To further leverage the style prior, a Glyph-adaptive Mutual Self-attention mechanism is proposed which deconstructs the implicit fine-grained features of the source image to enhance style consistency and vision quality during inference. Furthermore, to fill the vacancy of the real-world STE evaluation benchmark, we create the first real-world image-pair dataset termed ScenePair for fair comparisons. Experiments demonstrate the effectiveness of TextCtrl compared with previous methods concerning both style fidelity and text accuracy.</li>
</ul>

<h3>Title: Will the Inclusion of Generated Data Amplify Bias Across Generations in Future Image Classification Models?</h3>
<ul>
<li><strong>Authors: </strong>Zeliang Zhang, Xin Liang, Mingqian Feng, Susan Liang, Chenliang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10160">https://arxiv.org/abs/2410.10160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10160">https://arxiv.org/pdf/2410.10160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10160]] Will the Inclusion of Generated Data Amplify Bias Across Generations in Future Image Classification Models?(https://arxiv.org/abs/2410.10160)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As the demand for high-quality training data escalates, researchers have increasingly turned to generative models to create synthetic data, addressing data scarcity and enabling continuous model improvement. However, reliance on self-generated data introduces a critical question: Will this practice amplify bias in future models? While most research has focused on overall performance, the impact on model bias, particularly subgroup bias, remains underexplored. In this work, we investigate the effects of the generated data on image classification tasks, with a specific focus on bias. We develop a practical simulation environment that integrates a self-consuming loop, where the generative model and classification model are trained synergistically. Hundreds of experiments are conducted on Colorized MNIST, CIFAR-20/100, and Hard ImageNet datasets to reveal changes in fairness metrics across generations. In addition, we provide a conjecture to explain the bias dynamics when training models on continuously augmented datasets across generations. Our findings contribute to the ongoing debate on the implications of synthetic data for fairness in real-world applications.</li>
</ul>

<h3>Title: Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yongjin Yang, Sihyeon Kim, Hojung Jung, Sangmin Bae, SangMook Kim, Se-Young Yun, Kimin Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10166">https://arxiv.org/abs/2410.10166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10166">https://arxiv.org/pdf/2410.10166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10166]] Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models(https://arxiv.org/abs/2410.10166)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Fine-tuning text-to-image diffusion models with human feedback is an effective method for aligning model behavior with human intentions. However, this alignment process often suffers from slow convergence due to the large size and noise present in human feedback datasets. In this work, we propose FiFA, a novel automated data filtering algorithm designed to enhance the fine-tuning of diffusion models using human feedback datasets with direct preference optimization (DPO). Specifically, our approach selects data by solving an optimization problem to maximize three components: preference margin, text quality, and text diversity. The concept of preference margin is used to identify samples that contain high informational value to address the noisy nature of feedback dataset, which is calculated using a proxy reward model. Additionally, we incorporate text quality, assessed by large language models to prevent harmful contents, and consider text diversity through a k-nearest neighbor entropy estimator to improve generalization. Finally, we integrate all these components into an optimization process, with approximating the solution by assigning importance score to each data pair and selecting the most important ones. As a result, our method efficiently filters data automatically, without the need for manual intervention, and can be applied to any large-scale dataset. Experimental results show that FiFA significantly enhances training stability and achieves better performance, being preferred by humans 17% more, while using less than 0.5% of the full data and thus 1% of the GPU hours compared to utilizing full human feedback datasets.</li>
</ul>

<h3>Title: X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing</h3>
<ul>
<li><strong>Authors: </strong>Xinyan Chen, Jianfei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10167">https://arxiv.org/abs/2410.10167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10167">https://arxiv.org/pdf/2410.10167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10167]] X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing(https://arxiv.org/abs/2410.10167)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Human sensing, which employs various sensors and advanced deep learning technologies to accurately capture and interpret human body information, has significantly impacted fields like public security and robotics. However, current human sensing primarily depends on modalities such as cameras and LiDAR, each of which has its own strengths and limitations. Furthermore, existing multi-modal fusion solutions are typically designed for fixed modality combinations, requiring extensive retraining when modalities are added or removed for diverse scenarios. In this paper, we propose a modality-invariant foundation model for all modalities, X-Fi, to address this issue. X-Fi enables the independent or combinatory use of sensor modalities without additional training by utilizing a transformer structure to accommodate variable input sizes and incorporating a novel "X-fusion" mechanism to preserve modality-specific features during multimodal integration. This approach not only enhances adaptability but also facilitates the learning of complementary features across modalities. Extensive experiments conducted on the MM-Fi and XRF55 datasets, employing six distinct modalities, demonstrate that X-Fi achieves state-of-the-art performance in human pose estimation (HPE) and human activity recognition (HAR) tasks. The findings indicate that our proposed model can efficiently support a wide range of human sensing applications, ultimately contributing to the evolution of scalable, multimodal sensing technologies.</li>
</ul>

<h3>Title: First Creating Backgrounds Then Rendering Texts: A New Paradigm for Visual Text Blending</h3>
<ul>
<li><strong>Authors: </strong>Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10168">https://arxiv.org/abs/2410.10168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10168">https://arxiv.org/pdf/2410.10168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10168]] First Creating Backgrounds Then Rendering Texts: A New Paradigm for Visual Text Blending(https://arxiv.org/abs/2410.10168)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models, known for their impressive image generation abilities, have played a pivotal role in the rise of visual text generation. Nevertheless, existing visual text generation methods often focus on generating entire images with text prompts, leading to imprecise control and limited practicality. A more promising direction is visual text blending, which focuses on seamlessly merging texts onto text-free backgrounds. However, existing visual text blending methods often struggle to generate high-fidelity and diverse images due to a shortage of backgrounds for synthesis and limited generalization capabilities. To overcome these challenges, we propose a new visual text blending paradigm including both creating backgrounds and rendering texts. Specifically, a background generator is developed to produce high-fidelity and text-free natural images. Moreover, a text renderer named GlyphOnly is designed for achieving visually plausible text-background integration. GlyphOnly, built on a Stable Diffusion framework, utilizes glyphs and backgrounds as conditions for accurate rendering and consistency control, as well as equipped with an adaptive text block exploration strategy for small-scale text rendering. We also explore several downstream applications based on our method, including scene text dataset synthesis for boosting scene text detectors, as well as text image customization and editing. Code and model will be available at \url{this https URL}.</li>
</ul>

<h3>Title: Identity-Focused Inference and Extraction Attacks on Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jayneel Vora, Aditya Krishnan, Nader Bouacida, Prabhu RV Shankar, Prasant Mohapatra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10177">https://arxiv.org/abs/2410.10177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10177">https://arxiv.org/pdf/2410.10177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10177]] Identity-Focused Inference and Extraction Attacks on Diffusion Models(https://arxiv.org/abs/2410.10177)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The increasing reliance on diffusion models for generating synthetic images has amplified concerns about the unauthorized use of personal data, particularly facial images, in model training. In this paper, we introduce a novel identity inference framework to hold model owners accountable for including individuals' identities in their training data. Our approach moves beyond traditional membership inference attacks by focusing on identity-level inference, providing a new perspective on data privacy violations. Through comprehensive evaluations on two facial image datasets, Labeled Faces in the Wild (LFW) and CelebA, our experiments demonstrate that the proposed membership inference attack surpasses baseline methods, achieving an attack success rate of up to 89% and an AUC-ROC of 0.91, while the identity inference attack attains 92% on LDM models trained on LFW, and the data extraction attack achieves 91.6% accuracy on DDPMs, validating the effectiveness of our approach across diffusion models.</li>
</ul>

<h3>Title: GUISE: Graph GaUssIan Shading watErmark</h3>
<ul>
<li><strong>Authors: </strong>Renyi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10178">https://arxiv.org/abs/2410.10178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10178">https://arxiv.org/pdf/2410.10178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10178]] GUISE: Graph GaUssIan Shading watErmark(https://arxiv.org/abs/2410.10178)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In the expanding field of generative artificial intelligence, integrating robust watermarking technologies is essential to protect intellectual property and maintain content authenticity. Traditionally, watermarking techniques have been developed primarily for rich information media such as images and audio. However, these methods have not been adequately adapted for graph-based data, particularly molecular graphs. Latent 3D graph diffusion(LDM-3DG) is an ascendant approach in the molecular graph generation field. This model effectively manages the complexities of molecular structures, preserving essential symmetries and topological features. We adapt the Gaussian Shading, a proven performance lossless watermarking technique, to the latent graph diffusion domain to protect this sophisticated new technology. Our adaptation simplifies the watermark diffusion process through duplication and padding, making it adaptable and suitable for various message types. We conduct several experiments using the LDM-3DG model on publicly available datasets QM9 and Drugs, to assess the robustness and effectiveness of our technique. Our results demonstrate that the watermarked molecules maintain statistical parity in 9 out of 10 performance metrics compared to the original. Moreover, they exhibit a 100% detection rate and a 99% extraction rate in a 2D decoded pipeline, while also showing robustness against post-editing attacks.</li>
</ul>

<h3>Title: Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Yan, Jiawei Wu, Rushi Shah, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10180">https://arxiv.org/abs/2410.10180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10180">https://arxiv.org/pdf/2410.10180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10180]] Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior(https://arxiv.org/abs/2410.10180)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.</li>
</ul>

<h3>Title: Predicting from Strings: Language Model Embeddings for Bayesian Optimization</h3>
<ul>
<li><strong>Authors: </strong>Tung Nguyen, Qiuyi Zhang, Bangding Yang, Chansoo Lee, Jorg Bornschein, Yingjie Miao, Sagi Perel, Yutian Chen, Xingyou Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10190">https://arxiv.org/abs/2410.10190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10190">https://arxiv.org/pdf/2410.10190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10190]] Predicting from Strings: Language Model Embeddings for Bayesian Optimization(https://arxiv.org/abs/2410.10190)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Bayesian Optimization is ubiquitous in the field of experimental design and blackbox optimization for improving search efficiency, but has been traditionally restricted to regression models which are only applicable to fixed search spaces and tabular input features. We propose Embed-then-Regress, a paradigm for applying in-context regression over string inputs, through the use of string embedding capabilities of pretrained language models. By expressing all inputs as strings, we are able to perform general-purpose regression for Bayesian Optimization over various domains including synthetic, combinatorial, and hyperparameter optimization, obtaining comparable results to state-of-the-art Gaussian Process-based algorithms. Code can be found at this http URL.</li>
</ul>

<h3>Title: Fed-piLot: Optimizing LoRA Assignment for Efficient Federated Foundation Model Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Zikai Zhang, Jiahao Xu, Ping Liu, Rui Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10200">https://arxiv.org/abs/2410.10200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10200">https://arxiv.org/pdf/2410.10200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10200]] Fed-piLot: Optimizing LoRA Assignment for Efficient Federated Foundation Model Fine-Tuning(https://arxiv.org/abs/2410.10200)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) have shown remarkable advancements in enhancing the performance of intelligent applications. To address the need for data privacy in FM fine-tuning, federated learning has emerged as the de facto framework. Specifically, Federated FMs (FedFMs) fine-tuning using low-rank adaptation (LoRA) modules instead of the full model over multiple clients can achieve both parameter efficiency and data privacy. However, recent studies rarely address the challenges posed by clients with heterogeneous resources, particularly in GPU memory capacity. In this paper, we introduce Fed-piLot, an efficient FedFM fine-tuning framework with optimized local LoRA assignments for heterogeneous clients. By emphasizing the different memory consumption for training different LoRA layers, as well as the varying contributions of different layers to model performance, we formulate the LoRA assignment as a Knapsack Optimization Problem. We design a Local-Global Information Gain Score (IG-Score) based value function to optimize LoRA assignment under clients' memory constraints. To further mitigate the impact of heterogeneity in model updates, we propose a novel Spatial-Temporal model aggregation (STAgg) rule using the Dynamic Weight Adjustment (DWA) strategy. Experimental results on three datasets under both IID and non-IID conditions demonstrate the effectiveness and efficiency of Fed-piLot. The code will be publicly available.</li>
</ul>

<h3>Title: MagicEraser: Erasing Any Objects via Semantics-Aware Control</h3>
<ul>
<li><strong>Authors: </strong>Fan Li, Zixiao Zhang, Yi Huang, Jianzhuang Liu, Renjing Pei, Bin Shao, Songcen Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10207">https://arxiv.org/abs/2410.10207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10207">https://arxiv.org/pdf/2410.10207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10207]] MagicEraser: Erasing Any Objects via Semantics-Aware Control(https://arxiv.org/abs/2410.10207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The traditional image inpainting task aims to restore corrupted regions by referencing surrounding background and foreground. However, the object erasure task, which is in increasing demand, aims to erase objects and generate harmonious background. Previous GAN-based inpainting methods struggle with intricate texture generation. Emerging diffusion model-based algorithms, such as Stable Diffusion Inpainting, exhibit the capability to generate novel content, but they often produce incongruent results at the locations of the erased objects and require high-quality text prompt inputs. To address these challenges, we introduce MagicEraser, a diffusion model-based framework tailored for the object erasure task. It consists of two phases: content initialization and controllable generation. In the latter phase, we develop two plug-and-play modules called prompt tuning and semantics-aware attention refocus. Additionally, we propose a data construction strategy that generates training data specially suitable for this task. MagicEraser achieves fine and effective control of content generation while mitigating undesired artifacts. Experimental results highlight a valuable advancement of our approach in the object erasure task.</li>
</ul>

<h3>Title: Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key</h3>
<ul>
<li><strong>Authors: </strong>Yingda Chen, Xingjun Wang, Jintao Huang, Yunlin Mao, Daoze Zhang, Yuze Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10210">https://arxiv.org/abs/2410.10210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10210">https://arxiv.org/pdf/2410.10210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10210]] Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key(https://arxiv.org/abs/2410.10210)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>As large language models rapidly evolve to support longer context, there is a notable disparity in their capability to generate output at greater lengths. Recent study suggests that the primary cause for this imbalance may arise from the lack of data with long-output during alignment training. In light of this observation, attempts are made to re-align foundation models with data that fills the gap, which result in models capable of generating lengthy output when instructed. In this paper, we explore the impact of data-quality in tuning a model for long output, and the possibility of doing so from the starting points of human-aligned (instruct or chat) models. With careful data curation, we show that it possible to achieve similar performance improvement in our tuned models, with only a small fraction of training data instances and compute. In addition, we assess the generalizability of such approaches by applying our tuning-recipes to several models. our findings suggest that, while capacities for generating long output vary across different models out-of-the-box, our approach to tune them with high-quality data using lite compute, consistently yields notable improvement across all models we experimented on. We have made public our curated dataset for tuning long-writing capability, the implementations of model tuning and evaluation, as well as the fine-tuned models, all of which can be openly-accessed.</li>
</ul>

<h3>Title: Detecting Unforeseen Data Properties with Diffusion Autoencoder Embeddings using Spine MRI data</h3>
<ul>
<li><strong>Authors: </strong>Robert Graf, Florian Hunecke, Soeren Pohl, Matan Atad, Hendrik Moeller, Sophie Starck, Thomas Kroencke, Stefanie Bette, Fabian Bamberg, Tobias Pischon, Thoralf Niendorf, Carsten Schmidt, Johannes C. Paetzold, Daniel Rueckert, Jan S Kirschke</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10220">https://arxiv.org/abs/2410.10220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10220">https://arxiv.org/pdf/2410.10220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10220]] Detecting Unforeseen Data Properties with Diffusion Autoencoder Embeddings using Spine MRI data(https://arxiv.org/abs/2410.10220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep learning has made significant strides in medical imaging, leveraging the use of large datasets to improve diagnostics and prognostics. However, large datasets often come with inherent errors through subject selection and acquisition. In this paper, we investigate the use of Diffusion Autoencoder (DAE) embeddings for uncovering and understanding data characteristics and biases, including biases for protected variables like sex and data abnormalities indicative of unwanted protocol variations. We use sagittal T2-weighted magnetic resonance (MR) images of the neck, chest, and lumbar region from 11186 German National Cohort (NAKO) participants. We compare DAE embeddings with existing generative models like StyleGAN and Variational Autoencoder. Evaluations on a large-scale dataset consisting of sagittal T2-weighted MR images of three spine regions show that DAE embeddings effectively separate protected variables such as sex and age. Furthermore, we used t-SNE visualization to identify unwanted variations in imaging protocols, revealing differences in head positioning. Our embedding can identify samples where a sex predictor will have issues learning the correct sex. Our findings highlight the potential of using advanced embedding techniques like DAEs to detect data quality issues and biases in medical imaging datasets. Identifying such hidden relations can enhance the reliability and fairness of deep learning models in healthcare applications, ultimately improving patient care and outcomes.</li>
</ul>

<h3>Title: LADMIM: Logical Anomaly Detection with Masked Image Modeling in Discrete Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Shunsuke Sakai, Tatushito Hasegawa, Makoto Koshino</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10234">https://arxiv.org/abs/2410.10234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10234">https://arxiv.org/pdf/2410.10234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10234]] LADMIM: Logical Anomaly Detection with Masked Image Modeling in Discrete Latent Space(https://arxiv.org/abs/2410.10234)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Detecting anomalies such as incorrect combinations of objects or deviations in their positions is a challenging problem in industrial anomaly detection. Traditional methods mainly focus on local features of normal images, such as scratches and dirt, making detecting anomalies in the relationships between features difficult. Masked image modeling(MIM) is a self-supervised learning technique that predicts the feature representation of masked regions in an image. To reconstruct the masked regions, it is necessary to understand how the image is composed, allowing the learning of relationships between features within the image. We propose a novel approach that leverages the characteristics of MIM to detect logical anomalies effectively. To address blurriness in the reconstructed image, we replace pixel prediction with predicting the probability distribution of discrete latent variables of the masked regions using a tokenizer. We evaluated the proposed method on the MVTecLOCO dataset, achieving an average AUC of 0.867, surpassing traditional reconstruction-based and distillation-based methods.</li>
</ul>

<h3>Title: Revisiting and Benchmarking Graph Autoencoders: A Contrastive Learning Perspective</h3>
<ul>
<li><strong>Authors: </strong>Jintang Li, Ruofan Wu, Yuchang Zhu, Huizhe Zhang, Xinzhou Jin, Guibin Zhang, Zulun Zhu, Zibin Zheng, Liang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10241">https://arxiv.org/abs/2410.10241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10241">https://arxiv.org/pdf/2410.10241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10241]] Revisiting and Benchmarking Graph Autoencoders: A Contrastive Learning Perspective(https://arxiv.org/abs/2410.10241)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph autoencoders (GAEs) are self-supervised learning models that can learn meaningful representations of graph-structured data by reconstructing the input graph from a low-dimensional latent space. Over the past few years, GAEs have gained significant attention in academia and industry. In particular, the recent advent of GAEs with masked autoencoding schemes marks a significant advancement in graph self-supervised learning research. While numerous GAEs have been proposed, the underlying mechanisms of GAEs are not well understood, and a comprehensive benchmark for GAEs is still lacking. In this work, we bridge the gap between GAEs and contrastive learning by establishing conceptual and methodological connections. We revisit the GAEs studied in previous works and demonstrate how contrastive learning principles can be applied to GAEs. Motivated by these insights, we introduce lrGAE (left-right GAE), a general and powerful GAE framework that leverages contrastive learning principles to learn meaningful representations. Our proposed lrGAE not only facilitates a deeper understanding of GAEs but also sets a new benchmark for GAEs across diverse graph-based learning tasks. The source code for lrGAE, including the baselines and all the code for reproducing the results, is publicly available at this https URL.</li>
</ul>

<h3>Title: Saliency Guided Optimization of Diffusion Latents</h3>
<ul>
<li><strong>Authors: </strong>Xiwen Wang, Jizhe Zhou, Xuekang Zhu, Cheng Li, Mao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10257">https://arxiv.org/abs/2410.10257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10257">https://arxiv.org/pdf/2410.10257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10257]] Saliency Guided Optimization of Diffusion Latents(https://arxiv.org/abs/2410.10257)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the rapid advances in diffusion models, generating decent images from text prompts is no longer challenging. The key to text-to-image generation is how to optimize the results of a text-to-image generation model so that they can be better aligned with human intentions or prompts. Existing optimization methods commonly treat the entire image uniformly and conduct global optimization. These methods overlook the fact that when viewing an image, the human visual system naturally prioritizes attention toward salient areas, often neglecting less or non-salient regions. That is, humans are likely to neglect optimizations in non-salient areas. Consequently, although model retaining is conducted under the guidance of additional large and multimodality models, existing methods, which perform uniform optimizations, yield sub-optimal results. To address this alignment challenge effectively and efficiently, we propose Saliency Guided Optimization Of Diffusion Latents (SGOOL). We first employ a saliency detector to mimic the human visual attention system and mark out the salient regions. To avoid retraining an additional model, our method directly optimizes the diffusion latents. Besides, SGOOL utilizes an invertible diffusion process and endows it with the merits of constant memory implementation. Hence, our method becomes a parameter-efficient and plug-and-play fine-tuning method. Extensive experiments have been done with several metrics and human evaluation. Experimental results demonstrate the superiority of SGOOL in image quality and prompt alignment.</li>
</ul>

<h3>Title: Fine-grained Abnormality Prompt Learning for Zero-shot Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiawen Zhu, Yew-Soon Ong, Chunhua Shen, Guansong Pang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10289">https://arxiv.org/abs/2410.10289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10289">https://arxiv.org/pdf/2410.10289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10289]] Fine-grained Abnormality Prompt Learning for Zero-shot Anomaly Detection(https://arxiv.org/abs/2410.10289)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Current zero-shot anomaly detection (ZSAD) methods show remarkable success in prompting large pre-trained vision-language models to detect anomalies in a target dataset without using any dataset-specific training or demonstration. However, these methods are often focused on crafting/learning prompts that capture only coarse-grained semantics of abnormality, e.g., high-level semantics like "damaged", "imperfect", or "defective" on carpet. They therefore have limited capability in recognizing diverse abnormality details with distinctive visual appearance, e.g., specific defect types like color stains, cuts, holes, and threads on carpet. To address this limitation, we propose FAPrompt, a novel framework designed to learn Fine-grained Abnormality Prompts for more accurate ZSAD. To this end, we introduce a novel compound abnormality prompting module in FAPrompt to learn a set of complementary, decomposed abnormality prompts, where each abnormality prompt is formed by a compound of shared normal tokens and a few learnable abnormal tokens. On the other hand, the fine-grained abnormality patterns can be very different from one dataset to another. To enhance their cross-dataset generalization, we further introduce a data-dependent abnormality prior module that learns to derive abnormality features from each query/test image as a sample-wise abnormality prior to ground the abnormality prompts in a given target dataset. Comprehensive experiments conducted across 19 real-world datasets, covering both industrial defects and medical anomalies, demonstrate that FAPrompt substantially outperforms state-of-the-art methods by at least 3%-5% AUC/AP in both image- and pixel-level ZSAD tasks. Code is available at this https URL.</li>
</ul>

<h3>Title: MentalGLM Series: Explainable Large Language Models for Mental Health Analysis on Chinese Social Media</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhai, Nan Bai, Qing Zhao, Jianqiang Li, Fan Wang, Hongzhi Qi, Meng Jiang, Xiaoqin Wang, Bing Xiang Yang, Guanghui Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10323">https://arxiv.org/abs/2410.10323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10323">https://arxiv.org/pdf/2410.10323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10323]] MentalGLM Series: Explainable Large Language Models for Mental Health Analysis on Chinese Social Media(https://arxiv.org/abs/2410.10323)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As the prevalence of mental health challenges, social media has emerged as a key platform for individuals to express their this http URL learning tends to be a promising solution for analyzing mental health on social media. However, black box models are often inflexible when switching between tasks, and their results typically lack explanations. With the rise of large language models (LLMs), their flexibility has introduced new approaches to the field. Also due to the generative nature, they can be prompted to explain decision-making processes. However, their performance on complex psychological analysis still lags behind deep learning. In this paper, we introduce the first multi-task Chinese Social Media Interpretable Mental Health Instructions (C-IMHI) dataset, consisting of 9K samples, which has been quality-controlled and manually validated. We also propose MentalGLM series models, the first open-source LLMs designed for explainable mental health analysis targeting Chinese social media, trained on a corpus of 50K instructions. The proposed models were evaluated on three downstream tasks and achieved better or comparable performance compared to deep learning models, generalized LLMs, and task fine-tuned LLMs. We validated a portion of the generated decision explanations with experts, showing promising results. We also evaluated the proposed models on a clinical dataset, where they outperformed other LLMs, indicating their potential applicability in the clinical field. Our models show strong performance, validated across tasks and perspectives. The decision explanations enhance usability and facilitate better understanding and practical application of the models. Both the constructed dataset and the models are publicly available via: this https URL.</li>
</ul>

<h3>Title: GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs</h3>
<ul>
<li><strong>Authors: </strong>Yun Zhu, Haizhou Shi, Xiaotang Wang, Yongchao Liu, Yaoke Wang, Boci Peng, Chuntao Hong, Siliang Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10329">https://arxiv.org/abs/2410.10329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10329">https://arxiv.org/pdf/2410.10329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10329]] GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs(https://arxiv.org/abs/2410.10329)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Recently, research on Text-Attributed Graphs (TAGs) has gained significant attention due to the prevalence of free-text node features in real-world applications and the advancements in Large Language Models (LLMs) that bolster TAG methodologies. However, current TAG approaches face two primary challenges: (i) Heavy reliance on label information and (ii) Limited cross-domain zero/few-shot transferability. These issues constrain the scaling of both data and model size, owing to high labor costs and scaling laws, complicating the development of graph foundation models with strong transferability. In this work, we propose the GraphCLIP framework to address these challenges by learning graph foundation models with strong cross-domain zero/few-shot transferability through a self-supervised contrastive graph-summary pretraining method. Specifically, we generate and curate large-scale graph-summary pair data with the assistance of LLMs, and introduce a novel graph-summary pretraining method, combined with invariant learning, to enhance graph foundation models with strong cross-domain zero-shot transferability. For few-shot learning, we propose a novel graph prompt tuning technique aligned with our pretraining objective to mitigate catastrophic forgetting and minimize learning costs. Extensive experiments show the superiority of GraphCLIP in both zero-shot and few-shot settings, while evaluations across various downstream tasks confirm the versatility of GraphCLIP. Our code is available at: this https URL</li>
</ul>

<h3>Title: Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and Refinement</h3>
<ul>
<li><strong>Authors: </strong>Joseph Shtok, Amit Alfassy, Foad Abo Dahood, Eliyahu Schwartz, Sivan Doveh, Assaf Arbelle</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10348">https://arxiv.org/abs/2410.10348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10348">https://arxiv.org/pdf/2410.10348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10348]] Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and Refinement(https://arxiv.org/abs/2410.10348)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>It has been shown that Large Language Models' (LLMs) performance can be improved for many tasks using Chain of Thought (CoT) or In-Context Learning (ICL), which involve demonstrating the steps needed to solve a task using a few examples. However, while datasets with input-output pairs are relatively easy to produce, providing demonstrations which include intermediate steps requires cumbersome manual work. These steps may be executable programs, as in agentic flows, or step-by-step reasoning as in CoT. In this work, we propose Automatic Data Labeling and Refinement (ADLR), a method to automatically generate and filter demonstrations which include the above intermediate steps, starting from a small seed of manually crafted examples. We demonstrate the advantage of ADLR in code-based table QA and mathematical reasoning, achieving up to a 5.5% gain. The code implementing our method is provided in the Supplementary material and will be made available.</li>
</ul>

<h3>Title: FasterDiT: Towards Faster Diffusion Transformers Training without Architecture Modification</h3>
<ul>
<li><strong>Authors: </strong>Jingfeng Yao, Wang Cheng, Wenyu Liu, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10356">https://arxiv.org/abs/2410.10356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10356">https://arxiv.org/pdf/2410.10356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10356]] FasterDiT: Towards Faster Diffusion Transformers Training without Architecture Modification(https://arxiv.org/abs/2410.10356)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiT) have attracted significant attention in research. However, they suffer from a slow convergence rate. In this paper, we aim to accelerate DiT training without any architectural modification. We identify the following issues in the training process: firstly, certain training strategies do not consistently perform well across different data. Secondly, the effectiveness of supervision at specific timesteps is limited. In response, we propose the following contributions: (1) We introduce a new perspective for interpreting the failure of the strategies. Specifically, we slightly extend the definition of Signal-to-Noise Ratio (SNR) and suggest observing the Probability Density Function (PDF) of SNR to understand the essence of the data robustness of the strategy. (2) We conduct numerous experiments and report over one hundred experimental results to empirically summarize a unified accelerating strategy from the perspective of PDF. (3) We develop a new supervision method that further accelerates the training process of DiT. Based on them, we propose FasterDiT, an exceedingly simple and practicable design strategy. With few lines of code modifications, it achieves 2.30 FID on ImageNet 256 resolution at 1000k iterations, which is comparable to DiT (2.27 FID) but 7 times faster in training.</li>
</ul>

<h3>Title: SpeGCL: Self-supervised Graph Spectrum Contrastive Learning without Positive Samples</h3>
<ul>
<li><strong>Authors: </strong>Yuntao Shou, Xiangyong Cao, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10365">https://arxiv.org/abs/2410.10365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10365">https://arxiv.org/pdf/2410.10365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10365]] SpeGCL: Self-supervised Graph Spectrum Contrastive Learning without Positive Samples(https://arxiv.org/abs/2410.10365)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Graph Contrastive Learning (GCL) excels at managing noise and fluctuations in input data, making it popular in various fields (e.g., social networks, and knowledge graphs). Our study finds that the difference in high-frequency information between augmented graphs is greater than that in low-frequency information. However, most existing GCL methods focus mainly on the time domain (low-frequency information) for node feature representations and cannot make good use of high-frequency information to speed up model convergence. Furthermore, existing GCL paradigms optimize graph embedding representations by pulling the distance between positive sample pairs closer and pushing the distance between positive and negative sample pairs farther away, but our theoretical analysis shows that graph contrastive learning benefits from pushing negative pairs farther away rather than pulling positive pairs closer. To solve the above-mentioned problems, we propose a novel spectral GCL framework without positive samples, named SpeGCL. Specifically, to solve the problem that existing GCL methods cannot utilize high-frequency information, SpeGCL uses a Fourier transform to extract high-frequency and low-frequency information of node features, and constructs a contrastive learning mechanism in a Fourier space to obtain better node feature representation. Furthermore, SpeGCL relies entirely on negative samples to refine the graph embedding. We also provide a theoretical justification for the efficacy of using only negative samples in SpeGCL. Extensive experiments on un-supervised learning, transfer learning, and semi-supervised learning have validated the superiority of our SpeGCL framework over the state-of-the-art GCL methods.</li>
</ul>

<h3>Title: GIFT-Eval: A Benchmark For General Time Series Forecasting Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Taha Aksu, Gerald Woo, Juncheng Liu, Xu Liu, Chenghao Liu, Silvio Savarese, Caiming Xiong, Doyen Sahoo</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10393">https://arxiv.org/abs/2410.10393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10393">https://arxiv.org/pdf/2410.10393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10393]] GIFT-Eval: A Benchmark For General Time Series Forecasting Model Evaluation(https://arxiv.org/abs/2410.10393)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series foundation models excel in zero-shot forecasting, handling diverse tasks without explicit training. However, the advancement of these models has been hindered by the lack of comprehensive benchmarks. To address this gap, we introduce the General Time Series Forecasting Model Evaluation, GIFT-Eval, a pioneering benchmark aimed at promoting evaluation across diverse datasets. GIFT-Eval encompasses 28 datasets over 144,000 time series and 177 million data points, spanning seven domains, 10 frequencies, multivariate inputs, and prediction lengths ranging from short to long-term forecasts. To facilitate the effective pretraining and evaluation of foundation models, we also provide a non-leaking pretraining dataset containing approximately 230 billion data points. Additionally, we provide a comprehensive analysis of 17 baselines, which includes statistical models, deep learning models, and foundation models. We discuss each model in the context of various benchmark characteristics and offer a qualitative analysis that spans both deep learning and foundation models. We believe the insights from this analysis, along with access to this new standard zero-shot time series forecasting benchmark, will guide future developments in time series foundation models. The codebase, datasets, and a leaderboard showing all the results in detail will be available soon.</li>
</ul>

<h3>Title: DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model</h3>
<ul>
<li><strong>Authors: </strong>Songen Gu, Wei Yin, Bu Jin, Xiaoyang Guo, Junming Wang, Haodong Li, Qian Zhang, Xiaoxiao Long</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10429">https://arxiv.org/abs/2410.10429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10429">https://arxiv.org/pdf/2410.10429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10429]] DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model(https://arxiv.org/abs/2410.10429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose DOME, a diffusion-based world model that predicts future occupancy frames based on past occupancy observations. The ability of this world model to capture the evolution of the environment is crucial for planning in autonomous driving. Compared to 2D video-based world models, the occupancy world model utilizes a native 3D representation, which features easily obtainable annotations and is modality-agnostic. This flexibility has the potential to facilitate the development of more advanced world models. Existing occupancy world models either suffer from detail loss due to discrete tokenization or rely on simplistic diffusion architectures, leading to inefficiencies and difficulties in predicting future occupancy with controllability. Our DOME exhibits two key features:(1) High-Fidelity and Long-Duration Generation. We adopt a spatial-temporal diffusion transformer to predict future occupancy frames based on historical context. This architecture efficiently captures spatial-temporal information, enabling high-fidelity details and the ability to generate predictions over long durations. (2)Fine-grained Controllability. We address the challenge of controllability in predictions by introducing a trajectory resampling method, which significantly enhances the model's ability to generate controlled predictions. Extensive experiments on the widely used nuScenes dataset demonstrate that our method surpasses existing baselines in both qualitative and quantitative evaluations, establishing a new state-of-the-art performance on nuScenes. Specifically, our approach surpasses the baseline by 10.5% in mIoU and 21.2% in IoU for occupancy reconstruction and by 36.0% in mIoU and 24.6% in IoU for 4D occupancy forecasting.</li>
</ul>

<h3>Title: Diversity-Aware Reinforcement Learning for de novo Drug Design</h3>
<ul>
<li><strong>Authors: </strong>Hampus Gummesson Svensson, Christian Tyrchan, Ola Engkvist, Morteza Haghir Chehreghani</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10431">https://arxiv.org/abs/2410.10431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10431">https://arxiv.org/pdf/2410.10431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10431]] Diversity-Aware Reinforcement Learning for de novo Drug Design(https://arxiv.org/abs/2410.10431)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning a pre-trained generative model has demonstrated good performance in generating promising drug molecules. The fine-tuning task is often formulated as a reinforcement learning problem, where previous methods efficiently learn to optimize a reward function to generate potential drug molecules. Nevertheless, in the absence of an adaptive update mechanism for the reward function, the optimization process can become stuck in local optima. The efficacy of the optimal molecule in a local optimization may not translate to usefulness in the subsequent drug optimization process or as a potential standalone clinical candidate. Therefore, it is important to generate a diverse set of promising molecules. Prior work has modified the reward function by penalizing structurally similar molecules, primarily focusing on finding molecules with higher rewards. To date, no study has comprehensively examined how different adaptive update mechanisms for the reward function influence the diversity of generated molecules. In this work, we investigate a wide range of intrinsic motivation methods and strategies to penalize the extrinsic reward, and how they affect the diversity of the set of generated molecules. Our experiments reveal that combining structure- and prediction-based methods generally yields better results in terms of molecular diversity.</li>
</ul>

<h3>Title: Self-Assessed Generation: Trustworthy Label Generation for Optical Flow and Stereo Matching in Real-world</h3>
<ul>
<li><strong>Authors: </strong>Han Ling, Yinghui Sun, Quansen Sun, Ivor Tsang, Yuhui Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10453">https://arxiv.org/abs/2410.10453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10453">https://arxiv.org/pdf/2410.10453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10453]] Self-Assessed Generation: Trustworthy Label Generation for Optical Flow and Stereo Matching in Real-world(https://arxiv.org/abs/2410.10453)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>A significant challenge facing current optical flow and stereo methods is the difficulty in generalizing them well to the real world. This is mainly due to the high costs required to produce datasets, and the limitations of existing self-supervised methods on fuzzy results and complex model training problems. To address the above challenges, we propose a unified self-supervised generalization framework for optical flow and stereo tasks: Self-Assessed Generation (SAG). Unlike previous self-supervised methods, SAG is data-driven, using advanced reconstruction techniques to construct a reconstruction field from RGB images and generate datasets based on it. Afterward, we quantified the confidence level of the generated results from multiple perspectives, such as reconstruction field distribution, geometric consistency, and structural similarity, to eliminate inevitable defects in the generation process. We also designed a 3D flight foreground automatic rendering pipeline in SAG to encourage the network to learn occlusion and motion foreground. Experimentally, because SAG does not involve changes to methods or loss functions, it can directly self-supervised train the state-of-the-art deep networks, greatly improving the generalization performance of self-supervised methods on current mainstream optical flow and stereo-matching datasets. Compared to previous training modes, SAG is more generalized, cost-effective, and accurate.</li>
</ul>

<h3>Title: Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Xu Liu, Juncheng Liu, Gerald Woo, Taha Aksu, Yuxuan Liang, Roger Zimmermann, Chenghao Liu, Silvio Savarese, Caiming Xiong, Doyen Sahoo</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10469">https://arxiv.org/abs/2410.10469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10469">https://arxiv.org/pdf/2410.10469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10469]] Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts(https://arxiv.org/abs/2410.10469)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series foundation models have demonstrated impressive performance as zero-shot forecasters. However, achieving effectively unified training on time series remains an open challenge. Existing approaches introduce some level of model specialization to account for the highly heterogeneous nature of time series data. For instance, Moirai pursues unified training by employing multiple input/output projection layers, each tailored to handle time series at a specific frequency. Similarly, TimesFM maintains a frequency embedding dictionary for this purpose. We identify two major drawbacks to this human-imposed frequency-level model specialization: (1) Frequency is not a reliable indicator of the underlying patterns in time series. For example, time series with different frequencies can display similar patterns, while those with the same frequency may exhibit varied patterns. (2) Non-stationarity is an inherent property of real-world time series, leading to varied distributions even within a short context window of a single time series. Frequency-level specialization is too coarse-grained to capture this level of diversity. To address these limitations, this paper introduces Moirai-MoE, using a single input/output projection layer while delegating the modeling of diverse time series patterns to the sparse mixture of experts (MoE) within Transformers. With these designs, Moirai-MoE reduces reliance on human-defined heuristics and enables automatic token-level specialization. Extensive experiments on 39 datasets demonstrate the superiority of Moirai-MoE over existing foundation models in both in-distribution and zero-shot scenarios. Furthermore, this study conducts comprehensive model analyses to explore the inner workings of time series MoE foundation models and provides valuable insights for future research.</li>
</ul>

<h3>Title: Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Roccabruna, Massimo Rizzoli, Giuseppe Riccardi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10476">https://arxiv.org/abs/2410.10476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10476">https://arxiv.org/pdf/2410.10476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10476]] Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?(https://arxiv.org/abs/2410.10476)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The automatic detection of temporal relations among events has been mainly investigated with encoder-only models such as RoBERTa. Large Language Models (LLM) have recently shown promising performance in temporal reasoning tasks such as temporal question answering. Nevertheless, recent studies have tested the LLMs' performance in detecting temporal relations of closed-source models only, limiting the interpretability of those results. In this work, we investigate LLMs' performance and decision process in the Temporal Relation Classification task. First, we assess the performance of seven open and closed-sourced LLMs experimenting with in-context learning and lightweight fine-tuning approaches. Results show that LLMs with in-context learning significantly underperform smaller encoder-only models based on RoBERTa. Then, we delve into the possible reasons for this gap by applying explainable methods. The outcome suggests a limitation of LLMs in this task due to their autoregressive nature, which causes them to focus only on the last part of the sequence. Additionally, we evaluate the word embeddings of these two models to better understand their pre-training differences. The code and the fine-tuned models can be found respectively on GitHub.</li>
</ul>

<h3>Title: Vision-guided and Mask-enhanced Adaptive Denoising for Prompt-based Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Kejie Wang, Xuemeng Song, Meng Liu, Weili Guan, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10496">https://arxiv.org/abs/2410.10496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10496">https://arxiv.org/pdf/2410.10496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10496]] Vision-guided and Mask-enhanced Adaptive Denoising for Prompt-based Image Editing(https://arxiv.org/abs/2410.10496)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have demonstrated remarkable progress in synthesizing high-quality images from text prompts, which boosts researches on prompt-based image editing that edits a source image according to a target prompt. Despite their advances, existing methods still encounter three key issues: 1) limited capacity of the text prompt in guiding target image generation, 2) insufficient mining of word-to-patch and patch-to-patch relationships for grounding editing areas, and 3) unified editing strength for all regions during each denoising step. To address these issues, we present a Vision-guided and Mask-enhanced Adaptive Editing (ViMAEdit) method with three key novel designs. First, we propose to leverage image embeddings as explicit guidance to enhance the conventional textual prompt-based denoising process, where a CLIP-based target image embedding estimation strategy is introduced. Second, we devise a self-attention-guided iterative editing area grounding strategy, which iteratively exploits patch-to-patch relationships conveyed by self-attention maps to refine those word-to-patch relationships contained in cross-attention maps. Last, we present a spatially adaptive variance-guided sampling, which highlights sampling variances for critical image regions to promote the editing capability. Experimental results demonstrate the superior editing capacity of ViMAEdit over all existing methods.</li>
</ul>

<h3>Title: Continual Learning Improves Zero-Shot Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shreyank N Gowda, Davide Moltisanti, Laura Sevilla-Lara</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10497">https://arxiv.org/abs/2410.10497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10497">https://arxiv.org/pdf/2410.10497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10497]] Continual Learning Improves Zero-Shot Action Recognition(https://arxiv.org/abs/2410.10497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Zero-shot action recognition requires a strong ability to generalize from pre-training and seen classes to novel unseen classes. Similarly, continual learning aims to develop models that can generalize effectively and learn new tasks without forgetting the ones previously learned. The generalization goals of zero-shot and continual learning are closely aligned, however techniques from continual learning have not been applied to zero-shot action recognition. In this paper, we propose a novel method based on continual learning to address zero-shot action recognition. This model, which we call {\em Generative Iterative Learning} (GIL) uses a memory of synthesized features of past classes, and combines these synthetic features with real ones from novel classes. The memory is used to train a classification model, ensuring a balanced exposure to both old and new classes. Experiments demonstrate that {\em GIL} improves generalization in unseen classes, achieving a new state-of-the-art in zero-shot recognition across multiple benchmarks. Importantly, {\em GIL} also boosts performance in the more challenging generalized zero-shot setting, where models need to retain knowledge about classes seen before fine-tuning.</li>
</ul>

<h3>Title: UniGEM: A Unified Approach to Generation and Property Prediction for Molecules</h3>
<ul>
<li><strong>Authors: </strong>Shikun Feng, Yuyan Ni, Yan Lu, Zhi-Ming Ma, Wei-Ying Ma, Yanyan Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10516">https://arxiv.org/abs/2410.10516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10516">https://arxiv.org/pdf/2410.10516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10516]] UniGEM: A Unified Approach to Generation and Property Prediction for Molecules(https://arxiv.org/abs/2410.10516)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Molecular generation and molecular property prediction are both crucial for drug discovery, but they are often developed independently. Inspired by recent studies, which demonstrate that diffusion model, a prominent generative approach, can learn meaningful data representations that enhance predictive tasks, we explore the potential for developing a unified generative model in the molecular domain that effectively addresses both molecular generation and property prediction tasks. However, the integration of these tasks is challenging due to inherent inconsistencies, making simple multi-task learning ineffective. To address this, we propose UniGEM, the first unified model to successfully integrate molecular generation and property prediction, delivering superior performance in both tasks. Our key innovation lies in a novel two-phase generative process, where predictive tasks are activated in the later stages, after the molecular scaffold is formed. We further enhance task balance through innovative training strategies. Rigorous theoretical analysis and comprehensive experiments demonstrate our significant improvements in both tasks. The principles behind UniGEM hold promise for broader applications, including natural language processing and computer vision.</li>
</ul>

<h3>Title: AI-based particle track identification in scintillating fibres read out with imaging sensors</h3>
<ul>
<li><strong>Authors: </strong>Noemi Bührer, Saúl Alonso-Monsalve, Matthew Franks, Till Dieminger, Davide Sgalaberna</a></li>
<li><strong>Subjects: </strong>cs.LG, hep-ex, physics.ins-det</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10519">https://arxiv.org/abs/2410.10519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10519">https://arxiv.org/pdf/2410.10519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10519]] AI-based particle track identification in scintillating fibres read out with imaging sensors(https://arxiv.org/abs/2410.10519)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This paper presents the development and application of an AI-based method for particle track identification using scintillating fibres read out with imaging sensors. We propose a variational autoencoder (VAE) to efficiently filter and identify frames containing signal from the substantial data generated by SPAD array sensors. Our VAE model, trained on purely background frames, demonstrated a high capability to distinguish frames containing particle tracks from background noise. The performance of the VAE-based anomaly detection was validated with experimental data, demonstrating the method's ability to efficiently identify relevant events with rapid processing time, suggesting a solid prospect for deployment as a fast inference tool on hardware for real-time anomaly detection. This work highlights the potential of combining advanced sensor technology with machine learning techniques to enhance particle detection and tracking.</li>
</ul>

<h3>Title: Hybrid Transformer for Early Alzheimer's Detection: Integration of Handwriting-Based 2D Images and 1D Signal Features</h3>
<ul>
<li><strong>Authors: </strong>Changqing Gong, Huafeng Qin, Mounîm A. El-Yacoubi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10547">https://arxiv.org/abs/2410.10547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10547">https://arxiv.org/pdf/2410.10547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10547]] Hybrid Transformer for Early Alzheimer's Detection: Integration of Handwriting-Based 2D Images and 1D Signal Features(https://arxiv.org/abs/2410.10547)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's Disease (AD) is a prevalent neurodegenerative condition where early detection is vital. Handwriting, often affected early in AD, offers a non-invasive and cost-effective way to capture subtle motor changes. State-of-the-art research on handwriting, mostly online, based AD detection has predominantly relied on manually extracted features, fed as input to shallow machine learning models. Some recent works have proposed deep learning (DL)-based models, either 1D-CNN or 2D-CNN architectures, with performance comparing favorably to handcrafted schemes. These approaches, however, overlook the intrinsic relationship between the 2D spatial patterns of handwriting strokes and their 1D dynamic characteristics, thus limiting their capacity to capture the multimodal nature of handwriting data. Moreover, the application of Transformer models remains basically unexplored. To address these limitations, we propose a novel approach for AD detection, consisting of a learnable multimodal hybrid attention model that integrates simultaneously 2D handwriting images with 1D dynamic handwriting signals. Our model leverages a gated mechanism to combine similarity and difference attention, blending the two modalities and learning robust features by incorporating information at different scales. Our model achieved state-of-the-art performance on the DARWIN dataset, with an F1-score of 90.32\% and accuracy of 90.91\% in Task 8 ('L' writing), surpassing the previous best by 4.61% and 6.06% respectively.</li>
</ul>

<h3>Title: RICASSO: Reinforced Imbalance Learning with Class-Aware Self-Supervised Outliers Exposure</h3>
<ul>
<li><strong>Authors: </strong>Xuan Zhang, Sin Chee Chin, Tingxuan Gao, Wenming Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10548">https://arxiv.org/abs/2410.10548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10548">https://arxiv.org/pdf/2410.10548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10548]] RICASSO: Reinforced Imbalance Learning with Class-Aware Self-Supervised Outliers Exposure(https://arxiv.org/abs/2410.10548)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In real-world scenarios, deep learning models often face challenges from both imbalanced (long-tailed) and out-of-distribution (OOD) data. However, existing joint methods rely on real OOD data, which leads to unnecessary trade-offs. In contrast, our research shows that data mixing, a potent augmentation technique for long-tailed recognition, can generate pseudo-OOD data that exhibit the features of both in-distribution (ID) data and OOD data. Therefore, by using mixed data instead of real OOD data, we can address long-tailed recognition and OOD detection holistically. We propose a unified framework called Reinforced Imbalance Learning with Class-Aware Self-Supervised Outliers Exposure (RICASSO), where "self-supervised" denotes that we only use ID data for outlier exposure. RICASSO includes three main strategies: Norm-Odd-Duality-Based Outlier Exposure: Uses mixed data as pseudo-OOD data, enabling simultaneous ID data rebalancing and outlier exposure through a single loss function. Ambiguity-Aware Logits Adjustment: Utilizes the ambiguity of ID data to adaptively recalibrate logits. Contrastive Boundary-Center Learning: Combines Virtual Boundary Learning and Dual-Entropy Center Learning to use mixed data for better feature separation and clustering, with Representation Consistency Learning for robustness. Extensive experiments demonstrate that RICASSO achieves state-of-the-art performance in long-tailed recognition and significantly improves OOD detection compared to our baseline (27% improvement in AUROC and 61% reduction in FPR on the iNaturalist2018 dataset). On iNaturalist2018, we even outperforms methods using real OOD data. The code will be made public soon.</li>
</ul>

<h3>Title: MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer</h3>
<ul>
<li><strong>Authors: </strong>Minghao Zhu, Zhengpu Wang, Mengxian Hu, Ronghao Dang, Xiao Lin, Xun Zhou, Chengju Liu, Qijun Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10589">https://arxiv.org/abs/2410.10589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10589">https://arxiv.org/pdf/2410.10589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10589]] MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer(https://arxiv.org/abs/2410.10589)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Transferring visual-language knowledge from large-scale foundation models for video recognition has proved to be effective. To bridge the domain gap, additional parametric modules are added to capture the temporal information. However, zero-shot generalization diminishes with the increase in the number of specialized parameters, making existing works a trade-off between zero-shot and close-set performance. In this paper, we present MoTE, a novel framework that enables generalization and specialization to be balanced in one unified model. Our approach tunes a mixture of temporal experts to learn multiple task views with various degrees of data fitting. To maximally preserve the knowledge of each expert, we propose \emph{Weight Merging Regularization}, which regularizes the merging process of experts in weight space. Additionally with temporal feature modulation to regularize the contribution of temporal feature during test. We achieve a sound balance between zero-shot and close-set video recognition tasks and obtain state-of-the-art or competitive results on various datasets, including Kinetics-400 \& 600, UCF, and HMDB. Code is available at \url{this https URL}.</li>
</ul>

<h3>Title: SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, Flora D. Salim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10624">https://arxiv.org/abs/2410.10624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10624">https://arxiv.org/pdf/2410.10624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10624]] SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition(https://arxiv.org/abs/2410.10624)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In this work, we bridge the gap between wearable sensor technology and personalized AI assistants by enabling Large Language Models (LLMs) to understand time-series tasks like human activity recognition (HAR). Despite the strong reasoning and generalization capabilities of LLMs, leveraging them for sensor data tasks remains largely unexplored. This gap stems from challenges like the lack of semantic context in time-series data, computational limitations, and LLMs' difficulty processing numerical inputs. To address these issues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential for sensor data tasks. In the Sensor-Language Alignment Stage, we introduce special tokens for each sensor channel and automatically generate trend-descriptive text to align sensor data with textual inputs, enabling SensorLLM to capture numerical changes, channel-specific information, and sensor data of varying lengths-capabilities that existing LLMs typically struggle with, all without the need for human annotations. Next, in Task-Aware Tuning Stage, we refine the model for HAR classification using the frozen LLM and alignment module, achieving performance on par with or surpassing state-of-the-art models. We further demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through Sensor-Language Alignment, enabling it to generalize across diverse datasets for HAR tasks. We strongly believe our work lays the stepstone for future time-series and text alignment research, offering a path toward foundation models for sensor data.</li>
</ul>

<h3>Title: SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10629">https://arxiv.org/abs/2410.10629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10629">https://arxiv.org/pdf/2410.10629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10629]] SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers(https://arxiv.org/abs/2410.10629)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>We introduce \model, a text-to-image framework that can efficiently generate images up to 4096$\times$4096 resolution. \model can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8$\times$, we trained an AE that can compress images 32$\times$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, \model-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, \model-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024$\times$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released.</li>
</ul>

<h3>Title: Generative AI and Its Impact on Personalized Intelligent Tutoring Systems</h3>
<ul>
<li><strong>Authors: </strong>Subhankar Maity, Aniket Deroy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10650">https://arxiv.org/abs/2410.10650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10650">https://arxiv.org/pdf/2410.10650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10650]] Generative AI and Its Impact on Personalized Intelligent Tutoring Systems(https://arxiv.org/abs/2410.10650)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (AI) is revolutionizing educational technology by enabling highly personalized and adaptive learning environments within Intelligent Tutoring Systems (ITS). This report delves into the integration of Generative AI, particularly large language models (LLMs) like GPT-4, into ITS to enhance personalized education through dynamic content generation, real-time feedback, and adaptive learning pathways. We explore key applications such as automated question generation, customized feedback mechanisms, and interactive dialogue systems that respond to individual learner needs. The report also addresses significant challenges, including ensuring pedagogical accuracy, mitigating inherent biases in AI models, and maintaining learner engagement. Future directions highlight the potential advancements in multimodal AI integration, emotional intelligence in tutoring systems, and the ethical implications of AI-driven education. By synthesizing current research and practical implementations, this report underscores the transformative potential of Generative AI in creating more effective, equitable, and engaging educational experiences.</li>
</ul>

<h3>Title: Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Zhengwei Yang, Yuke Li, Qiang Sun, Basura Fernando, Heng Huang, Zheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10663">https://arxiv.org/abs/2410.10663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10663">https://arxiv.org/pdf/2410.10663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10663]] Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework(https://arxiv.org/abs/2410.10663)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Most existing studies on few-shot learning focus on unimodal settings, where models are trained to generalize on unseen data using only a small number of labeled examples from the same modality. However, real-world data are inherently multi-modal, and unimodal approaches limit the practical applications of few-shot learning. To address this gap, this paper introduces the Cross-modal Few-Shot Learning (CFSL) task, which aims to recognize instances from multiple modalities when only a few labeled examples are available. This task presents additional challenges compared to classical few-shot learning due to the distinct visual characteristics and structural properties unique to each modality. To tackle these challenges, we propose a Generative Transfer Learning (GTL) framework consisting of two stages: the first stage involves training on abundant unimodal data, and the second stage focuses on transfer learning to adapt to novel data. Our GTL framework jointly estimates the latent shared concept across modalities and in-modality disturbance in both stages, while freezing the generative module during the transfer phase to maintain the stability of the learned representations and prevent overfitting to the limited multi-modal samples. Our finds demonstrate that GTL has superior performance compared to state-of-the-art methods across four distinct multi-modal datasets: Sketchy, TU-Berlin, Mask1K, and SKSF-A. Additionally, the results suggest that the model can estimate latent concepts from vast unimodal data and generalize these concepts to unseen modalities using only a limited number of available samples, much like human cognitive processes.</li>
</ul>

<h3>Title: TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Jiazhi Guan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Shengyi He, Zhiliang Xu, Haocheng Feng, Errui Ding, Jingdong Wang, Hongtao Xie, Youjian Zhao, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10696">https://arxiv.org/abs/2410.10696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10696">https://arxiv.org/pdf/2410.10696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10696]] TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model(https://arxiv.org/abs/2410.10696)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, 2D speaking avatars have increasingly participated in everyday scenarios due to the fast development of facial animation techniques. However, most existing works neglect the explicit control of human bodies. In this paper, we propose to drive not only the faces but also the torso and gesture movements of a speaking figure. Inspired by recent advances in diffusion models, we propose the Motion-Enhanced Textural-Aware ModeLing for SpeaKing Avatar Reenactment (TALK-Act) framework, which enables high-fidelity avatar reenactment from only short footage of monocular video. Our key idea is to enhance the textural awareness with explicit motion guidance in diffusion modeling. Specifically, we carefully construct 2D and 3D structural information as intermediate guidance. While recent diffusion models adopt a side network for control information injection, they fail to synthesize temporally stable results even with person-specific fine-tuning. We propose a Motion-Enhanced Textural Alignment module to enhance the bond between driving and target signals. Moreover, we build a Memory-based Hand-Recovering module to help with the difficulties in hand-shape preserving. After pre-training, our model can achieve high-fidelity 2D avatar reenactment with only 30 seconds of person-specific data. Extensive experiments demonstrate the effectiveness and superiority of our proposed framework. Resources can be found at this https URL.</li>
</ul>

<h3>Title: Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10733">https://arxiv.org/abs/2410.10733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10733">https://arxiv.org/pdf/2410.10733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10733]] Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models(https://arxiv.org/abs/2410.10733)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder models for accelerating high-resolution diffusion models. Existing autoencoder models have demonstrated impressive results at a moderate spatial compression ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for high spatial compression ratios (e.g., 64x). We address this challenge by introducing two key techniques: (1) Residual Autoencoding, where we design our models to learn residuals based on the space-to-channel transformed features to alleviate the optimization difficulty of high spatial-compression autoencoders; (2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases training strategy for mitigating the generalization penalty of high spatial-compression autoencoders. With these designs, we improve the autoencoder's spatial compression ratio up to 128 while maintaining the reconstruction quality. Applying our DC-AE to latent diffusion models, we achieve significant speedup without accuracy drop. For example, on ImageNet 512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup on H100 GPU for UViT-H while achieving a better FID, compared with the widely used SD-VAE-f8 autoencoder. Our code is available at this https URL.</li>
</ul>

<h3>Title: FlexGen: Flexible Multi-View Generation from Text and Image Inputs</h3>
<ul>
<li><strong>Authors: </strong>Xinli Xu, Wenhang Ge, Jiantao Lin, Jiawei Feng, Lie Xu, HanFeng Zhao, Shunsi Zhang, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10745">https://arxiv.org/abs/2410.10745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10745">https://arxiv.org/pdf/2410.10745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10745]] FlexGen: Flexible Multi-View Generation from Text and Image Inputs(https://arxiv.org/abs/2410.10745)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we introduce FlexGen, a flexible framework designed to generate controllable and consistent multi-view images, conditioned on a single-view image, or a text prompt, or both. FlexGen tackles the challenges of controllable multi-view synthesis through additional conditioning on 3D-aware text annotations. We utilize the strong reasoning capabilities of GPT-4V to generate 3D-aware text annotations. By analyzing four orthogonal views of an object arranged as tiled multi-view images, GPT-4V can produce text annotations that include 3D-aware information with spatial relationship. By integrating the control signal with proposed adaptive dual-control module, our model can generate multi-view images that correspond to the specified text. FlexGen supports multiple controllable capabilities, allowing users to modify text prompts to generate reasonable and corresponding unseen parts. Additionally, users can influence attributes such as appearance and material properties, including metallic and roughness. Extensive experiments demonstrate that our approach offers enhanced multiple controllability, marking a significant advancement over existing multi-view diffusion models. This work has substantial implications for fields requiring rapid and flexible 3D content creation, including game development, animation, and virtual reality. Project page: this https URL.</li>
</ul>

<h3>Title: DragEntity: Trajectory Guided Video Generation using Entity and Positional Relationships</h3>
<ul>
<li><strong>Authors: </strong>Zhang Wan, Sheng Tang, Jiawei Wei, Ruize Zhang, Juan Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10751">https://arxiv.org/abs/2410.10751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10751">https://arxiv.org/pdf/2410.10751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10751]] DragEntity: Trajectory Guided Video Generation using Entity and Positional Relationships(https://arxiv.org/abs/2410.10751)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, diffusion models have achieved tremendous success in the field of video generation, with controllable video generation receiving significant attention. However, existing control methods still face two limitations: Firstly, control conditions (such as depth maps, 3D Mesh) are difficult for ordinary users to obtain directly. Secondly, it's challenging to drive multiple objects through complex motions with multiple trajectories simultaneously. In this paper, we introduce DragEntity, a video generation model that utilizes entity representation for controlling the motion of multiple objects. Compared to previous methods, DragEntity offers two main advantages: 1) Our method is more user-friendly for interaction because it allows users to drag entities within the image rather than individual pixels. 2) We use entity representation to represent any object in the image, and multiple objects can maintain relative spatial relationships. Therefore, we allow multiple trajectories to control multiple objects in the image with different levels of complexity simultaneously. Our experiments validate the effectiveness of DragEntity, demonstrating its excellent performance in fine-grained control in video generation.</li>
</ul>

<h3>Title: Use Random Selection for Now: Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation for Classification</h3>
<ul>
<li><strong>Authors: </strong>Jan Cegin, Branislav Pecher, Jakub Simko, Ivan Srba, Maria Bielikova, Peter Brusilovsky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10756">https://arxiv.org/abs/2410.10756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10756">https://arxiv.org/pdf/2410.10756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10756]] Use Random Selection for Now: Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation for Classification(https://arxiv.org/abs/2410.10756)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generative large language models (LLMs) are increasingly used for data augmentation tasks, where text samples are paraphrased (or generated anew) and then used for classifier fine-tuning. Existing works on augmentation leverage the few-shot scenarios, where samples are given to LLMs as part of prompts, leading to better augmentations. Yet, the samples are mostly selected randomly and a comprehensive overview of the effects of other (more ``informed'') sample selection strategies is lacking. In this work, we compare sample selection strategies existing in few-shot learning literature and investigate their effects in LLM-based textual augmentation. We evaluate this on in-distribution and out-of-distribution classifier performance. Results indicate, that while some ``informed'' selection strategies increase the performance of models, especially for out-of-distribution data, it happens only seldom and with marginal performance increases. Unless further advances are made, a default of random sample selection remains a good option for augmentation practitioners.</li>
</ul>

<h3>Title: Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention</h3>
<ul>
<li><strong>Authors: </strong>Dejia Xu, Yifan Jiang, Chen Huang, Liangchen Song, Thorsten Gernoth, Liangliang Cao, Zhangyang Wang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10774">https://arxiv.org/abs/2410.10774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10774">https://arxiv.org/pdf/2410.10774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10774]] Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention(https://arxiv.org/abs/2410.10774)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: this https URL</li>
</ul>

<h3>Title: ControlMM: Controllable Masked Motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Korrawe Karunratanakul, Pu Wang, Hongfei Xue, Chen Chen, Chuan Guo, Junli Cao, Jian Ren, Sergey Tulyakov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10780">https://arxiv.org/abs/2410.10780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10780">https://arxiv.org/pdf/2410.10780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10780]] ControlMM: Controllable Masked Motion Generation(https://arxiv.org/abs/2410.10780)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, despite achieving acceptable control precision, these models suffer from generation speed and fidelity limitations. To address these challenges, we propose ControlMM, a novel approach incorporating spatial control signals into the generative masked motion model. ControlMM achieves real-time, high-fidelity, and high-precision controllable motion generation simultaneously. Our approach introduces two key innovations. First, we propose masked consistency modeling, which ensures high-fidelity motion generation via random masking and reconstruction, while minimizing the inconsistency between the input control signals and the extracted control signals from the generated motion. To further enhance control precision, we introduce inference-time logit editing, which manipulates the predicted conditional motion distribution so that the generated motion, sampled from the adjusted distribution, closely adheres to the input control signals. During inference, ControlMM enables parallel and iterative decoding of multiple motion tokens, allowing for high-speed motion generation. Extensive experiments show that, compared to the state of the art, ControlMM delivers superior results in motion quality, with better FID scores (0.061 vs 0.271), and higher control precision (average error 0.0091 vs 0.0108). ControlMM generates motions 20 times faster than diffusion-based methods. Additionally, ControlMM unlocks diverse applications such as any joint any frame control, body part timeline control, and obstacle avoidance. Video visualization can be found at this https URL</li>
</ul>

<h3>Title: 3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for Human-Object Interaction (HOI) and Autonomous Driving Applications</h3>
<ul>
<li><strong>Authors: </strong>Eduardo R. Corral-Soto, Yang Liu, Tongtong Cao, Yuan Ren, Liu Bingbing</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10782">https://arxiv.org/abs/2410.10782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10782">https://arxiv.org/pdf/2410.10782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10782]] 3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for Human-Object Interaction (HOI) and Autonomous Driving Applications(https://arxiv.org/abs/2410.10782)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human-object interaction (HOI) and human-scene interaction (HSI) are crucial for human-centric scene understanding applications in Embodied Artificial Intelligence (EAI), robotics, and augmented reality (AR). A common limitation faced in these research areas is the data scarcity problem: insufficient labeled human-scene object pairs on the input images, and limited interaction complexity and granularity between them. Recent HOI and HSI methods have addressed this issue by generating dynamic interactions with rigid objects. But more complex dynamic interactions such as a human rider pedaling an articulated bicycle have been unexplored. To address this limitation, and to enable research on complex dynamic human-articulated object interactions, in this paper we propose a method to generate simulated 3D dynamic cyclist assets and interactions. We designed a methodology for creating a new part-based multi-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes that can be used to train NeRF and 3DGS-based 3D reconstruction methods. We then propose a 3DGS-based parametric bicycle composition model to assemble 8-DoF pose-controllable 3D bicycles. Finally, using dynamic information from cyclist videos, we build a complete synthetic dynamic 3D cyclist (rider pedaling a bicycle) by re-posing a selectable synthetic 3D person while automatically placing the rider onto one of our new articulated 3D bicycles using a proposed 3D Keypoint optimization-based Inverse Kinematics pose refinement. We present both, qualitative and quantitative results where we compare our generated cyclists against those from a recent stable diffusion-based method.</li>
</ul>

<h3>Title: LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content</h3>
<ul>
<li><strong>Authors: </strong>Nimrod Shabtay, Felipe Maia Polo, Sivan Doveh, Wei Lin, M. Jehanzeb Mirza, Leshem Chosen, Mikhail Yurochkin, Yuekai Sun, Assaf Arbelle, Leonid Karlinsky, Raja Giryes</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10783">https://arxiv.org/abs/2410.10783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10783">https://arxiv.org/pdf/2410.10783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10783]] LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content(https://arxiv.org/abs/2410.10783)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here.</li>
</ul>

<h3>Title: Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, Wen-Sheng Chu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10792">https://arxiv.org/abs/2410.10792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10792">https://arxiv.org/pdf/2410.10792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10792]] Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations(https://arxiv.org/abs/2410.10792)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.</li>
</ul>

<h3>Title: MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei Zhai, Yang Cao, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10798">https://arxiv.org/abs/2410.10798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10798">https://arxiv.org/pdf/2410.10798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10798]] MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling(https://arxiv.org/abs/2410.10798)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in multi-modal large language models have propelled the development of joint probabilistic models capable of both image understanding and generation. However, we have identifed that recent methods inevitably suffer from loss of image information during understanding task, due to either image discretization or diffusion denoising steps. To address this issue, we propose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling framework. Unlike discretization line of method, MMAR takes in continuous-valued image tokens to avoid information loss. Differing from diffusion-based approaches, we disentangle the diffusion process from auto-regressive backbone model by employing a light-weight diffusion head on top each auto-regressed image patch embedding. In this way, when the model transits from image generation to understanding through text generation, the backbone model's hidden representation of the image is not limited to the last denoising step. To successfully train our method, we also propose a theoretically proven technique that addresses the numerical stability issue and a training strategy that balances the generation and understanding task goals. Through extensive evaluations on 18 image understanding benchmarks, MMAR demonstrates much more superior performance than other joint multi-modal models, matching the method that employs pretrained CLIP vision encoder, meanwhile being able to generate high quality images at the same time. We also showed that our method is scalable with larger data and model size.</li>
</ul>

<h3>Title: Towards Foundation Models for 3D Vision: How Close Are We?</h3>
<ul>
<li><strong>Authors: </strong>Yiming Zuo, Karhan Kayan, Maggie Wang, Kevin Jeon, Jia Deng, Thomas L. Griffiths</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10799">https://arxiv.org/abs/2410.10799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10799">https://arxiv.org/pdf/2410.10799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10799]] Towards Foundation Models for 3D Vision: How Close Are We?(https://arxiv.org/abs/2410.10799)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Building a foundation model for 3D vision is a complex challenge that remains unsolved. Towards that goal, it is important to understand the 3D reasoning capabilities of current models as well as identify the gaps between these models and humans. Therefore, we construct a new 3D visual understanding benchmark that covers fundamental 3D vision tasks in the Visual Question Answering (VQA) format. We evaluate state-of-the-art Vision-Language Models (VLMs), specialized models, and human subjects on it. Our results show that VLMs generally perform poorly, while the specialized models are accurate but not robust, failing under geometric perturbations. In contrast, human vision continues to be the most reliable 3D visual system. We further demonstrate that neural networks align more closely with human 3D vision mechanisms compared to classical computer vision methods, and Transformer-based networks such as ViT align more closely with human 3D vision mechanisms than CNNs. We hope our study will benefit the future development of foundation models for 3D vision.</li>
</ul>

<h3>Title: Boosting Camera Motion Control for Video Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, Chun-Hao Paul Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10802">https://arxiv.org/abs/2410.10802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10802">https://arxiv.org/pdf/2410.10802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10802]] Boosting Camera Motion Control for Video Diffusion Transformers(https://arxiv.org/abs/2410.10802)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have significantly enhanced the quality of video generation. However, fine-grained control over camera pose remains a challenge. While U-Net-based models have shown promising results for camera control, transformer-based diffusion models (DiT)-the preferred architecture for large-scale video generation - suffer from severe degradation in camera motion accuracy. In this paper, we investigate the underlying causes of this issue and propose solutions tailored to DiT architectures. Our study reveals that camera control performance depends heavily on the choice of conditioning methods rather than camera pose representations that is commonly believed. To address the persistent motion degradation in DiT, we introduce Camera Motion Guidance (CMG), based on classifier-free guidance, which boosts camera control by over 400%. Additionally, we present a sparse camera control pipeline, significantly simplifying the process of specifying camera poses for long videos. Our method universally applies to both U-Net and DiT models, offering improved camera control for video generation tasks.</li>
</ul>

<h3>Title: TrajDiffuse: A Conditional Diffusion Model for Environment-Aware Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Qingze (Tony)Liu, Danrui Li, Samuel S. Sohn, Sejong Yoon, Mubbasir Kapadia, Vladimir Pavlovic</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10804">https://arxiv.org/abs/2410.10804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10804">https://arxiv.org/pdf/2410.10804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10804]] TrajDiffuse: A Conditional Diffusion Model for Environment-Aware Trajectory Prediction(https://arxiv.org/abs/2410.10804)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate prediction of human or vehicle trajectories with good diversity that captures their stochastic nature is an essential task for many applications. However, many trajectory prediction models produce unreasonable trajectory samples that focus on improving diversity or accuracy while neglecting other key requirements, such as collision avoidance with the surrounding environment. In this work, we propose TrajDiffuse, a planning-based trajectory prediction method using a novel guided conditional diffusion model. We form the trajectory prediction problem as a denoising impaint task and design a map-based guidance term for the diffusion process. TrajDiffuse is able to generate trajectory predictions that match or exceed the accuracy and diversity of the SOTA, while adhering almost perfectly to environmental constraints. We demonstrate the utility of our model through experiments on the nuScenes and PFSD datasets and provide an extensive benchmark analysis against the SOTA methods.</li>
</ul>

<h3>Title: HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</h3>
<ul>
<li><strong>Authors: </strong>Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10812">https://arxiv.org/abs/2410.10812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10812">https://arxiv.org/pdf/2410.10812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10812]] HART: Efficient Visual Generation with Hybrid Autoregressive Transformer(https://arxiv.org/abs/2410.10812)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at this https URL.</li>
</ul>

<h3>Title: Depth Any Video with Scalable Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, Tong He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10815">https://arxiv.org/abs/2410.10815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10815">https://arxiv.org/pdf/2410.10815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10815]] Depth Any Video with Scalable Synthetic Data(https://arxiv.org/abs/2410.10815)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Video depth estimation has long been hindered by the scarcity of consistent and scalable ground truth data, leading to inconsistent and unreliable results. In this paper, we introduce Depth Any Video, a model that tackles the challenge through two key innovations. First, we develop a scalable synthetic data pipeline, capturing real-time video depth data from diverse synthetic environments, yielding 40,000 video clips of 5-second duration, each with precise depth annotations. Second, we leverage the powerful priors of generative video diffusion models to handle real-world videos effectively, integrating advanced techniques such as rotary position encoding and flow matching to further enhance flexibility and efficiency. Unlike previous models, which are limited to fixed-length video sequences, our approach introduces a novel mixed-duration training strategy that handles videos of varying lengths and performs robustly across different frame rates-even on single frames. At inference, we propose a depth interpolation method that enables our model to infer high-resolution video depth across sequences of up to 150 frames. Our model outperforms all previous generative depth models in terms of spatial accuracy and temporal consistency.</li>
</ul>

<h3>Title: Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jingzhi Bao, Xueting Li, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10821">https://arxiv.org/abs/2410.10821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10821">https://arxiv.org/pdf/2410.10821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10821]] Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models(https://arxiv.org/abs/2410.10821)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D meshes are widely used in computer vision and graphics for their efficiency in animation and minimal memory use, playing a crucial role in movies, games, AR, and VR. However, creating temporally consistent and realistic textures for mesh sequences remains labor-intensive for professional artists. On the other hand, while video diffusion models excel at text-driven video generation, they often lack 3D geometry awareness and struggle with achieving multi-view consistent texturing for 3D meshes. In this work, we present Tex4D, a zero-shot approach that integrates inherent 3D geometry knowledge from mesh sequences with the expressiveness of video diffusion models to produce multi-view and temporally consistent 4D textures. Given an untextured mesh sequence and a text prompt as inputs, our method enhances multi-view consistency by synchronizing the diffusion process across different views through latent aggregation in the UV space. To ensure temporal consistency, we leverage prior knowledge from a conditional video generation model for texture synthesis. However, straightforwardly combining the video diffusion model and the UV texture aggregation leads to blurry results. We analyze the underlying causes and propose a simple yet effective modification to the DDIM sampling process to address this issue. Additionally, we introduce a reference latent texture to strengthen the correlation between frames during the denoising process. To the best of our knowledge, Tex4D is the first method specifically designed for 4D scene texturing. Extensive experiments demonstrate its superiority in producing multi-view and multi-frame consistent videos based on untextured mesh sequences.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
