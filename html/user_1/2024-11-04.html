<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-11-04</h1>
<h3>Title: A Novel Psychometrics-Based Approach to Developing Professional Competency Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Elena Kardanova, Alina Ivanova, Ksenia Tarasova, Taras Pashchenko, Aleksei Tikhoniuk, Elen Yusupova, Anatoly Kasprzhak, Yaroslav Kuzminov, Ekaterina Kruchinskaia, Irina Brun (National Research University Higher School of Economics, Moscow, Russia)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00045">https://arxiv.org/abs/2411.00045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00045">https://arxiv.org/pdf/2411.00045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00045]] A Novel Psychometrics-Based Approach to Developing Professional Competency Benchmark for Large Language Models(https://arxiv.org/abs/2411.00045)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The era of large language models (LLM) raises questions not only about how to train models, but also about how to evaluate them. Despite numerous existing benchmarks, insufficient attention is often given to creating assessments that test LLMs in a valid and reliable manner. To address this challenge, we accommodate the Evidence-centered design (ECD) methodology and propose a comprehensive approach to benchmark development based on rigorous psychometric principles. In this paper, we have made the first attempt to illustrate this approach by creating a new benchmark in the field of pedagogy and education, highlighting the limitations of existing benchmark development approach and taking into account the development of LLMs. We conclude that a new approach to benchmarking is required to match the growing complexity of AI applications in the educational context. We construct a novel benchmark guided by the Bloom's taxonomy and rigorously designed by a consortium of education experts trained in test development. Thus the current benchmark provides an academically robust and practical assessment tool tailored for LLMs, rather than human participants. Tested empirically on the GPT model in the Russian language, it evaluates model performance across varied task complexities, revealing critical gaps in current LLM capabilities. Our results indicate that while generative AI tools hold significant promise for education - potentially supporting tasks such as personalized tutoring, real-time feedback, and multilingual learning - their reliability as autonomous teachers' assistants right now remain rather limited, particularly in tasks requiring deeper cognitive engagement.</li>
</ul>

<h3>Title: CurateGPT: A flexible language-model assisted biocuration tool</h3>
<ul>
<li><strong>Authors: </strong>Harry Caufield, Carlo Kroll, Shawn T O'Neil, Justin T Reese, Marcin P Joachimiak, Harshad Hegde, Nomi L Harris, Madan Krishnamurthy, James A McLaughlin, Damian Smedley, Melissa A Haendel, Peter N Robinson, Christopher J Mungall</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00046">https://arxiv.org/abs/2411.00046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00046">https://arxiv.org/pdf/2411.00046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00046]] CurateGPT: A flexible language-model assisted biocuration tool(https://arxiv.org/abs/2411.00046)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Effective data-driven biomedical discovery requires data curation: a time-consuming process of finding, organizing, distilling, integrating, interpreting, annotating, and validating diverse information into a structured form suitable for databases and knowledge bases. Accurate and efficient curation of these digital assets is critical to ensuring that they are FAIR, trustworthy, and sustainable. Unfortunately, expert curators face significant time and resource constraints. The rapid pace of new information being published daily is exceeding their capacity for curation. Generative AI, exemplified by instruction-tuned large language models (LLMs), has opened up new possibilities for assisting human-driven curation. The design philosophy of agents combines the emerging abilities of generative AI with more precise methods. A curator's tasks can be aided by agents for performing reasoning, searching ontologies, and integrating knowledge across external sources, all efforts otherwise requiring extensive manual effort. Our LLM-driven annotation tool, CurateGPT, melds the power of generative AI together with trusted knowledge bases and literature sources. CurateGPT streamlines the curation process, enhancing collaboration and efficiency in common workflows. Compared to direct interaction with an LLM, CurateGPT's agents enable access to information beyond that in the LLM's training data and they provide direct links to the data supporting each claim. This helps curators, researchers, and engineers scale up curation efforts to keep pace with the ever-increasing volume of scientific data.</li>
</ul>

<h3>Title: How Good Are We? Evaluating Cell AI Foundation Models in Kidney Pathology with Human-in-the-Loop Enrichment</h3>
<ul>
<li><strong>Authors: </strong>Junlin Guo, Siqi Lu, Can Cui, Ruining Deng, Tianyuan Yao, Zhewen Tao, Yizhe Lin, Marilyn Lionts, Quan Liu, Juming Xiong, Yu Wang, Shilin Zhao, Catie Chang, Mitchell Wilkes, Mengmeng Yin, Haichun Yang, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00078">https://arxiv.org/abs/2411.00078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00078">https://arxiv.org/pdf/2411.00078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00078]] How Good Are We? Evaluating Cell AI Foundation Models in Kidney Pathology with Human-in-the-Loop Enrichment(https://arxiv.org/abs/2411.00078)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Training AI foundation models has emerged as a promising large-scale learning approach for addressing real-world healthcare challenges, including digital pathology. While many of these models have been developed for tasks like disease diagnosis and tissue quantification using extensive and diverse training datasets, their readiness for deployment on some arguably simplest tasks, such as nuclei segmentation within a single organ (e.g., the kidney), remains uncertain. This paper seeks to answer this key question, "How good are we?", by thoroughly evaluating the performance of recent cell foundation models on a curated multi-center, multi-disease, and multi-species external testing dataset. Additionally, we tackle a more challenging question, "How can we improve?", by developing and assessing human-in-the-loop data enrichment strategies aimed at enhancing model performance while minimizing the reliance on pixel-level human annotation. To address the first question, we curated a multicenter, multidisease, and multispecies dataset consisting of 2,542 kidney whole slide images (WSIs). Three state-of-the-art (SOTA) cell foundation models-Cellpose, StarDist, and CellViT-were selected for evaluation. To tackle the second question, we explored data enrichment algorithms by distilling predictions from the different foundation models with a human-in-the-loop framework, aiming to further enhance foundation model performance with minimal human efforts. Our experimental results showed that all three foundation models improved over their baselines with model fine-tuning with enriched data. Interestingly, the baseline model with the highest F1 score does not yield the best segmentation outcomes after fine-tuning. This study establishes a benchmark for the development and deployment of cell vision foundation models tailored for real-world data applications.</li>
</ul>

<h3>Title: Label Noise: Ignorance Is Bliss</h3>
<ul>
<li><strong>Authors: </strong>Yilun Zhu, Jianxin Zhang, Aditya Gangrade, Clayton Scott</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00079">https://arxiv.org/abs/2411.00079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00079">https://arxiv.org/pdf/2411.00079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00079]] Label Noise: Ignorance Is Bliss(https://arxiv.org/abs/2411.00079)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We establish a new theoretical framework for learning under multi-class, instance-dependent label noise. This framework casts learning with label noise as a form of domain adaptation, in particular, domain adaptation under posterior drift. We introduce the concept of \emph{relative signal strength} (RSS), a pointwise measure that quantifies the transferability from noisy to clean posterior. Using RSS, we establish nearly matching upper and lower bounds on the excess risk. Our theoretical findings support the simple \emph{Noise Ignorant Empirical Risk Minimization (NI-ERM)} principle, which minimizes empirical risk while ignoring label noise. Finally, we translate this theoretical insight into practice: by using NI-ERM to fit a linear classifier on top of a self-supervised feature extractor, we achieve state-of-the-art performance on the CIFAR-N data challenge.</li>
</ul>

<h3>Title: Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales</h3>
<ul>
<li><strong>Authors: </strong>Tang Li, Mengmeng Ma, Xi Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00132">https://arxiv.org/abs/2411.00132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00132">https://arxiv.org/pdf/2411.00132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00132]] Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales(https://arxiv.org/abs/2411.00132)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large pretrained foundation models demonstrate exceptional performance and, in some high-stakes applications, even surpass human experts. However, most of these models are currently evaluated primarily on prediction accuracy, overlooking the validity of the rationales behind their accurate predictions. For the safe deployment of foundation models, there is a pressing need to ensure double-correct predictions, i.e., correct prediction backed by correct rationales. To achieve this, we propose a two-phase scheme: First, we curate a new dataset that offers structured rationales for visual recognition tasks. Second, we propose a rationale-informed optimization method to guide the model in disentangling and localizing visual evidence for each rationale, without requiring manual annotations. Extensive experiments and ablation studies demonstrate that our model outperforms state-of-the-art models by up to 10.1% in prediction accuracy across a wide range of tasks. Furthermore, our method significantly improves the model's rationale correctness, improving localization by 7.5% and disentanglement by 36.5%. Our dataset, source code, and pretrained weights: this https URL</li>
</ul>

<h3>Title: A Recipe for Geometry-Aware 3D Mesh Transformers</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Farazi, Yalin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00164">https://arxiv.org/abs/2411.00164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00164">https://arxiv.org/pdf/2411.00164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00164]] A Recipe for Geometry-Aware 3D Mesh Transformers(https://arxiv.org/abs/2411.00164)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Utilizing patch-based transformers for unstructured geometric data such as polygon meshes presents significant challenges, primarily due to the absence of a canonical ordering and variations in input sizes. Prior approaches to handling 3D meshes and point clouds have either relied on computationally intensive node-level tokens for large objects or resorted to resampling to standardize patch size. Moreover, these methods generally lack a geometry-aware, stable Structural Embedding (SE), often depending on simplistic absolute SEs such as 3D coordinates, which compromise isometry invariance essential for tasks like semantic segmentation. In our study, we meticulously examine the various components of a geometry-aware 3D mesh transformer, from tokenization to structural encoding, assessing the contribution of each. Initially, we introduce a spectral-preserving tokenization rooted in algebraic multigrid methods. Subsequently, we detail an approach for embedding features at the patch level, accommodating patches with variable node counts. Through comparative analyses against a baseline model employing simple point-wise Multi-Layer Perceptrons (MLP), our research highlights critical insights: 1) the importance of structural and positional embeddings facilitated by heat diffusion in general 3D mesh transformers; 2) the effectiveness of novel components such as geodesic masking and feature interaction via cross-attention in enhancing learning; and 3) the superior performance and efficiency of our proposed methods in challenging segmentation and classification tasks.</li>
</ul>

<h3>Title: Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Coding</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Ganjidoost, Jeff Orchard</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00222">https://arxiv.org/abs/2411.00222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00222">https://arxiv.org/pdf/2411.00222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00222]] Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Coding(https://arxiv.org/abs/2411.00222)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An adversarial example is a modified input image designed to cause a Machine Learning (ML) model to make a mistake; these perturbations are often invisible or subtle to human observers and highlight vulnerabilities in a model's ability to generalize from its training data. Several adversarial attacks can create such examples, each with a different perspective, effectiveness, and perceptibility of changes. Conversely, defending against such adversarial attacks improves the robustness of ML models in image processing and other domains of deep learning. Most defence mechanisms require either a level of model awareness, changes to the model, or access to a comprehensive set of adversarial examples during training, which is impractical. Another option is to use an auxiliary model in a preprocessing manner without changing the primary model. This study presents a practical and effective solution -- using predictive coding networks (PCnets) as an auxiliary step for adversarial defence. By seamlessly integrating PCnets into feed-forward networks as a preprocessing step, we substantially bolster resilience to adversarial perturbations. Our experiments on MNIST and CIFAR10 demonstrate the remarkable effectiveness of PCnets in mitigating adversarial examples with about 82% and 65% improvements in robustness, respectively. The PCnet, trained on a small subset of the dataset, leverages its generative nature to effectively counter adversarial efforts, reverting perturbed images closer to their original forms. This innovative approach holds promise for enhancing the security and reliability of neural network classifiers in the face of the escalating threat of adversarial attacks.</li>
</ul>

<h3>Title: Fashion-VDM: Video Diffusion Model for Virtual Try-On</h3>
<ul>
<li><strong>Authors: </strong>Johanna Karras, Yingwei Li, Nan Liu, Luyang Zhu, Innfarn Yoo, Andreas Lugmayr, Chris Lee, Ira Kemelmacher-Shlizerman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00225">https://arxiv.org/abs/2411.00225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00225">https://arxiv.org/pdf/2411.00225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00225]] Fashion-VDM: Video Diffusion Model for Virtual Try-On(https://arxiv.org/abs/2411.00225)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present Fashion-VDM, a video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate a high-quality try-on video of the person wearing the given garment, while preserving the person's identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose a diffusion-based architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and a progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-the-art for video virtual try-on. For additional results, visit our project page: this https URL.</li>
</ul>

<h3>Title: KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks</h3>
<ul>
<li><strong>Authors: </strong>Quan Zhou, Changhua Pei, Fei Sun, Jing Han, Zhengwei Gao, Dan Pei, Haiming Zhang, Gaogang Xie, Jianhui Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00278">https://arxiv.org/abs/2411.00278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00278">https://arxiv.org/pdf/2411.00278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00278]] KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks(https://arxiv.org/abs/2411.00278)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) has become an essential component of large-scale cloud services and web systems because it can promptly identify anomalies, providing early warnings to prevent greater losses. Deep learning-based forecasting methods have become very popular in TSAD due to their powerful learning capabilities. However, accurate predictions don't necessarily lead to better anomaly detection. Due to the common occurrence of noise, i.e., local peaks and drops in time series, existing black-box learning methods can easily learn these unintended patterns, significantly affecting anomaly detection performance. Kolmogorov-Arnold Networks (KAN) offers a potential solution by decomposing complex temporal sequences into a combination of multiple univariate functions, making the training process more controllable. However, KAN optimizes univariate functions using spline functions, which are also susceptible to the influence of local anomalies. To address this issue, we present KAN-AD, which leverages the Fourier series to emphasize global temporal patterns, thereby mitigating the influence of local peaks and drops. KAN-AD improves both effectiveness and efficiency by transforming the existing black-box learning approach into learning the weights preceding univariate functions. Experimental results show that, compared to the current state-of-the-art, we achieved an accuracy increase of 15% while boosting inference speed by 55 times.</li>
</ul>

<h3>Title: Inducing Semi-Structured Sparsity by Masking for Efficient Model Inference in Convolutional Networks</h3>
<ul>
<li><strong>Authors: </strong>David A. Danhofer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.NE, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00288">https://arxiv.org/abs/2411.00288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00288">https://arxiv.org/pdf/2411.00288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00288]] Inducing Semi-Structured Sparsity by Masking for Efficient Model Inference in Convolutional Networks(https://arxiv.org/abs/2411.00288)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The crucial role of convolutional models, both as standalone vision models and backbones in foundation models, necessitates effective acceleration techniques. This paper proposes a novel method to learn semi-structured sparsity patterns for convolution kernels in the form of maskings enabling the utilization of readily available hardware accelerations. The approach accelerates convolutional models more than two-fold during inference without decreasing model performance. At the same time, the original model weights and structure remain unchanged keeping the model thus easily updatable. Beyond the immediate practical use, the effect of maskings on prediction is easily quantifiable. Therefore, guarantees on model predictions under maskings are derived showing stability bounds for learned maskings even after updating the original underlying model.</li>
</ul>

<h3>Title: Unified Generative and Discriminative Training for Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Chow, Juncheng Li, Qifan Yu, Kaihang Pan, Hao Fei, Zhiqi Ge, Shuai Yang, Siliang Tang, Hanwang Zhang, Qianru Sun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00304">https://arxiv.org/abs/2411.00304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00304">https://arxiv.org/pdf/2411.00304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00304]] Unified Generative and Discriminative Training for Multi-modal Large Language Models(https://arxiv.org/abs/2411.00304)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM's hidden state. This approach enhances the MLLM's ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling.</li>
</ul>

<h3>Title: Personalized Federated Learning via Feature Distribution Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Connor J. Mclaughlin, Lili Su</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00329">https://arxiv.org/abs/2411.00329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00329">https://arxiv.org/pdf/2411.00329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00329]] Personalized Federated Learning via Feature Distribution Adaptation(https://arxiv.org/abs/2411.00329)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a distributed learning framework that leverages commonalities between distributed client datasets to train a global model. Under heterogeneous clients, however, FL can fail to produce stable training results. Personalized federated learning (PFL) seeks to address this by learning individual models tailored to each client. One approach is to decompose model training into shared representation learning and personalized classifier training. Nonetheless, previous works struggle to navigate the bias-variance trade-off in classifier learning, relying solely on limited local datasets or introducing costly techniques to improve generalization. In this work, we frame representation learning as a generative modeling task, where representations are trained with a classifier based on the global feature distribution. We then propose an algorithm, pFedFDA, that efficiently generates personalized models by adapting global generative classifiers to their local feature distributions. Through extensive computer vision benchmarks, we demonstrate that our method can adjust to complex distribution shifts with significant improvements over current state-of-the-art in data-scarce settings.</li>
</ul>

<h3>Title: TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images</h3>
<ul>
<li><strong>Authors: </strong>Mengcheng Li, Mingbao Lin, Fei Chao, Chia-Wen Lin, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00355">https://arxiv.org/abs/2411.00355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00355">https://arxiv.org/pdf/2411.00355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00355]] TextDestroyer: A Training- and Annotation-Free Diffusion Method for Destroying Anomal Text from Images(https://arxiv.org/abs/2411.00355)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose TextDestroyer, the first training- and annotation-free method for scene text destruction using a pre-trained diffusion model. Existing scene text removal models require complex annotation and retraining, and may leave faint yet recognizable text information, compromising privacy protection and content concealment. TextDestroyer addresses these issues by employing a three-stage hierarchical process to obtain accurate text masks. Our method scrambles text areas in the latent start code using a Gaussian distribution before reconstruction. During the diffusion denoising process, self-attention key and value are referenced from the original latent to restore the compromised background. Latent codes saved at each inversion step are used for replacement during reconstruction, ensuring perfect background restoration. The advantages of TextDestroyer include: (1) it eliminates labor-intensive data annotation and resource-intensive training; (2) it achieves more thorough text destruction, preventing recognizable traces; and (3) it demonstrates better generalization capabilities, performing well on both real-world scenes and generated images.</li>
</ul>

<h3>Title: Constrained Diffusion Implicit Models</h3>
<ul>
<li><strong>Authors: </strong>Vivek Jayaram, Ira Kemelmacher-Shlizerman, Steven M. Seitz, John Thickstun</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00359">https://arxiv.org/abs/2411.00359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00359">https://arxiv.org/pdf/2411.00359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00359]] Constrained Diffusion Implicit Models(https://arxiv.org/abs/2411.00359)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across a variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconstrained DDIM: 10 to 50 times faster than previous conditional diffusion methods. We demonstrate the versatility of our approach on many problems including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction.</li>
</ul>

<h3>Title: STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing</h3>
<ul>
<li><strong>Authors: </strong>Jiaru Zou, Qing Wang, Pratyush Thakur, Nickvash Kani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00387">https://arxiv.org/abs/2411.00387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00387">https://arxiv.org/pdf/2411.00387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00387]] STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing(https://arxiv.org/abs/2411.00387)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Advances in large language models (LLMs) have spurred research into enhancing their reasoning capabilities, particularly in math-rich STEM documents. While LLMs can generate equations or solve math-related queries, their ability to fully understand and interpret abstract mathematical symbols in long, math-rich documents remains limited. In this paper, we introduce STEM-PoM, a comprehensive benchmark dataset designed to evaluate LLMs' reasoning abilities on math symbols within contextual scientific text. The dataset, sourced from real-world ArXiv documents, contains over 2K math symbols classified as main attributes of variables, constants, operators, and unit descriptors, with additional sub-attributes including scalar/vector/matrix for variables and local/global/discipline-specific labels for both constants and operators. Our extensive experiments show that state-of-the-art LLMs achieve an average of 20-60% accuracy under in-context learning and 50-60% accuracy with fine-tuning, revealing a significant gap in their mathematical reasoning capabilities. STEM-PoM fuels future research of developing advanced Math-AI models that can robustly handle math symbols.</li>
</ul>

<h3>Title: Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization</h3>
<ul>
<li><strong>Authors: </strong>Junlin He, Jinxiao Du, Wei Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00392">https://arxiv.org/abs/2411.00392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00392">https://arxiv.org/pdf/2411.00392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00392]] Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization(https://arxiv.org/abs/2411.00392)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has rapidly advanced in recent years, approaching the performance of its supervised counterparts through the extraction of representations from unlabeled data. However, dimensional collapse, where a few large eigenvalues dominate the eigenspace, poses a significant obstacle for SSL. When dimensional collapse occurs on features (e.g. hidden features and representations), it prevents features from representing the full information of the data; when dimensional collapse occurs on weight matrices, their filters are self-related and redundant, limiting their expressive power. Existing studies have predominantly concentrated on the dimensional collapse of representations, neglecting whether this can sufficiently prevent the dimensional collapse of the weight matrices and hidden features. To this end, we first time propose a mitigation approach employing orthogonal regularization (OR) across the encoder, targeting both convolutional and linear layers during pretraining. OR promotes orthogonality within weight matrices, thus safeguarding against the dimensional collapse of weight matrices, hidden features, and representations. Our empirical investigations demonstrate that OR significantly enhances the performance of SSL methods across diverse benchmarks, yielding consistent gains with both CNNs and Transformer-based architectures.</li>
</ul>

<h3>Title: StyleTex: Style Image-Guided Texture Generation for 3D Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Xie, Yuqing Zhang, Xiangjun Tang, Yiqian Wu, Dehan Chen, Gongsheng Li, Xaogang Jin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00399">https://arxiv.org/abs/2411.00399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00399">https://arxiv.org/pdf/2411.00399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00399]] StyleTex: Style Image-Guided Texture Generation for 3D Models(https://arxiv.org/abs/2411.00399)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Style-guided texture generation aims to generate a texture that is harmonious with both the style of the reference image and the geometry of the input mesh, given a reference style image and a 3D mesh with its text description. Although diffusion-based 3D texture generation methods, such as distillation sampling, have numerous promising applications in stylized games and films, it requires addressing two challenges: 1) decouple style and content completely from the reference image for 3D models, and 2) align the generated texture with the color tone, style of the reference image, and the given text prompt. To this end, we introduce StyleTex, an innovative diffusion-model-based framework for creating stylized textures for 3D models. Our key insight is to decouple style information from the reference image while disregarding content in diffusion-based distillation sampling. Specifically, given a reference image, we first decompose its style feature from the image CLIP embedding by subtracting the embedding's orthogonal projection in the direction of the content feature, which is represented by a text CLIP embedding. Our novel approach to disentangling the reference image's style and content information allows us to generate distinct style and content features. We then inject the style feature into the cross-attention mechanism to incorporate it into the generation process, while utilizing the content feature as a negative prompt to further dissociate content information. Finally, we incorporate these strategies into StyleTex to obtain stylized textures. The resulting textures generated by StyleTex retain the style of the reference image, while also aligning with the text prompts and intrinsic details of the given 3D mesh. Quantitative and qualitative experiments show that our method outperforms existing baseline methods by a significant margin.</li>
</ul>

<h3>Title: Cityscape-Adverse: Benchmarking Robustness of Semantic Segmentation with Realistic Scene Modifications via Diffusion-Based Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Naufal Suryanto, Andro Aprila Adiputra, Ahmada Yusril Kadiptya, Thi-Thu-Huong Le, Derry Pratama, Yongsu Kim, Howon Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00425">https://arxiv.org/abs/2411.00425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00425">https://arxiv.org/pdf/2411.00425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00425]] Cityscape-Adverse: Benchmarking Robustness of Semantic Segmentation with Realistic Scene Modifications via Diffusion-Based Image Editing(https://arxiv.org/abs/2411.00425)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative AI, particularly diffusion-based image editing, have enabled the transformation of images into highly realistic scenes using only text instructions. This technology offers significant potential for generating diverse synthetic datasets to evaluate model robustness. In this paper, we introduce Cityscape-Adverse, a benchmark that employs diffusion-based image editing to simulate eight adverse conditions, including variations in weather, lighting, and seasons, while preserving the original semantic labels. We evaluate the reliability of diffusion-based models in generating realistic scene modifications and assess the performance of state-of-the-art CNN and Transformer-based semantic segmentation models under these challenging conditions. Additionally, we analyze which modifications have the greatest impact on model performance and explore how training on synthetic datasets can improve robustness in real-world adverse scenarios. Our results demonstrate that all tested models, particularly CNN-based architectures, experienced significant performance degradation under extreme conditions, while Transformer-based models exhibited greater resilience. We verify that models trained on Cityscape-Adverse show significantly enhanced resilience when applied to unseen domains. Code and datasets will be released at this https URL.</li>
</ul>

<h3>Title: Diffusion Models as Network Optimizers: Explorations and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Ruihuai Liang, Bo Yang, Pengyu Chen, Xianjin Li, Yifan Xue, Zhiwen Yu, Xuelin Cao, Yan Zhang, MÃ©rouane Debbah, H. Vincent Poor, Chau Yuen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00453">https://arxiv.org/abs/2411.00453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00453">https://arxiv.org/pdf/2411.00453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00453]] Diffusion Models as Network Optimizers: Explorations and Analysis(https://arxiv.org/abs/2411.00453)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Network optimization is a fundamental challenge in the Internet of Things (IoT) network, often characterized by complex features that make it difficult to solve these problems. Recently, generative diffusion models (GDMs) have emerged as a promising new approach to network optimization, with the potential to directly address these optimization problems. However, the application of GDMs in this field is still in its early stages, and there is a noticeable lack of theoretical research and empirical findings. In this study, we first explore the intrinsic characteristics of generative models. Next, we provide a concise theoretical proof and intuitive demonstration of the advantages of generative models over discriminative models in network optimization. Based on this exploration, we implement GDMs as optimizers aimed at learning high-quality solution distributions for given inputs, sampling from these distributions during inference to approximate or achieve optimal solutions. Specifically, we utilize denoising diffusion probabilistic models (DDPMs) and employ a classifier-free guidance mechanism to manage conditional guidance based on input parameters. We conduct extensive experiments across three challenging network optimization problems. By investigating various model configurations and the principles of GDMs as optimizers, we demonstrate the ability to overcome prediction errors and validate the convergence of generated solutions to optimal solutions.</li>
</ul>

<h3>Title: Generative AI-based Pipeline Architecture for Increasing Training Efficiency in Intelligent Weed Control Systems</h3>
<ul>
<li><strong>Authors: </strong>Sourav Modak, Anthony Stein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00548">https://arxiv.org/abs/2411.00548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00548">https://arxiv.org/pdf/2411.00548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00548]] Generative AI-based Pipeline Architecture for Increasing Training Efficiency in Intelligent Weed Control Systems(https://arxiv.org/abs/2411.00548)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In automated crop protection tasks such as weed control, disease diagnosis, and pest monitoring, deep learning has demonstrated significant potential. However, these advanced models rely heavily on high-quality, diverse datasets, often limited and costly in agricultural settings. Traditional data augmentation can increase dataset volume but usually lacks the real-world variability needed for robust training. This study presents a new approach for generating synthetic images to improve deep learning-based object detection models for intelligent weed control. Our GenAI-based image generation pipeline integrates the Segment Anything Model (SAM) for zero-shot domain adaptation with a text-to-image Stable Diffusion Model, enabling the creation of synthetic images that capture diverse real-world conditions. We evaluate these synthetic datasets using lightweight YOLO models, measuring data efficiency with mAP50 and mAP50-95 scores across varying proportions of real and synthetic data. Notably, YOLO models trained on datasets with 10% synthetic and 90% real images generally demonstrate superior mAP50 and mAP50-95 scores compared to those trained solely on real images. This approach not only reduces dependence on extensive real-world datasets but also enhances predictive performance. The integration of this approach opens opportunities for achieving continual self-improvement of perception modules in intelligent technical systems.</li>
</ul>

<h3>Title: Conditional Synthesis of 3D Molecules with Time Correction Sampler</h3>
<ul>
<li><strong>Authors: </strong>Hojung Jung, Youngrok Park, Laura Schmid, Jaehyeong Jo, Dongkyu Lee, Bongsang Kim, Se-Young Yun, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00551">https://arxiv.org/abs/2411.00551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00551">https://arxiv.org/pdf/2411.00551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00551]] Conditional Synthesis of 3D Molecules with Time Correction Sampler(https://arxiv.org/abs/2411.00551)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable success in various domains, including molecular generation. However, conditional molecular generation remains a fundamental challenge due to an intrinsic trade-off between targeting specific chemical properties and generating meaningful samples from the data distribution. In this work, we present Time-Aware Conditional Synthesis (TACS), a novel approach to conditional generation on diffusion models. It integrates adaptively controlled plug-and-play "online" guidance into a diffusion model, driving samples toward the desired properties while maintaining validity and stability. A key component of our algorithm is our new type of diffusion sampler, Time Correction Sampler (TCS), which is used to control guidance and ensure that the generated molecules remain on the correct manifold at each reverse step of the diffusion process at the same time. Our proposed method demonstrates significant performance in conditional 3D molecular generation and offers a promising approach towards inverse molecular design, potentially facilitating advancements in drug discovery, materials science, and other related fields.</li>
</ul>

<h3>Title: $\alpha$-TCVAE: On the relationship between Disentanglement and Diversity</h3>
<ul>
<li><strong>Authors: </strong>Cristian Meo, Louis Mahon, Anirudh Goyal, Justin Dauwels</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00588">https://arxiv.org/abs/2411.00588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00588">https://arxiv.org/pdf/2411.00588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00588]] $\alpha$-TCVAE: On the relationship between Disentanglement and Diversity(https://arxiv.org/abs/2411.00588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While disentangled representations have shown promise in generative modeling and representation learning, their downstream usefulness remains debated. Recent studies re-defined disentanglement through a formal connection to symmetries, emphasizing the ability to reduce latent domains and consequently enhance generative capabilities. However, from an information theory viewpoint, assigning a complex attribute to a specific latent variable may be infeasible, limiting the applicability of disentangled representations to simple datasets. In this work, we introduce $\alpha$-TCVAE, a variational autoencoder optimized using a novel total correlation (TC) lower bound that maximizes disentanglement and latent variables informativeness. The proposed TC bound is grounded in information theory constructs, generalizes the $\beta$-VAE lower bound, and can be reduced to a convex combination of the known variational information bottleneck (VIB) and conditional entropy bottleneck (CEB) terms. Moreover, we present quantitative analyses that support the idea that disentangled representations lead to better generative capabilities and diversity. Additionally, we perform downstream task experiments from both representation and RL domains to assess our questions from a broader ML perspective. Our results demonstrate that $\alpha$-TCVAE consistently learns more disentangled representations than baselines and generates more diverse observations without sacrificing visual fidelity. Notably, $\alpha$-TCVAE exhibits marked improvements on MPI3D-Real, the most realistic disentangled dataset in our study, confirming its ability to represent complex datasets when maximizing the informativeness of individual variables. Finally, testing the proposed model off-the-shelf on a state-of-the-art model-based RL agent, Director, significantly shows $\alpha$-TCVAE downstream usefulness on the loconav Ant Maze task.</li>
</ul>

<h3>Title: Dual Low-Rank Adaptation for Continual Learning with Pre-Trained Models</h3>
<ul>
<li><strong>Authors: </strong>Huancheng Chen, Jingtao Li, Nidham Gazagnadou, Weiming Zhuang, Chen Chen, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00623">https://arxiv.org/abs/2411.00623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00623">https://arxiv.org/pdf/2411.00623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00623]] Dual Low-Rank Adaptation for Continual Learning with Pre-Trained Models(https://arxiv.org/abs/2411.00623)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In the era of foundation models, we revisit continual learning~(CL), which aims to enable vision transformers (ViTs) to learn new tasks over time. However, as the scale of these models increases, catastrophic forgetting remains a persistent challenge, particularly in the presence of significant domain shifts across tasks. Recent studies highlight a crossover between CL techniques and parameter-efficient fine-tuning (PEFT), which focuses on fine-tuning only a small set of trainable parameters to adapt to downstream tasks, such as low-rank adaptation (LoRA). While LoRA achieves faster convergence and requires fewer trainable parameters, it has seldom been explored in the context of continual learning. To address this gap, we propose a novel PEFT-CL method called Dual Low-Rank Adaptation (DualLoRA), which introduces both an orthogonal LoRA adapter and a residual LoRA adapter parallel to pre-trained weights in each layer. These components are orchestrated by a dynamic memory mechanism to strike a balance between stability and plasticity. The orthogonal LoRA adapter's parameters are updated in an orthogonal subspace of previous tasks to mitigate catastrophic forgetting, while the residual LoRA adapter's parameters are updated in the residual subspace spanned by task-specific bases without interaction across tasks, offering complementary capabilities for fine-tuning new tasks. On ViT-based models, we demonstrate that DualLoRA offers significant advantages in accuracy, inference speed, and memory efficiency over existing CL methods across multiple benchmarks.</li>
</ul>

<h3>Title: ZIM: Zero-Shot Image Matting for Anything</h3>
<ul>
<li><strong>Authors: </strong>Beomyoung Kim, Chanyong Shin, Joonhyun Jeong, Hyungsik Jung, Se-Yun Lee, Sewhan Chun, Dong-Hyun Hwang, Joonsang Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00626">https://arxiv.org/abs/2411.00626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00626">https://arxiv.org/pdf/2411.00626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00626]] ZIM: Zero-Shot Image Matting for Anything(https://arxiv.org/abs/2411.00626)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The recent segmentation foundation model, Segment Anything Model (SAM), exhibits strong zero-shot segmentation capabilities, but it falls short in generating fine-grained precise masks. To address this limitation, we propose a novel zero-shot image matting model, called ZIM, with two key contributions: First, we develop a label converter that transforms segmentation labels into detailed matte labels, constructing the new SA1B-Matte dataset without costly manual annotations. Training SAM with this dataset enables it to generate precise matte masks while maintaining its zero-shot capability. Second, we design the zero-shot matting model equipped with a hierarchical pixel decoder to enhance mask representation, along with a prompt-aware masked attention mechanism to improve performance by enabling the model to focus on regions specified by visual prompts. We evaluate ZIM using the newly introduced MicroMat-3K test set, which contains high-quality micro-level matte labels. Experimental results show that ZIM outperforms existing methods in fine-grained mask generation and zero-shot generalization. Furthermore, we demonstrate the versatility of ZIM in various downstream tasks requiring precise masks, such as image inpainting and 3D NeRF. Our contributions provide a robust foundation for advancing zero-shot matting and its downstream applications across a wide range of computer vision tasks. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Wasserstein Flow Matching: Generative modeling over families of distributions</h3>
<ul>
<li><strong>Authors: </strong>Doron Haviv, Aram-Alexandre Pooladian, Dana Pe'er, Brandon Amos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00698">https://arxiv.org/abs/2411.00698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00698">https://arxiv.org/pdf/2411.00698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00698]] Wasserstein Flow Matching: Generative modeling over families of distributions(https://arxiv.org/abs/2411.00698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative modeling typically concerns the transport of a single source distribution to a single target distribution by learning (i.e., regressing onto) simple probability flows. However, in modern data-driven fields such as computer graphics and single-cell genomics, samples (say, point-clouds) from datasets can themselves be viewed as distributions (as, say, discrete measures). In these settings, the standard generative modeling paradigm of flow matching would ignore the relevant geometry of the samples. To remedy this, we propose \emph{Wasserstein flow matching} (WFM), which appropriately lifts flow matching onto families of distributions by appealing to the Riemannian nature of the Wasserstein geometry. Our algorithm leverages theoretical and computational advances in (entropic) optimal transport, as well as the attention mechanism in our neural network architecture. We present two novel algorithmic contributions. First, we demonstrate how to perform generative modeling over Gaussian distributions, where we generate representations of granular cell states from single-cell genomics data. Secondly, we show that WFM can learn flows between high-dimensional and variable sized point-clouds and synthesize cellular microenvironments from spatial transcriptomics datasets. Code is available at [WassersteinFlowMatching](this https URL).</li>
</ul>

<h3>Title: B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable</h3>
<ul>
<li><strong>Authors: </strong>Shreyash Arya, Sukrut Rao, Moritz BÃ¶hle, Bernt Schiele</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00715">https://arxiv.org/abs/2411.00715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00715">https://arxiv.org/pdf/2411.00715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00715]] B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable(https://arxiv.org/abs/2411.00715)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>B-cos Networks have been shown to be effective for obtaining highly human interpretable explanations of model decisions by architecturally enforcing stronger alignment between inputs and weight. B-cos variants of convolutional networks (CNNs) and vision transformers (ViTs), which primarily replace linear layers with B-cos transformations, perform competitively to their respective standard variants while also yielding explanations that are faithful by design. However, it has so far been necessary to train these models from scratch, which is increasingly infeasible in the era of large, pre-trained foundation models. In this work, inspired by the architectural similarities in standard DNNs and B-cos networks, we propose 'B-cosification', a novel approach to transform existing pre-trained models to become inherently interpretable. We perform a thorough study of design choices to perform this conversion, both for convolutional neural networks and vision transformers. We find that B-cosification can yield models that are on par with B-cos models trained from scratch in terms of interpretability, while often outperforming them in terms of classification performance at a fraction of the training cost. Subsequently, we apply B-cosification to a pretrained CLIP model, and show that, even with limited data and compute cost, we obtain a B-cosified version that is highly interpretable and competitive on zero shot performance across a variety of datasets. We release our code and pre-trained model weights at this https URL.</li>
</ul>

<h3>Title: PedSleepMAE: Generative Model for Multimodal Pediatric Sleep Signals</h3>
<ul>
<li><strong>Authors: </strong>Saurav R. Pandey, Aaqib Saeed, Harlin Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00718">https://arxiv.org/abs/2411.00718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00718">https://arxiv.org/pdf/2411.00718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00718]] PedSleepMAE: Generative Model for Multimodal Pediatric Sleep Signals(https://arxiv.org/abs/2411.00718)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Pediatric sleep is an important but often overlooked area in health informatics. We present PedSleepMAE, a generative model that fully leverages multimodal pediatric sleep signals including multichannel EEGs, respiratory signals, EOGs and EMG. This masked autoencoder-based model performs comparably to supervised learning models in sleep scoring and in the detection of apnea, hypopnea, EEG arousal and oxygen desaturation. Its embeddings are also shown to capture subtle differences in sleep signals coming from a rare genetic disorder. Furthermore, PedSleepMAE generates realistic signals that can be used for sleep segment retrieval, outlier detection, and missing channel imputation. This is the first general-purpose generative model trained on multiple types of pediatric sleep signals.</li>
</ul>

<h3>Title: Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Aashiq Muhamed, Mona Diab, Virginia Smith</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00743">https://arxiv.org/abs/2411.00743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00743">https://arxiv.org/pdf/2411.00743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00743]] Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models(https://arxiv.org/abs/2411.00743)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and $L_0$ sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5\% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.</li>
</ul>

<h3>Title: Minibatch Optimal Transport and Perplexity Bound Estimation in Discrete Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Etrit Haxholli, Yeti Z. GÃ¼rbÃ¼z, OÄul Can, Eli Waxman</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00759">https://arxiv.org/abs/2411.00759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00759">https://arxiv.org/pdf/2411.00759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00759]] Minibatch Optimal Transport and Perplexity Bound Estimation in Discrete Flow Matching(https://arxiv.org/abs/2411.00759)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Outperforming autoregressive models on categorical data distributions, such as textual data, remains challenging for continuous diffusion and flow models. Discrete flow matching, a recent framework for modeling categorical data, has shown competitive performance with autoregressive models. Despite its similarities with continuous flow matching, the rectification strategy applied in the continuous version does not directly extend to the discrete one due to the inherent stochasticity of discrete paths. This limitation necessitates exploring alternative methods to minimize state transitions during generation. To address this, we propose a dynamic-optimal-transport-like minimization objective for discrete flows with convex interpolants and derive its equivalent Kantorovich formulation. The latter defines transport cost solely in terms of inter-state similarity and is optimized using a minibatch strategy. Another limitation we address in the discrete flow framework is model evaluation. Unlike continuous flows, wherein the instantaneous change of variables enables density estimation, discrete models lack a similar mechanism due to the inherent non-determinism and discontinuity of their paths. To alleviate this issue, we propose an upper bound on the perplexity of discrete flow models, enabling performance evaluation and comparison with other methods.</li>
</ul>

<h3>Title: Face Anonymization Made Simple</h3>
<ul>
<li><strong>Authors: </strong>Han-Wei Kung, Tuomas Varanka, Sanjay Saha, Terence Sim, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00762">https://arxiv.org/abs/2411.00762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00762">https://arxiv.org/pdf/2411.00762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00762]] Face Anonymization Made Simple(https://arxiv.org/abs/2411.00762)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current face anonymization techniques often depend on identity loss calculated by face recognition models, which can be inaccurate and unreliable. Additionally, many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast, our approach uses diffusion models with only a reconstruction loss, eliminating the need for facial landmarks or masks while still producing images with intricate, fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization, facial attribute preservation, and image quality. Beyond its primary function of anonymization, our model can also perform face swapping tasks by incorporating an additional facial image as input, demonstrating its versatility and potential for diverse applications. Our code and models are available at this https URL .</li>
</ul>

<h3>Title: GameGen-X: Interactive Open-world Game Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00769">https://arxiv.org/abs/2411.00769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00769">https://arxiv.org/pdf/2411.00769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00769]] GameGen-X: Interactive Open-world Game Video Generation(https://arxiv.org/abs/2411.00769)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>We introduce GameGen-X, the first diffusion transformer model specifically designed for both generating and interactively controlling open-world game videos. This model facilitates high-quality, open-domain generation by simulating an extensive array of game engine features, such as innovative characters, dynamic environments, complex actions, and diverse events. Additionally, it provides interactive controllability, predicting and altering future content based on the current clip, thus allowing for gameplay simulation. To realize this vision, we first collected and built an Open-World Video Game Dataset from scratch. It is the first and largest dataset for open-world game video generation and control, which comprises over a million diverse gameplay video clips sampling from over 150 games with informative captions from GPT-4o. GameGen-X undergoes a two-stage training process, consisting of foundation model pre-training and instruction tuning. Firstly, the model was pre-trained via text-to-video generation and video continuation, endowing it with the capability for long-sequence, high-quality open-domain game video generation. Further, to achieve interactive controllability, we designed InstructNet to incorporate game-related multi-modal control signal experts. This allows the model to adjust latent representations based on user inputs, unifying character interaction and scene content control for the first time in video generation. During instruction tuning, only the InstructNet is updated while the pre-trained foundation model is frozen, enabling the integration of interactive controllability without loss of diversity and quality of generated video content.</li>
</ul>

<h3>Title: Randomized Autoregressive Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, Liang-Chieh Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.00776">https://arxiv.org/abs/2411.00776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.00776">https://arxiv.org/pdf/2411.00776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.00776]] Randomized Autoregressive Visual Generation(https://arxiv.org/abs/2411.00776)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
