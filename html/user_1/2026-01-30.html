<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-01-30</h1>
<h3>Title: A generative machine learning model for designing metal hydrides applied to hydrogen storage</h3>
<ul>
<li><strong>Authors: </strong>Xiyuan Liu, Christian Hacker, Shengnian Wang, Yuhua Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20892">https://arxiv.org/abs/2601.20892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20892">https://arxiv.org/pdf/2601.20892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20892]] A generative machine learning model for designing metal hydrides applied to hydrogen storage(https://arxiv.org/abs/2601.20892)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing new metal hydrides is a critical step toward efficient hydrogen storage in carbon-neutral energy systems. However, existing materials databases, such as the Materials Project, contain a limited number of well-characterized hydrides, which constrains the discovery of optimal candidates. This work presents a framework that integrates causal discovery with a lightweight generative machine learning model to generate novel metal hydride candidates that may not exist in current databases. Using a dataset of 450 samples (270 training, 90 validation, and 90 testing), the model generates 1,000 candidates. After ranking and filtering, six previously unreported chemical formulas and crystal structures are identified, four of which are validated by density functional theory simulations and show strong potential for future experimental investigation. Overall, the proposed framework provides a scalable and time-efficient approach for expanding hydrogen storage datasets and accelerating materials discovery.</li>
</ul>

<h3>Title: TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins</h3>
<ul>
<li><strong>Authors: </strong>Nikita Makarov, Maria Bordukova, Lena Voith von Voithenberg, Estrella Pivel-Villanueva, Sabrina Mielke, Jonathan Wickes, Hanchen Wang, Mingyu Derek Ma, Keunwoo Choi, Kyunghyun Cho, Stephen Ra, Raul Rodriguez-Esteban, Fabian Schmich, Michael Menden</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20906">https://arxiv.org/abs/2601.20906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20906">https://arxiv.org/pdf/2601.20906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20906]] TwinWeaver: An LLM-Based Foundation Model Framework for Pan-Cancer Digital Twins(https://arxiv.org/abs/2601.20906)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Precision oncology requires forecasting clinical events and trajectories, yet modeling sparse, multi-modal clinical time series remains a critical challenge. We introduce TwinWeaver, an open-source framework that serializes longitudinal patient histories into text, enabling unified event prediction as well as forecasting with large language models, and use it to build Genie Digital Twin (GDT) on 93,054 patients across 20 cancer types. In benchmarks, GDT significantly reduces forecasting error, achieving a median Mean Absolute Scaled Error (MASE) of 0.87 compared to 0.97 for the strongest time-series baseline (p<0.001). Furthermore, GDT improves risk stratification, achieving an average concordance index (C-index) of 0.703 across survival, progression, and therapy switching tasks, surpassing the best baseline of 0.662. GDT also generalizes to out-of-distribution clinical trials, matching trained baselines at zero-shot and surpassing them with fine-tuning, achieving a median MASE of 0.75-0.88 and outperforming the strongest baseline in event prediction with an average C-index of 0.672 versus 0.648. Finally, TwinWeaver enables an interpretable clinical reasoning extension, providing a scalable and transparent foundation for longitudinal clinical modeling.</li>
</ul>

<h3>Title: MADE: Benchmark Environments for Closed-Loop Materials Discovery</h3>
<ul>
<li><strong>Authors: </strong>Shreshth A Malik, Tiarnan Doherty, Panagiotis Tigas, Muhammed Razzak, Stephen J. Roberts, Aron Walsh, Yarin Gal</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.20996">https://arxiv.org/abs/2601.20996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.20996">https://arxiv.org/pdf/2601.20996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.20996]] MADE: Benchmark Environments for Closed-Loop Materials Discovery(https://arxiv.org/abs/2601.20996)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing benchmarks for computational materials discovery primarily evaluate static predictive tasks or isolated computational sub-tasks. While valuable, these evaluations neglect the inherently iterative and adaptive nature of scientific discovery. We introduce MAterials Discovery Environments (MADE), a novel framework for benchmarking end-to-end autonomous materials discovery pipelines. MADE simulates closed-loop discovery campaigns in which an agent or algorithm proposes, evaluates, and refines candidate materials under a constrained oracle budget, capturing the sequential and resource-limited nature of real discovery workflows. We formalize discovery as a search for thermodynamically stable compounds relative to a given convex hull, and evaluate efficacy and efficiency via comparison to baseline algorithms. The framework is flexible; users can compose discovery agents from interchangeable components such as generative models, filters, and planners, enabling the study of arbitrary workflows ranging from fixed pipelines to fully agentic systems with tool use and adaptive decision making. We demonstrate this by conducting systematic experiments across a family of systems, enabling ablation of components in discovery pipelines, and comparison of how methods scale with system complexity.</li>
</ul>

<h3>Title: Conditional Denoising Model as a Physical Surrogate Model</h3>
<ul>
<li><strong>Authors: </strong>José Afonso, Pedro Viegas, Rodrigo Ventura, Vasco Guerra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.plasm-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21021">https://arxiv.org/abs/2601.21021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21021">https://arxiv.org/pdf/2601.21021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21021]] Conditional Denoising Model as a Physical Surrogate Model(https://arxiv.org/abs/2601.21021)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Surrogate modeling for complex physical systems typically faces a trade-off between data-fitting accuracy and physical consistency. Physics-consistent approaches typically treat physical laws as soft constraints within the loss function, a strategy that frequently fails to guarantee strict adherence to the governing equations, or rely on post-processing corrections that do not intrinsically learn the underlying solution geometry. To address these limitations, we introduce the {Conditional Denoising Model (CDM)}, a generative model designed to learn the geometry of the physical manifold itself. By training the network to restore clean states from noisy ones, the model learns a vector field that points continuously towards the valid solution subspace. We introduce a time-independent formulation that transforms inference into a deterministic fixed-point iteration, effectively projecting noisy approximations onto the equilibrium manifold. Validated on a low-temperature plasma physics and chemistry benchmark, the CDM achieves higher parameter and data efficiency than physics-consistent baselines. Crucially, we demonstrate that the denoising objective acts as a powerful implicit regularizer: despite never seeing the governing equations during training, the model adheres to physical constraints more strictly than baselines trained with explicit physics losses.</li>
</ul>

<h3>Title: AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples</h3>
<ul>
<li><strong>Authors: </strong>Andrea Camilloni (1), Chiara Micoli (1), Nita Mulliqi (2), Erik Everett Palm (1), Thorgerdur Palsdottir (1), Kelvin Szolnoky (1), Xiaoyi Ji (1), Sol Erika Boman (1 and 3), Andrea Discacciati (1), Henrik Grönberg (1), Lars Egevad (4), Tobias Nordström (1 and 5), Kimmo Kartasalo (2), Martin Eklund (1) ((1) Department of Medical Epidemiology and Biostatistics, Karolinska Institutet, Stockholm, Sweden, (2) Department of Medical Epidemiology and Biostatistics, SciLifeLab, Karolinska Institutet, Stockholm, Sweden, (3) Department of Molecular Medicine and Surgery, Karolinska Institutet, Stockholm, Sweden, (4) Department of Oncology and Pathology, Karolinska Institutet, Stockholm, Sweden, (5) Department of Clinical Sciences at Danderyd Hospital, Karolinska Institutet, Stockholm, Sweden)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21022">https://arxiv.org/abs/2601.21022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21022">https://arxiv.org/pdf/2601.21022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21022]] AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples(https://arxiv.org/abs/2601.21022)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.</li>
</ul>

<h3>Title: SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Zongheng Guo, Tao Chen, Yang Jiao, Yi Pan, Xiao Hu, Manuela Ferrario</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21031">https://arxiv.org/abs/2601.21031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21031">https://arxiv.org/pdf/2601.21031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21031]] SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model(https://arxiv.org/abs/2601.21031)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Current foundation model for photoplethysmography (PPG) signals is challenged by the intrinsic redundancy and noise of the signal. Standard masked modeling often yields trivial solutions while contrastive methods lack morphological precision. To address these limitations, we propose a Statistical-prior Informed Generative Masking Architecture (SIGMA-PPG), a generative foundation model featuring a Prior-Guided Adversarial Masking mechanism, where a reinforcement learning-driven teacher leverages statistical priors to create challenging learning paths that prevent overfitting to noise. We also incorporate a semantic consistency constraint via vector quantization to ensure that physiologically identical waveforms (even those altered by recording artifacts or minor perturbations) map to shared indices. This enhances codebook semantic density and eliminates redundant feature structures. Pre-trained on over 120,000 hours of data, SIGMA-PPG achieves superior average performance compared to five state-of-the-art baselines across 12 diverse downstream tasks. The code is available at this https URL.</li>
</ul>

<h3>Title: Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints</h3>
<ul>
<li><strong>Authors: </strong>Omer Rochman-Sharabi, Gilles Louppe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21033">https://arxiv.org/abs/2601.21033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21033">https://arxiv.org/pdf/2601.21033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21033]] Predict-Project-Renoise: Sampling Diffusion Models under Hard Constraints(https://arxiv.org/abs/2601.21033)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Neural emulators based on diffusion models show promise for scientific applications, but vanilla models cannot guarantee physical accuracy or constraint satisfaction. We address this by introducing a constrained sampling framework that enforces hard constraints, such as physical laws or observational consistency, at generation time. Our approach defines a constrained forward process that diffuses only over the feasible set of constraint-satisfying samples, inducing constrained marginal distributions. To reverse this, we propose Predict-Project-Renoise (PPR), an iterative algorithm that samples from the constrained marginals by alternating between denoising predictions, projecting onto the feasible set, and renoising. Experiments on 2D distributions, PDEs, and global weather forecasting demonstrate that PPR reduces constraint violations by over an order of magnitude while improving sample consistency and better matching the true constrained distribution compared to baselines.</li>
</ul>

<h3>Title: SMKC: Sketch Based Kernel Correlation Images for Variable Cardinality Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Haokun Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21050">https://arxiv.org/abs/2601.21050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21050">https://arxiv.org/pdf/2601.21050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21050]] SMKC: Sketch Based Kernel Correlation Images for Variable Cardinality Time Series Anomaly Detection(https://arxiv.org/abs/2601.21050)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Conventional anomaly detection in multivariate time series relies on the assumption that the set of observed variables remains static. In operational environments, however, monitoring systems frequently experience sensor churn. Signals may appear, disappear, or be renamed, creating data windows where the cardinality varies and may include values unseen during training. To address this challenge, we propose SMKC, a framework that decouples the dynamic input structure from the anomaly detector. We first employ permutation-invariant feature hashing to sketch raw inputs into a fixed size state sequence. We then construct a hybrid kernel image to capture global temporal structure through pairwise comparisons of the sequence and its derivatives. The model learns normal patterns using masked reconstruction and a teacher-student prediction objective. Our evaluation reveals that robust log-distance channels provide the primary discriminative signal, whereas cosine representations often fail to capture sufficient contrast. Notably, we find that a detector using random projections and nearest neighbors on the SMKC representation performs competitively with fully trained baselines without requiring gradient updates. This highlights the effectiveness of the representation itself and offers a practical cold-start solution for resource-constrained deployments.</li>
</ul>

<h3>Title: Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Larouche, Audrey Durand</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21061">https://arxiv.org/abs/2601.21061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21061">https://arxiv.org/pdf/2601.21061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21061]] Signal from Structure: Exploiting Submodular Upper Bounds in Generative Flow Networks(https://arxiv.org/abs/2601.21061)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets; GFNs) are a class of generative models that learn to sample compositional objects proportionally to their a priori unknown value, their reward. We focus on the case where the reward has a specified, actionable structure, namely that it is submodular. We show submodularity can be harnessed to retrieve upper bounds on the reward of compositional objects that have not yet been observed. We provide in-depth analyses of the probability of such bounds occurring, as well as how many unobserved compositional objects can be covered by a bound. Following the Optimism in the Face of Uncertainty principle, we then introduce SUBo-GFN, which uses the submodular upper bounds to train a GFN. We show that SUBo-GFN generates orders of magnitude more training data than classical GFNs for the same number of queries to the reward function. We demonstrate the effectiveness of SUBo-GFN in terms of distribution matching and high-quality candidate generation on synthetic and real-world submodular tasks.</li>
</ul>

<h3>Title: Out-of-Distribution Generalization in Graph Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyang Li, Haibo Chen, Xin Wang, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21067">https://arxiv.org/abs/2601.21067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21067">https://arxiv.org/pdf/2601.21067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21067]] Out-of-Distribution Generalization in Graph Foundation Models(https://arxiv.org/abs/2601.21067)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Graphs are a fundamental data structure for representing relational information in domains such as social networks, molecular systems, and knowledge graphs. However, graph learning models often suffer from limited generalization when applied beyond their training distributions. In practice, distribution shifts may arise from changes in graph structure, domain semantics, available modalities, or task formulations. To address these challenges, graph foundation models (GFMs) have recently emerged, aiming to learn general-purpose representations through large-scale pretraining across diverse graphs and tasks. In this survey, we review recent progress on GFMs from the perspective of out-of-distribution (OOD) generalization. We first discuss the main challenges posed by distribution shifts in graph learning and outline a unified problem setting. We then organize existing approaches based on whether they are designed to operate under a fixed task specification or to support generalization across heterogeneous task formulations, and summarize the corresponding OOD handling strategies and pretraining objectives. Finally, we review common evaluation protocols and discuss open directions for future research. To the best of our knowledge, this paper is the first survey for OOD generalization in GFMs.</li>
</ul>

<h3>Title: Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Yu Huo, Siyu Zhang, Kun Zeng, Haoyue Liu, Owen Lee, Junlin Chen, Yuquan Lu, Yifu Guo, Yaodong Liang, Xiaoying Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21081">https://arxiv.org/abs/2601.21081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21081">https://arxiv.org/pdf/2601.21081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21081]] Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought(https://arxiv.org/abs/2601.21081)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at this https URL. The SoT-26K dataset will be released upon acceptance.</li>
</ul>

<h3>Title: Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations</h3>
<ul>
<li><strong>Authors: </strong>Amit Meghanani, Thomas Hain</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21084">https://arxiv.org/abs/2601.21084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21084">https://arxiv.org/pdf/2601.21084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21084]] Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations(https://arxiv.org/abs/2601.21084)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.</li>
</ul>

<h3>Title: MapPFN: Learning Causal Perturbation Maps in Context</h3>
<ul>
<li><strong>Authors: </strong>Marvin Sextro, Weronika Kłos, Gabriel Dernbach</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21092">https://arxiv.org/abs/2601.21092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21092">https://arxiv.org/pdf/2601.21092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21092]] MapPFN: Learning Causal Perturbation Maps in Context(https://arxiv.org/abs/2601.21092)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Planning effective interventions in biological systems requires treatment-effect models that adapt to unseen biological contexts by identifying their specific underlying mechanisms. Yet single-cell perturbation datasets span only a handful of biological contexts, and existing methods cannot leverage new interventional evidence at inference time to adapt beyond their training data. To meta-learn a perturbation effect estimator, we present MapPFN, a prior-data fitted network (PFN) pretrained on synthetic data generated from a prior over causal perturbations. Given a set of experiments, MapPFN uses in-context learning to predict post-perturbation distributions, without gradient-based optimization. Despite being pretrained on in silico gene knockouts alone, MapPFN identifies differentially expressed genes, matching the performance of models trained on real single-cell data. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement</h3>
<ul>
<li><strong>Authors: </strong>Maria Despoina Siampou, Shushman Choudhury, Shang-Ling Hsu, Neha Arora, Cyrus Shahabi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21149">https://arxiv.org/abs/2601.21149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21149">https://arxiv.org/pdf/2601.21149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21149]] Mobility-Embedded POIs: Learning What A Place Is and How It Is Used from Human Movement(https://arxiv.org/abs/2601.21149)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent progress in geospatial foundation models highlights the importance of learning general-purpose representations for real-world locations, particularly points-of-interest (POIs) where human activity concentrates. Existing approaches, however, focus primarily on place identity derived from static textual metadata, or learn representations tied to trajectory context, which capture movement regularities rather than how places are actually used (i.e., POI's function). We argue that POI function is a missing but essential signal for general POI representations. We introduce Mobility-Embedded POIs (ME-POIs), a framework that augments POI embeddings derived, from language models with large-scale human mobility data to learn POI-centric, context-independent representations grounded in real-world usage. ME-POIs encodes individual visits as temporally contextualized embeddings and aligns them with learnable POI representations via contrastive learning to capture usage patterns across users and time. To address long-tail sparsity, we propose a novel mechanism that propagates temporal visit patterns from nearby, frequently visited POIs across multiple spatial scales. We evaluate ME-POIs on five newly proposed map enrichment tasks, testing its ability to capture both the identity and function of POIs. Across all tasks, augmenting text-based embeddings with ME-POIs consistently outperforms both text-only and mobility-only baselines. Notably, ME-POIs trained on mobility data alone can surpass text-only models on certain tasks, highlighting that POI function is a critical component of accurate and generalizable POI representations.</li>
</ul>

<h3>Title: Learning to Advect: A Neural Semi-Lagrangian Architecture for Weather Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Carlos A. Pereira, Stéphane Gaudreault, Valentin Dallerit, Christopher Subich, Shoyon Panday, Siqi Wei, Sasa Zhang, Siddharth Rout, Eldad Haber, Raymond J. Spiteri, David Millard, Emilia Diaconescu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21151">https://arxiv.org/abs/2601.21151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21151">https://arxiv.org/pdf/2601.21151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21151]] Learning to Advect: A Neural Semi-Lagrangian Architecture for Weather Forecasting(https://arxiv.org/abs/2601.21151)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent machine-learning approaches to weather forecasting often employ a monolithic architecture, where distinct physical mechanisms (advection, transport), diffusion-like mixing, thermodynamic processes, and forcing are represented implicitly within a single large network. This representation is particularly problematic for advection, where long-range transport must be treated with expensive global interaction mechanisms or through deep, stacked convolutional layers. To mitigate this, we present PARADIS, a physics-inspired global weather prediction model that imposes inductive biases on network behavior through a functional decomposition into advection, diffusion, and reaction blocks acting on latent variables. We implement advection through a Neural Semi-Lagrangian operator that performs trajectory-based transport via differentiable interpolation on the sphere, enabling end-to-end learning of both the latent modes to be transported and their characteristic trajectories. Diffusion-like processes are modeled through depthwise-separable spatial mixing, while local source terms and vertical interactions are modeled via pointwise channel interactions, enabling operator-level physical structure. PARADIS provides state-of-the-art forecast skill at a fraction of the training cost. On ERA5-based benchmarks, the 1 degree PARADIS model, with a total training cost of less than a GPU month, meets or exceeds the performance of 0.25 degree traditional and machine-learning baselines, including the ECMWF HRES forecast and DeepMind's GraphCast.</li>
</ul>

<h3>Title: Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery</h3>
<ul>
<li><strong>Authors: </strong>Jianzheng Wang, Huan Ni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21159">https://arxiv.org/abs/2601.21159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21159">https://arxiv.org/pdf/2601.21159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21159]] Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery(https://arxiv.org/abs/2601.21159)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: this https URL.</li>
</ul>

<h3>Title: AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Kamal Berahmand, Saman Forouzandeh, Mehrnoush Mohammadi, Parham Moradi, Mahdi Jalili</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21171">https://arxiv.org/abs/2601.21171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21171">https://arxiv.org/pdf/2601.21171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21171]] AC2L-GAD: Active Counterfactual Contrastive Learning for Graph Anomaly Detection(https://arxiv.org/abs/2601.21171)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Graph anomaly detection aims to identify abnormal patterns in networks, but faces significant challenges from label scarcity and extreme class imbalance. While graph contrastive learning offers a promising unsupervised solution, existing methods suffer from two critical limitations: random augmentations break semantic consistency in positive pairs, while naive negative sampling produces trivial, uninformative contrasts. We propose AC2L-GAD, an Active Counterfactual Contrastive Learning framework that addresses both limitations through principled counterfactual reasoning. By combining information-theoretic active selection with counterfactual generation, our approach identifies structurally complex nodes and generates anomaly-preserving positive augmentations alongside normal negative counterparts that provide hard contrasts, while restricting expensive counterfactual generation to a strategically selected subset. This design reduces computational overhead by approximately 65% compared to full-graph counterfactual generation while maintaining detection quality. Experiments on nine benchmark datasets, including real-world financial transaction graphs from GADBench, show that AC2L-GAD achieves competitive or superior performance compared to state-of-the-art baselines, with notable gains in datasets where anomalies exhibit complex attribute-structure interactions.</li>
</ul>

<h3>Title: Breaking the Reasoning Horizon in Entity Alignment Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yuanning Cui, Zequn Sun, Wei Hu, Kexuan Xin, Zhangjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21174">https://arxiv.org/abs/2601.21174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21174">https://arxiv.org/pdf/2601.21174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21174]] Breaking the Reasoning Horizon in Entity Alignment Foundation Models(https://arxiv.org/abs/2601.21174)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Entity alignment (EA) is critical for knowledge graph (KG) fusion. Existing EA models lack transferability and are incapable of aligning unseen KGs without retraining. While using graph foundation models (GFMs) offer a solution, we find that directly adapting GFMs to EA remains largely ineffective. This stems from a critical "reasoning horizon gap": unlike link prediction in GFMs, EA necessitates capturing long-range dependencies across sparse and heterogeneous KG structuresTo address this challenge, we propose a EA foundation model driven by a parallel encoding strategy. We utilize seed EA pairs as local anchors to guide the information flow, initializing and encoding two parallel streams simultaneously. This facilitates anchor-conditioned message passing and significantly shortens the inference trajectory by leveraging local structural proximity instead of global search. Additionally, we incorporate a merged relation graph to model global dependencies and a learnable interaction module for precise matching. Extensive experiments verify the effectiveness of our framework, highlighting its strong generalizability to unseen KGs.</li>
</ul>

<h3>Title: Enhancing Underwater Light Field Images via Global Geometry-aware Diffusion Process</h3>
<ul>
<li><strong>Authors: </strong>Yuji Lin, Qian Zhao, Zongsheng Yue, Junhui Hou, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21179">https://arxiv.org/abs/2601.21179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21179">https://arxiv.org/pdf/2601.21179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21179]] Enhancing Underwater Light Field Images via Global Geometry-aware Diffusion Process(https://arxiv.org/abs/2601.21179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work studies the challenging problem of acquiring high-quality underwater images via 4-D light field (LF) imaging. To this end, we propose GeoDiff-LF, a novel diffusion-based framework built upon SD-Turbo to enhance underwater 4-D LF imaging by leveraging its spatial-angular structure. GeoDiff-LF consists of three key adaptations: (1) a modified U-Net architecture with convolutional and attention adapters to model geometric cues, (2) a geometry-guided loss function using tensor decomposition and progressive weighting to regularize global structure, and (3) an optimized sampling strategy with noise prediction to improve efficiency. By integrating diffusion priors and LF geometry, GeoDiff-LF effectively mitigates color distortion in underwater scenes. Extensive experiments demonstrate that our framework outperforms existing methods across both visual fidelity and quantitative performance, advancing the state-of-the-art in enhancing underwater imaging. The code will be publicly available at this https URL.</li>
</ul>

<h3>Title: Rethinking Refinement: Correcting Generative Bias without Noise Injection</h3>
<ul>
<li><strong>Authors: </strong>Xin Peng, Ang Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21182">https://arxiv.org/abs/2601.21182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21182">https://arxiv.org/pdf/2601.21182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21182]] Rethinking Refinement: Correcting Generative Bias without Noise Injection(https://arxiv.org/abs/2601.21182)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models, including diffusion and flow-based models, often exhibit systematic biases that degrade sample quality, particularly in high-dimensional settings. We revisit refinement methods and show that effective bias correction can be achieved as a post-hoc procedure, without noise injection or multi-step resampling of the sampling process. We propose a flow-matching-based \textbf{Bi-stage Flow Refinement (BFR)} framework with two refinement strategies operating at different stages: latent space alignment for approximately invertible generators and data space refinement trained with lightweight augmentations. Unlike previous refiners that perturb sampling dynamics, BFR preserves the original ODE trajectory and applies deterministic corrections to generated samples. Experiments on MNIST, CIFAR-10, and FFHQ at 256x256 resolution demonstrate consistent improvements in fidelity and coverage; notably, starting from base samples with FID 3.95, latent space refinement achieves a \textbf{state-of-the-art} FID of \textbf{1.46} on MNIST using only a single additional function evaluation (1-NFE), while maintaining sample diversity.</li>
</ul>

<h3>Title: Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Zecheng Zhao, Zhi Chen, Zi Huang, Shazia Sadiq, Tong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21193">https://arxiv.org/abs/2601.21193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21193">https://arxiv.org/pdf/2601.21193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21193]] Generative Recall, Dense Reranking: Learning Multi-View Semantic IDs for Efficient Text-to-Video Retrieval(https://arxiv.org/abs/2601.21193)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-Video Retrieval (TVR) is essential in video platforms. Dense retrieval with dual-modality encoders leads in accuracy, but its computation and storage scale poorly with corpus size. Thus, real-time large-scale applications adopt two-stage retrieval, where a fast recall model gathers a small candidate pool, which is reranked by an advanced dense retriever. Due to hugely reduced candidates, the reranking model can use any off-the-shelf dense retriever without hurting efficiency, meaning the recall model bounds two-stage TVR performance. Recently, generative retrieval (GR) replaces dense video embeddings with discrete semantic IDs and retrieves by decoding text queries into ID tokens. GR offers near-constant inference and storage complexity, and its semantic IDs capture high-level video features via quantization, making it ideal for quickly eliminating irrelevant candidates during recall. However, as a recall model in two-stage TVR, GR suffers from (i) semantic ambiguity, where each video satisfies diverse queries but is forced into one semantic ID; and (ii) cross-modal misalignment, as semantic IDs are solely derived from visual features without text supervision. We propose Generative Recall and Dense Reranking (GRDR), designing a novel GR method to uplift recalled candidate quality. GRDR assigns multiple semantic IDs to each video using a query-guided multi-view tokenizer exposing diverse semantic access paths, and jointly trains the tokenizer and generative retriever via a shared codebook to cast semantic IDs as the semantic bridge between texts and videos. At inference, trie-constrained decoding generates a compact candidate set reranked by a dense model for fine-grained matching. Experiments on TVR benchmarks show GRDR matches strong dense retrievers in accuracy while reducing index storage by an order of magnitude and accelerating up to 300$\times$ in full-corpus retrieval.</li>
</ul>

<h3>Title: Thinker: A vision-language foundation model for embodied intelligence</h3>
<ul>
<li><strong>Authors: </strong>Baiyu Pan, Daqin Luo, Junpeng Yang, Jiyuan Wang, Yixuan Zhang, Hailin Shi, Jichao Jiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21199">https://arxiv.org/abs/2601.21199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21199">https://arxiv.org/pdf/2601.21199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21199]] Thinker: A vision-language foundation model for embodied intelligence(https://arxiv.org/abs/2601.21199)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model's capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.</li>
</ul>

<h3>Title: A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models</h3>
<ul>
<li><strong>Authors: </strong>Chuan-Shen Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.AT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21207">https://arxiv.org/abs/2601.21207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21207">https://arxiv.org/pdf/2601.21207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21207]] A Sheaf-Theoretic and Topological Perspective on Complex Network Modeling and Attention Mechanisms in Graph Neural Models(https://arxiv.org/abs/2601.21207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Combinatorial and topological structures, such as graphs, simplicial complexes, and cell complexes, form the foundation of geometric and topological deep learning (GDL and TDL) architectures. These models aggregate signals over such domains, integrate local features, and generate representations for diverse real-world applications. However, the distribution and diffusion behavior of GDL and TDL features during training remains an open and underexplored problem. Motivated by this gap, we introduce a cellular sheaf theoretic framework for modeling and analyzing the local consistency and harmonicity of node features and edge weights in graph-based architectures. By tracking local feature alignments and agreements through sheaf structures, the framework offers a topological perspective on feature diffusion and aggregation. Furthermore, a multiscale extension inspired by topological data analysis (TDA) is proposed to capture hierarchical feature interactions in graph models. This approach enables a joint characterization of GDL and TDL architectures based on their underlying geometric and topological structures and the learned signals defined on them, providing insights for future studies on conventional tasks such as node classification, substructure detection, and community detection.</li>
</ul>

<h3>Title: PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations</h3>
<ul>
<li><strong>Authors: </strong>Kaiyuan Tan, Kendra Givens, Peilun Li, Thomas Beckers</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21234">https://arxiv.org/abs/2601.21234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21234">https://arxiv.org/pdf/2601.21234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21234]] PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations(https://arxiv.org/abs/2601.21234)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models provide expressive priors for forecasting trajectories of dynamical systems, but are typically unreliable in the sparse data regime. Physics-informed machine learning (PIML) improves reliability in such settings; however, most methods require \emph{explicit governing equations} during training, which are often only partially known due to complex and nonlinear dynamics. We introduce \textbf{PHDME}, a port-Hamiltonian diffusion framework designed for \emph{sparse observations} and \emph{incomplete physics}. PHDME leverages port-Hamiltonian structural prior but does not require full knowledge of the closed-form governing equations. Our approach first trains a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) on limited observations to capture an energy-based representation of the dynamics. The GP-dPHS is then used to generate a physically consistent artificial dataset for diffusion training, and to inform the diffusion model with a structured physics residual loss. After training, the diffusion model acts as an amortized sampler and forecaster for fast trajectory generation. Finally, we apply split conformal calibration to provide uncertainty statements for the generated predictions. Experiments on PDE benchmarks and a real-world spring system show improved accuracy and physical consistency under data scarcity.</li>
</ul>

<h3>Title: PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Xuewen Liu, Zhikai Li, Jing Zhang, Mengjuan Chen, Qingyi Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21238">https://arxiv.org/abs/2601.21238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21238">https://arxiv.org/pdf/2601.21238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21238]] PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models(https://arxiv.org/abs/2601.21238)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>AutoRegressive Visual Generation (ARVG) models retain an architecture compatible with language models, while achieving performance comparable to diffusion-based models. Quantization is commonly employed in neural networks to reduce model size and computational latency. However, applying quantization to ARVG remains largely underexplored, and existing quantization methods fail to generalize effectively to ARVG models. In this paper, we explore this issue and identify three key challenges: (1) severe outliers at channel-wise level, (2) highly dynamic activations at token-wise level, and (3) mismatched distribution information at sample-wise level. To these ends, we propose PTQ4ARVG, a training-free post-training quantization (PTQ) framework consisting of: (1) Gain-Projected Scaling (GPS) mitigates the channel-wise outliers, which expands the quantization loss via a Taylor series to quantify the gain of scaling for activation-weight quantization, and derives the optimal scaling factor through differentiation.(2) Static Token-Wise Quantization (STWQ) leverages the inherent properties of ARVG, fixed token length and position-invariant distribution across samples, to address token-wise variance without incurring dynamic calibration overhead.(3) Distribution-Guided Calibration (DGC) selects samples that contribute most to distributional entropy, eliminating the sample-wise distribution mismatch. Extensive experiments show that PTQ4ARVG can effectively quantize the ARVG family models to 8-bit and 6-bit while maintaining competitive performance. Code is available at this http URL .</li>
</ul>

<h3>Title: Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks</h3>
<ul>
<li><strong>Authors: </strong>Luwei Sun, Dongrui Shen, Jianfe Li, Yulong Zhao, Han Feng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21242">https://arxiv.org/abs/2601.21242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21242">https://arxiv.org/pdf/2601.21242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21242]] Understanding Diffusion Models via Ratio-Based Function Approximation with SignReLU Networks(https://arxiv.org/abs/2601.21242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Motivated by challenges in conditional generative modeling, where the target conditional density takes the form of a ratio f1 over f2, this paper develops a theoretical framework for approximating such ratio-type functionals. Here, f1 and f2 are kernel-based marginal densities that capture structured interactions, a setting central to diffusion-based generative models. We provide a concise proof for approximating these ratio-type functionals using deep neural networks with the SignReLU activation function, leveraging the activation's piecewise structure. Under standard regularity assumptions, we establish L^p(Omega) approximation bounds and convergence rates. Specializing to Denoising Diffusion Probabilistic Models (DDPMs), we construct a SignReLU-based neural estimator for the reverse process and derive bounds on the excess Kullback-Leibler (KL) risk between the generated and true data distributions. Our analysis decomposes this excess risk into approximation and estimation error components. These results provide generalization guarantees for finite-sample training of diffusion-based generative models.</li>
</ul>

<h3>Title: Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences</h3>
<ul>
<li><strong>Authors: </strong>Namkyung Yoon, Sanghong Kim, Hwangnam Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21246">https://arxiv.org/abs/2601.21246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21246">https://arxiv.org/pdf/2601.21246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21246]] Conditional Generative Framework with Peak-Aware Attention for Robust Chemical Detection under Interferences(https://arxiv.org/abs/2601.21246)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Gas chromatography-mass spectrometry (GC-MS) is a widely used analytical method for chemical substance detection, but measurement reliability tends to deteriorate in the presence of interfering substances. In particular, interfering substances cause nonspecific peaks, residence time shifts, and increased background noise, resulting in reduced sensitivity and false alarms. To overcome these challenges, in this paper, we propose an artificial intelligence discrimination framework based on a peak-aware conditional generative model to improve the reliability of GC-MS measurements under interference conditions. The framework is learned with a novel peak-aware mechanism that highlights the characteristic peaks of GC-MS data, allowing it to generate important spectral features more faithfully. In addition, chemical and solvent information is encoded in a latent vector embedded with it, allowing a conditional generative adversarial neural network (CGAN) to generate a synthetic GC-MS signal consistent with the experimental conditions. This generates an experimental dataset that assumes indirect substance situations in chemical substance data, where acquisition is limited without conducting real experiments. These data are used for the learning of AI-based GC-MS discrimination models to help in accurate chemical substance discrimination. We conduct various quantitative and qualitative evaluations of the generated simulated data to verify the validity of the proposed framework. We also verify how the generative model improves the performance of the AI discrimination framework. Representatively, the proposed method is shown to consistently achieve cosine similarity and Pearson correlation coefficient values above 0.9 while preserving peak number diversity and reducing false alarms in the discrimination model.</li>
</ul>

<h3>Title: NFCDS: A Plug-and-Play Noise Frequency-Controlled Diffusion Sampling Strategy for Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zhen Wang, Hongyi Liu, Jianing Li, Zhihui Wei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21248">https://arxiv.org/abs/2601.21248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21248">https://arxiv.org/pdf/2601.21248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21248]] NFCDS: A Plug-and-Play Noise Frequency-Controlled Diffusion Sampling Strategy for Image Restoration(https://arxiv.org/abs/2601.21248)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion sampling-based Plug-and-Play (PnP) methods produce images with high perceptual quality but often suffer from reduced data fidelity, primarily due to the noise introduced during reverse diffusion. To address this trade-off, we propose Noise Frequency-Controlled Diffusion Sampling (NFCDS), a spectral modulation mechanism for reverse diffusion noise. We show that the fidelity-perception conflict can be fundamentally understood through noise frequency: low-frequency components induce blur and degrade fidelity, while high-frequency components drive detail generation. Based on this insight, we design a Fourier-domain filter that progressively suppresses low-frequency noise and preserves high-frequency content. This controlled refinement injects a data-consistency prior directly into sampling, enabling fast convergence to results that are both high-fidelity and perceptually convincing--without additional training. As a PnP module, NFCDS seamlessly integrates into existing diffusion-based restoration frameworks and improves the fidelity-perception balance across diverse zero-shot tasks.</li>
</ul>

<h3>Title: Lossless Copyright Protection via Intrinsic Model Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Chen, Liqin Wang, Wei Lu, Xiangyang Luo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21252">https://arxiv.org/abs/2601.21252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21252">https://arxiv.org/pdf/2601.21252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21252]] Lossless Copyright Protection via Intrinsic Model Fingerprinting(https://arxiv.org/abs/2601.21252)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The exceptional performance of diffusion models establishes them as high-value intellectual property but exposes them to unauthorized replication. Existing protection methods either modify the model to embed watermarks, which impairs performance, or extract model fingerprints by manipulating the denoising process, rendering them incompatible with black-box APIs. In this paper, we propose TrajPrint, a completely lossless and training-free framework that verifies model copyright by extracting unique manifold fingerprints formed during deterministic generation. Specifically, we first utilize a watermarked image as an anchor and exactly trace the path back to its trajectory origin, effectively locking the model fingerprint mapped by this path. Subsequently, we implement a joint optimization strategy that employs dual-end anchoring to synthesize a specific fingerprint noise, which strictly adheres to the target manifold for robust watermark recovery. As input, it enables the protected target model to recover the watermarked image, while failing on non-target models. Finally, we achieved verification via atomic inference and statistical hypothesis testing. Extensive experiments demonstrate that TrajPrint achieves lossless verification in black-box API scenarios with superior robustness against model modifications.</li>
</ul>

<h3>Title: Hypersolid: Emergent Vision Representations via Short-Range Repulsion</h3>
<ul>
<li><strong>Authors: </strong>Esteban Rodríguez-Betancourt, Edgar Casasola-Murillo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21255">https://arxiv.org/abs/2601.21255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21255">https://arxiv.org/pdf/2601.21255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21255]] Hypersolid: Emergent Vision Representations via Short-Range Repulsion(https://arxiv.org/abs/2601.21255)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.</li>
</ul>

<h3>Title: CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Huo, Yu Huang, Yibo Yan, Ye Pan, Yi Cao, Mingdong Ou, Philip S. Yu, Xuming Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21262">https://arxiv.org/abs/2601.21262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21262">https://arxiv.org/pdf/2601.21262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21262]] CausalEmbed: Auto-Regressive Multi-Vector Generation in Latent Space for Visual Document Embedding(https://arxiv.org/abs/2601.21262)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although Multimodal Large Language Models (MLLMs) have shown remarkable potential in Visual Document Retrieval (VDR) through generating high-quality multi-vector embeddings, the substantial storage overhead caused by representing a page with thousands of visual tokens limits their practicality in real-world applications. To address this challenge, we propose an auto-regressive generation approach, CausalEmbed, for constructing multi-vector embeddings. By incorporating iterative margin loss during contrastive training, CausalEmbed encourages the embedding models to learn compact and well-structured representations. Our method enables efficient VDR tasks using only dozens of visual tokens, achieving a 30-155x reduction in token count while maintaining highly competitive performance across various backbones and benchmarks. Theoretical analysis and empirical results demonstrate the unique advantages of auto-regressive embedding generation in terms of training efficiency and scalability at test time. As a result, CausalEmbed introduces a flexible test-time scaling strategy for multi-vector VDR representations and sheds light on the generative paradigm within multimodal document retrieval.</li>
</ul>

<h3>Title: WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models</h3>
<ul>
<li><strong>Authors: </strong>Rishi Upadhyay, Howard Zhang, Jim Solomon, Ayush Agrawal, Pranay Boreddy, Shruti Satya Narayana, Yunhao Ba, Alex Wong, Celso M de Melo, Achuta Kadambi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21282">https://arxiv.org/abs/2601.21282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21282">https://arxiv.org/pdf/2601.21282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21282]] WorldBench: Disambiguating Physics for Diagnostic Evaluation of World Models(https://arxiv.org/abs/2601.21282)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative foundational models, often termed "world models," have propelled interest in applying them to critical tasks like robotic planning and autonomous system training. For reliable deployment, these models must exhibit high physical fidelity, accurately simulating real-world dynamics. Existing physics-based video benchmarks, however, suffer from entanglement, where a single test simultaneously evaluates multiple physical laws and concepts, fundamentally limiting their diagnostic capability. We introduce WorldBench, a novel video-based benchmark specifically designed for concept-specific, disentangled evaluation, allowing us to rigorously isolate and assess understanding of a single physical concept or law at a time. To make WorldBench comprehensive, we design benchmarks at two different levels: 1) an evaluation of intuitive physical understanding with concepts such as object permanence or scale/perspective, and 2) an evaluation of low-level physical constants and material properties such as friction coefficients or fluid viscosity. When SOTA video-based world models are evaluated on WorldBench, we find specific patterns of failure in particular physics concepts, with all tested models lacking the physical consistency required to generate reliable real-world interactions. Through its concept-specific evaluation, WorldBench offers a more nuanced and scalable framework for rigorously evaluating the physical reasoning capabilities of video generation and world models, paving the way for more robust and generalizable world-model-driven learning.</li>
</ul>

<h3>Title: DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher</h3>
<ul>
<li><strong>Authors: </strong>Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21283">https://arxiv.org/abs/2601.21283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21283">https://arxiv.org/pdf/2601.21283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21283]] DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher(https://arxiv.org/abs/2601.21283)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.</li>
</ul>

<h3>Title: PILD: Physics-Informed Learning via Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Zeng, Tianyi Wang, Jiaru Zhang, Zimo Zeng, Feiyang Zhang, Yiming Xu, Sikai Chen, Yajie Zou, Yangyang Wang, Junfeng Jiao, Christian Claudel, Xinbo Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, math.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21284">https://arxiv.org/abs/2601.21284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21284">https://arxiv.org/pdf/2601.21284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21284]] PILD: Physics-Informed Learning via Diffusion(https://arxiv.org/abs/2601.21284)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative tools for modeling complex data distributions, yet their purely data-driven nature limits applicability in practical engineering and scientific problems where physical laws need to be followed. This paper proposes Physics-Informed Learning via Diffusion (PILD), a framework that unifies diffusion modeling and first-principles physical constraints by introducing a virtual residual observation sampled from a Laplace distribution to supervise generation during training. To further integrate physical laws, a conditional embedding module is incorporated to inject physical information into the denoising network at multiple layers, ensuring consistent guidance throughout the diffusion process. The proposed PILD framework is concise, modular, and broadly applicable to problems governed by ordinary differential equations, partial differential equations, as well as algebraic equations or inequality constraints. Extensive experiments across engineering and scientific tasks including estimating vehicle trajectories, tire forces, Darcy flow and plasma dynamics, demonstrate that our PILD substantially improves accuracy, stability, and generalization over existing physics-informed and diffusion-based baselines.</li>
</ul>

<h3>Title: Do Pathology Foundation Models Encode Disease Progression? A Pseudotime Analysis of Visual Representations</h3>
<ul>
<li><strong>Authors: </strong>Pritika Vig (1 and 2), Ren-Chin Wu (3), William Lotter (2, 4 and 5) ((1) Massachusetts Institute of Technology, (2) Department of Data Science, Dana-Farber Cancer Institute, (3) Department of Pathology, Dana-Farber Cancer Institute, (4) Brigham and Women's Hospital, (5) Harvard Medical School)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21334">https://arxiv.org/abs/2601.21334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21334">https://arxiv.org/pdf/2601.21334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21334]] Do Pathology Foundation Models Encode Disease Progression? A Pseudotime Analysis of Visual Representations(https://arxiv.org/abs/2601.21334)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Vision foundation models trained on discretely sampled images achieve strong performance on classification benchmarks, yet whether their representations encode the continuous processes underlying their training data remains unclear. This question is especially pertinent in computational pathology, where we posit that models whose latent representations implicitly capture continuous disease progression may better reflect underlying biology, support more robust generalization, and enable quantitative analyses of features associated with disease transitions. Using diffusion pseudotime, a method developed to infer developmental trajectories from single-cell transcriptomics, we probe whether foundation models organize disease states along coherent progression directions in representation space. Across four cancer progressions and six models, we find that all pathology-specific models recover trajectory orderings significantly exceeding null baselines, with vision-only models achieving the highest fidelities $(\tau > 0.78$ on CRC-Serrated). Model rankings by trajectory fidelity on reference diseases strongly predict few-shot classification performance on held-out diseases ($\rho = 0.92$), and exploratory analysis shows cell-type composition varies smoothly along inferred trajectories in patterns consistent with known stromal remodeling. Together, these results demonstrate that vision foundation models can implicitly learn to represent continuous processes from independent static observations, and that trajectory fidelity provides a complementary measure of representation quality beyond downstream performance. While demonstrated in pathology, this framework could be applied to other domains where continuous processes are observed through static snapshots.</li>
</ul>

<h3>Title: Qwen3-ASR Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Xian Shi, Xiong Wang, Zhifang Guo, Yongqi Wang, Pei Zhang, Xinyu Zhang, Zishan Guo, Hongkun Hao, Yu Xi, Baosong Yang, Jin Xu, Jingren Zhou, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21337">https://arxiv.org/abs/2601.21337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21337">https://arxiv.org/pdf/2601.21337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21337]] Qwen3-ASR Technical Report(https://arxiv.org/abs/2601.21337)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.</li>
</ul>

<h3>Title: Memorization Control in Diffusion Models from Denoising-centric Perspective</h3>
<ul>
<li><strong>Authors: </strong>Thuy Phuong Vu, Mai Viet Hoang Do, Minhhuy Le, Dinh-Cuong Hoang, Phan Xuan Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21348">https://arxiv.org/abs/2601.21348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21348">https://arxiv.org/pdf/2601.21348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21348]] Memorization Control in Diffusion Models from Denoising-centric Perspective(https://arxiv.org/abs/2601.21348)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Controlling memorization in diffusion models is critical for applications that require generated data to closely match the training distribution. Existing approaches mainly focus on data centric or model centric modifications, treating the diffusion model as an isolated predictor. In this paper, we study memorization in diffusion models from a denoising centric perspective. We show that uniform timestep sampling leads to unequal learning contributions across denoising steps due to differences in signal to noise ratio, which biases training toward memorization. To address this, we propose a timestep sampling strategy that explicitly controls where learning occurs along the denoising trajectory. By adjusting the width of the confidence interval, our method provides direct control over the memorization generalization trade off. Experiments on image and 1D signal generation tasks demonstrate that shifting learning emphasis toward later denoising steps consistently reduces memorization and improves distributional alignment with training data, validating the generality and effectiveness of our approach.</li>
</ul>

<h3>Title: Graph-Free Root Cause Analysis</h3>
<ul>
<li><strong>Authors: </strong>Luan Pham</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21359">https://arxiv.org/abs/2601.21359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21359">https://arxiv.org/pdf/2601.21359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21359]] Graph-Free Root Cause Analysis(https://arxiv.org/abs/2601.21359)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Failures in complex systems demand rapid Root Cause Analysis (RCA) to prevent cascading damage. Existing RCA methods that operate without dependency graph typically assume that the root cause having the highest anomaly score. This assumption fails when faults propagate, as a small delay at the root cause can accumulate into a much larger anomaly downstream. In this paper, we propose PRISM, a simple and efficient framework for RCA when the dependency graph is absent. We formulate a class of component-based systems under which PRISM performs RCA with theoretical guarantees. On 735 failures across 9 real-world datasets, PRISM achieves 68% Top-1 accuracy, a 258% improvement over the best baseline, while requiring only 8ms per diagnosis.</li>
</ul>

<h3>Title: Rethinking Federated Graph Foundation Models: A Graph-Language Alignment-based Approach</h3>
<ul>
<li><strong>Authors: </strong>Yinlin Zhu, Di Wu, Xianzhi Zhang, Yuming Ai, Xunkai Li, Miao Hu, Guocong Quan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21369">https://arxiv.org/abs/2601.21369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21369">https://arxiv.org/pdf/2601.21369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21369]] Rethinking Federated Graph Foundation Models: A Graph-Language Alignment-based Approach(https://arxiv.org/abs/2601.21369)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent studies of federated graph foundational models (FedGFMs) break the idealized and untenable assumption of having centralized data storage to train graph foundation models, and accommodate the reality of distributed, privacy-restricted data silos. Despite their simplicity and intuition, existing studies that project aligned generalizable knowledge onto a discrete token space via vector-quantized backbones suffer from irreversible knowledge loss during the quantization process. In this context, we argue that reconciling the semantic-structural orthogonality and integrity between pre-trained language models (PLMs) and graph neural networks (GNNs) is paramount for developing effective FedGFMs while simultaneously mitigating the severe data heterogeneity and communication constraints inherent in distributed, resource-limited environments. To address these issues, we propose FedGALA (Federated Graph And Language Alignment), a framework that resolves graph-based semantic-structural orthogonality and integrity in federated settings by employing unsupervised contrastive learning to align GNNs and frozen PLMs within a continuous embedding space, thereby capturing robust, transferable general knowledge. Subsequently, FedGALA leverages a communication-efficient prompt tuning mechanism to steer these pre-aligned encoders and frozen PLMs, facilitating effective adaptation to diverse downstream tasks while circumventing the prohibitive overhead of full-parameter fine-tuning. The comprehensive experiments validate that FedGALA outperforms all competitive baselines across multi-domain datasets on multiple tasks with up to 14.37% performance improvement.</li>
</ul>

<h3>Title: MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations</h3>
<ul>
<li><strong>Authors: </strong>Xinan He, Kaiqing Lin, Yue Zhou, Jiaming Zhong, Wei Ye, Wenhui Yi, Bing Fan, Feng Ding, Haodong Li, Bo Cao, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21408">https://arxiv.org/abs/2601.21408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21408">https://arxiv.org/pdf/2601.21408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21408]] MPF-Net: Exposing High-Fidelity AI-Generated Video Forgeries via Hierarchical Manifold Deviation and Micro-Temporal Fluctuations(https://arxiv.org/abs/2601.21408)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of video generation models such as Veo and Wan, the visual quality of synthetic content has reached a level where macro-level semantic errors and temporal inconsistencies are no longer prominent. However, this does not imply that the distinction between real and cutting-edge high-fidelity fake is untraceable. We argue that AI-generated videos are essentially products of a manifold-fitting process rather than a physical recording. Consequently, the pixel composition logic of consecutive adjacent frames residual in AI videos exhibits a structured and homogenous characteristic. We term this phenomenon `Manifold Projection Fluctuations' (MPF). Driven by this insight, we propose a hierarchical dual-path framework that operates as a sequential filtering process. The first, the Static Manifold Deviation Branch, leverages the refined perceptual boundaries of Large-Scale Vision Foundation Models (VFMs) to capture residual spatial anomalies or physical violations that deviate from the natural real-world manifold (off-manifold). For the remaining high-fidelity videos that successfully reside on-manifold and evade spatial detection, we introduce the Micro-Temporal Fluctuation Branch as a secondary, fine-grained filter. By analyzing the structured MPF that persists even in visually perfect sequences, our framework ensures that forgeries are exposed regardless of whether they manifest as global real-world manifold deviations or subtle computational fingerprints.</li>
</ul>

<h3>Title: Revisiting Diffusion Model Predictions Through Dimensionality</h3>
<ul>
<li><strong>Authors: </strong>Qing Jin, Chaoyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21419">https://arxiv.org/abs/2601.21419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21419">https://arxiv.org/pdf/2601.21419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21419]] Revisiting Diffusion Model Predictions Through Dimensionality(https://arxiv.org/abs/2601.21419)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.</li>
</ul>

<h3>Title: Accurate Network Traffic Matrix Prediction via LEAD: an LLM-Enhanced Adapter-Based Conditional Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yu Sun, Yaqiong Liu, Nan Cheng, Jiayuan Li, Zihan Jia, Xialin Du, Mugen Peng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21437">https://arxiv.org/abs/2601.21437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21437">https://arxiv.org/pdf/2601.21437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21437]] Accurate Network Traffic Matrix Prediction via LEAD: an LLM-Enhanced Adapter-Based Conditional Diffusion Model(https://arxiv.org/abs/2601.21437)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Driven by the evolution toward 6G and AI-native edge intelligence, network operations increasingly require predictive and risk-aware adaptation under stringent computation and latency constraints. Network Traffic Matrix (TM), which characterizes flow volumes between nodes, is a fundamental signal for proactive traffic engineering. However, accurate TM forecasting remains challenging due to the stochastic, non-linear, and bursty nature of network dynamics. Existing discriminative models often suffer from over-smoothing and provide limited uncertainty awareness, leading to poor fidelity under extreme bursts. To address these limitations, we propose LEAD, a Large Language Model (LLM)-Enhanced Adapter-based conditional Diffusion model. First, LEAD adopts a "Traffic-to-Image" paradigm to transform traffic matrices into RGB images, enabling global dependency modeling via vision backbones. Then, we design a "Frozen LLM with Trainable Adapter" model, which efficiently captures temporal semantics with limited computational cost. Moreover, we propose a Dual-Conditioning Strategy to precisely guide a diffusion model to generate complex, dynamic network traffic matrices. Experiments on the Abilene and GEANT datasets demonstrate that LEAD outperforms all baselines. On the Abilene dataset, LEAD attains a remarkable 45.2% reduction in RMSE against the best baseline, with the error margin rising only marginally from 0.1098 at one-step to 0.1134 at 20-step predictions. Meanwhile, on the GEANT dataset, LEAD achieves a 0.0258 RMSE at 20-step prediction horizon which is 27.3% lower than the best baseline.</li>
</ul>

<h3>Title: SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Yu Xie, Xing Kai Ren, Ying Qi, Hu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21452">https://arxiv.org/abs/2601.21452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21452">https://arxiv.org/pdf/2601.21452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21452]] SAGE: Sequence-level Adaptive Gradient Evolution for Generative Recommendation(https://arxiv.org/abs/2601.21452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While works such as OneRec have validated the scaling laws of Large Language Models (LLMs) in recommender systems, they rely on a cumbersome separate vocabulary. This dependency prevents the model architecture from reusing native LLM vocabularies, resulting in high maintenance costs and poor scalability. In response, we aim to efficiently reuse open-source LLM architectures without constructing a separate tokenization vocabulary. Furthermore, we identify that the optimization strategy of OneRec Gradient Bounded Policy Optimization (GBPO),suffers from a "Symmetric Conservatism" problem: its static gradient boundaries structurally suppress the update momentum required for cold-start items and fail to prevent diversity collapse in high-noise this http URL address this issue, we propose SAGE (Sequence-level Adaptive Gradient Evolution), a unified optimization framework tailored for list-wise generative recommendation. SAGE introduces two key innovations:(1) Sequence-level Signal Decoupling: By combining a geometric mean importance ratio with decoupled multi-objective advantages, we eliminate token-level variance and resolve the "Reward Collapse" problem. (2) Asymmetric Adaptive Dynamics: We construct a dynamic gradient manifold that applies a "Boost Factor" to high-potential cold start items to achieve super-linear updates and employs an "Entropy Aware Penalty" to break information cocoons. Theoretical analysis and empirical results demonstrate that SAGE effectively unblocks cold-start traffic and sustains recommendation diversity, all while retaining the numerical stability of GBPO.</li>
</ul>

<h3>Title: ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment</h3>
<ul>
<li><strong>Authors: </strong>Xiuyu Li, Jinkai Zhang, Mingyang Yi, Yu Li, Longqiang Wang, Yue Wang, Ju Fan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21484">https://arxiv.org/abs/2601.21484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21484">https://arxiv.org/pdf/2601.21484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21484]] ETS: Energy-Guided Test-Time Scaling for Training-Free RL Alignment(https://arxiv.org/abs/2601.21484)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) post-training alignment for language models is effective, but also costly and unstable in practice, owing to its complicated training process. To address this, we propose a training-free inference method to sample directly from the optimal RL policy. The transition probability applied to Masked Language Modeling (MLM) consists of a reference policy model and an energy term. Based on this, our algorithm, Energy-Guided Test-Time Scaling (ETS), estimates the key energy term via online Monte Carlo, with a provable convergence rate. Moreover, to ensure practical efficiency, ETS leverages modern acceleration frameworks alongside tailored importance sampling estimators, substantially reducing inference latency while provably preserving sampling quality. Experiments on MLM (including autoregressive models and diffusion language models) across reasoning, coding, and science benchmarks show that our ETS consistently improves generation quality, validating its effectiveness and design.</li>
</ul>

<h3>Title: SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing</h3>
<ul>
<li><strong>Authors: </strong>Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21498">https://arxiv.org/abs/2601.21498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21498">https://arxiv.org/pdf/2601.21498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21498]] SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing(https://arxiv.org/abs/2601.21498)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in Generative Artificial Intelligence (GenAI) have significantly enhanced the capabilities of both image generation and editing. However, current approaches often treat these tasks separately, leading to inefficiencies and challenges in maintaining spatial consistency and semantic coherence between generated content and edits. Moreover, a major obstacle is the lack of structured control over object relationships and spatial arrangements. Scene graph-based methods, which represent objects and their interrelationships in a structured format, offer a solution by providing greater control over composition and interactions in both image generation and editing. To address this, we introduce SimGraph, a unified framework that integrates scene graph-based image generation and editing, enabling precise control over object interactions, layouts, and spatial coherence. In particular, our framework integrates token-based generation and diffusion-based editing within a single scene graph-driven model, ensuring high-quality and consistent results. Through extensive experiments, we empirically demonstrate that our approach outperforms existing state-of-the-art methods.</li>
</ul>

<h3>Title: HERS: Hidden-Pattern Expert Learning for Risk-Specific Vehicle Damage Adaptation in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Teerapong Panboonyuen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21517">https://arxiv.org/abs/2601.21517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21517">https://arxiv.org/pdf/2601.21517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21517]] HERS: Hidden-Pattern Expert Learning for Risk-Specific Vehicle Damage Adaptation in Diffusion Models(https://arxiv.org/abs/2601.21517)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in text-to-image (T2I) diffusion models have enabled increasingly realistic synthesis of vehicle damage, raising concerns about their reliability in automated insurance workflows. The ability to generate crash-like imagery challenges the boundary between authentic and synthetic data, introducing new risks of misuse in fraud or claim manipulation. To address these issues, we propose HERS (Hidden-Pattern Expert Learning for Risk-Specific Damage Adaptation), a framework designed to improve fidelity, controllability, and domain alignment of diffusion-generated damage images. HERS fine-tunes a base diffusion model via domain-specific expert adaptation without requiring manual annotation. Using self-supervised image-text pairs automatically generated by a large language model and T2I pipeline, HERS models each damage category, such as dents, scratches, broken lights, or cracked paint, as a separate expert. These experts are later integrated into a unified multi-damage model that balances specialization with generalization. We evaluate HERS across four diffusion backbones and observe consistent improvements: plus 5.5 percent in text faithfulness and plus 2.3 percent in human preference ratings compared to baselines. Beyond image fidelity, we discuss implications for fraud detection, auditability, and safe deployment of generative models in high-stakes domains. Our findings highlight both the opportunities and risks of domain-specific diffusion, underscoring the importance of trustworthy generation in safety-critical applications such as auto insurance.</li>
</ul>

<h3>Title: Bi-Anchor Interpolation Solver for Accelerating Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Hongxu Chen, Hongxiang Li, Zhen Wang, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21542">https://arxiv.org/abs/2601.21542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21542">https://arxiv.org/pdf/2601.21542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21542]] Bi-Anchor Interpolation Solver for Accelerating Generative Modeling(https://arxiv.org/abs/2601.21542)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow Matching (FM) models have emerged as a leading paradigm for high-fidelity synthesis. However, their reliance on iterative Ordinary Differential Equation (ODE) solving creates a significant latency bottleneck. Existing solutions face a dichotomy: training-free solvers suffer from significant performance degradation at low Neural Function Evaluations (NFEs), while training-based one- or few-steps generation methods incur prohibitive training costs and lack plug-and-play versatility. To bridge this gap, we propose the Bi-Anchor Interpolation Solver (BA-solver). BA-solver retains the versatility of standard training-free solvers while achieving significant acceleration by introducing a lightweight SideNet (1-2% backbone size) alongside the frozen backbone. Specifically, our method is founded on two synergistic components: \textbf{1) Bidirectional Temporal Perception}, where the SideNet learns to approximate both future and historical velocities without retraining the heavy backbone; and 2) Bi-Anchor Velocity Integration, which utilizes the SideNet with two anchor velocities to efficiently approximate intermediate velocities for batched high-order integration. By utilizing the backbone to establish high-precision ``anchors'' and the SideNet to densify the trajectory, BA-solver enables large interval sizes with minimized error. Empirical results on ImageNet-256^2 demonstrate that BA-solver achieves generation quality comparable to 100+ NFEs Euler solver in just 10 NFEs and maintains high fidelity in as few as 5 NFEs, incurring negligible training costs. Furthermore, BA-solver ensures seamless integration with existing generative pipelines, facilitating downstream tasks such as image editing.</li>
</ul>

<h3>Title: ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses</h3>
<ul>
<li><strong>Authors: </strong>Ningyuan He, Ronghong Huang, Qianqian Tang, Hongyu Wang, Xianghang Mi, Shanqing Guo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21586">https://arxiv.org/abs/2601.21586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21586">https://arxiv.org/pdf/2601.21586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21586]] ICL-EVADER: Zero-Query Black-Box Evasion Attacks on In-Context Learning and Their Defenses(https://arxiv.org/abs/2601.21586)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) has become a powerful, data-efficient paradigm for text classification using large language models. However, its robustness against realistic adversarial threats remains largely unexplored. We introduce ICL-Evader, a novel black-box evasion attack framework that operates under a highly practical zero-query threat model, requiring no access to model parameters, gradients, or query-based feedback during attack generation. We design three novel attacks, Fake Claim, Template, and Needle-in-a-Haystack, that exploit inherent limitations of LLMs in processing in-context prompts. Evaluated across sentiment analysis, toxicity, and illicit promotion tasks, our attacks significantly degrade classifier performance (e.g., achieving up to 95.3% attack success rate), drastically outperforming traditional NLP attacks which prove ineffective under the same constraints. To counter these vulnerabilities, we systematically investigate defense strategies and identify a joint defense recipe that effectively mitigates all attacks with minimal utility loss (<5% accuracy degradation). Finally, we translate our defensive insights into an automated tool that proactively fortifies standard ICL prompts against adversarial evasion. This work provides a comprehensive security assessment of ICL, revealing critical vulnerabilities and offering practical solutions for building more robust systems. Our source code and evaluation datasets are publicly available at: this https URL .</li>
</ul>

<h3>Title: Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening</h3>
<ul>
<li><strong>Authors: </strong>Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21590">https://arxiv.org/abs/2601.21590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21590">https://arxiv.org/pdf/2601.21590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21590]] Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening(https://arxiv.org/abs/2601.21590)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.</li>
</ul>

<h3>Title: Unifying Heterogeneous Degradations: Uncertainty-Aware Diffusion Bridge Model for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Luwei Tu, Jiawei Wu, Xing Luo, Zhi Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21592">https://arxiv.org/abs/2601.21592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21592">https://arxiv.org/pdf/2601.21592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21592]] Unifying Heterogeneous Degradations: Uncertainty-Aware Diffusion Bridge Model for All-in-One Image Restoration(https://arxiv.org/abs/2601.21592)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>All-in-One Image Restoration (AiOIR) faces the fundamental challenge in reconciling conflicting optimization objectives across heterogeneous degradations. Existing methods are often constrained by coarse-grained control mechanisms or fixed mapping schedules, yielding suboptimal adaptation. To address this, we propose an Uncertainty-Aware Diffusion Bridge Model (UDBM), which innovatively reformulates AiOIR as a stochastic transport problem steered by pixel-wise uncertainty. By introducing a relaxed diffusion bridge formulation which replaces the strict terminal constraint with a relaxed constraint, we model the uncertainty of degradations while theoretically resolving the drift singularity inherent in standard diffusion bridges. Furthermore, we devise a dual modulation strategy: the noise schedule aligns diverse degradations into a shared high-entropy latent space, while the path schedule adaptively regulates the transport trajectory motivated by the viscous dynamics of entropy regularization. By effectively rectifying the transport geometry and dynamics, UDBM achieves state-of-the-art performance across diverse restoration tasks within a single inference step.</li>
</ul>

<h3>Title: WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zijin Yang, Yu Sun, Kejiang Chen, Jiawei Zhao, Jun Jiang, Weiming Zhang, Nenghai Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21610">https://arxiv.org/abs/2601.21610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21610">https://arxiv.org/pdf/2601.21610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21610]] WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models(https://arxiv.org/abs/2601.21610)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Digital watermarking is essential for securing generated images from diffusion models. Accurate watermark evaluation is critical for algorithm development, yet existing methods have significant limitations: they lack a unified framework for both residual and semantic watermarks, provide results without interpretability, neglect comprehensive security considerations, and often use inappropriate metrics for semantic watermarks. To address these gaps, we propose WMVLM, the first unified and interpretable evaluation framework for diffusion model image watermarking via vision-language models (VLMs). We redefine quality and security metrics for each watermark type: residual watermarks are evaluated by artifact strength and erasure resistance, while semantic watermarks are assessed through latent distribution shifts. Moreover, we introduce a three-stage training strategy to progressively enable the model to achieve classification, scoring, and interpretable text generation. Experiments show WMVLM outperforms state-of-the-art VLMs with strong generalization across datasets, diffusion models, and watermarking methods.</li>
</ul>

<h3>Title: Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise</h3>
<ul>
<li><strong>Authors: </strong>Puwei Lian, Yujun Cai, Songze Li, Bingkun Bao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21628">https://arxiv.org/abs/2601.21628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21628">https://arxiv.org/pdf/2601.21628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21628]] Noise as a Probe: Membership Inference Attacks on Diffusion Models Leveraging Initial Noise(https://arxiv.org/abs/2601.21628)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable progress in image generation, but their increasing deployment raises serious concerns about privacy. In particular, fine-tuned models are highly vulnerable, as they are often fine-tuned on small and private datasets. Membership inference attacks (MIAs) are used to assess privacy risks by determining whether a specific sample was part of a model's training data. Existing MIAs against diffusion models either assume obtaining the intermediate results or require auxiliary datasets for training the shadow model. In this work, we utilized a critical yet overlooked vulnerability: the widely used noise schedules fail to fully eliminate semantic information in the images, resulting in residual semantic signals even at the maximum noise step. We empirically demonstrate that the fine-tuned diffusion model captures hidden correlations between the residual semantics in initial noise and the original images. Building on this insight, we propose a simple yet effective membership inference attack, which injects semantic information into the initial noise and infers membership by analyzing the model's generation result. Extensive experiments demonstrate that the semantic initial noise can strongly reveal membership information, highlighting the vulnerability of diffusion models to MIAs.</li>
</ul>

<h3>Title: A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Pu Cao, Yiyang Ma, Feng Zhou, Xuedan Yin, Qing Song, Lu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21633">https://arxiv.org/abs/2601.21633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21633">https://arxiv.org/pdf/2601.21633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21633]] A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion(https://arxiv.org/abs/2601.21633)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In latent diffusion models, the autoencoder (AE) is typically expected to balance two capabilities: faithful reconstruction and a generation-friendly latent space (e.g., low gFID). In recent ImageNet-scale AE studies, we observe a systematic bias toward generative metrics in handling this trade-off: reconstruction metrics are increasingly under-reported, and ablation-based AE selection often favors the best-gFID configuration even when reconstruction fidelity degrades. We theoretically analyze why this gFID-dominant preference can appear unproblematic for ImageNet generation, yet becomes risky when scaling to controllable diffusion: AEs can induce condition drift, which limits achievable condition alignment. Meanwhile, we find that reconstruction fidelity, especially instance-level measures, better indicates controllability. We empirically validate the impact of tilted autoencoder evaluation on controllability by studying several recent ImageNet AEs. Using a multi-dimensional condition-drift evaluation protocol reflecting controllable generation tasks, we find that gFID is only weakly predictive of condition preservation, whereas reconstruction-oriented metrics are substantially more aligned. ControlNet experiments further confirm that controllability tracks condition preservation rather than gFID. Overall, our results expose a gap between ImageNet-centric AE evaluation and the requirements of scalable controllable diffusion, offering practical guidance for more reliable benchmarking and model selection.</li>
</ul>

<h3>Title: Generative Design of Ship Propellers using Conditional Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Patrick Kruger, Rafael Diaz, Simon Hauschulz, Stefan Harries, Hanno Gottschalk</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21637">https://arxiv.org/abs/2601.21637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21637">https://arxiv.org/pdf/2601.21637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21637]] Generative Design of Ship Propellers using Conditional Flow Matching(https://arxiv.org/abs/2601.21637)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the use of generative artificial intelligence (GenAI) for ship propeller design. While traditional forward machine learning models predict the performance of mechanical components based on given design parameters, GenAI models aim to generate designs that achieve specified performance targets. In particular, we employ conditional flow matching to establish a bidirectional mapping between design parameters and simulated noise that is conditioned on performance labels. This approach enables the generation of multiple valid designs corresponding to the same performance targets by sampling over the noise vector. To support model training, we generate data using a vortex lattice method for numerical simulation and analyze the trade-off between model accuracy and the amount of available data. We further propose data augmentation using pseudo-labels derived from less data-intensive forward surrogate models, which can often improve overall model performance. Finally, we present examples of distinct propeller geometries that exhibit nearly identical performance characteristics, illustrating the versatility and potential of GenAI in engineering design.</li>
</ul>

<h3>Title: ILRR: Inference-Time Steering Method for Masked Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eden Avrahami, Eliya Nachmani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21647">https://arxiv.org/abs/2601.21647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21647">https://arxiv.org/pdf/2601.21647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21647]] ILRR: Inference-Time Steering Method for Masked Diffusion Language Models(https://arxiv.org/abs/2601.21647)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete Diffusion Language Models (DLMs) offer a promising non-autoregressive alternative for text generation, yet effective mechanisms for inference-time control remain relatively underexplored. Existing approaches include sampling-level guidance procedures or trajectory optimization mechanisms. In this work, we introduce Iterative Latent Representation Refinement (ILRR), a learning-free framework for steering DLMs using a single reference sequence. ILRR guides generation by dynamically aligning the internal activations of the generated sequence with those of a given reference throughout the denoising process. This approach captures and transfers high-level semantic properties, with a tunable steering scale enabling flexible control over attributes such as sentiment. We further introduce Spatially Modulated Steering, an extension that enables steering long texts using shorter references by regulating guidance intensity across the sequence. Empirically, we demonstrate that ILRR achieves effective attribute steering on LLaDA and MDLM architectures with a minor computational overhead, requiring only one additional parallel forward pass per denoising step. Under the same compute budget, ILRR improves attribute accuracy over comparable baselines by 10$\%$ to 60$\%$ points, while maintaining high generation quality.</li>
</ul>

<h3>Title: Multimodal Visual Surrogate Compression for Alzheimer's Disease Classification</h3>
<ul>
<li><strong>Authors: </strong>Dexuan Ding, Ciyuan Peng, Endrowednes Kuantama, Jingcai Guo, Jia Wu, Jian Yang, Amin Beheshti, Ming-Hsuan Yang, Yuankai Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21673">https://arxiv.org/abs/2601.21673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21673">https://arxiv.org/pdf/2601.21673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21673]] Multimodal Visual Surrogate Compression for Alzheimer's Disease Classification(https://arxiv.org/abs/2601.21673)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>High-dimensional structural MRI (sMRI) images are widely used for Alzheimer's Disease (AD) diagnosis. Most existing methods for sMRI representation learning rely on 3D architectures (e.g., 3D CNNs), slice-wise feature extraction with late aggregation, or apply training-free feature extractions using 2D foundation models (e.g., DINO). However, these three paradigms suffer from high computational cost, loss of cross-slice relations, and limited ability to extract discriminative features, respectively. To address these challenges, we propose Multimodal Visual Surrogate Compression (MVSC). It learns to compress and adapt large 3D sMRI volumes into compact 2D features, termed as visual surrogates, which are better aligned with frozen 2D foundation models to extract powerful representations for final AD classification. MVSC has two key components: a Volume Context Encoder that captures global cross-slice context under textual guidance, and an Adaptive Slice Fusion module that aggregates slice-level information in a text-enhanced, patch-wise manner. Extensive experiments on three large-scale Alzheimer's disease benchmarks demonstrate our MVSC performs favourably on both binary and multi-class classification tasks compared against state-of-the-art methods.</li>
</ul>

<h3>Title: LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Qisong Xiao, Xinhai Chen, Qinglin Wang, Xiaowei Guo, Binglin Wang, Weifeng Chen, Zhichao Wang, Yunfei Liu, Rui Xia, Hang Zou, Gencheng Liu, Shuai Li, Jie Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21681">https://arxiv.org/abs/2601.21681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21681">https://arxiv.org/pdf/2601.21681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21681]] LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics(https://arxiv.org/abs/2601.21681)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Deep learning has emerged as a promising paradigm for spatio-temporal modeling of fluid dynamics. However, existing approaches often suffer from limited generalization to unseen flow conditions and typically require retraining when applied to new scenarios. In this paper, we present LLM4Fluid, a spatio-temporal prediction framework that leverages Large Language Models (LLMs) as generalizable neural solvers for fluid dynamics. The framework first compresses high-dimensional flow fields into a compact latent space via reduced-order modeling enhanced with a physics-informed disentanglement mechanism, effectively mitigating spatial feature entanglement while preserving essential flow structures. A pretrained LLM then serves as a temporal processor, autoregressively predicting the dynamics of physical sequences with time series prompts. To bridge the modality gap between prompts and physical sequences, which can otherwise degrade prediction accuracy, we propose a dedicated modality alignment strategy that resolves representational mismatch and stabilizes long-term prediction. Extensive experiments across diverse flow scenarios demonstrate that LLM4Fluid functions as a robust and generalizable neural solver without retraining, achieving state-of-the-art accuracy while exhibiting powerful zero-shot and in-context learning capabilities. Code and datasets are publicly available at this https URL.</li>
</ul>

<h3>Title: Can Local Learning Match Self-Supervised Backpropagation?</h3>
<ul>
<li><strong>Authors: </strong>Wu S. Zihan, Ariane Delrocq, Wulfram Gerstner, Guillaume Bellec</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21683">https://arxiv.org/abs/2601.21683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21683">https://arxiv.org/pdf/2601.21683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21683]] Can Local Learning Match Self-Supervised Backpropagation?(https://arxiv.org/abs/2601.21683)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>While end-to-end self-supervised learning with backpropagation (global BP-SSL) has become central for training modern AI systems, theories of local self-supervised learning (local-SSL) have struggled to build functional representations in deep neural networks. To establish a link between global and local rules, we first develop a theory for deep linear networks: we identify conditions for local-SSL algorithms (like Forward-forward or CLAPP) to implement exactly the same weight update as a global BP-SSL. Starting from the theoretical insights, we then develop novel variants of local-SSL algorithms to approximate global BP-SSL in deep non-linear convolutional neural networks. Variants that improve the similarity between gradient updates of local-SSL with those of global BP-SSL also show better performance on image datasets (CIFAR-10, STL-10, and Tiny ImageNet). The best local-SSL rule with the CLAPP loss function matches the performance of a comparable global BP-SSL with InfoNCE or CPC-like loss functions, and improves upon state-of-the-art for local SSL on these benchmarks.</li>
</ul>

<h3>Title: Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Tien Dang, The-Hai Nguyen, Dinh Mai Phuong, Nguyen Minh Phuong, Hoang Thanh-Tung, Le-Minh Nguyen, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21702">https://arxiv.org/abs/2601.21702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21702">https://arxiv.org/pdf/2601.21702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21702]] Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities(https://arxiv.org/abs/2601.21702)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We consider representation misdirection (RM), a class of LLM unlearning methods that achieves forgetting by manipulating the forget-representations, that is, latent representations of forget samples. Despite being important, the roles of target vectors used in RM, however, remain underexplored. Here, we approach and revisit RM through the lens of the linear representation hypothesis. Specifically, if one can somehow identify a one-dimensional representation corresponding to a high-level concept, the linear representation hypothesis enables linear operations on this concept vector within the forget-representation space. Under this view, we hypothesize that, beyond forgetting, machine unlearning elicits controllable side behaviors and stronger side capabilities corresponding to the high-level concept. Our hypothesis is empirically validated across a wide range of tasks, including behavioral control (e.g., controlling unlearned models' truth, sentiment, and refusal) and capability enhancement (e.g., improving unlearned models' in-context learning capability). Our findings reveal that this fairly attractive phenomenon could be either a hidden risk if misused or a mechanism that can be harnessed for developing models that require stronger capabilities and controllable behaviors.</li>
</ul>

<h3>Title: SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models</h3>
<ul>
<li><strong>Authors: </strong>Nan Lin, Yanbo Wang, Jacco Heres, Peter Palensky, Pedro P. Vergara</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21706">https://arxiv.org/abs/2601.21706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21706">https://arxiv.org/pdf/2601.21706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21706]] SmartMeterFM: Unifying Smart Meter Data Generative Tasks Using Flow Matching Models(https://arxiv.org/abs/2601.21706)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Smart meter data is the foundation for planning and operating the distribution network. Unfortunately, such data are not always available due to privacy regulations. Meanwhile, the collected data may be corrupted due to sensor or transmission failure, or it may not have sufficient resolution for downstream tasks. A wide range of generative tasks is formulated to address these issues, including synthetic data generation, missing data imputation, and super-resolution. Despite the success of machine learning models on these tasks, dedicated models need to be designed and trained for each task, leading to redundancy and inefficiency. In this paper, by recognizing the powerful modeling capability of flow matching models, we propose a new approach to unify diverse smart meter data generative tasks with a single model trained for conditional generation. The proposed flow matching models are trained to generate challenging, high-dimensional time series data, specifically monthly smart meter data at a 15 min resolution. By viewing different generative tasks as distinct forms of partial data observations and injecting them into the generation process, we unify tasks such as imputation and super-resolution with a single model, eliminating the need for re-training. The data generated by our model not only are consistent with the given observations but also remain realistic, showing better performance against interpolation and other machine learning based baselines dedicated to the tasks.</li>
</ul>

<h3>Title: DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingshuang Luo, Shuang Liang, Zhengkun Rong, Yuxuan Luo, Tianshu Hu, Ruibing Hou, Hong Chang, Yong Li, Yuan Zhang, Mingyuan Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21716">https://arxiv.org/abs/2601.21716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21716">https://arxiv.org/pdf/2601.21716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21716]] DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning(https://arxiv.org/abs/2601.21716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: this https URL</li>
</ul>

<h3>Title: CoFrGeNet: Continued Fraction Architectures for Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Amit Dhurandhar, Vijil Chenthamarakshan, Dennis Wei, Tejaswini Pedapati, Karthikeyan Natesan Ramamurthy, Rahul Nair</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21766">https://arxiv.org/abs/2601.21766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21766">https://arxiv.org/pdf/2601.21766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21766]] CoFrGeNet: Continued Fraction Architectures for Language Generation(https://arxiv.org/abs/2601.21766)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\frac{2}{3}$ to $\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.</li>
</ul>

<h3>Title: Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention</h3>
<ul>
<li><strong>Authors: </strong>Alon Rozental</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21768">https://arxiv.org/abs/2601.21768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21768">https://arxiv.org/pdf/2601.21768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21768]] Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention(https://arxiv.org/abs/2601.21768)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical diffusion model that addresses these limitations through a fully trainable pipeline from raw characters to document-level representations. At its core is a differentiable tokenizer (Segment Splitter) that learns probabilistic beginning-of-sequence (BOS) decisions, enabling adaptive splits that emerge as linguistically meaningful (e.g., word boundaries at spaces, sentence starts at periods) without explicit supervision. This differentiability is enabled by our novel Probabilistic Attention mechanism, which incorporates position-specific existence probabilities to simulate soft masking over theoretically infinite sequences while preserving gradients. Sequences decay probabilistically rather than relying on end-of-sequence tokens, supporting variable-length outputs. Hierarchical levels compress sequences into higher abstractions (e.g., character n-grams to word-like vectors, then sentence-like), with reconstruction via our Denoising Diffusion Mixed Model (DDMM) for stable and efficient denoising in latent space. A Stitcher ensures overlap invariance across segments. Trained end-to-end on Wikipedia, Zonkey generates coherent, variable-length text from noise, demonstrating emergent hierarchies and promising qualitative alignment to data distributions compared to entropy-based learnable tokenizers. Our approach advances toward fully gradient-based LLMs, with potential for better domain adaptation and scalable generation. We release the source code for training and reproducing our experiments.</li>
</ul>

<h3>Title: Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Sidney Bender, Marco Morik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21851">https://arxiv.org/abs/2601.21851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21851">https://arxiv.org/pdf/2601.21851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21851]] Visual Disentangled Diffusion Autoencoders: Scalable Counterfactual Generation for Foundation Models(https://arxiv.org/abs/2601.21851)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models, despite their robust zero-shot capabilities, remain vulnerable to spurious correlations and 'Clever Hans' strategies. Existing mitigation methods often rely on unavailable group labels or computationally expensive gradient-based adversarial optimization. To address these limitations, we propose Visual Disentangled Diffusion Autoencoders (DiDAE), a novel framework integrating frozen foundation models with disentangled dictionary learning for efficient, gradient-free counterfactual generation directly for the foundation model. DiDAE first edits foundation model embeddings in interpretable disentangled directions of the disentangled dictionary and then decodes them via a diffusion autoencoder. This allows the generation of multiple diverse, disentangled counterfactuals for each factual, much faster than existing baselines, which generate single entangled counterfactuals. When paired with Counterfactual Knowledge Distillation, DiDAE-CFKD achieves state-of-the-art performance in mitigating shortcut learning, improving downstream performance on unbalanced datasets.</li>
</ul>

<h3>Title: Trajectory-Guided Diffusion for Foreground-Preserving Background Generation in Multi-Layer Documents</h3>
<ul>
<li><strong>Authors: </strong>Taewon Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21857">https://arxiv.org/abs/2601.21857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21857">https://arxiv.org/pdf/2601.21857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21857]] Trajectory-Guided Diffusion for Foreground-Preserving Background Generation in Multi-Layer Documents(https://arxiv.org/abs/2601.21857)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present a diffusion-based framework for document-centric background generation that achieves foreground preservation and multi-page stylistic consistency through latent-space design rather than explicit constraints. Instead of suppressing diffusion updates or applying masking heuristics, our approach reinterprets diffusion as the evolution of stochastic trajectories through a structured latent space. By shaping the initial noise and its geometric alignment, background generation naturally avoids designated foreground regions, allowing readable content to remain intact without auxiliary mechanisms. To address the long-standing issue of stylistic drift across pages, we decouple style control from text conditioning and introduce cached style directions as persistent vectors in latent space. Once selected, these directions constrain diffusion trajectories to a shared stylistic subspace, ensuring consistent appearance across pages and editing iterations. This formulation eliminates the need for repeated prompt-based style specification and provides a more stable foundation for multi-page generation. Our framework admits a geometric and physical interpretation, where diffusion paths evolve on a latent manifold shaped by preferred directions, and foreground regions are rarely traversed as a consequence of trajectory initialization rather than explicit exclusion. The proposed method is training-free, compatible with existing diffusion backbones, and produces visually coherent, foreground-preserving results across complex documents. By reframing diffusion as trajectory design in latent space, we offer a principled approach to consistent and structured generative modeling.</li>
</ul>

<h3>Title: Improving Classifier-Free Guidance of Flow Matching via Manifold Projection</h3>
<ul>
<li><strong>Authors: </strong>Jian-Feng Cai, Haixia Liu, Zhengyi Su, Chao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21892">https://arxiv.org/abs/2601.21892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21892">https://arxiv.org/pdf/2601.21892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21892]] Improving Classifier-Free Guidance of Flow Matching via Manifold Projection(https://arxiv.org/abs/2601.21892)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Classifier-free guidance (CFG) is a widely used technique for controllable generation in diffusion and flow-based models. Despite its empirical success, CFG relies on a heuristic linear extrapolation that is often sensitive to the guidance scale. In this work, we provide a principled interpretation of CFG through the lens of optimization. We demonstrate that the velocity field in flow matching corresponds to the gradient of a sequence of smoothed distance functions, which guides latent variables toward the scaled target image set. This perspective reveals that the standard CFG formulation is an approximation of this gradient, where the prediction gap, the discrepancy between conditional and unconditional outputs, governs guidance sensitivity. Leveraging this insight, we reformulate the CFG sampling as a homotopy optimization with a manifold constraint. This formulation necessitates a manifold projection step, which we implement via an incremental gradient descent scheme during sampling. To improve computational efficiency and stability, we further enhance this iterative process with Anderson Acceleration without requiring additional model evaluations. Our proposed methods are training-free and consistently refine generation fidelity, prompt alignment, and robustness to the guidance scale. We validate their effectiveness across diverse benchmarks, demonstrating significant improvements on large-scale models such as DiT-XL-2-256, Flux, and Stable Diffusion 3.5.</li>
</ul>

<h3>Title: Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Hanmo Chen, Chenghao Xu, Xu Yang, Xuan Chen, Cheng Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21896">https://arxiv.org/abs/2601.21896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21896">https://arxiv.org/pdf/2601.21896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21896]] Past- and Future-Informed KV Cache Policy with Salience Estimation in Autoregressive Video Diffusion(https://arxiv.org/abs/2601.21896)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video generation is pivotal to digital media creation, and recent advances in autoregressive video generation have markedly enhanced the efficiency of real-time video synthesis. However, existing approaches generally rely on heuristic KV Cache policies, which ignore differences in token importance in long-term video generation. This leads to the loss of critical spatiotemporal information and the accumulation of redundant, invalid cache, thereby degrading video generation quality and efficiency. To address this limitation, we first observe that token contributions to video generation are highly time-heterogeneous and accordingly propose a novel Past- and Future-Informed KV Cache Policy (PaFu-KV). Specifically, PaFu-KV introduces a lightweight Salience Estimation Head distilled from a bidirectional teacher to estimate salience scores, allowing the KV cache to retain informative tokens while discarding less relevant ones. This policy yields a better quality-efficiency trade-off by shrinking KV cache capacity and reducing memory footprint at inference time. Extensive experiments on benchmarks demonstrate that our method preserves high-fidelity video generation quality while enables accelerated inference, thereby enabling more efficient long-horizon video generation. Our code will be released upon paper acceptance.</li>
</ul>

<h3>Title: A Low-Complexity Plug-and-Play Deep Learning Model for Generalizable Massive MIMO Precoding</h3>
<ul>
<li><strong>Authors: </strong>Ali Hasanzadeh Karkan, Ahmed Ibrahim, Jean-François Frigon, François Leduc-Primeau</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21897">https://arxiv.org/abs/2601.21897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21897">https://arxiv.org/pdf/2601.21897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21897]] A Low-Complexity Plug-and-Play Deep Learning Model for Generalizable Massive MIMO Precoding(https://arxiv.org/abs/2601.21897)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Massive multiple-input multiple-output (mMIMO) downlink precoding offers high spectral efficiency but remains challenging to deploy in practice because near-optimal algorithms such as the weighted minimum mean squared error (WMMSE) are computationally expensive, and sensitive to SNR and channel-estimation quality, while existing deep learning (DL)-based solutions often lack robustness and require retraining for each deployment site. This paper proposes a plug-and-play precoder (PaPP), a DL framework with a backbone that can be trained for either fully digital (FDP) or hybrid beamforming (HBF) precoding and reused across sites, transmit-power levels, and with varying amounts of channel estimation error, avoiding the need to train a new model from scratch at each deployment. PaPP combines a high-capacity teacher and a compact student with a self-supervised loss that balances teacher imitation and normalized sum-rate, trained using meta-learning domain-generalization and transmit-power-aware input normalization. Numerical results on ray-tracing data from three unseen sites show that the PaPP FDP and HBF models both outperform conventional and deep learning baselines, after fine-tuning with a small set of local unlabeled samples. Across both architectures, PaPP achieves more than 21$\times$ reduction in modeled computation energy and maintains good performance under channel-estimation errors, making it a practical solution for energy-efficient mMIMO precoding.</li>
</ul>

<h3>Title: Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhiqing Cui, Siru Zhong, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21899">https://arxiv.org/abs/2601.21899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21899">https://arxiv.org/pdf/2601.21899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21899]] Breaking the Regional Barrier: Inductive Semantic Topology Learning for Worldwide Air Quality Forecasting(https://arxiv.org/abs/2601.21899)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Global air quality forecasting grapples with extreme spatial heterogeneity and the poor generalization of existing transductive models to unseen regions. To tackle this, we propose OmniAir, a semantic topology learning framework tailored for global station-level prediction. By encoding invariant physical environmental attributes into generalizable station identities and dynamically constructing adaptive sparse topologies, our approach effectively captures long-range non-Euclidean correlations and physical diffusion patterns across unevenly distributed global networks. We further curate WorldAir, a massive dataset covering over 7,800 stations worldwide. Extensive experiments show that OmniAir achieves state-of-the-art performance against 18 baselines, maintaining high efficiency and scalability with speeds nearly 10 times faster than existing models, while effectively bridging the monitoring gap in data-sparse regions.</li>
</ul>

<h3>Title: TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention</h3>
<ul>
<li><strong>Authors: </strong>Chuancheng Shi, Shangze Li, Wenjun Lu, Wenhua Wu, Cong Wang, Zifeng Cheng, Fei Shen, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21900">https://arxiv.org/abs/2601.21900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21900">https://arxiv.org/pdf/2601.21900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21900]] TraceRouter: Robust Safety for Large Foundation Models via Path-Level Intervention(https://arxiv.org/abs/2601.21900)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Despite their capabilities, large foundation models (LFMs) remain susceptible to adversarial manipulation. Current defenses predominantly rely on the "locality hypothesis", suppressing isolated neurons or features. However, harmful semantics act as distributed, cross-layer circuits, rendering such localized interventions brittle and detrimental to utility. To bridge this gap, we propose \textbf{TraceRouter}, a path-level framework that traces and disconnects the causal propagation circuits of illicit semantics. TraceRouter operates in three stages: (1) it pinpoints a sensitive onset layer by analyzing attention divergence; (2) it leverages sparse autoencoders (SAEs) and differential activation analysis to disentangle and isolate malicious features; and (3) it maps these features to downstream causal pathways via feature influence scores (FIS) derived from zero-out interventions. By selectively suppressing these causal chains, TraceRouter physically severs the flow of harmful information while leaving orthogonal computation routes intact. Extensive experiments demonstrate that TraceRouter significantly outperforms state-of-the-art baselines, achieving a superior trade-off between adversarial robustness and general utility. Our code will be publicly released. WARNING: This paper contains unsafe model responses.</li>
</ul>

<h3>Title: Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Cong Cao, Huanjing Yue, Shangbin Xie, Xin Liu, Jingyu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21922">https://arxiv.org/abs/2601.21922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21922">https://arxiv.org/pdf/2601.21922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21922]] Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models(https://arxiv.org/abs/2601.21922)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.</li>
</ul>

<h3>Title: Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Aghapour, Erhan Bayraktar, Ziqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21943">https://arxiv.org/abs/2601.21943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21943">https://arxiv.org/pdf/2601.21943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21943]] Entropy-Based Dimension-Free Convergence and Loss-Adaptive Schedules for Diffusion Models(https://arxiv.org/abs/2601.21943)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion generative models synthesize samples by discretizing reverse-time dynamics driven by a learned score (or denoiser). Existing convergence analyses of diffusion models typically scale at least linearly with the ambient dimension, and sharper rates often depend on intrinsic-dimension assumptions or other geometric restrictions on the target distribution. We develop an alternative, information-theoretic approach to dimension-free convergence that avoids any geometric assumptions. Under mild assumptions on the target distribution, we bound KL divergence between the target and generated distributions by $O(H^2/K)$ (up to endpoint factors), where $H$ is the Shannon entropy and $K$ is the number of sampling steps. Moreover, using a reformulation of the KL divergence, we propose a Loss-Adaptive Schedule (LAS) for efficient discretization of reverse SDE which is lightweight and relies only on the training loss, requiring no post-training heavy computation. Empirically, LAS improves sampling quality over common heuristic schedules.</li>
</ul>

<h3>Title: From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes</h3>
<ul>
<li><strong>Authors: </strong>Fariba Afrin Irany</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21955">https://arxiv.org/abs/2601.21955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21955">https://arxiv.org/pdf/2601.21955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21955]] From Generative Modeling to Clinical Classification: A GPT-Based Architecture for EHR Notes(https://arxiv.org/abs/2601.21955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing availability of unstructured clinical narratives in electronic health records (EHRs) has created new opportunities for automated disease characterization, cohort identification, and clinical decision support. However, modeling long, domain-specific clinical text remains challenging due to limited labeled data, severe class imbalance, and the high computational cost of adapting large pretrained language models. This study presents a GPT-based architecture for clinical text classification that adapts a pretrained decoder-only Transformer using a selective fine-tuning strategy. Rather than updating all model parameters, the majority of the GPT-2 backbone is frozen, and training is restricted to the final Transformer block, the final layer normalization, and a lightweight classification head. This approach substantially reduces the number of trainable parameters while preserving the representational capacity required to model complex clinical language. The proposed method is evaluated on radiology reports from the MIMIC-IV-Note dataset using uncertainty-aware CheXpert-style labels derived directly from report text. Experiments cover multiple problem formulations, including multi-label classification of radiographic findings, binary per-label classification under different uncertainty assumptions, and aggregate disease outcome prediction. Across varying dataset sizes, the model exhibits stable convergence behavior and strong classification performance, particularly in settings dominated by non-mention and negated findings. Overall, the results indicate that selective fine-tuning of pretrained generative language models provides an efficient and effective pathway for clinical text classification, enabling scalable adaptation to real-world EHR data while significantly reducing computational complexity.</li>
</ul>

<h3>Title: From Tokens to Blocks: A Block-Diffusion Perspective on Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Qianwei Yang, Dong Xu, Zhangfan Yang, Sisi Yuan, Zexuan Zhu, Jianqiang Li, Junkai Ji</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21964">https://arxiv.org/abs/2601.21964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21964">https://arxiv.org/pdf/2601.21964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21964]] From Tokens to Blocks: A Block-Diffusion Perspective on Molecular Generation(https://arxiv.org/abs/2601.21964)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Drug discovery can be viewed as a combinatorial search over an immense chemical space, motivating the development of deep generative models for de novo molecular design. Among these, GPT-based molecular language models (MLM) have shown strong molecular design performance by learning chemical syntax and semantics from large-scale data. However, existing MLMs face two fundamental limitations: they inadequately capture the graph-structured nature of molecules when formulated as next-token prediction problems, and they typically lack explicit mechanisms for target-aware generation. Here, we propose SoftMol, a unified framework that co-designs molecular representation, model architecture, and search strategy for target-aware molecular generation. SoftMol introduces soft fragments, a rule-free block representation of SMILES that enables diffusion-native modeling, and develops SoftBD, the first block-diffusion molecular language model that combines local bidirectional diffusion with autoregressive generation under molecular structural constraints. To favor generated molecules with high drug-likeness and synthetic accessibility, SoftBD is trained on a carefully curated dataset named ZINC-Curated. SoftMol further integrates a gated Monte Carlo tree search to assemble fragments in a target-aware manner. Experimental results show that, compared with current state-of-the-art models, SoftMol achieves 100% chemical validity, improves binding affinity by 9.7%, yields a 2-3x increase in molecular diversity, and delivers a 6.6x speedup in inference efficiency. Code is available at this https URL</li>
</ul>

<h3>Title: PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters</h3>
<ul>
<li><strong>Authors: </strong>Jian Gao, Yiwei Zou, Abhishek Pradhan, Wenhao Huang, Yumin Su, Kaiyuan Yang, Xuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21984">https://arxiv.org/abs/2601.21984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21984">https://arxiv.org/pdf/2601.21984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21984]] PowerGenie: Analytically-Guided Evolutionary Discovery of Superior Reconfigurable Power Converters(https://arxiv.org/abs/2601.21984)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Discovering superior circuit topologies requires navigating an exponentially large design space-a challenge traditionally reserved for human experts. Existing AI methods either select from predefined templates or generate novel topologies at a limited scale without rigorous verification, leaving large-scale performance-driven discovery underexplored. We present PowerGenie, a framework for automated discovery of higher-performance reconfigurable power converters at scale. PowerGenie introduces: (1) an automated analytical framework that determines converter functionality and theoretical performance limits without component sizing or SPICE simulation, and (2) an evolutionary finetuning method that co-evolves a generative model with its training distribution through fitness selection and uniqueness verification. Unlike existing methods that suffer from mode collapse and overfitting, our approach achieves higher syntax validity, function validity, novelty rate, and figure-of-merit (FoM). PowerGenie discovers a novel 8-mode reconfigurable converter with 23% higher FoM than the best training topology. SPICE simulations confirm average absolute efficiency gains of 10% across 8 modes and up to 17% at a single mode. Code is available at this https URL.</li>
</ul>

<h3>Title: Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields</h3>
<ul>
<li><strong>Authors: </strong>Yunyang Li, Lin Huang, Luojia Xia, Wenhe Zhang, Mark Gerstein</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21985">https://arxiv.org/abs/2601.21985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21985">https://arxiv.org/pdf/2601.21985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21985]] Elign: Equivariant Diffusion Model Alignment from Foundational Machine Learning Force Fields(https://arxiv.org/abs/2601.21985)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models for 3D molecular conformations must respect Euclidean symmetries and concentrate probability mass on thermodynamically favorable, mechanically stable structures. However, E(3)-equivariant diffusion models often reproduce biases from semi-empirical training data rather than capturing the equilibrium distribution of a high-fidelity Hamiltonian. While physics-based guidance can correct this, it faces two computational bottlenecks: expensive quantum-chemical evaluations (e.g., DFT) and the need to repeat such queries at every sampling step. We present Elign, a post-training framework that amortizes both costs. First, we replace expensive DFT evaluations with a faster, pretrained foundational machine-learning force field (MLFF) to provide physical signals. Second, we eliminate repeated run-time queries by shifting physical steering to the training phase. To achieve the second amortization, we formulate reverse diffusion as a reinforcement learning problem and introduce Force--Energy Disentangled Group Relative Policy Optimization (FED-GRPO) to fine-tune the denoising policy. FED-GRPO includes a potential-based energy reward and a force-based stability reward, which are optimized and group-normalized independently. Experiments show that Elign generates conformations with lower gold-standard DFT energies and forces, while improving stability. Crucially, inference remains as fast as unguided sampling, since no energy evaluations are required during generation.</li>
</ul>

<h3>Title: Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units</h3>
<ul>
<li><strong>Authors: </strong>Jianhui Chen, Yuzhang Luo, Liangming Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21996">https://arxiv.org/abs/2601.21996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21996">https://arxiv.org/pdf/2601.21996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21996]] Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units(https://arxiv.org/abs/2601.21996)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the emergence of interpretable heads, whereas random interventions show no effect. Our analysis reveals that repetitive structural data (e.g., LaTeX, XML) acts as a mechanistic catalyst. Furthermore, we observe that interventions targeting induction head formation induce a concurrent change in the model's in-context learning (ICL) capability. This provides direct causal evidence for the long-standing hypothesis regarding the functional link between induction heads and ICL. Finally, we propose a mechanistic data augmentation pipeline that consistently accelerates circuit convergence across model scales, providing a principled methodology for steering the developmental trajectories of LLMs.</li>
</ul>

<h3>Title: Causal World Modeling for Robot Control</h3>
<ul>
<li><strong>Authors: </strong>Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, Yinghao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.21998">https://arxiv.org/abs/2601.21998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.21998">https://arxiv.org/pdf/2601.21998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.21998]] Causal World Modeling for Robot Control(https://arxiv.org/abs/2601.21998)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.</li>
</ul>

<h3>Title: The Ensemble Inverse Problem: Applications and Methods</h3>
<ul>
<li><strong>Authors: </strong>Zhengyan Huan, Camila Pazos, Martin Klassen, Vincent Croft, Pierre-Hugues Beauchemin, Shuchin Aeron</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22029">https://arxiv.org/abs/2601.22029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22029">https://arxiv.org/pdf/2601.22029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22029]] The Ensemble Inverse Problem: Applications and Methods(https://arxiv.org/abs/2601.22029)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce a new multivariate statistical problem that we refer to as the Ensemble Inverse Problem (EIP). The aim of EIP is to invert for an ensemble that is distributed according to the pushforward of a prior under a forward process. In high energy physics (HEP), this is related to a widely known problem called unfolding, which aims to reconstruct the true physics distribution of quantities, such as momentum and angle, from measurements that are distorted by detector effects. In recent applications, the EIP also arises in full waveform inversion (FWI) and inverse imaging with unknown priors. We propose non-iterative inference-time methods that construct posterior samplers based on a new class of conditional generative models, which we call ensemble inverse generative models. For the posterior modeling, these models additionally use the ensemble information contained in the observation set on top of single measurements. Unlike existing methods, our proposed methods avoid explicit and iterative use of the forward model at inference time via training across several sets of truth-observation pairs that are consistent with the same forward model, but originate from a wide range of priors. We demonstrate that this training procedure implicitly encodes the likelihood model. The use of ensemble information helps posterior inference and enables generalization to unseen priors. We benchmark the proposed method on several synthetic and real datasets in inverse imaging, HEP, and FWI. The codes are available at this https URL.</li>
</ul>

<h3>Title: Causal Autoregressive Diffusion Language Model</h3>
<ul>
<li><strong>Authors: </strong>Junhao Ruan, Bei Li, Yongjing Yin, Pengcheng Huang, Xin Chen, Jingang Wang, Xunliang Cai, Tong Xiao, JingBo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22031">https://arxiv.org/abs/2601.22031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22031">https://arxiv.org/pdf/2601.22031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22031]] Causal Autoregressive Diffusion Language Model(https://arxiv.org/abs/2601.22031)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.</li>
</ul>

<h3>Title: Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving</h3>
<ul>
<li><strong>Authors: </strong>Linhan Wang, Zichong Yang, Chen Bai, Guoxiang Zhang, Xiaotong Liu, Xiaoyin Zheng, Xiao-Xiao Long, Chang-Tien Lu, Cheng Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22032">https://arxiv.org/abs/2601.22032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22032">https://arxiv.org/pdf/2601.22032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22032]] Drive-JEPA: Video JEPA Meets Multimodal Trajectory Distillation for End-to-End Driving(https://arxiv.org/abs/2601.22032)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>End-to-end autonomous driving increasingly leverages self-supervised video pretraining to learn transferable planning representations. However, pretraining video world models for scene understanding has so far brought only limited improvements. This limitation is compounded by the inherent ambiguity of driving: each scene typically provides only a single human trajectory, making it difficult to learn multimodal behaviors. In this work, we propose Drive-JEPA, a framework that integrates Video Joint-Embedding Predictive Architecture (V-JEPA) with multimodal trajectory distillation for end-to-end driving. First, we adapt V-JEPA for end-to-end driving, pretraining a ViT encoder on large-scale driving videos to produce predictive representations aligned with trajectory planning. Second, we introduce a proposal-centric planner that distills diverse simulator-generated trajectories alongside human trajectories, with a momentum-aware selection mechanism to promote stable and safe behavior. When evaluated on NAVSIM, the V-JEPA representation combined with a simple transformer-based decoder outperforms prior methods by 3 PDMS in the perception-free setting. The complete Drive-JEPA framework achieves 93.3 PDMS on v1 and 87.8 EPDMS on v2, setting a new state-of-the-art.</li>
</ul>

<h3>Title: Holographic generative flows with AdS/CFT</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Mirafzali, Sanjit Shashi, Sanya Murdeshwar, Edgar Shaghoulian, Daniele Venturi, Razvan Marinescu</a></li>
<li><strong>Subjects: </strong>cs.LG, gr-qc, hep-th</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22033">https://arxiv.org/abs/2601.22033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22033">https://arxiv.org/pdf/2601.22033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22033]] Holographic generative flows with AdS/CFT(https://arxiv.org/abs/2601.22033)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a framework for generative machine learning that leverages the holographic principle of quantum gravity, or to be more precise its manifestation as the anti-de Sitter/conformal field theory (AdS/CFT) correspondence, with techniques for deep learning and transport theory. Our proposal is to represent the flow of data from a base distribution to some learned distribution using the bulk-to-boundary mapping of scalar fields in AdS. In the language of machine learning, we are representing and augmenting the flow-matching algorithm with AdS physics. Using a checkerboard toy dataset and MNIST, we find that our model achieves faster and higher quality convergence than comparable physics-free flow-matching models. Our method provides a physically interpretable version of flow matching. More broadly, it establishes the utility of AdS physics and geometry in the development of novel paradigms in generative modeling.</li>
</ul>

<h3>Title: Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Longxuan Yu, Yu Fu, Shaorong Zhang, Hui Liu, Mukund Varma T, Greg Ver Steeg, Yue Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22035">https://arxiv.org/abs/2601.22035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22035">https://arxiv.org/pdf/2601.22035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22035]] Thinking Out of Order: When Output Order Stops Reflecting Reasoning Order in Diffusion Language Models(https://arxiv.org/abs/2601.22035)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) language models enforce a fixed left-to-right generation order, creating a fundamental limitation when the required output structure conflicts with natural reasoning (e.g., producing answers before explanations due to presentation or schema constraints). In such cases, AR models must commit to answers before generating intermediate reasoning, and this rigid constraint forces premature commitment. Masked diffusion language models (MDLMs), which iteratively refine all tokens in parallel, offer a way to decouple computation order from output structure. We validate this capability on GSM8K, Math500, and ReasonOrderQA, a benchmark we introduce with controlled difficulty and order-level evaluation. When prompts request answers before reasoning, AR models exhibit large accuracy gaps compared to standard chain-of-thought ordering (up to 67% relative drop), while MDLMs remain stable ($\leq$14% relative drop), a property we term "order robustness". Using ReasonOrderQA, we present evidence that MDLMs achieve order robustness by stabilizing simpler tokens (e.g., reasoning steps) earlier in the diffusion process than complex ones (e.g., final answers), enabling reasoning tokens to stabilize before answer commitment. Finally, we identify failure conditions where this advantage weakens, outlining the limits required for order robustness.</li>
</ul>

<h3>Title: MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</h3>
<ul>
<li><strong>Authors: </strong>Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Yan Xie, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22054">https://arxiv.org/abs/2601.22054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22054">https://arxiv.org/pdf/2601.22054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22054]] MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources(https://arxiv.org/abs/2601.22054)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at this http URL to support community research.</li>
</ul>

<h3>Title: Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Archer Wang, Emile Anand, Yilun Du, Marin Soljačić</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22057">https://arxiv.org/abs/2601.22057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22057">https://arxiv.org/pdf/2601.22057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22057]] Unsupervised Decomposition and Recombination with Discriminator-Driven Diffusion Models(https://arxiv.org/abs/2601.22057)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Decomposing complex data into factorized representations can reveal reusable components and enable synthesizing new samples via component recombination. We investigate this in the context of diffusion-based models that learn factorized latent spaces without factor-level supervision. In images, factors can capture background, illumination, and object attributes; in robotic videos, they can capture reusable motion components. To improve both latent factor discovery and quality of compositional generation, we introduce an adversarial training signal via a discriminator trained to distinguish between single-source samples and those generated by recombining factors across sources. By optimizing the generator to fool this discriminator, we encourage physical and semantic consistency in the resulting recombinations. Our method outperforms implementations of prior baselines on CelebA-HQ, Virtual KITTI, CLEVR, and Falcor3D, achieving lower FID scores and better disentanglement as measured by MIG and MCC. Furthermore, we demonstrate a novel application to robotic video trajectories: by recombining learned action components, we generate diverse sequences that significantly increase state-space coverage for exploration on the LIBERO benchmark.</li>
</ul>

<h3>Title: Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Qingyu Yin, Shuang Chen, Zhenfei Yin, Lin Chen, Zehui Chen, Yao Hu, Philip Torr, Feng Zhao, Wanli Ouyang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22060">https://arxiv.org/abs/2601.22060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22060">https://arxiv.org/pdf/2601.22060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22060]] Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models(https://arxiv.org/abs/2601.22060)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in this https URL.</li>
</ul>

<h3>Title: Making Foundation Models Probabilistic via Singular Value Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Mehmet Ozgur Turkoglu, Dominik J. Mühlematter, Alexander Becker, Konrad Schindler, Helge Aasen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22068">https://arxiv.org/abs/2601.22068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22068">https://arxiv.org/pdf/2601.22068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22068]] Making Foundation Models Probabilistic via Singular Value Ensembles(https://arxiv.org/abs/2601.22068)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model's knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) "knowledge directions". To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.</li>
</ul>

<h3>Title: Where Do the Joules Go? Diagnosing Inference Energy Consumption</h3>
<ul>
<li><strong>Authors: </strong>Jae-Won Chung, Ruofan Wu, Jeff J. Ma, Mosharaf Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22076">https://arxiv.org/abs/2601.22076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22076">https://arxiv.org/pdf/2601.22076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22076]] Where Do the Joules Go? Diagnosing Inference Energy Consumption(https://arxiv.org/abs/2601.22076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.</li>
</ul>

<h3>Title: RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Hanzhuo Huang, Qingyang Bao, Zekai Gu, Zhongshuo Du, Cheng Lin, Yuan Liu, Sibei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22094">https://arxiv.org/abs/2601.22094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22094">https://arxiv.org/pdf/2601.22094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22094]] RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation(https://arxiv.org/abs/2601.22094)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.</li>
</ul>

<h3>Title: Prior-Informed Flow Matching for Graph Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Harvey Chen, Nicolas Zilberstein, Santiago Segarra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22107">https://arxiv.org/abs/2601.22107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22107">https://arxiv.org/pdf/2601.22107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22107]] Prior-Informed Flow Matching for Graph Reconstruction(https://arxiv.org/abs/2601.22107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.</li>
</ul>

<h3>Title: Value-Based Pre-Training with Downstream Feedback</h3>
<ul>
<li><strong>Authors: </strong>Shuqi Ke, Giulia Fanti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22108">https://arxiv.org/abs/2601.22108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22108">https://arxiv.org/pdf/2601.22108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22108]] Value-Based Pre-Training with Downstream Feedback(https://arxiv.org/abs/2601.22108)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.</li>
</ul>

<h3>Title: Creative Image Generation with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Kunpeng Song, Ahmed Elgammal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22125">https://arxiv.org/abs/2601.22125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22125">https://arxiv.org/pdf/2601.22125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22125]] Creative Image Generation with Diffusion Model(https://arxiv.org/abs/2601.22125)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</li>
</ul>

<h3>Title: EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22127">https://arxiv.org/abs/2601.22127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22127">https://arxiv.org/pdf/2601.22127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22127]] EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers(https://arxiv.org/abs/2601.22127)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.</li>
</ul>

<h3>Title: PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</h3>
<ul>
<li><strong>Authors: </strong>Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22135">https://arxiv.org/abs/2601.22135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22135">https://arxiv.org/pdf/2601.22135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22135]] PI-Light: Physics-Inspired Diffusion for Full-Image Relighting(https://arxiv.org/abs/2601.22135)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($\pi$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $\pi$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.</li>
</ul>

<h3>Title: FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale</h3>
<ul>
<li><strong>Authors: </strong>Ajay Patel, Colin Raffel, Chris Callison-Burch</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22146">https://arxiv.org/abs/2601.22146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22146">https://arxiv.org/pdf/2601.22146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22146]] FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale(https://arxiv.org/abs/2601.22146)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised "predict the next word" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of "instruction-tuning" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With "supervised" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at this https URL .</li>
</ul>

<h3>Title: One-step Latent-free Image Generation with Pixel Mean Flows</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22158">https://arxiv.org/abs/2601.22158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22158">https://arxiv.org/pdf/2601.22158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22158]] One-step Latent-free Image Generation with Pixel Mean Flows(https://arxiv.org/abs/2601.22158)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
