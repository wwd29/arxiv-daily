<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-18</h1>
<h3>Title: On Unsupervised Image-to-image translation and GAN stability</h3>
<ul>
<li><strong>Authors: </strong>BahaaEddin AlAila, Zahra Jandaghi, Abolfazl Farahani, Mohammad Ziad Al-Saad</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09646">https://arxiv.org/abs/2403.09646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09646">https://arxiv.org/pdf/2403.09646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09646]] On Unsupervised Image-to-image translation and GAN stability(https://arxiv.org/abs/2403.09646)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The problem of image-to-image translation is one that is intruiging and challenging at the same time, for the impact potential it can have on a wide variety of other computer vision applications like colorization, inpainting, segmentation and others. Given the high-level of sophistication needed to extract patterns from one domain and successfully applying them to another, especially, in a completely unsupervised (unpaired) manner, this problem has gained much attention as of the last few years. It is one of the first problems where successful applications to deep generative models, and especially Generative Adversarial Networks achieved astounding results that are actually of realworld impact, rather than just a show of theoretical prowess; the such that has been dominating the GAN world. In this work, we study some of the failure cases of a seminal work in the field, CycleGAN [1] and hypothesize that they are GAN-stability related, and propose two general models to try to alleviate these problems. We also reach the same conclusion of the problem being ill-posed that has been also circulating in the literature lately.</li>
</ul>

<h3>Title: STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video  Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Pum Jun Kim, Seojun Kim, Jaejun Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09669">https://arxiv.org/abs/2403.09669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09669">https://arxiv.org/pdf/2403.09669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09669]] STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video  Generative Models(https://arxiv.org/abs/2403.09669)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image generative models have made significant progress in generating realistic and diverse images, supported by comprehensive guidance from various evaluation metrics. However, current video generative models struggle to generate even short video clips, with limited tools that provide insights for improvements. Current video evaluation metrics are simple adaptations of image metrics by switching the embeddings with video embedding networks, which may underestimate the unique characteristics of video. Our analysis reveals that the widely used Frechet Video Distance (FVD) has a stronger emphasis on the spatial aspect than the temporal naturalness of video and is inherently constrained by the input size of the embedding networks used, limiting it to 16 frames. Additionally, it demonstrates considerable instability and diverges from human evaluations. To address the limitations, we propose STREAM, a new video evaluation metric uniquely designed to independently evaluate spatial and temporal aspects. This feature allows comprehensive analysis and evaluation of video generative models from various perspectives, unconstrained by video length. We provide analytical and experimental evidence demonstrating that STREAM provides an effective evaluation tool for both visual and temporal quality of videos, offering insights into area of improvement for video generative models. To the best of our knowledge, STREAM is the first evaluation metric that can separately assess the temporal and spatial aspects of videos. Our code is available at STREAM.</li>
</ul>

<h3>Title: Open-Universe Indoor Scene Generation using LLM Program Synthesis and  Uncurated Object Databases</h3>
<ul>
<li><strong>Authors: </strong>Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, Daniel Ritchie</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09675">https://arxiv.org/abs/2403.09675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09675">https://arxiv.org/pdf/2403.09675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09675]] Open-Universe Indoor Scene Generation using LLM Program Synthesis and  Uncurated Object Databases(https://arxiv.org/abs/2403.09675)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a system for generating indoor scenes in response to text prompts. The prompts are not limited to a fixed vocabulary of scene descriptions, and the objects in generated scenes are not restricted to a fixed set of object categories -- we call this setting indoor scene generation. Unlike most prior work on indoor scene generation, our system does not require a large training dataset of existing 3D scenes. Instead, it leverages the world knowledge encoded in pre-trained large language models (LLMs) to synthesize programs in a domain-specific layout language that describe objects and spatial relations between them. Executing such a program produces a specification of a constraint satisfaction problem, which the system solves using a gradient-based optimization scheme to produce object positions and orientations. To produce object geometry, the system retrieves 3D meshes from a database. Unlike prior work which uses databases of category-annotated, mutually-aligned meshes, we develop a pipeline using vision-language models (VLMs) to retrieve meshes from massive databases of un-annotated, inconsistently-aligned meshes. Experimental evaluations show that our system outperforms generative models trained on 3D data for traditional, closed-universe scene generation tasks; it also outperforms a recent LLM-based layout generation method on open-universe scene generation.</li>
</ul>

<h3>Title: Counterfactual Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Yushu Pan, Elias Bareinboim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09683">https://arxiv.org/abs/2403.09683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09683">https://arxiv.org/pdf/2403.09683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09683]] Counterfactual Image Editing(https://arxiv.org/abs/2403.09683)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Counterfactual image editing is an important task in generative AI, which asks how an image would look if certain features were different. The current literature on the topic focuses primarily on changing individual features while remaining silent about the causal relationships between these features, as present in the real world. In this paper, we formalize the counterfactual image editing task using formal language, modeling the causal relationships between latent generative factors and images through a special type of model called augmented structural causal models (ASCMs). Second, we show two fundamental impossibility results: (1) counterfactual editing is impossible from i.i.d. image samples and their corresponding labels alone; (2) even when the causal relationships between the latent generative factors and images are available, no guarantees regarding the output of the model can be provided. Third, we propose a relaxation for this challenging problem by approximating non-identifiable counterfactual distributions with a new family of counterfactual-consistent estimators. This family exhibits the desirable property of preserving features that the user cares about across both factual and counterfactual worlds. Finally, we develop an efficient algorithm to generate counterfactual images by leveraging neural causal models.</li>
</ul>

<h3>Title: Shapley Values-Powered Framework for Fair Reward Split in Content  Produced by GenAI</h3>
<ul>
<li><strong>Authors: </strong>Alex Glinsky, Alexey Sokolsky</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09700">https://arxiv.org/abs/2403.09700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09700">https://arxiv.org/pdf/2403.09700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09700]] Shapley Values-Powered Framework for Fair Reward Split in Content  Produced by GenAI(https://arxiv.org/abs/2403.09700)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>It is evident that, currently, generative models are surpassed in quality by human professionals. However, with the advancements in Artificial Intelligence, this gap will narrow, leading to scenarios where individuals who have dedicated years of their lives to mastering a skill become obsolete due to their high costs, which are inherently linked to the time they require to complete a task -- a task that AI could accomplish in minutes or seconds. To avoid future social upheavals, we must, even now, contemplate how to fairly assess the contributions of such individuals in training generative models and how to compensate them for the reduction or complete loss of their incomes. In this work, we propose a method to structure collaboration between model developers and data providers. To achieve this, we employ Shapley Values to quantify the contribution of artist(s) in an image generated by the Stable Diffusion-v1.5 model and to equitably allocate the reward among them.</li>
</ul>

<h3>Title: Generator-Guided Crowd Reaction Assessment</h3>
<ul>
<li><strong>Authors: </strong>Sohom Ghosh, Chung-Chi Chen, Sudip Kumar Naskar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09702">https://arxiv.org/abs/2403.09702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09702">https://arxiv.org/pdf/2403.09702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09702]] Generator-Guided Crowd Reaction Assessment(https://arxiv.org/abs/2403.09702)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the realm of social media, understanding and predicting post reach is a significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM) task designed to estimate if a given social media post will receive more reaction than another, a particularly essential task for digital marketers and content writers. We introduce the Crowd Reaction Estimation Dataset (CRED), consisting of pairs of tweets from The White House with comparative measures of retweet count. The proposed Generator-Guided Estimation Approach (GGEA) leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2, and Claude, to guide classification models for making better predictions. Our results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder architecture with tweet content and responses generated by Claude, performs optimally. We further use a T5-based paraphraser to generate paraphrases of a given post and demonstrate GGEA's ability to predict which post will elicit the most reactions. We believe this novel application of LLMs provides a significant advancement in predicting social media post reach.</li>
</ul>

<h3>Title: Concept-aware Data Construction Improves In-context Learning of Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Michal Štefánik, Marek Kadlčík, Petr Sojka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09703">https://arxiv.org/abs/2403.09703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09703">https://arxiv.org/pdf/2403.09703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09703]] Concept-aware Data Construction Improves In-context Learning of Language  Models(https://arxiv.org/abs/2403.09703)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings. In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functional deficiencies of the previous models. Finally, we show that concept-aware in-context learning is more effective for a majority of new tasks when compared to traditional instruction tuning, resulting in a performance comparable to the previous in-context learners using magnitudes of more training data.</li>
</ul>

<h3>Title: Investigating the performance of Retrieval-Augmented Generation and  fine-tuning for the development of AI-driven knowledge-based systems</h3>
<ul>
<li><strong>Authors: </strong>Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09727">https://arxiv.org/abs/2403.09727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09727">https://arxiv.org/pdf/2403.09727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09727]] Investigating the performance of Retrieval-Augmented Generation and  fine-tuning for the development of AI-driven knowledge-based systems(https://arxiv.org/abs/2403.09727)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case of the BLEU score, and 53% based on the cosine similarity. This shows the significant advantage of RAG over FN in terms of hallucination, which is not offset by the fact that the average 8% better METEOR score of FN models indicates greater creativity compared to RAG.</li>
</ul>

<h3>Title: PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with  Cross-consistency</h3>
<ul>
<li><strong>Authors: </strong>Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09732">https://arxiv.org/abs/2403.09732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09732">https://arxiv.org/pdf/2403.09732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09732]] PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with  Cross-consistency(https://arxiv.org/abs/2403.09732)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the prompt's schema information and instruct the LLM to produce the final SQL. Finally, as the post-refinement module, we propose using cross-consistency across different LLMs rather than self-consistency within a particular LLM. Our methods achieve new SOTA results on the Spider benchmark, with an execution accuracy of 87.6%.</li>
</ul>

<h3>Title: Evaluating Large Language Models as Generative User Simulators for  Conversational Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09738">https://arxiv.org/abs/2403.09738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09738">https://arxiv.org/pdf/2403.09738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09738]] Evaluating Large Language Models as Generative User Simulators for  Conversational Recommendation(https://arxiv.org/abs/2403.09738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.</li>
</ul>

<h3>Title: Re-Search for The Truth: Multi-round Retrieval-augmented Large Language  Models are Strong Fake News Detectors</h3>
<ul>
<li><strong>Authors: </strong>Guanghua Li, Wensheng Lu, Wei Zhang, Defu Lian, Kezhong Lu, Rui Mao, Kai Shu, Hao Liao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09747">https://arxiv.org/abs/2403.09747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09747">https://arxiv.org/pdf/2403.09747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09747]] Re-Search for The Truth: Multi-round Retrieval-augmented Large Language  Models are Strong Fake News Detectors(https://arxiv.org/abs/2403.09747)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The proliferation of fake news has had far-reaching implications on politics, the economy, and society at large. While Fake news detection methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. Large Language Models (LLMs), known for their remarkable reasoning and generative capabilities, introduce a new frontier for fake news detection. However, like traditional methods, LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these challenges, we introduce a novel, retrieval-augmented LLMs framework--the first of its kind to automatically and strategically extract key evidence from web sources for claim verification. Employing a multi-round retrieval strategy, our framework ensures the acquisition of sufficient, relevant evidence, thereby enhancing performance. Comprehensive experiments across three real-world datasets validate the framework's superiority over existing methods. Importantly, our model not only delivers accurate verdicts but also offers human-readable explanations to improve result interpretability.</li>
</ul>

<h3>Title: Explainable Machine Learning-Based Security and Privacy Protection  Framework for Internet of Medical Things Systems</h3>
<ul>
<li><strong>Authors: </strong>Ayoub Si-ahmed, Mohammed Ali Al-Garadi, Narhimene Boustia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09752">https://arxiv.org/abs/2403.09752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09752">https://arxiv.org/pdf/2403.09752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09752]] Explainable Machine Learning-Based Security and Privacy Protection  Framework for Internet of Medical Things Systems(https://arxiv.org/abs/2403.09752)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The Internet of Medical Things (IoMT) transcends traditional medical boundaries, enabling a transition from reactive treatment to proactive prevention. This innovative method revolutionizes healthcare by facilitating early disease detection and tailored care, particularly in chronic disease management, where IoMT automates treatments based on real-time health data collection. Nonetheless, its benefits are countered by significant security challenges that endanger the lives of its users due to the sensitivity and value of the processed data, thereby attracting malicious interests. Moreover, the utilization of wireless communication for data transmission exposes medical data to interception and tampering by cybercriminals. Additionally, anomalies may arise due to human errors, network interference, or hardware malfunctions. In this context, anomaly detection based on Machine Learning (ML) is an interesting solution, but it comes up against obstacles in terms of explicability and protection of privacy. To address these challenges, a new framework for Intrusion Detection Systems (IDS) is introduced, leveraging Artificial Neural Networks (ANN) for intrusion detection while utilizing Federated Learning (FL) for privacy preservation. Additionally, eXplainable Artificial Intelligence (XAI) methods are incorporated to enhance model explanation and interpretation. The efficacy of the proposed framework is evaluated and compared with centralized approaches using multiple datasets containing network and medical data, simulating various attack types impacting the confidentiality, integrity, and availability of medical and physiological data. The results obtained offer compelling evidence that the FL method performs comparably to the centralized method, demonstrating high performance. Additionally, it affords the dual advantage of safeguarding privacy and providing model explanation.</li>
</ul>

<h3>Title: Emotional Intelligence Through Artificial Intelligence : NLP and Deep  Learning in the Analysis of Healthcare Texts</h3>
<ul>
<li><strong>Authors: </strong>Prashant Kumar Nag, Amit Bhagat, R. Vishnu Priya, Deepak kumar Khare</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09762">https://arxiv.org/abs/2403.09762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09762">https://arxiv.org/pdf/2403.09762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09762]] Emotional Intelligence Through Artificial Intelligence : NLP and Deep  Learning in the Analysis of Healthcare Texts(https://arxiv.org/abs/2403.09762)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This manuscript presents a methodical examination of the utilization of Artificial Intelligence in the assessment of emotions in texts related to healthcare, with a particular focus on the incorporation of Natural Language Processing and deep learning technologies. We scrutinize numerous research studies that employ AI to augment sentiment analysis, categorize emotions, and forecast patient outcomes based on textual information derived from clinical narratives, patient feedback on medications, and online health discussions. The review demonstrates noteworthy progress in the precision of algorithms used for sentiment classification, the prognostic capabilities of AI models for neurodegenerative diseases, and the creation of AI-powered systems that offer support in clinical decision-making. Remarkably, the utilization of AI applications has exhibited an enhancement in personalized therapy plans by integrating patient sentiment and contributing to the early identification of mental health disorders. There persist challenges, which encompass ensuring the ethical application of AI, safeguarding patient confidentiality, and addressing potential biases in algorithmic procedures. Nevertheless, the potential of AI to revolutionize healthcare practices is unmistakable, offering a future where healthcare is not only more knowledgeable and efficient but also more empathetic and centered around the needs of patients. This investigation underscores the transformative influence of AI on healthcare, delivering a comprehensive comprehension of its role in examining emotional content in healthcare texts and highlighting the trajectory towards a more compassionate approach to patient care. The findings advocate for a harmonious synergy between AI's analytical capabilities and the human aspects of healthcare.</li>
</ul>

<h3>Title: Helpful or Harmful? Exploring the Efficacy of Large Language Models for  Online Grooming Prevention</h3>
<ul>
<li><strong>Authors: </strong>Ellie Prosser, Matthew Edwards</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09795">https://arxiv.org/abs/2403.09795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09795">https://arxiv.org/pdf/2403.09795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09795]] Helpful or Harmful? Exploring the Efficacy of Large Language Models for  Online Grooming Prevention(https://arxiv.org/abs/2403.09795)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Powerful generative Large Language Models (LLMs) are becoming popular tools amongst the general public as question-answering systems, and are being utilised by vulnerable groups such as children. With children increasingly interacting with these tools, it is imperative for researchers to scrutinise the safety of LLMs, especially for applications that could lead to serious outcomes, such as online child safety queries. In this paper, the efficacy of LLMs for online grooming prevention is explored both for identifying and avoiding grooming through advice generation, and the impact of prompt design on model performance is investigated by varying the provided context and prompt specificity. In results reflecting over 6,000 LLM interactions, we find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models. We outline where and how models fall short, providing suggestions for improvement, and identify prompt designs that heavily altered model performance in troubling ways, with findings that can be used to inform best practice usage guides.</li>
</ul>

<h3>Title: Self-Supervised Learning for Time Series: Contrastive or Generative?</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09809">https://arxiv.org/abs/2403.09809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09809">https://arxiv.org/pdf/2403.09809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09809]] Self-Supervised Learning for Time Series: Contrastive or Generative?(https://arxiv.org/abs/2403.09809)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has recently emerged as a powerful approach to learning representations from large-scale unlabeled data, showing promising results in time series analysis. The self-supervised representation learning can be categorized into two mainstream: contrastive and generative. In this paper, we will present a comprehensive comparative study between contrastive and generative methods in time series. We first introduce the basic frameworks for contrastive and generative SSL, respectively, and discuss how to obtain the supervision signal that guides the model optimization. We then implement classical algorithms (SimCLR vs. MAE) for each type and conduct a comparative analysis in fair settings. Our results provide insights into the strengths and weaknesses of each approach and offer practical recommendations for choosing suitable SSL methods. We also discuss the implications of our findings for the broader field of representation learning and propose future research directions. All the code and data are released at \url{https://github.com/DL4mHealth/SSL_Comparison}.</li>
</ul>

<h3>Title: Scaling Behavior of Machine Translation with Large Language Models under  Prompt Injection Attacks</h3>
<ul>
<li><strong>Authors: </strong>Zhifan Sun, Antonio Valerio Miceli-Barone</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09832">https://arxiv.org/abs/2403.09832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09832">https://arxiv.org/pdf/2403.09832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09832]] Scaling Behavior of Machine Translation with Large Language Models under  Prompt Injection Attacks(https://arxiv.org/abs/2403.09832)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023). To our knowledge, this is the first work to study non-trivial LLM scaling behaviour in a multi-lingual setting.</li>
</ul>

<h3>Title: ProMark: Proactive Diffusion Watermarking for Causal Attribution</h3>
<ul>
<li><strong>Authors: </strong>Vishal Asnani, John Collomosse, Tu Bui, Xiaoming Liu, Shruti Agarwal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09914">https://arxiv.org/abs/2403.09914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09914">https://arxiv.org/pdf/2403.09914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09914]] ProMark: Proactive Diffusion Watermarking for Causal Attribution(https://arxiv.org/abs/2403.09914)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) is transforming creative workflows through the capability to synthesize and manipulate images via high-level prompts. Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a synthetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input training images using imperceptible watermarks, and the diffusion models (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as $2^{16}$ unique watermarks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attribution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images.</li>
</ul>

<h3>Title: Predicting Generalization of AI Colonoscopy Models to Unseen Data</h3>
<ul>
<li><strong>Authors: </strong>Joel Shor, Carson McNeil, Yotam Intrator, Joseph R Ledsam, Hiro-o Yamano, Daisuke Tsurumaru, Hiroki Kayama, Atsushi Hamabe, Koji Ando, Mitsuhiko Ota, Haruei Ogino, Hiroshi Nakase, Kaho Kobayashi, Masaaki Miyo, Eiji Oki, Ichiro Takemasa, Ehud Rivlin, Roman Goldenberg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09920">https://arxiv.org/abs/2403.09920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09920">https://arxiv.org/pdf/2403.09920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09920]] Predicting Generalization of AI Colonoscopy Models to Unseen Data(https://arxiv.org/abs/2403.09920)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Background and aims Generalizability of AI colonoscopy algorithms is important for wider adoption in clinical practice. However, current techniques for evaluating performance on unseen data require expensive and time-intensive labels. Methods We use a "Masked Siamese Network" (MSN) to identify novel phenomena in unseen data and predict polyp detector performance. MSN is trained to predict masked out regions of polyp images, without any labels. We test MSN's ability to be trained on data only from Israel and detect unseen techniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes from Japan (354 videos, 128 hours). We also test MSN's ability to predict performance of Computer Aided Detection (CADe) of polyps on colonoscopies from both countries, even though MSN is not trained on data from Japan. Results MSN correctly identifies NBI and CE as less similar to Israel whitelight than Japan whitelight (bootstrapped z-test, |z| > 496, p < 10-8 for both) using the label-free Frechet distance. MSN detects NBI with 99% accuracy, predicts CE better than our heuristic (90% vs 79% accuracy) despite being trained only on whitelight, and is the only method that is robust to noisy labels. MSN predicts CADe polyp detector performance on in-domain Israel and out-of-domain Japan colonoscopies (r=0.79, 0.37 respectively). With few examples of Japan detector performance to train on, MSN prediction of Japan performance improves (r=0.56). Conclusion Our technique can identify distribution shifts in clinical data and can predict CADe detector performance on unseen data, without labels. Our self-supervised approach can aid in detecting when data in practice is different from training, such as between hospitals or data has meaningfully shifted from training. MSN has potential for application to medical image domains beyond colonoscopy.</li>
</ul>

<h3>Title: RadCLIP: Enhancing Radiologic Image Analysis through Contrastive  Language-Image Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Zhixiu Lu, Hailong Li, Lili He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09948">https://arxiv.org/abs/2403.09948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09948">https://arxiv.org/pdf/2403.09948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09948]] RadCLIP: Enhancing Radiologic Image Analysis through Contrastive  Language-Image Pre-training(https://arxiv.org/abs/2403.09948)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The integration of artificial intelligence (AI) with radiology has marked a transformative era in medical diagnostics. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiological imaging, including the interpretation of 2D and 3D radiological data, pose unique challenges that existing models, trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in medical imaging, we introduce RadCLIP: a pioneering cross-modal foundational model that harnesses Contrastive Language-Image Pre-training (CLIP) to refine radiologic image analysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored for volumetric image analysis and is trained using a comprehensive and diverse dataset of radiologic image-text pairs. Our evaluations demonstrate that RadCLIP effectively aligns radiological images with their corresponding textual annotations, and in the meantime, offers a robust vision backbone for radiologic imagery with significant promise.</li>
</ul>

<h3>Title: Controllable Text-to-3D Generation via Surface-Aligned Gaussian  Splatting</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Li, Yiming Chen, Lingzhe Zhao, Peidong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.09981">https://arxiv.org/abs/2403.09981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.09981">https://arxiv.org/pdf/2403.09981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.09981]] Controllable Text-to-3D Generation via Surface-Aligned Gaussian  Splatting(https://arxiv.org/abs/2403.09981)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While text-to-3D and image-to-3D generation tasks have received considerable attention, one important but under-explored field between them is controllable text-to-3D generation, which we mainly focus on in this work. To address this task, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network architecture designed to enhance existing pre-trained multi-view diffusion models by integrating additional input conditions, such as edge, depth, normal, and scribble maps. Our innovation lies in the introduction of a conditioning module that controls the base diffusion model using both local and global embeddings, which are computed from the input condition images and camera poses. Once trained, MVControl is able to offer 3D diffusion guidance for optimization-based 3D generation. And, 2) we propose an efficient multi-stage 3D generation pipeline that leverages the benefits of recent large reconstruction models and score distillation algorithm. Building upon our MVControl architecture, we employ a unique hybrid diffusion guidance method to direct the optimization process. In pursuit of efficiency, we adopt 3D Gaussians as our representation instead of the commonly used implicit representations. We also pioneer the use of SuGaR, a hybrid representation that binds Gaussians to mesh triangle faces. This approach alleviates the issue of poor geometry in 3D Gaussians and enables the direct sculpting of fine-grained geometry on the mesh. Extensive experiments demonstrate that our method achieves robust generalization and enables the controllable generation of high-quality 3D content.</li>
</ul>

<h3>Title: Federated Learning with Anomaly Detection via Gradient and  Reconstruction Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zahir Alsulaimawi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10000">https://arxiv.org/abs/2403.10000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10000">https://arxiv.org/pdf/2403.10000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10000]] Federated Learning with Anomaly Detection via Gradient and  Reconstruction Analysis(https://arxiv.org/abs/2403.10000)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of Federated Learning (FL), the challenge of ensuring data integrity against poisoning attacks is paramount, particularly for applications demanding stringent privacy preservation. Traditional anomaly detection strategies often struggle to adapt to the distributed nature of FL, leaving a gap our research aims to bridge. We introduce a novel framework that synergizes gradient-based analysis with autoencoder-driven data reconstruction to detect and mitigate poisoned data with unprecedented precision. Our approach uniquely combines detecting anomalous gradient patterns with identifying reconstruction errors, significantly enhancing FL model security. Validated through extensive experiments on MNIST and CIFAR-10 datasets, our method outperforms existing solutions by 15\% in anomaly detection accuracy while maintaining a minimal false positive rate. This robust performance, consistent across varied data types and network sizes, underscores our framework's potential in securing FL deployments in critical domains such as healthcare and finance. By setting new benchmarks for anomaly detection within FL, our work paves the way for future advancements in distributed learning security.</li>
</ul>

<h3>Title: Visual Foundation Models Boost Cross-Modal Unsupervised Domain  Adaptation for 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Xu, Weidong Yang, Lingdong Kong, Youquan Liu, Rui Zhang, Qingyuan Zhou, Ben Fei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10001">https://arxiv.org/abs/2403.10001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10001">https://arxiv.org/pdf/2403.10001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10001]] Visual Foundation Models Boost Cross-Modal Unsupervised Domain  Adaptation for 3D Semantic Segmentation(https://arxiv.org/abs/2403.10001)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Unsupervised domain adaptation (UDA) is vital for alleviating the workload of labeling 3D point cloud data and mitigating the absence of labels when facing a newly defined domain. Various methods of utilizing images to enhance the performance of cross-domain 3D segmentation have recently emerged. However, the pseudo labels, which are generated from models trained on the source domain and provide additional supervised signals for the unseen domain, are inadequate when utilized for 3D segmentation due to their inherent noisiness and consequently restrict the accuracy of neural networks. With the advent of 2D visual foundation models (VFMs) and their abundant knowledge prior, we propose a novel pipeline VFMSeg to further enhance the cross-modal unsupervised domain adaptation framework by leveraging these models. In this work, we study how to harness the knowledge priors learned by VFMs to produce more accurate labels for unlabeled target domains and improve overall performance. We first utilize a multi-modal VFM, which is pre-trained on large scale image-text pairs, to provide supervised labels (VFM-PL) for images and point clouds from the target domain. Then, another VFM trained on fine-grained 2D masks is adopted to guide the generation of semantically augmented images and point clouds to enhance the performance of neural networks, which mix the data from source and target domains like view frustums (FrustumMixing). Finally, we merge class-wise prediction across modalities to produce more accurate annotations for unlabeled target domains. Our method is evaluated on various autonomous driving datasets and the results demonstrate a significant improvement for 3D segmentation task.</li>
</ul>

<h3>Title: ST-LDM: A Universal Framework for Text-Grounded Object Generation in  Real Images</h3>
<ul>
<li><strong>Authors: </strong>Xiangtian Xue, Jiasong Wu, Youyong Kong, Lotfi Senhadji, Huazhong Shu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10004">https://arxiv.org/abs/2403.10004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10004">https://arxiv.org/pdf/2403.10004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10004]] ST-LDM: A Universal Framework for Text-Grounded Object Generation in  Real Images(https://arxiv.org/abs/2403.10004)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present a novel image editing scenario termed Text-grounded Object Generation (TOG), defined as generating a new object in the real image spatially conditioned by textual descriptions. Existing diffusion models exhibit limitations of spatial perception in complex real-world scenes, relying on additional modalities to enforce constraints, and TOG imposes heightened challenges on scene comprehension under the weak supervision of linguistic information. We propose a universal framework ST-LDM based on Swin-Transformer, which can be integrated into any latent diffusion model with training-free backward guidance. ST-LDM encompasses a global-perceptual autoencoder with adaptable compression scales and hierarchical visual features, parallel with deformable multimodal transformer to generate region-wise guidance for the subsequent denoising process. We transcend the limitation of traditional attention mechanisms that only focus on existing visual features by introducing deformable feature alignment to hierarchically refine spatial positioning fused with multi-scale visual and linguistic information. Extensive Experiments demonstrate that our model enhances the localization of attention mechanisms while preserving the generative capabilities inherent to diffusion models.</li>
</ul>

<h3>Title: SphereDiffusion: Spherical Geometry-Aware Distortion Resilient Diffusion  Model</h3>
<ul>
<li><strong>Authors: </strong>Tao Wu, Xuewei Li, Zhongang Qi, Di Hu, Xintao Wang, Ying Shan, Xi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10044">https://arxiv.org/abs/2403.10044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10044">https://arxiv.org/pdf/2403.10044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10044]] SphereDiffusion: Spherical Geometry-Aware Distortion Resilient Diffusion  Model(https://arxiv.org/abs/2403.10044)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Controllable spherical panoramic image generation holds substantial applicative potential across a variety of domains.However, it remains a challenging task due to the inherent spherical distortion and geometry characteristics, resulting in low-quality content generation.In this paper, we introduce a novel framework of SphereDiffusion to address these unique challenges, for better generating high-quality and precisely controllable spherical panoramic images.For the spherical distortion characteristic, we embed the semantics of the distorted object with text encoding, then explicitly construct the relationship with text-object correspondence to better use the pre-trained knowledge of the planar images.Meanwhile, we employ a deformable technique to mitigate the semantic deviation in latent space caused by spherical distortion.For the spherical geometry characteristic, in virtue of spherical rotation invariance, we improve the data diversity and optimization objectives in the training process, enabling the model to better learn the spherical geometry characteristic.Furthermore, we enhance the denoising process of the diffusion model, enabling it to effectively use the learned geometric characteristic to ensure the boundary continuity of the generated images.With these specific techniques, experiments on Structured3D dataset show that SphereDiffusion significantly improves the quality of controllable spherical image generation and relatively reduces around 35% FID on average.</li>
</ul>

<h3>Title: RID-TWIN: An end-to-end pipeline for automatic face de-identification in  videos</h3>
<ul>
<li><strong>Authors: </strong>Anirban Mukherjee, Monjoy Narayan Choudhury, Dinesh Babu Jayagopi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10058">https://arxiv.org/abs/2403.10058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10058">https://arxiv.org/pdf/2403.10058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10058]] RID-TWIN: An end-to-end pipeline for automatic face de-identification in  videos(https://arxiv.org/abs/2403.10058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face de-identification in videos is a challenging task in the domain of computer vision, primarily used in privacy-preserving applications. Despite the considerable progress achieved through generative vision models, there remain multiple challenges in the latest approaches. They lack a comprehensive discussion and evaluation of aspects such as realism, temporal coherence, and preservation of non-identifiable features. In our work, we propose RID-Twin: a novel pipeline that leverages the state-of-the-art generative models, and decouples identity from motion to perform automatic face de-identification in videos. We investigate the task from a holistic point of view and discuss how our approach addresses the pertinent existing challenges in this domain. We evaluate the performance of our methodology on the widely employed VoxCeleb2 dataset, and also a custom dataset designed to accommodate the limitations of certain behavioral variations absent in the VoxCeleb2 dataset. We discuss the implications and advantages of our work and suggest directions for future research.</li>
</ul>

<h3>Title: PAME: Self-Supervised Masked Autoencoder for No-Reference Point Cloud  Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Shan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10061">https://arxiv.org/abs/2403.10061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10061">https://arxiv.org/pdf/2403.10061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10061]] PAME: Self-Supervised Masked Autoencoder for No-Reference Point Cloud  Quality Assessment(https://arxiv.org/abs/2403.10061)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>No-reference point cloud quality assessment (NR-PCQA) aims to automatically predict the perceptual quality of point clouds without reference, which has achieved remarkable performance due to the utilization of deep learning-based models. However, these data-driven models suffer from the scarcity of labeled data and perform unsatisfactorily in cross-dataset evaluations. To address this problem, we propose a self-supervised pre-training framework using masked autoencoders (PAME) to help the model learn useful representations without labels. Specifically, after projecting point clouds into images, our PAME employs dual-branch autoencoders, reconstructing masked patches from distorted images into the original patches within reference and distorted images. In this manner, the two branches can separately learn content-aware features and distortion-aware features from the projected images. Furthermore, in the model fine-tuning stage, the learned content-aware features serve as a guide to fuse the point cloud quality features extracted from different perspectives. Extensive experiments show that our method outperforms the state-of-the-art NR-PCQA methods on popular benchmarks in terms of prediction accuracy and generalizability.</li>
</ul>

<h3>Title: A survey of synthetic data augmentation methods in computer vision</h3>
<ul>
<li><strong>Authors: </strong>Alhassan Mumuni, Fuseini Mumuni, Nana Kobina Gerrar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10075">https://arxiv.org/abs/2403.10075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10075">https://arxiv.org/pdf/2403.10075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10075]] A survey of synthetic data augmentation methods in computer vision(https://arxiv.org/abs/2403.10075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The standard approach to tackling computer vision problems is to train deep convolutional neural network (CNN) models using large-scale image datasets which are representative of the target task. However, in many scenarios, it is often challenging to obtain sufficient image data for the target task. Data augmentation is a way to mitigate this challenge. A common practice is to explicitly transform existing images in desired ways so as to create the required volume and variability of training data necessary to achieve good generalization performance. In situations where data for the target domain is not accessible, a viable workaround is to synthesize training data from scratch--i.e., synthetic data augmentation. This paper presents an extensive review of synthetic data augmentation techniques. It covers data synthesis approaches based on realistic 3D graphics modeling, neural style transfer (NST), differential neural rendering, and generative artificial intelligence (AI) techniques such as generative adversarial networks (GANs) and variational autoencoders (VAEs). For each of these classes of methods, we focus on the important data generation and augmentation techniques, general scope of application and specific use-cases, as well as existing limitations and possible workarounds. Additionally, we provide a summary of common synthetic datasets for training computer vision models, highlighting the main features, application domains and supported tasks. Finally, we discuss the effectiveness of synthetic data augmentation methods. Since this is the first paper to explore synthetic data augmentation methods in great detail, we are hoping to equip readers with the necessary background information and in-depth knowledge of existing methods and their attendant issues.</li>
</ul>

<h3>Title: RangeLDM: Fast Realistic LiDAR Point Cloud Generation</h3>
<ul>
<li><strong>Authors: </strong>Qianjiang Hu, Zhimin Zhang, Wei Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10094">https://arxiv.org/abs/2403.10094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10094">https://arxiv.org/pdf/2403.10094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10094]] RangeLDM: Fast Realistic LiDAR Point Cloud Generation(https://arxiv.org/abs/2403.10094)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autonomous driving demands high-quality LiDAR data, yet the cost of physical LiDAR sensors presents a significant scaling-up challenge. While recent efforts have explored deep generative models to address this issue, they often consume substantial computational resources with slow generation speeds while suffering from a lack of realism. To address these limitations, we introduce RangeLDM, a novel approach for rapidly generating high-quality range-view LiDAR point clouds via latent diffusion models. We achieve this by correcting range-view data distribution for accurate projection from point clouds to range images via Hough voting, which has a critical impact on generative learning. We then compress the range images into a latent space with a variational autoencoder, and leverage a diffusion model to enhance expressivity. Additionally, we instruct the model to preserve 3D structural fidelity by devising a range-guided discriminator. Experimental results on KITTI-360 and nuScenes datasets demonstrate both the robust expressiveness and fast speed of our LiDAR point cloud generation.</li>
</ul>

<h3>Title: DiffMAC: Diffusion Manifold Hallucination Correction for High  Generalization Blind Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Nan Gao, Jia Li, Huaibo Huang, Zhi Zeng, Ke Shang, Shuwu Zhang, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10098">https://arxiv.org/abs/2403.10098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10098">https://arxiv.org/pdf/2403.10098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10098]] DiffMAC: Diffusion Manifold Hallucination Correction for High  Generalization Blind Face Restoration(https://arxiv.org/abs/2403.10098)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Blind face restoration (BFR) is a highly challenging problem due to the uncertainty of degradation patterns. Current methods have low generalization across photorealistic and heterogeneous domains. In this paper, we propose a Diffusion-Information-Diffusion (DID) framework to tackle diffusion manifold hallucination correction (DiffMAC), which achieves high-generalization face restoration in diverse degraded scenes and heterogeneous domains. Specifically, the first diffusion stage aligns the restored face with spatial feature embedding of the low-quality face based on AdaIN, which synthesizes degradation-removal results but with uncontrollable artifacts for some hard cases. Based on Stage I, Stage II considers information compression using manifold information bottleneck (MIB) and finetunes the first diffusion model to improve facial fidelity. DiffMAC effectively fights against blind degradation patterns and synthesizes high-quality faces with attribute and identity consistencies. Experimental results demonstrate the superiority of DiffMAC over state-of-the-art methods, with a high degree of generalization in real-world and heterogeneous settings. The source code and models will be public.</li>
</ul>

<h3>Title: TransLandSeg: A Transfer Learning Approach for Landslide Semantic  Segmentation Based on Vision Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Changhong Hou, Junchuan Yu, Daqing Ge, Liu Yang, Laidian Xi, Yunxuan Pang, Yi Wen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10127">https://arxiv.org/abs/2403.10127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10127">https://arxiv.org/pdf/2403.10127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10127]] TransLandSeg: A Transfer Learning Approach for Landslide Semantic  Segmentation Based on Vision Foundation Model(https://arxiv.org/abs/2403.10127)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Landslides are one of the most destructive natural disasters in the world, posing a serious threat to human life and safety. The development of foundation models has provided a new research paradigm for large-scale landslide detection. The Segment Anything Model (SAM) has garnered widespread attention in the field of image segmentation. However, our experiment found that SAM performed poorly in the task of landslide segmentation. We propose TransLandSeg, which is a transfer learning approach for landslide semantic segmentation based on a vision foundation model (VFM). TransLandSeg outperforms traditional semantic segmentation models on both the Landslide4Sense dataset and the Bijie landslide dataset. Our proposed adaptive transfer learning (ATL) architecture enables the powerful segmentation capability of SAM to be transferred to landslide detection by training only 1.3% of the number of the parameters of SAM, which greatly improves the training efficiency of the model. Finally we also conducted ablation experiments on models with different ATL structures, concluded that the deployment location and residual connection of ATL play an important role in TransLandSeg accuracy improvement.</li>
</ul>

<h3>Title: E4C: Enhance Editability for Text-Based Image Editing by Harnessing  Efficient CLIP Guidance</h3>
<ul>
<li><strong>Authors: </strong>Tianrui Huang, Pu Cao, Lu Yang, Chun Liu, Mengjie Hu, Zhiwei Liu, Qing Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10133">https://arxiv.org/abs/2403.10133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10133">https://arxiv.org/pdf/2403.10133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10133]] E4C: Enhance Editability for Text-Based Image Editing by Harnessing  Efficient CLIP Guidance(https://arxiv.org/abs/2403.10133)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based image editing is a composite process of preserving the source image content and generating new content or applying modifications. While current editing approaches have made improvements under text guidance, most of them have only focused on preserving the information of the input image, disregarding the importance of editability and alignment to the target prompt. In this paper, we prioritize the editability by proposing a zero-shot image editing method, named \textbf{E}nhance \textbf{E}ditability for text-based image \textbf{E}diting via \textbf{E}fficient \textbf{C}LIP guidance (\textbf{E4C}), which only requires inference-stage optimization to explicitly enhance the edibility and text alignment. Specifically, we develop a unified dual-branch feature-sharing pipeline that enables the preservation of the structure or texture of the source image while allowing the other to be adapted based on the editing task. We further integrate CLIP guidance into our pipeline by utilizing our novel random-gateway optimization mechanism to efficiently enhance the semantic alignment with the target prompt. Comprehensive quantitative and qualitative experiments demonstrate that our method effectively resolves the text alignment issues prevalent in existing methods while maintaining the fidelity to the source image, and performs well across a wide range of editing tasks.</li>
</ul>

<h3>Title: SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human  Generation</h3>
<ul>
<li><strong>Authors: </strong>Peng Zheng, Tao Liu, Zili Yi, Rui Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10166">https://arxiv.org/abs/2403.10166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10166">https://arxiv.org/pdf/2403.10166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10166]] SemanticHuman-HD: High-Resolution Semantic Disentangled 3D Human  Generation(https://arxiv.org/abs/2403.10166)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the development of neural radiance fields and generative models, numerous methods have been proposed for learning 3D human generation from 2D images. These methods allow control over the pose of the generated 3D human and enable rendering from different viewpoints. However, none of these methods explore semantic disentanglement in human image synthesis, i.e., they can not disentangle the generation of different semantic parts, such as the body, tops, and bottoms. Furthermore, existing methods are limited to synthesize images at $512^2$ resolution due to the high computational cost of neural radiance fields. To address these limitations, we introduce SemanticHuman-HD, the first method to achieve semantic disentangled human image synthesis. Notably, SemanticHuman-HD is also the first method to achieve 3D-aware image synthesis at $1024^2$ resolution, benefiting from our proposed 3D-aware super-resolution module. By leveraging the depth maps and semantic masks as guidance for the 3D-aware super-resolution, we significantly reduce the number of sampling points during volume rendering, thereby reducing the computational cost. Our comparative experiments demonstrate the superiority of our method. The effectiveness of each proposed component is also verified through ablation studies. Moreover, our method opens up exciting possibilities for various applications, including 3D garment generation, semantic-aware image synthesis, controllable image synthesis, and out-of-domain image synthesis.</li>
</ul>

<h3>Title: Animate Your Motion: Turning Still Images into Dynamic Videos</h3>
<ul>
<li><strong>Authors: </strong>Mingxiao Li, Bo Wan, Marie-Francine Moens, Tinne Tuytelaars</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10179">https://arxiv.org/abs/2403.10179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10179">https://arxiv.org/pdf/2403.10179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10179]] Animate Your Motion: Turning Still Images into Dynamic Videos(https://arxiv.org/abs/2403.10179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, diffusion models have made remarkable strides in text-to-video generation, sparking a quest for enhanced control over video outputs to more accurately reflect user intentions. Traditional efforts predominantly focus on employing either semantic cues, like images or depth maps, or motion-based conditions, like moving sketches or object bounding boxes. Semantic inputs offer a rich scene context but lack detailed motion specificity; conversely, motion inputs provide precise trajectory information but miss the broader semantic narrative. For the first time, we integrate both semantic and motion cues within a diffusion model for video generation, as demonstrated in Fig 1. To this end, we introduce the Scene and Motion Conditional Diffusion (SMCD), a novel methodology for managing multimodal inputs. It incorporates a recognized motion conditioning module and investigates various approaches to integrate scene conditions, promoting synergy between different modalities. For model training, we separate the conditions for the two modalities, introducing a two-stage training pipeline. Experimental results demonstrate that our design significantly enhances video quality, motion precision, and semantic coherence.</li>
</ul>

<h3>Title: Generative Region-Language Pretraining for Open-Ended Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, Jianfei Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10191">https://arxiv.org/abs/2403.10191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10191">https://arxiv.org/pdf/2403.10191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10191]] Generative Region-Language Pretraining for Open-Ended Object Detection(https://arxiv.org/abs/2403.10191)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent research, significant attention has been devoted to the open-vocabulary object detection task, aiming to generalize beyond the limited number of classes labeled during training and detect objects described by arbitrary category names at inference. Compared with conventional object detection, open vocabulary object detection largely extends the object detection categories. However, it relies on calculating the similarity between image regions and a set of arbitrary category names with a pretrained vision-and-language model. This implies that, despite its open-set nature, the task still needs the predefined object categories during the inference stage. This raises the question: What if we do not have exact knowledge of object categories during inference? In this paper, we call such a new setting as generative open-ended object detection, which is a more general and practical problem. To address it, we formulate object detection as a generative problem and propose a simple framework named GenerateU, which can detect dense objects and generate their names in a free-form way. Particularly, we employ Deformable DETR as a region proposal generator with a language model translating visual regions to object names. To assess the free-form object detection task, we introduce an evaluation method designed to quantitatively measure the performance of generative outcomes. Extensive experiments demonstrate strong zero-shot detection performance of our GenerateU. For example, on the LVIS dataset, our GenerateU achieves comparable results to the open-vocabulary object detection method GLIP, even though the category names are not seen by GenerateU during inference. Code is available at: https:// github.com/FoundationVision/GenerateU .</li>
</ul>

<h3>Title: BlindDiff: Empowering Degradation Modelling in Diffusion Models for  Blind Image Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Feng Li, Yixuan Wu, Zichao Liang, Runmin Cong, Huihui Bai, Yao Zhao, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10211">https://arxiv.org/abs/2403.10211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10211">https://arxiv.org/pdf/2403.10211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10211]] BlindDiff: Empowering Degradation Modelling in Diffusion Models for  Blind Image Super-Resolution(https://arxiv.org/abs/2403.10211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DM) have achieved remarkable promise in image super-resolution (SR). However, most of them are tailored to solving non-blind inverse problems with fixed known degradation settings, limiting their adaptability to real-world applications that involve complex unknown degradations. In this work, we propose BlindDiff, a DM-based blind SR method to tackle the blind degradation settings in SISR. BlindDiff seamlessly integrates the MAP-based optimization into DMs, which constructs a joint distribution of the low-resolution (LR) observation, high-resolution (HR) data, and degradation kernels for the data and kernel priors, and solves the blind SR problem by unfolding MAP approach along with the reverse process. Unlike most DMs, BlindDiff firstly presents a modulated conditional transformer (MCFormer) that is pre-trained with noise and kernel constraints, further serving as a posterior sampler to provide both priors simultaneously. Then, we plug a simple yet effective kernel-aware gradient term between adjacent sampling iterations that guides the diffusion model to learn degradation consistency knowledge. This also enables to joint refine the degradation model as well as HR images by observing the previous denoised sample. With the MAP-based reverse diffusion process, we show that BlindDiff advocates alternate optimization for blur kernel estimation and HR image restoration in a mutual reinforcing manner. Experiments on both synthetic and real-world datasets show that BlindDiff achieves the state-of-the-art performance with significant model complexity reduction compared to recent DM-based methods. Code will be available at \url{https://github.com/lifengcs/BlindDiff}</li>
</ul>

<h3>Title: From Chaos to Clarity: Time Series Anomaly Detection in Astronomical  Observations</h3>
<ul>
<li><strong>Authors: </strong>Xinli Hao, Yile Chen, Chen Yang, Zhihui Du, Chaohong Ma, Chao Wu, Xiaofeng Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10220">https://arxiv.org/abs/2403.10220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10220">https://arxiv.org/pdf/2403.10220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10220]] From Chaos to Clarity: Time Series Anomaly Detection in Astronomical  Observations(https://arxiv.org/abs/2403.10220)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>With the development of astronomical facilities, large-scale time series data observed by these facilities is being collected. Analyzing anomalies in these astronomical observations is crucial for uncovering potential celestial events and physical phenomena, thus advancing the scientific research process. However, existing time series anomaly detection methods fall short in tackling the unique characteristics of astronomical observations where each star is inherently independent but interfered by random concurrent noise, resulting in a high rate of false alarms. To overcome the challenges, we propose AERO, a novel two-stage framework tailored for unsupervised anomaly detection in astronomical observations. In the first stage, we employ a Transformer-based encoder-decoder architecture to learn the normal temporal patterns on each variate (i.e., star) in alignment with the characteristic of variate independence. In the second stage, we enhance the graph neural network with a window-wise graph structure learning to tackle the occurrence of concurrent noise characterized by spatial and temporal randomness. In this way, AERO is not only capable of distinguishing normal temporal patterns from potential anomalies but also effectively differentiating concurrent noise, thus decreasing the number of false alarms. We conducted extensive experiments on three synthetic datasets and three real-world datasets. The results demonstrate that AERO outperforms the compared baselines. Notably, compared to the state-of-the-art model, AERO improves the F1-score by up to 8.76% and 2.63% on synthetic and real-world datasets respectively.</li>
</ul>

<h3>Title: FDGaussian: Fast Gaussian Splatting from Single Image via  Geometric-aware Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Qijun Feng, Zhen Xing, Zuxuan Wu, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10242">https://arxiv.org/abs/2403.10242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10242">https://arxiv.org/pdf/2403.10242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10242]] FDGaussian: Fast Gaussian Splatting from Single Image via  Geometric-aware Diffusion Model(https://arxiv.org/abs/2403.10242)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reconstructing detailed 3D objects from single-view images remains a challenging task due to the limited information available. In this paper, we introduce FDGaussian, a novel two-stage framework for single-image 3D reconstruction. Recent methods typically utilize pre-trained 2D diffusion models to generate plausible novel views from the input image, yet they encounter issues with either multi-view inconsistency or lack of geometric fidelity. To overcome these challenges, we propose an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, enabling the generation of consistent multi-view images. Moreover, we further accelerate the state-of-the-art Gaussian Splatting incorporating epipolar attention to fuse images from different viewpoints. We demonstrate that FDGaussian generates images with high consistency across different views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. More examples can be found at our website https://qjfeng.net/FDGaussian/.</li>
</ul>

<h3>Title: Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion  Model and Implicit Neural Decoder</h3>
<ul>
<li><strong>Authors: </strong>Jinseok Kim, Tae-Kyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10255">https://arxiv.org/abs/2403.10255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10255">https://arxiv.org/pdf/2403.10255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10255]] Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion  Model and Implicit Neural Decoder(https://arxiv.org/abs/2403.10255)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Super-resolution (SR) and image generation are important tasks in computer vision and are widely adopted in real-world applications. Most existing methods, however, generate images only at fixed-scale magnification and suffer from over-smoothing and artifacts. Additionally, they do not offer enough diversity of output images nor image consistency at different scales. Most relevant work applied Implicit Neural Representation (INR) to the denoising diffusion model to obtain continuous-resolution yet diverse and high-quality SR results. Since this model operates in the image space, the larger the resolution of image is produced, the more memory and inference time is required, and it also does not maintain scale-specific consistency. We propose a novel pipeline that can super-resolve an input image or generate from a random noise a novel image at arbitrary scales. The method consists of a pretrained auto-encoder, a latent diffusion model, and an implicit neural decoder, and their learning strategies. The proposed method adopts diffusion processes in a latent space, thus efficient, yet aligned with output image space decoded by MLPs at arbitrary scales. More specifically, our arbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling from the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in series. The latent diffusion process is learnt by the denoising and the alignment losses jointly. Errors in output images are backpropagated via the fixed decoder, improving the quality of output images. In the extensive experiments using multiple public benchmarks on the two tasks i.e. image super-resolution and novel image generation at arbitrary scales, the proposed method outperforms relevant methods in metrics of image quality, diversity and scale consistency. It is significantly better than the relevant prior-art in the inference speed and memory usage.</li>
</ul>

<h3>Title: Towards Generalizable Deepfake Video Detection with Thumbnail Layout and  Graph Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuting Xu, Jian Liang, Lijun Sheng, Xiao-Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10261">https://arxiv.org/abs/2403.10261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10261">https://arxiv.org/pdf/2403.10261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10261]] Towards Generalizable Deepfake Video Detection with Thumbnail Layout and  Graph Reasoning(https://arxiv.org/abs/2403.10261)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The deepfake threats to society and cybersecurity have provoked significant public apprehension, driving intensified efforts within the realm of deepfake video detection. Current video-level methods are mostly based on {3D CNNs} resulting in high computational demands, although have achieved good performance. This paper introduces an elegantly simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. This transformation process involves sequentially masking frames at the same positions within each frame. These frames are then resized into sub-frames and reorganized into the predetermined layout, forming thumbnails. TALL is model-agnostic and has remarkable simplicity, necessitating only minimal code modifications. Furthermore, we introduce a graph reasoning block (GRB) and semantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB enhances interactions between different semantic regions to capture semantic-level inconsistency clues. The semantic consistency loss imposes consistency constraints on semantic features to improve model generalization ability. Extensive experiments on intra-dataset, cross-dataset, diffusion-generated image detection, and deepfake generation method recognition show that TALL++ achieves results surpassing or comparable to the state-of-the-art methods, demonstrating the effectiveness of our approaches for various deepfake detection problems. The code is available at https://github.com/rainy-xu/TALL4Deepfake.</li>
</ul>

<h3>Title: Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification  with Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Shang-Hsuan Chiang, Ming-Chih Lo, Lin-Wei Chao, Wen-Chih Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10281">https://arxiv.org/abs/2403.10281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10281">https://arxiv.org/pdf/2403.10281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10281]] Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification  with Fine-Tuning(https://arxiv.org/abs/2403.10281)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.</li>
</ul>

<h3>Title: Few-Shot Image Classification and Segmentation as Visual Question  Answering Using Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tian Meng, Yang Tao, Ruilin Lyu, Wuliang Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10287">https://arxiv.org/abs/2403.10287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10287">https://arxiv.org/pdf/2403.10287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10287]] Few-Shot Image Classification and Segmentation as Visual Question  Answering Using Vision-Language Models(https://arxiv.org/abs/2403.10287)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The task of few-shot image classification and segmentation (FS-CS) involves classifying and segmenting target objects in a query image, given only a few examples of the target classes. We introduce the Vision-Instructed Segmentation and Evaluation (VISE) method that transforms the FS-CS problem into the Visual Question Answering (VQA) problem, utilising Vision-Language Models (VLMs), and addresses it in a training-free manner. By enabling a VLM to interact with off-the-shelf vision models as tools, the proposed method is capable of classifying and segmenting target objects using only image-level labels. Specifically, chain-of-thought prompting and in-context learning guide the VLM to answer multiple-choice questions like a human; vision models such as YOLO and Segment Anything Model (SAM) assist the VLM in completing the task. The modular framework of the proposed method makes it easily extendable. Our approach achieves state-of-the-art performance on the Pascal-5i and COCO-20i datasets.</li>
</ul>

<h3>Title: Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time  (CBoTT)</h3>
<ul>
<li><strong>Authors: </strong>Varol Kayhan, Shivendu Shivendu, Rouzbeh Behnia, Clinton Daniel, Manish Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10327">https://arxiv.org/abs/2403.10327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10327">https://arxiv.org/pdf/2403.10327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10327]] Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time  (CBoTT)(https://arxiv.org/abs/2403.10327)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Threat hunting is sifting through system logs to detect malicious activities that might have bypassed existing security measures. It can be performed in several ways, one of which is based on detecting anomalies. We propose an unsupervised framework, called continuous bag-of-terms-and-time (CBoTT), and publish its application programming interface (API) to help researchers and cybersecurity analysts perform anomaly-based threat hunting among SIEM logs geared toward process auditing on endpoint devices. Analyses show that our framework consistently outperforms benchmark approaches. When logs are sorted by likelihood of being an anomaly (from most likely to least), our approach identifies anomalies at higher percentiles (between 1.82-6.46) while benchmark approaches identify the same anomalies at lower percentiles (between 3.25-80.92). This framework can be used by other researchers to conduct benchmark analyses and cybersecurity analysts to find anomalies in SIEM logs.</li>
</ul>

<h3>Title: Generation is better than Modification: Combating High Class Homophily  Variance in Graph Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Rui Zhang, Dawei Cheng, Xin Liu, Jie Yang, Yi Ouyang, Xian Wu, Yefeng Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10339">https://arxiv.org/abs/2403.10339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10339">https://arxiv.org/pdf/2403.10339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10339]] Generation is better than Modification: Combating High Class Homophily  Variance in Graph Anomaly Detection(https://arxiv.org/abs/2403.10339)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Graph-based anomaly detection is currently an important research topic in the field of graph neural networks (GNNs). We find that in graph anomaly detection, the homophily distribution differences between different classes are significantly greater than those in homophilic and heterophilic graphs. For the first time, we introduce a new metric called Class Homophily Variance, which quantitatively describes this phenomenon. To mitigate its impact, we propose a novel GNN model named Homophily Edge Generation Graph Neural Network (HedGe). Previous works typically focused on pruning, selecting or connecting on original relationships, and we refer to these methods as modifications. Different from these works, our method emphasizes generating new relationships with low class homophily variance, using the original relationships as an auxiliary. HedGe samples homophily adjacency matrices from scratch using a self-attention mechanism, and leverages nodes that are relevant in the feature space but not directly connected in the original graph. Additionally, we modify the loss function to punish the generation of unnecessary heterophilic edges by the model. Extensive comparison experiments demonstrate that HedGe achieved the best performance across multiple benchmark datasets, including anomaly detection and edgeless node classification. The proposed model also improves the robustness under the novel Heterophily Attack with increased class homophily variance on other graph classification tasks.</li>
</ul>

<h3>Title: SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric  hybrid solution</h3>
<ul>
<li><strong>Authors: </strong>Hala Djeghim, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou, Désiré Sidibé</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10344">https://arxiv.org/abs/2403.10344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10344">https://arxiv.org/pdf/2403.10344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10344]] SCILLA: SurfaCe Implicit Learning for Large Urban Area, a volumetric  hybrid solution(https://arxiv.org/abs/2403.10344)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Neural implicit surface representation methods have recently shown impressive 3D reconstruction results. However, existing solutions struggle to reconstruct urban outdoor scenes due to their large, unbounded, and highly detailed nature. Hence, to achieve accurate reconstructions, additional supervision data such as LiDAR, strong geometric priors, and long training times are required. To tackle such issues, we present SCILLA, a new hybrid implicit surface learning method to reconstruct large driving scenes from 2D images. SCILLA's hybrid architecture models two separate implicit fields: one for the volumetric density and another for the signed distance to the surface. To accurately represent urban outdoor scenarios, we introduce a novel volume-rendering strategy that relies on self-supervised probabilistic density estimation to sample points near the surface and transition progressively from volumetric to surface representation. Our solution permits a proper and fast initialization of the signed distance field without relying on any geometric prior on the scene, compared to concurrent methods. By conducting extensive experiments on four outdoor driving datasets, we show that SCILLA can learn an accurate and detailed 3D surface scene representation in various urban scenarios while being two times faster to train compared to previous state-of-the-art solutions.</li>
</ul>

<h3>Title: Denoising Task Difficulty-based Curriculum for Training Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jin-Young Kim, Hyojun Go, Soonwoo Kwon, Hyun-Gyoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10348">https://arxiv.org/abs/2403.10348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10348">https://arxiv.org/pdf/2403.10348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10348]] Denoising Task Difficulty-based Curriculum for Training Diffusion Models(https://arxiv.org/abs/2403.10348)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based generative models have emerged as powerful tools in the realm of generative modeling. Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks. While various studies argue that lower timesteps present more challenging tasks, others contend that higher timesteps are more difficult. To address this conflict, our study undertakes a comprehensive examination of task difficulties, focusing on convergence behavior and changes in relative entropy between consecutive probability distributions across timesteps. Our observational study reveals that denoising at earlier timesteps poses challenges characterized by slower convergence and higher relative entropy, indicating increased task difficulty at these lower timesteps. Building on these observations, we introduce an easy-to-hard learning scheme, drawing from curriculum learning, to enhance the training process of diffusion models. By organizing timesteps or noise levels into clusters and training models with descending orders of difficulty, we facilitate an order-aware training regime, progressing from easier to harder denoising tasks, thereby deviating from the conventional approach of training diffusion models simultaneously across all timesteps. Our approach leads to improved performance and faster convergence by leveraging the benefits of curriculum learning, while maintaining orthogonality with existing improvements in diffusion training techniques. We validate these advantages through comprehensive experiments in image generation tasks, including unconditional, class-conditional, and text-to-image generation.</li>
</ul>

<h3>Title: Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding</h3>
<ul>
<li><strong>Authors: </strong>Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang Xue, Xinzhou Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10395">https://arxiv.org/abs/2403.10395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10395">https://arxiv.org/pdf/2403.10395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10395]] Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding(https://arxiv.org/abs/2403.10395)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Encouraged by the growing availability of pre-trained 2D diffusion models, image-to-3D generation by leveraging Score Distillation Sampling (SDS) is making remarkable progress. Most existing methods combine novel-view lifting from 2D diffusion models which usually take the reference image as a condition while applying hard L2 image supervision at the reference view. Yet heavily adhering to the image is prone to corrupting the inductive knowledge of the 2D diffusion model leading to flat or distorted 3D generation frequently. In this work, we reexamine image-to-3D in a novel perspective and present Isotropic3D, an image-to-3D generation pipeline that takes only an image CLIP embedding as input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth angle by solely resting on the SDS loss. The core of our framework lies in a two-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D diffusion model by substituting its text encoder with an image encoder, by which the model preliminarily acquires image-to-image capabilities. Secondly, we perform fine-tuning using our Explicit Multi-view Attention (EMA) which combines noisy multi-view images with the noise-free reference image as an explicit condition. CLIP embedding is sent to the diffusion model throughout the whole process while reference images are discarded once after fine-tuning. As a result, with a single image CLIP embedding, Isotropic3D is capable of generating multi-view mutually consistent images and also a 3D model with more symmetrical and neat content, well-proportioned geometry, rich colored texture, and less distortion compared with existing image-to-3D methods while still preserving the similarity to the reference image to a large extent. The project page is available at https://isotropic3d.github.io/. The code and models are available at https://github.com/pkunliu/Isotropic3D.</li>
</ul>

<h3>Title: SocialGenPod: Privacy-Friendly Generative AI Social Web Applications  with Decentralised Personal Data Stores</h3>
<ul>
<li><strong>Authors: </strong>Vidminas Vizgirda (1), Rui Zhao (2), Naman Goel (2) ((1) University of Edinburgh, (2) University of Oxford)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.IR, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10408">https://arxiv.org/abs/2403.10408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10408">https://arxiv.org/pdf/2403.10408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10408]] SocialGenPod: Privacy-Friendly Generative AI Social Web Applications  with Decentralised Personal Data Stores(https://arxiv.org/abs/2403.10408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid -- a decentralised Web specification -- to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model or application providers. Besides better privacy controls, this approach also enables portability across different services and applications. Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address. Our prototype is open-source and available at: https://github.com/Vidminas/socialgenpod/.</li>
</ul>

<h3>Title: Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A  Pilot Study</h3>
<ul>
<li><strong>Authors: </strong>Chenguang Wang, Ruoxi Jia, Xin Liu, Dawn Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10499">https://arxiv.org/abs/2403.10499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10499">https://arxiv.org/pdf/2403.10499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10499]] Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A  Pilot Study(https://arxiv.org/abs/2403.10499)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift. Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a pilot study. We show that CLIP leads to a significant robustness drop compared to supervised ImageNet models on our benchmark, especially under synthetic distribution shift and adversarial attacks. Furthermore, data overlap analysis suggests that the observed robustness under natural distribution shifts could be attributed, at least in part, to data overlap. In summary, our evaluation shows a comprehensive evaluation of robustness is necessary; and there is a significant need to improve the robustness of zero-shot multimodal models.</li>
</ul>

<h3>Title: VideoAgent: Long-form Video Understanding with Large Language Model as  Agent</h3>
<ul>
<li><strong>Authors: </strong>Xiaohan Wang, Yuhui Zhang, Orr Zohar, Serena Yeung-Levy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10517">https://arxiv.org/abs/2403.10517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10517">https://arxiv.org/pdf/2403.10517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10517]] VideoAgent: Long-form Video Understanding with Large Language Model as  Agent(https://arxiv.org/abs/2403.10517)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.</li>
</ul>

<h3>Title: Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation  Guided by the Characteristic Dance Primitives</h3>
<ul>
<li><strong>Authors: </strong>Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.10518">https://arxiv.org/abs/2403.10518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.10518">https://arxiv.org/pdf/2403.10518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.10518]] Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation  Guided by the Characteristic Dance Primitives(https://arxiv.org/abs/2403.10518)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose Lodge, a network capable of generating extremely long dance sequences conditioned on given music. We design Lodge as a two-stage coarse to fine diffusion architecture, and propose the characteristic dance primitives that possess significant expressiveness as intermediate representations between two diffusion models. The first stage is global diffusion, which focuses on comprehending the coarse-level music-dance correlation and production characteristic dance primitives. In contrast, the second-stage is the local diffusion, which parallelly generates detailed motion sequences under the guidance of the dance primitives and choreographic rules. In addition, we propose a Foot Refine Block to optimize the contact between the feet and the ground, enhancing the physical realism of the motion. Our approach can parallelly generate dance sequences of extremely long length, striking a balance between global choreographic patterns and local motion quality and expressiveness. Extensive experiments validate the efficacy of our method.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
