<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-13</h1>
<h3>Title: AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zihao Tang, Zheqi Lv, Shengyu Zhang, Yifan Zhou, Xinyu Duan, Fei Wu, Kun Kuang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07030">https://arxiv.org/abs/2403.07030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07030">https://arxiv.org/pdf/2403.07030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07030]] AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge  Distillation(https://arxiv.org/abs/2403.07030)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However, simply adopting models derived from DFKD for real-world applications suffers significant performance degradation, due to the discrepancy between teachers' training data and real-world scenarios (student domain). The degradation stems from the portions of teachers' knowledge that are not applicable to the student domain. They are specific to the teacher domain and would undermine students' performance. Hence, selectively transferring teachers' appropriate knowledge becomes the primary challenge in DFKD. In this work, we propose a simple but effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific anchor to align student-domain data with the teacher domain and leverages a generative method to progressively trade off the learning process between OOD knowledge distillation and domain-specific information learning via mixup learning. Extensive experiments in 3 datasets and 8 settings demonstrate the stability and superiority of our approach. Code available at https://github.com/IshiKura-a/AuG-KD .</li>
</ul>

<h3>Title: Ant Colony Sampling with GFlowNets for Combinatorial Optimization</h3>
<ul>
<li><strong>Authors: </strong>Minsu Kim, Sanghyeok Choi, Jiwoo Son, Hyeonah Kim, Jinkyoo Park, Yoshua Bengio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07041">https://arxiv.org/abs/2403.07041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07041">https://arxiv.org/pdf/2403.07041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07041]] Ant Colony Sampling with GFlowNets for Combinatorial Optimization(https://arxiv.org/abs/2403.07041)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}.</li>
</ul>

<h3>Title: LISO: Lidar-only Self-Supervised 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Stefan Baur, Frank Moosmann, Andreas Geiger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07071">https://arxiv.org/abs/2403.07071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07071">https://arxiv.org/pdf/2403.07071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07071]] LISO: Lidar-only Self-Supervised 3D Object Detection(https://arxiv.org/abs/2403.07071)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>3D object detection is one of the most important components in any Self-Driving stack, but current state-of-the-art (SOTA) lidar object detectors require costly & slow manual annotation of 3D bounding boxes to perform well. Recently, several methods emerged to generate pseudo ground truth without human supervision, however, all of these methods have various drawbacks: Some methods require sensor rigs with full camera coverage and accurate calibration, partly supplemented by an auxiliary optical flow engine. Others require expensive high-precision localization to find objects that disappeared over multiple drives. We introduce a novel self-supervised method to train SOTA lidar object detection networks which works on unlabeled sequences of lidar point clouds only, which we call trajectory-regularized self-training. It utilizes a SOTA self-supervised lidar scene flow network under the hood to generate, track, and iteratively refine pseudo ground truth. We demonstrate the effectiveness of our approach for multiple SOTA object detection networks across multiple real-world datasets. Code will be released.</li>
</ul>

<h3>Title: Narrating Causal Graphs with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Atharva Phatak, Vijay K. Mago, Ameeta Agrawal, Aravind Inbasekaran, Philippe J. Giabbanelli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07118">https://arxiv.org/abs/2403.07118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07118">https://arxiv.org/pdf/2403.07118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07118]] Narrating Causal Graphs with Large Language Models(https://arxiv.org/abs/2403.07118)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The use of generative AI to create text descriptions from graphs has mostly focused on knowledge graphs, which connect concepts using facts. In this work we explore the capability of large pretrained language models to generate text from causal graphs, where salient concepts are represented as nodes and causality is represented via directed, typed edges. The causal reasoning encoded in these graphs can support applications as diverse as healthcare or marketing. Using two publicly available causal graph datasets, we empirically investigate the performance of four GPT-3 models under various settings. Our results indicate that while causal text descriptions improve with training data, compared to fact-based graphs, they are harder to generate under zero-shot settings. Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset.</li>
</ul>

<h3>Title: One Category One Prompt: Dataset Distillation using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ali Abbasi, Ashkan Shahbazi, Hamed Pirsiavash, Soheil Kolouri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07142">https://arxiv.org/abs/2403.07142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07142">https://arxiv.org/pdf/2403.07142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07142]] One Category One Prompt: Dataset Distillation using Diffusion Models(https://arxiv.org/abs/2403.07142)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>The extensive amounts of data required for training deep neural networks pose significant challenges on storage and transmission fronts. Dataset distillation has emerged as a promising technique to condense the information of massive datasets into a much smaller yet representative set of synthetic samples. However, traditional dataset distillation approaches often struggle to scale effectively with high-resolution images and more complex architectures due to the limitations in bi-level optimization. Recently, several works have proposed exploiting knowledge distillation with decoupled optimization schemes to scale up dataset distillation. Although these methods effectively address the scalability issue, they rely on extensive image augmentations requiring the storage of soft labels for augmented images. In this paper, we introduce Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for dataset distillation, leveraging recent advancements in generative text-to-image foundation models. Our approach utilizes textual inversion, a technique for fine-tuning text-to-image generative models, to create concise and informative representations for large datasets. By employing these learned text prompts, we can efficiently store and infer new samples for introducing data variability within a fixed memory budget. We show the effectiveness of our method through extensive experiments across various computer vision benchmark datasets with different memory budgets.</li>
</ul>

<h3>Title: 3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of  Molecular Graphs</h3>
<ul>
<li><strong>Authors: </strong>Huaisheng Zhu, Teng Xiao, Vasant G Honavar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07179">https://arxiv.org/abs/2403.07179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07179">https://arxiv.org/pdf/2403.07179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07179]] 3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of  Molecular Graphs(https://arxiv.org/abs/2403.07179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the latent molecular graph space using a diffusion model. The results of our extensive experiments on several datasets demonstrate that 3M-Diffusion can generate high-quality, novel and diverse molecular graphs that semantically match the textual description provided.</li>
</ul>

<h3>Title: UPS: Towards Foundation Models for PDE Solving via Cross-Modal  Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Junhong Shen, Tanya Marwah, Ameet Talwalkar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07187">https://arxiv.org/abs/2403.07187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07187">https://arxiv.org/pdf/2403.07187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07187]] UPS: Towards Foundation Models for PDE Solving via Cross-Modal  Adaptation(https://arxiv.org/abs/2403.07187)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE families, coefficients, and resolutions.</li>
</ul>

<h3>Title: $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking  Reinforcement Learning Algorithms in Generative Language Model</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Zhang, Liyu Chen, Boyi Liu, Yingxiang Yang, Qiwen Cui, Yunzhe Tao, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07191">https://arxiv.org/abs/2403.07191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07191">https://arxiv.org/pdf/2403.07191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07191]] $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking  Reinforcement Learning Algorithms in Generative Language Model(https://arxiv.org/abs/2403.07191)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in reinforcement learning (RL) algorithms aim to enhance the performance of language models at scale. Yet, there is a noticeable absence of a cost-effective and standardized testbed tailored to evaluating and comparing these algorithms. To bridge this gap, we present a generalized version of the 24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a target value $K$ with $N$ integers. We evaluate the effectiveness of established RL algorithms such as Proximal Policy Optimization (PPO), alongside novel approaches like Identity Policy Optimization (IPO) and Direct Policy Optimization (DPO).</li>
</ul>

<h3>Title: Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers</h3>
<ul>
<li><strong>Authors: </strong>Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07214">https://arxiv.org/abs/2403.07214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07214">https://arxiv.org/pdf/2403.07214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07214]] Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers(https://arxiv.org/abs/2403.07214)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the model's feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements.</li>
</ul>

<h3>Title: SoK: Can Trajectory Generation Combine Privacy and Utility?</h3>
<ul>
<li><strong>Authors: </strong>Erik Buchholz, Alsharif Abuadbba, Shuo Wang, Surya Nepal, Salil S. Kanhere</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07218">https://arxiv.org/abs/2403.07218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07218">https://arxiv.org/pdf/2403.07218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07218]] SoK: Can Trajectory Generation Combine Privacy and Utility?(https://arxiv.org/abs/2403.07218)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While location trajectories represent a valuable data source for analyses and location-based services, they can reveal sensitive information, such as political and religious preferences. Differentially private publication mechanisms have been proposed to allow for analyses under rigorous privacy guarantees. However, the traditional protection schemes suffer from a limiting privacy-utility trade-off and are vulnerable to correlation and reconstruction attacks. Synthetic trajectory data generation and release represent a promising alternative to protection algorithms. While initial proposals achieve remarkable utility, they fail to provide rigorous privacy guarantees. This paper proposes a framework for designing a privacy-preserving trajectory publication approach by defining five design goals, particularly stressing the importance of choosing an appropriate Unit of Privacy. Based on this framework, we briefly discuss the existing trajectory protection approaches, emphasising their shortcomings. This work focuses on the systematisation of the state-of-the-art generative models for trajectories in the context of the proposed framework. We find that no existing solution satisfies all requirements. Thus, we perform an experimental study evaluating the applicability of six sequential generative models to the trajectory domain. Finally, we conclude that a generative trajectory model providing semantic guarantees remains an open research question and propose concrete next steps for future research.</li>
</ul>

<h3>Title: It's All About Your Sketch: Democratising Sketch Control in Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07234">https://arxiv.org/abs/2403.07234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07234">https://arxiv.org/pdf/2403.07234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07234]] It's All About Your Sketch: Democratising Sketch Control in Diffusion  Models(https://arxiv.org/abs/2403.07234)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper unravels the potential of sketches for diffusion models, addressing the deceptive promise of direct sketch control in generative AI. We importantly democratise the process, enabling amateur sketches to generate precise images, living up to the commitment of "what you sketch is what you get". A pilot study underscores the necessity, revealing that deformities in existing models stem from spatial-conditioning. To rectify this, we propose an abstraction-aware framework, utilising a sketch adapter, adaptive time-step sampling, and discriminative guidance from a pre-trained fine-grained sketch-based image retrieval model, working synergistically to reinforce fine-grained sketch-photo association. Our approach operates seamlessly during inference without the need for textual prompts; a simple, rough sketch akin to what you and I can create suffices! We welcome everyone to examine results presented in the paper and its supplementary. Contributions include democratising sketch control, introducing an abstraction-aware framework, and leveraging discriminative guidance, validated through extensive experiments.</li>
</ul>

<h3>Title: A Bayesian Approach to OOD Robustness in Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Prakhar Kaushik, Adam Kortylewski, Alan Yuille</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07277">https://arxiv.org/abs/2403.07277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07277">https://arxiv.org/pdf/2403.07277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07277]] A Bayesian Approach to OOD Robustness in Image Classification(https://arxiv.org/abs/2403.07277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transitional dictionary of vMF kernels that are intermediate between the source and target domains and train the generative model on this dictionary using the annotations on the source domain, followed by iterative refinement. This approach, termed Unsupervised Generative Transition (UGT), performs very well in OOD scenarios even when occlusion is present. UGT is evaluated on different OOD benchmarks including the OOD-CV dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image corruptions (including adding occluders), and synthetic-to-real domain transfer, and does well in all scenarios outperforming SOTA alternatives (e.g. up to 10% top-1 accuracy on Occluded OOD-CV dataset).</li>
</ul>

<h3>Title: Verification-Aided Learning of Neural Network Barrier Functions with  Termination Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Shaoru Chen, Lekan Molu, Mahyar Fazlyab</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07308">https://arxiv.org/abs/2403.07308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07308">https://arxiv.org/pdf/2403.07308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07308]] Verification-Aided Learning of Neural Network Barrier Functions with  Termination Guarantees(https://arxiv.org/abs/2403.07308)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Barrier functions are a general framework for establishing a safety guarantee for a system. However, there is no general method for finding these functions. To address this shortcoming, recent approaches use self-supervised learning techniques to learn these functions using training data that are periodically generated by a verification procedure, leading to a verification-aided learning framework. Despite its immense potential in automating barrier function synthesis, the verification-aided learning framework does not have termination guarantees and may suffer from a low success rate of finding a valid barrier function in practice. In this paper, we propose a holistic approach to address these drawbacks. With a convex formulation of the barrier function synthesis, we propose to first learn an empirically well-behaved NN basis function and then apply a fine-tuning algorithm that exploits the convexity and counterexamples from the verification failure to find a valid barrier function with finite-step termination guarantees: if there exist valid barrier functions, the fine-tuning algorithm is guaranteed to find one in a finite number of iterations. We demonstrate that our fine-tuning method can significantly boost the performance of the verification-aided learning framework on examples of different scales and using various neural network verifiers.</li>
</ul>

<h3>Title: Knowledge Graph Large Language Model (KG-LLM) for Link Prediction</h3>
<ul>
<li><strong>Authors: </strong>Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07311">https://arxiv.org/abs/2403.07311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07311">https://arxiv.org/pdf/2403.07311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07311]] Knowledge Graph Large Language Model (KG-LLM) for Link Prediction(https://arxiv.org/abs/2403.07311)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities for handling previously unseen prompts. Our experimental findings discover that integrating ICL and CoT not only augments the performance of our approach but also significantly boosts the models' generalization capacity, thereby ensuring more precise predictions in unfamiliar scenarios.</li>
</ul>

<h3>Title: Efficient Diffusion Model for Image Restoration by Residual Shifting</h3>
<ul>
<li><strong>Authors: </strong>Zongsheng Yue, Jianyi Wang, Chen Change Loy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07319">https://arxiv.org/abs/2403.07319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07319">https://arxiv.org/pdf/2403.07319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07319]] Efficient Diffusion Model for Image Restoration by Residual Shifting(https://arxiv.org/abs/2403.07319)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{https://github.com/zsyOAOA/ResShift}.</li>
</ul>

<h3>Title: GPT-generated Text Detection: Benchmark Dataset and Tensor-based  Detection Method</h3>
<ul>
<li><strong>Authors: </strong>Zubair Qazi, William Shiao, Evangelos E. Papalexakis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07321">https://arxiv.org/abs/2403.07321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07321">https://arxiv.org/pdf/2403.07321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07321]] GPT-generated Text Detection: Benchmark Dataset and Tensor-based  Detection Method(https://arxiv.org/abs/2403.07321)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As natural language models like ChatGPT become increasingly prevalent in applications and services, the need for robust and accurate methods to detect their output is of paramount importance. In this paper, we present GPT Reddit Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text detection dataset designed to assess the performance of detection models in identifying generated responses from ChatGPT. The dataset consists of a diverse collection of context-prompt pairs based on Reddit, with human-generated and ChatGPT-generated responses. We provide an analysis of the dataset's characteristics, including linguistic diversity, context complexity, and response quality. To showcase the dataset's utility, we benchmark several detection methods on it, demonstrating their efficacy in distinguishing between human and ChatGPT-generated responses. This dataset serves as a resource for evaluating and advancing detection techniques in the context of ChatGPT and contributes to the ongoing efforts to ensure responsible and trustworthy AI-driven communication on the internet. Finally, we propose GpTen, a novel tensor-based GPT text detection method that is semi-supervised in nature since it only has access to human-generated text and performs on par with fully-supervised baselines.</li>
</ul>

<h3>Title: Premonition: Using Generative Models to Preempt Future Data Changes in  Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Mark D. McDonnell, Dong Gong, Ehsan Abbasnejad, Anton van den Hengel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07356">https://arxiv.org/abs/2403.07356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07356">https://arxiv.org/pdf/2403.07356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07356]] Premonition: Using Generative Models to Preempt Future Data Changes in  Continual Learning(https://arxiv.org/abs/2403.07356)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Continual learning requires a model to adapt to ongoing changes in the data distribution, and often to the set of tasks to be performed. It is rare, however, that the data and task changes are completely unpredictable. Given a description of an overarching goal or data theme, which we call a realm, humans can often guess what concepts are associated with it. We show here that the combination of a large language model and an image generation model can similarly provide useful premonitions as to how a continual learning challenge might develop over time. We use the large language model to generate text descriptions of semantically related classes that might potentially appear in the data stream in future. These descriptions are then rendered using Stable Diffusion to generate new labelled image samples. The resulting synthetic dataset is employed for supervised pre-training, but is discarded prior to commencing continual learning, along with the pre-training classification head. We find that the backbone of our pre-trained networks can learn representations useful for the downstream continual learning problem, thus becoming a valuable input to any existing continual learning method. Although there are complexities arising from the domain gap between real and synthetic images, we show that pre-training models in this manner improves multiple Class Incremenal Learning (CIL) methods on fine-grained image classification benchmarks. Supporting code can be found at https://github.com/cl-premonition/premonition.</li>
</ul>

<h3>Title: Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine  Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Chongyu Fan, Jiancheng Liu, Alfred Hero, Sijia Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07362">https://arxiv.org/abs/2403.07362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07362">https://arxiv.org/pdf/2403.07362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07362]] Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine  Unlearning(https://arxiv.org/abs/2403.07362)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization level to emulate worst-case scenarios, while simultaneously engaging in standard training and unlearning at the lower level, achieving a balance between data influence erasure and model utility. Our proposal offers a worst-case evaluation of MU's resilience and effectiveness. Through extensive experiments across different datasets (including CIFAR-10, 100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image classifiers and generative models), we expose critical pros and cons in existing (approximate) unlearning strategies. Our results illuminate the complex challenges of MU in practice, guiding the future development of more accurate and robust unlearning algorithms. The code is available at https://github.com/OPTML-Group/Unlearn-WorstCase.</li>
</ul>

<h3>Title: Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of  Altered Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Phuong Dam, Jihoon Jeong, Anh Tran, Daeyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07371">https://arxiv.org/abs/2403.07371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07371">https://arxiv.org/pdf/2403.07371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07371]] Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of  Altered Diffusion Models(https://arxiv.org/abs/2403.07371)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This study discusses the critical issues of Virtual Try-On in contemporary e-commerce and the prospective metaverse, emphasizing the challenges of preserving intricate texture details and distinctive features of the target person and the clothes in various scenarios, such as clothing texture and identity characteristics like tattoos or accessories. In addition to the fidelity of the synthesized images, the efficiency of the synthesis process presents a significant hurdle. Various existing approaches are explored, highlighting the limitations and unresolved aspects, e.g., identity information omission, uncontrollable artifacts, and low synthesis speed. It then proposes a novel diffusion-based solution that addresses garment texture preservation and user identity retention during virtual try-on. The proposed network comprises two primary modules - a warping module aligning clothing with individual features and a try-on module refining the attire and generating missing parts integrated with a mask-aware post-processing technique ensuring the integrity of the individual's identity. It demonstrates impressive results, surpassing the state-of-the-art in speed by nearly 20 times during inference, with superior fidelity in qualitative assessments. Quantitative evaluations confirm comparable performance with the recent SOTA method on the VITON-HD and Dresscode datasets.</li>
</ul>

<h3>Title: Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from  Duplex to Monoplex IHC Images</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Brieu, Nicolas Triltsch, Philipp Wortmann, Dominik Winter, Shashank Saran, Marlon Rebelatto, Günter Schmidt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07389">https://arxiv.org/abs/2403.07389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07389">https://arxiv.org/pdf/2403.07389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07389]] Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from  Duplex to Monoplex IHC Images(https://arxiv.org/abs/2403.07389)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches.</li>
</ul>

<h3>Title: Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07398">https://arxiv.org/abs/2403.07398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07398">https://arxiv.org/pdf/2403.07398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07398]] Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs(https://arxiv.org/abs/2403.07398)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations.</li>
</ul>

<h3>Title: In-context learning enables multimodal large language models to classify  cancer pathology images</h3>
<ul>
<li><strong>Authors: </strong>Dyke Ferber, Georg Wölflein, Isabella C. Wiest, Marta Ligero, Srividhya Sainath, Narmin Ghaffari Laleh, Omar S.M. El Nahhas, Gustav Müller-Franzes, Dirk Jäger, Daniel Truhn, Jakob Nikolas Kather</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07407">https://arxiv.org/abs/2403.07407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07407">https://arxiv.org/pdf/2403.07407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07407]] In-context learning enables multimodal large language models to classify  cancer pathology images(https://arxiv.org/abs/2403.07407)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative, in-context</a></li>
<li><strong>Abstract: </strong>Medical image classification requires labeled, task-specific datasets which are used to train deep learning networks de novo, or to fine-tune foundation models. However, this process is computationally and technically demanding. In language processing, in-context learning provides an alternative, where models learn from within prompts, bypassing the need for parameter updates. Yet, in-context learning remains underexplored in medical image analysis. Here, we systematically evaluate the model Generative Pretrained Transformer 4 with Vision capabilities (GPT-4V) on cancer image processing with in-context learning on three cancer histopathology tasks of high importance: Classification of tissue subtypes in colorectal cancer, colon polyp subtyping and breast tumor detection in lymph node sections. Our results show that in-context learning is sufficient to match or even outperform specialized neural networks trained for particular tasks, while only requiring a minimal number of samples. In summary, this study demonstrates that large vision language models trained on non-domain specific data can be applied out-of-the box to solve medical image-processing tasks in histopathology. This democratizes access of generalist AI models to medical experts without technical background especially for areas where annotated data is scarce.</li>
</ul>

<h3>Title: Motion Mamba: Efficient and Long Sequence Motion Generation with  Hierarchical and Bidirectional Selective SSM</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, Hao Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07487">https://arxiv.org/abs/2403.07487</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07487">https://arxiv.org/pdf/2403.07487</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07487]] Motion Mamba: Efficient and Long Sequence Motion Generation with  Hierarchical and Bidirectional Selective SSM(https://arxiv.org/abs/2403.07487)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Human motion generation stands as a significant pursuit in generative computer vision, while achieving long-sequence and efficient motion generation remains challenging. Recent advancements in state space models (SSMs), notably Mamba, have showcased considerable promise in long sequence modeling with an efficient hardware-aware design, which appears to be a promising direction to build motion generation model upon it. Nevertheless, adapting SSMs to motion generation faces hurdles since the lack of a specialized design architecture to model motion sequence. To address these challenges, we propose Motion Mamba, a simple and efficient approach that presents the pioneering motion generation model utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba (HTM) block to process temporal data by ensemble varying numbers of isolated SSM modules across a symmetric U-Net architecture aimed at preserving motion consistency between frames. We also design a Bidirectional Spatial Mamba (BSM) block to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to 50% FID improvement and up to 4 times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation. See project website https://steve-zeyu-zhang.github.io/MotionMamba/</li>
</ul>

<h3>Title: Block-wise LoRA: Revisiting Fine-grained LoRA for Effective  Personalization and Stylization in Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Likun Li, Haoqi Zeng, Changpeng Yang, Haozhe Jia, Di Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07500">https://arxiv.org/abs/2403.07500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07500">https://arxiv.org/pdf/2403.07500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07500]] Block-wise LoRA: Revisiting Fine-grained LoRA for Effective  Personalization and Stylization in Text-to-Image Generation(https://arxiv.org/abs/2403.07500)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The objective of personalization and stylization in text-to-image is to instruct a pre-trained diffusion model to analyze new concepts introduced by users and incorporate them into expected styles. Recently, parameter-efficient fine-tuning (PEFT) approaches have been widely adopted to address this task and have greatly propelled the development of this field. Despite their popularity, existing efficient fine-tuning methods still struggle to achieve effective personalization and stylization in T2I generation. To address this issue, we propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained fine-tuning for different blocks of SD, which can generate images faithful to input prompts and target identity and also with desired style. Extensive experiments demonstrate the effectiveness of the proposed method.</li>
</ul>

<h3>Title: D4D: An RGBD diffusion model to boost monocular depth estimation</h3>
<ul>
<li><strong>Authors: </strong>L. Papa, P. Russo, I. Amerini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07516">https://arxiv.org/abs/2403.07516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07516">https://arxiv.org/pdf/2403.07516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07516]] D4D: An RGBD diffusion model to boost monocular depth estimation(https://arxiv.org/abs/2403.07516)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce. A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step. In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion model able to generate realistic RGBD samples. We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements. Our supervised training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset.</li>
</ul>

<h3>Title: Open-World Semantic Segmentation Including Class Similarity</h3>
<ul>
<li><strong>Authors: </strong>Matteo Sodano, Federico Magistri, Lucas Nunes, Jens Behley, Cyrill Stachniss</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07532">https://arxiv.org/abs/2403.07532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07532">https://arxiv.org/pdf/2403.07532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07532]] Open-World Semantic Segmentation Including Class Similarity(https://arxiv.org/abs/2403.07532)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Interpreting camera data is key for autonomously acting systems, such as autonomous vehicles. Vision systems that operate in real-world environments must be able to understand their surroundings and need the ability to deal with novel situations. This paper tackles open-world semantic segmentation, i.e., the variant of interpreting image data in which objects occur that have not been seen during training. We propose a novel approach that performs accurate closed-world semantic segmentation and, at the same time, can identify new categories without requiring any additional training data. Our approach additionally provides a similarity measure for every newly discovered class in an image to a known category, which can be useful information in downstream tasks such as planning or mapping. Through extensive experiments, we show that our model achieves state-of-the-art results on classes known from training data as well as for anomaly segmentation and can distinguish between different unknown classes.</li>
</ul>

<h3>Title: RSBuilding: Towards General Remote Sensing Image Building Extraction and  Change Detection with Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Mingze Wang, Keyan Chen, Lili Su, Cilin Yan, Sheng Xu, Haotian Zhang, Pengcheng Yuan, Xiaolong Jiang, Baochang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07564">https://arxiv.org/abs/2403.07564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07564">https://arxiv.org/pdf/2403.07564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07564]] RSBuilding: Towards General Remote Sensing Image Building Extraction and  Change Detection with Foundation Model(https://arxiv.org/abs/2403.07564)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The intelligent interpretation of buildings plays a significant role in urban planning and management, macroeconomic analysis, population dynamics, etc. Remote sensing image building interpretation primarily encompasses building extraction and change detection. However, current methodologies often treat these two tasks as separate entities, thereby failing to leverage shared knowledge. Moreover, the complexity and diversity of remote sensing image scenes pose additional challenges, as most algorithms are designed to model individual small datasets, thus lacking cross-scene generalization. In this paper, we propose a comprehensive remote sensing image building understanding model, termed RSBuilding, developed from the perspective of the foundation model. RSBuilding is designed to enhance cross-scene generalization and task universality. Specifically, we extract image features based on the prior knowledge of the foundation model and devise a multi-level feature sampler to augment scale information. To unify task representation and integrate image spatiotemporal clues, we introduce a cross-attention decoder with task prompts. Addressing the current shortage of datasets that incorporate annotations for both tasks, we have developed a federated training strategy to facilitate smooth model convergence even when supervision for some tasks is missing, thereby bolstering the complementarity of different tasks. Our model was trained on a dataset comprising up to 245,000 images and validated on multiple building extraction and change detection datasets. The experimental results substantiate that RSBuilding can concurrently handle two structurally distinct tasks and exhibits robust zero-shot generalization capabilities.</li>
</ul>

<h3>Title: AACP: Aesthetics assessment of children's paintings based on  self-supervised learning</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Jiang, Ning Li, Chen Shi, Liping Guo, Changbo Wang, Chenhui Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07578">https://arxiv.org/abs/2403.07578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07578">https://arxiv.org/pdf/2403.07578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07578]] AACP: Aesthetics assessment of children's paintings based on  self-supervised learning(https://arxiv.org/abs/2403.07578)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The Aesthetics Assessment of Children's Paintings (AACP) is an important branch of the image aesthetics assessment (IAA), playing a significant role in children's education. This task presents unique challenges, such as limited available data and the requirement for evaluation metrics from multiple perspectives. However, previous approaches have relied on training large datasets and subsequently providing an aesthetics score to the image, which is not applicable to AACP. To solve this problem, we construct an aesthetics assessment dataset of children's paintings and a model based on self-supervised learning. 1) We build a novel dataset composed of two parts: the first part contains more than 20k unlabeled images of children's paintings; the second part contains 1.2k images of children's paintings, and each image contains eight attributes labeled by multiple design experts. 2) We design a pipeline that includes a feature extraction module, perception modules and a disentangled evaluation module. 3) We conduct both qualitative and quantitative experiments to compare our model's performance with five other methods using the AACP dataset. Our experiments reveal that our method can accurately capture aesthetic features and achieve state-of-the-art performance.</li>
</ul>

<h3>Title: Visual Privacy Auditing with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kristian Schwethelm, Johannes Kaiser, Moritz Knolle, Daniel Rueckert, Georgios Kaissis, Alexander Ziller</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07588">https://arxiv.org/abs/2403.07588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07588">https://arxiv.org/pdf/2403.07588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07588]] Visual Privacy Auditing with Diffusion Models(https://arxiv.org/abs/2403.07588)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image reconstruction attacks on machine learning models pose a significant risk to privacy by potentially leaking sensitive information. Although defending against such attacks using differential privacy (DP) has proven effective, determining appropriate DP parameters remains challenging. Current formal guarantees on data reconstruction success suffer from overly theoretical assumptions regarding adversary knowledge about the target data, particularly in the image domain. In this work, we empirically investigate this discrepancy and find that the practicality of these assumptions strongly depends on the domain shift between the data prior and the reconstruction target. We propose a reconstruction attack based on diffusion models (DMs) that assumes adversary access to real-world image priors and assess its implications on privacy leakage under DP-SGD. We show that (1) real-world data priors significantly influence reconstruction success, (2) current reconstruction bounds do not model the risk posed by data priors well, and (3) DMs can serve as effective auditing tools for visualizing privacy leakage.</li>
</ul>

<h3>Title: Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for  Video Adverse Weather Removal</h3>
<ul>
<li><strong>Authors: </strong>Yijun Yang, Hongtao Wu, Angelica I. Aviles-Rivero, Yulun Zhang, Jing Qin, Lei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07684">https://arxiv.org/abs/2403.07684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07684">https://arxiv.org/pdf/2403.07684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07684]] Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for  Video Adverse Weather Removal(https://arxiv.org/abs/2403.07684)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Real-world vision tasks frequently suffer from the appearance of unexpected adverse weather conditions, including rain, haze, snow, and raindrops. In the last decade, convolutional neural networks and vision transformers have yielded outstanding results in single-weather video removal. However, due to the absence of appropriate adaptation, most of them fail to generalize to other weather conditions. Although ViWS-Net is proposed to remove adverse weather conditions in videos with a single set of pre-trained weights, it is seriously blinded by seen weather at train-time and degenerates when coming to unseen weather during test-time. In this work, we introduce test-time adaptation into adverse weather removal in videos, and propose the first framework that integrates test-time adaptation into the iterative diffusion reverse process. Specifically, we devise a diffusion-based network with a novel temporal noise model to efficiently explore frame-correlated information in degraded video clips at training stage. During inference stage, we introduce a proxy task named Diffusion Tubelet Self-Calibration to learn the primer distribution of test video stream and optimize the model by approximating the temporal noise model for online adaptation. Experimental results, on benchmark datasets, demonstrate that our Test-Time Adaptation method with Diffusion-based network(Diff-TTA) outperforms state-of-the-art methods in terms of restoring videos degraded by seen weather conditions. Its generalizable capability is also validated with unseen weather conditions in both synthesized and real-world videos.</li>
</ul>

<h3>Title: Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model  Performance and Annotation Cost</h3>
<ul>
<li><strong>Authors: </strong>Oana Ignat, Longju Bai, Joan Nwatu, Rada Mihalcea</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07687">https://arxiv.org/abs/2403.07687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07687">https://arxiv.org/pdf/2403.07687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07687]] Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model  Performance and Annotation Cost(https://arxiv.org/abs/2403.07687)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Current foundation models have shown impressive performance across various tasks. However, several studies have revealed that these models are not effective for everyone due to the imbalanced geographical and economic representation of the data used in the training process. Most of this data comes from Western countries, leading to poor results for underrepresented countries. To address this issue, more data needs to be collected from these countries, but the cost of annotation can be a significant bottleneck. In this paper, we propose methods to identify the data to be annotated to balance model performance and annotation costs. Our approach first involves finding the countries with images of topics (objects and actions) most visually distinct from those already in the training datasets used by current large vision-language foundation models. Next, we identify countries with higher visual similarity for these topics and show that using data from these countries to supplement the training data improves model performance and reduces annotation costs. The resulting lists of countries and corresponding topics are made available at https://github.com/MichiganNLP/visual_diversity_budget.</li>
</ul>

<h3>Title: CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive  Self-Supervised Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shahaf Arica, Or Rubin, Sapir Gershov, Shlomi Laufer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07700">https://arxiv.org/abs/2403.07700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07700">https://arxiv.org/pdf/2403.07700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07700]] CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive  Self-Supervised Transformers(https://arxiv.org/abs/2403.07700)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce VoteCut, an innovative method for unsupervised object discovery that leverages feature representations from multiple self-supervised models. VoteCut employs normalized-cut based graph partitioning, clustering and a pixel voting approach. Additionally, We present CuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels, generated by VoteCut, and a novel soft target loss to refine segmentation accuracy. Through rigorous evaluations across multiple datasets and several unsupervised setups, our methods demonstrate significant improvements in comparison to previous state-of-the-art models. Our ablation studies further highlight the contributions of each component, revealing the robustness and efficacy of our approach. Collectively, VoteCut and CuVLER pave the way for future advancements in image segmentation.</li>
</ul>

<h3>Title: SSM Meets Video Diffusion Models: Efficient Video Generation with  Structured State Spaces</h3>
<ul>
<li><strong>Authors: </strong>Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07711">https://arxiv.org/abs/2403.07711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07711">https://arxiv.org/pdf/2403.07711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07711]] SSM Meets Video Diffusion Models: Efficient Video Generation with  Structured State Spaces(https://arxiv.org/abs/2403.07711)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.</li>
</ul>

<h3>Title: Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, Haibo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07764">https://arxiv.org/abs/2403.07764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07764">https://arxiv.org/pdf/2403.07764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07764]] Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model(https://arxiv.org/abs/2403.07764)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields.</li>
</ul>

<h3>Title: SemCity: Semantic Scene Generation with Triplane Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, Sung-Eui Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07773">https://arxiv.org/abs/2403.07773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07773">https://arxiv.org/pdf/2403.07773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07773]] SemCity: Semantic Scene Generation with Triplane Diffusion(https://arxiv.org/abs/2403.07773)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present "SemCity," a 3D diffusion model for semantic scene generation in real-world outdoor environments. Most 3D diffusion models focus on generating a single object, synthetic indoor scenes, or synthetic outdoor scenes, while the generation of real-world outdoor scenes is rarely addressed. In this paper, we concentrate on generating a real-outdoor scene through learning a diffusion model on a real-world outdoor dataset. In contrast to synthetic data, real-outdoor datasets often contain more empty spaces due to sensor limitations, causing challenges in learning real-outdoor distributions. To address this issue, we exploit a triplane representation as a proxy form of scene distributions to be learned by our diffusion model. Furthermore, we propose a triplane manipulation that integrates seamlessly with our triplane diffusion model. The manipulation improves our diffusion model's applicability in a variety of downstream tasks related to outdoor scene generation such as scene inpainting, scene outpainting, and semantic scene completion refinements. In experimental results, we demonstrate that our triplane diffusion model shows meaningful generation results compared with existing work in a real-outdoor dataset, SemanticKITTI. We also show our triplane manipulation facilitates seamlessly adding, removing, or modifying objects within a scene. Further, it also enables the expansion of scenes toward a city-level scale. Finally, we evaluate our method on semantic scene completion refinements where our diffusion model enhances predictions of semantic scene completion networks by learning scene distribution. Our code is available at https://github.com/zoomin-lee/SemCity.</li>
</ul>

<h3>Title: Beyond Memorization: The Challenge of Random Memory Access in Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07805">https://arxiv.org/abs/2403.07805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07805">https://arxiv.org/pdf/2403.07805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07805]] Beyond Memorization: The Challenge of Random Memory Access in Language  Models(https://arxiv.org/abs/2403.07805)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github. com/sail-sg/lm-random-memory-access.</li>
</ul>

<h3>Title: Quantifying and Mitigating Privacy Risks for Tabular Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. Pérez, Marten van Dijk, Lydia Y. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07842">https://arxiv.org/abs/2403.07842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07842">https://arxiv.org/pdf/2403.07842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07842]] Quantifying and Mitigating Privacy Risks for Tabular Generative Models(https://arxiv.org/abs/2403.07842)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk.</li>
</ul>

<h3>Title: Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias</h3>
<ul>
<li><strong>Authors: </strong>Sierra Wyllie, Ilia Shumailov, Nicolas Papernot</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07857">https://arxiv.org/abs/2403.07857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07857">https://arxiv.org/pdf/2403.07857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07857]] Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias(https://arxiv.org/abs/2403.07857)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective.</li>
</ul>

<h3>Title: Bridging Different Language Models and Generative Vision Models for  Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07860">https://arxiv.org/abs/2403.07860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07860">https://arxiv.org/pdf/2403.07860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07860]] Bridging Different Language Models and Generative Vision Models for  Text-to-Image Generation(https://arxiv.org/abs/2403.07860)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at https://github.com/ShihaoZhaoZSH/LaVi-Bridge.</li>
</ul>

<h3>Title: Rethinking Generative Large Language Model Evaluation for Semantic  Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Fangyun Wei, Xi Chen, Lin Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.07872">https://arxiv.org/abs/2403.07872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.07872">https://arxiv.org/pdf/2403.07872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.07872]] Rethinking Generative Large Language Model Evaluation for Semantic  Comprehension(https://arxiv.org/abs/2403.07872)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite their sophisticated capabilities, large language models (LLMs) encounter a major hurdle in effective assessment. This paper first revisits the prevalent evaluation method-multiple choice question answering (MCQA), which allows for straightforward accuracy measurement. Through a comprehensive evaluation of 24 models across 11 benchmarks, we highlight several potential drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation and the generation of open-ended responses in practical scenarios. In response, we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5, Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This system is designed to mirror real-world usage, and for this purpose, we have compiled a new benchmark called ``Real-world questions'' (RWQ), comprising 20,772 authentic user inquiries. Additionally, we thoroughly analyze the characteristics of our system and compare it with prior leaderboards like AlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo system, the feasibility of registering new models, and its potential to reshape LLM leaderboards.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
