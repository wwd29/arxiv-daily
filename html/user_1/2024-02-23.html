<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-23</h1>
<h3>Title: Wisdom of Committee: Distilling from Foundation Model to  SpecializedApplication Model</h3>
<ul>
<li><strong>Authors: </strong>Zichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14035">https://arxiv.org/abs/2402.14035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14035">https://arxiv.org/pdf/2402.14035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14035]] Wisdom of Committee: Distilling from Foundation Model to  SpecializedApplication Model(https://arxiv.org/abs/2402.14035)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foundation model teachers and complementary teachers. Complementary teachers possess model characteristics akin to the student's, aiming to bridge the gap between the foundation model and specialized application models for a smoother knowledge transfer. Further, to accommodate the dissimilarity among the teachers in the committee, we introduce DiverseDistill, which allows the student to understand the expertise of each teacher and extract task knowledge. Our evaluations demonstrate that adding complementary teachers enhances student performance. Finally, DiverseDistill consistently outperforms baseline distillation methods, regardless of the teacher choices, resulting in significantly improved student performance.</li>
</ul>

<h3>Title: Protect and Extend -- Using GANs for Synthetic Data Generation of  Time-Series Medical Records</h3>
<ul>
<li><strong>Authors: </strong>Navid Ashrafi, Vera Schmitt, Robert P. Spang, Sebastian Möller, Jan-Niklas Voigt-Antons</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14042">https://arxiv.org/abs/2402.14042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14042">https://arxiv.org/pdf/2402.14042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14042]] Protect and Extend -- Using GANs for Synthetic Data Generation of  Time-Series Medical Records(https://arxiv.org/abs/2402.14042)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. The privacy preservation of the respective models is assessed by applying membership inference attacks to determine potential data leakage risks. Our experiments indicate the superiority of the privacy-preserving GAN (PPGAN) model over other models regarding privacy preservation while maintaining an acceptable level of QoG. The presented results can support better data protection for medical use cases in the future.</li>
</ul>

<h3>Title: Generative Adversarial Models for Extreme Downscaling of Climate  Datasets</h3>
<ul>
<li><strong>Authors: </strong>Guiye Li, Guofeng Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14049">https://arxiv.org/abs/2402.14049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14049">https://arxiv.org/pdf/2402.14049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14049]] Generative Adversarial Models for Extreme Downscaling of Climate  Datasets(https://arxiv.org/abs/2402.14049)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables. However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand. Deep-learning-based methods, particularly generative adversarial networks (GANs) and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets. In this paper, we describe a conditional GAN-based geospatial downscaling method for extreme downscaling of gridded climate datasets. Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs. More importantly, the method explicitly considers the uncertainty inherent to the downscaling process that tends to be ignored in existing methods. Given an input, the method can produce a multitude of plausible high-resolution samples instead of one single deterministic result. These samples allow for an empirical exploration and inferences of model uncertainty and robustness. With a case study of gridded climate datasets (wind velocity and solar irradiance), we demonstrate the performances of the framework in downscaling tasks with very high scaling factors (up to $64\times$) and highlight the advantages of the framework with a comprehensive comparison with commonly used downscaling methods, including area-to-point (ATP) kriging, deep image prior (DIP), enhanced deep super-resolution network (EDSR), enhanced super-resolution generative adversarial networks (ESRGAN), and physics-informed resolution-enhancing GAN (PhIRE GAN).</li>
</ul>

<h3>Title: Multi-organ Self-supervised Contrastive Learning for Breast Lesion  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hugo Figueiras, Helena Aidos, Nuno Cruz Garcia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14114">https://arxiv.org/abs/2402.14114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14114">https://arxiv.org/pdf/2402.14114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14114]] Multi-organ Self-supervised Contrastive Learning for Breast Lesion  Segmentation(https://arxiv.org/abs/2402.14114)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has proven to be an effective way to learn representations in domains where annotated labels are scarce, such as medical imaging. A widely adopted framework for this purpose is contrastive learning and it has been applied to different scenarios. This paper seeks to advance our understanding of the contrastive learning framework by exploring a novel perspective: employing multi-organ datasets for pre-training models tailored to specific organ-related target tasks. More specifically, our target task is breast tumour segmentation in ultrasound images. The pre-training datasets include ultrasound images from other organs, such as the lungs and heart, and large datasets of natural images. Our results show that conventional contrastive learning pre-training improves performance compared to supervised baseline approaches. Furthermore, our pre-trained models achieve comparable performance when fine-tuned with only half of the available labelled data. Our findings also show the advantages of pre-training on diverse organ data for improving performance in the downstream task.</li>
</ul>

<h3>Title: Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for  Intent Recognition?</h3>
<ul>
<li><strong>Authors: </strong>Amogh Mannekote, Xiaoyi Tian, Kristy Elizabeth Boyer, Bonnie J. Dorr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14155">https://arxiv.org/abs/2402.14155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14155">https://arxiv.org/pdf/2402.14155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14155]] Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for  Intent Recognition?(https://arxiv.org/abs/2402.14155)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Task-oriented dialogue systems are expected to handle a constantly expanding set of intents and domains even after they have been deployed to support more and more functionalities. To live up to this expectation, it becomes critical to mitigate the catastrophic forgetting problem (CF) that occurs in continual learning (CL) settings for a task such as intent recognition. While existing dialogue systems research has explored replay-based and regularization-based methods to this end, the effect of domain ordering on the CL performance of intent recognition models remains unexplored. If understood well, domain ordering has the potential to be an orthogonal technique that can be leveraged alongside existing techniques such as experience replay. Our work fills this gap by comparing the impact of three domain-ordering strategies (min-sum path, max-sum path, random) on the CL performance of a generative intent recognition model. Our findings reveal that the min-sum path strategy outperforms the others in reducing catastrophic forgetting when training on the 220M T5-Base model. However, this advantage diminishes with the larger 770M T5-Large model. These results underscores the potential of domain ordering as a complementary strategy for mitigating catastrophic forgetting in continually learning intent recognition models, particularly in resource-constrained scenarios.</li>
</ul>

<h3>Title: T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with  Trajectory Stitching</h3>
<ul>
<li><strong>Authors: </strong>Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding Yu, Chaowei Xiao, Jianfei Cai, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14167">https://arxiv.org/abs/2402.14167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14167">https://arxiv.org/pdf/2402.14167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14167]] T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with  Trajectory Stitching(https://arxiv.org/abs/2402.14167)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Sampling from diffusion probabilistic models (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a simple yet efficient technique to improve the sampling efficiency with little or no generation degradation. Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage. Our key insight is that different diffusion models learn similar encodings under the same training data distribution and smaller models are capable of generating good global structures in the early steps. Extensive experiments demonstrate that T-Stitch is training-free, generally applicable for different architectures, and complements most existing fast sampling techniques with flexible speed and quality trade-offs. On DiT-XL, for example, 40% of the early timesteps can be safely replaced with a 10x faster DiT-S without performance drop on class-conditional ImageNet generation. We further show that our method can also be used as a drop-in technique to not only accelerate the popular pretrained stable diffusion (SD) models but also improve the prompt alignment of stylized SD models from the public model zoo. Code is released at https://github.com/NVlabs/T-Stitch</li>
</ul>

<h3>Title: Linear Transformers are Versatile In-Context Learners</h3>
<ul>
<li><strong>Authors: </strong>Max Vladymyrov, Johannes von Oswald, Mark Sandler, Rong Ge</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14180">https://arxiv.org/abs/2402.14180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14180">https://arxiv.org/pdf/2402.14180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14180]] Linear Transformers are Versatile In-Context Learners(https://arxiv.org/abs/2402.14180)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.</li>
</ul>

<h3>Title: A Self-supervised Pressure Map human keypoint Detection Approch:  Optimizing Generalization and Computational Efficiency Across Datasets</h3>
<ul>
<li><strong>Authors: </strong>Chengzhang Yu, Xianjun Yang, Wenxia Bao, Shaonan Wang, Zhiming Yao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14241">https://arxiv.org/abs/2402.14241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14241">https://arxiv.org/pdf/2402.14241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14241]] A Self-supervised Pressure Map human keypoint Detection Approch:  Optimizing Generalization and Computational Efficiency Across Datasets(https://arxiv.org/abs/2402.14241)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In environments where RGB images are inadequate, pressure maps is a viable alternative, garnering scholarly attention. This study introduces a novel self-supervised pressure map keypoint detection (SPMKD) method, addressing the current gap in specialized designs for human keypoint extraction from pressure maps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model, which is a robust framework that integrates a lightweight encoder for precise human keypoint detection, a fuser for efficient gradient propagation, and a decoder that transforms human keypoints into reconstructed pressure maps. This structure is further enhanced by the Classification-to-Regression Weight Transfer (CRWT) method, which fine-tunes accuracy through initial classification task training. This innovation not only enhances human keypoint generalization without manual annotations but also showcases remarkable efficiency and generalization, evidenced by a reduction to only $5.96\%$ in FLOPs and $1.11\%$ in parameter count compared to the baseline methods.</li>
</ul>

<h3>Title: Reconstruction-Based Anomaly Localization via Knowledge-Informed  Self-Training</h3>
<ul>
<li><strong>Authors: </strong>Cheng Qian, Xiaoxian Lao, Chunguang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14246">https://arxiv.org/abs/2402.14246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14246">https://arxiv.org/pdf/2402.14246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14246]] Reconstruction-Based Anomaly Localization via Knowledge-Informed  Self-Training(https://arxiv.org/abs/2402.14246)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly localization, which involves localizing anomalous regions within images, is a significant industrial task. Reconstruction-based methods are widely adopted for anomaly localization because of their low complexity and high interpretability. Most existing reconstruction-based methods only use normal samples to construct model. If anomalous samples are appropriately utilized in the process of anomaly localization, the localization performance can be improved. However, usually only weakly labeled anomalous samples are available, which limits the improvement. In many cases, we can obtain some knowledge of anomalies summarized by domain experts. Taking advantage of such knowledge can help us better utilize the anomalous samples and thus further improve the localization performance. In this paper, we propose a novel reconstruction-based method named knowledge-informed self-training (KIST) which integrates knowledge into reconstruction model through self-training. Specifically, KIST utilizes weakly labeled anomalous samples in addition to the normal ones and exploits knowledge to yield pixel-level pseudo-labels of the anomalous samples. Based on the pseudo labels, a novel loss which promotes the reconstruction of normal pixels while suppressing the reconstruction of anomalous pixels is used. We conduct experiments on different datasets and demonstrate the advantages of KIST over the existing reconstruction-based methods.</li>
</ul>

<h3>Title: MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xin-Yang Zheng, Hao Pan, Yu-Xiao Guo, Xin Tong, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14253">https://arxiv.org/abs/2402.14253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14253">https://arxiv.org/pdf/2402.14253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14253]] MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion(https://arxiv.org/abs/2402.14253)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As a promising 3D generation technique, multiview diffusion (MVD) has received a lot of attention due to its advantages in terms of generalizability, quality, and efficiency. By finetuning pretrained large image diffusion models with 3D data, the MVD methods first generate multiple views of a 3D object based on an image or text prompt and then reconstruct 3D shapes with multiview 3D reconstruction. However, the sparse views and inconsistent details in the generated images make 3D reconstruction challenging. We present MVD$^2$, an efficient 3D reconstruction method for multiview diffusion (MVD) images. MVD$^2$ aggregates image features into a 3D feature volume by projection and convolution and then decodes volumetric features into a 3D mesh. We train MVD$^2$ with 3D shape collections and MVD images prompted by rendered views of 3D shapes. To address the discrepancy between the generated multiview images and ground-truth views of the 3D shapes, we design a simple-yet-efficient view-dependent training scheme. MVD$^2$ improves the 3D generation quality of MVD and is fast and robust to various MVD methods. After training, it can efficiently decode 3D meshes from multiview images within one second. We train MVD$^2$ with Zero-123++ and ObjectVerse-LVIS 3D dataset and demonstrate its superior performance in generating 3D models from multiview images generated by different MVD methods, using both synthetic and real images as prompts.</li>
</ul>

<h3>Title: Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form  Medical Question Answering Applications and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen, Huaxiu Yao, Yue Zhang, Ren Wang, Kaidi Xu, Xiaoshuang Shi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14259">https://arxiv.org/abs/2402.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14259">https://arxiv.org/pdf/2402.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14259]] Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form  Medical Question Answering Applications and Beyond(https://arxiv.org/abs/2402.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on accurate uncertainty measurement under two standard criteria for correctness evaluation (e.g., WSE outperforms existing state-of-the-art method by 3.23% AUROC on the MedQA dataset). Additionally, in terms of the potential for real-world medical QA applications, we achieve a significant enhancement in the performance of LLMs when employing sequences with lower uncertainty, identified by WSE, as final answers (e.g., +6.36% accuracy improvement on the COVID-QA dataset), without requiring any additional task-specific fine-tuning or architectural modifications.</li>
</ul>

<h3>Title: Qsnail: A Questionnaire Dataset for Sequential Question Generation</h3>
<ul>
<li><strong>Authors: </strong>Yan Lei, Liang Pang, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14272">https://arxiv.org/abs/2402.14272</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14272">https://arxiv.org/pdf/2402.14272</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14272]] Qsnail: A Questionnaire Dataset for Sequential Question Generation(https://arxiv.org/abs/2402.14272)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The questionnaire is a professional research methodology used for both qualitative and quantitative analysis of human opinions, preferences, attitudes, and behaviors. However, designing and evaluating questionnaires demands significant effort due to their intricate and complex structure. Questionnaires entail a series of questions that must conform to intricate constraints involving the questions, options, and overall structure. Specifically, the questions should be relevant and specific to the given research topic and intent. The options should be tailored to the questions, ensuring they are mutually exclusive, completed, and ordered sensibly. Moreover, the sequence of questions should follow a logical order, grouping similar topics together. As a result, automatically generating questionnaires presents a significant challenge and this area has received limited attention primarily due to the scarcity of high-quality datasets. To address these issues, we present Qsnail, the first dataset specifically constructed for the questionnaire generation task, which comprises 13,168 human-written questionnaires gathered from online platforms. We further conduct experiments on Qsnail, and the results reveal that retrieval models and traditional generative models do not fully align with the given research topic and intents. Large language models, while more closely related to the research topic and intents, exhibit significant limitations in terms of diversity and specificity. Despite enhancements through the chain-of-thought prompt and finetuning, questionnaires generated by language models still fall short of human-written questionnaires. Therefore, questionnaire generation is challenging and needs to be further explored. The dataset is available at: https://github.com/LeiyanGithub/qsnail.</li>
</ul>

<h3>Title: A Simple Framework Uniting Visual In-context Learning with Masked Image  Modeling to Improve Ultrasound Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuyue Zhou, Banafshe Felfeliyan, Shrimanti Ghosh, Jessica Knight, Fatima Alves-Pereira, Christopher Keen, Jessica Küpper, Abhilash Rakkunedeth Hareendranathan, Jacob L. Jaremko</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14300">https://arxiv.org/abs/2402.14300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14300">https://arxiv.org/pdf/2402.14300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14300]] A Simple Framework Uniting Visual In-context Learning with Masked Image  Modeling to Improve Ultrasound Segmentation(https://arxiv.org/abs/2402.14300)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, in-context</a></li>
<li><strong>Abstract: </strong>Conventional deep learning models deal with images one-by-one, requiring costly and time-consuming expert labeling in the field of medical imaging, and domain-specific restriction limits model generalizability. Visual in-context learning (ICL) is a new and exciting area of research in computer vision. Unlike conventional deep learning, ICL emphasizes the model's ability to adapt to new tasks based on given examples quickly. Inspired by MAE-VQGAN, we proposed a new simple visual ICL method called SimICL, combining visual ICL pairing images with masked image modeling (MIM) designed for self-supervised learning. We validated our method on bony structures segmentation in a wrist ultrasound (US) dataset with limited annotations, where the clinical objective was to segment bony structures to help with further fracture detection. We used a test set containing 3822 images from 18 patients for bony region segmentation. SimICL achieved an remarkably high Dice coeffient (DC) of 0.96 and Jaccard Index (IoU) of 0.92, surpassing state-of-the-art segmentation and visual ICL models (a maximum DC 0.86 and IoU 0.76), with SimICL DC and IoU increasing up to 0.10 and 0.16. This remarkably high agreement with limited manual annotations indicates SimICL could be used for training AI models even on small US datasets. This could dramatically decrease the human expert time required for image labeling compared to conventional approaches, and enhance the real-world use of AI assistance in US image analysis.</li>
</ul>

<h3>Title: Font Style Interpolation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tetta Kondo, Shumpei Takezaki, Daichi Haraguchi, Seiichi Uchida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14311">https://arxiv.org/abs/2402.14311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14311">https://arxiv.org/pdf/2402.14311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14311]] Font Style Interpolation with Diffusion Models(https://arxiv.org/abs/2402.14311)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Fonts have huge variations in their styles and give readers different impressions. Therefore, generating new fonts is worthy of giving new impressions to readers. In this paper, we employ diffusion models to generate new font styles by interpolating a pair of reference fonts with different styles. More specifically, we propose three different interpolation approaches, image-blending, condition-blending, and noise-blending, with the diffusion models. We perform qualitative and quantitative experimental analyses to understand the style generation ability of the three approaches. According to experimental results, three proposed approaches can generate not only expected font styles but also somewhat serendipitous font styles. We also compare the approaches with a state-of-the-art style-conditional Latin-font generative network model to confirm the validity of using the diffusion models for the style interpolation task.</li>
</ul>

<h3>Title: Typographic Text Generation with Off-the-Shelf Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>KhayTze Peong, Seiichi Uchida, Daichi Haraguchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14314">https://arxiv.org/abs/2402.14314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14314">https://arxiv.org/pdf/2402.14314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14314]] Typographic Text Generation with Off-the-Shelf Diffusion Model(https://arxiv.org/abs/2402.14314)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent diffusion-based generative models show promise in their ability to generate text images, but limitations in specifying the styles of the generated texts render them insufficient in the realm of typographic design. This paper proposes a typographic text generation system to add and modify text on typographic designs while specifying font styles, colors, and text effects. The proposed system is a novel combination of two off-the-shelf methods for diffusion models, ControlNet and Blended Latent Diffusion. The former functions to generate text images under the guidance of edge conditions specifying stroke contours. The latter blends latent noise in Latent Diffusion Models (LDM) to add typographic text naturally onto an existing background. We first show that given appropriate text edges, ControlNet can generate texts in specified fonts while incorporating effects described by prompts. We further introduce text edge manipulation as an intuitive and customizable way to produce texts with complex effects such as ``shadows'' and ``reflections''. Finally, with the proposed system, we successfully add and modify texts on a predefined background while preserving its overall coherence.</li>
</ul>

<h3>Title: GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a  Gradient-Aware Mask and Semantic Constraints</h3>
<ul>
<li><strong>Authors: </strong>Anqi Cheng, Zhiyuan Yang, Haiyue Zhu, Kezhi Mao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14354">https://arxiv.org/abs/2402.14354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14354">https://arxiv.org/pdf/2402.14354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14354]] GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a  Gradient-Aware Mask and Semantic Constraints(https://arxiv.org/abs/2402.14354)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and robust supervision for both key areas and textureless regions by allocating weights based on gradient magnitudes.The incorporation of semantic constraints for indoor self-supervised depth estimation improves depth discrepancies at object boundaries, leveraging a co-optimization network and proxy semantic labels derived from a pretrained segmentation model. Experimental studies on three indoor datasets, including NYUv2, ScanNet, and InteriorNet, show that GAM-Depth outperforms existing methods and achieves state-of-the-art performance, signifying a meaningful step forward in indoor depth estimation. Our code will be available at https://github.com/AnqiCheng1234/GAM-Depth.</li>
</ul>

<h3>Title: Rule or Story, Which is a Better Commonsense Expression for Talking with  Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Ning Bian, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, Le Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14355">https://arxiv.org/abs/2402.14355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14355">https://arxiv.org/pdf/2402.14355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14355]] Rule or Story, Which is a Better Commonsense Expression for Talking with  Large Language Models?(https://arxiv.org/abs/2402.14355)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.</li>
</ul>

<h3>Title: Rethinking Scientific Summarization Evaluation: Grounding Explainable  Metrics on Facet-aware Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Xiuying Chen, Tairan Wang, Qingqing Zhu, Taicheng Guo, Shen Gao, Zhiyong Lu, Xin Gao, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14359">https://arxiv.org/abs/2402.14359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14359">https://arxiv.org/pdf/2402.14359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14359]] Rethinking Scientific Summarization Evaluation: Grounding Explainable  Metrics on Facet-aware Benchmark(https://arxiv.org/abs/2402.14359)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains. This suggests an area for future enhancement of LLMs.</li>
</ul>

<h3>Title: Generative Adversarial Network with Soft-Dynamic Time Warping and  Parallel Reconstruction for Energy Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Hardik Prabhu, Jayaraman Valadi, Pandarasamy Arjunan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14384">https://arxiv.org/abs/2402.14384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14384">https://arxiv.org/pdf/2402.14384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14384]] Generative Adversarial Network with Soft-Dynamic Time Warping and  Parallel Reconstruction for Energy Time Series Anomaly Detection(https://arxiv.org/abs/2402.14384)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>In this paper, we employ a 1D deep convolutional generative adversarial network (DCGAN) for sequential anomaly detection in energy time series data. Anomaly detection involves gradient descent to reconstruct energy sub-sequences, identifying the noise vector that closely generates them through the generator network. Soft-DTW is used as a differentiable alternative for the reconstruction loss and is found to be superior to Euclidean distance. Combining reconstruction loss and the latent space's prior probability distribution serves as the anomaly score. Our novel method accelerates detection by parallel computation of reconstruction of multiple points and shows promise in identifying anomalous energy consumption in buildings, as evidenced by performing experiments on hourly energy time series from 15 buildings.</li>
</ul>

<h3>Title: Diffusion Model Based Visual Compensation Guidance and Visual Difference  Analysis for No-Reference Image Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Wang, Bo Hu, Mingyang Zhang, Jie Li, Leida Li, Maoguo Gong, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14401">https://arxiv.org/abs/2402.14401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14401">https://arxiv.org/pdf/2402.14401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14401]] Diffusion Model Based Visual Compensation Guidance and Visual Difference  Analysis for No-Reference Image Quality Assessment(https://arxiv.org/abs/2402.14401)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA. Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual information. Secondly, two visual evaluation branches are designed to comprehensively analyze the obtained high-level feature information. These include the visual compensation guidance branch, grounded in the transformer architecture and noise embedding strategy, and the visual difference analysis branch, built on the ResNet architecture and the residual transposed attention block. Extensive experiments are conducted on seven public NR-IQA datasets, and the results demonstrate that the proposed model outperforms SOTA methods for NR-IQA.</li>
</ul>

<h3>Title: On the Tip of the Tongue: Analyzing Conceptual Representation in Large  Language Models with Reverse-Dictionary Probe</h3>
<ul>
<li><strong>Authors: </strong>Ningyu Xu, Qi Zhang, Menghan Zhang, Peng Qian, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14404">https://arxiv.org/abs/2402.14404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14404">https://arxiv.org/pdf/2402.14404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14404]] On the Tip of the Tongue: Analyzing Conceptual Representation in Large  Language Models with Reverse-Dictionary Probe(https://arxiv.org/abs/2402.14404)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commonsense reasoning problems.</li>
</ul>

<h3>Title: Large-Scale Actionless Video Pre-Training via Discrete Diffusion for  Efficient Policy Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14407">https://arxiv.org/abs/2402.14407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14407">https://arxiv.org/pdf/2402.14407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14407]] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for  Efficient Policy Learning(https://arxiv.org/abs/2402.14407)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we harness the imagined future videos to guide low-level action learning trained on a limited set of robot data. Experiments demonstrate that our method generates high-fidelity future videos for planning and enhances the fine-tuned policies compared to previous state-of-the-art approaches with superior generalization ability. Our project website is available at https://video-diff.github.io/.</li>
</ul>

<h3>Title: Reimagining Anomalies: What If Anomalies Were Normal?</h3>
<ul>
<li><strong>Authors: </strong>Philipp Liznerski, Saurabh Varshneya, Ece Calikus, Sophie Fellenz, Marius Kloft</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14469">https://arxiv.org/abs/2402.14469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14469">https://arxiv.org/pdf/2402.14469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14469]] Reimagining Anomalies: What If Anomalies Were Normal?(https://arxiv.org/abs/2402.14469)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Deep learning-based methods have achieved a breakthrough in image anomaly detection, but their complexity introduces a considerable challenge to understanding why an instance is predicted to be anomalous. We introduce a novel explanation method that generates multiple counterfactual examples for each anomaly, capturing diverse concepts of anomalousness. A counterfactual example is a modification of the anomaly that is perceived as normal by the anomaly detector. The method provides a high-level semantic explanation of the mechanism that triggered the anomaly detector, allowing users to explore "what-if scenarios." Qualitative and quantitative analyses across various image datasets show that the method applied to state-of-the-art anomaly detectors can achieve high-quality semantic explanations of detectors.</li>
</ul>

<h3>Title: DynGMA: a robust approach for learning stochastic differential equations  from data</h3>
<ul>
<li><strong>Authors: </strong>Aiqing Zhu, Qianxiao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14475">https://arxiv.org/abs/2402.14475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14475">https://arxiv.org/pdf/2402.14475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14475]] DynGMA: a robust approach for learning stochastic differential equations  from data(https://arxiv.org/abs/2402.14475)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Learning unknown stochastic differential equations (SDEs) from observed data is a significant and challenging task with applications in various fields. Current approaches often use neural networks to represent drift and diffusion functions, and construct likelihood-based loss by approximating the transition density to train these networks. However, these methods often rely on one-step stochastic numerical schemes, necessitating data with sufficiently high time resolution. In this paper, we introduce novel approximations to the transition density of the parameterized SDE: a Gaussian density approximation inspired by the random perturbation theory of dynamical systems, and its extension, the dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust density approximation, our method exhibits superior accuracy compared to baseline methods in learning the fully unknown drift and diffusion functions and computing the invariant distribution from trajectory data. And it is capable of handling trajectory data with low time resolution and variable, even uncontrollable, time step sizes, such as data generated from Gillespie's stochastic simulations. We then conduct several experiments across various scenarios to verify the advantages and robustness of the proposed method.</li>
</ul>

<h3>Title: Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation  and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Takehiro Takayanagi, Masahiro Suzuki, Ryotaro Kobayashi, Hiroki Sakaji, Kiyoshi Izumi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14484">https://arxiv.org/abs/2402.14484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14484">https://arxiv.org/pdf/2402.14484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14484]] Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation  and Analysis(https://arxiv.org/abs/2402.14484)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities within text data is crucial, and causal text mining plays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets. We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causal text mining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency to falsely recognize non-causal sequences as causal sequences. These issues become even more pronounced with advanced versions of the model, such as GPT-4. In addition, we highlight the constraints of ChatGPT in handling complex causality types, including both intra/inter-sentential and implicit causality. The model also faces challenges with effectively leveraging in-context learning and domain adaptation. Our code is available on \url{https://github.com/retarfi/gemcausal}</li>
</ul>

<h3>Title: Does the Generator Mind its Contexts? An Analysis of Generative Model  Faithfulness under Context Transfer</h3>
<ul>
<li><strong>Authors: </strong>Xinshuo Hu, Baotian Hu, Dongfang Li, Xiaoguang Li, Lifeng Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14488">https://arxiv.org/abs/2402.14488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14488">https://arxiv.org/pdf/2402.14488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14488]] Does the Generator Mind its Contexts? An Analysis of Generative Model  Faithfulness under Context Transfer(https://arxiv.org/abs/2402.14488)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The present study introduces the knowledge-augmented generator, which is specifically designed to produce information that remains grounded in contextual knowledge, regardless of alterations in the context. Previous research has predominantly focused on examining hallucinations stemming from static input, such as in the domains of summarization or machine translation. However, our investigation delves into the faithfulness of generative question answering in the presence of dynamic knowledge. Our objective is to explore the existence of hallucinations arising from parametric memory when contextual knowledge undergoes changes, while also analyzing the underlying causes for their occurrence. In order to efficiently address this issue, we propose a straightforward yet effective measure for detecting such hallucinations. Intriguingly, our investigation uncovers that all models exhibit a tendency to generate previous answers as hallucinations. To gain deeper insights into the underlying causes of this phenomenon, we conduct a series of experiments that verify the critical role played by context in hallucination, both during training and testing, from various perspectives.</li>
</ul>

<h3>Title: Towards Seamless Adaptation of Pre-trained Models for Visual Place  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14505">https://arxiv.org/abs/2402.14505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14505">https://arxiv.org/pdf/2402.14505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14505]] Towards Seamless Adaptation of Pre-trained Models for Visual Place  Recognition(https://arxiv.org/abs/2402.14505)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time, and uses about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the time of submission). The code is released at https://github.com/Lu-Feng/SelaVPR.</li>
</ul>

<h3>Title: Self-supervised Visualisation of Medical Image Datasets</h3>
<ul>
<li><strong>Authors: </strong>Ifeoma Veronica Nwabufo, Jan Niklas Böhm, Philipp Berens, Dmitry Kobak</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14566">https://arxiv.org/abs/2402.14566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14566">https://arxiv.org/pdf/2402.14566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14566]] Self-supervised Visualisation of Medical Image Datasets(https://arxiv.org/abs/2402.14566)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning methods based on data augmentations, such as SimCLR, BYOL, or DINO, allow obtaining semantically meaningful representations of image datasets and are widely used prior to supervised fine-tuning. A recent self-supervised learning method, $t$-SimCNE, uses contrastive learning to directly train a 2D representation suitable for visualisation. When applied to natural image datasets, $t$-SimCNE yields 2D visualisations with semantically meaningful clusters. In this work, we used $t$-SimCNE to visualise medical image datasets, including examples from dermatology, histology, and blood microscopy. We found that increasing the set of data augmentations to include arbitrary rotations improved the results in terms of class separability, compared to data augmentations used for natural images. Our 2D representations show medically relevant structures and can be used to aid data exploration and annotation, improving on common approaches for data visualisation.</li>
</ul>

<h3>Title: Debiasing Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ruifei He, Chuhui Xue, Haoru Tan, Wenqing Zhang, Yingchen Yu, Song Bai, Xiaojuan Qi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14577">https://arxiv.org/abs/2402.14577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14577">https://arxiv.org/pdf/2402.14577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14577]] Debiasing Text-to-Image Diffusion Models(https://arxiv.org/abs/2402.14577)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Learning-based Text-to-Image (TTI) models like Stable Diffusion have revolutionized the way visual content is generated in various domains. However, recent research has shown that nonnegligible social bias exists in current state-of-the-art TTI systems, which raises important concerns. In this work, we target resolving the social bias in TTI diffusion models. We begin by formalizing the problem setting and use the text descriptions of bias groups to establish an unsafe direction for guiding the diffusion process. Next, we simplify the problem into a weight optimization problem and attempt a Reinforcement solver, Policy Gradient, which shows sub-optimal performance with slow convergence. Further, to overcome limitations, we propose an iterative distribution alignment (IDA) method. Despite its simplicity, we show that IDA shows efficiency and fast convergence in resolving the social bias in TTI diffusion models. Our code will be released.</li>
</ul>

<h3>Title: Overcoming Dimensional Collapse in Self-supervised Contrastive Learning  for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jamshid Hassanpour, Vinkle Srivastav, Didier Mutter, Nicolas Padoy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14611">https://arxiv.org/abs/2402.14611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14611">https://arxiv.org/pdf/2402.14611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14611]] Overcoming Dimensional Collapse in Self-supervised Contrastive Learning  for Medical Image Segmentation(https://arxiv.org/abs/2402.14611)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) approaches have achieved great success when the amount of labeled data is limited. Within SSL, models learn robust feature representations by solving pretext tasks. One such pretext task is contrastive learning, which involves forming pairs of similar and dissimilar input samples, guiding the model to distinguish between them. In this work, we investigate the application of contrastive learning to the domain of medical image analysis. Our findings reveal that MoCo v2, a state-of-the-art contrastive learning method, encounters dimensional collapse when applied to medical images. This is attributed to the high degree of inter-image similarity shared between the medical images. To address this, we propose two key contributions: local feature learning and feature decorrelation. Local feature learning improves the ability of the model to focus on the local regions of the image, while feature decorrelation removes the linear dependence among the features. Our experimental findings demonstrate that our contributions significantly enhance the model's performance in the downstream task of medical segmentation, both in the linear evaluation and full fine-tuning settings. This work illustrates the importance of effectively adapting SSL techniques to the characteristics of medical imaging tasks.</li>
</ul>

<h3>Title: Rethinking Invariance Regularization in Adversarial Training to Improve  Robustness-Accuracy Trade-off</h3>
<ul>
<li><strong>Authors: </strong>Futa Waseda, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14648">https://arxiv.org/abs/2402.14648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14648">https://arxiv.org/pdf/2402.14648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14648]] Rethinking Invariance Regularization in Adversarial Training to Improve  Robustness-Accuracy Trade-off(https://arxiv.org/abs/2402.14648)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning approach, and a split-BatchNorm (BN) structure to resolve the mixture distribution problem. Our method significantly improves the robustness-accuracy trade-off by learning adversarially invariant representations without sacrificing discriminative power. Furthermore, we discuss the relevance of our findings to knowledge-distillation-based defense methods, contributing to a deeper understanding of their relative successes.</li>
</ul>

<h3>Title: ConceptMath: A Bilingual Concept-wise Benchmark for Measuring  Mathematical Reasoning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14660">https://arxiv.org/abs/2402.14660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14660">https://arxiv.org/pdf/2402.14660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14660]] ConceptMath: A Bilingual Concept-wise Benchmark for Measuring  Mathematical Reasoning of Large Language Models(https://arxiv.org/abs/2402.14660)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models.</li>
</ul>

<h3>Title: Visual Hallucinations of Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14683">https://arxiv.org/abs/2402.14683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14683">https://arxiv.org/pdf/2402.14683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14683]] Visual Hallucinations of Multi-modal Large Language Models(https://arxiv.org/abs/2402.14683)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.</li>
</ul>

<h3>Title: COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies  with Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Baihan Lin, Djallel Bouneffouf, Yulia Landa, Rachel Jespersen, Cheryl Corcoran, Guillermo Cecchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14701">https://arxiv.org/abs/2402.14701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14701">https://arxiv.org/pdf/2402.14701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14701]] COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies  with Language Modeling(https://arxiv.org/abs/2402.14701)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic modeling techniques in combination with generative language prompting, we analyze the topical characteristics of different psychiatric conditions and incorporate temporal modeling to capture the evolution of topics at a turn-level resolution. This combined framework enhances the understanding of therapeutic interactions, enabling timely feedback for therapists regarding conversation quality and providing interpretable insights to improve the effectiveness of psychotherapy.</li>
</ul>

<h3>Title: Two-stage Cytopathological Image Synthesis for Augmenting Cervical  Abnormality Screening</h3>
<ul>
<li><strong>Authors: </strong>Zhenrong Shen, Manman Fei, Xin Wang, Jiangdong Cai, Sheng Wang, Lichi Zhang, Qian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14707">https://arxiv.org/abs/2402.14707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14707">https://arxiv.org/pdf/2402.14707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14707]] Two-stage Cytopathological Image Synthesis for Augmenting Cervical  Abnormality Screening(https://arxiv.org/abs/2402.14707)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Automatic thin-prep cytologic test (TCT) screening can assist pathologists in finding cervical abnormality towards accurate and efficient cervical cancer diagnosis. Current automatic TCT screening systems mostly involve abnormal cervical cell detection, which generally requires large-scale and diverse training data with high-quality annotations to achieve promising performance. Pathological image synthesis is naturally raised to minimize the efforts in data collection and annotation. However, it is challenging to generate realistic large-size cytopathological images while simultaneously synthesizing visually plausible appearances for small-size abnormal cervical cells. In this paper, we propose a two-stage image synthesis framework to create synthetic data for augmenting cervical abnormality screening. In the first Global Image Generation stage, a Normal Image Generator is designed to generate cytopathological images full of normal cervical cells. In the second Local Cell Editing stage, normal cells are randomly selected from the generated images and then are converted to different types of abnormal cells using the proposed Abnormal Cell Synthesizer. Both Normal Image Generator and Abnormal Cell Synthesizer are built upon the pre-trained Stable Diffusion via parameter-efficient fine-tuning methods for customizing cytopathological image contents and extending spatial layout controllability, respectively. Our experiments demonstrate the synthetic image quality, diversity, and controllability of the proposed synthesis framework, and validate its data augmentation effectiveness in enhancing the performance of abnormal cervical cell detection.</li>
</ul>

<h3>Title: How Transformers Learn Causal Structure with Gradient Descent</h3>
<ul>
<li><strong>Authors: </strong>Eshaan Nichani, Alex Damian, Jason D. Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14735">https://arxiv.org/abs/2402.14735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14735">https://arxiv.org/pdf/2402.14735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14735]] How Transformers Learn Causal Structure with Gradient Descent(https://arxiv.org/abs/2402.14735)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.</li>
</ul>

<h3>Title: Customize-A-Video: One-Shot Motion Customization of Text-to-Video  Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14780">https://arxiv.org/abs/2402.14780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14780">https://arxiv.org/pdf/2402.14780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14780]] Customize-A-Video: One-Shot Motion Customization of Text-to-Video  Diffusion Models(https://arxiv.org/abs/2402.14780)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapting it to new subjects and scenes with both spatial and temporal varieties. It leverages low-rank adaptation (LoRA) on temporal attention layers to tailor the pre-trained T2V diffusion model for specific motion modeling from the reference videos. To disentangle the spatial and temporal information during the training pipeline, we introduce a novel concept of appearance absorbers that detach the original appearance from the single reference video prior to motion learning. Our proposed method can be easily extended to various downstream tasks, including custom video generation and editing, video appearance customization, and multiple motion combination, in a plug-and-play fashion. Our project page can be found at https://anonymous-314.github.io.</li>
</ul>

<h3>Title: Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised  Learning</h3>
<ul>
<li><strong>Authors: </strong>Johnathan Xie, Yoonho Lee, Annie S. Chen, Chelsea Finn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14789">https://arxiv.org/abs/2402.14789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14789">https://arxiv.org/pdf/2402.14789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14789]] Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised  Learning(https://arxiv.org/abs/2402.14789)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is promising as a domain-agnostic framework for self-supervised learning because it does not rely on input augmentations, its mask sampling procedure remains domain-specific. We present Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling method. SMA trains an attention based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. We evaluate SMA on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics. We find SMA is capable of learning representations without domain-specific knowledge and achieves state-of-the-art performance on these three benchmarks.</li>
</ul>

<h3>Title: Consolidating Attention Features for Multi-view Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14792">https://arxiv.org/abs/2402.14792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14792">https://arxiv.org/pdf/2402.14792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14792]] Consolidating Attention Features for Multi-view Image Editing(https://arxiv.org/abs/2402.14792)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the self-attention layers during generation, greatly improving multi-view consistency. We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps. We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene. These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target geometry.</li>
</ul>

<h3>Title: GeneOH Diffusion: Towards Generalizable Hand-Object Interaction  Denoising via Denoising Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xueyi Liu, Li Yi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14810">https://arxiv.org/abs/2402.14810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14810">https://arxiv.org/pdf/2402.14810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14810]] GeneOH Diffusion: Towards Generalizable Hand-Object Interaction  Denoising via Denoising Diffusion(https://arxiv.org/abs/2402.14810)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, GeneOH Diffusion, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a "denoising via diffusion" strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications. Project website: https://meowuu7.github.io/GeneOH-Diffusion/.</li>
</ul>

<h3>Title: WeakSAM: Segment Anything Meets Weakly-supervised Instance-level  Recognition</h3>
<ul>
<li><strong>Authors: </strong>Lianghui Zhu, Junwei Zhou, Yan Liu, Xin Hao, Wenyu Liu, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14812">https://arxiv.org/abs/2402.14812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14812">https://arxiv.org/pdf/2402.14812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14812]] WeakSAM: Segment Anything Meets Weakly-supervised Instance-level  Recognition(https://arxiv.org/abs/2402.14812)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%, respectively. The code is available at \url{https://github.com/hustvl/WeakSAM}.</li>
</ul>

<h3>Title: Cameras as Rays: Pose Estimation via Ray Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, Shubham Tulsiani</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14817">https://arxiv.org/abs/2402.14817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14817">https://arxiv.org/pdf/2402.14817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14817]] Cameras as Rays: Pose Estimation via Ray Diffusion(https://arxiv.org/abs/2402.14817)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparse views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while generalizing to unseen object categories and in-the-wild captures.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
