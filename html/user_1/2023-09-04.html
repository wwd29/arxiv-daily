<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models. (arXiv:2309.00158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00158">http://arxiv.org/abs/2309.00158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00158]] BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models(http://arxiv.org/abs/2309.00158)</code></li>
<li>Summary: <p>3D building generation with low data acquisition costs, such as single
image-to-3D, becomes increasingly important. However, most of the existing
single image-to-3D building creation works are restricted to those images with
specific viewing angles, hence they are difficult to scale to general-view
images that commonly appear in practical cases. To fill this gap, we propose a
novel 3D building shape generation method exploiting point cloud diffusion
models with image conditioning schemes, which demonstrates flexibility to the
input images. By cooperating two conditional diffusion models and introducing a
regularization strategy during denoising process, our method is able to
synthesize building roofs while maintaining the overall structures. We validate
our framework on two newly built datasets and extensive experiments show that
our method outperforms previous works in terms of building generation quality.
</p></li>
</ul>

<h3>Title: Diffusion Model with Clustering-based Conditioning for Food Image Generation. (arXiv:2309.00199v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00199">http://arxiv.org/abs/2309.00199</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00199]] Diffusion Model with Clustering-based Conditioning for Food Image Generation(http://arxiv.org/abs/2309.00199)</code></li>
<li>Summary: <p>Image-based dietary assessment serves as an efficient and accurate solution
for recording and analyzing nutrition intake using eating occasion images as
input. Deep learning-based techniques are commonly used to perform image
analysis such as food classification, segmentation, and portion size
estimation, which rely on large amounts of food images with annotations for
training. However, such data dependency poses significant barriers to
real-world applications, because acquiring a substantial, diverse, and balanced
set of food images can be challenging. One potential solution is to use
synthetic food images for data augmentation. Although existing work has
explored the use of generative adversarial networks (GAN) based structures for
generation, the quality of synthetic food images still remains subpar. In
addition, while diffusion-based generative models have shown promising results
for general image generation tasks, the generation of food images can be
challenging due to the substantial intra-class variance. In this paper, we
investigate the generation of synthetic food images based on the conditional
diffusion model and propose an effective clustering-based training framework,
named ClusDiff, for generating high-quality and representative food images. The
proposed method is evaluated on the Food-101 dataset and shows improved
performance when compared with existing image generation works. We also
demonstrate that the synthetic food images generated by ClusDiff can help
address the severe class imbalance issue in long-tailed food classification
using the VFN-LT dataset.
</p></li>
</ul>

<h3>Title: DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using Stable Diffusion Models. (arXiv:2309.00248v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00248">http://arxiv.org/abs/2309.00248</a></li>
<li>Code URL: https://github.com/mshenoda/diffugen</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00248]] DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using Stable Diffusion Models(http://arxiv.org/abs/2309.00248)</code></li>
<li>Summary: <p>Generating high-quality labeled image datasets is crucial for training
accurate and robust machine learning models in the field of computer vision.
However, the process of manually labeling real images is often time-consuming
and costly. To address these challenges associated with dataset generation, we
introduce "DiffuGen," a simple and adaptable approach that harnesses the power
of stable diffusion models to create labeled image datasets efficiently. By
leveraging stable diffusion models, our approach not only ensures the quality
of generated datasets but also provides a versatile solution for label
generation. In this paper, we present the methodology behind DiffuGen, which
combines the capabilities of diffusion models with two distinct labeling
techniques: unsupervised and supervised. Distinctively, DiffuGen employs prompt
templating for adaptable image generation and textual inversion to enhance
diffusion model capabilities.
</p></li>
</ul>

<h3>Title: Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution. (arXiv:2309.00287v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00287">http://arxiv.org/abs/2309.00287</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00287]] Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution(http://arxiv.org/abs/2309.00287)</code></li>
<li>Summary: <p>Using diffusion models to solve inverse problems is a growing field of
research. Current methods assume the degradation to be known and provide
impressive results in terms of restoration quality and diversity. In this work,
we leverage the efficiency of those models to jointly estimate the restored
image and unknown parameters of the degradation model. In particular, we
designed an algorithm based on the well-known Expectation-Minimization (EM)
estimation method and diffusion models. Our method alternates between
approximating the expected log-likelihood of the inverse problem using samples
drawn from a diffusion model and a maximization step to estimate unknown model
parameters. For the maximization step, we also introduce a novel blur kernel
regularization based on a Plug \&amp; Play denoiser. Diffusion models are long to
run, thus we provide a fast version of our algorithm. Extensive experiments on
blind image deblurring demonstrate the effectiveness of our method when
compared to other state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation. (arXiv:2309.00398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00398">http://arxiv.org/abs/2309.00398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00398]] VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation(http://arxiv.org/abs/2309.00398)</code></li>
<li>Summary: <p>In this paper, we present VideoGen, a text-to-video generation approach,
which can generate a high-definition video with high frame fidelity and strong
temporal consistency using reference-guided latent diffusion. We leverage an
off-the-shelf text-to-image generation model, e.g., Stable Diffusion, to
generate an image with high content quality from the text prompt, as a
reference image to guide video generation. Then, we introduce an efficient
cascaded latent diffusion module conditioned on both the reference image and
the text prompt, for generating latent video representations, followed by a
flow-based temporal upsampling step to improve the temporal resolution.
Finally, we map latent video representations into a high-definition video
through an enhanced video decoder. During training, we use the first frame of a
ground-truth video as the reference image for training the cascaded latent
diffusion module. The main characterises of our approach include: the reference
image generated by the text-to-image model improves the visual fidelity; using
it as the condition makes the diffusion model focus more on learning the video
dynamics; and the video decoder is trained over unlabeled video data, thus
benefiting from high-quality easily-available videos. VideoGen sets a new
state-of-the-art in text-to-video generation in terms of both qualitative and
quantitative evaluation.
</p></li>
</ul>

<h3>Title: Iterative Multi-granular Image Editing using Diffusion Models. (arXiv:2309.00613v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00613">http://arxiv.org/abs/2309.00613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00613]] Iterative Multi-granular Image Editing using Diffusion Models(http://arxiv.org/abs/2309.00613)</code></li>
<li>Summary: <p>Recent advances in text-guided image synthesis has dramatically changed how
creative professionals generate artistic and aesthetically pleasing visual
assets. To fully support such creative endeavors, the process should possess
the ability to: 1) iteratively edit the generations and 2) control the spatial
reach of desired changes (global, local or anything in between). We formalize
this pragmatic problem setting as Iterative Multi-granular Editing. While there
has been substantial progress with diffusion-based models for image synthesis
and editing, they are all one shot (i.e., no iterative editing capabilities)
and do not naturally yield multi-granular control (i.e., covering the full
spectrum of local-to-global edits). To overcome these drawbacks, we propose
EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent
iteration strategy, which re-purposes a pre-trained diffusion model to
facilitate iterative editing. This is complemented by a gradient control
operation for multi-granular control. We introduce a new benchmark dataset to
evaluate our newly proposed setting. We conduct exhaustive quantitatively and
qualitatively evaluation against recent state-of-the-art approaches adapted to
our task, to being out the mettle of EMILIE. We hope our work would attract
attention to this newly identified, pragmatic problem setting.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: STint: Self-supervised Temporal Interpolation for Geospatial Data. (arXiv:2309.00059v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00059">http://arxiv.org/abs/2309.00059</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00059]] STint: Self-supervised Temporal Interpolation for Geospatial Data(http://arxiv.org/abs/2309.00059)</code></li>
<li>Summary: <p>Supervised and unsupervised techniques have demonstrated the potential for
temporal interpolation of video data. Nevertheless, most prevailing temporal
interpolation techniques hinge on optical flow, which encodes the motion of
pixels between video frames. On the other hand, geospatial data exhibits lower
temporal resolution while encompassing a spectrum of movements and deformations
that challenge several assumptions inherent to optical flow. In this work, we
propose an unsupervised temporal interpolation technique, which does not rely
on ground truth data or require any motion information like optical flow, thus
offering a promising alternative for better generalization across geospatial
domains. Specifically, we introduce a self-supervised technique of dual cycle
consistency. Our proposed technique incorporates multiple cycle consistency
losses, which result from interpolating two frames between consecutive input
frames through a series of stages. This dual cycle consistent constraint causes
the model to produce intermediate frames in a self-supervised manner. To the
best of our knowledge, this is the first attempt at unsupervised temporal
interpolation without the explicit use of optical flow. Our experimental
evaluations across diverse geospatial datasets show that STint significantly
outperforms existing state-of-the-art methods for unsupervised temporal
interpolation.
</p></li>
</ul>

<h3>Title: Self-supervised Semantic Segmentation: Consistency over Transformation. (arXiv:2309.00143v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00143">http://arxiv.org/abs/2309.00143</a></li>
<li>Code URL: https://github.com/mindflow-institue/ssct</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00143]] Self-supervised Semantic Segmentation: Consistency over Transformation(http://arxiv.org/abs/2309.00143)</code></li>
<li>Summary: <p>Accurate medical image segmentation is of utmost importance for enabling
automated clinical decision procedures. However, prevailing supervised deep
learning approaches for medical image segmentation encounter significant
challenges due to their heavy dependence on extensive labeled training data. To
tackle this issue, we propose a novel self-supervised algorithm,
\textbf{S$^3$-Net}, which integrates a robust framework based on the proposed
Inception Large Kernel Attention (I-LKA) modules. This architectural
enhancement makes it possible to comprehensively capture contextual information
while preserving local intricacies, thereby enabling precise semantic
segmentation. Furthermore, considering that lesions in medical images often
exhibit deformations, we leverage deformable convolution as an integral
component to effectively capture and delineate lesion deformations for superior
object boundary definition. Additionally, our self-supervised strategy
emphasizes the acquisition of invariance to affine transformations, which is
commonly encountered in medical scenarios. This emphasis on robustness with
respect to geometric distortions significantly enhances the model's ability to
accurately model and handle such distortions. To enforce spatial consistency
and promote the grouping of spatially connected image pixels with similar
feature representations, we introduce a spatial consistency loss term. This
aids the network in effectively capturing the relationships among neighboring
pixels and enhancing the overall segmentation quality. The S$^3$-Net approach
iteratively learns pixel-level feature representations for image content
clustering in an end-to-end manner. Our experimental results on skin lesion and
lung organ segmentation tasks show the superior performance of our method
compared to the SOTA approaches. https://github.com/mindflow-institue/SSCT
</p></li>
</ul>

<h3>Title: Object-Centric Multiple Object Tracking. (arXiv:2309.00233v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00233">http://arxiv.org/abs/2309.00233</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00233]] Object-Centric Multiple Object Tracking(http://arxiv.org/abs/2309.00233)</code></li>
<li>Summary: <p>Unsupervised object-centric learning methods allow the partitioning of scenes
into entities without additional localization information and are excellent
candidates for reducing the annotation burden of multiple-object tracking (MOT)
pipelines. Unfortunately, they lack two key properties: objects are often split
into parts and are not consistently tracked over time. In fact,
state-of-the-art models achieve pixel-level accuracy and temporal consistency
by relying on supervised object detection with additional ID labels for the
association through time. This paper proposes a video object-centric model for
MOT. It consists of an index-merge module that adapts the object-centric slots
into detection outputs and an object memory module that builds complete object
prototypes to handle occlusions. Benefited from object-centric learning, we
only require sparse detection labels (0%-6.25%) for object localization and
feature binding. Relying on our self-supervised
Expectation-Maximization-inspired loss for object association, our approach
requires no ID labels. Our experiments significantly narrow the gap between the
existing object-centric model and the fully supervised state-of-the-art and
outperform several unsupervised trackers.
</p></li>
</ul>

<h3>Title: SparseSat-NeRF: Dense Depth Supervised Neural Radiance Fields for Sparse Satellite Images. (arXiv:2309.00277v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00277">http://arxiv.org/abs/2309.00277</a></li>
<li>Code URL: https://github.com/lulinzhang/sps-nerf</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00277]] SparseSat-NeRF: Dense Depth Supervised Neural Radiance Fields for Sparse Satellite Images(http://arxiv.org/abs/2309.00277)</code></li>
<li>Summary: <p>Digital surface model generation using traditional multi-view stereo matching
(MVS) performs poorly over non-Lambertian surfaces, with asynchronous
acquisitions, or at discontinuities. Neural radiance fields (NeRF) offer a new
paradigm for reconstructing surface geometries using continuous volumetric
representation. NeRF is self-supervised, does not require ground truth geometry
for training, and provides an elegant way to include in its representation
physical parameters about the scene, thus potentially remedying the challenging
scenarios where MVS fails. However, NeRF and its variants require many views to
produce convincing scene's geometries which in earth observation satellite
imaging is rare. In this paper we present SparseSat-NeRF (SpS-NeRF) - an
extension of Sat-NeRF adapted to sparse satellite views. SpS-NeRF employs dense
depth supervision guided by crosscorrelation similarity metric provided by
traditional semi-global MVS matching. We demonstrate the effectiveness of our
approach on stereo and tri-stereo Pleiades 1B/WorldView-3 images, and compare
against NeRF and Sat-NeRF. The code is available at
https://github.com/LulinZhang/SpS-NeRF
</p></li>
</ul>

<h3>Title: SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation. (arXiv:2309.00526v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00526">http://arxiv.org/abs/2309.00526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00526]] SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation(http://arxiv.org/abs/2309.00526)</code></li>
<li>Summary: <p>Recently, self-supervised monocular depth estimation has gained popularity
with numerous applications in autonomous driving and robotics. However,
existing solutions primarily seek to estimate depth from immediate visual
features, and struggle to recover fine-grained scene details with limited
generalization. In this paper, we introduce SQLdepth, a novel approach that can
effectively learn fine-grained scene structures from motion. In SQLdepth, we
propose a novel Self Query Layer (SQL) to build a self-cost volume and infer
depth from it, rather than inferring depth from feature maps. The self-cost
volume implicitly captures the intrinsic geometry of the scene within a single
frame. Each individual slice of the volume signifies the relative distances
between points and objects within a latent space. Ultimately, this volume is
compressed to the depth map via a novel decoding approach. Experimental results
on KITTI and Cityscapes show that our method attains remarkable
state-of-the-art performance (AbsRel = $0.082$ on KITTI, $0.052$ on KITTI with
improved ground-truth and $0.106$ on Cityscapes), achieves $9.9\%$, $5.5\%$ and
$4.5\%$ error reduction from the previous best. In addition, our approach
showcases reduced training complexity, computational efficiency, improved
generalization, and the ability to recover fine-grained scene details.
Moreover, the self-supervised pre-trained and metric fine-tuned SQLdepth can
surpass existing supervised methods by significant margins (AbsRel = $0.043$,
$14\%$ error reduction). self-matching-oriented relative distance querying in
SQL improves the robustness and zero-shot generalization capability of
SQLdepth. Code and the pre-trained weights will be publicly available. Code is
available at
\href{https://github.com/hisfog/SQLdepth-Impl}{https://github.com/hisfog/SQLdepth-Impl}.
</p></li>
</ul>

<h3>Title: Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction. (arXiv:2309.00483v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00483">http://arxiv.org/abs/2309.00483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00483]] Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction(http://arxiv.org/abs/2309.00483)</code></li>
<li>Summary: <p>Molecular property prediction with deep learning has gained much attention
over the past years. Owing to the scarcity of labeled molecules, there has been
growing interest in self-supervised learning methods that learn generalizable
molecular representations from unlabeled data. Molecules are typically treated
as 2D topological graphs in modeling, but it has been discovered that their 3D
geometry is of great importance in determining molecular functionalities. In
this paper, we propose the Geometry-aware line graph transformer (Galformer)
pre-training, a novel self-supervised learning framework that aims to enhance
molecular representation learning with 2D and 3D modalities. Specifically, we
first design a dual-modality line graph transformer backbone to encode the
topological and geometric information of a molecule. The designed backbone
incorporates effective structural encodings to capture graph structures from
both modalities. Then we devise two complementary pre-training tasks at the
inter and intra-modality levels. These tasks provide properly supervised
information and extract discriminative 2D and 3D knowledge from unlabeled
molecules. Finally, we evaluate Galformer against six state-of-the-art
baselines on twelve property prediction benchmarks via downstream fine-tuning.
Experimental results show that Galformer consistently outperforms all baselines
on both classification and regression tasks, demonstrating its effectiveness.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: Large-Scale Public Data Improves Differentially Private Image Generation Quality. (arXiv:2309.00008v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00008">http://arxiv.org/abs/2309.00008</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00008]] Large-Scale Public Data Improves Differentially Private Image Generation Quality(http://arxiv.org/abs/2309.00008)</code></li>
<li>Summary: <p>Public data has been frequently used to improve the privacy-accuracy
trade-off of differentially private machine learning, but prior work largely
assumes that this data come from the same distribution as the private. In this
work, we look at how to use generic large-scale public data to improve the
quality of differentially private image generation in Generative Adversarial
Networks (GANs), and provide an improved method that uses public data
effectively. Our method works under the assumption that the support of the
public data distribution contains the support of the private; an example of
this is when the public data come from a general-purpose internet-scale image
source, while the private data consist of images of a specific type. Detailed
evaluations show that our method achieves SOTA in terms of FID score and other
metrics compared with existing methods that use public data, and can generate
high-quality, photo-realistic images in a differentially private manner.
</p></li>
</ul>

<h3>Title: Model Inversion Attack via Dynamic Memory Learning. (arXiv:2309.00013v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00013">http://arxiv.org/abs/2309.00013</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00013]] Model Inversion Attack via Dynamic Memory Learning(http://arxiv.org/abs/2309.00013)</code></li>
<li>Summary: <p>Model Inversion (MI) attacks aim to recover the private training data from
the target model, which has raised security concerns about the deployment of
DNNs in practice. Recent advances in generative adversarial models have
rendered them particularly effective in MI attacks, primarily due to their
ability to generate high-fidelity and perceptually realistic images that
closely resemble the target data. In this work, we propose a novel Dynamic
Memory Model Inversion Attack (DMMIA) to leverage historically learned
knowledge, which interacts with samples (during the training) to induce diverse
generations. DMMIA constructs two types of prototypes to inject the information
about historically learned knowledge: Intra-class Multicentric Representation
(IMR) representing target-related concepts by multiple learnable prototypes,
and Inter-class Discriminative Representation (IDR) characterizing the
memorized samples as learned prototypes to capture more privacy-related
information. As a result, our DMMIA has a more informative representation,
which brings more diverse and discriminative generated results. Experiments on
multiple benchmarks show that DMMIA performs better than state-of-the-art MI
attack methods.
</p></li>
</ul>

<h3>Title: Unsupervised evaluation of GAN sample quality: Introducing the TTJac Score. (arXiv:2309.00107v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00107">http://arxiv.org/abs/2309.00107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00107]] Unsupervised evaluation of GAN sample quality: Introducing the TTJac Score(http://arxiv.org/abs/2309.00107)</code></li>
<li>Summary: <p>Evaluation metrics are essential for assessing the performance of generative
models in image synthesis. However, existing metrics often involve high memory
and time consumption as they compute the distance between generated samples and
real data points. In our study, the new evaluation metric called the "TTJac
score" is proposed to measure the fidelity of individual synthesized images in
a data-free manner. The study first establishes a theoretical approach to
directly evaluate the generated sample density. Then, a method incorporating
feature extractors and discrete function approximation through tensor train is
introduced to effectively assess the quality of generated samples. Furthermore,
the study demonstrates that this new metric can be used to improve the
fidelity-variability trade-off when applying the truncation trick. The
experimental results of applying the proposed metric to StyleGAN 2 and StyleGAN
2 ADA models on FFHQ, AFHQ-Wild, LSUN-Cars, and LSUN-Horse datasets are
presented. The code used in this research will be made publicly available
online for the research community to access and utilize.
</p></li>
</ul>

<h3>Title: Segmenta\c{c}\~ao e contagem de troncos de madeira utilizando deep learning e processamento de imagens. (arXiv:2309.00123v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00123">http://arxiv.org/abs/2309.00123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00123]] Segmenta\c{c}\~ao e contagem de troncos de madeira utilizando deep learning e processamento de imagens(http://arxiv.org/abs/2309.00123)</code></li>
<li>Summary: <p>Counting objects in images is a pattern recognition problem that focuses on
identifying an element to determine its incidence and is approached in the
literature as Visual Object Counting (VOC). In this work, we propose a
methodology to count wood logs. First, wood logs are segmented from the image
background. This first segmentation step is obtained using the Pix2Pix
framework that implements Conditional Generative Adversarial Networks (CGANs).
Second, the clusters are counted using Connected Components. The average
accuracy of the segmentation exceeds 89% while the average amount of wood logs
identified based on total accounted is over 97%.
</p></li>
</ul>

<h3>Title: CityDreamer: Compositional Generative Model of Unbounded 3D Cities. (arXiv:2309.00610v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00610">http://arxiv.org/abs/2309.00610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00610]] CityDreamer: Compositional Generative Model of Unbounded 3D Cities(http://arxiv.org/abs/2309.00610)</code></li>
<li>Summary: <p>In recent years, extensive research has focused on 3D natural scene
generation, but the domain of 3D city generation has not received as much
exploration. This is due to the greater challenges posed by 3D city generation,
mainly because humans are more sensitive to structural distortions in urban
environments. Additionally, generating 3D cities is more complex than 3D
natural scenes since buildings, as objects of the same class, exhibit a wider
range of appearances compared to the relatively consistent appearance of
objects like trees in natural scenes. To address these challenges, we propose
CityDreamer, a compositional generative model designed specifically for
unbounded 3D cities, which separates the generation of building instances from
other background objects, such as roads, green lands, and water areas, into
distinct modules. Furthermore, we construct two datasets, OSM and GoogleEarth,
containing a vast amount of real-world city imagery to enhance the realism of
the generated 3D cities both in their layouts and appearances. Through
extensive experiments, CityDreamer has proven its superiority over
state-of-the-art methods in generating a wide range of lifelike 3D cities.
</p></li>
</ul>

<h3>Title: LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00155">http://arxiv.org/abs/2309.00155</a></li>
<li>Code URL: https://github.com/stratosphereips/shellm</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00155]] LLM in the Shell: Generative Honeypots(http://arxiv.org/abs/2309.00155)</code></li>
<li>Summary: <p>Honeypots are essential tools in cybersecurity. However, most of them (even
the high-interaction ones) lack the required realism to engage and fool human
attackers. This limitation makes them easily discernible, hindering their
effectiveness. This work introduces a novel method to create dynamic and
realistic software honeypots based on Large Language Models. Preliminary
results indicate that LLMs can create credible and dynamic honeypots capable of
addressing important limitations of previous honeypots, such as deterministic
responses, lack of adaptability, etc. We evaluated the realism of each command
by conducting an experiment with human attackers who needed to say if the
answer from the honeypot was fake or not. Our proposed honeypot, called shelLM,
reached an accuracy rate of 0.92.
</p></li>
</ul>

<h3>Title: Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00236">http://arxiv.org/abs/2309.00236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00236]] Image Hijacking: Adversarial Images can Control Generative Models at Runtime(http://arxiv.org/abs/2309.00236)</code></li>
<li>Summary: <p>Are foundation models secure from malicious actors? In this work, we focus on
the image input to a vision-language model (VLM). We discover image hijacks,
adversarial images that control generative models at runtime. We introduce
Behavior Matching, a general method for creating image hijacks, and we use it
to explore three types of attacks. Specific string attacks generate arbitrary
output of the adversary's choosing. Leak context attacks leak information from
the context window into the output. Jailbreak attacks circumvent a model's
safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM
based on CLIP and LLaMA-2, and find that all our attack types have above a 90\%
success rate. Moreover, our attacks are automated and require only small image
perturbations. These findings raise serious concerns about the security of
foundation models. If image hijacks are as difficult to defend against as
adversarial examples in CIFAR-10, then it might be many years before a solution
is found -- if it even exists.
</p></li>
</ul>

<h3>Title: FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning. (arXiv:2309.00127v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00127">http://arxiv.org/abs/2309.00127</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00127]] FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning(http://arxiv.org/abs/2309.00127)</code></li>
<li>Summary: <p>Current backdoor attacks against federated learning (FL) strongly rely on
universal triggers or semantic patterns, which can be easily detected and
filtered by certain defense mechanisms such as norm clipping, comparing
parameter divergences among local updates. In this work, we propose a new
stealthy and robust backdoor attack with flexible triggers against FL defenses.
To achieve this, we build a generative trigger function that can learn to
manipulate the benign samples with an imperceptible flexible trigger pattern
and simultaneously make the trigger pattern include the most significant hidden
features of the attacker-chosen label. Moreover, our trigger generator can keep
learning and adapt across different rounds, allowing it to adjust to changes in
the global model. By filling the distinguishable difference (the mapping
between the trigger pattern and target label), we make our attack naturally
stealthy. Extensive experiments on real-world datasets verify the effectiveness
and stealthiness of our attack compared to prior attacks on decentralized
learning framework with eight well-studied defenses.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Deep Semi-Supervised Anomaly Detection for Finding Fraud in the Futures Market. (arXiv:2309.00088v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00088">http://arxiv.org/abs/2309.00088</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00088]] Deep Semi-Supervised Anomaly Detection for Finding Fraud in the Futures Market(http://arxiv.org/abs/2309.00088)</code></li>
<li>Summary: <p>Modern financial electronic exchanges are an exciting and fast-paced
marketplace where billions of dollars change hands every day. They are also
rife with manipulation and fraud. Detecting such activity is a major
undertaking, which has historically been a job reserved exclusively for humans.
Recently, more research and resources have been focused on automating these
processes via machine learning and artificial intelligence. Fraud detection is
overwhelmingly associated with the greater field of anomaly detection, which is
usually performed via unsupervised learning techniques because of the lack of
labeled data needed for supervised learning. However, a small quantity of
labeled data does often exist. This research article aims to evaluate the
efficacy of a deep semi-supervised anomaly detection technique, called Deep
SAD, for detecting fraud in high-frequency financial data. We use exclusive
proprietary limit order book data from the TMX exchange in Montr\'eal, with a
small set of true labeled instances of fraud, to evaluate Deep SAD against its
unsupervised predecessor. We show that incorporating a small amount of labeled
data into an unsupervised anomaly detection framework can greatly improve its
accuracy.
</p></li>
</ul>

<h3>Title: Anomaly detection with semi-supervised classification based on risk estimators. (arXiv:2309.00379v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.00379">http://arxiv.org/abs/2309.00379</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.00379]] Anomaly detection with semi-supervised classification based on risk estimators(http://arxiv.org/abs/2309.00379)</code></li>
<li>Summary: <p>A significant limitation of one-class classification anomaly detection
methods is their reliance on the assumption that unlabeled training data only
contains normal instances. To overcome this impractical assumption, we propose
two novel classification-based anomaly detection methods. Firstly, we introduce
a semi-supervised shallow anomaly detection method based on an unbiased risk
estimator. Secondly, we present a semi-supervised deep anomaly detection method
utilizing a nonnegative (biased) risk estimator. We establish estimation error
bounds and excess risk bounds for both risk minimizers. Additionally, we
propose techniques to select appropriate regularization parameters that ensure
the nonnegativity of the empirical risk in the shallow model under specific
loss functions. Our extensive experiments provide strong evidence of the
effectiveness of the risk-based anomaly detection methods.
</p></li>
</ul>

<h2>in-context</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
