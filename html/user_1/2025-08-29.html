<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-08-29</h1>
<h3>Title: CrystalICL: Enabling In-Context Learning for Crystal Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruobing Wang, Qiaoyu Tan, Yili Wang, Ying Wang, Xin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20143">https://arxiv.org/abs/2508.20143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20143">https://arxiv.org/pdf/2508.20143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20143]] CrystalICL: Enabling In-Context Learning for Crystal Generation(https://arxiv.org/abs/2508.20143)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Designing crystal materials with desired physicochemical properties remains a fundamental challenge in materials science. While large language models (LLMs) have demonstrated strong in-context learning (ICL) capabilities, existing LLM-based crystal generation approaches are limited to zero-shot scenarios and are unable to benefit from few-shot scenarios. In contrast, human experts typically design new materials by modifying relevant known structures which aligns closely with the few-shot ICL paradigm. Motivated by this, we propose CrystalICL, a novel model designed for few-shot crystal generation. Specifically, we introduce a space-group based crystal tokenization method, which effectively reduces the complexity of modeling crystal symmetry in LLMs. We further introduce a condition-structure aware hybrid instruction tuning framework and a multi-task instruction tuning strategy, enabling the model to better exploit ICL by capturing structure-property relationships from limited data. Extensive experiments on four crystal generation benchmarks demonstrate the superiority of CrystalICL over the leading baseline methods on conditional and unconditional generation tasks.</li>
</ul>

<h3>Title: SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization</h3>
<ul>
<li><strong>Authors: </strong>Yang Su, Shunquan Tan, Jiwu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20182">https://arxiv.org/abs/2508.20182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20182">https://arxiv.org/pdf/2508.20182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20182]] SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization(https://arxiv.org/abs/2508.20182)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Driven by the new generation of multi-modal large models, such as Stable Diffusion (SD), image manipulation technologies have advanced rapidly, posing significant challenges to image forensics. However, existing image forgery localization methods, which heavily rely on labor-intensive and costly annotated data, are struggling to keep pace with these emerging image manipulation technologies. To address these challenges, we are the first to integrate both image generation and powerful perceptual capabilities of SD into an image forensic framework, enabling more efficient and accurate forgery localization. First, we theoretically show that the multi-modal architecture of SD can be conditioned on forgery-related information, enabling the model to inherently output forgery localization results. Then, building on this foundation, we specifically leverage the multimodal framework of Stable DiffusionV3 (SD3) to enhance forgery localization this http URL leverage the multi-modal processing capabilities of SD3 in the latent space by treating image forgery residuals -- high-frequency signals extracted using specific highpass filters -- as an explicit modality. This modality is fused into the latent space during training to enhance forgery localization performance. Notably, our method fully preserves the latent features extracted by SD3, thereby retaining the rich semantic information of the input image. Experimental results show that our framework achieves up to 12% improvements in performance on widely used benchmarking datasets compared to current state-of-the-art image forgery localization models. Encouragingly, the model demonstrates strong performance on forensic tasks involving real-world document forgery images and natural scene forging images, even when such data were entirely unseen during training.</li>
</ul>

<h3>Title: Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels</h3>
<ul>
<li><strong>Authors: </strong>Hossein Ahmadi, Banafsheh Saffari</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20193">https://arxiv.org/abs/2508.20193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20193">https://arxiv.org/pdf/2508.20193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20193]] Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels(https://arxiv.org/abs/2508.20193)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Automatic modulation recognition (AMR) is critical for cognitive radio, spectrum monitoring, and secure wireless communication. However, existing solutions often rely on large labeled datasets or multi-stage training pipelines, which limit scalability and generalization in practice. We propose a unified Vision Transformer (ViT) framework that integrates supervised, self-supervised, and reconstruction objectives. The model combines a ViT encoder, a lightweight convolutional decoder, and a linear classifier; the reconstruction branch maps augmented signals back to their originals, anchoring the encoder to fine-grained I/Q structure. This strategy promotes robust, discriminative feature learning during pretraining, while partial label supervision in fine-tuning enables effective classification with limited labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and ViT baselines in low-label regimes, approaches ResNet-level accuracy with only 15-20% labeled data, and maintains strong performance across varying SNR levels. Overall, the framework provides a simple, generalizable, and label-efficient solution for AMR.</li>
</ul>

<h3>Title: A Systematic Review on the Generative AI Applications in Human Medical Genomics</h3>
<ul>
<li><strong>Authors: </strong>Anton Changalidis, Yury Barbitoff, Yulia Nasykhova, Andrey Glotov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20275">https://arxiv.org/abs/2508.20275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20275">https://arxiv.org/pdf/2508.20275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20275]] A Systematic Review on the Generative AI Applications in Human Medical Genomics(https://arxiv.org/abs/2508.20275)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Although traditional statistical techniques and machine learning methods have contributed significantly to genetics and, in particular, inherited disease diagnosis, they often struggle with complex, high-dimensional data, a challenge now addressed by state-of-the-art deep learning models. Large language models (LLMs), based on transformer architectures, have excelled in tasks requiring contextual comprehension of unstructured medical data. This systematic review examines the role of LLMs in the genetic research and diagnostics of both rare and common diseases. Automated keyword-based search in PubMed, bioRxiv, medRxiv, and arXiv was conducted, targeting studies on LLM applications in diagnostics and education within genetics and removing irrelevant or outdated models. A total of 172 studies were analyzed, highlighting applications in genomic variant identification, annotation, and interpretation, as well as medical imaging advancements through vision transformers. Key findings indicate that while transformer-based models significantly advance disease and risk stratification, variant interpretation, medical imaging analysis, and report generation, major challenges persist in integrating multimodal data (genomic sequences, imaging, and clinical records) into unified and clinically robust pipelines, facing limitations in generalizability and practical implementation in clinical settings. This review provides a comprehensive classification and assessment of the current capabilities and limitations of LLMs in transforming hereditary disease diagnostics and supporting genetic education, serving as a guide to navigate this rapidly evolving field.</li>
</ul>

<h3>Title: Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization</h3>
<ul>
<li><strong>Authors: </strong>Frank RÃ¶der, Jan Benad, Manfred Eppe, Pradeep Kr. Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20294">https://arxiv.org/abs/2508.20294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20294">https://arxiv.org/pdf/2508.20294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20294]] Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization(https://arxiv.org/abs/2508.20294)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI's latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.</li>
</ul>

<h3>Title: Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)</h3>
<ul>
<li><strong>Authors: </strong>Zhi Li, Hau Phan, Matthew Emigh, Austin J. Brockmeier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20322">https://arxiv.org/abs/2508.20322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20322">https://arxiv.org/pdf/2508.20322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20322]] Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)(https://arxiv.org/abs/2508.20322)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Vision-language co-embedding networks, such as CLIP, provide a latent embedding space with semantic information that is useful for downstream tasks. We hypothesize that the embedding space can be disentangled to separate the information on the content of complex scenes by decomposing the embedding into multiple concept-specific component vectors that lie in different subspaces. We propose a supervised dictionary learning approach to estimate a linear synthesis model consisting of sparse, non-negative combinations of groups of vectors in the dictionary (atoms), whose group-wise activity matches the multi-label information. Each concept-specific component is a non-negative combination of atoms associated to a label. The group-structured dictionary is optimized through a novel alternating optimization with guaranteed convergence. Exploiting the text co-embeddings, we detail how semantically meaningful descriptions can be found based on text embeddings of words best approximated by a concept's group of atoms, and unsupervised dictionary learning can exploit zero-shot classification of training set images using the text embeddings of concept labels to provide instance-wise multi-labels. We show that the disentangled embeddings provided by our sparse linear concept subspaces (SLiCS) enable concept-filtered image retrieval (and conditional generation using image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed autoencoder embeddings from TiTok and the latent embedding from self-supervised DINOv2. Quantitative and qualitative results highlight the improved precision of the concept-filtered image retrieval for all embeddings.</li>
</ul>

<h3>Title: MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xiao Li, Yanfan Zhu, Ruining Deng, Wei-Qi Wei, Yu Wang, Shilin Zhao, Yaohong Wang, Haichun Yang, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20345">https://arxiv.org/abs/2508.20345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20345">https://arxiv.org/pdf/2508.20345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20345]] MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models(https://arxiv.org/abs/2508.20345)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent advances in medical vision-language models (VLMs) open up remarkable opportunities for clinical applications such as automated report generation, copilots for physicians, and uncertainty quantification. However, despite their promise, medical VLMs introduce serious security concerns, most notably risks of Protected Health Information (PHI) exposure, data leakage, and vulnerability to cyberthreats - which are especially critical in hospital environments. Even when adopted for research or non-clinical purposes, healthcare organizations must exercise caution and implement safeguards. To address these challenges, we present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1) enables physicians to manually select and use different models without programming expertise, (2) supports engineers in efficiently deploying medical VLMs in a plug-and-play fashion, with seamless integration of Hugging Face open-source models, and (3) ensures privacy-preserving inference through Docker-orchestrated, operating system agnostic deployment. MedFoundationHub requires only an offline local workstation equipped with a single NVIDIA A6000 GPU, making it both secure and accessible within the typical resources of academic research labs. To evaluate current capabilities, we engaged board-certified pathologists to deploy and assess five state-of-the-art VLMs (Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases, yielding 1015 clinician-model scoring events. These assessments revealed recurring limitations, including off-target answers, vague reasoning, and inconsistent pathology terminology.</li>
</ul>

<h3>Title: Audio-Guided Visual Editing with Complex Multi-Modal Prompts</h3>
<ul>
<li><strong>Authors: </strong>Hyeonyu Kim, Seokhoon Jeong, Seonghee Han, Chanhyuk Choi, Taehwan Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20379">https://arxiv.org/abs/2508.20379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20379">https://arxiv.org/pdf/2508.20379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20379]] Audio-Guided Visual Editing with Complex Multi-Modal Prompts(https://arxiv.org/abs/2508.20379)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.</li>
</ul>

<h3>Title: CAPE: Context-Aware Personality Evaluation Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jivnesh Sandhan, Fei Cheng, Tushar Sandhan, Yugo Murawaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20385">https://arxiv.org/abs/2508.20385</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20385">https://arxiv.org/pdf/2508.20385</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20385]] CAPE: Context-Aware Personality Evaluation Framework for Large Language Models(https://arxiv.org/abs/2508.20385)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Psychometric tests, traditionally used to assess humans, are now being applied to Large Language Models (LLMs) to evaluate their behavioral traits. However, existing studies follow a context-free approach, answering each question in isolation to avoid contextual influence. We term this the Disney World test, an artificial setting that ignores real-world applications, where conversational history shapes responses. To bridge this gap, we propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. To thoroughly analyze the influence of context, we introduce novel metrics to quantify the consistency of LLM responses, a fundamental trait in human behavior. Our exhaustive experiments on 7 LLMs reveal that conversational history enhances response consistency via in-context learning but also induces personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash and Llama-8B display significant sensitivity. Moreover, GPT models response stem from their intrinsic personality traits as well as prior interactions, whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions. Finally, applying our framework to Role Playing Agents (RPAs) shows context-dependent personality shifts improve response consistency and better align with human judgments. Our code and datasets are publicly available at: this https URL</li>
</ul>

<h3>Title: MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Wang, Junyang Zhang, Guanquan Shi, HaoRan Cheng, Yunhao Yao, Kaiwen Guo, Haohua Du, Xiang-Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20412">https://arxiv.org/abs/2508.20412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20412">https://arxiv.org/pdf/2508.20412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20412]] MindGuard: Tracking, Detecting, and Attributing MCP Tool Poisoning Attack via Decision Dependence Graph(https://arxiv.org/abs/2508.20412)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The Model Context Protocol (MCP) is increasingly adopted to standardize the interaction between LLM agents and external tools. However, this trend introduces a new threat: Tool Poisoning Attacks (TPA), where tool metadata is poisoned to induce the agent to perform unauthorized operations. Existing defenses that primarily focus on behavior-level analysis are fundamentally ineffective against TPA, as poisoned tools need not be executed, leaving no behavioral trace to monitor. Thus, we propose MindGuard, a decision-level guardrail for LLM agents, providing provenance tracking of call decisions, policy-agnostic detection, and poisoning source attribution against TPA. While fully explaining LLM decision remains challenging, our empirical findings uncover a strong correlation between LLM attention mechanisms and tool invocation decisions. Therefore, we choose attention as an empirical signal for decision tracking and formalize this as the Decision Dependence Graph (DDG), which models the LLM's reasoning process as a weighted, directed graph where vertices represent logical concepts and edges quantify the attention-based dependencies. We further design robust DDG construction and graph-based anomaly analysis mechanisms that efficiently detect and attribute TPA attacks. Extensive experiments on real-world datasets demonstrate that MindGuard achieves 94\%-99\% average precision in detecting poisoned invocations, 95\%-100\% attribution accuracy, with processing times under one second and no additional token cost. Moreover, DDG can be viewed as an adaptation of the classical Program Dependence Graph (PDG), providing a solid foundation for applying traditional security policies at the decision level.</li>
</ul>

<h3>Title: Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Desen Sun, Shuncheng Jie, Sihang Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20424">https://arxiv.org/abs/2508.20424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20424">https://arxiv.org/pdf/2508.20424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20424]] Breaking Diffusion with Cache: Exploiting Approximate Caches in Diffusion Models(https://arxiv.org/abs/2508.20424)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are a powerful class of generative models that produce content, such as images, from user prompts, but they are computationally intensive. To mitigate this cost, recent academic and industry work has adopted approximate caching, which reuses intermediate states from similar prompts in a cache. While efficient, this optimization introduces new security risks by breaking isolation among users. This work aims to comprehensively assess new security vulnerabilities arising from approximate caching. First, we demonstrate a remote covert channel established with the cache, where a sender injects prompts with special keywords into the cache and a receiver can recover that even after days, to exchange information. Second, we introduce a prompt stealing attack using the cache, where an attacker can recover existing cached prompts based on cache hit prompts. Finally, we introduce a poisoning attack that embeds the attacker's logos into the previously stolen prompt, to render them in future user prompts that hit the cache. These attacks are all performed remotely through the serving system, which indicates severe security vulnerabilities in approximate caching.</li>
</ul>

<h3>Title: On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating</h3>
<ul>
<li><strong>Authors: </strong>Michael Widener, Kausik Lakkaraju, John Aydin, Biplav Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20437">https://arxiv.org/abs/2508.20437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20437">https://arxiv.org/pdf/2508.20437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20437]] On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating(https://arxiv.org/abs/2508.20437)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time-series forecasting models (TSFM) have evolved from classical statistical methods to sophisticated foundation models, yet understanding why and when these models succeed or fail remains challenging. Despite this known limitation, time series forecasting models are increasingly used to generate information that informs real-world actions with equally real consequences. Understanding the complexity, performance variability, and opaque nature of these models then becomes a valuable endeavor to combat serious concerns about how users should interact with and rely on these models' outputs. This work addresses these concerns by combining traditional explainable AI (XAI) methods with Rating Driven Explanations (RDE) to assess TSFM performance and interpretability across diverse domains and use cases. We evaluate four distinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series specific foundation model), Llama (general-purpose; both fine-tuned and base models) on four heterogeneous datasets spanning finance, energy, transportation, and automotive sales domains. In doing so, we demonstrate that feature-engineered models (e.g., Gradient Boosting) consistently outperform foundation models (e.g., Chronos) in volatile or sparse domains (e.g., power, car parts) while providing more interpretable explanations, whereas foundation models excel only in stable or trend-driven contexts (e.g., finance).</li>
</ul>

<h3>Title: Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Liu, Jian Lou, Yuke Hu, Xiaochen Li, Tailun Chen, Yitian Chen, Zhan Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20443">https://arxiv.org/abs/2508.20443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20443">https://arxiv.org/pdf/2508.20443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20443]] Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint(https://arxiv.org/abs/2508.20443)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained on massive datasets that may include private or copyrighted content. Due to growing privacy and ownership concerns, data owners may request the removal of their data from trained models. Machine unlearning provides a practical solution by removing the influence of specific data without full retraining. However, most existing methods lack a sound forgetting boundary, causing some samples to be under-forgotten, leaving residual leakage risks, while others remain over-forgotten at the expense of degraded utility. In this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss Reweighting with Proxy Constraint), a novel unlearning framework that addresses these limitations through two key components. First, entanglement-awareness guided loss reweighting determines the forgetting effort of each sample by measuring its similarity to retain samples in the embedding space, enabling more targeted and effective unlearning. Second, a proxy constraint leveraging ICL (In-Context Learning) generated test data softly regularizes the forgetting process, effectively mitigating over-forgetting. EAGLE-PC is compatible with existing gradient-based objectives and serves as a plug-and-play enhancement. We evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent improvements in the forgetting-utility trade-off across multiple LLMs. Combined with the NPO+GD optimizer, it approaches full retraining performance, offering a scalable and robust unlearning solution.</li>
</ul>

<h3>Title: Evaluating Differentially Private Generation of Domain-Specific Text</h3>
<ul>
<li><strong>Authors: </strong>Yidan Sun, Viktor Schlegel, Srinivasan Nandakumar, Iqra Zahid, Yuping Wu, Warren Del-Pinto, Goran Nenadic, Siew-Kei Lam, Jie Zhang, Anil A Bharath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20452">https://arxiv.org/abs/2508.20452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20452">https://arxiv.org/pdf/2508.20452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20452]] Evaluating Differentially Private Generation of Domain-Specific Text(https://arxiv.org/abs/2508.20452)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI offers transformative potential for high-stakes domains such as healthcare and finance, yet privacy and regulatory barriers hinder the use of real-world data. To address this, differentially private synthetic data generation has emerged as a promising alternative. In this work, we introduce a unified benchmark to systematically evaluate the utility and fidelity of text datasets generated under formal Differential Privacy (DP) guarantees. Our benchmark addresses key challenges in domain-specific benchmarking, including choice of representative data and realistic privacy budgets, accounting for pre-training and a variety of evaluation metrics. We assess state-of-the-art privacy-preserving generation methods across five domain-specific datasets, revealing significant utility and fidelity degradation compared to real data, especially under strict privacy constraints. These findings underscore the limitations of current approaches, outline the need for advanced privacy-preserving data sharing methods and set a precedent regarding their evaluation in realistic scenarios.</li>
</ul>

<h3>Title: Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaochuan Li, Guoguang Du, Runze Zhang, Liang Jin, Qi Jia, Lihua Lu, Zhenhua Guo, Yaqian Zhao, Haiyang Liu, Tianqi Wang, Changsheng Li, Xiaoli Gong, Rengang Li, Baoyu Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20470">https://arxiv.org/abs/2508.20470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20470">https://arxiv.org/pdf/2508.20470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20470]] Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation(https://arxiv.org/abs/2508.20470)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Scaling laws have validated the success and promise of large-data-trained models in creative generation across text, image, and video domains. However, this paradigm faces data scarcity in the 3D domain, as there is far less of it available on the internet compared to the aforementioned modalities. Fortunately, there exist adequate videos that inherently contain commonsense priors, offering an alternative supervisory signal to mitigate the generalization bottleneck caused by limited native 3D data. On the one hand, videos capturing multiple views of an object or scene provide a spatial consistency prior for 3D generation. On the other hand, the rich semantic information contained within the videos enables the generated content to be more faithful to the text prompts and semantically plausible. This paper explores how to apply the video modality in 3D asset generation, spanning datasets to models. We introduce Droplet3D-4M, the first large-scale video dataset with multi-view level annotations, and train Droplet3D, a generative model supporting both image and dense text input. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to produce spatially consistent and semantically plausible content. Moreover, in contrast to the prevailing 3D solutions, our approach exhibits the potential for extension to scene-level applications. This indicates that the commonsense priors from the videos significantly facilitate 3D creation. We have open-sourced all resources including the dataset, code, technical framework, and model weights: this https URL.</li>
</ul>

<h3>Title: Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiusi Li, Jackson Jiang, Jinyu Miao, Miao Long, Tuopu Wen, Peijin Jia, Shengxiang Liu, Chunlei Yu, Maolin Liu, Yuzhan Cai, Kun Jiang, Mengmeng Yang, Diange Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20471">https://arxiv.org/abs/2508.20471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20471">https://arxiv.org/pdf/2508.20471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20471]] Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation(https://arxiv.org/abs/2508.20471)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Corner cases are crucial for training and validating autonomous driving systems, yet collecting them from the real world is often costly and hazardous. Editing objects within captured sensor data offers an effective alternative for generating diverse scenarios, commonly achieved through 3D Gaussian Splatting or image generative models. However, these approaches often suffer from limited visual fidelity or imprecise pose control. To address these issues, we propose G^2Editor, a framework designed for photorealistic and precise object editing in driving videos. Our method leverages a 3D Gaussian representation of the edited object as a dense prior, injected into the denoising process to ensure accurate pose control and spatial consistency. A scene-level 3D bounding box layout is employed to reconstruct occluded areas of non-target objects. Furthermore, to guide the appearance details of the edited object, we incorporate hierarchical fine-grained features as additional conditions during generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor effectively supports object repositioning, insertion, and deletion within a unified framework, outperforming existing methods in both pose controllability and visual quality, while also benefiting downstream data-driven tasks.</li>
</ul>

<h3>Title: IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Xuanming Cao, Chengyu Tao, Yifeng Cheng, Juan Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20492">https://arxiv.org/abs/2508.20492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20492">https://arxiv.org/pdf/2508.20492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20492]] IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection(https://arxiv.org/abs/2508.20492)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce an novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet achieves a new state-of-the-art with a markedly lower false positive rate, underscoring its practical value for industrial deployment.</li>
</ul>

<h3>Title: Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent</h3>
<ul>
<li><strong>Authors: </strong>En Ci, Shanyan Guan, Yanhao Ge, Yilin Zhang, Wei Li, Zhenyu Zhang, Jian Yang, Ying Tai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20505">https://arxiv.org/abs/2508.20505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20505">https://arxiv.org/pdf/2508.20505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20505]] Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent(https://arxiv.org/abs/2508.20505)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the progress in text-to-image generation, semantic image editing remains a challenge. Inversion-based algorithms unavoidably introduce reconstruction errors, while instruction-based models mainly suffer from limited dataset quality and scale. To address these problems, we propose a descriptive-prompt-based editing framework, named DescriptiveEdit. The core idea is to re-frame `instruction-based image editing' as `reference-image-based text-to-image generation', which preserves the generative power of well-trained Text-to-Image models without architectural modifications or inversion. Specifically, taking the reference image and a prompt as input, we introduce a Cross-Attentive UNet, which newly adds attention bridges to inject reference image features into the prompt-to-edit-image generation process. Owing to its text-to-image nature, DescriptiveEdit overcomes limitations in instruction dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other extensions, and is more scalable. Experiments on the Emu Edit benchmark show it improves editing accuracy and consistency.</li>
</ul>

<h3>Title: DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample</h3>
<ul>
<li><strong>Authors: </strong>Wenting Yin, Han Sun, Xinru Meng, Ningzhong Liu, Huiyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20516">https://arxiv.org/abs/2508.20516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20516">https://arxiv.org/pdf/2508.20516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20516]] DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample(https://arxiv.org/abs/2508.20516)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Continual test-time adaptation aims to continuously adapt a pre-trained model to a stream of target domain data without accessing source data. Without access to source domain data, the model focuses solely on the feature characteristics of the target data. Relying exclusively on these features can lead to confusion and introduce learning biases. Currently, many existing methods generate pseudo-labels via model predictions. However, the quality of pseudo-labels cannot be guaranteed and the problem of error accumulation must be solved. To address these challenges, we propose DCFS, a novel CTTA framework that introduces dual-path feature consistency and confidence-aware sample learning. This framework disentangles the whole feature representation of the target data into semantic-related feature and domain-related feature using dual classifiers to learn distinct feature representations. By maintaining consistency between the sub-features and the whole feature, the model can comprehensively capture data features from multiple perspectives. Additionally, to ensure that the whole feature information of the target domain samples is not overlooked, we set a adaptive threshold and calculate a confidence score for each sample to carry out loss weighted self-supervised learning, effectively reducing the noise of pseudo-labels and alleviating the problem of error accumulation. The efficacy of our proposed method is validated through extensive experimentation across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C, demonstrating consistent performance in continual test-time adaptation scenarios.</li>
</ul>

<h3>Title: Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Mingqian Ji, Jian Yang, Shanshan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20530">https://arxiv.org/abs/2508.20530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20530">https://arxiv.org/pdf/2508.20530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20530]] Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection(https://arxiv.org/abs/2508.20530)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation benchmark.</li>
</ul>

<h3>Title: MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning</h3>
<ul>
<li><strong>Authors: </strong>Weihai Zhi, Jiayan Guo, Shangyang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20549">https://arxiv.org/abs/2508.20549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20549">https://arxiv.org/pdf/2508.20549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20549]] MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning(https://arxiv.org/abs/2508.20549)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>The application of Vision-Language Models (VLMs) in medicine is critically hampered by the scarcity of high-quality, expert-annotated data. Supervised Fine-Tuning (SFT) on existing datasets often leads to poor generalization on unseen modalities and tasks, while Reinforcement Learning (RL), a promising alternative, is stymied by the lack of reliable reward signals in this data-scarce domain. To break this impasse, we introduce Generative Reward Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a reward model, enabling the automated, continuous creation of high-quality, multi-modal medical data that serves as both a superior training source for SFT and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data already surpasses baselines trained on large-scale, human-curated datasets. Crucially, when leveraging this data for RL via Group Relative Policy Optimization (GRPO), our model achieves state-of-the-art cross-modality and cross-task generalization, significantly outperforming specialized RL-based methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves performance competitive with foundation models possessing over 10 times more parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in high-stakes domains, transforming the problem from data scarcity to data generation and unlocking the full potential of RL for building truly generalizable medical AI.</li>
</ul>

<h3>Title: Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Xiong, Yixuan Nan, Li Gao, Hengzhu Tang, Shuaiqiang Wang, Junfeng Wang, Dawei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20559">https://arxiv.org/abs/2508.20559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20559">https://arxiv.org/pdf/2508.20559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20559]] Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search(https://arxiv.org/abs/2508.20559)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In the dynamic landscape of large-scale web search, Query-Driven Text Summarization (QDTS) aims to generate concise and informative summaries from textual documents based on a given query, which is essential for improving user engagement and facilitating rapid decision-making. Traditional extractive summarization models, based primarily on ranking candidate summary segments, have been the dominant approach in industrial applications. However, these approaches suffer from two key limitations: 1) The multi-stage pipeline often introduces cumulative information loss and architectural bottlenecks due to its weakest component; 2) Traditional models lack sufficient semantic understanding of both user queries and documents, particularly when dealing with complex search intents. In this study, we propose a novel framework to pioneer the application of generative models to address real-time QDTS in industrial web search. Our approach integrates large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to transform a lightweight model with only 0.1B parameters into a domain-specialized QDTS expert. Evaluated on multiple industry-relevant metrics, our model outperforms the production baseline and achieves a new state of the art. Furthermore, it demonstrates excellent deployment efficiency, requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per second under 55~ms average latency per query.</li>
</ul>

<h3>Title: FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Chong, Yanwei Lei, Shiyue Zhang, Zhuandi He, Zhen Wang, Xujie Zhang, Xiao Dong, Yiling Wu, Dongmei Jiang, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20586">https://arxiv.org/abs/2508.20586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20586">https://arxiv.org/pdf/2508.20586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20586]] FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models(https://arxiv.org/abs/2508.20586)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency.</li>
</ul>

<h3>Title: Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Mengxiao Huang, Minglei Shu, Shuwang Zhou, Zhaoyang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20595">https://arxiv.org/abs/2508.20595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20595">https://arxiv.org/pdf/2508.20595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20595]] Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations(https://arxiv.org/abs/2508.20595)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deepfake technology, driven by Generative Adversarial Networks (GANs), poses significant risks to privacy and societal security. Existing detection methods are predominantly passive, focusing on post-event analysis without preventing attacks. To address this, we propose an active defense method based on low-frequency perceptual perturbations to disrupt face swapping manipulation, reducing the performance and naturalness of generated content. Unlike prior approaches that used low-frequency perturbations to impact classification accuracy,our method directly targets the generative process of deepfake techniques. We combine frequency and spatial domain features to strengthen defenses. By introducing artifacts through low-frequency perturbations while preserving high-frequency details, we ensure the output remains visually plausible. Additionally, we design a complete architecture featuring an encoder, a perturbation generator, and a decoder, leveraging discrete wavelet transform (DWT) to extract low-frequency components and generate perturbations that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW demonstrate significant reductions in face-swapping effectiveness, improved defense success rates, and preservation of visual quality.</li>
</ul>

<h3>Title: Physics Informed Generative Models for Magnetic Field Images</h3>
<ul>
<li><strong>Authors: </strong>Aye Phyu Phyu Aung, Lucas Lum, Zhansen Shi, Wen Qiu, Bernice Zee, JM Chin, Yeow Kheng Lim, J.Senthilnath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20612">https://arxiv.org/abs/2508.20612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20612">https://arxiv.org/pdf/2508.20612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20612]] Physics Informed Generative Models for Magnetic Field Images(https://arxiv.org/abs/2508.20612)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In semiconductor manufacturing, defect detection and localization are critical to ensuring product quality and yield. While X-ray imaging is a reliable non-destructive testing method, it is memory-intensive and time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a more efficient means to localize regions of interest (ROI) for targeted X-ray scanning. However, the limited availability of MFI datasets due to proprietary concerns presents a significant bottleneck for training machine learning (ML) models using MFI. To address this challenge, we consider an ML-driven approach leveraging diffusion models with two physical constraints. We propose Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate synthetic MFI samples by integrating specific physical information. We generate MFI images for the most common defect types: power shorts. These synthetic images will serve as training data for ML algorithms designed to localize defect areas efficiently. To evaluate generated MFIs, we compare our model to SOTA generative models from both variational autoencoder (VAE) and diffusion methods. We present a domain expert evaluation to assess the generated samples. In addition, we present qualitative and quantitative evaluation using various metrics used for image generation and signal processing, showing promising results to optimize the defect localization process.</li>
</ul>

<h3>Title: EmoCAST: Emotional Talking Portrait via Emotive Text Description</h3>
<ul>
<li><strong>Authors: </strong>Yiguo Jiang, Xiaodong Cun, Yong Zhang, Yudian Zheng, Fan Tang, Chi-Man Pun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20615">https://arxiv.org/abs/2508.20615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20615">https://arxiv.org/pdf/2508.20615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20615]] EmoCAST: Emotional Talking Portrait via Emotive Text Description(https://arxiv.org/abs/2508.20615)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are primarily collected in lab settings, further exacerbating these shortcomings. Consequently, these limitations substantially hinder practical applications in real-world scenarios. To address these challenges, we propose EmoCAST, a diffusion-based framework with two key modules for precise text-driven emotional synthesis. In appearance modeling, emotional prompts are integrated through a text-guided decoupled emotive module, enhancing the spatial knowledge to improve emotion comprehension. To improve the relationship between audio and emotion, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide more precise facial motion synthesis. Additionally, we construct an emotional talking head dataset with comprehensive emotive text descriptions to optimize the framework's performance. Based on the proposed dataset, we propose an emotion-aware sampling training strategy and a progressive functional training strategy that further improve the model's ability to capture nuanced expressive features and achieve accurate lip-synchronization. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: this https URL</li>
</ul>

<h3>Title: Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications</h3>
<ul>
<li><strong>Authors: </strong>Immanuel RoÃteutscher, Klaus S. Drese, Thorsten Uphues</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20622">https://arxiv.org/abs/2508.20622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20622">https://arxiv.org/pdf/2508.20622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20622]] Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications(https://arxiv.org/abs/2508.20622)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We investigated the adaptation and performance of Masked Autoencoders (MAEs) with Vision Transformer (ViT) architectures for self-supervised representation learning on one-dimensional (1D) ultrasound signals. Although MAEs have demonstrated significant success in computer vision and other domains, their use for 1D signal analysis, especially for raw ultrasound data, remains largely unexplored. Ultrasound signals are vital in industrial applications such as non-destructive testing (NDT) and structural health monitoring (SHM), where labeled data are often scarce and signal processing is highly task-specific. We propose an approach that leverages MAE to pre-train on unlabeled synthetic ultrasound signals, enabling the model to learn robust representations that enhance performance in downstream tasks, such as time-of-flight (ToF) classification. This study systematically investigated the impact of model size, patch size, and masking ratio on pre-training efficiency and downstream accuracy. Our results show that pre-trained models significantly outperform models trained from scratch and strong convolutional neural network (CNN) baselines optimized for the downstream task. Additionally, pre-training on synthetic data demonstrates superior transferability to real-world measured signals compared with training solely on limited real datasets. This study underscores the potential of MAEs for advancing ultrasound signal analysis through scalable, self-supervised learning.</li>
</ul>

<h3>Title: AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Xin, Xiaolin Zhang, Yanbin Liu, Peng Zhang, Caifeng Shan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20623">https://arxiv.org/abs/2508.20623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20623">https://arxiv.org/pdf/2508.20623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20623]] AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images(https://arxiv.org/abs/2508.20623)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in Gaussian Splatting have significantly boosted the reconstruction of head avatars, enabling high-quality facial modeling by representing an 3D avatar as a collection of 3D Gaussians. However, existing methods predominantly rely on frontal-view images, leaving the back-head poorly constructed. This leads to geometric inconsistencies, structural blurring, and reduced realism in the rear regions, ultimately limiting the fidelity of reconstructed avatars. To address this challenge, we propose AvatarBack, a novel plug-and-play framework specifically designed to reconstruct complete and consistent 3D Gaussian avatars by explicitly modeling the missing back-head regions. AvatarBack integrates two core technical innovations,i.e., the Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy (ASA). The former leverages a generative prior to synthesize identity-consistent, plausible back-view pseudo-images from sparse frontal inputs, providing robust multi-view supervision. To achieve precise geometric alignment between these synthetic views and the 3D Gaussian representation, the later employs learnable transformation matrices optimized during training, effectively resolving inherent pose and coordinate discrepancies. Extensive experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric, photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack significantly enhances back-head reconstruction quality while preserving frontal fidelity. Moreover, the reconstructed avatars maintain consistent visual realism under diverse motions and remain fully animatable.</li>
</ul>

<h3>Title: ArtFace: Towards Historical Portrait Face Identification via Model Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Francois Poh, Anjith George, SÃ©bastien Marcel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20626">https://arxiv.org/abs/2508.20626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20626">https://arxiv.org/pdf/2508.20626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20626]] ArtFace: Towards Historical Portrait Face Identification via Model Adaptation(https://arxiv.org/abs/2508.20626)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Identifying sitters in historical paintings is a key task for art historians, offering insight into their lives and how they chose to be seen. However, the process is often subjective and limited by the lack of data and stylistic variations. Automated facial recognition is capable of handling challenging conditions and can assist, but while traditional facial recognition models perform well on photographs, they struggle with paintings due to domain shift and high intra-class variation. Artistic factors such as style, skill, intent, and influence from other works further complicate recognition. In this work, we investigate the potential of foundation models to improve facial recognition in artworks. By fine-tuning foundation models and integrating their embeddings with those from conventional facial recognition networks, we demonstrate notable improvements over current state-of-the-art methods. Our results show that foundation models can bridge the gap where traditional methods are ineffective. Paper page at this https URL</li>
</ul>

<h3>Title: CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ayan Banerjee, Fernando VilariÃ±o, Josep LladÃ³s</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20640">https://arxiv.org/abs/2508.20640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20640">https://arxiv.org/pdf/2508.20640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20640]] CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models(https://arxiv.org/abs/2508.20640)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the "style-first, identity-after" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications.</li>
</ul>

<h3>Title: VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation</h3>
<ul>
<li><strong>Authors: </strong>Leyang Wang, Mingtian Zhang, Zijing Ou, David Barber</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20646">https://arxiv.org/abs/2508.20646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20646">https://arxiv.org/pdf/2508.20646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20646]] VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation(https://arxiv.org/abs/2508.20646)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, diffusion distillation methods have compressed thousand-step teacher diffusion models into one-step student generators while preserving sample quality. Most existing approaches train the student model using a diffusive divergence whose gradient is approximated via the student's score function, learned through denoising score matching (DSM). Since DSM training is imperfect, the resulting gradient estimate is inevitably biased, leading to sub-optimal performance. In this paper, we propose VarDiU (pronounced /va:rdju:/), a Variational Diffusive Upper Bound that admits an unbiased gradient estimator and can be directly applied to diffusion distillation. Using this objective, we compare our method with Diff-Instruct and demonstrate that it achieves higher generation quality and enables a more efficient and stable training procedure for one-step diffusion distillation.</li>
</ul>

<h3>Title: "Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Anastasios Skoularikis, Stefanos-Iordanis Papadopoulos, Symeon Papadopoulos, Panagiotis C. Petrantonakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20670">https://arxiv.org/abs/2508.20670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20670">https://arxiv.org/pdf/2508.20670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20670]] "Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection(https://arxiv.org/abs/2508.20670)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal AI have enabled progress in detecting synthetic and out-of-context content. However, existing efforts largely overlook the intent behind AI-generated images. To fill this gap, we introduce S-HArM, a multimodal dataset for intent-aware classification, comprising 9,576 "in the wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art, or Misinformation. Additionally, we explore three prompting strategies (image-guided, description-guided, and multimodally-guided) to construct a large-scale synthetic training dataset with Stable Diffusion. We conduct an extensive comparative study including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models. Our results show that models trained on image- and multimodally-guided data generalize better to "in the wild" content, due to preserved visual context. However, overall performance remains limited, highlighting the complexity of inferring intent and the need for specialized architectures.</li>
</ul>

<h3>Title: Generative Annotation for ASR Named Entity Correction</h3>
<ul>
<li><strong>Authors: </strong>Yuanchang Luo, Daimeng Wei, Shaojun Li, Hengchao Shang, Jiaxin Guo, Zongyao Li, Zhanglin Wu, Xiaoyu Chen, Zhiqiang Rao, Jinlong Yang, Hao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20700">https://arxiv.org/abs/2508.20700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20700">https://arxiv.org/pdf/2508.20700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20700]] Generative Annotation for ASR Named Entity Correction(https://arxiv.org/abs/2508.20700)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>End-to-end automatic speech recognition systems often fail to transcribe domain-specific named entities, causing catastrophic failures in downstream tasks. Numerous fast and lightweight named entity correction (NEC) models have been proposed in recent years. These models, mainly leveraging phonetic-level edit distance algorithms, have shown impressive performances. However, when the forms of the wrongly-transcribed words(s) and the ground-truth entity are significantly different, these methods often fail to locate the wrongly transcribed words in hypothesis, thus limiting their usage. We propose a novel NEC method that utilizes speech sound features to retrieve candidate entities. With speech sound features and candidate entities, we inovatively design a generative method to annotate entity errors in ASR transcripts and replace the text with correct entities. This method is effective in scenarios of word form difference. We test our method using open-source and self-constructed test sets. The results demonstrate that our NEC method can bring significant improvement to entity accuracy. We will open source our self-constructed test set and training data.</li>
</ul>

<h3>Title: EEGDM: Learning EEG Representation with Latent Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Shaocong Wang, Tong Liu, Ming Li, Minjing Yu, Yong-Jin Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20705">https://arxiv.org/abs/2508.20705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20705">https://arxiv.org/pdf/2508.20705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20705]] EEGDM: Learning EEG Representation with Latent Diffusion Model(https://arxiv.org/abs/2508.20705)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>While electroencephalography (EEG) signal analysis using deep learning has shown great promise, existing approaches still face significant challenges in learning generalizable representations that perform well across diverse tasks, particularly when training data is limited. Current EEG representation learning methods including EEGPT and LaBraM typically rely on simple masked reconstruction objective, which may not fully capture the rich semantic information and complex patterns inherent in EEG signals. In this paper, we propose EEGDM, a novel self-supervised EEG representation learning method based on the latent diffusion model, which leverages EEG signal generation as a self-supervised objective, turning the diffusion model into a strong representation learner capable of capturing EEG semantics. EEGDM incorporates an EEG encoder that distills EEG signals and their channel augmentations into a compact representation, acting as conditional information to guide the diffusion model for generating EEG signals. This design endows EEGDM with a compact latent space, which not only offers ample control over the generative process but also can be leveraged for downstream tasks. Experimental results show that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively learns robust representations, and (3) achieves competitive performance with modest pre-training data size across diverse downstream tasks, underscoring its generalizability and practical utility.</li>
</ul>

<h3>Title: Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Gowreesh Mago, Pascal Mettes, Stevan Rudinac</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20765">https://arxiv.org/abs/2508.20765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20765">https://arxiv.org/pdf/2508.20765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20765]] Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding(https://arxiv.org/abs/2508.20765)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The automatic understanding of video content is advancing rapidly. Empowered by deeper neural networks and large datasets, machines are increasingly capable of understanding what is concretely visible in video frames, whether it be objects, actions, events, or scenes. In comparison, humans retain a unique ability to also look beyond concrete entities and recognize abstract concepts like justice, freedom, and togetherness. Abstract concept recognition forms a crucial open challenge in video understanding, where reasoning on multiple semantic levels based on contextual information is key. In this paper, we argue that the recent advances in foundation models make for an ideal setting to address abstract concept understanding in videos. Automated understanding of high-level abstract concepts is imperative as it enables models to be more aligned with human reasoning and values. In this survey, we study different tasks and datasets used to understand abstract concepts in video content. We observe that, periodically and over a long period, researchers have attempted to solve these tasks, making the best use of the tools available at their disposal. We advocate that drawing on decades of community experience will help us shed light on this important open grand challenge and avoid ``re-inventing the wheel'' as we start revisiting it in the era of multi-modal foundation models.</li>
</ul>

<h3>Title: Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Christoforos N. Spartalis, Theodoros Semertzidis, Petros Daras, Efstratios Gavves</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20773">https://arxiv.org/abs/2508.20773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20773">https://arxiv.org/pdf/2508.20773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20773]] Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI(https://arxiv.org/abs/2508.20773)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce SAFEMax, a novel method for Machine Unlearning in diffusion models. Grounded in information-theoretic principles, SAFEMax maximizes the entropy in generated images, causing the model to generate Gaussian noise when conditioned on impermissible classes by ultimately halting its denoising process. Also, our method controls the balance between forgetting and retention by selectively focusing on the early diffusion steps, where class-specific information is prominent. Our results demonstrate the effectiveness of SAFEMax and highlight its substantial efficiency gains over state-of-the-art methods.</li>
</ul>

<h3>Title: Evaluating Compositional Generalisation in VLMs and Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Beth Pearson, Bilal Boulbarss, Michael Wray, Martha Lewis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20783">https://arxiv.org/abs/2508.20783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20783">https://arxiv.org/pdf/2508.20783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20783]] Evaluating Compositional Generalisation in VLMs and Diffusion Models(https://arxiv.org/abs/2508.20783)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>A fundamental aspect of the semantics of natural language is that novel meanings can be formed from the composition of previously known parts. Vision-language models (VLMs) have made significant progress in recent years, however, there is evidence that they are unable to perform this kind of composition. For example, given an image of a red cube and a blue cylinder, a VLM such as CLIP is likely to incorrectly label the image as a red cylinder or a blue cube, indicating it represents the image as a `bag-of-words' and fails to capture compositional semantics. Diffusion models have recently gained significant attention for their impressive generative abilities, and zero-shot classifiers based on diffusion models have been shown to perform competitively with CLIP in certain compositional tasks. In this work we explore whether the generative Diffusion Classifier has improved compositional generalisation abilities compared to discriminative models. We assess three models -- Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with attributes and relations in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Our results show that the Diffusion Classifier and ViLT perform well at concept binding tasks, but that all models struggle significantly with the relational GZSL task, underscoring the broader challenges VLMs face with relational reasoning. Analysis of CLIP embeddings suggests that the difficulty may stem from overly similar representations of relational concepts such as left and right. Code and dataset are available at: this https URL</li>
</ul>

<h3>Title: Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training</h3>
<ul>
<li><strong>Authors: </strong>Tao Luo, Han Wu, Tong Yang, Dinggang Shen, Zhiming Cui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20813">https://arxiv.org/abs/2508.20813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20813">https://arxiv.org/pdf/2508.20813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20813]] Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training(https://arxiv.org/abs/2508.20813)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Accurate dental caries detection from panoramic X-rays plays a pivotal role in preventing lesion progression. However, current detection methods often yield suboptimal accuracy due to subtle contrast variations and diverse lesion morphology of dental caries. In this work, inspired by the clinical workflow where dentists systematically combine whole-image screening with detailed tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training network for accurate dental caries detection. Our DVCTNet starts with employing automated tooth detection to establish two complementary views: a global view from panoramic X-ray images and a local view from cropped tooth images. We then pretrain two vision foundation models separately on the two views. The global-view foundation model serves as the detection backbone, generating region proposals and global features, while the local-view model extracts detailed features from corresponding cropped tooth patches matched by the region proposals. To effectively integrate information from both views, we introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically fuses dual-view features, enhancing the detection pipeline by integrating the fused features back into the detection model for final caries detection. To rigorously evaluate our DVCTNet, we test it on a public dataset and further validate its performance on a newly curated, high-precision dental caries detection dataset, annotated using both intra-oral images and panoramic X-rays for double verification. Experimental results demonstrate DVCTNet's superior performance against existing state-of-the-art (SOTA) methods on both datasets, indicating the clinical applicability of our method. Our code and labeled dataset are available at this https URL.</li>
</ul>

<h3>Title: GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yang Gao, Dongjie Wang, Scott Piersall, Ye Zhang, Liqiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20824">https://arxiv.org/abs/2508.20824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20824">https://arxiv.org/pdf/2508.20824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20824]] GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement(https://arxiv.org/abs/2508.20824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Feature transformation plays a critical role in enhancing machine learning model performance by optimizing data representations. Recent state-of-the-art approaches address this task as a continuous embedding optimization problem, converting discrete search into a learnable process. Although effective, these methods often rely on sequential encoder-decoder structures that cause high computational costs and parameter requirements, limiting scalability and efficiency. To address these limitations, we propose a novel framework that accomplishes automated feature transformation through four steps: transformation records collection, embedding space construction with a revised Generative Pre-trained Transformer (GPT) model, gradient-ascent search, and autoregressive reconstruction. In our approach, the revised GPT model serves two primary functions: (a) feature transformation sequence reconstruction and (b) model performance estimation and enhancement for downstream tasks by constructing the embedding space. Such a multi-objective optimization framework reduces parameter size and accelerates transformation processes. Experimental results on benchmark datasets show that the proposed framework matches or exceeds baseline performance, with significant gains in computational efficiency. This work highlights the potential of transformer-based architectures for scalable, high-performance automated feature transformation.</li>
</ul>

<h3>Title: ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks</h3>
<ul>
<li><strong>Authors: </strong>Zeyue Zhang, Lin Song, Erkang Bao, Xiaoling Lv, Xinyue Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20829">https://arxiv.org/abs/2508.20829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20829">https://arxiv.org/pdf/2508.20829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20829]] ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks(https://arxiv.org/abs/2508.20829)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Financial fraud detection is essential to safeguard billions of dollars, yet the intertwined entities and fast-changing transaction behaviors in modern financial systems routinely defeat conventional machine learning models. Recent graph-based detectors make headway by representing transactions as networks, but they still overlook two fraud hallmarks rooted in time: (1) temporal motifs--recurring, telltale subgraphs that reveal suspicious money flows as they unfold--and (2) account-specific intervals of anomalous activity, when fraud surfaces only in short bursts unique to each entity. To exploit both signals, we introduce ATM-GAD, an adaptive graph neural network that leverages temporal motifs for financial anomaly detection. A Temporal Motif Extractor condenses each account's transaction history into the most informative motifs, preserving both topology and temporal patterns. These motifs are then analyzed by dual-attention blocks: IntraA reasons over interactions within a single motif, while InterA aggregates evidence across motifs to expose multi-step fraud schemes. In parallel, a differentiable Adaptive Time-Window Learner tailors the observation window for every node, allowing the model to focus precisely on the most revealing time slices. Experiments on four real-world datasets show that ATM-GAD consistently outperforms seven strong anomaly-detection baselines, uncovering fraud patterns missed by earlier methods.</li>
</ul>

<h3>Title: Understanding and evaluating computer vision models through the lens of counterfactuals</h3>
<ul>
<li><strong>Authors: </strong>Pushkar Shukla</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20881">https://arxiv.org/abs/2508.20881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20881">https://arxiv.org/pdf/2508.20881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20881]] Understanding and evaluating computer vision models through the lens of counterfactuals(https://arxiv.org/abs/2508.20881)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Counterfactual reasoning -- the practice of asking ``what if'' by varying inputs and observing changes in model behavior -- has become central to interpretable and fair AI. This thesis develops frameworks that use counterfactuals to explain, audit, and mitigate bias in vision classifiers and generative models. By systematically altering semantically meaningful attributes while holding others fixed, these methods uncover spurious correlations, probe causal dependencies, and help build more robust systems. The first part addresses vision classifiers. CAVLI integrates attribution (LIME) with concept-level analysis (TCAV) to quantify how strongly decisions rely on human-interpretable concepts. With localized heatmaps and a Concept Dependency Score, CAVLI shows when models depend on irrelevant cues like backgrounds. Extending this, ASAC introduces adversarial counterfactuals that perturb protected attributes while preserving semantics. Through curriculum learning, ASAC fine-tunes biased models for improved fairness and accuracy while avoiding stereotype-laden artifacts. The second part targets generative Text-to-Image (TTI) models. TIBET provides a scalable pipeline for evaluating prompt-sensitive biases by varying identity-related terms, enabling causal auditing of how race, gender, and age affect image generation. To capture interactions, BiasConnect builds causal graphs diagnosing intersectional biases. Finally, InterMit offers a modular, training-free algorithm that mitigates intersectional bias via causal sensitivity scores and user-defined fairness goals. Together, these contributions show counterfactuals as a unifying lens for interpretability, fairness, and causality in both discriminative and generative models, establishing principled, scalable methods for socially responsible bias evaluation and mitigation.</li>
</ul>

<h3>Title: Turning Tabular Foundation Models into Graph Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Eremeev, Gleb Bazhenov, Oleg Platonov, Artem Babenko, Liudmila Prokhorenkova</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20906">https://arxiv.org/abs/2508.20906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20906">https://arxiv.org/pdf/2508.20906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20906]] Turning Tabular Foundation Models into Graph Foundation Models(https://arxiv.org/abs/2508.20906)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>While foundation models have revolutionized such fields as natural language processing and computer vision, their application and potential within graph machine learning remain largely unexplored. One of the key challenges in designing graph foundation models (GFMs) is handling diverse node features that can vary across different graph datasets. Although many works on GFMs have been focused exclusively on text-attributed graphs, the problem of handling arbitrary features of other types in GFMs has not been fully addressed. However, this problem is not unique to the graph domain, as it also arises in the field of machine learning for tabular data. In this work, motivated by the recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a simple graph foundation model that employs TabPFNv2 as a backbone. Specifically, G2T-FM augments the original node features with neighborhood feature aggregation, adds structural embeddings, and then applies TabPFNv2 to the constructed node representations. Even in a fully in-context regime, our model achieves strong results, significantly outperforming publicly available GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover, after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the potential of the proposed approach. More broadly, our paper reveals a previously overlooked direction of utilizing tabular foundation models for graph machine learning tasks.</li>
</ul>

<h3>Title: Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20909">https://arxiv.org/abs/2508.20909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20909">https://arxiv.org/pdf/2508.20909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20909]] Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation(https://arxiv.org/abs/2508.20909)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models pre-trained on large-scale natural image datasets offer a powerful paradigm for medical image segmentation. However, effectively transferring their learned representations for precise clinical applications remains a challenge. In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. Our architecture introduces an encoder built upon a frozen DINOv3 backbone, which employs a specialized adapter to fuse the model's rich semantic features with low-level spatial details. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. We conducted extensive experiments on seven diverse public medical image segmentation datasets. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Our framework proves to be highly scalable, with segmentation accuracy consistently improving as the backbone model size increases up to the 7-billion-parameter variant. The findings demonstrate that leveraging the superior, dense-pretrained features from a general-purpose foundation model provides a highly effective and parameter-efficient approach to advance the accuracy of medical image segmentation. The code is available at this https URL.</li>
</ul>

<h3>Title: STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment</h3>
<ul>
<li><strong>Authors: </strong>Jiaqian Li, Qisheng Hu, Jing Li, Wenya Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20944">https://arxiv.org/abs/2508.20944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20944">https://arxiv.org/pdf/2508.20944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20944]] STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment(https://arxiv.org/abs/2508.20944)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to perform a wide range of tasks without task-specific fine-tuning. However, the effectiveness of ICL heavily depends on the quality of exemplar selection. In particular, for structured prediction tasks such as semantic parsing, existing ICL selection strategies often overlook structural alignment, leading to suboptimal performance and poor generalization. To address this issue, we propose a novel two-stage exemplar selection strategy that achieves a strong balance between efficiency, generalizability, and performance. First, we fine-tune a BERT-based retriever using structure-aware supervision, guiding it to select exemplars that are both semantically relevant and structurally aligned. Then, we enhance the retriever with a plug-in module, which amplifies syntactically meaningful information in the hidden representations. This plug-in is model-agnostic, requires minimal overhead, and can be seamlessly integrated into existing pipelines. Experiments on four benchmarks spanning three semantic parsing tasks demonstrate that our method consistently outperforms existing baselines with multiple recent LLMs as inference-time models.</li>
</ul>

<h3>Title: Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement</h3>
<ul>
<li><strong>Authors: </strong>Amir Jmal, Chaima Chtourou, Mahdi Louati, Abdelaziz Kallel, Houda Khmila</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20954">https://arxiv.org/abs/2508.20954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20954">https://arxiv.org/pdf/2508.20954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20954]] Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement(https://arxiv.org/abs/2508.20954)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In the context of proven climate change, maintaining olive biodiversity through early anomaly detection and treatment using remote sensing technology is crucial, offering effective management solutions. This paper presents an innovative approach to olive tree segmentation from satellite images. By leveraging foundational models and advanced segmentation techniques, the study integrates the Segment Anything Model (SAM) to accurately identify and segment olive trees in agricultural plots. The methodology includes SAM segmentation and corrections based on trees alignement in the field and a learanble constraint about the shape and the size. Our approach achieved a 98\% accuracy rate, significantly surpassing the initial SAM performance of 82\%.</li>
</ul>

<h3>Title: ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts</h3>
<ul>
<li><strong>Authors: </strong>Patryk BÄdkowski, Jan DubiÅski, Filip Szatkowski, Kamil Deja, PrzemysÅaw Rokita, Tomasz TrzciÅski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.20991">https://arxiv.org/abs/2508.20991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.20991">https://arxiv.org/pdf/2508.20991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.20991]] ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts(https://arxiv.org/abs/2508.20991)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at this https URL.</li>
</ul>

<h3>Title: Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees</h3>
<ul>
<li><strong>Authors: </strong>Yaniv Hassidof, Tom Jurgenson, Kiril Solovey</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21001">https://arxiv.org/abs/2508.21001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21001">https://arxiv.org/pdf/2508.21001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21001]] Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees(https://arxiv.org/abs/2508.21001)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a \emph{provably-generalizable} framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs to yield \emph{provably-safe} solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a \emph{single environment}. In comprehensive evaluations on OOD scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than classical SBPs), while improving the average success rate over DP and SBPs. DiTree is on average 3x faster than classical SBPs, and outperforms all other approaches by achieving roughly 30\% higher success rate. Project webpage: this https URL.</li>
</ul>

<h3>Title: InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity</h3>
<ul>
<li><strong>Authors: </strong>Souradeep Nanda, Anay Majee, Rishabh Iyer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21003">https://arxiv.org/abs/2508.21003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21003">https://arxiv.org/pdf/2508.21003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21003]] InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity(https://arxiv.org/abs/2508.21003)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce InSQuAD, designed to enhance the performance of In-Context Learning (ICL) models through Submodular Mutual Information} (SMI) enforcing Quality and Diversity among in-context exemplars. InSQuAD achieves this through two principal strategies: First, we model the ICL task as a targeted selection problem and introduce a unified selection strategy based on SMIs which mines relevant yet diverse in-context examples encapsulating the notions of quality and diversity. Secondly, we address a common pitfall in existing retrieval models which model query relevance, often overlooking diversity, critical for ICL. InSQuAD introduces a combinatorial training paradigm which learns the parameters of an SMI function to enforce both quality and diversity in the retrieval model through a novel likelihood-based loss. To further aid the learning process we augment an existing multi-hop question answering dataset with synthetically generated paraphrases. Adopting the retrieval model trained using this strategy alongside the novel targeted selection formulation for ICL on nine benchmark datasets shows significant improvements validating the efficacy of our approach.</li>
</ul>

<h3>Title: Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance</h3>
<ul>
<li><strong>Authors: </strong>Luozhijie Jin, Zijie Qiu, Jie Liu, Zijie Diao, Lifeng Qiao, Ning Ding, Alex Lamb, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21016">https://arxiv.org/abs/2508.21016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21016">https://arxiv.org/pdf/2508.21016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21016]] Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance(https://arxiv.org/abs/2508.21016)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Denoising-based generative models, particularly diffusion and flow matching algorithms, have achieved remarkable success. However, aligning their output distributions with complex downstream objectives, such as human preferences, compositional accuracy, or data compressibility, remains challenging. While reinforcement learning (RL) fine-tuning methods, inspired by advances in RL from human feedback (RLHF) for large language models, have been adapted to these generative frameworks, current RL approaches are suboptimal for diffusion models and offer limited flexibility in controlling alignment strength after fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models through the lens of stochastic differential equations and implicit reward conditioning. We introduce Reinforcement Learning Guidance (RLG), an inference-time method that adapts Classifier-Free Guidance (CFG) by combining the outputs of the base and RL fine-tuned models via a geometric average. Our theoretical analysis shows that RLG's guidance scale is mathematically equivalent to adjusting the KL-regularization coefficient in standard RL objectives, enabling dynamic control over the alignment-quality trade-off without further training. Extensive experiments demonstrate that RLG consistently improves the performance of RL fine-tuned models across various architectures, RL algorithms, and downstream tasks, including human preferences, compositional control, compressibility, and text rendering. Furthermore, RLG supports both interpolation and extrapolation, thereby offering unprecedented flexibility in controlling generative alignment. Our approach provides a practical and theoretically sound solution for enhancing and controlling diffusion model alignment at inference. The source code for RLG is publicly available at the Github: this https URL.</li>
</ul>

<h3>Title: POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Cheng, Bing Ma, Xuhua Ren, Hongyi Jin, Kai Yu, Peng Zhang, Wenyue Li, Yuan Zhou, Tianxiang Zheng, Qinglin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21019">https://arxiv.org/abs/2508.21019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21019">https://arxiv.org/pdf/2508.21019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21019]] POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models(https://arxiv.org/abs/2508.21019)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The field of video diffusion generation faces critical bottlenecks in sampling efficiency, especially for large-scale models and long sequences. Existing video acceleration methods adopt image-based techniques but suffer from fundamental limitations: they neither model the temporal coherence of video frames nor provide single-step distillation for large-scale video models. To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a distillation framework that reduces the sampling steps of large-scale video diffusion models, enabling the generation of high-quality videos in a single step. POSE employs a carefully designed two-phase process to distill video models:(i) stability priming: a warm-up mechanism to stabilize adversarial distillation that adapts the high-quality trajectory of the one-step generator from high to low signal-to-noise ratio regimes, optimizing the video quality of single-step mappings near the endpoints of flow trajectories. (ii) unified adversarial equilibrium: a flexible self-adversarial distillation mechanism that promotes stable single-step adversarial training towards a Nash equilibrium within the Gaussian noise space, generating realistic single-step videos close to real videos. For conditional video generation, we propose (iii) conditional adversarial consistency, a method to improve both semantic consistency and frame consistency between conditional frames and generated frames. Comprehensive experiments demonstrate that POSE outperforms other acceleration methods on VBench-I2V by average 7.15% in semantic alignment, temporal conference and frame quality, reducing the latency of the pre-trained model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining competitive performance.</li>
</ul>

<h3>Title: Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets</h3>
<ul>
<li><strong>Authors: </strong>Dale Decatur, Thibault Groueix, Wang Yifan, Rana Hanocka, Vladimir Kim, Matheus Gadelha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21032">https://arxiv.org/abs/2508.21032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21032">https://arxiv.org/pdf/2508.21032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21032]] Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets(https://arxiv.org/abs/2508.21032)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models enable high-quality image generation but are computationally expensive. While prior work optimizes per-inference efficiency, we explore an orthogonal approach: reducing redundancy across correlated prompts. Our method leverages the coarse-to-fine nature of diffusion models, where early denoising steps capture shared structures among similar prompts. We propose a training-free approach that clusters prompts based on semantic similarity and shares computation in early diffusion steps. Experiments show that for models trained conditioned on image embeddings, our approach significantly reduces compute cost while improving image quality. By leveraging UnClip's text-to-image prior, we enhance diffusion step allocation for greater efficiency. Our method seamlessly integrates with existing pipelines, scales with prompt sets, and reduces the environmental and financial burden of large-scale text-to-image generation. Project page: this https URL</li>
</ul>

<h3>Title: OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, Xinglong Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21066">https://arxiv.org/abs/2508.21066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21066">https://arxiv.org/pdf/2508.21066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21066]] OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning(https://arxiv.org/abs/2508.21066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only \textit{One Reward} model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: this https URL</li>
</ul>

<h3>Title: Dress&Dance: Dress up and Dance as You Like It - Technical Preview</h3>
<ul>
<li><strong>Authors: </strong>Jun-Kun Chen, Aayush Bansal, Minh Phuoc Vo, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21070">https://arxiv.org/abs/2508.21070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21070">https://arxiv.org/pdf/2508.21070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21070]] Dress&Dance: Dress up and Dance as You Like It - Technical Preview(https://arxiv.org/abs/2508.21070)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present Dress&Dance, a video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a user wearing desired garments while moving in accordance with a given reference video. Our approach requires a single user image and supports a range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in a single pass. Key to our framework is CondNet, a novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and a larger, more readily available image dataset, in a multistage progressive manner. Dress&Dance outperforms existing open source and commercial solutions and enables a high quality and flexible try-on experience.</li>
</ul>

<h3>Title: First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge</h3>
<ul>
<li><strong>Authors: </strong>Fahad Shamshad, Tameem Bakr, Yahia Shaaban, Noor Hussein, Karthik Nandakumar, Nils Lukas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2508.21072">https://arxiv.org/abs/2508.21072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2508.21072">https://arxiv.org/pdf/2508.21072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2508.21072]] First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge(https://arxiv.org/abs/2508.21072)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
