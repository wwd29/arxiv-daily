<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture. (arXiv:2311.10123v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10123">http://arxiv.org/abs/2311.10123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10123]] MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture(http://arxiv.org/abs/2311.10123)</code></li>
<li>Summary: <p>Generative models for 3D object synthesis have seen significant advancements
with the incorporation of prior knowledge distilled from 2D diffusion models.
Nevertheless, challenges persist in the form of multi-view geometric
inconsistencies and slow generation speeds within the existing 3D synthesis
frameworks. This can be attributed to two factors: firstly, the deficiency of
abundant geometric a priori knowledge in optimization, and secondly, the
entanglement issue between geometry and texture in conventional 3D generation
methods.In response, we introduce MetaDreammer, a two-stage optimization
approach that leverages rich 2D and 3D prior knowledge. In the first stage, our
emphasis is on optimizing the geometric representation to ensure multi-view
consistency and accuracy of 3D objects. In the second stage, we concentrate on
fine-tuning the geometry and optimizing the texture, thereby achieving a more
refined 3D object. Through leveraging 2D and 3D prior knowledge in two stages,
respectively, we effectively mitigate the interdependence between geometry and
texture. MetaDreamer establishes clear optimization objectives for each stage,
resulting in significant time savings in the 3D generation process. Ultimately,
MetaDreamer can generate high-quality 3D objects based on textual prompts
within 20 minutes, and to the best of our knowledge, it is the most efficient
text-to-3D generation method. Furthermore, we introduce image control into the
process, enhancing the controllability of 3D generation. Extensive empirical
evidence confirms that our method is not only highly efficient but also
achieves a quality level that is at the forefront of current state-of-the-art
3D generation techniques.
</p></li>
</ul>

<h3>Title: High-fidelity Person-centric Subject-to-Image Synthesis. (arXiv:2311.10329v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10329">http://arxiv.org/abs/2311.10329</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10329]] High-fidelity Person-centric Subject-to-Image Synthesis(http://arxiv.org/abs/2311.10329)</code></li>
<li>Summary: <p>Current subject-driven image generation methods encounter significant
challenges in person-centric image generation. The reason is that they learn
the semantic scene and person generation by fine-tuning a common pre-trained
diffusion, which involves an irreconcilable training imbalance. Precisely, to
generate realistic persons, they need to sufficiently tune the pre-trained
model, which inevitably causes the model to forget the rich semantic scene
prior and makes scene generation over-fit to the training data. Moreover, even
with sufficient fine-tuning, these methods can still not generate high-fidelity
persons since joint learning of the scene and person generation also lead to
quality compromise. In this paper, we propose Face-diffuser, an effective
collaborative generation pipeline to eliminate the above training imbalance and
quality compromise. Specifically, we first develop two specialized pre-trained
diffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented
Diffusion Model (SDM), for scene and person generation, respectively. The
sampling process is divided into three sequential stages, i.e., semantic scene
construction, subject-scene fusion, and subject enhancement. The first and last
stages are performed by TDM and SDM respectively. The subject-scene fusion
stage, that is the collaboration achieved through a novel and highly effective
mechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on
our key observation that there exists a robust link between classifier-free
guidance responses and the saliency of generated images. In each time step, SNF
leverages the unique strengths of each model and allows for the spatial
blending of predicted noises from both models automatically in a saliency-aware
manner. Extensive experiments confirm the impressive effectiveness and
robustness of the Face-diffuser.
</p></li>
</ul>

<h3>Title: Enhancing Object Coherence in Layout-to-Image Synthesis. (arXiv:2311.10522v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10522">http://arxiv.org/abs/2311.10522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10522]] Enhancing Object Coherence in Layout-to-Image Synthesis(http://arxiv.org/abs/2311.10522)</code></li>
<li>Summary: <p>Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel's generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
</p></li>
</ul>

<h3>Title: Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. (arXiv:2311.10709v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10709">http://arxiv.org/abs/2311.10709</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10709]] Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning(http://arxiv.org/abs/2311.10709)</code></li>
<li>Summary: <p>We present Emu Video, a text-to-video generation model that factorizes the
generation into two steps: first generating an image conditioned on the text,
and then generating a video conditioned on the text and the generated image. We
identify critical design decisions--adjusted noise schedules for diffusion, and
multi-stage training--that enable us to directly generate high quality and high
resolution videos, without requiring a deep cascade of models as in prior work.
In human evaluations, our generated videos are strongly preferred in quality
compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's
PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial
solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing
approach naturally lends itself to animating images based on a user's text
prompt, where our generations are preferred 96% over prior work.
</p></li>
</ul>

<h3>Title: Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers. (arXiv:2311.10242v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10242">http://arxiv.org/abs/2311.10242</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10242]] Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers(http://arxiv.org/abs/2311.10242)</code></li>
<li>Summary: <p>The launch of ChatGPT has garnered global attention, marking a significant
milestone in the field of Generative Artificial Intelligence. While Generative
AI has been in effect for the past decade, the introduction of ChatGPT has
ignited a new wave of research and innovation in the AI domain. This surge in
interest has led to the development and release of numerous cutting-edge tools,
such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,
among others. These tools exhibit remarkable capabilities, encompassing tasks
ranging from text generation and music composition, image creation, video
production, code generation, and even scientific work. They are built upon
various state-of-the-art models, including Stable Diffusion, transformer models
like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial
networks. This advancement in Generative AI presents a wealth of exciting
opportunities and, simultaneously, unprecedented challenges. Throughout this
paper, we have explored these state-of-the-art models, the diverse array of
tasks they can accomplish, the challenges they pose, and the promising future
of Generative Artificial Intelligence.
</p></li>
</ul>

<h3>Title: Multiscale Hodge Scattering Networks for Data Analysis. (arXiv:2311.10270v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10270">http://arxiv.org/abs/2311.10270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10270]] Multiscale Hodge Scattering Networks for Data Analysis(http://arxiv.org/abs/2311.10270)</code></li>
<li>Summary: <p>We propose new scattering networks for signals measured on simplicial
complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs).
Our construction is based on multiscale basis dictionaries on simplicial
complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently
developed for simplices of dimension $\kappa \in \N$ in a given simplicial
complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT)
and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and
the $\kk$-HGLET both form redundant sets (i.e., dictionaries) of multiscale
basis vectors and the corresponding expansion coefficients of a given signal.
Our MHSNs use a layered structure analogous to a convolutional neural network
(CNN) to cascade the moments of the modulus of the dictionary coefficients. The
resulting features are invariant to reordering of the simplices (i.e., node
permutation of the underlying graphs). Importantly, the use of multiscale basis
dictionaries in our MHSNs admits a natural pooling operation that is akin to
local pooling in CNNs, and which may be performed either locally or per-scale.
These pooling operations are harder to define in both traditional scattering
networks based on Morlet wavelets, and geometric scattering networks based on
Diffusion Wavelets. As a result, we are able to extract a rich set of
descriptive yet robust features that can be used along with very simple machine
learning methods (i.e., logistic regression or support vector machines) to
achieve high-accuracy classification systems with far fewer parameters to train
than most modern graph neural networks. Finally, we demonstrate the usefulness
of our MHSNs in three distinct types of problems: signal classification, domain
(i.e., graph/simplex) classification, and molecular dynamics prediction.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification. (arXiv:2311.10319v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10319">http://arxiv.org/abs/2311.10319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10319]] Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification(http://arxiv.org/abs/2311.10319)</code></li>
<li>Summary: <p>Advancements in clinical treatment and research are limited by supervised
learning techniques that rely on large amounts of annotated data, an expensive
task requiring many hours of clinical specialists' time. In this paper, we
propose using self-supervised and semi-supervised learning. These techniques
perform an auxiliary task that is label-free, scaling up machine-supervision is
easier compared with fully-supervised techniques. This paper proposes S4MI
(Self-Supervision and Semi-Supervision for Medical Imaging), our pipeline to
leverage advances in self and semi-supervision learning. We benchmark them on
three medical imaging datasets to analyze their efficacy for classification and
segmentation. This advancement in self-supervised learning with 10% annotation
performed better than 100% annotation for the classification of most datasets.
The semi-supervised approach yielded favorable outcomes for segmentation,
outperforming the fully-supervised approach by using 50% fewer labels in all
three datasets.
</p></li>
</ul>

<h3>Title: Self-trained Panoptic Segmentation. (arXiv:2311.10648v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10648">http://arxiv.org/abs/2311.10648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10648]] Self-trained Panoptic Segmentation(http://arxiv.org/abs/2311.10648)</code></li>
<li>Summary: <p>Panoptic segmentation is an important computer vision task which combines
semantic and instance segmentation. It plays a crucial role in domains of
medical image analysis, self-driving vehicles, and robotics by providing a
comprehensive understanding of visual environments. Traditionally, deep
learning panoptic segmentation models have relied on dense and accurately
annotated training data, which is expensive and time consuming to obtain.
Recent advancements in self-supervised learning approaches have shown great
potential in leveraging synthetic and unlabelled data to generate pseudo-labels
using self-training to improve the performance of instance and semantic
segmentation models. The three available methods for self-supervised panoptic
segmentation use proposal-based transformer architectures which are
computationally expensive, complicated and engineered for specific tasks. The
aim of this work is to develop a framework to perform embedding-based
self-supervised panoptic segmentation using self-training in a
synthetic-to-real domain adaptation problem setting.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework. (arXiv:2311.10125v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10125">http://arxiv.org/abs/2311.10125</a></li>
<li>Code URL: https://github.com/lhbuilder/sa-segment-anything</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10125]] UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework(http://arxiv.org/abs/2311.10125)</code></li>
<li>Summary: <p>In the current landscape of artificial intelligence, foundation models serve
as the bedrock for advancements in both language and vision domains. OpenAI
GPT-4 has emerged as the pinnacle in large language models (LLMs), while the
computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models
such as Meta's SAM and DINO, and YOLOS. However, the financial and
computational burdens of training new models from scratch remain a significant
barrier to progress. In response to this challenge, we introduce
UnifiedVisionGPT, a novel framework designed to consolidate and automate the
integration of SOTA vision models, thereby facilitating the development of
vision-oriented AI. UnifiedVisionGPT distinguishes itself through four key
features: (1) provides a versatile multimodal framework adaptable to a wide
range of applications, building upon the strengths of multimodal foundation
models; (2) seamlessly integrates various SOTA vision models to create a
comprehensive multimodal platform, capitalizing on the best components of each
model; (3) prioritizes vision-oriented AI, ensuring a more rapid progression in
the CV domain compared to the current trajectory of LLMs; and (4) introduces
automation in the selection of SOTA vision models, generating optimal results
based on diverse multimodal inputs such as text prompts and images. This paper
outlines the architecture and capabilities of UnifiedVisionGPT, demonstrating
its potential to revolutionize the field of computer vision through enhanced
efficiency, versatility, generalization, and performance. Our implementation,
along with the unified multimodal framework and comprehensive dataset, is made
publicly available at https://github.com/LHBuilder/SA-Segment-Anything.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: VideoCon: Robust Video-Language Alignment via Contrast Captions. (arXiv:2311.10111v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10111">http://arxiv.org/abs/2311.10111</a></li>
<li>Code URL: https://github.com/hritikbansal/videocon</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10111]] VideoCon: Robust Video-Language Alignment via Contrast Captions(http://arxiv.org/abs/2311.10111)</code></li>
<li>Summary: <p>Despite being (pre)trained on a massive amount of data, state-of-the-art
video-language alignment models are not robust to semantically-plausible
contrastive changes in the video captions. Our work addresses this by
identifying a broad spectrum of contrast misalignments, such as replacing
entities, actions, and flipping event order, which alignment models should be
robust against. To this end, we introduce the VideoCon, a video-language
alignment dataset constructed by a large language model that generates
plausible contrast video captions and explanations for differences between
original and contrast video captions. Then, a generative video-language model
is finetuned with VideoCon to assess video-language entailment and generate
explanations. Our VideoCon-based alignment model significantly outperforms
current models. It exhibits a 12-point increase in AUC for the video-language
alignment task on human-generated contrast captions. Finally, our model sets
new state of the art zero-shot performance in temporally-extensive
video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video
question answering (ATP-Hard). Moreover, our model shows superior performance
on novel videos and human-crafted captions and explanations. Our code and data
are available at https://github.com/Hritikbansal/videocon.
</p></li>
</ul>

<h3>Title: SelfEval: Leveraging the discriminative nature of generative models for evaluation. (arXiv:2311.10708v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10708">http://arxiv.org/abs/2311.10708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10708]] SelfEval: Leveraging the discriminative nature of generative models for evaluation(http://arxiv.org/abs/2311.10708)</code></li>
<li>Summary: <p>In this work, we show that text-to-image generative models can be 'inverted'
to assess their own text-image understanding capabilities in a completely
automated manner.
</p>
<p>Our method, called SelfEval, uses the generative model to compute the
likelihood of real images given text prompts, making the generative model
directly applicable to discriminative tasks.
</p>
<p>Using SelfEval, we repurpose standard datasets created for evaluating
multimodal text-image discriminative models to evaluate generative models in a
fine-grained manner: assessing their performance on attribute binding, color
recognition, counting, shape recognition, spatial understanding.
</p>
<p>To the best of our knowledge SelfEval is the first automated metric to show a
high degree of agreement for measuring text-faithfulness with the gold-standard
human evaluations across multiple models and benchmarks.
</p>
<p>Moreover, SelfEval enables us to evaluate generative models on challenging
tasks such as Winoground image-score where they demonstrate competitive
performance to discriminative models.
</p>
<p>We also show severe drawbacks of standard automated metrics such as
CLIP-score to measure text faithfulness on benchmarks such as DrawBench, and
how SelfEval sidesteps these issues.
</p>
<p>We hope SelfEval enables easy and reliable automated evaluation for diffusion
models.
</p></li>
</ul>

<h3>Title: Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2. (arXiv:2311.10266v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10266">http://arxiv.org/abs/2311.10266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10266]] Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2(http://arxiv.org/abs/2311.10266)</code></li>
<li>Summary: <p>The training of large language models (LLMs) on extensive, unfiltered corpora
sourced from the internet is a common and advantageous practice. Consequently,
LLMs have learned and inadvertently reproduced various types of biases,
including violent, offensive, and toxic language. However, recent research
shows that generative pretrained transformer (GPT) language models can
recognize their own biases and detect toxicity in generated content, a process
referred to as self-diagnosis. In response, researchers have developed a
decoding algorithm that allows LLMs to self-debias, or reduce their likelihood
of generating harmful text. This study investigates the efficacy of the
diagnosing-debiasing approach in mitigating two additional types of biases:
insults and political bias. These biases are often used interchangeably in
discourse, despite exhibiting potentially dissimilar semantic and syntactic
properties. We aim to contribute to the ongoing effort of investigating the
ethical and social implications of human-AI interaction.
</p></li>
</ul>

<h3>Title: Supervised structure learning. (arXiv:2311.10300v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10300">http://arxiv.org/abs/2311.10300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10300]] Supervised structure learning(http://arxiv.org/abs/2311.10300)</code></li>
<li>Summary: <p>This paper concerns structure learning or discovery of discrete generative
models. It focuses on Bayesian model selection and the assimilation of training
data or content, with a special emphasis on the order in which data are
ingested. A key move - in the ensuing schemes - is to place priors on the
selection of models, based upon expected free energy. In this setting, expected
free energy reduces to a constrained mutual information, where the constraints
inherit from priors over outcomes (i.e., preferred outcomes). The resulting
scheme is first used to perform image classification on the MNIST dataset to
illustrate the basic idea, and then tested on a more challenging problem of
discovering models with dynamics, using a simple sprite-based visual
disentanglement paradigm and the Tower of Hanoi (cf., blocks world) problem. In
these examples, generative models are constructed autodidactically to recover
(i.e., disentangle) the factorial structure of latent states - and their
characteristic paths or dynamics.
</p></li>
</ul>

<h3>Title: Implicit Maximum a Posteriori Filtering via Adaptive Optimization. (arXiv:2311.10580v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10580">http://arxiv.org/abs/2311.10580</a></li>
<li>Code URL: https://github.com/gianlucabencomo/implicitmap</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10580]] Implicit Maximum a Posteriori Filtering via Adaptive Optimization(http://arxiv.org/abs/2311.10580)</code></li>
<li>Summary: <p>Bayesian filtering approximates the true underlying behavior of a
time-varying system by inverting an explicit generative model to convert noisy
measurements into state estimates. This process typically requires either
storage, inversion, and multiplication of large matrices or Monte Carlo
estimation, neither of which are practical in high-dimensional state spaces
such as the weight spaces of artificial neural networks. Here, we frame the
standard Bayesian filtering problem as optimization over a time-varying
objective. Instead of maintaining matrices for the filtering equations or
simulating particles, we specify an optimizer that defines the Bayesian filter
implicitly. In the linear-Gaussian setting, we show that every Kalman filter
has an equivalent formulation using K steps of gradient descent. In the
nonlinear setting, our experiments demonstrate that our framework results in
filters that are effective, robust, and scalable to high-dimensional systems,
comparing well against the standard toolbox of Bayesian filtering solutions. We
suggest that it is easier to fine-tune an optimizer than it is to specify the
correct filtering equations, making our framework an attractive option for
high-dimensional filtering problems.
</p></li>
</ul>

<h3>Title: Concept-free Causal Disentanglement with Variational Graph Auto-Encoder. (arXiv:2311.10638v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10638">http://arxiv.org/abs/2311.10638</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10638]] Concept-free Causal Disentanglement with Variational Graph Auto-Encoder(http://arxiv.org/abs/2311.10638)</code></li>
<li>Summary: <p>In disentangled representation learning, the goal is to achieve a compact
representation that consists of all interpretable generative factors in the
observational data. Learning disentangled representations for graphs becomes
increasingly important as graph data rapidly grows. Existing approaches often
rely on Variational Auto-Encoder (VAE) or its causal structure learning-based
refinement, which suffer from sub-optimality in VAEs due to the independence
factor assumption and unavailability of concept labels, respectively. In this
paper, we propose an unsupervised solution, dubbed concept-free causal
disentanglement, built on a theoretically provable tight upper bound
approximating the optimal factor. This results in an SCM-like causal structure
modeling that directly learns concept structures from data. Based on this idea,
we propose Concept-free Causal VGAE (CCVGAE) by incorporating a novel causal
disentanglement layer into Variational Graph Auto-Encoder. Furthermore, we
prove concept consistency under our concept-free causal disentanglement
framework, hence employing it to enhance the meta-learning framework, called
concept-free causal Meta-Graph (CC-Meta-Graph). We conduct extensive
experiments to demonstrate the superiority of the proposed models: CCVGAE and
CC-Meta-Graph, reaching up to $29\%$ and $11\%$ absolute improvements over
baselines in terms of AUC, respectively.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning. (arXiv:2311.10246v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10246">http://arxiv.org/abs/2311.10246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10246]] Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning(http://arxiv.org/abs/2311.10246)</code></li>
<li>Summary: <p>Nonparametric learning is a fundamental concept in machine learning that aims
to capture complex patterns and relationships in data without making strong
assumptions about the underlying data distribution. Owing to simplicity and
familiarity, one of the most well-known algorithms under this paradigm is the
$k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine
learning in safety-critical applications, in this work, we shed new light on
the traditional nearest neighbors algorithm from the perspective of information
theory and propose a robust and interpretable framework for tasks such as
classification, regression, and anomaly detection using a single model. Instead
of using a traditional distance measure which needs to be scaled and
contextualized, we use a novel formulation of \textit{surprisal} (amount of
information required to explain the difference between the observed and
expected result). Finally, we demonstrate this architecture's capability to
perform at-par or above the state-of-the-art on classification, regression, and
anomaly detection tasks using a single model with enhanced interpretability by
providing novel concepts for characterizing data and predictions.
</p></li>
</ul>

<h3>Title: Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly Detection. (arXiv:2311.10370v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10370">http://arxiv.org/abs/2311.10370</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10370]] Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly Detection(http://arxiv.org/abs/2311.10370)</code></li>
<li>Summary: <p>Graph anomaly detection plays a crucial role in identifying exceptional
instances in graph data that deviate significantly from the majority. It has
gained substantial attention in various domains of information security,
including network intrusion, financial fraud, and malicious comments, et al.
Existing methods are primarily developed in an unsupervised manner due to the
challenge in obtaining labeled data. For lack of guidance from prior knowledge
in unsupervised manner, the identified anomalies may prove to be data noise or
individual data instances. In real-world scenarios, a limited batch of labeled
anomalies can be captured, making it crucial to investigate the few-shot
problem in graph anomaly detection. Taking advantage of this potential, we
propose a novel few-shot Graph Anomaly Detection model called FMGAD (Few-shot
Message-Enhanced Contrastive-based Graph Anomaly Detector). FMGAD leverages a
self-supervised contrastive learning strategy within and across views to
capture intrinsic and transferable structural representations. Furthermore, we
propose the Deep-GNN message-enhanced reconstruction module, which extensively
exploits the few-shot label information and enables long-range propagation to
disseminate supervision signals to deeper unlabeled nodes. This module in turn
assists in the training of self-supervised contrastive learning. Comprehensive
experimental results on six real-world datasets demonstrate that FMGAD can
achieve better performance than other state-of-the-art methods, regardless of
artificially injected anomalies or domain-organic anomalies.
</p></li>
</ul>

<h3>Title: Maintenance Techniques for Anomaly Detection AIOps Solutions. (arXiv:2311.10421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10421">http://arxiv.org/abs/2311.10421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10421]] Maintenance Techniques for Anomaly Detection AIOps Solutions(http://arxiv.org/abs/2311.10421)</code></li>
<li>Summary: <p>Anomaly detection techniques are essential in automating the monitoring of IT
systems and operations. These techniques imply that machine learning algorithms
are trained on operational data corresponding to a specific period of time and
that they are continuously evaluated on newly emerging data. Operational data
is constantly changing over time, which affects the performance of deployed
anomaly detection models. Therefore, continuous model maintenance is required
to preserve the performance of anomaly detectors over time. In this work, we
analyze two different anomaly detection model maintenance techniques in terms
of the model update frequency, namely blind model retraining and informed model
retraining. We further investigate the effects of updating the model by
retraining it on all the available data (full-history approach) and on only the
newest data (sliding window approach). Moreover, we investigate whether a data
change monitoring tool is capable of determining when the anomaly detection
model needs to be updated through retraining.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Exploring the Relationship between In-Context Learning and Instruction Tuning. (arXiv:2311.10367v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10367">http://arxiv.org/abs/2311.10367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10367]] Exploring the Relationship between In-Context Learning and Instruction Tuning(http://arxiv.org/abs/2311.10367)</code></li>
<li>Summary: <p>In-Context Learning (ICL) and Instruction Tuning (IT) are two primary
paradigms of adopting Large Language Models (LLMs) to downstream applications.
However, they are significantly different. In ICL, a set of demonstrations are
provided at inference time but the LLM's parameters are not updated. In IT, a
set of demonstrations are used to tune LLM's parameters in training time but no
demonstrations are used at inference time. Although a growing body of
literature has explored ICL and IT, studies on these topics have largely been
conducted in isolation, leading to a disconnect between these two paradigms. In
this work, we explore the relationship between ICL and IT by examining how the
hidden states of LLMs change in these two paradigms. Through carefully designed
experiments conducted with LLaMA-2 (7B and 13B), we find that ICL is implicit
IT. In other words, ICL changes an LLM's hidden states as if the demonstrations
were used to instructionally tune the model. Furthermore, the convergence
between ICL and IT is largely contingent upon several factors related to the
provided demonstrations. Overall, this work offers a unique perspective to
explore the connection between ICL and IT and sheds light on understanding the
behaviors of LLM.
</p></li>
</ul>

<h3>Title: Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks. (arXiv:2311.10609v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.10609">http://arxiv.org/abs/2311.10609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.10609]] Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks(http://arxiv.org/abs/2311.10609)</code></li>
<li>Summary: <p>Tabular classification has traditionally relied on supervised algorithms,
which estimate the parameters of a prediction model using its training data.
Recently, Prior-Data Fitted Networks (PFNs) such as TabPFN have successfully
learned to classify tabular data in-context: the model parameters are designed
to classify new samples based on labelled training samples given after the
model training. While such models show great promise, their applicability to
real-world data remains limited due to the computational scale needed. Here we
study the following question: given a pre-trained PFN for tabular data, what is
the best way to summarize the labelled training samples before feeding them to
the model? We conduct an initial investigation of sketching and
feature-selection methods for TabPFN, and note certain key differences between
it and conventionally fitted tabular models.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
