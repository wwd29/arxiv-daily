<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-15</h1>
<h3>Title: Investigating the Impact of Text Summarization on Topic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Trishia Khandelwal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09063">https://arxiv.org/abs/2410.09063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09063">https://arxiv.org/pdf/2410.09063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09063]] Investigating the Impact of Text Summarization on Topic Modeling(https://arxiv.org/abs/2410.09063)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Topic models are used to identify and group similar themes in a set of documents. Recent advancements in deep learning based neural topic models has received significant research interest. In this paper, an approach is proposed that further enhances topic modeling performance by utilizing a pre-trained large language model (LLM) to generate summaries of documents before inputting them into the topic model. Few shot prompting is used to generate summaries of different lengths to compare their impact on topic modeling. This approach is particularly effective for larger documents because it helps capture the most essential information while reducing noise and irrelevant details that could obscure the overall theme. Additionally, it is observed that datasets exhibit an optimal summary length that leads to improved topic modeling performance. The proposed method yields better topic diversity and comparable coherence values compared to previous models.</li>
<li><strong>摘要：</strong>主题模型用于识别和分组一组文档中的相似主题。基于深度学习的神经主题模型的最新进展引起了广泛的研究兴趣。本文提出了一种方法，该方法利用预先训练的大型语言模型 (LLM) 生成文档摘要，然后再将其输入主题模型，从而进一步提高主题建模性能。使用少量镜头提示来生成不同长度的摘要，以比较它们对主题建模的影响。这种方法对于较大的文档特别有效，因为它有助于捕获最重要的信息，同时减少可能掩盖整体主题的噪音和不相关的细节。此外，据观察，数据集表现出最佳摘要长度，从而提高主题建模性能。与以前的模型相比，所提出的方法产生了更好的主题多样性和可比的连贯性值。</li>
</ul>

<h3>Title: Llettuce: An Open Source Natural Language Processing Tool for the Translation of Medical Terms into Uniform Clinical Encoding</h3>
<ul>
<li><strong>Authors: </strong>James Mitchell-White, Reza Omdivar, Esmond Urwin, Karthikeyan Sivakumar, Ruizhe Li, Andy Rae, Xiaoyan Wang, Theresia Mina, John Chambers, Grazziela Figueredo, Philip R Quinlan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09076">https://arxiv.org/abs/2410.09076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09076">https://arxiv.org/pdf/2410.09076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09076]] Llettuce: An Open Source Natural Language Processing Tool for the Translation of Medical Terms into Uniform Clinical Encoding(https://arxiv.org/abs/2410.09076)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces Llettuce, an open-source tool designed to address the complexities of converting medical terms into OMOP standard concepts. Unlike existing solutions such as the Athena database search and Usagi, which struggle with semantic nuances and require substantial manual input, Llettuce leverages advanced natural language processing, including large language models and fuzzy matching, to automate and enhance the mapping process. Developed with a focus on GDPR compliance, Llettuce can be deployed locally, ensuring data protection while maintaining high performance in converting informal medical terms to standardised concepts.</li>
<li><strong>摘要：</strong>本文介绍了 Llettuce，这是一款开源工具，旨在解决将医学术语转换为 OMOP 标准概念的复杂性。与 Athena 数据库搜索和 Usagi 等现有解决方案不同，这些解决方案难以处理语义细微差别，需要大量手动输入，而 Llettuce 利用先进的自然语言处理（包括大型语言模型和模糊匹配）来自动化和增强映射过程。Llettuce 的开发重点是 GDPR 合规性，可以在本地部署，确保数据保护，同时在将非正式医学术语转换为标准化概念方面保持高性能。</li>
</ul>

<h3>Title: A Large Language Model-based Framework for Semi-Structured Tender Document Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yilong Zhao, Daifeng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09077">https://arxiv.org/abs/2410.09077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09077">https://arxiv.org/pdf/2410.09077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09077]] A Large Language Model-based Framework for Semi-Structured Tender Document Retrieval-Augmented Generation(https://arxiv.org/abs/2410.09077)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>The drafting of documents in the procurement field has progressively become more complex and diverse, driven by the need to meet legal requirements, adapt to technological advancements, and address stakeholder demands. While large language models (LLMs) show potential in document generation, most LLMs lack specialized knowledge in procurement. To address this gap, we use retrieval-augmented techniques to achieve professional document generation, ensuring accuracy and relevance in procurement documentation.</li>
<li><strong>摘要：</strong>采购领域的文件起草工作变得越来越复杂和多样化，这得益于满足法律要求、适应技术进步和满足利益相关者需求的需求。虽然大型语言模型 (LLM) 在文档生成方面显示出潜力，但大多数 LLM 缺乏采购方面的专业知识。为了弥补这一差距，我们使用检索增强技术来实现专业的文档生成，确保采购文档的准确性和相关性。</li>
</ul>

<h3>Title: Knowledge-Augmented Reasoning for EUAIA Compliance and Adversarial Robustness of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tomas Bueno Momcilovic, Dian Balta, Beat Buesser, Giulio Zizzo, Mark Purcell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09078">https://arxiv.org/abs/2410.09078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09078">https://arxiv.org/pdf/2410.09078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09078]] Knowledge-Augmented Reasoning for EUAIA Compliance and Adversarial Robustness of LLMs(https://arxiv.org/abs/2410.09078)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The EU AI Act (EUAIA) introduces requirements for AI systems which intersect with the processes required to establish adversarial robustness. However, given the ambiguous language of regulation and the dynamicity of adversarial attacks, developers of systems with highly complex models such as LLMs may find their effort to be duplicated without the assurance of having achieved either compliance or robustness. This paper presents a functional architecture that focuses on bridging the two properties, by introducing components with clear reference to their source. Taking the detection layer recommended by the literature, and the reporting layer required by the law, we aim to support developers and auditors with a reasoning layer based on knowledge augmentation (rules, assurance cases, contextual mappings). Our findings demonstrate a novel direction for ensuring LLMs deployed in the EU are both compliant and adversarially robust, which underpin trustworthiness.</li>
<li><strong>摘要：</strong>《欧盟人工智能法案》（EUAIA）引入了对人工智能系统的要求，这些要求与建立对抗性鲁棒性所需的过程相交叉。然而，鉴于监管语言的模糊性和对抗性攻击的动态性，具有 LLM 等高度复杂模型的系统的开发人员可能会发现他们的努力是重复的，而无法保证实现合规性或鲁棒性。本文介绍了一种功能架构，该架构侧重于弥合这两个属性，通过引入具有明确来源参考的组件。采用文献推荐的检测层和法律要求的报告层，我们旨在通过基于知识增强（规则、保证案例、上下文映射）的推理层为开发人员和审计人员提供支持。我们的研究结果为确保部署在欧盟的 LLM 既合规又具有对抗性鲁棒性提供了一个新方向，这为可信度奠定了基础。</li>
</ul>

<h3>Title: BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aofei Chang, Jiaqi Wang, Han Liu, Parminder Bhatia, Cao Xiao, Ting Wang, Fenglong Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09079">https://arxiv.org/abs/2410.09079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09079">https://arxiv.org/pdf/2410.09079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09079]] BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models(https://arxiv.org/abs/2410.09079)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Parameter Efficient Fine-Tuning (PEFT) offers an efficient solution for fine-tuning large pretrained language models for downstream tasks. However, most PEFT strategies are manually designed, often resulting in suboptimal performance. Recent automatic PEFT approaches aim to address this but face challenges such as search space entanglement, inefficiency, and lack of integration between parameter budgets and search processes. To overcome these issues, we introduce a novel Budget-guided Iterative search strategy for automatic PEFT (BIPEFT), significantly enhancing search efficiency. BIPEFT employs a new iterative search strategy to disentangle the binary module and rank dimension search spaces. Additionally, we design early selection strategies based on parameter budgets, accelerating the learning process by gradually removing unimportant modules and fixing rank dimensions. Extensive experiments on public benchmarks demonstrate the superior performance of BIPEFT in achieving efficient and effective PEFT for downstream tasks with a low parameter budget.</li>
<li><strong>摘要：</strong>参数高效微调 (PEFT) 为下游任务的大型预训练语言模型的微调提供了一种有效的解决方案。然而，大多数 PEFT 策略都是手动设计的，通常会导致性能不佳。最近的自动 PEFT 方法旨在解决这一问题，但面临着搜索空间纠缠、效率低下以及参数预算和搜索过程之间缺乏整合等挑战。为了克服这些问题，我们引入了一种新颖的预算引导迭代搜索策略，用于自动 PEFT (BIPEFT)，显著提高了搜索效率。BIPEFT 采用一种新的迭代搜索策略来解开二元模块和秩维度搜索空间。此外，我们根据参数预算设计早期选择策略，通过逐步删除不重要的模块和固定秩维度来加速学习过程。在公共基准上进行的大量实验证明了 BIPEFT 在以低参数预算实现下游任务的高效 PEFT 方面的卓越性能。</li>
</ul>

<h3>Title: Diagnosing Robotics Systems Issues with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jordis Emilia Herrmann, Aswath Mandakath Gopinath, Mikael Norrlof, Mark Niklas Müller</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09084">https://arxiv.org/abs/2410.09084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09084">https://arxiv.org/pdf/2410.09084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09084]] Diagnosing Robotics Systems Issues with Large Language Models(https://arxiv.org/abs/2410.09084)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Quickly resolving issues reported in industrial applications is crucial to minimize economic impact. However, the required data analysis makes diagnosing the underlying root causes a challenging and time-consuming task, even for experts. In contrast, large language models (LLMs) excel at analyzing large amounts of data. Indeed, prior work in AI-Ops demonstrates their effectiveness in analyzing IT systems. Here, we extend this work to the challenging and largely unexplored domain of robotics systems. To this end, we create SYSDIAGBENCH, a proprietary system diagnostics benchmark for robotics, containing over 2500 reported issues. We leverage SYSDIAGBENCH to investigate the performance of LLMs for root cause analysis, considering a range of model sizes and adaptation techniques. Our results show that QLoRA finetuning can be sufficient to let a 7B-parameter model outperform GPT-4 in terms of diagnostic accuracy while being significantly more cost-effective. We validate our LLM-as-a-judge results with a human expert study and find that our best model achieves similar approval ratings as our reference labels.</li>
<li><strong>摘要：</strong>快速解决工业应用中报告的问题对于最大限度地减少经济影响至关重要。然而，所需的数据分析使得诊断根本原因成为一项具有挑战性且耗时的任务，即使对于专家来说也是如此。相比之下，大型语言模型 (LLM) 擅长分析大量数据。事实上，AI-Ops 中的先前工作证明了它们在分析 IT 系统方面的有效性。在这里，我们将这项工作扩展到具有挑战性且尚未开发的机器人系统领域。为此，我们创建了 SYSDIAGBENCH，这是一个专有的机器人系统诊断基准，包含 2500 多个报告的问题。我们利用 SYSDIAGBENCH 来研究 LLM 在根本原因分析方面的性能，考虑了一系列模型大小和自适应技术。我们的结果表明，QLoRA 微调足以让 7B 参数模型在诊断准确性方面胜过 GPT-4，同时显著提高成本效益。我们通过人类专家的研究验证了我们的 LLM 作为评判的结果，发现我们的最佳模型获得了与我们的参考标签相似的支持率。</li>
</ul>

<h3>Title: Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations</h3>
<ul>
<li><strong>Authors: </strong>Tarun Raheja, Nilay Pochhi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09097">https://arxiv.org/abs/2410.09097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09097">https://arxiv.org/pdf/2410.09097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09097]] Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations(https://arxiv.org/abs/2410.09097)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their vulnerability to jailbreak attacks poses significant security risks. This survey paper presents a comprehensive analysis of recent advancements in attack strategies and defense mechanisms within the field of Large Language Model (LLM) red-teaming. We analyze various attack methods, including gradient-based optimization, reinforcement learning, and prompt engineering approaches. We discuss the implications of these attacks on LLM safety and the need for improved defense mechanisms. This work aims to provide a thorough understanding of the current landscape of red-teaming attacks and defenses on LLMs, enabling the development of more secure and reliable language models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言处理任务中表现出了卓越的能力，但它们易受越狱攻击，带来了重大的安全风险。这篇综述论文全面分析了大型语言模型 (LLM) 红队攻击领域的最新进展和防御机制。我们分析了各种攻击方法，包括基于梯度的优化、强化学习和快速工程方法。我们讨论了这些攻击对 LLM 安全的影响以及改进防御机制的必要性。这项工作旨在全面了解当前对 LLM 的红队攻击和防御形势，从而开发出更安全、更可靠的语言模型。</li>
</ul>

<h3>Title: ACER: Automatic Language Model Context Extension via Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Luyu Gao, Yunyi Zhang, Jamie Callan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09141">https://arxiv.org/abs/2410.09141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09141">https://arxiv.org/pdf/2410.09141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09141]] ACER: Automatic Language Model Context Extension via Retrieval(https://arxiv.org/abs/2410.09141)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Long-context modeling is one of the critical capabilities of language AI for digesting and reasoning over complex information pieces. In practice, long-context capabilities are typically built into a pre-trained language model~(LM) through a carefully designed context extension stage, with the goal of producing generalist long-context capabilities. In our preliminary experiments, however, we discovered that the current open-weight generalist long-context models are still lacking in practical long-context processing tasks. While this means perfectly effective long-context modeling demands task-specific data, the cost can be prohibitive. In this paper, we draw inspiration from how humans process a large body of information: a lossy \textbf{retrieval} stage ranks a large set of documents while the reader ends up reading deeply only the top candidates. We build an \textbf{automatic} data synthesis pipeline that mimics this process using short-context LMs. The short-context LMs are further tuned using these self-generated data to obtain task-specific long-context capabilities. Similar to how pre-training learns from imperfect data, we hypothesize and further demonstrate that the short-context model can bootstrap over the synthetic data, outperforming not only long-context generalist models but also the retrieval and read pipeline used to synthesize the training data in real-world tasks such as long-context retrieval augmented generation.</li>
<li><strong>摘要：</strong>长上下文建模是语言 AI ​​消化和推理复杂信息片段的关键能力之一。在实践中，长上下文能力通常通过精心设计的上下文扩展阶段内置到预训练语言模型 (LM) 中，目的是产生通用的长上下文能力。然而，在我们的初步实验中，我们发现当前的开放权重通用长上下文模型在实际的长上下文处理任务中仍然缺乏。虽然这意味着完美有效的长上下文建模需要特定于任务的数据，但成本可能过高。在本文中，我们从人类处理大量信息的方式中汲取灵感：有损 \textbf{检索} 阶段对大量文档进行排名，而读者最终只会深入阅读排名靠前的候选内容。我们构建了一个 \textbf{自动} 数据合成管道，使用短上下文 LM 模拟此过程。使用这些自生成的数据进一步调整短上下文 LM，以获得特定于任务的长上下文能力。与预训练从不完善的数据中学习的方式类似，我们假设并进一步证明，短上下文模型可以在合成数据上进行引导，不仅优于长上下文通用模型，而且还优于用于在实际任务（例如长上下文检索增强生成）中合成训练数据的检索和读取管道。</li>
</ul>

<h3>Title: Hybrid Training Approaches for LLMs: Leveraging Real and Synthetic Data to Enhance Model Performance in Domain-Specific Applications</h3>
<ul>
<li><strong>Authors: </strong>Alexey Zhezherau, Alexei Yanockin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09168">https://arxiv.org/abs/2410.09168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09168">https://arxiv.org/pdf/2410.09168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09168]] Hybrid Training Approaches for LLMs: Leveraging Real and Synthetic Data to Enhance Model Performance in Domain-Specific Applications(https://arxiv.org/abs/2410.09168)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This research explores a hybrid approach to fine-tuning large language models (LLMs) by integrating real-world and synthetic data to boost model performance, particularly in generating accurate and contextually relevant responses. By leveraging a dataset combining transcribed real interactions with high-quality synthetic sessions, we aimed to overcome the limitations of scarce, noisy, and domain-specific real data. Synthetic personas and scenarios were employed to enhance training diversity. The study evaluated three models: a base foundational model, a model fine-tuned with real data, and a hybrid fine-tuned model. Experimental results showed that the hybrid model consistently outperformed the others in specific vertical applications, achieving the highest scores across all metrics. Further testing confirmed the hybrid model's superior adaptability and contextual understanding across diverse scenarios. These findings suggest that combining real and synthetic data can significantly improve the robustness and contextual sensitivity of LLMs, particularly in domain-specific and vertical use cases.</li>
<li><strong>摘要：</strong>本研究探索了一种混合方法，通过整合真实数据和合成数据来微调大型语言模型 (LLM)，以提高模型性能，特别是在生成准确且与上下文相关的响应方面。通过利用将转录的真实交互与高质量的合成会话相结合的数据集，我们旨在克服稀缺、嘈杂和特定领域的真实数据的局限性。采用合成角色和场景来增强训练多样性。该研究评估了三个模型：基本基础模型、使用真实数据微调的模型和混合微调模型。实验结果表明，混合模型在特定的垂直应用中始终优于其他模型，在所有指标中均获得最高分。进一步的测试证实了混合模型在不同场景中的卓越适应性和上下文理解能力。这些发现表明，结合真实数据和合成数据可以显著提高 LLM 的稳健性和上下文敏感性，特别是在特定领域和垂直用例中。</li>
</ul>

<h3>Title: M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Gitanjali Kumari, Kirtan Jain, Asif Ekbal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09220">https://arxiv.org/abs/2410.09220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09220">https://arxiv.org/pdf/2410.09220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09220]] M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought(https://arxiv.org/abs/2410.09220)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In recent years, there has been a significant rise in the phenomenon of hate against women on social media platforms, particularly through the use of misogynous memes. These memes often target women with subtle and obscure cues, making their detection a challenging task for automated systems. Recently, Large Language Models (LLMs) have shown promising results in reasoning using Chain-of-Thought (CoT) prompting to generate the intermediate reasoning chains as the rationale to facilitate multimodal tasks, but often neglect cultural diversity and key aspects like emotion and contextual knowledge hidden in the visual modalities. To address this gap, we introduce a Multimodal Multi-hop CoT (M3Hop-CoT) framework for Misogynous meme identification, combining a CLIP-based classifier and a multimodal CoT module with entity-object-relationship integration. M3Hop-CoT employs a three-step multimodal prompting principle to induce emotions, target awareness, and contextual knowledge for meme analysis. Our empirical evaluation, including both qualitative and quantitative analysis, validates the efficacy of the M3Hop-CoT framework on the SemEval-2022 Task 5 (MAMI task) dataset, highlighting its strong performance in the macro-F1 score. Furthermore, we evaluate the model's generalizability by evaluating it on various benchmark meme datasets, offering a thorough insight into the effectiveness of our approach across different datasets.</li>
<li><strong>摘要：</strong>近年来，社交媒体平台上针对女性的仇恨现象显著增加，尤其是通过使用厌女模因。这些模因通常以微妙和模糊的线索针对女性，这使得检测它们成为自动化系统的一项艰巨任务。最近，大型语言模型 (LLM) 在使用思维链 (CoT) 提示生成中间推理链作为促进多模态任务的理由进行推理方面表现出良好的结果，但往往忽视了文化多样性和隐藏在视觉模态中的情感和背景知识等关键方面。为了解决这一差距，我们引入了一个用于厌女模因识别的多模态多跳 CoT (M3Hop-CoT) 框架，将基于 CLIP 的分类器和多模态 CoT 模块与实体-对象-关系集成相结合。M3Hop-CoT 采用三步多模态提示原则来诱导情绪、目标意识和背景知识以进行模因分析。我们的实证评估包括定性和定量分析，验证了 M3Hop-CoT 框架在 SemEval-2022 任务 5（MAMI 任务）数据集上的有效性，突出了其在宏 F1 分数中的出色表现。此外，我们通过在各种基准 meme 数据集上对该模型进行评估来评估其通用性，从而深入了解我们的方法在不同数据集上的有效性。</li>
</ul>

<h3>Title: The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Ruochen Zhang, Qinan Yu, Matianyu Zang, Carsten Eickhoff, Ellie Pavlick</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09223">https://arxiv.org/abs/2410.09223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09223">https://arxiv.org/pdf/2410.09223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09223]] The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling(https://arxiv.org/abs/2410.09223)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>We employ new tools from mechanistic interpretability in order to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? Using English and Chinese multilingual and monolingual models, we analyze the internal circuitry involved in two tasks. We find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs, and that this is the case even for monolingual models trained completely independently. Moreover, we show that multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages. Together, our results provide new insights into how LLMs trade off between exploiting common structures and preserving linguistic differences when tasked with modeling multiple languages simultaneously.</li>
<li><strong>摘要：</strong>我们利用机械可解释性的新工具来探究大型语言模型 (LLM) 的内部结构是否与训练它们的语言所基于的语言结构相对应。具体来说，我们要问 (1) 当两种语言使用相同的形态句法过程时，LLM 是否使用共享的内部电路来处理它们？以及 (2) 当两种语言需要不同的形态句法过程时，LLM 是否使用不同的内部电路来处理它们？使用英语和中文多语言和单语模型，我们分析了两个任务所涉及的内部电路。我们发现证据表明，模型使用相同的电路来处理相同的句法过程，而与它出现的语言无关，即使是完全独立训练的单语模型也是如此。此外，我们表明，多语言模型在需要处理仅存在于某些语言中的语言过程（例如形态标记）时，会使用特定于语言的组件（注意头和前馈网络）。总之，我们的研究结果为 LLM 在同时对多种语言进行建模时如何在利用共同结构和保留语言差异之间进行权衡提供了新的见解。</li>
</ul>

<h3>Title: Improving semantic understanding in speech language models via brain-tuning</h3>
<ul>
<li><strong>Authors: </strong>Omer Moussa, Dietrich Klakow, Mariya Toneva</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09230">https://arxiv.org/abs/2410.09230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09230">https://arxiv.org/pdf/2410.09230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09230]] Improving semantic understanding in speech language models via brain-tuning(https://arxiv.org/abs/2410.09230)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Speech-language models align impressively with human brain responses to natural language. However, current models rely heavily on low-level speech features, indicating they lack brain-relevant semantics, limiting their utility as models of semantic processing in the brain. In this work, we address this limitation by inducing brain-relevant bias into the models via fine-tuning with fMRI recordings of people listening to natural stories, a process we call brain-tuning. After testing it on three different pretrained backbones, we show that brain-tuning improves alignment with new brain recordings in semantic language regions and reduces reliance on low-level speech features. Notably, brain-tuning leads to 1) consistent improvements in performance across various downstream tasks and 2) a representational space with increased semantic preference. Our results provide the first evidence that incorporating brain signals into the training of language models improves their semantic understanding.</li>
<li><strong>摘要：</strong>语音语言模型与人类大脑对自然语言的反应非常吻合。然而，目前的模型严重依赖低级语音特征，这表明它们缺乏与大脑相关的语义，限制了它们作为大脑语义处理模型的效用。在这项研究中，我们通过使用人们听自然故事的 fMRI 记录对模型进行微调，将与大脑相关的偏差引入模型中，从而解决了这一限制，我们将这一过程称为大脑调整。在三个不同的预训练主干上进行测试后，我们发现大脑调整可以改善与语义语言区域的新大脑记录的一致性，并减少对低级语音特征的依赖。值得注意的是，大脑调整可导致 1) 各种下游任务的性能持续改善和 2) 表征空间语义偏好增加。我们的结果首次证明，将大脑信号纳入语言模型的训练可以提高其语义理解能力。</li>
</ul>

<h3>Title: Fine-Tuning In-House Large Language Models to Infer Differential Diagnosis from Radiology Reports</h3>
<ul>
<li><strong>Authors: </strong>Luoyao Chen, Revant Teotia, Antonio Verdone, Aidan Cardall, Lakshay Tyagi, Yiqiu Shen, Sumit Chopra</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09234">https://arxiv.org/abs/2410.09234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09234">https://arxiv.org/pdf/2410.09234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09234]] Fine-Tuning In-House Large Language Models to Infer Differential Diagnosis from Radiology Reports(https://arxiv.org/abs/2410.09234)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Radiology reports summarize key findings and differential diagnoses derived from medical imaging examinations. The extraction of differential diagnoses is crucial for downstream tasks, including patient management and treatment planning. However, the unstructured nature of these reports, characterized by diverse linguistic styles and inconsistent formatting, presents significant challenges. Although proprietary large language models (LLMs) such as GPT-4 can effectively retrieve clinical information, their use is limited in practice by high costs and concerns over the privacy of protected health information (PHI). This study introduces a pipeline for developing in-house LLMs tailored to identify differential diagnoses from radiology reports. We first utilize GPT-4 to create 31,056 labeled reports, then fine-tune open source LLM using this dataset. Evaluated on a set of 1,067 reports annotated by clinicians, the proposed model achieves an average F1 score of 92.1\%, which is on par with GPT-4 (90.8\%). Through this study, we provide a methodology for constructing in-house LLMs that: match the performance of GPT, reduce dependence on expensive proprietary models, and enhance the privacy and security of PHI.</li>
<li><strong>摘要：</strong>放射学报告总结了医学影像检查得出的关键发现和鉴别诊断。鉴别诊断的提取对于下游任务（包括患者管理和治疗计划）至关重要。然而，这些报告的非结构化性质，以多种语言风格和不一致的格式为特征，带来了重大挑战。尽管专有的大型语言模型 (LLM)（如 GPT-4）可以有效地检索临床信息，但由于成本高昂以及对受保护健康信息 (PHI) 隐私的担忧，它们在实践中的使用受到限制。本研究介绍了一种开发内部 LLM 的流程，专门用于从放射学报告中识别鉴别诊断。我们首先利用 GPT-4 创建 31,056 份带标签的报告，然后使用此数据集微调开源 LLM。在一组由临床医生注释的 1,067 份报告上进行评估，所提出的模型的平均 F1 得分为 92.1\%，与 GPT-4（90.8\%）相当。通过这项研究，我们提供了一种构建内部 LLM 的方法：匹配 GPT 的性能，减少对昂贵的专有模型的依赖，并增强 PHI 的隐私和安全性。</li>
</ul>

<h3>Title: Sui Generis: Large Language Models for Authorship Attribution and Verification in Latin</h3>
<ul>
<li><strong>Authors: </strong>Gleb Schmidt, Svetlana Gorovaia, Ivan P. Yamshchikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09245">https://arxiv.org/abs/2410.09245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09245">https://arxiv.org/pdf/2410.09245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09245]] Sui Generis: Large Language Models for Authorship Attribution and Verification in Latin(https://arxiv.org/abs/2410.09245)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper evaluates the performance of Large Language Models (LLMs) in authorship attribution and authorship verification tasks for Latin texts of the Patristic Era. The study showcases that LLMs can be robust in zero-shot authorship verification even on short texts without sophisticated feature engineering. Yet, the models can also be easily "mislead" by semantics. The experiments also demonstrate that steering the model's authorship analysis and decision-making is challenging, unlike what is reported in the studies dealing with high-resource modern languages. Although LLMs prove to be able to beat, under certain circumstances, the traditional baselines, obtaining a nuanced and truly explainable decision requires at best a lot of experimentation.</li>
<li><strong>摘要：</strong>本文评估了大型语言模型 (LLM) 在拉丁语教父时代文本的作者归属和作者验证任务中的表现。研究表明，即使没有复杂的特征工程，LLM 也可以在零样本作者验证中表现出色。然而，这些模型也很容易被语义“误导”。实验还表明，操纵模型的作者分析和决策是一项挑战，这与处理高资源现代语言的研究报告不同。尽管在某些情况下，LLM 被证明能够超越传统基线，但要获得细致入微且真正可解释的决策，至少需要大量的实验。</li>
</ul>

<h3>Title: ReasonPlanner: Enhancing Autonomous Planning in Dynamic Environments with Temporal Knowledge Graphs and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Minh Pham Dinh, Munira Syed, Michael G Yankoski, Trenton W. Ford</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09252">https://arxiv.org/abs/2410.09252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09252">https://arxiv.org/pdf/2410.09252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09252]] ReasonPlanner: Enhancing Autonomous Planning in Dynamic Environments with Temporal Knowledge Graphs and LLMs(https://arxiv.org/abs/2410.09252)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Planning and performing interactive tasks, such as conducting experiments to determine the melting point of an unknown substance, is straightforward for humans but poses significant challenges for autonomous agents. We introduce ReasonPlanner, a novel generalist agent designed for reflective thinking, planning, and interactive reasoning. This agent leverages LLMs to plan hypothetical trajectories by building a World Model based on a Temporal Knowledge Graph. The agent interacts with the environment using a natural language actor-critic module, where the actor translates the imagined trajectory into a sequence of actionable steps, and the critic determines if replanning is necessary. ReasonPlanner significantly outperforms previous state-of-the-art prompting-based methods on the ScienceWorld benchmark by more than 1.8 times, while being more sample-efficient and interpretable. It relies solely on frozen weights thus requiring no gradient updates. ReasonPlanner can be deployed and utilized without specialized knowledge of Machine Learning, making it accessible to a wide range of users.</li>
<li><strong>摘要：</strong>规划和执行交互式任务（例如进行实验以确定未知物质的熔点）对人类来说很简单，但对自主代理来说却带来了重大挑战。我们引入了 ReasonPlanner，这是一种新型的通用代理，旨在进行反思性思考、规划和交互式推理。该代理利用 LLM 来规划假设轨迹，方法是基于时间知识图构建世界模型。代理使用自然语言演员-评论家模块与环境交互，其中演员将想象的轨迹转换为一系列可操作的步骤，评论家确定是否需要重新规划。ReasonPlanner 在 ScienceWorld 基准上的表现明显优于之前最先进的基于提示的方法 1.8 倍以上，同时具有更高的采样效率和可解释性。它完全依赖于冻结权重，因此不需要梯度更新。无需机器学习的专业知识即可部署和使用 ReasonPlanner，从而使广泛的用户都可以使用它。</li>
</ul>

<h3>Title: Nudging: Inference-time Alignment via Model Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Yu Fei, Yasaman Razeghi, Sameer Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09300">https://arxiv.org/abs/2410.09300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09300">https://arxiv.org/pdf/2410.09300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09300]] Nudging: Inference-time Alignment via Model Collaboration(https://arxiv.org/abs/2410.09300)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) require alignment, such as instruction-tuning or reinforcement learning from human feedback, to effectively and safely follow user instructions. This process necessitates training aligned versions for every model size in each model family, resulting in significant computational overhead. In this work, we propose nudging, a simple, plug-and-play, and training-free algorithm that aligns any base model at inference time using a small aligned model. Nudging is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens, such as "Sure" or "Thank". We find that base models are significantly more uncertain when generating these tokens. Leveraging this observation, nudging employs a small aligned model to generate nudging tokens to steer the large base model's output toward desired directions when the base model's uncertainty is high. We evaluate the effectiveness of nudging across 3 model families and 13 tasks, covering reasoning, general knowledge, instruction following, and safety benchmarks. Without any additional training, nudging a large base model with a 7x - 14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. For example, nudging OLMo-7b with OLMo-1b-instruct, affecting less than 9% of tokens, achieves a 10% absolute improvement on GSM8K over OLMo-7b-instruct. Unlike prior inference-time tuning methods, nudging enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-2-7b-chat outperforms Llama-2-70b-chat on various tasks. Overall, this work introduces a simple yet powerful approach to token-level model collaboration, offering a modular solution to LLM alignment. Our project website: this https URL .</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 需要对齐，例如指令调整或从人类反馈中进行强化学习，以有效且安全地遵循用户指令。此过程需要为每个模型系列中的每个模型大小训练对齐版本，从而产生大量的计算开销。在这项工作中，我们提出了一种简单、即插即用且无需训练的算法，该算法使用小型对齐模型在推理时对齐任何基础模型。Nudging 的动机是最近的发现，对齐主要改变模型对一小部分风格标记（例如“Sure”或“Thank”）的行为。我们发现基础模型在生成这些标记时明显更加不确定。利用这一观察结果，Nudging 采用小型对齐模型来生成 Nudging 标记，以在基础模型的不确定性很高时将大型基础模型的输出引导至期望的方向。我们在 3 个模型系列和 13 个任务中评估了 Nudging 的有效性，涵盖了推理、一般知识、指令遵循和安全基准。无需任何额外训练，使用 7 到 14 倍小对齐模型对大型基础模型进行微调，可实现与大型对齐模型相当的零样本性能，有时甚至超过大型对齐模型。例如，使用 OLMo-1b-instruct 对 OLMo-7b 进行微调，影响不到 9% 的 token，与 OLMo-7b-instruct 相比，在 GSM8K 上实现了 10% 的绝对改进。与之前的推理时间调整方法不同，微调可以实现模型系列之间的现成协作。例如，使用 Llama-2-7b-chat 对 Gemma-2-27b 进行微调在各种任务上的表现都优于 Llama-2-70b-chat。总的来说，这项工作引入了一种简单而强大的 token 级模型协作方法，为 LLM 对齐提供了一个模块化解决方案。我们的项目网站：这个 https URL 。</li>
</ul>

<h3>Title: Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Buu Phan, Brandon Amos, Itai Gat, Marton Havasi, Matthew Muckley, Karen Ullrich</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09303">https://arxiv.org/abs/2410.09303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09303">https://arxiv.org/pdf/2410.09303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09303]] Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles(https://arxiv.org/abs/2410.09303)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Tokenization is associated with many poorly understood shortcomings in language models (LMs), yet remains an important component for long sequence scaling purposes. This work studies how tokenization impacts model performance by analyzing and comparing the stochastic behavior of tokenized models with their byte-level, or token-free, counterparts. We discover that, even when the two models are statistically equivalent, their predictive distributions over the next byte can be substantially different, a phenomenon we term as "tokenization bias''. To fully characterize this phenomenon, we introduce the Byte-Token Representation Lemma, a framework that establishes a mapping between the learned token distribution and its equivalent byte-level distribution. From this result, we develop a next-byte sampling algorithm that eliminates tokenization bias without requiring further training or optimization. In other words, this enables zero-shot conversion of tokenized LMs into statistically equivalent token-free ones. We demonstrate its broad applicability with two use cases: fill-in-the-middle (FIM) tasks and model ensembles. In FIM tasks where input prompts may terminate mid-token, leading to out-of-distribution tokenization, our method mitigates performance degradation and achieves an approximately 18% improvement in FIM coding benchmarks, consistently outperforming the standard token healing fix. For model ensembles where each model employs a distinct vocabulary, our approach enables seamless integration, resulting in improved performance (up to 3.7%) over individual models across various standard baselines in reasoning, knowledge, and coding.</li>
<li><strong>摘要：</strong>标记化与语言模型 (LM) 中许多不太为人所知的缺点有关，但对于长序列扩展而言，它仍然是一个重要组成部分。本研究通过分析和比较标记化模型与字节级或无标记模型的随机行为，研究标记化如何影响模型性能。我们发现，即使两个模型在统计上是等效的，它们对下一个字节的预测分布也可能存在很大差异，我们将这种现象称为“标记化偏差”。为了充分描述这种现象，我们引入了字节标记表示引理，这是一个在学习到的标记分布与其等效字节级分布之间建立映射的框架。根据这一结果，我们开发了一种下一个字节采样算法，该算法无需进一步训练或优化即可消除标记化偏差。换句话说，这使得标记化 LM 能够零样本转换为统计上等效的无标记 LM。我们通过两个用例展示了它的广泛适用性：中间填充 (FIM) 任务和模型集成。在 FIM 任务中，输入提示可能会在中间标记处终止，从而导致分布外的标记化，我们的方法减轻了性能下降，并在 FIM 编码基准中实现了大约 18% 的改进，始终优于标准标记修复修复。对于每个模型采用不同词汇的模型集成，我们的方法可以实现无缝集成，从而比推理、知识和编码等各种标准基线上的单个模型的性能提高（高达 3.7%）。</li>
</ul>

<h3>Title: \llinstruct: An Instruction-tuned model for English Language Proficiency Assessments</h3>
<ul>
<li><strong>Authors: </strong>Debanjan Ghosh, Sophia Chan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09314">https://arxiv.org/abs/2410.09314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09314">https://arxiv.org/pdf/2410.09314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09314]] \llinstruct: An Instruction-tuned model for English Language Proficiency Assessments(https://arxiv.org/abs/2410.09314)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>We present \llinstruct: An 8B instruction-tuned model that is designed to generate content for English Language Proficiency Assessments (ELPA) and related applications. Our work involves creating a new dataset of 70K instructions and explanations in the ELPA domain and using these to fine-tune Llama-3 8B models (SFT) of different sizes (e.g., SFT-17K, SFT-50K and SFT-70K). Human evaluations are conducted over unseen instructions to compare these SFT models against SOTA models (e.g., Dolly-2, Mistral, Llama-3 base version, and GPT-3.5). The findings show although all three SFT models perform comparably, the model trained on largest instruction dataset -- SFT-70K - leads to the most valid outputs ready for assessments. However, although the SFT models perform better than larger model, e.g., GPT 3.5 on the aspect of explanations of outputs, many outputs still need human interventions to make them actual ready for real world assessments.</li>
<li><strong>摘要：</strong>我们提出了 \llinstruct：一种 8B 指令调整模型，旨在为英语语言能力评估 (ELPA) 和相关应用生成内容。我们的工作包括在 ELPA 领域创建一个包含 70K 条指令和解释的新数据集，并使用这些数据集微调不同大小的 Llama-3 8B 模型 (SFT)（例如 SFT-17K、SFT-50K 和 SFT-70K）。对未见过的指令进行人工评估，以将这些 SFT 模型与 SOTA 模型（例如 Dolly-2、Mistral、Llama-3 基本版本和 GPT-3.5）进行比较。研究结果表明，尽管这三个 SFT 模型的表现相当，但在最大的指令数据集 SFT-70K 上训练的模型可以产生最有效的评估输出。然而，尽管 SFT 模型在输出解释方面表现优于更大的模型（例如 GPT 3.5），但许多输出仍然需要人工干预才能真正为现实世界的评估做好准备。</li>
</ul>

<h3>Title: Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Saiful Islam Salim, Rubin Yuchan Yang, Alexander Cooper, Suryashree Ray, Saumya Debray, Sazzadur Rahaman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09318">https://arxiv.org/abs/2410.09318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09318">https://arxiv.org/pdf/2410.09318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09318]] Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbations(https://arxiv.org/abs/2410.09318)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>While Large language model (LLM)-based programming assistants such as CoPilot and ChatGPT can help improve the productivity of professional software developers, they can also facilitate cheating in introductory computer programming courses. Assuming instructors have limited control over the industrial-strength models, this paper investigates the baseline performance of 5 widely used LLMs on a collection of introductory programming problems, examines adversarial perturbations to degrade their performance, and describes the results of a user study aimed at understanding the efficacy of such perturbations in hindering actual code generation for introductory programming assignments. The user study suggests that i) perturbations combinedly reduced the average correctness score by 77%, ii) the drop in correctness caused by these perturbations was affected based on their detectability.</li>
<li><strong>摘要：</strong>虽然基于大型语言模型 (LLM) 的编程助手（例如 CoPilot 和 ChatGPT）可以帮助提高专业软件开发人员的工作效率，但它们也可能助长入门级计算机编程课程中的作弊行为。假设教师对工业级模型的控制有限，本文研究了 5 种广泛使用的 LLM 在一系列入门级编程问题上的基线性能，研究了对抗性扰动会降低其性能，并描述了一项用户研究的结果，该研究旨在了解此类扰动在阻碍入门级编程作业的实际代码生成方面的有效性。用户研究表明：i) 扰动合计使平均正确性得分降低了 77%，ii) 这些扰动导致的正确性下降受到其可检测性的影响。</li>
</ul>

<h3>Title: Rethinking Data Selection at Scale: Random Selection is Almost All You Need</h3>
<ul>
<li><strong>Authors: </strong>Tingyu Xia, Bowen Yu, Kai Dang, An Yang, Yuan Wu, Yuan Tian, Yi Chang, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09335">https://arxiv.org/abs/2410.09335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09335">https://arxiv.org/pdf/2410.09335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09335]] Rethinking Data Selection at Scale: Random Selection is Almost All You Need(https://arxiv.org/abs/2410.09335)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even exceeding those obtained using the entire dataset. However, most existing data selection techniques are designed for small-scale data pools, which fail to meet the demands of real-world SFT scenarios. In this paper, we replicated several self-scoring methods those that do not rely on external model assistance on two million scale datasets, and found that nearly all methods struggled to significantly outperform random selection when dealing with such large-scale data pools. Moreover, our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high quality data. We also analyzed the limitations of several current approaches, explaining why they perform poorly on large-scale datasets and why they are unsuitable for such contexts. Finally, we found that filtering data by token length offers a stable and efficient method for improving results. This approach, particularly when training on long text data, proves highly beneficial for relatively weaker base models, such as Llama3.</li>
<li><strong>摘要：</strong>监督式微调 (SFT) 对于将大型语言模型 (LLM) 与人类指令对齐至关重要。SFT 期间的主要目标是从较大的池中选择一小部分但具有代表性的训练数据子集，以便使用该子集进行微调可实现与使用整个数据集获得的结果相当甚至超过的结果。然而，大多数现有的数据选择技术都是为小规模数据池设计的，无法满足现实世界 SFT 场景的需求。在本文中，我们在两百万规模的数据集上复制了几种不依赖外部模型辅助的自评分方法，发现在处理如此大规模的数据池时，几乎所有方法都难以显著优于随机选择。此外，我们的比较表明，在 SFT 期间，数据选择的多样性比仅仅关注高质量数据更为重要。我们还分析了几种当前方法的局限性，解释了它们在大规模数据集上表现不佳的原因以及它们不适合此类环境的原因。最后，我们发现按标记长度过滤数据是一种稳定有效的改进结果的方法。这种方法，特别是在对长文本数据进行训练时，对于相对较弱的基础模型（例如 Llama3）非常有益。</li>
</ul>

<h3>Title: Keys to Robust Edits: from Theoretical Insights to Practical Advances</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Yan, Futing Wang, Yun Luo, Yafu Li, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09338">https://arxiv.org/abs/2410.09338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09338">https://arxiv.org/pdf/2410.09338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09338]] Keys to Robust Edits: from Theoretical Insights to Practical Advances(https://arxiv.org/abs/2410.09338)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized knowledge storage and retrieval, but face challenges with conflicting and outdated information. Knowledge editing techniques have been proposed to address these issues, yet they struggle with robustness tests involving long contexts, paraphrased subjects, and continuous edits. This work investigates the cause of these failures in locate-and-edit methods, offering theoretical insights into their key-value modeling and deriving mathematical bounds for robust and specific edits, leading to a novel 'group discussion' conceptual model for locate-and-edit methods. Empirical analysis reveals that keys used by current methods fail to meet robustness and specificity requirements. To address this, we propose a Robust Edit Pathway (REP) that disentangles editing keys from LLMs' inner representations. Evaluations on LLaMA2-7B and Mistral-7B using the CounterFact dataset show that REP significantly improves robustness across various metrics, both in-domain and out-of-domain, with minimal trade-offs in success rate and locality. Our findings advance the development of reliable and flexible knowledge updating in LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了知识存储和检索，但面临着信息冲突和过时的挑战。已经提出了知识编辑技术来解决这些问题，但它们在涉及长上下文、释义主题和连续编辑的稳健性测试中遇到了困难。这项工作调查了定位和编辑方法中这些失败的原因，为其键值建模提供了理论见解，并推导出稳健和特定编辑的数学界限，从而为定位和编辑方法提供了一种新颖的“小组讨论”概念模型。实证分析表明，当前方法使用的键无法满足稳健性和特异性要求。为了解决这个问题，我们提出了一种稳健编辑路径 (REP)，将编辑键与 LLM 的内部表示区分开来。使用 CounterFact 数据集对 LLaMA2-7B 和 Mistral-7B 进行的评估表明，REP 显著提高了域内和域外各种指标的稳健性，同时将成功率和局部性的权衡降到最低。我们的研究成果推动了法学硕士 (LLM) 中可靠且灵活的知识更新的发展。</li>
</ul>

<h3>Title: LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Chong Li, Xinyi Chen, Shuo Wang, Yu Chao, Zhili Li, Haoyu Wang, Rongqiao An, Qi Shi, Zhixing Tan, Xu Han, Xiaodong Shi, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09342">https://arxiv.org/abs/2410.09342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09342">https://arxiv.org/pdf/2410.09342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09342]] LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models(https://arxiv.org/abs/2410.09342)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLM$\times$MapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLM$\times$MapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.</li>
<li><strong>摘要：</strong>扩大大型语言模型 (LLM) 的上下文窗口已成为一个关键的研究领域，特别是对于涉及极长文本的应用。在这项工作中，我们提出了一种新颖的无需训练的长文本处理框架，利用分而治之的策略实现全面的文档理解。所提出的 LLM$\times$MapReduce 框架将整个文档拆分为几个块供 LLM 读取，然后聚合中间答案以生成最终输出。分而治之的长文本处理框架的主要挑战在于拆分文档时可能会丢失必要的长距离信息，这可能导致模型根据分段文本生成不完整或不正确的答案。中断的长距离信息可分为两类：块间依赖性和块间冲突。我们设计了一个结构化信息协议来更好地应对块间依赖性，并设计了一个上下文置信度校准机制来解决块间冲突。实验结果表明，LLM$\times$MapReduce 的性能优于具有代表性的开源和商业长上下文 LLM，并且适用于多种不同的模型。</li>
</ul>

<h3>Title: ELICIT: LLM Augmentation via External In-Context Capability</h3>
<ul>
<li><strong>Authors: </strong>Futing Wang, Jianhao Yan, Yue Zhang, Tao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09343">https://arxiv.org/abs/2410.09343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09343">https://arxiv.org/pdf/2410.09343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09343]] ELICIT: LLM Augmentation via External In-Context Capability(https://arxiv.org/abs/2410.09343)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application. Traditional fine-tuning methods require substantial data and computational resources, especially for enhancing specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage. Inspired by the expression of in-context learned capabilities through task vectors and the concept of modularization, we propose \alg, a framework consisting of two modules designed to effectively store and reuse task vectors to elicit the diverse capabilities of models without additional training or inference tokens. Our comprehensive experiments and analysis demonstrate that our pipeline is highly transferable across different input formats, tasks, and model architectures. ELICIT serves as a plug-and-play performance booster to enable adaptive elicitation of model capabilities. By externally storing and reusing vectors that represent in-context learned capabilities, \alg not only demonstrates the potential to operate modular capabilities but also significantly enhances the performance, versatility, adaptability, and scalability of large language models. Our code will be publicly available at this https URL.</li>
<li><strong>摘要：</strong>增强大型语言模型的自适应能力是研究和应用的关键追求。传统的微调方法需要大量的数据和计算资源，尤其是为了增强特定能力，而上下文学习则受到需要适当的演示和有效的标记使用的限制。受到通过任务向量表达上下文学习能力和模块化概念的启发，我们提出了 \alg，这是一个由两个模块组成的框架，旨在有效地存储和重用任务向量，以引出模型的各种能力，而无需额外的训练或推理标记。我们全面的实验和分析表明，我们的管道在不同的输入格式、任务和模型架构之间具有高度可移植性。ELICIT 可作为即插即用的性能增强器，实现模型能力的自适应引出。通过外部存储和重用表示上下文学习能力的向量，\alg 不仅展示了操作模块化能力的潜力，而且还显著提高了大型语言模型的性能、多功能性、适应性和可扩展性。我们的代码将在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyoung Park, Minseok Joo, Joo-Kyung Kim, Hyunwoo J. Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09350">https://arxiv.org/abs/2410.09350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09350">https://arxiv.org/pdf/2410.09350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09350]] Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation(https://arxiv.org/abs/2410.09350)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Knowledge graph-grounded dialog generation requires retrieving a dialog-relevant subgraph from the given knowledge base graph and integrating it with the dialog history. Previous works typically represent the graph using an external encoder, such as graph neural networks, and retrieve relevant triplets based on the similarity between single-vector representations of triplets and the dialog history. However, these external encoders fail to leverage the rich knowledge of pretrained language models, and the retrieval process is also suboptimal due to the information bottleneck caused by the single-vector abstraction of the dialog history. In this work, we propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant knowledge subgraphs by directly generating their token sequences on top of language models. For effective generative subgraph retrieval, we introduce two key methods: (i) structure-aware knowledge graph linearization with self-supervised graph-specific tokens and (ii) graph-constrained decoding utilizing graph structural proximity-based entity informativeness scores for valid and relevant generative retrieval. DialogGSR achieves state-of-the-art performance in knowledge graph-grounded dialog generation, as demonstrated on OpenDialKG and KOMODIS datasets.</li>
<li><strong>摘要：</strong>基于知识图谱的对话生成需要从给定的知识库图中检索与对话相关的子图，并将其与对话历史相结合。以前的研究通常使用外部编码器（例如图神经网络）表示图，并根据三元组的单向量表示与对话历史之间的相似性检索相关三元组。然而，这些外部编码器无法利用预训练语言模型的丰富知识，而且由于对话历史的单向量抽象导致的信息瓶颈，检索过程也不是最优的。在这项工作中，我们提出了使用生成子图检索的对话生成 (DialogGSR)，它通过直接在语言模型之上生成相关知识子图的标记序列来检索相关知识子图。为了实现有效的生成子图检索，我们引入了两种关键方法：(i) 使用自监督的图特定标记进行结构感知知识图谱线性化和 (ii) 使用基于图结构接近度的实体信息量分数进行图约束解码，以实现有效且相关的生成检索。 DialogGSR 在基于知识图谱的对话生成方面实现了最先进的性能，如 OpenDialKG 和 KOMODIS 数据集所展示的。</li>
</ul>

<h3>Title: CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning On Device</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Fu, Raviteja Anantha, Jianpeng Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09407">https://arxiv.org/abs/2410.09407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09407">https://arxiv.org/pdf/2410.09407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09407]] CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning On Device(https://arxiv.org/abs/2410.09407)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>While server-side Large Language Models (LLMs) demonstrate proficiency in function calling and complex reasoning, deploying Small Language Models (SLMs) directly on devices brings opportunities to improve latency and privacy but also introduces unique challenges for accuracy and memory. We introduce CAMPHOR, an innovative on-device SLM multi-agent framework designed to handle multiple user inputs and reason over personal context locally, ensuring privacy is maintained. CAMPHOR employs a hierarchical architecture where a high-order reasoning agent decomposes complex tasks and coordinates expert agents responsible for personal context retrieval, tool interaction, and dynamic plan generation. By implementing parameter sharing across agents and leveraging prompt compression, we significantly reduce model size, latency, and memory usage. To validate our approach, we present a novel dataset capturing multi-agent task trajectories centered on personalized mobile assistant use-cases. Our experiments reveal that fine-tuned SLM agents not only surpass closed-source LLMs in task completion F1 by~35\% but also eliminate the need for server-device communication, all while enhancing privacy.</li>
<li><strong>摘要：</strong>虽然服务器端大型语言模型 (LLM) 在函数调用和复杂推理方面表现出色，但直接在设备上部署小型语言模型 (SLM) 带来了改善延迟和隐私的机会，但也带来了准确性和内存方面的独特挑战。我们推出了 CAMPHOR，这是一种创新的设备 SLM 多代理框架，旨在处理多个用户输入并在本地推理个人背景，确保隐私得到维护。CAMPHOR 采用分层架构，其中高阶推理代理分解复杂任务并协调负责个人背景检索、工具交互和动态计划生成的专家代理。通过实现代理之间的参数共享并利用快速压缩，我们显着减少了模型大小、延迟和内存使用量。为了验证我们的方法，我们提出了一个新颖的数据集，以个性化移动助手用例为中心捕获多代理任务轨迹。我们的实验表明，经过微调的 SLM 代理不仅在任务完成 F1 方面超过闭源 LLM 约 35\%，而且还消除了对服务器设备通信的需求，同时增强了隐私。</li>
</ul>

<h3>Title: FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs' Responsiveness to Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Youquan Li, Miao Zheng, Fan Yang, Guosheng Dong, Bin Cui, Weipeng Chen, Zenan Zhou, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09412">https://arxiv.org/abs/2410.09412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09412">https://arxiv.org/pdf/2410.09412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09412]] FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs' Responsiveness to Human Feedback(https://arxiv.org/abs/2410.09412)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Human feedback is crucial in the interactions between humans and Large Language Models (LLMs). However, existing research primarily focuses on benchmarking LLMs in single-turn dialogues. Even in benchmarks designed for multi-turn dialogues, the user inputs are often independent, neglecting the nuanced and complex nature of human feedback within real-world usage scenarios. To fill this research gap, we introduce FB-Bench, a fine-grained, multi-task benchmark designed to evaluate LLMs' responsiveness to human feedback in real-world usage scenarios. Drawing from the two main interaction scenarios, FB-Bench comprises 734 meticulously curated samples, encompassing eight task types, five deficiency types of response, and nine feedback types. We extensively evaluate a broad array of popular LLMs, revealing significant variations in their performance across different interaction scenarios. Further analysis indicates that task, human feedback, and deficiencies of previous responses can also significantly impact LLMs' responsiveness. Our findings underscore both the strengths and limitations of current models, providing valuable insights and directions for future research. Both the toolkits and the dataset of FB-Bench are available at this https URL.</li>
<li><strong>摘要：</strong>人类反馈在人类与大型语言模型 (LLM) 的交互中至关重要。然而，现有的研究主要侧重于在单轮对话中对 LLM 进行基准测试。即使在为多轮对话设计的基准测试中，用户输入也往往是独立的，忽略了真实使用场景中人类反馈的细微和复杂性。为了填补这一研究空白，我们引入了 FB-Bench，这是一个细粒度、多任务基准测试，旨在评估 LLM 在真实使用场景中对人类反馈的响应能力。从两个主要交互场景中汲取灵感，FB-Bench 包含 734 个精心策划的样本，涵盖八种任务类型、五种缺陷类型的响应和九种反馈类型。我们对各种流行的 LLM 进行了广泛评估，揭示了它们在不同交互场景中的表现存在显著差异。进一步的分析表明，任务、人类反馈和先前响应的缺陷也会显著影响 LLM 的响应能力。我们的研究结果强调了当前模型的优势和局限性，为未来的研究提供了宝贵的见解和方向。 FB-Bench 的工具包和数据集均可通过此 https URL 获得。</li>
</ul>

<h3>Title: Beyond Exact Match: Semantically Reassessing Event Extraction by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yi-Fan Lu, Xian-Ling Mao, Tian Lan, Chen Xu, Heyan Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09418">https://arxiv.org/abs/2410.09418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09418">https://arxiv.org/pdf/2410.09418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09418]] Beyond Exact Match: Semantically Reassessing Event Extraction by Large Language Models(https://arxiv.org/abs/2410.09418)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Event extraction has gained extensive research attention due to its broad range of applications. However, the current mainstream evaluation method for event extraction relies on token-level exact match, which misjudges numerous semantic-level correct cases. This reliance leads to a significant discrepancy between the evaluated performance of models under exact match criteria and their real performance. To address this problem, we propose RAEE, an automatic evaluation framework that accurately assesses event extraction results at semantic-level instead of token-level. Specifically, RAEE leverages Large Language Models (LLMs) as automatic evaluation agents, incorporating chain-of-thought prompting and an adaptive mechanism to achieve interpretable and adaptive evaluations for precision and recall of triggers and arguments. Extensive experimental results demonstrate that: (1) RAEE achieves a very high correlation with the human average; (2) after reassessing 14 models, including advanced LLMs, on 10 datasets, there is a significant performance gap between exact match and RAEE. The exact match evaluation significantly underestimates the performance of existing event extraction models, particularly underestimating the capabilities of LLMs; (3) fine-grained analysis under RAEE evaluation reveals insightful phenomena worth further exploration. The evaluation toolkit of our proposed RAEE will be publicly released.</li>
<li><strong>摘要：</strong>事件抽取由于其广泛的应用范围而得到了广泛的研究关注。然而，目前主流的事件抽取评估方法依赖于token级别的精确匹配，而这种方法会误判大量语义级别的正确情况。这种依赖导致模型在精确匹配标准下的评估性能与其真实性能之间存在显著差异。针对这一问题，我们提出了RAEE，这是一个自动评估框架，可以在语义级别而不是token级别准确评估事件抽取结果。具体而言，RAEE利用大型语言模型(LLM)作为自动评估代理，结合思路链提示和自适应机制，实现对触发器和论据的准确率和召回率的可解释和自适应评估。大量实验结果表明：(1) RAEE与人类平均水平具有非常高的相关性；(2)在10个数据集上重新评估14个模型（包括高级LLM）后，精确匹配与RAEE之间存在显著的性能差距。精确匹配评估显著低估了现有事件提取模型的性能，尤其是低估了 LLM 的能力；（3）RAEE 评估下的细粒度分析揭示了值得进一步探索的深刻现象。我们提出的 RAEE 评估工具包将公开发布。</li>
</ul>

<h3>Title: FlatQuant: Flatness Matters for LLM Quantization</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Sun, Ruikang Liu, Haoli Bai, Han Bao, Kang Zhao, Yuening Li, Jiaxin Hu, Xianzhi Yu, Lu Hou, Chun Yuan, Xin Jiang, Wulong Liu, Jun Yao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09426">https://arxiv.org/abs/2410.09426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09426">https://arxiv.org/pdf/2410.09426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09426]] FlatQuant: Flatness Matters for LLM Quantization(https://arxiv.org/abs/2410.09426)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still remain steep and outspread. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach to enhance flatness of weights and activations. Our approach identifies optimal affine transformations tailored to each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead, we apply Kronecker decomposition to the transformation matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments show that FlatQuant sets up a new state-of-the-art quantization benchmark. For instance, it achieves less than $\textbf{1}\%$ accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by $\textbf{7.5}\%$. For inference latency, FlatQuant reduces the slowdown induced by pre-quantization transformation from 0.26x of QuaRot to merely $\textbf{0.07x}$, bringing up to $\textbf{2.3x}$ speedup for prefill and $\textbf{1.7x}$ speedup for decoding, respectively. Code is available at: \url{this https URL}.</li>
<li><strong>摘要：</strong>最近，量化已广泛用于大型语言模型 (LLM) 的压缩和加速。由于 LLM 中存在异常值，因此必须平坦化权重和激活，以使用等间距量化点最小化量化误差。先前的研究探索了各种预量化变换以抑制异常值，例如每通道缩放和 Hadamard 变换。然而，我们观察到这些变换后的权重和激活仍然可能保持陡峭和扩散。在本文中，我们提出了 FlatQuant（快速且可学习的仿射变换），这是一种新的训练后量化方法，可增强权重和激活的平坦度。我们的方法确定了针对每个线性层量身定制的最佳仿射变换，并通过轻量级目标在数小时内进行校准。为了减少运行时开销，我们对变换矩阵应用 Kronecker 分解，并将 FlatQuant 中的所有操作融合到单个内核中。大量实验表明，FlatQuant 建立了新的最先进的量化基准。例如，它在 LLaMA-3-70B 模型上对 W4A4 量化的准确率下降不到 $\textbf{1}\%$，比 SpinQuant 高出 $\textbf{7.5}\%$。对于推理延迟，FlatQuant 将预量化变换引起的减速从 QuaRot 的 0.26 倍降低到仅仅 $\textbf{0.07 倍}$，分别使预填充速度提高 $\textbf{2.3 倍}$ 和解码速度提高 $\textbf{1.7 倍}$。代码可在以下位置获取：\url{此 https URL}。</li>
</ul>

<h3>Title: Solving the Challenge Set without Solving the Task: On Winograd Schemas as a Test of Pronominal Coreference Resolution</h3>
<ul>
<li><strong>Authors: </strong>Ian Porada, Jackie Chi Kit Cheung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09448">https://arxiv.org/abs/2410.09448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09448">https://arxiv.org/pdf/2410.09448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09448]] Solving the Challenge Set without Solving the Task: On Winograd Schemas as a Test of Pronominal Coreference Resolution(https://arxiv.org/abs/2410.09448)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Challenge sets such as the Winograd Schema Challenge (WSC) are used to benchmark systems' ability to resolve ambiguities in natural language. If one assumes as in existing work that solving a given challenge set is at least as difficult as solving some more general task, then high performance on the challenge set should indicate high performance on the general task overall. However, we show empirically that this assumption of difficulty does not always hold. In particular, we demonstrate that despite the strong performance of prompted language models (LMs) on the WSC and its variants, these same modeling techniques perform relatively poorly at resolving certain pronominal ambiguities attested in OntoNotes and related datasets that are perceived to be easier. Motivated by these findings, we propose a method for ensembling a prompted LM with a supervised, task-specific system that is overall more accurate at resolving pronominal coreference across datasets. Finally, we emphasize that datasets involving the same linguistic phenomenon draw on distinct, but overlapping, capabilities, and evaluating on any one dataset alone does not provide a complete picture of a system's overall capability.</li>
<li><strong>摘要：</strong>诸如 Winograd Schema Challenge (WSC) 之类的挑战集用于衡量系统解决自然语言歧义的能力。如果像现有研究一样假设解决给定挑战集至少与解决某些更一般的任务一样困难，那么挑战集上的高性能应该表明总体上一般任务上的高性能。然而，我们通过经验表明，这种难度假设并不总是成立。具体而言，我们证明，尽管提示语言模型 (LM) 在 WSC 及其变体上表现强劲，但这些相同的建模技术在解决 OntoNotes 和相关数据集中证明的某些代词歧义方面表现相对较差，而这些数据集被认为更容易解决。受这些发现的启发，我们提出了一种将提示语言模型与监督的任务特定系统集成的方法，该系统在解决跨数据集的代词共指方面总体上更准确。最后，我们要强调的是，涉及相同语言现象的数据集具有不同但重叠的功能，单独评估任何一个数据集都不能完整地反映出一个系统的整体功能。</li>
</ul>

<h3>Title: Interpretable Video based Stress Detection with Self-Refine Chain-of-thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yi Dai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09449">https://arxiv.org/abs/2410.09449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09449">https://arxiv.org/pdf/2410.09449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09449]] Interpretable Video based Stress Detection with Self-Refine Chain-of-thought Reasoning(https://arxiv.org/abs/2410.09449)</code><input type="text"></li>
<li><strong>Keywords: </strong>chain-of-thought</a></li>
<li><strong>Abstract: </strong>Stress detection is a critical area of research with significant implications for health monitoring and intervention systems. In this paper, we propose a novel interpretable approach for video-based stress detection, leveraging self-refine chain-of-thought reasoning to enhance both accuracy and transparency in decision-making processes. Our method focuses on extracting subtle behavioral and physiological cues from video sequences that indicate stress levels. By incorporating a chain-of-thought reasoning mechanism, the system refines its predictions iteratively, ensuring that the decision-making process can be traced and explained. The model also learns to self-refine through feedback loops, improving its reasoning capabilities over time. We evaluate our approach on several public and private datasets, demonstrating its superior performance in comparison to traditional video-based stress detection methods. Additionally, we provide comprehensive insights into the interpretability of the model's predictions, making the system highly valuable for applications in both healthcare and human-computer interaction domains.</li>
<li><strong>摘要：</strong>压力检测是一个关键的研究领域，对健康监测和干预系统具有重要意义。在本文中，我们提出了一种基于视频的压力检测的新型可解释方法，利用自我完善的思路链推理来提高决策过程的准确性和透明度。我们的方法侧重于从指示压力水平的视频序列中提取细微的行为和生理线索。通过结合思路链推理机制，系统会迭代地完善其预测，确保可以追踪和解释决策过程。该模型还通过反馈回路学习自我完善，随着时间的推移提高其推理能力。我们在几个公共和私人数据集上评估了我们的方法，证明了它与传统的基于视频的压力检测方法相比具有卓越的性能。此外，我们还对模型预测的可解释性提供了全面的见解，使该系统在医疗保健和人机交互领域的应用中具有很高的价值。</li>
</ul>

<h3>Title: Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Sungkyung Kim, Adam Lee, Junyoung Park, Andrew Chung, Jusang Oh, Jay-Yoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09489">https://arxiv.org/abs/2410.09489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09489">https://arxiv.org/pdf/2410.09489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09489]] Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks(https://arxiv.org/abs/2410.09489)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models have demonstrated enhanced capabilities in visual reasoning tasks by employing additional encoders for aligning different modalities. While the Q-Former has been widely used as a general encoder for aligning several modalities including image, video, audio, and 3D with large language models, previous works on its efficient training and the analysis of its individual components have been limited. In this work, we investigate the effectiveness of parameter efficient fine-tuning (PEFT) the Q-Former using InstructBLIP with visual reasoning benchmarks ScienceQA and IconQA. We observe that applying PEFT to the Q-Former achieves comparable performance to full fine-tuning using under 2% of the trainable parameters. Additionally, we employ AdaLoRA for dynamic parameter budget reallocation to examine the relative importance of the Q-Former's sublayers with 4 different benchmarks. Our findings reveal that the self-attention layers are noticeably more important in perceptual visual-language reasoning tasks, and relative importance of FFN layers depends on the complexity of visual-language patterns involved in tasks. The code is available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型的最新进展表明，通过使用额外的编码器来对齐不同的模态，可以增强视觉推理任务的能力。虽然 Q-Former 已被广泛用作将图像、视频、音频和 3D 等多种模态与大型语言模型对齐的通用编码器，但之前对其有效训练和各个组件分析的研究有限。在这项工作中，我们使用 InstructBLIP 和视觉推理基准 ScienceQA 和 IconQA 研究了参数高效微调 (PEFT) Q-Former 的有效性。我们观察到，将 PEFT 应用于 Q-Former 可实现与使用不到 2% 的可训练参数进行完全微调相当的性能。此外，我们使用 AdaLoRA 进行动态参数预算重新分配，以检查 Q-Former 子层与 4 个不同基准的相对重要性。我们的研究结果表明，自注意力层在感知视觉语言推理任务中明显更为重要，而 FFN 层的相对重要性取决于任务中涉及的视觉语言模式的复杂性。代码可从此 https URL 获取。</li>
</ul>

<h3>Title: AERA Chat: An Interactive Platform for Automated Explainable Student Answer Assessment</h3>
<ul>
<li><strong>Authors: </strong>Jiazheng Li, Artem Bobrov, David West, Cesare Aloisi, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09507">https://arxiv.org/abs/2410.09507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09507">https://arxiv.org/pdf/2410.09507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09507]] AERA Chat: An Interactive Platform for Automated Explainable Student Answer Assessment(https://arxiv.org/abs/2410.09507)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Generating rationales that justify scoring decisions has emerged as a promising approach to enhance explainability in the development of automated scoring systems. However, the scarcity of publicly available rationale data and the high cost of annotation have resulted in existing methods typically relying on noisy rationales generated by large language models (LLMs). To address these challenges, we have developed AERA Chat, an interactive platform, to provide visually explained assessment of student answers and streamline the verification of rationales. Users can input questions and student answers to obtain automated, explainable assessment results from LLMs. The platform's innovative visualization features and robust evaluation tools make it useful for educators to assist their marking process, and for researchers to evaluate assessment performance and quality of rationales generated by different LLMs, or as a tool for efficient annotation. We evaluated three rationale generation approaches on our platform to demonstrate its capability.</li>
<li><strong>摘要：</strong>生成合理性理由来证明评分决策的合理性已成为一种有前途的方法，可提高自动评分系统开发的可解释性。然而，由于公开的合理性数据稀缺和注释成本高昂，现有方法通常依赖于大型语言模型 (LLM) 生成的嘈杂合理性理由。为了应对这些挑战，我们开发了一个交互式平台 AERA Chat，以提供对学生答案的视觉解释评估并简化理由的验证。用户可以输入问题和学生答案，以获得来自 LLM 的自动化、可解释的评估结果。该平台的创新可视化功能和强大的评估工具使其对教育工作者有帮助，可以帮助研究人员评估不同 LLM 生成的评估绩效和理由的质量，或作为高效注释的工具。我们在我们的平台上评估了三种理由生成方法，以证明其能力。</li>
</ul>

<h3>Title: CollabEdit: Towards Non-destructive Collaborative Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Jiamu Zheng, Jinghuai Zhang, Tianyu Du, Xuhong Zhang, Jianwei Yin, Tao Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09508">https://arxiv.org/abs/2410.09508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09508">https://arxiv.org/pdf/2410.09508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09508]] CollabEdit: Towards Non-destructive Collaborative Knowledge Editing(https://arxiv.org/abs/2410.09508)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Collaborative learning of large language models (LLMs) has emerged as a new paradigm for utilizing private data from different parties to guarantee efficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also garnered increased attention due to its ability to manipulate the behaviors of LLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits of multiple parties are aggregated in a privacy-preserving and continual manner) unexamined. To this end, this manuscript dives into the first investigation of collaborative KE, in which we start by carefully identifying the unique three challenges therein, including knowledge overlap, knowledge conflict, and knowledge forgetting. We then propose a non-destructive collaborative KE framework, COLLABEDIT, which employs a novel model merging mechanism to mimic the global KE behavior while preventing the severe performance drop. Extensive experiments on two canonical datasets demonstrate the superiority of COLLABEDIT compared to other destructive baselines, and results shed light on addressing three collaborative KE challenges and future applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的协作学习已成为一种利用来自不同方的私人数据来保证效率和隐私的新范式。同时，LLM 的知识编辑 (KE) 也因其能够明确操纵 LLM 的行为而受到越来越多的关注，但协作 KE 的情况（其中多方的知识编辑以隐私保护和连续的方式聚合）尚未得到检验。为此，本文深入研究了协作 KE 的首次调查，我们首先仔细识别其中独特的三个挑战，包括知识重叠、知识冲突和知识遗忘。然后，我们提出了一个非破坏性的协作 KE 框架 COLLABEDIT，它采用一种新颖的模型合并机制来模拟全局 KE 行为，同时防止严重的性能下降。在两个规范数据集上进行的大量实验证明了 COLLABEDIT 相对于其他破坏性基线的优越性，结果为解决三个协作 KE 挑战和未来应用提供了启示。</li>
</ul>

<h3>Title: LexSumm and LexT5: Benchmarking and Modeling Legal Summarization Tasks in English</h3>
<ul>
<li><strong>Authors: </strong>T.Y.S.S. Santosh, Cornelius Weiss, Matthias Grabmair</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09527">https://arxiv.org/abs/2410.09527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09527">https://arxiv.org/pdf/2410.09527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09527]] LexSumm and LexT5: Benchmarking and Modeling Legal Summarization Tasks in English(https://arxiv.org/abs/2410.09527)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In the evolving NLP landscape, benchmarks serve as yardsticks for gauging progress. However, existing Legal NLP benchmarks only focus on predictive tasks, overlooking generative tasks. This work curates LexSumm, a benchmark designed for evaluating legal summarization tasks in English. It comprises eight English legal summarization datasets, from diverse jurisdictions, such as the US, UK, EU and India. Additionally, we release LexT5, legal oriented sequence-to-sequence model, addressing the limitation of the existing BERT-style encoder-only models in the legal domain. We assess its capabilities through zero-shot probing on LegalLAMA and fine-tuning on LexSumm. Our analysis reveals abstraction and faithfulness errors even in summaries generated by zero-shot LLMs, indicating opportunities for further improvements. LexSumm benchmark and LexT5 model are available at this https URL.</li>
<li><strong>摘要：</strong>在不断发展的 NLP 领域，基准测试是衡量进展的标准。然而，现有的法律 NLP 基准测试仅关注预测任务，而忽略了生成任务。这项工作整理了 LexSumm，这是一个用于评估英语法律摘要任务的基准测试。它包含八个英语法律摘要数据集，来自美国、英国、欧盟和印度等不同司法管辖区。此外，我们发布了面向法律的序列到序列模型 LexT5，解决了法律领域现有 BERT 样式编码器模型的局限性。我们通过对 LegalLAMA 进行零样本探测和对 LexSumm 进行微调来评估其能力。我们的分析表明，即使在零样本 LLM 生成的摘要中也存在抽象和忠实性错误，这表明还有进一步改进的机会。LexSumm 基准测试和 LexT5 模型可在此 https URL 上获得。</li>
</ul>

<h3>Title: LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09541">https://arxiv.org/abs/2410.09541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09541">https://arxiv.org/pdf/2410.09541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09541]] LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning(https://arxiv.org/abs/2410.09541)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 有时在知识密集型任务上表现不佳，常识推理就是其中之一。研究人员通常通过从知识图谱中检索相关知识或采用自我增强方法在 LLM 中引出知识来解决这些问题。然而，嘈杂的知识和无效推理问题阻碍了他们准确回答问题的能力。为此，我们提出了一种名为在大型语言模型中引出、过滤和整合知识的新方法 (LINKED)。在其中，我们设计了一个奖励模型来过滤掉嘈杂的知识，并采用边际一致性推理模块来减少无效推理。通过对两个复杂的常识推理基准进行全面的实验，我们的方法优于 SOTA 基线（准确率提高了 9.0%）。此外，为了衡量注入知识的积极和消极影响，我们为知识增强工作提出了一个新的指标，称为有效性保存分数。最后，通过大量的实验，我们进行了深入分析，并发现了许多关于 LLM 在常识推理任务中的有意义的结论。</li>
</ul>

<h3>Title: MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiachun Li, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09542">https://arxiv.org/abs/2410.09542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09542">https://arxiv.org/pdf/2410.09542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09542]] MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models(https://arxiv.org/abs/2410.09542)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Inductive reasoning is an essential capability for large language models (LLMs) to achieve higher intelligence, which requires the model to generalize rules from observed facts and then apply them to unseen examples. We present {\scshape Mirage}, a synthetic dataset that addresses the limitations of previous work, specifically the lack of comprehensive evaluation and flexible test data. In it, we evaluate LLMs' capabilities in both the inductive and deductive stages, allowing for flexible variation in input distribution, task scenario, and task difficulty to analyze the factors influencing LLMs' inductive reasoning. Based on these multi-faceted evaluations, we demonstrate that the LLM is a poor rule-based reasoner. In many cases, when conducting inductive reasoning, they do not rely on a correct rule to answer the unseen case. From the perspectives of different prompting methods, observation numbers, and task forms, models tend to consistently conduct correct deduction without correct inductive rules. Besides, we find that LLMs are good neighbor-based reasoners. In the inductive reasoning process, the model tends to focus on observed facts that are close to the current test example in feature space. By leveraging these similar examples, the model maintains strong inductive capabilities within a localized region, significantly improving its deductive performance.</li>
<li><strong>摘要：</strong>归纳推理是大型语言模型（LLM）实现更高智能的必备能力，它要求模型从观察到的事实中概括规则，然后将其应用于未见过的例子中。我们提出了一个合成数据集{\scshape Mirage}，它解决了以前工作的局限性，特别是缺乏全面的评估和灵活的测试数据。在其中，我们评估了LLM在归纳和演绎阶段的能力，允许输入分布、任务场景和任务难度的灵活变化，以分析影响LLM归纳推理的因素。基于这些多方面的评估，我们证明了LLM是一个糟糕的基于规则的推理器。在许多情况下，当它们进行归纳推理时，它们并不依赖正确的规则来回答未见过的案例。从不同的提示方法、观察数量和任务形式的角度来看，模型倾向于在没有正确归纳规则的情况下始终进行正确的推理。此外，我们发现LLM是良好的基于​​邻居的推理器。在归纳推理过程中，模型倾向于关注特征空间中与当前测试样例接近的观测事实，利用这些相似样例，模型在局部区域内保持较强的归纳能力，从而显著提升推理性能。</li>
</ul>

<h3>Title: Extended Japanese Commonsense Morality Dataset with Masked Token and Label Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Takumi Ohashi, Tsubasa Nakagawa, Hitoshi Iyatomi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09564">https://arxiv.org/abs/2410.09564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09564">https://arxiv.org/pdf/2410.09564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09564]] Extended Japanese Commonsense Morality Dataset with Masked Token and Label Enhancement(https://arxiv.org/abs/2410.09564)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Rapid advancements in artificial intelligence (AI) have made it crucial to integrate moral reasoning into AI systems. However, existing models and datasets often overlook regional and cultural differences. To address this shortcoming, we have expanded the JCommonsenseMorality (JCM) dataset, the only publicly available dataset focused on Japanese morality. The Extended JCM (eJCM) has grown from the original 13,975 sentences to 31,184 sentences using our proposed sentence expansion method called Masked Token and Label Enhancement (MTLE). MTLE selectively masks important parts of sentences related to moral judgment and replaces them with alternative expressions generated by a large language model (LLM), while re-assigning appropriate labels. The model trained using our eJCM achieved an F1 score of 0.857, higher than the scores for the original JCM (0.837), ChatGPT one-shot classification (0.841), and data augmented using AugGPT, a state-of-the-art augmentation method (0.850). Specifically, in complex moral reasoning tasks unique to Japanese culture, the model trained with eJCM showed a significant improvement in performance (increasing from 0.681 to 0.756) and achieved a performance close to that of GPT-4 Turbo (0.787). These results demonstrate the validity of the eJCM dataset and the importance of developing models and datasets that consider the cultural context.</li>
<li><strong>摘要：</strong>人工智能 (AI) 的快速发展使得将道德推理融入 AI 系统变得至关重要。然而，现有的模型和数据集往往忽视了地区和文化差异。为了解决这一缺陷，我们扩展了 JCommonsenseMorality (JCM) 数据集，这是唯一一个关注日本道德的公开数据集。使用我们提出的句子扩展方法（称为屏蔽标记和标签增强 (MTLE)），扩展 JCM (eJCM) 已从原来的 13,975 个句子增加到 31,184 个句子。MTLE 有选择地屏蔽与道德判断相关的句子的重要部分，并用大型语言模型 (LLM) 生成的替代表达替换它们，同时重新分配适当的标签。使用我们的 eJCM 训练的模型获得了 0.857 的 F1 分数，高于原始 JCM (0.837)、ChatGPT 一次性分类 (0.841) 和使用最先进的增强方法 AugGPT 增强的数据 (0.850) 的分数。具体来说，在日本文化独有的复杂道德推理任务中，使用 eJCM 训练的模型表现出了显著的性能提升（从 0.681 提升到 0.756），并取得了接近 GPT-4 Turbo（0.787）的性能。这些结果证明了 eJCM 数据集的有效性以及开发考虑文化背景的模型和数据集的重要性。</li>
</ul>

<h3>Title: Are You Human? An Adversarial Benchmark to Expose LLMs</h3>
<ul>
<li><strong>Authors: </strong>Gilad Gressel, Rahul Pankajakshan, Yisroel Mirsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09569">https://arxiv.org/abs/2410.09569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09569">https://arxiv.org/pdf/2410.09569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09569]] Are You Human? An Adversarial Benchmark to Expose LLMs(https://arxiv.org/abs/2410.09569)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated an alarming ability to impersonate humans in conversation, raising concerns about their potential misuse in scams and deception. Humans have a right to know if they are conversing to an LLM. We evaluate text-based prompts designed as challenges to expose LLM imposters in real-time. To this end we compile and release an open-source benchmark dataset that includes 'implicit challenges' that exploit an LLM's instruction-following mechanism to cause role deviation, and 'exlicit challenges' that test an LLM's ability to perform simple tasks typically easy for humans but difficult for LLMs. Our evaluation of 9 leading models from the LMSYS leaderboard revealed that explicit challenges successfully detected LLMs in 78.4% of cases, while implicit challenges were effective in 22.9% of instances. User studies validate the real-world applicability of our methods, with humans outperforming LLMs on explicit challenges (78% vs 22% success rate). Our framework unexpectedly revealed that many study participants were using LLMs to complete tasks, demonstrating its effectiveness in detecting both AI impostors and human misuse of AI tools. This work addresses the critical need for reliable, real-time LLM detection methods in high-stakes conversations.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已展现出在对话中模仿人类的惊人能力，这引发了人们对其可能被滥用于诈骗和欺骗的担忧。人类有权知道他们是否正在与 LLM 交谈。我们评估旨在挑战的基于文本的提示，以实时揭露 LLM 冒名顶替者。为此，我们编制并发布了一个开源基准数据集，其中包括利用 LLM 的指令遵循机制来引起角色偏差的“隐式挑战”和测试 LLM 执行简单任务的能力的“显式挑战”，这些任务通常对人类来说很容易，但对 LLM 来说却很难。我们对 LMSYS 排行榜上的 9 个领先模型的评估显示，显式挑战在 78.4% 的案例中成功检测到了 LLM，而隐式挑战在 22.9% 的案例中有效。用户研究验证了我们方法在现实世界中的适用性，人类在显式挑战中的表现优于 LLM（成功率为 78% vs 22%）。我们的框架意外地发现，许多研究参与者正在使用 LLM 来完成任务，这证明了它在检测 AI 冒名顶替者和人类滥用 AI 工具方面的有效性。这项工作解决了高风险对话中对可靠、实时 LLM 检测方法的迫切需求。</li>
</ul>

<h3>Title: The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Subhankar Maity, Aniket Deroy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09576">https://arxiv.org/abs/2410.09576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09576">https://arxiv.org/pdf/2410.09576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09576]] The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models(https://arxiv.org/abs/2410.09576)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) and generative AI have revolutionized natural language processing (NLP), offering unprecedented capabilities in education. This chapter explores the transformative potential of LLMs in automated question generation and answer assessment. It begins by examining the mechanisms behind LLMs, emphasizing their ability to comprehend and generate human-like text. The chapter then discusses methodologies for creating diverse, contextually relevant questions, enhancing learning through tailored, adaptive strategies. Key prompting techniques, such as zero-shot and chain-of-thought prompting, are evaluated for their effectiveness in generating high-quality questions, including open-ended and multiple-choice formats in various languages. Advanced NLP methods like fine-tuning and prompt-tuning are explored for their role in generating task-specific questions, despite associated costs. The chapter also covers the human evaluation of generated questions, highlighting quality variations across different methods and areas for improvement. Furthermore, it delves into automated answer assessment, demonstrating how LLMs can accurately evaluate responses, provide constructive feedback, and identify nuanced understanding or misconceptions. Examples illustrate both successful assessments and areas needing improvement. The discussion underscores the potential of LLMs to replace costly, time-consuming human assessments when appropriately guided, showcasing their advanced understanding and reasoning capabilities in streamlining educational processes.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 和生成式 AI 彻底改变了自然语言处理 (NLP)，为教育领域提供了前所未有的能力。本章探讨了 LLM 在自动生成问题和答案评估方面的变革潜力。它首先研究 LLM 背后的机制，强调它们理解和生成类似人类的文本的能力。然后，本章讨论了创建多样化、上下文相关的问题的方法，通过量身定制的自适应策略增强学习。对关键的提示技术（例如零样本和思路链提示）在生成高质量问题方面的有效性进行了评估，包括各种语言的开放式和多项选择格式。尽管有相关成本，但仍探索了微调和提示调整等高级 NLP 方法在生成特定于任务的问题方面的作用。本章还介绍了对生成问题的人工评估，强调了不同方法之间的质量差异和需要改进的领域。此外，它深入研究了自动答案评估，展示了 LLM 如何准确评估答案、提供建设性反馈并识别细微的理解或误解。示例说明了成功的评估和需要改进的领域。讨论强调了 LLM 在适当指导下取代昂贵、耗时的人工评估的潜力，展示了其在简化教育流程方面的高级理解和推理能力。</li>
</ul>

<h3>Title: SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Hanwen Du, Bo Peng, Xia Ning</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09580">https://arxiv.org/abs/2410.09580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09580">https://arxiv.org/pdf/2410.09580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09580]] SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search(https://arxiv.org/abs/2410.09580)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Conversational Recommender Systems (CRS) proactively engage users in interactive dialogues to elicit user preferences and provide personalized recommendations. Existing methods train Reinforcement Learning (RL)-based agent with greedy action selection or sampling strategy, and may suffer from suboptimal conversational planning. To address this, we present a novel Monte Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a conversational agent (S-agent) and a conversational planner (S-planner). S-planner builds a conversational search tree with MCTS based on the initial actions proposed by S-agent to find conversation plans. The best conversation plans from S-planner are used to guide the training of S-agent, creating a self-training loop where S-agent can iteratively improve its capability for conversational planning. Furthermore, we propose an efficient variant SAPIENT-e for trade-off between training efficiency and performance. Extensive experiments on four benchmark datasets validate the effectiveness of our approach, showing that SAPIENT outperforms the state-of-the-art baselines.</li>
<li><strong>摘要：</strong>对话推荐系统 (CRS) 主动吸引用户参与互动对话，以引出用户偏好并提供个性化推荐。现有方法使用贪婪的动作选择或采样策略来训练基于强化学习 (RL) 的代理，并且可能遭受次优对话规划的影响。为了解决这个问题，我们提出了一种基于蒙特卡洛树搜索 (MCTS) 的新型 CRS 框架 SAPIENT。SAPIENT 由对话代理 (S-agent) 和对话规划器 (S-planner) 组成。S-planner 根据 S-agent 提出的初始动作使用 MCTS 构建对话搜索树以查找对话计划。S-planner 中的最佳对话计划用于指导 S-agent 的训练，从而创建一个自训练循环，S-agent 可以在其中迭代地提高其对话规划能力。此外，我们提出了一种有效的变体 SAPIENT-e，以在训练效率和性能之间进行权衡。在四个基准数据集上进行的大量实验验证了我们方法的有效性，表明 SAPIENT 的表现优于最先进的基线。</li>
</ul>

<h3>Title: Toward General Instruction-Following Alignment for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, Ji-Rong Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09584">https://arxiv.org/abs/2410.09584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09584">https://arxiv.org/pdf/2410.09584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09584]] Toward General Instruction-Following Alignment for Retrieval-Augmented Generation(https://arxiv.org/abs/2410.09584)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at this https URL.</li>
<li><strong>摘要：</strong>遵循自然指令对于检索增强生成 (RAG) 系统的有效应用至关重要。尽管大型语言模型 (LLM) 取得了最新进展，但评估和改进 RAG 领域内的指令跟随 (IF) 对齐的研究仍然有限。为了解决这个问题，我们提出了 VIF-RAG，这是 RAG 系统中第一个用于指令跟随对齐的自动化、可扩展且可验证的合成管道。我们首先手动制作一组最小的原子指令 (<100)，并开发组合规则来合成和验证种子集的复杂指令。然后，我们使用监督模型进行指令重写，同时生成代码以通过 Python 执行器自动验证指令质量。最后，我们将这些指令与广泛的 RAG 和一般数据样本集成，通过自动化流程扩展到高质量的 VIF-RAG-QA 数据集 (>100k)。为了进一步弥补 RAG 系统在指令跟随自动评估方面的差距，我们引入了 FollowRAG Benchmark，其中包括大约 3K 个测试样本，涵盖 22 类通用指令约束和四个知识密集型 QA 数据集。由于其强大的管道设计，FollowRAG 可以与不同的 RAG 基准无缝集成。使用 FollowRAG 和八个广泛使用的 IF 和 LLM 基础能力基准，我们证明 VIF-RAG 显著提高了 LLM 在广泛通用指令约束下的性能，同时有效地利用了其在 RAG 场景中的功能。进一步的分析为在 RAG 系统中实现 IF 对齐提供了实用的见解。我们的代码和数据集在此 https URL 上发布。</li>
</ul>

<h3>Title: Transformer-based Language Models for Reasoning in the Description Logic ALCQ</h3>
<ul>
<li><strong>Authors: </strong>Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09613">https://arxiv.org/abs/2410.09613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09613">https://arxiv.org/pdf/2410.09613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09613]] Transformer-based Language Models for Reasoning in the Description Logic ALCQ(https://arxiv.org/abs/2410.09613)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>Recent advancements in transformer-based language models have sparked research into their logical reasoning capabilities. Most of the benchmarks used to evaluate these models are simple: generated from short (fragments of) first-order logic sentences with only a few logical operators and quantifiers. We construct the natural language dataset, DELTA$_D$, using the expressive description logic language $\mathcal{ALCQ}$. DELTA$_D$ comprises 384K examples and increases in two dimensions: i) reasoning depth, and ii) linguistic complexity. In this way, we systematically investigate the logical reasoning capabilities of a supervised fine-tuned DeBERTa-based model and two large language models (GPT-3.5, GPT-4) with few-shot prompting. We show that the DeBERTa-based model fine-tuned on our dataset can master the entailment checking task. Moreover, the performance of GPTs can improve significantly even when a small number of samples is provided (9 shots). We open-source our code and datasets.</li>
<li><strong>摘要：</strong>基于 Transformer 的语言模型的最新进展引发了对其逻辑推理能力的研究。用于评估这些模型的大多数基准都很简单：由短（片段）一阶逻辑句子生成，仅包含少量逻辑运算符和量词。我们使用表达性描述逻辑语言 $\mathcal{ALCQ}$ 构建自然语言数据集 DELTA$_D$。DELTA$_D$ 包含 384K 个示例，并在两个维度上增加：i）推理深度和 ii）语言复杂性。通过这种方式，我们系统地研究了基于 DeBERTa 的监督微调模型和两个大型语言模型（GPT-3.5、GPT-4）的逻辑推理能力，并进行了少量提示。我们表明，在我们的数据集上微调的基于 DeBERTa 的模型可以掌握蕴涵检查任务。此外，即使提供少量样本（9 个样本），GPT 的性能也可以显着提高。我们开源了我们的代码和数据集。</li>
</ul>

<h3>Title: Quebec Automobile Insurance Question-Answering With Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>David Beauchemin, Zachary Gagnon, Ricahrd Khoury</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09623">https://arxiv.org/abs/2410.09623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09623">https://arxiv.org/pdf/2410.09623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09623]] Quebec Automobile Insurance Question-Answering With Retrieval-Augmented Generation(https://arxiv.org/abs/2410.09623)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) perform outstandingly in various downstream tasks, and the use of the Retrieval-Augmented Generation (RAG) architecture has been shown to improve performance for legal question answering (Nuruzzaman and Hussain, 2020; Louis et al., 2024). However, there are limited applications in insurance questions-answering, a specific type of legal document. This paper introduces two corpora: the Quebec Automobile Insurance Expertise Reference Corpus and a set of 82 Expert Answers to Layperson Automobile Insurance Questions. Our study leverages both corpora to automatically and manually assess a GPT4-o, a state-of-the-art LLM, to answer Quebec automobile insurance questions. Our results demonstrate that, on average, using our expertise reference corpus generates better responses on both automatic and manual evaluation metrics. However, they also highlight that LLM QA is unreliable enough for mass utilization in critical areas. Indeed, our results show that between 5% to 13% of answered questions include a false statement that could lead to customer misunderstanding.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种下游任务中表现出色，并且使用检索增强生成 (RAG) 架构已被证明可以提高法律问答的性能（Nuruzzaman 和 Hussain，2020 年；Louis 等人，2024 年）。然而，在保险问答（一种特定类型的法律文件）中的应用有限。本文介绍了两个语料库：魁北克汽车保险专业知识参考语料库和一组 82 个专家对普通人汽车保险问题的回答。我们的研究利用这两个语料库自动和手动评估 GPT4-o（一种最先进的 LLM）来回答魁北克汽车保险问题。我们的结果表明，平均而言，使用我们的专业知识参考语料库可以在自动和手动评估指标上生成更好的响应。然而，它们也强调 LLM QA 不够可靠，无法在关键领域大规模使用。事实上，我们的结果显示，5% 到 13% 的已回答问题包含可能导致客户误解的错误陈述。</li>
</ul>

<h3>Title: Enhanced Electronic Health Records Text Summarization Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruvarashe Madzime, Clement Nyirenda</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09628">https://arxiv.org/abs/2410.09628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09628">https://arxiv.org/pdf/2410.09628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09628]] Enhanced Electronic Health Records Text Summarization Using Large Language Models(https://arxiv.org/abs/2410.09628)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The development of Electronic Health Records summarization systems has revolutionized patient data management. Previous research advanced this field by adapting Large Language Models for clinical tasks, using diverse datasets to generate general EHR summaries. However, clinicians often require specific, focused summaries for quicker insights. This project builds on prior work by creating a system that generates clinician-preferred, focused summaries, improving EHR summarization for more efficient patient care. The proposed system leverages the Google Flan-T5 model to generate tailored EHR summaries based on clinician-specified topics. The approach involved fine-tuning the Flan-T5 model on an EHR question-answering dataset formatted in the Stanford Question Answering Dataset (SQuAD) style, which is a large-scale reading comprehension dataset with questions and answers. Fine-tuning utilized the Seq2SeqTrainer from the Hugging Face Transformers library with optimized hyperparameters. Key evaluation metrics demonstrated promising results: the system achieved an Exact Match (EM) score of 81.81%. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics showed strong performance, with ROUGE-1 at 96.03%, ROUGE-2 at 86.67%, and ROUGE-L at 96.10%. Additionally, the Bilingual Evaluation Understudy (BLEU) score was 63%, reflecting the model's coherence in generating summaries. By enhancing EHR summarization through LLMs, this project supports digital transformation efforts in healthcare, streamlining workflows, and enabling more personalized patient care.</li>
<li><strong>摘要：</strong>电子健康记录摘要系统的开发彻底改变了患者数据管理。先前的研究通过调整大型语言模型以适应临床任务，使用不同的数据集生成一般的 EHR 摘要，推动了该领域的发展。然而，临床医生通常需要特定的、有针对性的摘要才能更快地获得见解。该项目在前期工作的基础上，创建了一个系统，可以生成临床医生喜欢的、有针对性的摘要，从而改进 EHR 摘要，提高患者护理效率。所提出的系统利用 Google Flan-T5 模型根据临床医生指定的主题生成定制的 EHR 摘要。该方法涉及在采用斯坦福问答数据集 (SQuAD) 格式的 EHR 问答数据集上对 Flan-T5 模型进行微调，这是一个包含问题和答案的大规模阅读理解数据集。微调利用了 Hugging Face Transformers 库中的 Seq2SeqTrainer 和优化的超参数。关键评估指标显示了令人鼓舞的结果：该系统实现了 81.81% 的精确匹配 (EM) 分数。 ROUGE（面向回忆的摘要评估替代模型）指标表现出色，其中 ROUGE-1 为 96.03%，ROUGE-2 为 86.67%，ROUGE-L 为 96.10%。此外，双语评估替代模型 (BLEU) 得分为 63%，反映了该模型在生成摘要方面的一致性。通过 LLM 增强 EHR 摘要，该项目支持医疗保健领域的数字化转型工作，简化工作流程，并实现更加个性化的患者护理。</li>
</ul>

<h3>Title: Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Zhang, Wendi Cui, Yiran Huang, Kamalika Das, Sricharan Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09629">https://arxiv.org/abs/2410.09629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09629">https://arxiv.org/pdf/2410.09629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09629]] Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models(https://arxiv.org/abs/2410.09629)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are proficient in capturing factual knowledge across various domains. However, refining their capabilities on previously seen knowledge or integrating new knowledge from external sources remains a significant challenge. In this work, we propose a novel synthetic knowledge ingestion method called Ski, which leverages fine-grained synthesis, interleaved generation, and assemble augmentation strategies to construct high-quality data representations from raw knowledge sources. We then integrate Ski and its variations with three knowledge injection techniques: Retrieval Augmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual Pre-training (CPT) to inject and refine knowledge in language models. Extensive empirical experiments are conducted on various question-answering tasks spanning finance, biomedicine, and open-generation domains to demonstrate that Ski significantly outperforms baseline methods by facilitating effective knowledge injection. We believe that our work is an important step towards enhancing the factual accuracy of LLM outputs by refining knowledge representation and injection capabilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 擅长捕捉各个领域的事实知识。然而，改进它们对先前看到的知识的能力或整合来自外部来源的新知识仍然是一项重大挑战。在这项工作中，我们提出了一种名为 Ski 的新型合成知识摄取方法，该方法利用细粒度合成、交错生成和组装增强策略从原始知识源构建高质量的数据表示。然后，我们将 Ski 及其变体与三种知识注入技术相结合：检索增强生成 (RAG)、监督微调 (SFT) 和持续预训练 (CPT)，以在语言模型中注入和改进知识。在金融、生物医学和开放生成领域的各种问答任务上进行了广泛的实证实验，以证明 Ski 通过促进有效的知识注入而显著优于基线方法。我们相信，我们的工作是通过改进知识表示和注入能力来提高 LLM 输出的事实准确性的重要一步。</li>
</ul>

<h3>Title: Society of Medical Simplifiers</h3>
<ul>
<li><strong>Authors: </strong>Chen Lyu, Gabriele Pergola</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09631">https://arxiv.org/abs/2410.09631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09631">https://arxiv.org/pdf/2410.09631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09631]] Society of Medical Simplifiers(https://arxiv.org/abs/2410.09631)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Medical text simplification is crucial for making complex biomedical literature more accessible to non-experts. Traditional methods struggle with the specialized terms and jargon of medical texts, lacking the flexibility to adapt the simplification process dynamically. In contrast, recent advancements in large language models (LLMs) present unique opportunities by offering enhanced control over text simplification through iterative refinement and collaboration between specialized agents. In this work, we introduce the Society of Medical Simplifiers, a novel LLM-based framework inspired by the "Society of Mind" (SOM) philosophy. Our approach leverages the strengths of LLMs by assigning five distinct roles, i.e., Layperson, Simplifier, Medical Expert, Language Clarifier, and Redundancy Checker, organized into interaction loops. This structure allows the agents to progressively improve text simplification while maintaining the complexity and accuracy of the original content. Evaluations on the Cochrane text simplification dataset demonstrate that our framework is on par with or outperforms state-of-the-art methods, achieving superior readability and content preservation through controlled simplification processes.</li>
<li><strong>摘要：</strong>医学文本简化对于让非专家更容易理解复杂的生物医学文献至关重要。传统方法难以处理医学文本的专业术语和行话，缺乏动态调整简化过程的灵活性。相比之下，大型语言模型 (LLM) 的最新进展提供了独特的机会，通过迭代改进和专业代理之间的协作，增强了对文本简化的控制。在这项工作中，我们引入了医学简化者协会，这是一个基于 LLM 的新型框架，灵感来自“心智社会”(SOM) 哲学。我们的方法利用 LLM 的优势，分配五个不同的角色，即外行、简化者、医学专家、语言澄清者和冗余检查者，并组织成交互循环。这种结构允许代理逐步改进文本简化，同时保持原始内容的复杂性和准确性。对 Cochrane 文本简化数据集的评估表明，我们的框架与最先进的方法相当或优于最先进的方法，通过受控的简化过程实现了卓越的可读性和内容保存。</li>
</ul>

<h3>Title: Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?</h3>
<ul>
<li><strong>Authors: </strong>HyoJung Han, Akiko Eriguchi, Haoran Xu, Hieu Hoang, Marine Carpuat, Huda Khayrallah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09644">https://arxiv.org/abs/2410.09644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09644">https://arxiv.org/pdf/2410.09644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09644]] Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?(https://arxiv.org/abs/2410.09644)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Vocabulary adaptation, which integrates new vocabulary into pre-trained language models (LMs), enables expansion to new languages and mitigates token over-fragmentation. However, existing approaches are limited by their reliance on heuristic or external embeddings. We propose VocADT, a novel method for vocabulary adaptation using adapter modules that are trained to learn the optimal linear combination of existing embeddings while keeping the model's weights fixed. VocADT offers a flexible and scalable solution without requiring external resources or language constraints. Across 11 languages-with various scripts, resource availability, and fragmentation-we demonstrate that VocADT outperforms the original Mistral model and other baselines across various multilingual tasks. We find that Latin-script languages and highly fragmented languages benefit the most from vocabulary adaptation. We further fine-tune the adapted model on the generative task of machine translation and find that vocabulary adaptation is still beneficial after fine-tuning and that VocADT is the most effective method.</li>
<li><strong>摘要：</strong>词汇适应将新词汇整合到预训练语言模型 (LM) 中，可以扩展到新语言并缓解标记过度碎片化。然而，现有方法受限于对启发式或外部嵌入的依赖。我们提出了 VocADT，这是一种使用适配器模块进行词汇适应的新方法，这些模块经过训练可以学习现有嵌入的最佳线性组合，同时保持模型的权重不变。VocADT 提供了一种灵活且可扩展的解决方案，无需外部资源或语言限制。在 11 种语言中（具有各种脚本、资源可用性和碎片化），我们证明 VocADT 在各种多语言任务中的表现优于原始 Mistral 模型和其他基线。我们发现拉丁脚本语言和高度碎片化的语言从词汇适应中受益最多。我们进一步在机器翻译的生成任务上微调了适应后的模型，发现词汇适应在微调后仍然有益，并且 VocADT 是最有效的方法。</li>
</ul>

<h3>Title: COrAL: Order-Agnostic Language Modeling for Efficient Iterative Refinement</h3>
<ul>
<li><strong>Authors: </strong>Yuxi Xie, Anirudh Goyal, Xiaobao Wu, Xunjian Yin, Xiao Xu, Min-Yen Kan, Liangming Pan, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09675">https://arxiv.org/abs/2410.09675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09675">https://arxiv.org/pdf/2410.09675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09675]] COrAL: Order-Agnostic Language Modeling for Efficient Iterative Refinement(https://arxiv.org/abs/2410.09675)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Iterative refinement has emerged as an effective paradigm for enhancing the capabilities of large language models (LLMs) on complex tasks. However, existing approaches typically implement iterative refinement at the application or prompting level, relying on autoregressive (AR) modeling. The sequential token generation in AR models can lead to high inference latency. To overcome these challenges, we propose Context-Wise Order-Agnostic Language Modeling (COrAL), which incorporates iterative refinement directly into the LLM architecture while maintaining computational efficiency. Our approach models multiple token dependencies within manageable context windows, enabling the model to perform iterative refinement internally during the generation process. Leveraging the order-agnostic nature of COrAL, we introduce sliding blockwise order-agnostic decoding, which performs multi-token forward prediction and backward reconstruction within context windows. This allows the model to iteratively refine its outputs in parallel in the sliding block, effectively capturing diverse dependencies without the high inference cost of sequential generation. Empirical evaluations on reasoning tasks demonstrate that COrAL improves performance and inference speed, respectively, achieving absolute accuracy gains of $4.6\%$ on GSM8K and $4.0\%$ on LogiQA, along with inference speedups of up to $3.9\times$ over next-token baselines. Preliminary results on code generation indicate a drop in pass rates due to inconsistencies in order-agnostic outputs, highlighting the inherent quality--speed trade-off. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>迭代细化已成为增强大型语言模型 (LLM) 处理复杂任务的能力的有效范例。然而，现有方法通常在应用程序或提示级别实现迭代细化，依赖于自回归 (AR) 建模。AR 模型中的顺序令牌生成可能导致高推理延迟。为了克服这些挑战，我们提出了上下文顺序不可知语言建模 (COrAL)，它将迭代细化直接纳入 LLM 架构，同时保持计算效率。我们的方法在可管理的上下文窗口内对多个令牌依赖关系进行建模，使模型能够在生成过程中在内部执行迭代细化。利用 COrAL 的顺序不可知特性，我们引入了滑动块顺序不可知解码，它在上下文窗口内执行多令牌前向预测和后向重构。这允许模型在滑块中并行迭代细化其输出，有效地捕获各种依赖关系，而无需顺序生成的高推理成本。对推理任务的实证评估表明，COrAL 分别提高了性能和推理速度，在 GSM8K 上实现了绝对准确率 $4.6\%$ 的增益，在 LogiQA 上实现了 $4.0\%$ 的增益，同时推理速度比下一个标记基线提高了高达 $3.9\times$。代码生成的初步结果表明，由于顺序无关输出不一致，通过率有所下降，这突显了固有的质量-速度权衡。我们的代码在此 https URL 上公开提供。</li>
</ul>

<h3>Title: Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG</h3>
<ul>
<li><strong>Authors: </strong>Xinxi Chen, Li Wang, Wei Wu, Qi Tang, Yiyao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09699">https://arxiv.org/abs/2410.09699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09699">https://arxiv.org/pdf/2410.09699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09699]] Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG(https://arxiv.org/abs/2410.09699)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune "small" language models to say "I don't know" to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.</li>
<li><strong>摘要：</strong>幻觉是大型语言模型 (LLM) 应用的一个主要障碍，特别是对于对信息准确性敏感的企业应用。为了解决这个问题，人们探索了两种通用方法：检索增强生成 (RAG)，为 LLM 提供更新的信息作为上下文，并使用新信息和所需的输出样式对 LLM 进行微调。在本文中，我们提出了诚实的人工智能：一种新颖的策略，可以微调“小型”语言模型以说“我不知道”以减少幻觉，以及几种替代的 RAG 方法。该解决方案在错误前提问题的任务 2 中排名第一。替代方法包括将 RAG 与搜索引擎和知识图谱结果结合使用、使用新信息微调基础 LLM 以及两种方法的组合。虽然所有方法都可以提高 LLM 的性能，但仅使用 RAG 并不能显著提高性能，需要进行微调才能获得更好的结果。最后，混合方法在 CRAG 基准测试中获得了最高分。此外，我们的方法强调使用少于 100 亿个参数的相对较小的模型，从而提高资源效率。</li>
</ul>

<h3>Title: Taming Overconfidence in LLMs: Reward Calibration in RLHF</h3>
<ul>
<li><strong>Authors: </strong>Jixuan Leng, Chengsong Huang, Banghua Zhu, Jiaxin Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09724">https://arxiv.org/abs/2410.09724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09724">https://arxiv.org/pdf/2410.09724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09724]] Taming Overconfidence in LLMs: Reward Calibration in RLHF(https://arxiv.org/abs/2410.09724)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with a more sharpened output probability, in this study, we reveal that RLHF tends to lead models to express verbalized overconfidence in their own responses. We investigate the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, we propose two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. We evaluate our methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of our methods can reduce calibration error and maintain performance comparable to standard PPO. We further show that they do not compromise model capabilities in open-ended conversation settings.</li>
<li><strong>摘要：</strong>语言模型校准是指模型的置信度与其响应的实际性能之间的一致性。虽然先前的研究指出了大型语言模型 (LLM) 中的过度自信现象，并表明使用人类反馈强化学习 (RLHF) 训练的 LLM 过于自信，输出概率更高，但在本研究中，我们发现 RLHF 倾向于导致模型在自己的响应中表达口头化的过度自信。我们调查了这种过度自信的根本原因，并证明用于近端策略优化 (PPO) 的奖励模型无论响应的实际质量如何，都表现出对高置信度分数的固有偏见。基于这一见解，我们提出了两种 PPO 变体：PPO-M：具有校准奖励建模的 PPO 和 PPO-C：具有校准奖励计算的 PPO。PPO-M 在奖励模型训练中集成了显式置信度分数，从而校准了奖励模型以更好地捕捉响应质量与口头化置信度之间的一致性。 PPO-C 根据当前奖励与过去奖励的移动平均值之间的差异来调整 PPO 期间的奖励分数。PPO-M 和 PPO-C 都可以无缝集成到当前的 PPO 管道中，并且不需要额外的黄金标签。我们在六个不同的数据集（包括多项选择和开放式生成）上对 Llama3-8B 和 Mistral-7B 上的方法进行了评估。实验结果表明，我们的两种方法都可以减少校准误差并保持与标准 PPO 相当的性能。我们进一步表明，它们不会损害开放式对话设置中的模型能力。</li>
</ul>

<h3>Title: Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt</h3>
<ul>
<li><strong>Authors: </strong>Chengguang Gan, Tatsunori Mori</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09745">https://arxiv.org/abs/2410.09745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09745">https://arxiv.org/pdf/2410.09745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09745]] Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt(https://arxiv.org/abs/2410.09745)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ empirical experiment to observe and substantiate the MRE theory. Our experiments on 21 MRE mix datasets revealed the presence of MRE in the model and its impact. Specifically, we conducted compare experiments use fine-tune. The results of findings from comparison experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.</li>
<li><strong>摘要：</strong>相互强化效应 (MRE) 研究文本分类任务中词级分类和文本级分类之间的协同关系。它假定两个分类级别的性能可以相互增强。然而，这种机制在先前的研究中没有得到充分的证明或解释。为了解决这一空白，我们采用实证实验来观察和证实 MRE 理论。我们在 21 个 MRE 混合数据集上的实验揭示了模型中 MRE 的存在及其影响。具体来说，我们使用微调进行了比较实验。比较实验的结果证实了 MRE 的存在。此外，我们将 MRE 的应用扩展到提示学习，利用词级信息作为语言工具来支持模型对文本级分类标签的预测。在我们的最终实验中，F1 分数在 21 个 MRE 混合数据集中的 18 个中显著超过基线，进一步验证了词级信息增强语言模型对整个文本的理解这一观点。</li>
</ul>

<h3>Title: 'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated peer-reviews</h3>
<ul>
<li><strong>Authors: </strong>Sandeep Kumar, Mohit Sahu, Vardhan Gacche, Tirthankar Ghosal, Asif Ekbal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09770">https://arxiv.org/abs/2410.09770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09770">https://arxiv.org/pdf/2410.09770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09770]] 'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated peer-reviews(https://arxiv.org/abs/2410.09770)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise scientific publishing, including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model, which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally, we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than the other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, and model public.</li>
<li><strong>摘要：</strong>同行评审过程的完整性对于维护学术界的科学严谨性和信任至关重要。随着大型语言模型 (LLM)（如 ChatGPT）在学术写作中的使用稳步增加，人们越来越担心人工智能生成的文本可能会危及科学出版，包括同行评审。以前的研究主要集中在通用的人工智能生成的文本检测上，或者提出了一种估计人工智能生成的同行评审比例的方法。我们在这里关注的是通过协助编辑或主席确定评论是否由 ChatGPT 撰写来解决现实问题。为了解决这个问题，我们引入了词频 (TF) 模型，该模型假定人工智能经常重复标记，以及评论再生 (RR) 模型，该模型基于 ChatGPT 在重新提示时生成类似输出的想法。我们对这些检测器进行了针对标记攻击和释义的压力测试。最后，我们提出了一种有效的防御策略来减少释义对我们模型的影响。我们的研究结果表明，我们提出的两种方法都比其他 AI 文本检测器表现更好。我们的 RR 模型更稳健，尽管我们的 TF 模型在没有任何攻击的情况下表现优于 RR 模型。我们公开了我们的代码、数据集和模型。</li>
</ul>

<h3>Title: Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling Approach for LLM Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Gisang Lee, Sangwoo Park, Junyoung Park, Andrew Chung, Sieun Park, Yoonah Park, Byungju Kim, Min-gyu Cho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09780">https://arxiv.org/abs/2410.09780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09780">https://arxiv.org/pdf/2410.09780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09780]] Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling Approach for LLM Mathematical Reasoning(https://arxiv.org/abs/2410.09780)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have exhibited remarkable capabilities in many complex tasks including mathematical reasoning. However, traditional approaches heavily rely on ensuring self-consistency within single prompting method, which limits the exploration of diverse problem-solving strategies. This study addresses these limitations by performing an experimental analysis of distinct prompting methods within the domain of mathematical reasoning. Our findings demonstrate that each method explores a distinct search space, and this differentiation becomes more evident with increasing problem complexity. To leverage this phenomenon, we applied efficient sampling process that uniformly combines samples from these diverse methods, which not only expands the maximum search space but achieves higher performance with fewer runs compared to single methods. Especially, within the subset of difficult questions of MATH dataset named MATH-hard, The maximum search space was achieved while utilizing approximately 43% fewer runs than single methods on average. These findings highlight the importance of integrating diverse problem-solving strategies to enhance the reasoning abilities of LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在包括数学推理在内的许多复杂任务中表现出卓越的能力。然而，传统方法严重依赖于确保单一提示方法的自洽性，这限制了对各种问题解决策略的探索。本研究通过对数学推理领域内不同的提示方法进行实验分析来解决这些限制。我们的研究结果表明，每种方法都会探索不同的搜索空间，并且随着问题复杂性的增加，这种差异变得更加明显。为了利用这一现象，我们应用了高效的采样过程，将来自这些不同方法的样本均匀地组合在一起，这不仅扩大了最大搜索空间，而且与单一方法相比，以更少的运行次数实现了更高的性能。特别是在 MATH 数据集的难题子集 MATH-hard 中，实现了最大搜索空间，同时平均比单一方法少使用约 43% 的运行次数。这些发现强调了整合各种问题解决策略以增强 LLM 推理能力的重要性。</li>
</ul>

<h3>Title: Single Ground Truth Is Not Enough: Add Linguistic Variability to Aspect-based Sentiment Analysis Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Soyoung Yang, Hojun Cho, Jiyoung Lee, Sohee Yoon, Edward Choi, Jaegul Choo, Won Ik Cho</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09807">https://arxiv.org/abs/2410.09807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09807">https://arxiv.org/pdf/2410.09807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09807]] Single Ground Truth Is Not Enough: Add Linguistic Variability to Aspect-based Sentiment Analysis Evaluation(https://arxiv.org/abs/2410.09807)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Aspect-based sentiment analysis (ABSA) is the challenging task of extracting sentiment along with its corresponding aspects and opinions from human language. Due to the inherent variability of natural language, aspect and opinion terms can be expressed in various surface forms, making their accurate identification complex. Current evaluation methods for this task often restrict answers to a single ground truth, penalizing semantically equivalent predictions that differ in surface form. To address this limitation, we propose a novel, fully automated pipeline that augments existing test sets with alternative valid responses for aspect and opinion terms. This approach enables a fairer assessment of language models by accommodating linguistic diversity, resulting in higher human agreement than single-answer test sets (up to 10%p improvement in Kendall's Tau score). Our experimental results demonstrate that Large Language Models (LLMs) show substantial performance improvements over T5 models when evaluated using our augmented test set, suggesting that LLMs' capabilities in ABSA tasks may have been underestimated. This work contributes to a more comprehensive evaluation framework for ABSA, potentially leading to more accurate assessments of model performance in information extraction tasks, particularly those involving span extraction.</li>
<li><strong>摘要：</strong>基于方面的情绪分析 (ABSA) 是一项具有挑战性的任务，需要从人类语言中提取情绪及其相应的方面和观点。由于自然语言固有的可变性，方面和观点术语可以用各种表面形式表达，因此准确识别它们非常复杂。当前针对此任务的评估方法通常将答案限制为单一基本事实，惩罚表面形式不同的语义等效预测。为了解决这一限制，我们提出了一种新颖的全自动流程，该流程使用针对方面和观点术语的替代有效响应来增强现有测试集。这种方法通过适应语言多样性，可以更公平地评估语言模型，从而比单一答案测试集获得更高的人类一致性（Kendall 的 Tau 分数提高了 10%p）。我们的实验结果表明，使用我们增强的测试集进行评估时，大型语言模型 (LLM) 的性能显著优于 T5 模型，这表明 LLM 在 ABSA 任务中的能力可能被低估了。这项工作有助于建立更全面的 ABSA 评估框架，从而可能更准确地评估模型在信息提取任务（特别是涉及跨度提取的任务）中的性能。</li>
</ul>

<h3>Title: Reverse Modeling in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Yu, Yuanchen Xu, Cunxiao Du, Yanying Zhou, Minghui Qiu, Qianru Sun, Hao Zhang, Jiawei Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09817">https://arxiv.org/abs/2410.09817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09817">https://arxiv.org/pdf/2410.09817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09817]] Reverse Modeling in Large Language Models(https://arxiv.org/abs/2410.09817)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Humans are accustomed to reading and writing in a forward manner, and this natural bias extends to text understanding in auto-regressive large language models (LLMs). This paper investigates whether LLMs, like humans, struggle with reverse modeling, specifically with reversed text inputs. We found that publicly available pre-trained LLMs cannot understand such inputs. However, LLMs trained from scratch with both forward and reverse texts can understand them equally well during inference. Our case study shows that different-content texts result in different losses if input (to LLMs) in different directions -- some get lower losses for forward while some for reverse. This leads us to a simple and nice solution for data selection based on the loss differences between forward and reverse directions. Using our selected data in continued pretraining can boost LLMs' performance by a large margin across different language understanding benchmarks.</li>
<li><strong>摘要：</strong>人类习惯于以正向方式阅读和写作，这种自然的偏见延伸到自回归大型语言模型 (LLM) 中的文本理解。本文研究了 LLM 是否像人类一样难以进行反向建模，特别是反向文本输入。我们发现公开的预训练 LLM 无法理解此类输入。但是，使用正向和反向文本从头训练的 LLM 在推理过程中可以同样好地理解它们。我们的案例研究表明，如果输入（到 LLM）的方向不同，不同内容的文本会导致不同的损失——有些文本在正向时损失较低，而有些文本在反向时损失较低。这为我们提供了一个简单而好的解决方案，用于基于正向和反向之间的损失差异进行数据选择。在持续的预训练中使用我们选择的数据可以大幅提高 LLM 在不同的语言理解基准上的性能。</li>
</ul>

<h3>Title: Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent Simulation</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen Pan, Yaliang Li, Bolin Ding</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09824">https://arxiv.org/abs/2410.09824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09824">https://arxiv.org/pdf/2410.09824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09824]] Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent Simulation(https://arxiv.org/abs/2410.09824)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Graph generation is a fundamental task that has been extensively studied in social, technological, and scientific analysis. For modeling the dynamic graph evolution process, traditional rule-based methods struggle to capture community structures within graphs, while deep learning methods only focus on fitting training graphs. This limits existing graph generators to producing graphs that adhere to predefined rules or closely resemble training datasets, achieving poor performance in dynamic graph generation. Given that graphs are abstract representations arising from pairwise interactions in human activities, a realistic simulation of human-wise interaction could provide deeper insights into the graph evolution mechanism. With the increasing recognition of large language models (LLMs) in simulating human behavior, we introduce GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic graph generation. Without training or fine-tuning process of LLM, our framework effectively replicates seven macro-level structural characteristics in established network science theories while surpassing existing baselines in graph expansion tasks by 31\% on specific evaluation metrics. Through node classification task, we validate GAG effectively preserves characteristics of real-world network for node-wise textual features in generated text-rich graph. Furthermore, by incorporating parallel acceleration, GAG supports generating graphs with up to nearly 100,000 nodes or 10 million edges through large-scale LLM-based agent simulation, with a minimum speed-up of 90.4\%. The source code is available at this https URL.</li>
<li><strong>摘要：</strong>图生成是一项基本任务，在社会、技术和科学分析中得到了广泛的研究。对于动态图演化过程的建模，传统的基于规则的方法难以捕捉图中的社区结构，而深度学习方法仅专注于拟合训练图。这限制了现有的图生成器只能生成符合预定义规则或与训练数据集非常相似的图，从而导致动态图生成性能不佳。鉴于图是人类活动中成对交互产生的抽象表示，对人际交互的真实模拟可以更深入地了解图演化机制。随着大型语言模型 (LLM) 在模拟人类行为方面的认可度不断提高，我们引入了 GraphAgent-Generator (GAG)，这是一种基于模拟的新型动态图生成框架。无需 LLM 的训练或微调过程，我们的框架便可有效复制现有网络科学理论中的七个宏观结构特征，同时在特定评估指标上超越现有图扩展任务基线 31\%。通过节点分类任务，我们验证了 GAG 能够有效地保留真实网络的特征，以便在生成的富文本图形中保留节点文本特征。此外，通过结合并行加速，GAG 支持通过大规模基于 LLM 的代理模拟生成多达近 100,000 个节点或 1000 万条边的图形，速度至少可提高 90.4%。源代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Generating Driving Simulations via Conversation</h3>
<ul>
<li><strong>Authors: </strong>Rimvydas Rubavicius, Antonio Valerio Miceli-Barone, Alex Lascarides, Subramanian Ramamoorthy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09829">https://arxiv.org/abs/2410.09829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09829">https://arxiv.org/pdf/2410.09829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09829]] Generating Driving Simulations via Conversation(https://arxiv.org/abs/2410.09829)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Cyber-physical systems like autonomous vehicles are tested in simulation before deployment, using domain-specific programs for scenario specification. To aid the testing of autonomous vehicles in simulation, we design a natural language interface, using an instruction-following large language model, to assist a non-coding domain expert in synthesising the desired scenarios and vehicle behaviours. We show that using it to convert utterances to the symbolic program is feasible, despite the very small training dataset. Human experiments show that dialogue is critical to successful simulation generation, leading to a 4.5 times higher success rate than a generation without engaging in extended conversation.</li>
<li><strong>摘要：</strong>自动驾驶汽车等网络物理系统在部署之前会先在模拟环境中进行测试，并使用特定领域的程序来指定场景。为了帮助在模拟环境中测试自动驾驶汽车，我们设计了一个自然语言界面，使用遵循指令的大型语言模型来协助非编码领域专家合成所需的场景和车辆行为。我们表明，尽管训练数据集非常小，但使用它来将话语转换为符号程序是可行的。人类实验表明，对话对于成功的模拟生成至关重要，与不进行长时间对话的生成相比，对话的成功率高出 4.5 倍。</li>
</ul>

<h3>Title: ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains</h3>
<ul>
<li><strong>Authors: </strong>Yein Park, Chanwoong Yoon, Jungwoo Park, Donghyeon Lee, Minbyul Jeong, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09870">https://arxiv.org/abs/2410.09870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09870">https://arxiv.org/pdf/2410.09870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09870]] ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains(https://arxiv.org/abs/2410.09870)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly impacted many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the accumulative nature of knowledge, often relying on a single time stamp. To overcome this, we introduce ChroKnowBench, a benchmark dataset designed to evaluate chronologically accumulated knowledge across three key aspects: multiple domains, time dependency, temporal state. Our benchmark distinguishes between knowledge that evolves (e.g., scientific discoveries, amended laws) and knowledge that remain constant (e.g., mathematical truths, commonsense facts). Building on this benchmark, we present ChroKnowledge (Chronological Categorization of Knowledge), a novel sampling-based framework for evaluating and updating LLMs' non-parametric chronological knowledge. Our evaluation shows: (1) The ability of eliciting temporal knowledge varies depending on the data format that model was trained on. (2) LLMs partially recall knowledge or show a cut-off at temporal boundaries rather than recalling all aspects of knowledge correctly. Thus, we apply our ChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by traversing step-by-step through the surrounding time spans. We observe that our framework successfully updates the overall knowledge across the entire timeline in both the biomedical domain (+11.9%) and the general domain (+2.8%), demonstrating its effectiveness in refining temporal knowledge. This non-parametric approach also enables knowledge updates not only in open-source models but also in proprietary LLMs, ensuring comprehensive applicability across model types. We perform a comprehensive analysis based on temporal characteristics of ChroKnowPrompt and validate the potential of various models to elicit intrinsic temporal knowledge through our method.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 对我们生活的许多方面产生了重大影响。然而，评估和确保其按时间顺序排列的知识仍然具有挑战性。现有方法无法解决知识的累积性质，通常依赖于单个时间戳。为了克服这个问题，我们引入了 ChroKnowBench，这是一个基准数据集，旨在评估三个关键方面的按时间顺序积累的知识：多个领域、时间依赖性、时间状态。我们的基准区分了不断发展的知识（例如，科学发现、修订的法律）和保持不变的知识（例如，数学真理、常识事实）。在此基准的基础上，我们提出了 ChroKnowledge（知识按时间顺序分类），这是一种基于采样的新型框架，用于评估和更新 LLM 的非参数按时间顺序排列的知识。我们的评估表明：(1) 引出时间知识的能力因模型训练的数据格式而异。(2) LLM 部分回忆知识或在时间边界处显示截止，而不是正确回忆知识的所有方面。因此，我们应用了 ChroKnowPrompt，这是一种深入的提示，通过逐步遍历周围的时间跨度来引出时间顺序知识。我们观察到，我们的框架成功地更新了生物医学领域（+11.9%）和一般领域（+2.8%）整个时间线上的整体知识，证明了其在细化时间知识方面的有效性。这种非参数方法不仅可以在开源模型中更新知识，还可以在专有 LLM 中更新知识，确保跨模型类型的全面适用性。我们根据 ChroKnowPrompt 的时间特征进行了全面分析，并验证了各种模型通过我们的方法引出内在时间知识的潜力。</li>
</ul>

<h3>Title: RMB: Comprehensively Benchmarking Reward Models in LLM Alignment</h3>
<ul>
<li><strong>Authors: </strong>Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09893">https://arxiv.org/abs/2410.09893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09893">https://arxiv.org/pdf/2410.09893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09893]] RMB: Comprehensively Benchmarking Reward Models in LLM Alignment(https://arxiv.org/abs/2410.09893)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited distribution of evaluation data and evaluation methods that are not closely related to alignment objectives. To address these limitations, we propose RMB, a comprehensive RM benchmark that covers over 49 real-world scenarios and includes both pairwise and Best-of-N (BoN) evaluations to better reflect the effectiveness of RMs in guiding alignment optimization. We demonstrate a positive correlation between our benchmark and the downstream alignment task performance. Based on our benchmark, we conduct extensive analysis on the state-of-the-art RMs, revealing their generalization defects that were not discovered by previous benchmarks, and highlighting the potential of generative RMs. Furthermore, we delve into open questions in reward models, specifically examining the effectiveness of majority voting for the evaluation of reward models and analyzing the impact factors of generative RMs, including the influence of evaluation criteria and instructing methods. Our evaluation code and datasets are available at this https URL.</li>
<li><strong>摘要：</strong>奖励模型 (RM) 指导大型语言模型 (LLM) 的对齐，使其朝着人类喜欢的行为发展。评估 RM 是更好地对齐 LLM 的关键。然而，由于评估数据分布有限以及评估方法与对齐目标不密切相关，当前对 RM 的评估可能并不直接对应于它们的对齐性能。为了解决这些限制，我们提出了 RMB，这是一个全面的 RM 基准，涵盖 49 多个真实场景，包括成对和 Best-of-N (BoN) 评估，以更好地反映 RM 在指导对齐优化方面的有效性。我们证明了我们的基准与下游对齐任务性能之间存在正相关性。基于我们的基准，我们对最先进的 RM 进行了广泛的分析，揭示了以前的基准未发现的泛化缺陷，并强调了生成 RM 的潜力。此外，我们深入研究了奖励模型中的未决问题，特别是检查多数投票对奖励模型评估的有效性，并分析生成 RM 的影响因素，包括评估标准和指导方法的影响。我们的评估代码和数据集可在此 https URL 上找到。</li>
</ul>

<h3>Title: Reddit is all you need: Authorship profiling for Romanian</h3>
<ul>
<li><strong>Authors: </strong>Ecaterina Ştefănescu, Alexandru-Iulius Jerpelea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09907">https://arxiv.org/abs/2410.09907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09907">https://arxiv.org/pdf/2410.09907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09907]] Reddit is all you need: Authorship profiling for Romanian(https://arxiv.org/abs/2410.09907)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Authorship profiling is the process of identifying an author's characteristics based on their writings. This centuries old problem has become more intriguing especially with recent developments in Natural Language Processing (NLP). In this paper, we introduce a corpus of short texts in the Romanian language, annotated with certain author characteristic keywords; to our knowledge, the first of its kind. In order to do this, we exploit a social media platform called Reddit. We leverage its thematic community-based structure (subreddits structure), which offers information about the author's background. We infer an user's demographic and some broad personal traits, such as age category, employment status, interests, and social orientation based on the subreddit and other cues. We thus obtain a 23k+ samples corpus, extracted from 100+ Romanian subreddits. We analyse our dataset, and finally, we fine-tune and evaluate Large Language Models (LLMs) to prove baselines capabilities for authorship profiling using the corpus, indicating the need for further research in the field. We publicly release all our resources.</li>
<li><strong>摘要：</strong>作者身份分析是根据作者的作品识别作者特征的过程。这个存在了几个世纪的问题变得更加有趣，尤其是随着自然语言处理 (NLP) 的最新发展。在本文中，我们介绍了一个罗马尼亚语短文本语料库，并注释了某些作者特征关键词；据我们所知，这是此类语料库中的第一个。为了做到这一点，我们利用了一个名为 Reddit 的社交媒体平台。我们利用其基于主题社区的结构（subreddits 结构），该结构提供有关作者背景的信息。我们根据 subreddit 和其他线索推断用户的人口统计和一些广泛的个人特征，例如年龄类别、就业状况、兴趣和社会取向。因此，我们获得了一个 23k+ 样本语料库，这些样本来自 100 多个罗马尼亚语 subreddits。我们分析了我们的数据集，最后，我们对大型语言模型 (LLM) 进行了微调和评估，以证明使用该语料库进行作者身份分析的基线能力，表明需要在该领域进行进一步研究。我们公开发布我们的所有资源。</li>
</ul>

<h3>Title: Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization</h3>
<ul>
<li><strong>Authors: </strong>Alireza Salemi, Hamed Zamani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09942">https://arxiv.org/abs/2410.09942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09942">https://arxiv.org/pdf/2410.09942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09942]] Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization(https://arxiv.org/abs/2410.09942)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>This paper investigates the design of a unified search engine to serve multiple retrieval-augmented generation (RAG) agents, each with a distinct task, backbone large language model (LLM), and retrieval-augmentation strategy. We introduce an iterative approach where the search engine generates retrieval results for these RAG agents and gathers feedback on the quality of the retrieved documents during an offline phase. This feedback is then used to iteratively optimize the search engine using a novel expectation-maximization algorithm, with the goal of maximizing each agent's utility function. Additionally, we adapt this approach to an online setting, allowing the search engine to refine its behavior based on real-time individual agents feedback to better serve the results for each of them. Experiments on diverse datasets from the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our approach significantly on average outperforms competitive baselines across 18 RAG models. We also demonstrate that our method effectively ``personalizes'' the retrieval process for each RAG agent based on the collected feedback. Finally, we provide a comprehensive ablation study to explore various aspects of our method.</li>
<li><strong>摘要：</strong>本文研究了统一搜索引擎的设计，以服务于多个检索增强生成 (RAG) 代理，每个代理都有不同的任务、骨干大型语言模型 (LLM) 和检索增强策略。我们引入了一种迭代方法，其中搜索引擎为这些 RAG 代理生成检索结果，并在离线阶段收集有关检索到的文档质量的反馈。然后，该反馈用于使用新颖的期望最大化算法迭代优化搜索引擎，目标是最大化每个代理的效用函数。此外，我们将这种方法调整到在线设置，允许搜索引擎根据实时单个代理反馈改进其行为，以更好地为每个代理提供结果。对知识密集型语言任务 (KILT) 基准的不同数据集进行的实验表明，我们的方法在 18 个 RAG 模型中的平均表现显著优于竞争基线。我们还证明，我们的方法可以根据收集到的反馈有效地“个性化”每个 RAG 代理的检索过程。最后，我们提供了全面的消融研究来探索我们方法的各个方面。</li>
</ul>

<h3>Title: State of NLP in Kenya: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Cynthia Jayne Amol, Everlyn Asiko Chimoto, Rose Delilah Gesicho, Antony M. Gitau, Naome A. Etori, Caringtone Kinyanjui, Steven Ndung'u, Lawrence Moruye, Samson Otieno Ooko, Kavengi Kitonga, Brian Muhia, Catherine Gitau, Antony Ndolo, Lilian D. A. Wanzare, Albert Njoroge Kahira, Ronald Tombe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09948">https://arxiv.org/abs/2410.09948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09948">https://arxiv.org/pdf/2410.09948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09948]] State of NLP in Kenya: A Survey(https://arxiv.org/abs/2410.09948)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Kenya, known for its linguistic diversity, faces unique challenges and promising opportunities in advancing Natural Language Processing (NLP) technologies, particularly for its underrepresented indigenous languages. This survey provides a detailed assessment of the current state of NLP in Kenya, emphasizing ongoing efforts in dataset creation, machine translation, sentiment analysis, and speech recognition for local dialects such as Kiswahili, Dholuo, Kikuyu, and Luhya. Despite these advancements, the development of NLP in Kenya remains constrained by limited resources and tools, resulting in the underrepresentation of most indigenous languages in digital spaces. This paper uncovers significant gaps by critically evaluating the available datasets and existing NLP models, most notably the need for large-scale language models and the insufficient digital representation of Indigenous languages. We also analyze key NLP applications: machine translation, information retrieval, and sentiment analysis-examining how they are tailored to address local linguistic needs. Furthermore, the paper explores the governance, policies, and regulations shaping the future of AI and NLP in Kenya and proposes a strategic roadmap to guide future research and development efforts. Our goal is to provide a foundation for accelerating the growth of NLP technologies that meet Kenya's diverse linguistic demands.</li>
<li><strong>摘要：</strong>肯尼亚以其语言多样性而闻名，在推进自然语言处理 (NLP) 技术方面面临着独特的挑战和充满希望的机遇，尤其是对于其代表性不足的土著语言。这项调查对肯尼亚的 NLP 现状进行了详细评估，强调了在数据集创建、机器翻译、情感分析和当地方言（如斯瓦希里语、Dholuo 语、Kikuyu 语和 Luhya 语）的语音识别方面的持续努力。尽管取得了这些进步，但肯尼亚的 NLP 发展仍然受到资源和工具有限的制约，导致大多数土著语言在数字空间中的代表性不足。本文通过批判性地评估可用的数据集和现有的 NLP 模型，发现了重大差距，最明显的是需要大规模语言模型和土著语言的数字表示不足。我们还分析了关键的 NLP 应用：机器翻译、信息检索和情感分析 - 研究如何定制它们以满足当地的语言需求。此外，本文还探讨了影响肯尼亚 AI 和 NLP 未来的治理、政策和法规，并提出了指导未来研究和开发工作的战略路线图。我们的目标是为加速满足肯尼亚多样化语言需求的 NLP 技术的发展奠定基础。</li>
</ul>

<h3>Title: MisinfoEval: Generative AI in the Era of "Alternative Facts"</h3>
<ul>
<li><strong>Authors: </strong>Saadia Gabriel, Liang Lyu, James Siderius, Marzyeh Ghassemi, Jacob Andreas, Asu Ozdaglar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09949">https://arxiv.org/abs/2410.09949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09949">https://arxiv.org/pdf/2410.09949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09949]] MisinfoEval: Generative AI in the Era of "Alternative Facts"(https://arxiv.org/abs/2410.09949)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The spread of misinformation on social media platforms threatens democratic processes, contributes to massive economic losses, and endangers public health. Many efforts to address misinformation focus on a knowledge deficit model and propose interventions for improving users' critical thinking through access to facts. Such efforts are often hampered by challenges with scalability, and by platform users' personal biases. The emergence of generative AI presents promising opportunities for countering misinformation at scale across ideological barriers. In this paper, we introduce a framework (MisinfoEval) for generating and comprehensively evaluating large language model (LLM) based misinformation interventions. We present (1) an experiment with a simulated social media environment to measure effectiveness of misinformation interventions, and (2) a second experiment with personalized explanations tailored to the demographics and beliefs of users with the goal of countering misinformation by appealing to their pre-existing values. Our findings confirm that LLM-based interventions are highly effective at correcting user behavior (improving overall user accuracy at reliability labeling by up to 41.72%). Furthermore, we find that users favor more personalized interventions when making decisions about news reliability and users shown personalized interventions have significantly higher accuracy at identifying misinformation.</li>
<li><strong>摘要：</strong>社交媒体平台上的虚假信息传播威胁民主进程，造成巨大的经济损失，并危害公众健康。许多解决虚假信息的努力都集中在知识缺陷模型上，并提出了通过获取事实来提高用户批判性思维的干预措施。这类努力往往受到可扩展性挑战和平台用户个人偏见的阻碍。生成式人工智能的出现为跨越意识形态障碍大规模打击虚假信息提供了有希望的机会。在本文中，我们介绍了一个框架 (MisinfoEval)，用于生成和全面评估基于大型语言模型 (LLM) 的虚假信息干预措施。我们提出 (1) 一个模拟社交媒体环境的实验来衡量虚假信息干预的有效性，以及 (2) 第二个实验，根据用户的人口统计和信仰量身定制个性化解释，目的是通过诉诸他们先前存在的价值观来打击虚假信息。我们的研究结果证实，基于 LLM 的干预措施在纠正用户行为方面非常有效（将用户在可靠性标签方面的整体准确率提高了 41.72%）。此外，我们发现，用户在判断新闻可靠性时更喜欢个性化的干预，并且接受个性化干预的用户在识别错误​​信息方面的准确性明显更高。</li>
</ul>

<h3>Title: When Neutral Summaries are not that Neutral: Quantifying Political Neutrality in LLM-Generated News Summaries</h3>
<ul>
<li><strong>Authors: </strong>Supriti Vijay, Aman Priyanshu, Ashique R. KhudaBukhsh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09978">https://arxiv.org/abs/2410.09978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09978">https://arxiv.org/pdf/2410.09978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09978]] When Neutral Summaries are not that Neutral: Quantifying Political Neutrality in LLM-Generated News Summaries(https://arxiv.org/abs/2410.09978)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In an era where societal narratives are increasingly shaped by algorithmic curation, investigating the political neutrality of LLMs is an important research question. This study presents a fresh perspective on quantifying the political neutrality of LLMs through the lens of abstractive text summarization of polarizing news articles. We consider five pressing issues in current US politics: abortion, gun control/rights, healthcare, immigration, and LGBTQ+ rights. Via a substantial corpus of 20,344 news articles, our study reveals a consistent trend towards pro-Democratic biases in several well-known LLMs, with gun control and healthcare exhibiting the most pronounced biases (max polarization differences of -9.49% and -6.14%, respectively). Further analysis uncovers a strong convergence in the vocabulary of the LLM outputs for these divisive topics (55% overlap for Democrat-leaning representations, 52% for Republican). Being months away from a US election of consequence, we consider our findings important.</li>
<li><strong>摘要：</strong>在社会叙事日益受到算法策划影响的时代，研究法学硕士的政治中立性是一个重要的研究问题。本研究通过对两极分化新闻文章进行抽象文本摘要，为量化法学硕士的政治中立性提供了一个新视角。我们考虑了当前美国政治中的五个紧迫问题：堕胎、枪支管制/权利、医疗保健、移民和 LGBTQ+ 权利。通过 20,344 篇新闻文章的大量语料库，我们的研究揭示了几个著名的法学硕士课程中存在着一致的亲民主党偏见趋势，其中枪支管制和医疗保健表现出最明显的偏见（最大两极分化差异分别为 -9.49% 和 -6.14%）。进一步分析发现，这些分裂性话题的法学硕士输出词汇存在很强的趋同性（民主党倾向的代表重叠 55%，共和党重叠 52%）。距离美国大选还有几个月的时间，我们认为我们的发现很重要。</li>
</ul>

<h3>Title: Evaluating Gender Bias of LLMs in Making Morality Judgements</h3>
<ul>
<li><strong>Authors: </strong>Divij Bajaj, Yuanyuan Lei, Jonathan Tong, Ruihong Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09992">https://arxiv.org/abs/2410.09992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09992">https://arxiv.org/pdf/2410.09992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09992]] Evaluating Gender Bias of LLMs in Making Morality Judgements(https://arxiv.org/abs/2410.09992)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable capabilities in a multitude of Natural Language Processing (NLP) tasks. However, these models are still not immune to limitations such as social biases, especially gender bias. This work investigates whether current closed and open-source LLMs possess gender bias, especially when asked to give moral opinions. To evaluate these models, we curate and introduce a new dataset GenMO (Gender-bias in Morality Opinions) comprising parallel short stories featuring male and female characters respectively. Specifically, we test models from the GPT family (GPT-3.5-turbo, GPT-3.5-turbo-instruct, GPT-4-turbo), Llama 3 and 3.1 families (8B/70B), Mistral-7B and Claude 3 families (Sonnet and Opus). Surprisingly, despite employing safety checks, all production-standard models we tested display significant gender bias with GPT-3.5-turbo giving biased opinions in 24% of the samples. Additionally, all models consistently favour female characters, with GPT showing bias in 68-85% of cases and Llama 3 in around 81-85% instances. Additionally, our study investigates the impact of model parameters on gender bias and explores real-world situations where LLMs reveal biases in moral decision-making.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在众多自然语言处理 (NLP) 任务中展现出了非凡的能力。然而，这些模型仍然无法免受社会偏见（尤其是性别偏见）等限制。这项研究调查了当前闭源和开源 LLM 是否存在性别偏见，尤其是在被要求发表道德观点时。为了评估这些模型，我们整理并引入了一个新的数据集 GenMO（道德观点中的性别偏见），该数据集由分别以男性和女性角色为主角的平行短篇小说组成。具体来说，我们测试了 GPT 系列（GPT-3.5-turbo、GPT-3.5-turbo-instruct、GPT-4-turbo）、Llama 3 和 3.1 系列（8B/70B）、Mistral-7B 和 Claude 3 系列（Sonnet 和 Opus）的模型。令人惊讶的是，尽管采用了安全检查，但我们测试的所有生产标准模型都表现出明显的性别偏见，GPT-3.5-turbo 在 24% 的样本中给出了有偏见的意见。此外，所有模型都一致偏向女性角色，GPT 在 68-85% 的情况下表现出偏见，而 Llama 3 在 81-85% 的情况下表现出偏见。此外，我们的研究调查了模型参数对性别偏见的影响，并探索了 LLM 揭示道德决策偏见的现实情况。</li>
</ul>

<h3>Title: Safety-Aware Fine-Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyeong Kyu Choi, Xuefeng Du, Yixuan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10014">https://arxiv.org/abs/2410.10014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10014">https://arxiv.org/pdf/2410.10014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10014]] Safety-Aware Fine-Tuning of Large Language Models(https://arxiv.org/abs/2410.10014)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) has emerged as a common practice for tailoring models to individual needs and preferences. The choice of datasets for fine-tuning can be diverse, introducing safety concerns regarding the potential inclusion of harmful data samples. Manually filtering or avoiding such samples, however, can be labor-intensive and subjective. To address these difficulties, we propose a novel Safety-Aware Fine-Tuning (SAFT) framework designed to automatically detect and remove potentially harmful data, by leveraging a scoring function that exploits the subspace information of harmful and benign samples. Experimental results demonstrate the efficacy of SAFT across different LLMs and varying contamination rates, achieving reductions in harmfulness of up to 27.8%. Going beyond, we delve into the mechanism of our approach and validate its versatility in addressing practical challenges in real-world scenarios.</li>
<li><strong>摘要：</strong>微调大型语言模型 (LLM) 已成为一种根据个人需求和偏好定制模型的常见做法。用于微调的数据集的选择可能多种多样，从而引发了有关可能包含有害数据样本的安全问题。但是，手动过滤或避免此类样本可能非常耗费人力且主观性强。为了解决这些困难，我们提出了一种新颖的安全意识微调 (SAFT) 框架，旨在通过利用利用有害和良性样本子空间信息的评分函数来自动检测和删除潜在的有害数据。实验结果证明了 SAFT 在不同 LLM 和不同污染率中的有效性，可将有害性降低高达 27.8%。除此之外，我们还深入研究了我们方法的机制，并验证了其在解决现实场景中的实际挑战方面的多功能性。</li>
</ul>

<h3>Title: LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Saikrishna Sanniboina, Shiv Trivedi, Sreenidhi Vijayaraghavan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10042">https://arxiv.org/abs/2410.10042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10042">https://arxiv.org/pdf/2410.10042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10042]] LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering(https://arxiv.org/abs/2410.10042)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Retrieval-based question answering systems often suffer from positional bias, leading to suboptimal answer generation. We propose LoRE (Logit-Ranked Retriever Ensemble), a novel approach that improves answer accuracy and relevance by mitigating positional bias. LoRE employs an ensemble of diverse retrievers, such as BM25 and sentence transformers with FAISS indexing. A key innovation is a logit-based answer ranking algorithm that combines the logit scores from a large language model (LLM), with the retrieval ranks of the passages. Experimental results on NarrativeQA, SQuAD demonstrate that LoRE significantly outperforms existing retrieval-based methods in terms of exact match and F1 scores. On SQuAD, LoRE achieves 14.5\%, 22.83\%, and 14.95\% improvements over the baselines for ROUGE-L, EM, and F1, respectively. Qualitatively, LoRE generates more relevant and accurate answers, especially for complex queries.</li>
<li><strong>摘要：</strong>基于检索的问答系统经常受到位置偏差的影响，导致答案生成不理想。我们提出了 LoRE（Logit-Ranked Retriever Ensemble），这是一种通过减轻位置偏差来提高答案准确性和相关性的新方法。LoRE 采用多种检索器的组合，例如 BM25 和带有 FAISS 索引的句子转换器。一项关键创新是基于 logit 的答案排名算法，该算法将大型语言模型 (LLM) 的 logit 分数与段落的检索排名相结合。在 NarrativeQA、SQuAD 上的实验结果表明，LoRE 在精确匹配和 F1 分数方面明显优于现有的基于检索的方法。在 SQuAD 上，LoRE 分别比 ROUGE-L、EM 和 F1 的基线提高了 14.5%、22.83% 和 14.95%。从质量上看，LoRE 可以生成更相关、更准确的答案，尤其是对于复杂的查询。</li>
</ul>

<h3>Title: AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality</h3>
<ul>
<li><strong>Authors: </strong>Peijun Qing, Chongyang Gao, Yefan Zhou, Xingjian Diao, Yaoqing Yang, Soroush Vosoughi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10054">https://arxiv.org/abs/2410.10054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10054">https://arxiv.org/pdf/2410.10054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10054]] AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality(https://arxiv.org/abs/2410.10054)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), are known to enhance training efficiency in Large Language Models (LLMs). Due to the limited parameters of LoRA, recent studies seek to combine LoRA with Mixture-of-Experts (MoE) to boost performance across various tasks. However, inspired by the observed redundancy in traditional MoE structures, previous studies identify similar redundancy among LoRA experts within the MoE architecture, highlighting the necessity for non-uniform allocation of LoRA experts across different layers. In this paper, we leverage Heavy-Tailed Self-Regularization (HT-SR) Theory to design a fine-grained allocation strategy. Our analysis reveals that the number of experts per layer correlates with layer training quality, which exhibits significant variability across layers. Based on this, we introduce AlphaLoRA, a theoretically principled and training-free method for allocating LoRA experts to further mitigate redundancy. Experiments on three models across ten language processing and reasoning benchmarks demonstrate that AlphaLoRA achieves comparable or superior performance over all baselines. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>众所周知，参数高效的微调方法（例如低秩自适应 (LoRA)）可以提高大型语言模型 (LLM) 的训练效率。由于 LoRA 的参数有限，最近的研究试图将 LoRA 与混合专家 (MoE) 相结合，以提高各种任务的性能。然而，受传统 MoE 结构中观察到的冗余的启发，先前的研究发现 MoE 架构中的 LoRA 专家之间存在类似的冗余，这凸显了在不同层之间非均匀分配 LoRA 专家的必要性。在本文中，我们利用重尾自正则化 (HT-SR) 理论来设计细粒度的分配策略。我们的分析表明，每层的专家数量与层训练质量相关，这在不同层之间表现出显着的差异。基于此，我们引入了 AlphaLoRA，这是一种理论上有原则且无需训练的方法，用于分配 LoRA 专家以进一步减轻冗余。在十个语言处理和推理基准上对三个模型进行的实验表明，AlphaLoRA 在所有基准上都取得了相当或更优异的性能。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates</h3>
<ul>
<li><strong>Authors: </strong>Md Kowsher, Tara Esmaeilbeig, Chun-Nam Yu, Mojtaba Soltanalian, Niloofar Yousefi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10075">https://arxiv.org/abs/2410.10075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10075">https://arxiv.org/pdf/2410.10075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10075]] RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates(https://arxiv.org/abs/2410.10075)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We propose RoCoFT, a parameter-efficient fine-tuning method for large-scale language models (LMs) based on updating only a few rows and columns of the weight matrices in transformers. Through extensive experiments with medium-size LMs like BERT and RoBERTa, and larger LMs like Bloom-7B, Llama2-7B, and Llama2-13B, we show that our method gives comparable or better accuracies than state-of-art PEFT methods while also being more memory and computation-efficient. We also study the reason behind the effectiveness of our method with tools from neural tangent kernel theory. We empirically demonstrate that our kernel, constructed using a restricted set of row and column parameters, are numerically close to the full-parameter kernel and gives comparable classification performance. Ablation studies are conducted to investigate the impact of different algorithmic choices, including the selection strategy for rows and columns as well as the optimal rank for effective implementation of our method.</li>
<li><strong>摘要：</strong>我们提出了 RoCoFT，一种用于大规模语言模型 (LM) 的参数高效微调方法，该方法基于仅更新 Transformer 中权重矩阵的几行和几列。通过对 BERT 和 RoBERTa 等中型 LM 以及 Bloom-7B、Llama2-7B 和 Llama2-13B 等大型 LM 进行大量实验，我们表明，我们的方法与最先进的 PEFT 方法相比，可以提供相当或更好的准确率，同时内存和计算效率更高。我们还使用神经正切核理论工具研究了我们方法有效性的原因。我们通过经验证明，我们的核是使用一组受限的行和列参数构建的，在数值上接近全参数核，并提供可比的分类性能。进行了消融研究以调查不同算法选择的影响，包括行和列的选择策略以及有效实施我们方法的最佳等级。</li>
</ul>

<h3>Title: How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective</h3>
<ul>
<li><strong>Authors: </strong>Teng Xiao, Mingxiao Li, Yige Yuan, Huaisheng Zhu, Chao Cui, Vasant G Honavar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10093">https://arxiv.org/abs/2410.10093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10093">https://arxiv.org/pdf/2410.10093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10093]] How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective(https://arxiv.org/abs/2410.10093)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel generalized self-imitation learning ($\textbf{GSIL}$) framework, which effectively and efficiently aligns large language models with offline demonstration data. We develop $\textbf{GSIL}$ by deriving a surrogate objective of imitation learning with density ratio estimates, facilitating the use of self-generated data and optimizing the imitation learning objective with simple classification losses. $\textbf{GSIL}$ eliminates the need for complex adversarial training in standard imitation learning, achieving lightweight and efficient fine-tuning for large language models. In addition, $\textbf{GSIL}$ encompasses a family of offline losses parameterized by a general class of convex functions for density ratio estimation and enables a unified view for alignment with demonstration data. Extensive experiments show that $\textbf{GSIL}$ consistently and significantly outperforms baselines in many challenging benchmarks, such as coding (HuamnEval), mathematical reasoning (GSM8K) and instruction-following benchmark (MT-Bench).</li>
<li><strong>摘要：</strong>本文介绍了一种新颖的广义自模仿学习 ($\textbf{GSIL}$) 框架，该框架可有效且高效地将大型语言模型与离线演示数据对齐。我们通过推导具有密度比估计的模仿学习的替代目标来开发 $\textbf{GSIL}$，促进自生成数据的使用，并通过简单的分类损失优化模仿学习目标。$\textbf{GSIL}$ 消除了标准模仿学习中对复杂对抗训练的需求，实现了大型语言模型的轻量级和高效微调。此外，$\textbf{GSIL}$ 包含一组离线损失，这些损失由用于密度比估计的一般凸函数类参数化，并实现了与演示数据对齐的统一视图。大量实验表明，$\textbf{GSIL}$ 在许多具有挑战性的基准测试中始终如一地显著地超越基线，例如编码（HuamnEval）、数学推理（GSM8K）和指令跟踪基准测试（MT-Bench）。</li>
</ul>

<h3>Title: FormalAlign: Automated Alignment Evaluation for Autoformalization</h3>
<ul>
<li><strong>Authors: </strong>Jianqiao Lu, Yingjia Wan, Yinya Huang, Jing Xiong, Zhengying Liu, Zhijiang Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.FL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10135">https://arxiv.org/abs/2410.10135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10135">https://arxiv.org/pdf/2410.10135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10135]] FormalAlign: Automated Alignment Evaluation for Autoformalization(https://arxiv.org/abs/2410.10135)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Autoformalization aims to convert informal mathematical proofs into machine-verifiable formats, bridging the gap between natural and formal languages. However, ensuring semantic alignment between the informal and formalized statements remains challenging. Existing approaches heavily rely on manual verification, hindering scalability. To address this, we introduce \textsc{FormalAlign}, the first automated framework designed for evaluating the alignment between natural and formal languages in autoformalization. \textsc{FormalAlign} trains on both the autoformalization sequence generation task and the representational alignment between input and output, employing a dual loss that combines a pair of mutually enhancing autoformalization and alignment tasks. Evaluated across four benchmarks augmented by our proposed misalignment strategies, \textsc{FormalAlign} demonstrates superior performance. In our experiments, \textsc{FormalAlign} outperforms GPT-4, achieving an Alignment-Selection Score 11.58\% higher on \forml-Basic (99.21\% vs. 88.91\%) and 3.19\% higher on MiniF2F-Valid (66.39\% vs. 64.34\%). This effective alignment evaluation significantly reduces the need for manual verification. Both the dataset and code can be accessed via~\url{this https URL}.</li>
<li><strong>摘要：</strong>自动形式化旨在将非正式的数学证明转换为机器可验证的格式，从而弥合自然语言和形式语言之间的差距。然而，确保非正式和形式化语句之间的语义对齐仍然具有挑战性。现有的方法严重依赖人工验证，阻碍了可扩展性。为了解决这个问题，我们引入了 \textsc{FormalAlign}，这是第一个用于评估自动形式化中自然语言和形式语言之间的对齐的自动化框架。 \textsc{FormalAlign} 在自动形式化序列生成任务和输入与输出之间的表征对齐上进行训练，采用结合一对相互增强的自动形式化和对齐任务的双重损失。通过我们提出的错位策略增强的四个基准测试进行评估，\textsc{FormalAlign} 表现出卓越的性能。在我们的实验中，\textsc{FormalAlign} 的表现优于 GPT-4，在 \forml-Basic 上的对齐选择得分高出 11.58\%（99.21\% vs. 88.91\%），在 MiniF2F-Valid 上的对齐选择得分高出 3.19\%（66.39\% vs. 64.34\%）。这种有效的对齐评估大大减少了手动验证的需要。数据集和代码都可以通过~\url{此 https URL} 访问。</li>
</ul>

<h3>Title: Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations</h3>
<ul>
<li><strong>Authors: </strong>Garima Agrawal, Sashank Gummuluri, Cosimo Spera</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10136">https://arxiv.org/abs/2410.10136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10136">https://arxiv.org/pdf/2410.10136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10136]] Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations(https://arxiv.org/abs/2410.10136)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval augmented generation, agent</a></li>
<li><strong>Abstract: </strong>In customer contact centers, human agents often struggle with long average handling times (AHT) due to the need to manually interpret queries and retrieve relevant knowledge base (KB) articles. While retrieval augmented generation (RAG) systems using large language models (LLMs) have been widely adopted in industry to assist with such tasks, RAG faces challenges in real-time conversations, such as inaccurate query formulation and redundant retrieval of frequently asked questions (FAQs). To address these limitations, we propose a decision support system that can look beyond RAG by first identifying customer questions in real time. If the query matches an FAQ, the system retrieves the answer directly from the FAQ database; otherwise, it generates answers via RAG. Our approach reduces reliance on manual queries, providing responses to agents within 2 seconds. Deployed in AI-powered human-agent assist solution at Minerva CQ, this system improves efficiency, reduces AHT, and lowers operational costs. We also introduce an automated LLM-agentic workflow to identify FAQs from historical transcripts when no predefined FAQs exist.</li>
<li><strong>摘要：</strong>在客户联络中心，由于需要手动解释查询并检索相关知识库 (KB) 文章，人工代理通常会面临较长的平均处理时间 (AHT) 问题。虽然使用大型语言模型 (LLM) 的检索增强生成 (RAG) 系统已在行业中广泛用于协助完成此类任务，但 RAG 在实时对话中面临挑战，例如查询表述不准确和常见问题 (FAQ) 的冗余检索。为了解决这些限制，我们提出了一个决策支持系统，该系统可以超越 RAG，首先实时识别客户问题。如果查询与 FAQ 匹配，系统将直接从 FAQ 数据库中检索答案；否则，它会通过 RAG 生成答案。我们的方法减少了对手动查询的依赖，在 2 秒内向代理提供响应。该系统部署在 Minerva CQ 的 AI 驱动的人机代理辅助解决方案中，提高了效率，减少了 AHT，并降低了运营成本。我们还引入了一种自动化的 LLM-agentic 工作流程，当不存在预定义的 FAQ 时，可以从历史记录中识别 FAQ。</li>
</ul>

<h3>Title: Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Siru Ouyang, Shuohang Wang, Minhao Jiang, Ming Zhong, Donghan Yu, Jiawei Han, Yelong Shen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10141">https://arxiv.org/abs/2410.10141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10141">https://arxiv.org/pdf/2410.10141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10141]] Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation(https://arxiv.org/abs/2410.10141)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding stands as a pivotal technique to expedite inference in autoregressive (large) language models. This method employs a smaller draft model to speculate a block of tokens, which the target model then evaluates for acceptance. Despite a wealth of studies aimed at increasing the efficiency of speculative decoding, the influence of generation configurations on the decoding process remains poorly understood, especially concerning decoding temperatures. This paper delves into the effects of decoding temperatures on speculative decoding's efficacy. Beginning with knowledge distillation (KD), we first highlight the challenge of decoding at higher temperatures, and demonstrate KD in a consistent temperature setting could be a remedy. We also investigate the effects of out-of-domain testing sets with out-of-range temperatures. Building upon these findings, we take an initial step to further the speedup for speculative decoding, particularly in a high-temperature generation setting. Our work offers new insights into how generation configurations drastically affect the performance of speculative decoding, and underscores the need for developing methods that focus on diverse decoding configurations. Code is publically available at this https URL.</li>
<li><strong>摘要：</strong>推测解码是加速自回归（大型）语言模型推理的关键技术。该方法采用较小的草稿模型来推测一个标记块，然后目标模型对其进行评估以确定是否可接受。尽管有大量研究旨在提高推测解码的效率，但生成配置对解码过程的影响仍然知之甚少，尤其是解码温度方面。本文深入探讨了解码温度对推测解码效果的影响。从知识蒸馏 (KD) 开始，我们首先强调在较高温度下解码的挑战，并证明在一致的温度设置下进行 KD 可以是一种补救措施。我们还研究了超出范围温度的域外测试集的影响。基于这些发现，我们迈出了进一步加速推测解码的第一步，特别是在高温生成设置下。我们的工作提供了有关生成配置如何显著影响推测解码性能的新见解，并强调了开发专注于不同解码配置的方法的必要性。代码可在此 https URL 上公开获取。</li>
</ul>

<h3>Title: Jailbreak Instruction-Tuned LLMs via end-of-sentence MLP Re-weighting</h3>
<ul>
<li><strong>Authors: </strong>Yifan Luo, Zhennan Zhou, Meitan Wang, Bin Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10150">https://arxiv.org/abs/2410.10150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10150">https://arxiv.org/pdf/2410.10150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10150]] Jailbreak Instruction-Tuned LLMs via end-of-sentence MLP Re-weighting(https://arxiv.org/abs/2410.10150)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the safety mechanisms of instruction fine-tuned large language models (LLMs). We discover that re-weighting MLP neurons can significantly compromise a model's safety, especially for MLPs in end-of-sentence inferences. We hypothesize that LLMs evaluate the harmfulness of prompts during end-of-sentence inferences, and MLP layers plays a critical role in this process. Based on this hypothesis, we develop 2 novel white-box jailbreak methods: a prompt-specific method and a prompt-general method. The prompt-specific method targets individual prompts and optimizes the attack on the fly, while the prompt-general method is pre-trained offline and can generalize to unseen harmful prompts. Our methods demonstrate robust performance across 7 popular open-source LLMs, size ranging from 2B to 72B. Furthermore, our study provides insights into vulnerabilities of instruction-tuned LLM's safety and deepens the understanding of the internal mechanisms of LLMs.</li>
<li><strong>摘要：</strong>在本文中，我们研究了指令微调大型语言模型 (LLM) 的安全机制。我们发现重新加权 MLP 神经元会严重损害模型的安全性，尤其是对于句末推理中的 MLP。我们假设 LLM 在句末推理期间评估提示的有害性，而 MLP 层在此过程中起着关键作用。基于这一假设，我们开发了两种新颖的白盒越狱方法：一种提示特定方法和一种提示通用方法。提示特定方法针对单个提示并动态优化攻击，而提示通用方法经过离线预训练，可以推广到看不见的有害提示。我们的方法在 7 个流行的开源 LLM 中表现出强大的性能，大小从 2B 到 72B 不等。此外，我们的研究提供了对指令调整的 LLM 安全性漏洞的见解，并加深了对 LLM 内部机制的理解。</li>
</ul>

<h3>Title: Diagnosing Hate Speech Classification: Where Do Humans and Machines Disagree, and Why?</h3>
<ul>
<li><strong>Authors: </strong>Xilin Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10153">https://arxiv.org/abs/2410.10153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10153">https://arxiv.org/pdf/2410.10153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10153]] Diagnosing Hate Speech Classification: Where Do Humans and Machines Disagree, and Why?(https://arxiv.org/abs/2410.10153)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This study uses the cosine similarity ratio, embedding regression, and manual re-annotation to diagnose hate speech classification. We begin by computing cosine similarity ratio on a dataset "Measuring Hate Speech" that contains 135,556 annotated comments on social media. This way, we show a basic use of cosine similarity as a description of hate speech content. We then diagnose hate speech classification starting from understanding the inconsistency of human annotation from the dataset. Using embedding regression as a basic diagnostic, we found that female annotators are more sensitive to racial slurs that target the black population. We perform with a more complicated diagnostic by training a hate speech classifier using a SoTA pre-trained large language model, NV-Embed-v2, to convert texts to embeddings and run a logistic regression. This classifier achieves a testing accuracy of 94%. In diagnosing where machines disagree with human annotators, we found that machines make fewer mistakes than humans despite the fact that human annotations are treated as ground truth in the training set. Machines perform better in correctly labeling long statements of facts, but perform worse in labeling short instances of swear words. We hypothesize that this is due to model alignment - while curating models at their creation prevents the models from producing obvious hate speech, it also reduces the model's ability to detect such content.</li>
<li><strong>摘要：</strong>本研究使用余弦相似度比、嵌入回归和手动重新注释来诊断仇恨言论分类。我们首先在包含 135,556 条社交媒体注释评论的数据集“测量仇恨言论”上计算余弦相似度比。这样，我们展示了余弦相似度作为仇恨言论内容描述的基本用法。然后，我们从了解数据集中人工注释的不一致性开始诊断仇恨言论分类。使用嵌入回归作为基本诊断，我们发现女性注释者对针对黑人群体的种族诽谤更为敏感。我们使用更复杂的诊断方法，通过使用 SoTA 预训练的大型语言模型 NV-Embed-v2 训练仇恨言论分类器，将文本转换为嵌入并运行逻辑回归。该分类器的测试准确率达到 94%。在诊断机器与人类注释者意见不一致的地方时，我们发现，尽管人类注释在训练集中被视为基本事实，但机器犯的错误比人类少。机器在正确标记长篇事实陈述方面表现更好，但在标记短篇脏话方面表现更差。我们假设这是由于模型对齐造成的 - 虽然在创建模型时对其进行管理可以防止模型产生明显的仇恨言论，但它也会降低模型检测此类内容的能力。</li>
</ul>

<h3>Title: Scalable Multi-Domain Adaptation of Language Models using Modular Experts</h3>
<ul>
<li><strong>Authors: </strong>Peter Schafhalter, Shun Liao, Yanqi Zhou, Chih-Kuan Yeh, Arun Kandoor, James Laudon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10181">https://arxiv.org/abs/2410.10181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10181">https://arxiv.org/pdf/2410.10181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10181]] Scalable Multi-Domain Adaptation of Language Models using Modular Experts(https://arxiv.org/abs/2410.10181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Domain-specific adaptation is critical to maximizing the performance of pre-trained language models (PLMs) on one or multiple targeted tasks, especially under resource-constrained use cases, such as edge devices. However, existing methods often struggle to balance domain-specific performance, retention of general knowledge, and efficiency for training and inference. To address these challenges, we propose Modular Domain Experts (MoDE). MoDE is a mixture-of-experts architecture that augments a general PLMs with modular, domain-specialized experts. These experts are trained independently and composed together via a lightweight training process. In contrast to standard low-rank adaptation methods, each MoDE expert consists of several transformer layers which scale better with more training examples and larger parameter counts. Our evaluation demonstrates that MoDE achieves comparable target performances to full parameter fine-tuning while achieving 1.65% better retention performance. Moreover, MoDE's architecture enables flexible sharding configurations and improves training speeds by up to 38% over state-of-the-art distributed training configurations.</li>
<li><strong>摘要：</strong>领域特定适应对于最大化预训练语言模型 (PLM) 在一个或多个目标任务上的性能至关重要，尤其是在资源受限的用例（例如边缘设备）下。然而，现有方法通常难以平衡领域特定性能、一般知识的保留以及训练和推理的效率。为了应对这些挑战，我们提出了模块化领域专家 (MoDE)。MoDE 是一种混合专家架构，它通过模块化、领域专业化的专家增强了通用 PLM。这些专家经过独立训练，并通过轻量级训练过程组合在一起。与标准低秩适应方法相比，每个 MoDE 专家由多个转换器层组成，这些转换器层可以通过更多训练示例和更大参数数量更好地扩展。我们的评估表明，MoDE 实现了与全参数微调相当的目标性能，同时实现了 1.65% 更好的保留性能。此外，MoDE 的架构支持灵活的分片配置，并且与最先进的分布式训练配置相比，训练速度提高了 38%。</li>
</ul>

<h3>Title: Effi-Code: Unleashing Code Efficiency in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dong Huang, Guangtao Zeng, Jianbo Dai, Meng Luo, Han Weng, Yuhao Qing, Heming Cui, Zhijiang Guo, Jie M. Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10209">https://arxiv.org/abs/2410.10209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10209">https://arxiv.org/pdf/2410.10209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10209]] Effi-Code: Unleashing Code Efficiency in Language Models(https://arxiv.org/abs/2410.10209)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As the use of large language models (LLMs) for code generation becomes more prevalent in software development, it is critical to enhance both the efficiency and correctness of the generated code. Existing methods and models primarily focus on the correctness of LLM-generated code, ignoring efficiency. In this work, we present Effi-Code, an approach to enhancing code generation in LLMs that can improve both efficiency and correctness. We introduce a Self-Optimization process based on Overhead Profiling that leverages open-source LLMs to generate a high-quality dataset of correct and efficient code samples. This dataset is then used to fine-tune various LLMs. Our method involves the iterative refinement of generated code, guided by runtime performance metrics and correctness checks. Extensive experiments demonstrate that models fine-tuned on the Effi-Code show significant improvements in both code correctness and efficiency across task types. For example, the pass@1 of DeepSeek-Coder-6.7B-Instruct generated code increases from \textbf{43.3\%} to \textbf{76.8\%}, and the average execution time for the same correct tasks decreases by \textbf{30.5\%}. Effi-Code offers a scalable and generalizable approach to improving code generation in AI systems, with potential applications in software development, algorithm design, and computational problem-solving. The source code of Effi-Code was released in \url{this https URL}.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 在软件开发中越来越普遍地用于代码生成，提高生成代码的效率和正确性至关重要。现有的方法和模型主要关注 LLM 生成代码的正确性，而忽略了效率。在这项工作中，我们提出了 Effi-Code，这是一种增强 LLM 中代码生成的方法，可以提高效率和正确性。我们引入了一种基于开销分析的自优化过程，该过程利用开源 LLM 生成高质量、正确且高效的代码示例数据集。然后使用该数据集对各种 LLM 进行微调。我们的方法涉及对生成的代码进行迭代细化，并以运行时性能指标和正确性检查为指导。大量实验表明，在 Effi-Code 上微调的模型在各种任务类型的代码正确性和效率方面均有显著提高。例如，DeepSeek-Coder-6.7B-Instruct 生成代码的 pass@1 从 \textbf{43.3\%} 增加到 \textbf{76.8\%}，而相同正确任务的平均执行时间减少了 \textbf{30.5\%}。Effi-Code 提供了一种可扩展且可推广的方法来改进 AI 系统中的代码生成，在软件开发、算法设计和计算问题解决方面具有潜在应用。Effi-Code 的源代码已在 \url{此 https URL} 中发布。</li>
</ul>

<h3>Title: Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key</h3>
<ul>
<li><strong>Authors: </strong>Yingda Chen, Xingjun Wang, Jintao Huang, Yunlin Mao, Daoze Zhang, Yuze Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10210">https://arxiv.org/abs/2410.10210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10210">https://arxiv.org/pdf/2410.10210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10210]] Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key(https://arxiv.org/abs/2410.10210)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>As large language models rapidly evolve to support longer context, there is a notable disparity in their capability to generate output at greater lengths. Recent study suggests that the primary cause for this imbalance may arise from the lack of data with long-output during alignment training. In light of this observation, attempts are made to re-align foundation models with data that fills the gap, which result in models capable of generating lengthy output when instructed. In this paper, we explore the impact of data-quality in tuning a model for long output, and the possibility of doing so from the starting points of human-aligned (instruct or chat) models. With careful data curation, we show that it possible to achieve similar performance improvement in our tuned models, with only a small fraction of training data instances and compute. In addition, we assess the generalizability of such approaches by applying our tuning-recipes to several models. our findings suggest that, while capacities for generating long output vary across different models out-of-the-box, our approach to tune them with high-quality data using lite compute, consistently yields notable improvement across all models we experimented on. We have made public our curated dataset for tuning long-writing capability, the implementations of model tuning and evaluation, as well as the fine-tuned models, all of which can be openly-accessed.</li>
<li><strong>摘要：</strong>随着大型语言模型迅速发展以支持更长的上下文，它们在生成更长输出的能力方面存在显著差异。最近的研究表明，这种不平衡的主要原因可能是在对齐训练期间缺乏具有长输出的数据。鉴于这一观察，人们尝试用填补空白的数据重新对齐基础模型，从而使模型能够在指示时生成长输出。在本文中，我们探讨了数据质量对调整长输出模型的影响，以及从人机对齐（指导或聊天）模型的起点开始这样做的可能性。通过仔细的数据管理，我们表明，仅使用一小部分训练数据实例和计算，就可以在调整后的模型中实现类似的性能改进。此外，我们通过将我们的调整方法应用于多个模型来评估此类方法的通用性。我们的研究结果表明，虽然不同模型生成长输出的能力各不相同，但我们使用轻量计算对它们进行调整的方法，在所有我们试验的模型中都取得了显著的改进。我们已经公开了我们为调整长写入能力而精心挑选的数据集、模型调整和评估的实现以及微调后的模型，所有这些都可以公开访问。</li>
</ul>

<h3>Title: SkillAggregation: Reference-free LLM-Dependent Aggregation</h3>
<ul>
<li><strong>Authors: </strong>Guangzhi Sun, Anmol Kagrecha, Potsawee Manakul, Phil Woodland, Mark Gales</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10215">https://arxiv.org/abs/2410.10215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10215">https://arxiv.org/pdf/2410.10215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10215]] SkillAggregation: Reference-free LLM-Dependent Aggregation(https://arxiv.org/abs/2410.10215)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used to assess NLP tasks due to their ability to generate human-like judgments. Single LLMs were used initially, however, recent work suggests using multiple LLMs as judges yields improved performance. An important step in exploiting multiple judgements is the combination stage, aggregation. Existing methods in NLP either assign equal weight to all LLM judgments or are designed for specific tasks such as hallucination detection. This work focuses on aggregating predictions from multiple systems where no reference labels are available. A new method called SkillAggregation is proposed, which learns to combine estimates from LLM judges without needing additional data or ground truth. It extends the Crowdlayer aggregation method, developed for image classification, to exploit the judge estimates during inference. The approach is compared to a range of standard aggregation methods on HaluEval-Dialogue, TruthfulQA and Chatbot Arena tasks. SkillAggregation outperforms Crowdlayer on all tasks, and yields the best performance over all approaches on the majority of tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 越来越多地用于评估 NLP 任务，因为它们能够生成类似人类的判断。最初使用的是单个 LLM，但最近的研究表明，使用多个 LLM 作为评判者可以提高性能。利用多个判断的一个重要步骤是组合阶段，即聚合。NLP 中的现有方法要么为所有 LLM 判断分配相同的权重，要么专为特定任务（例如幻觉检测）而设计。这项工作侧重于聚合来自没有参考标签的多个系统的预测。提出了一种称为 SkillAggregation 的新方法，该方法可以学习组合来自 LLM 评判者的估计，而无需额外的数据或基本事实。它扩展了为图像分类开发的 Crowdlayer 聚合方法，以在推理过程中利用评判者的估计。该方法与 HaluEval-Dialogue、TruthfulQA 和 Chatbot Arena 任务的一系列标准聚合方法进行了比较。SkillAggregation 在所有任务上的表现都优于 Crowdlayer，并且在大多数任务上获得了优于所有方法的最佳性能。</li>
</ul>

<h3>Title: QE-EBM: Using Quality Estimators as Energy Loss for Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Gahyun Yoo, Jay Yoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10228">https://arxiv.org/abs/2410.10228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10228">https://arxiv.org/pdf/2410.10228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10228]] QE-EBM: Using Quality Estimators as Energy Loss for Machine Translation(https://arxiv.org/abs/2410.10228)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning has shown great promise in aligning language models with human preferences in a variety of text generation tasks, including machine translation. For translation tasks, rewards can easily be obtained from quality estimation (QE) models which can generate rewards for unlabeled data. Despite its usefulness, reinforcement learning cannot exploit the gradients with respect to the QE score. We propose QE-EBM, a method of employing quality estimators as trainable loss networks that can directly backpropagate to the NMT model. We examine our method on several low and high resource target languages with English as the source language. QE-EBM outperforms strong baselines such as REINFORCE and proximal policy optimization (PPO) as well as supervised fine-tuning for all target languages, especially low-resource target languages. Most notably, for English-to-Mongolian translation, our method achieves improvements of 2.5 BLEU, 7.1 COMET-KIWI, 5.3 COMET, and 6.4 XCOMET relative to the supervised baseline.</li>
<li><strong>摘要：</strong>强化学习在将语言模型与人类偏好相结合方面表现出了巨大的潜力，可用于各种文本生成任务，包括机器翻译。对于翻译任务，可以轻松从质量估计 (QE) 模型中获得奖励，该模型可以为未标记数据生成奖励。尽管强化学习很有用，但它无法利用 QE 分数的梯度。我们提出了 QE-EBM，这是一种将质量估计器用作可训练损失网络的方法，可以直接反向传播到 NMT 模型。我们在以英语为源语言的几种低资源和高资源目标语言上测试了我们的方法。对于所有目标语言，尤其是低资源目标语言，QE-EBM 的表现都优于 REINFORCE 和近端策略优化 (PPO) 等强基线以及监督微调。最值得注意的是，对于英语到蒙古语的翻译，我们的方法相对于监督基线实现了 2.5 BLEU、7.1 COMET-KIWI、5.3 COMET 和 6.4 XCOMET 的改进。</li>
</ul>

<h3>Title: A Multi-Task Text Classification Pipeline with Natural Language Explanations: A User-Centric Evaluation in Sentiment Analysis and Offensive Language Identification in Greek Tweets</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Mylonas, Nikolaos Stylianou, Theodora Tsikrika, Stefanos Vrochidis, Ioannis Kompatsiaris</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10290">https://arxiv.org/abs/2410.10290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10290">https://arxiv.org/pdf/2410.10290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10290]] A Multi-Task Text Classification Pipeline with Natural Language Explanations: A User-Centric Evaluation in Sentiment Analysis and Offensive Language Identification in Greek Tweets(https://arxiv.org/abs/2410.10290)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Interpretability is a topic that has been in the spotlight for the past few years. Most existing interpretability techniques produce interpretations in the form of rules or feature importance. These interpretations, while informative, may be harder to understand for non-expert users and therefore, cannot always be considered as adequate explanations. To that end, explanations in natural language are often preferred, as they are easier to comprehend and also more presentable to end-users. This work introduces an early concept for a novel pipeline that can be used in text classification tasks, offering predictions and explanations in natural language. It comprises of two models: a classifier for labelling the text and an explanation generator which provides the explanation. The proposed pipeline can be adopted by any text classification task, given that ground truth rationales are available to train the explanation generator. Our experiments are centred around the tasks of sentiment analysis and offensive language identification in Greek tweets, using a Greek Large Language Model (LLM) to obtain the necessary explanations that can act as rationales. The experimental evaluation was performed through a user study based on three different metrics and achieved promising results for both datasets.</li>
<li><strong>摘要：</strong>可解释性是过去几年备受关注的话题。大多数现有的可解释性技术都以规则或特征重要性的形式产生解释。这些解释虽然信息丰富，但对于非专家用户来说可能更难理解，因此不能总是被视为充分的解释。为此，人们通常更喜欢使用自然语言的解释，因为它们更容易理解，也更易于向最终用户呈现。这项工作引入了一种新型管道的早期概念，该管道可用于文本分类任务，以自然语言提供预测和解释。它由两个模型组成：用于标记文本的分类器和提供解释的解释生成器。只要有基本事实原理可用于训练解释生成器，任何文本分类任务都可以采用所提出的管道。我们的实验围绕希腊推文中的情绪分析和攻击性语言识别任务展开，使用希腊大型语言模型 (LLM) 来获得可以作为原理的必要解释。实验评估是通过基于三种不同指标的用户研究进行的，并对两个数据集取得了令人满意的结果。</li>
</ul>

<h3>Title: A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification</h3>
<ul>
<li><strong>Authors: </strong>Aryan Singhal, Veronica Shao, Gary Sun, Ryan Ding, Jonathan Lu, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10303">https://arxiv.org/abs/2410.10303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10303">https://arxiv.org/pdf/2410.10303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10303]] A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification(https://arxiv.org/abs/2410.10303)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The rise of digital misinformation has heightened interest in using multilingual Large Language Models (LLMs) for fact-checking. This study systematically evaluates translation bias and the effectiveness of LLMs for cross-lingual claim verification across 15 languages from five language families: Romance, Slavic, Turkic, Indo-Aryan, and Kartvelian. Using the XFACT dataset to assess their impact on accuracy and bias, we investigate two distinct translation methods: pre-translation and self-translation. We use mBERT's performance on the English dataset as a baseline to compare language-specific accuracies. Our findings reveal that low-resource languages exhibit significantly lower accuracy in direct inference due to underrepresentation in the training data. Furthermore, larger models demonstrate superior performance in self-translation, improving translation accuracy and reducing bias. These results highlight the need for balanced multilingual training, especially in low-resource languages, to promote equitable access to reliable fact-checking tools and minimize the risk of spreading misinformation in different linguistic contexts.</li>
<li><strong>摘要：</strong>数字错误信息的兴起引起了人们对使用多语言大型语言模型 (LLM) 进行事实核查的兴趣。本研究系统地评估了翻译偏见和 LLM 在五个语系的 15 种语言中进行跨语言声明验证的有效性：罗曼语、斯拉夫语、突厥语、印度-雅利安语和卡特维利语。使用 XFACT 数据集来评估它们对准确性和偏见的影响，我们研究了两种不同的翻译方法：预翻译和自翻译。我们使用 mBERT 在英语数据集上的表现作为基线来比较特定语言的准确性。我们的研究结果表明，由于训练数据中的代表性不足，资源匮乏的语言在直接推理中的准确性明显较低。此外，较大的模型在自翻译方面表现出卓越的性能，提高了翻译准确性并减少了偏见。这些结果强调了平衡多语言训练的必要性，特别是在资源匮乏的语言中，以促进公平使用可靠的事实核查工具，并最大限度地降低在不同语言环境中传播错误信息的风险。</li>
</ul>

<h3>Title: EasyRAG: Efficient Retrieval-Augmented Generation Framework for Network Automated Operations</h3>
<ul>
<li><strong>Authors: </strong>Zhangchi Feng, Dongdong Kuang, Zhongyuan Wang, Zhijie Nie, Yaowei Zheng, Richong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10315">https://arxiv.org/abs/2410.10315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10315">https://arxiv.org/pdf/2410.10315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10315]] EasyRAG: Efficient Retrieval-Augmented Generation Framework for Network Automated Operations(https://arxiv.org/abs/2410.10315)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>This paper presents EasyRAG, a simple, lightweight, and efficient retrieval-augmented generation framework for network automated operations. The advantages of our solution are: this http URL Question Answering: We designed a straightforward RAG scheme based on (1) a specific data processing workflow (2) dual-route sparse retrieval for coarse ranking (3) LLM Reranker for reranking (4) LLM answer generation and optimization. This approach achieved first place in the GLM4 track in the preliminary round and second place in the GLM4 track in the semifinals. this http URL Deployment: Our method primarily consists of BM25 retrieval and BGE-reranker reranking, requiring no fine-tuning of any models, occupying minimal VRAM, easy to deploy, and highly scalable; we provide a flexible code library with various search and generation strategies, facilitating custom process implementation. this http URL Inference: We designed an efficient inference acceleration scheme for the entire coarse ranking, reranking, and generation process that significantly reduces the inference latency of RAG while maintaining a good level of accuracy; each acceleration scheme can be plug-and-play into any component of the RAG process, consistently enhancing the efficiency of the RAG system. Our code and data are released at this https URL.</li>
<li><strong>摘要：</strong>本文介绍了 EasyRAG，一种简单、轻量、高效的检索增强生成框架，用于网络自动化运营。我们的解决方案的优势在于：此 http URL 问答：我们设计了一个简单的 RAG 方案，基于（1）特定的数据处理工作流程（2）双路由稀疏检索进行粗排序（3）LLM Reranker 用于重新排序（4）LLM 答案生成和优化。该方法在初赛中获得了 GLM4 赛道的第一名，在半决赛中获得了 GLM4 赛道的第二名。此 http URL 部署：我们的方法主要包括 BM25 检索和 BGE-reranker 重新排序，不需要对任何模型进行微调，占用最小的 VRAM，易于部署且高度可扩展；我们提供了一个灵活的代码库，具有各种搜索和生成策略，方便自定义流程实施。此 http URL 推理：我们为整个粗排序、重新排序和生成过程设计了高效的推理加速方案，在保持良好准确度水平的同时显著降低了 RAG 的推理延迟；每个加速方案都可以即插即用到 RAG 流程的任何组件中，从而持续提升 RAG 系统的效率。我们的代码和数据在此 https URL 发布。</li>
</ul>

<h3>Title: MentalGLM Series: Explainable Large Language Models for Mental Health Analysis on Chinese Social Media</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhai, Nan Bai, Qing Zhao, Jianqiang Li, Fan Wang, Hongzhi Qi, Meng Jiang, Xiaoqin Wang, Bing Xiang Yang, Guanghui Fu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10323">https://arxiv.org/abs/2410.10323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10323">https://arxiv.org/pdf/2410.10323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10323]] MentalGLM Series: Explainable Large Language Models for Mental Health Analysis on Chinese Social Media(https://arxiv.org/abs/2410.10323)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>As the prevalence of mental health challenges, social media has emerged as a key platform for individuals to express their this http URL learning tends to be a promising solution for analyzing mental health on social media. However, black box models are often inflexible when switching between tasks, and their results typically lack explanations. With the rise of large language models (LLMs), their flexibility has introduced new approaches to the field. Also due to the generative nature, they can be prompted to explain decision-making processes. However, their performance on complex psychological analysis still lags behind deep learning. In this paper, we introduce the first multi-task Chinese Social Media Interpretable Mental Health Instructions (C-IMHI) dataset, consisting of 9K samples, which has been quality-controlled and manually validated. We also propose MentalGLM series models, the first open-source LLMs designed for explainable mental health analysis targeting Chinese social media, trained on a corpus of 50K instructions. The proposed models were evaluated on three downstream tasks and achieved better or comparable performance compared to deep learning models, generalized LLMs, and task fine-tuned LLMs. We validated a portion of the generated decision explanations with experts, showing promising results. We also evaluated the proposed models on a clinical dataset, where they outperformed other LLMs, indicating their potential applicability in the clinical field. Our models show strong performance, validated across tasks and perspectives. The decision explanations enhance usability and facilitate better understanding and practical application of the models. Both the constructed dataset and the models are publicly available via: this https URL.</li>
<li><strong>摘要：</strong>随着心理健康问题的日益普遍，社交媒体已成为个人表达自己想法的重要平台，这种学习方式往往是分析社交媒体上心理健康状况的一种有前途的解决方案。然而，黑盒模型在任务之间切换时往往缺乏灵活性，其结果通常缺乏解释。随着大型语言模型 (LLM) 的兴起，它们的灵活性为该领域带来了新的方法。此外，由于其生成性，它们可以被提示解释决策过程。然而，它们在复杂心理分析上的表现仍然落后于深度学习。在本文中，我们介绍了第一个多任务中文社交媒体可解释心理健康指令 (C-IMHI) 数据集，该数据集由 9K 个样本组成，经过质量控制和手动验证。我们还提出了 MentalGLM 系列模型，这是第一个针对中文社交媒体的可解释心理健康分析而设计的开源 LLM，在 50K 条指令的语料库上进行训练。所提出的模型在三个下游任务上进行了评估，与深度学习模型、广义 LLM 和任务微调 LLM 相比，取得了更好或相当的性能。我们与专家一起验证了生成的部分决策解释，显示出令人鼓舞的结果。我们还在临床数据集上评估了所提出的模型，它们的表现优于其他 LLM，表明它们在临床领域具有潜在的适用性。我们的模型表现出色，在各个任务和视角中都得到了验证。决策解释增强了可用性，并有助于更好地理解和实际应用模型。构建的数据集和模型都可以通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Locking Down the Finetuned LLMs Safety</h3>
<ul>
<li><strong>Authors: </strong>Minjun Zhu, Linyi Yang, Yifan Wei, Ningyu Zhang, Yue Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10343">https://arxiv.org/abs/2410.10343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10343">https://arxiv.org/pdf/2410.10343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10343]] Locking Down the Finetuned LLMs Safety(https://arxiv.org/abs/2410.10343)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) on additional datasets is often necessary to optimize them for specific downstream tasks. However, existing safety alignment measures, which restrict harmful behavior during inference, are insufficient to mitigate safety risks during fine-tuning. Alarmingly, fine-tuning with just 10 toxic sentences can make models comply with harmful instructions. We introduce SafetyLock, a novel alignment intervention method that maintains robust safety post-fine-tuning through efficient and transferable mechanisms. SafetyLock leverages our discovery that fine-tuned models retain similar safety-related activation representations to their base models. This insight enables us to extract what we term the Meta-SafetyLock, a set of safety bias directions representing key activation patterns associated with safe responses in the original model. We can then apply these directions universally to fine-tuned models to enhance their safety. By searching for activation directions across multiple token dimensions, SafetyLock achieves enhanced robustness and transferability. SafetyLock re-aligns fine-tuned models in under 0.01 seconds without additional computational cost. Our experiments demonstrate that SafetyLock can reduce the harmful instruction response rate from 60% to below 1% in toxic fine-tuned models. It surpasses traditional methods in both performance and efficiency, offering a scalable, non-invasive solution for ensuring the safety of customized LLMs. Our analysis across various fine-tuning scenarios confirms SafetyLock's robustness, advocating its integration into safety protocols for aligned LLMs. The code is released at this https URL.</li>
<li><strong>摘要：</strong>通常需要在其他数据集上对大型语言模型 (LLM) 进行微调，以针对特定的下游任务对其进行优化。然而，现有的安全对齐措施（在推理过程中限制有害行为）不足以减轻微调过程中的安全风险。令人震惊的是，仅使用 10 个有毒句子进行微调就可以使模型遵守有害指令。我们引入了 SafetyLock，这是一种新颖的对齐干预方法，可通过高效且可转移的机制在微调后保持稳健的安全性。SafetyLock 利用了我们的发现，即微调后的模型保留了与其基础模型相似的安全相关激活表示。这一见解使我们能够提取我们称之为 Meta-SafetyLock 的东西，这是一组安全偏差方向，代表与原始模型中的安全响应相关的关键激活模式。然后，我们可以将这些方向普遍应用于微调后的模型以增强其安全性。通过在多个 token 维度上搜索激活方向，SafetyLock 实现了增强的稳健性和可转移性。SafetyLock 可在 0.01 秒内重新对齐微调后的模型，而无需额外的计算成本。我们的实验表明，SafetyLock 可以将有毒微调模型中的有害指令响应率从 60% 降低到 1% 以下。它在性能和效率方面都超越了传统方法，为确保定制 LLM 的安全性提供了一种可扩展、非侵入式的解决方案。我们对各种微调场景的分析证实了 SafetyLock 的稳健性，并主张将其集成到对齐 LLM 的安全协议中。代码在此 https URL 上发布。</li>
</ul>

<h3>Title: A Unified Approach to Routing and Cascading for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jasper Dekoninck, Maximilian Baader, Martin Vechev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10347">https://arxiv.org/abs/2410.10347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10347">https://arxiv.org/pdf/2410.10347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10347]] A Unified Approach to Routing and Cascading for LLMs(https://arxiv.org/abs/2410.10347)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The widespread applicability of large language models (LLMs) has increased the availability of many fine-tuned models of various sizes targeting specific tasks. Given a set of such specialized models, to maximize overall performance, it is important to figure out the optimal strategy for selecting the right model for a given user query. An effective strategy could drastically increase overall performance and even offer improvements over a single large monolithic model. Existing approaches typically fall into two categories: routing, where a single model is selected for each query, and cascading, which runs a sequence of increasingly larger models until a satisfactory answer is obtained. However, both have notable limitations: routing commits to an initial model without flexibility, while cascading requires executing every model in sequence, which can be inefficient. Additionally, the conditions under which these strategies are provably optimal remain unclear. In this work, we derive optimal strategies for both routing and cascading. Building on this analysis, we propose a novel approach called cascade routing, which combines the adaptability of routing with the cost-efficiency of cascading. Our experiments demonstrate that cascade routing consistently outperforms both routing and cascading across a variety of settings, improving both output quality and lowering computational cost, thus offering a unified and efficient solution to the model selection problem.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的广泛适用性增加了针对特定任务的各种大小的微调模型的可用性。给定一组这样的专用模型，为了最大限度地提高整体性能，重要的是找出为给定用户查询选择正确模型的最佳策略。有效的策略可以大大提高整体性能，甚至可以提供比单个大型单片模型更好的性能。现有方法通常分为两类：路由，其中​​为每个查询选择一个模型，以及级联，运行一系列越来越大的模型，直到获得满意的答案。然而，两者都有明显的局限性：路由承诺使用初始模型而没有灵活性，而级联需要按顺序执行每个模型，这可能效率低下。此外，这些策略可证明最佳的条件仍不清楚。在这项工作中，我们推导出路由和级联的最佳策略。基于这一分析，我们提出了一种称为级联路由的新方法，它将路由的适应性与级联的成本效率相结合。我们的实验表明，级联路由在各种设置中始终优于路由和级联，既提高了输出质量又降低了计算成本，从而为模型选择问题提供了统一而有效的解决方案。</li>
</ul>

<h3>Title: Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and Refinement</h3>
<ul>
<li><strong>Authors: </strong>Joseph Shtok, Amit Alfassy, Foad Abo Dahood, Eliyahu Schwartz, Sivan Doveh, Assaf Arbelle</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10348">https://arxiv.org/abs/2410.10348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10348">https://arxiv.org/pdf/2410.10348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10348]] Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and Refinement(https://arxiv.org/abs/2410.10348)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>It has been shown that Large Language Models' (LLMs) performance can be improved for many tasks using Chain of Thought (CoT) or In-Context Learning (ICL), which involve demonstrating the steps needed to solve a task using a few examples. However, while datasets with input-output pairs are relatively easy to produce, providing demonstrations which include intermediate steps requires cumbersome manual work. These steps may be executable programs, as in agentic flows, or step-by-step reasoning as in CoT. In this work, we propose Automatic Data Labeling and Refinement (ADLR), a method to automatically generate and filter demonstrations which include the above intermediate steps, starting from a small seed of manually crafted examples. We demonstrate the advantage of ADLR in code-based table QA and mathematical reasoning, achieving up to a 5.5% gain. The code implementing our method is provided in the Supplementary material and will be made available.</li>
<li><strong>摘要：</strong>事实证明，使用思路链 (CoT) 或上下文学习 (ICL) 可以提高大型语言模型 (LLM) 在许多任务上的性能，这些方法涉及使用一些示例演示解决任务所需的步骤。然而，虽然具有输入输出对的数据集相对容易生成，但提供包含中间步骤的演示需要繁琐的手动工作。这些步骤可能是可执行程序，如代理流，或逐步推理，如 CoT。在这项工作中，我们提出了自动数据标记和细化 (ADLR)，这是一种从少量手工制作的示例开始自动生成和过滤包含上述中间步骤的演示的方法。我们展示了 ADLR 在基于代码的表格 QA 和数学推理中的优势，实现了高达 5.5% 的增益。实现我们方法的代码在补充材料中提供，并将提供。</li>
</ul>

<h3>Title: LLM-based Code-Switched Text Generation for Grammatical Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Tom Potter, Zheng Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10349">https://arxiv.org/abs/2410.10349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10349">https://arxiv.org/pdf/2410.10349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10349]] LLM-based Code-Switched Text Generation for Grammatical Error Correction(https://arxiv.org/abs/2410.10349)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>With the rise of globalisation, code-switching (CSW) has become a ubiquitous part of multilingual conversation, posing new challenges for natural language processing (NLP), especially in Grammatical Error Correction (GEC). This work explores the complexities of applying GEC systems to CSW texts. Our objectives include evaluating the performance of state-of-the-art GEC systems on an authentic CSW dataset from English as a Second Language (ESL) learners, exploring synthetic data generation as a solution to data scarcity, and developing a model capable of correcting grammatical errors in monolingual and CSW texts. We generated synthetic CSW GEC data, resulting in one of the first substantial datasets for this task, and showed that a model trained on this data is capable of significant improvements over existing systems. This work targets ESL learners, aiming to provide educational technologies that aid in the development of their English grammatical correctness without constraining their natural multilingualism.</li>
<li><strong>摘要：</strong>随着全球化的兴起，代码转换 (CSW) 已成为多语言对话中无处不在的一部分，对自然语言处理 (NLP) 提出了新的挑战，尤其是在语法错误纠正 (GEC) 方面。这项工作探讨了将 GEC 系统应用于 CSW 文本的复杂性。我们的目标包括评估最先进的 GEC 系统在来自英语作为第二语言 (ESL) 学习者的真实 CSW 数据集上的表现，探索合成数据生成作为数据稀缺的解决方案，以及开发一种能够纠正单语和 CSW 文本中语法错误的模型。我们生成了合成的 CSW GEC 数据，从而为这项任务提供了首批大量数据集之一，并表明基于这些数据训练的模型能够比现有系统有显著的改进。这项工作针对 ESL 学习者，旨在提供教育技术，帮助他们提高英语语法的正确性，而不会限制他们自然的多语言能力。</li>
</ul>

<h3>Title: Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yongxin Xu, Ruizhe Zhang, Xinke Jiang, Yujie Feng, Yuzhen Xiao, Xinyu Ma, Runchuan Zhu, Xu Chu, Junfeng Zhao, Yasha Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10360">https://arxiv.org/abs/2410.10360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10360">https://arxiv.org/pdf/2410.10360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10360]] Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning(https://arxiv.org/abs/2410.10360)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) offers an effective solution to the issues faced by Large Language Models (LLMs) in hallucination generation and knowledge obsolescence by incorporating externally retrieved knowledge. However, due to potential conflicts between internal and external knowledge, as well as retrieval noise, LLMs often struggle to effectively integrate external evidence, leading to a decline in performance. Although existing methods attempt to tackle these challenges, they often struggle to strike a balance between model adherence and robustness, resulting in significant learning variance. Inspired by human cognitive processes, we propose Parenting, a novel framework that decouples adherence and robustness within the parameter space of LLMs. Specifically, Parenting utilizes a key parameter mining method based on forward activation gain to identify and isolate the crucial parameter units that are strongly linked to adherence and robustness. Then, Parenting employs a type-guided tailored tuning strategy, applying specific and appropriate fine-tuning methods to parameter units representing different capabilities, aiming to achieve a balanced enhancement of adherence and robustness. Extensive experiments on various datasets and models validate the effectiveness and generalizability of our methods.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 通过整合外部检索知识，为大型语言模型 (LLM) 在幻觉生成和知识过时方面面临的问题提供了有效的解决方案。然而，由于内部和外部知识之间的潜在冲突以及检索噪声，LLM 往往难以有效地整合外部证据，导致性能下降。尽管现有方法试图解决这些挑战，但它们往往难以在模型依从性和稳健性之间取得平衡，导致学习方差很大。受人类认知过程的启发，我们提出了 Parenting，这是一个在 LLM 参数空间内解耦依从性和稳健性的新颖框架。具体而言，Parenting 利用基于前向激活增益的关键参数挖掘方法来识别和隔离与依从性和稳健性密切相关的关键参数单元。然后，Parenting 采用类型引导的定制调整策略，对代表不同能力的参数单元应用特定且适当的微调方法，旨在实现依从性和稳健性的平衡增强。在各种数据集和模型上进行的大量实验验证了我们方法的有效性和通用性。</li>
</ul>

<h3>Title: Medico: Towards Hallucination Detection and Correction with Multi-source Evidence Fusion</h3>
<ul>
<li><strong>Authors: </strong>Xinping Zhao, Jindi Yu, Zhenyu Liu, Jifang Wang, Dongfang Li, Yibin Chen, Baotian Hu, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10408">https://arxiv.org/abs/2410.10408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10408">https://arxiv.org/pdf/2410.10408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10408]] Medico: Towards Hallucination Detection and Correction with Multi-source Evidence Fusion(https://arxiv.org/abs/2410.10408)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>As we all know, hallucinations prevail in Large Language Models (LLMs), where the generated content is coherent but factually incorrect, which inflicts a heavy blow on the widespread application of LLMs. Previous studies have shown that LLMs could confidently state non-existent facts rather than answering ``I don't know''. Therefore, it is necessary to resort to external knowledge to detect and correct the hallucinated content. Since manual detection and correction of factual errors is labor-intensive, developing an automatic end-to-end hallucination-checking approach is indeed a needful thing. To this end, we present Medico, a Multi-source evidence fusion enhanced hallucination detection and correction framework. It fuses diverse evidence from multiple sources, detects whether the generated content contains factual errors, provides the rationale behind the judgment, and iteratively revises the hallucinated content. Experimental results on evidence retrieval (0.964 HR@5, 0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination correction (0.973-0.979 approval rate) manifest the great potential of Medico. A video demo of Medico can be found at this https URL.</li>
<li><strong>摘要：</strong>众所周知，大型语言模型（LLM）中存在大量幻觉，生成的内容虽然连贯，但事实上却不正确，这严重打击了LLM的广泛应用。先前的研究表明，LLM可以自信地陈述不存在的事实，而不是回答“我不知道”。因此，需要借助外部知识来检测和纠正幻觉内容。由于人工检测和纠正事实错误是一项劳动密集型的工作，因此开发一种自动化的端到端幻觉检查方法确实是一件必要的事情。为此，我们提出了一种多源证据融合增强的幻觉检测和纠正框架Medico。它融合了来自多个来源的多样化证据，检测生成的内容是否包含事实错误，提供判断背后的理由，并迭代地修改幻觉内容。证据检索（0.964 HR@5、0.908 MRR@5）、幻觉检测（0.927-0.951 F1）和幻觉校正（0.973-0.979 批准率）的实验结果体现了 Medico 的巨大潜力。Medico 的视频演示可在此 https URL 中找到。</li>
</ul>

<h3>Title: QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Timo Pierre Schrader, Lukas Lange, Simon Razniewski, Annemarie Friedrich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10449">https://arxiv.org/abs/2410.10449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10449">https://arxiv.org/pdf/2410.10449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10449]] QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios(https://arxiv.org/abs/2410.10449)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Reasoning is key to many decision making processes. It requires consolidating a set of rule-like premises that are often associated with degrees of uncertainty and observations to draw conclusions. In this work, we address both the case where premises are specified as numeric probabilistic rules and situations in which humans state their estimates using words expressing degrees of certainty. Existing probabilistic reasoning datasets simplify the task, e.g., by requiring the model to only rank textual alternatives, by including only binary random variables, or by making use of a limited set of templates that result in less varied text. In this work, we present QUITE, a question answering dataset of real-world Bayesian reasoning scenarios with categorical random variables and complex relationships. QUITE provides high-quality natural language verbalizations of premises together with evidence statements and expects the answer to a question in the form of an estimated probability. We conduct an extensive set of experiments, finding that logic-based models outperform out-of-the-box large language models on all reasoning types (causal, evidential, and explaining-away). Our results provide evidence that neuro-symbolic models are a promising direction for improving complex reasoning. We release QUITE and code for training and experiments on Github.</li>
<li><strong>摘要：</strong>推理是许多决策过程的关键。它需要整合一组规则式前提，这些前提通常与不确定性程度和观察结果相关联，以便得出结论。在这项工作中，我们既解决了前提被指定为数字概率规则的情况，也解决了人类使用表达确定性程度的词语陈述其估计的情况。现有的概率推理数据集简化了任务，例如，通过要求模型仅对文本替代方案进行排序，仅包括二进制随机变量，或使用一组有限的模板来减少文本的变化。在这项工作中，我们提出了 QUITE，这是一个包含真实世界贝叶斯推理场景的问答数据集，具有分类随机变量和复杂关系。QUITE 提供前提的高质量自然语言表达以及证据陈述，并期望以估计概率的形式回答问题。我们进行了一系列广泛的实验，发现基于逻辑的模型在所有推理类型（因果推理、证据推理和解释推理）上的表现都优于现成的大型语言模型。我们的研究结果证明，神经符号模型是改进复杂推理的一个有希望的方向。我们在 Github 上发布了 QUITE 以及用于训练和实验的代码。</li>
</ul>

<h3>Title: Ada-K Routing: Boosting the Efficiency of MoE-based LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tongtian Yue, Longteng Guo, Jie Cheng, Xuange Gao, Jing Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10456">https://arxiv.org/abs/2410.10456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10456">https://arxiv.org/pdf/2410.10456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10456]] Ada-K Routing: Boosting the Efficiency of MoE-based LLMs(https://arxiv.org/abs/2410.10456)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE) architectures offer a promising approach to managing computational costs while scaling up model parameters. Conventional MoE-based LLMs typically employ static Top-K routing, which activates a fixed and equal number of experts for each token regardless of their significance within the context. In this paper, we propose a novel Ada-K routing strategy that dynamically adjusts the number of activated experts for each token, thereby improving the balance between computational efficiency and model performance. Specifically, our strategy incorporates learnable and lightweight allocator modules that decide customized expert resource allocation tailored to the contextual needs for each token. These allocators are designed to be fully pluggable, making it broadly applicable across all mainstream MoE-based LLMs. We leverage the Proximal Policy Optimization (PPO) algorithm to facilitate an end-to-end learning process for this non-differentiable decision-making framework. Extensive evaluations on four popular baseline models demonstrate that our Ada-K routing method significantly outperforms conventional Top-K routing. Compared to Top-K, our method achieves over 25% reduction in FLOPs and more than 20% inference speedup while still improving performance across various benchmarks. Moreover, the training of Ada-K is highly efficient. Even for Mixtral-8x22B, a MoE-based LLM with more than 140B parameters, the training time is limited to 8 hours. Detailed analysis shows that harder tasks, middle layers, and content words tend to activate more experts, providing valuable insights for future adaptive MoE system designs. Both the training code and model checkpoints will be publicly available.</li>
<li><strong>摘要：</strong>在大型语言模型 (LLM) 时代，混合专家 (MoE) 架构提供了一种有前途的方法，可以在扩大模型参数的同时管理计算成本。传统的基于 MoE 的 LLM 通常采用静态 Top-K 路由，这会为每个 token 激活固定且相等数量的专家，而不管它们在上下文中的重要性如何。在本文中，我们提出了一种新颖的 Ada-K 路由策略，该策略可以动态调整每个 token 的激活专家数量，从而改善计算效率和模型性能之间的平衡。具体而言，我们的策略结合了可学习和轻量级的分配器模块，这些模块可以根据每个 token 的上下文需求决定定制的专家资源分配。这些分配器设计为完全可插入，使其广泛适用于所有主流的基于 MoE 的 LLM。我们利用近端策略优化 (PPO) 算法来促进这种不可微决策框架的端到端学习过程。对四种流行基线模型的广泛评估表明，我们的 Ada-K 路由方法明显优于传统的 Top-K 路由。与 Top-K 相比，我们的方法实现了超过 25% 的 FLOP 减少和超过 20% 的推理加速，同时仍在各种基准上提高性能。此外，Ada-K 的训练效率很高。即使是 Mixtral-8x22B（基于 MoE 的 LLM，具有超过 140B 个参数），训练时间也限制在 8 小时内。详细分析表明，更难的任务、中间层和内容词往往会激活更多专家，为未来的自适应 MoE 系统设计提供了宝贵的见解。训练代码和模型检查点都将公开提供。</li>
</ul>

<h3>Title: Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Roccabruna, Massimo Rizzoli, Giuseppe Riccardi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10476">https://arxiv.org/abs/2410.10476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10476">https://arxiv.org/pdf/2410.10476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10476]] Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?(https://arxiv.org/abs/2410.10476)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The automatic detection of temporal relations among events has been mainly investigated with encoder-only models such as RoBERTa. Large Language Models (LLM) have recently shown promising performance in temporal reasoning tasks such as temporal question answering. Nevertheless, recent studies have tested the LLMs' performance in detecting temporal relations of closed-source models only, limiting the interpretability of those results. In this work, we investigate LLMs' performance and decision process in the Temporal Relation Classification task. First, we assess the performance of seven open and closed-sourced LLMs experimenting with in-context learning and lightweight fine-tuning approaches. Results show that LLMs with in-context learning significantly underperform smaller encoder-only models based on RoBERTa. Then, we delve into the possible reasons for this gap by applying explainable methods. The outcome suggests a limitation of LLMs in this task due to their autoregressive nature, which causes them to focus only on the last part of the sequence. Additionally, we evaluate the word embeddings of these two models to better understand their pre-training differences. The code and the fine-tuned models can be found respectively on GitHub.</li>
<li><strong>摘要：</strong>事件间时间关系的自动检测主要通过仅编码器模型（例如 RoBERTa）进行研究。大型语言模型 (LLM) 最近在时间推理任务（例如时间问答）中表现出色。然而，最近的研究仅测试了 LLM 在检测闭源模型的时间关系方面的表现，限制了这些结果的可解释性。在这项工作中，我们研究了 LLM 在时间关系分类任务中的性能和决策过程。首先，我们评估了七个开源和闭源 LLM 的性能，这些 LLM 尝试了上下文学习和轻量级微调方法。结果表明，具有上下文学习的 LLM 表现明显不如基于 RoBERTa 的小型仅编码器模型。然后，我们通过应用可解释的方法深入研究造成这种差距的可能原因。结果表明，由于 LLM 的自回归性质，它们在此任务中的局限性导致它们只关注序列的最后一部分。此外，我们评估了这两个模型的词嵌入，以更好地理解它们的训练前差异。代码和微调模型分别可以在 GitHub 上找到。</li>
</ul>

<h3>Title: Cultural Fidelity in Large-Language Models: An Evaluation of Online Language Resources as a Driver of Model Performance in Value Representation</h3>
<ul>
<li><strong>Authors: </strong>Sharif Kazemi, Gloria Gerhardt, Jonty Katz, Caroline Ida Kuria, Estelle Pan, Umang Prabhakar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10489">https://arxiv.org/abs/2410.10489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10489">https://arxiv.org/pdf/2410.10489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10489]] Cultural Fidelity in Large-Language Models: An Evaluation of Online Language Resources as a Driver of Model Performance in Value Representation(https://arxiv.org/abs/2410.10489)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The training data for LLMs embeds societal values, increasing their familiarity with the language's culture. Our analysis found that 44% of the variance in the ability of GPT-4o to reflect the societal values of a country, as measured by the World Values Survey, correlates with the availability of digital resources in that language. Notably, the error rate was more than five times higher for the languages of the lowest resource compared to the languages of the highest resource. For GPT-4-turbo, this correlation rose to 72%, suggesting efforts to improve the familiarity with the non-English language beyond the web-scraped data. Our study developed one of the largest and most robust datasets in this topic area with 21 country-language pairs, each of which contain 94 survey questions verified by native speakers. Our results highlight the link between LLM performance and digital data availability in target languages. Weaker performance in low-resource languages, especially prominent in the Global South, may worsen digital divides. We discuss strategies proposed to address this, including developing multilingual LLMs from the ground up and enhancing fine-tuning on diverse linguistic datasets, as seen in African language initiatives.</li>
<li><strong>摘要：</strong>LLM 的训练数据嵌入了社会价值观，增加了他们对语言文化的熟悉度。我们的分析发现，GPT-4o 反映一个国家社会价值观的能力的方差的 44%（以世界价值观调查为衡量标准）与该语言的数字资源可用性相关。值得注意的是，与资源最多的语言相比，资源最少的语言的错误率高出五倍多。对于 GPT-4-turbo，这种相关性上升到 72%，这表明除了网络抓取的数据之外，还需要努力提高对非英语语言的熟悉度。我们的研究开发了该主题领域中最大、最强大的数据集之一，其中包含 21 个国家-语言对，每个对包含 94 个由母语人士验证的调查问题。我们的结果强调了 LLM 表现与目标语言的数字数据可用性之间的联系。资源匮乏的语言表现较差，尤其是在全球南方国家，可能会加剧数字鸿沟。我们讨论了解决这一问题所提出的策略，包括从头开始开发多语言法学硕士 (LLM) 并加强对多种语言数据集的微调，就像非洲语言计划中所见的那样。</li>
</ul>

<h3>Title: Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shubham Kumar Nigam, Aniket Deroy, Subhankar Maity, Arnab Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10542">https://arxiv.org/abs/2410.10542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10542">https://arxiv.org/pdf/2410.10542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10542]] Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era of Large Language Models(https://arxiv.org/abs/2410.10542)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>This study investigates judgment prediction in a realistic scenario within the context of Indian judgments, utilizing a range of transformer-based models, including InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and GPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are predicted at the point when a case is presented for a decision in court, using only the information available at that time, such as the facts of the case, statutes, precedents, and arguments. This approach mimics real-world conditions, where decisions must be made without the benefit of hindsight, unlike retrospective analyses often found in previous studies. For transformer models, we experiment with hierarchical transformers and the summarization of judgment facts to optimize input for these models. Our experiments with LLMs reveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust performance in judgment prediction. Furthermore, incorporating additional legal information, such as statutes and precedents, significantly improves the outcome of the prediction task. The LLMs also provide explanations for their predictions. To evaluate the quality of these predictions and explanations, we introduce two human evaluation metrics: Clarity and Linking. Our findings from both automatic and human evaluations indicate that, despite advancements in LLMs, they are yet to achieve expert-level performance in judgment prediction and explanation tasks.</li>
<li><strong>摘要：</strong>本研究利用一系列基于 Transformer 的模型（包括 InLegalBERT、BERT 和 XLNet）以及 Llama-2 和 GPT-3.5 Turbo 等 LLM，在印度判决的背景下，研究现实场景中的判决预测。在这个现实场景中，我们仅使用当时可用的信息（例如案件事实、法规、判例和论点）来模拟在案件提交法院作出判决时如何预测判决。这种方法模拟了现实世界的情况，在这种情况下，必须在没有事后诸葛亮的情况下做出决定，这与以前的研究中经常发现的回顾性分析不同。对于 Transformer 模型，我们尝试使用分层 Transformer 和判决事实的总结来优化这些模型的输入。我们对 LLM 的实验表明，GPT-3.5 Turbo 在现实场景中表现出色，在判决预测方面表现出色。此外，结合其他法律信息（例如法规和判例），可以显著改善预测任务的结果。 LLM 还为其预测提供解释。为了评估这些预测和解释的质量，我们引入了两个人工评估指标：清晰度和链接性。我们从自动评估和人工评估中得出的结果表明，尽管 LLM 取得了进步，但它们在判断预测和解释任务中仍未达到专家级的表现。</li>
</ul>

<h3>Title: Recipe for Zero-shot POS Tagging: Is It Useful in Realistic Scenarios?</h3>
<ul>
<li><strong>Authors: </strong>Zeno Vandenbulcke, Lukas Vermeire, Miryam de Lhoneux</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10576">https://arxiv.org/abs/2410.10576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10576">https://arxiv.org/pdf/2410.10576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10576]] Recipe for Zero-shot POS Tagging: Is It Useful in Realistic Scenarios?(https://arxiv.org/abs/2410.10576)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>POS tagging plays a fundamental role in numerous applications. While POS taggers are highly accurate in well-resourced settings, they lag behind in cases of limited or missing training data. This paper focuses on POS tagging for languages with limited data. We seek to identify the characteristics of datasets that make them favourable for training POS tagging models without using any labelled training data from the target language. This is a zero-shot approach. We compare the accuracies of a multilingual large language model (mBERT) fine-tuned on one or more languages related to the target language. Additionally, we compare these results with models trained directly on the target language itself. We do this for three target low-resource languages. Our research highlights the importance of accurate dataset selection for effective zero-shot POS tagging. Particularly, a strong linguistic relationship and high-quality datasets ensure optimal results. For extremely low-resource languages, zero-shot models prove to be a viable option.</li>
<li><strong>摘要：</strong>POS 标记在许多应用中起着基础性作用。虽然 POS 标记器在资源充足的环境中具有高度准确性，但在训练数据有限或缺失的情况下，它们会落后。本文重点介绍数据有限的语言的 POS 标记。我们试图确定数据集的特征，使其有利于训练 POS 标记模型，而无需使用任何来自目标语言的标记训练数据。这是一种零样本方法。我们比较了在一种或多种与目标语言相关的语言上微调的多语言大型语言模型 (mBERT) 的准确性。此外，我们将这些结果与直接在目标语言本身上训练的模型进行比较。我们对三种目标低资源语言执行此操作。我们的研究强调了准确的数据集选择对于有效的零样本 POS 标记的重要性。特别是，强大的语言关系和高质量的数据集可确保获得最佳结果。对于资源极其匮乏的语言，零样本模型被证明是一种可行的选择。</li>
</ul>

<h3>Title: SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, Flora D. Salim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10624">https://arxiv.org/abs/2410.10624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10624">https://arxiv.org/pdf/2410.10624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10624]] SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition(https://arxiv.org/abs/2410.10624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In this work, we bridge the gap between wearable sensor technology and personalized AI assistants by enabling Large Language Models (LLMs) to understand time-series tasks like human activity recognition (HAR). Despite the strong reasoning and generalization capabilities of LLMs, leveraging them for sensor data tasks remains largely unexplored. This gap stems from challenges like the lack of semantic context in time-series data, computational limitations, and LLMs' difficulty processing numerical inputs. To address these issues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential for sensor data tasks. In the Sensor-Language Alignment Stage, we introduce special tokens for each sensor channel and automatically generate trend-descriptive text to align sensor data with textual inputs, enabling SensorLLM to capture numerical changes, channel-specific information, and sensor data of varying lengths-capabilities that existing LLMs typically struggle with, all without the need for human annotations. Next, in Task-Aware Tuning Stage, we refine the model for HAR classification using the frozen LLM and alignment module, achieving performance on par with or surpassing state-of-the-art models. We further demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through Sensor-Language Alignment, enabling it to generalize across diverse datasets for HAR tasks. We strongly believe our work lays the stepstone for future time-series and text alignment research, offering a path toward foundation models for sensor data.</li>
<li><strong>摘要：</strong>在这项工作中，我们通过使大型语言模型 (LLM) 能够理解人类活动识别 (HAR) 等时间序列任务，弥合了可穿戴传感器技术与个性化 AI 助手之间的差距。尽管 LLM 具有强大的推理和泛化能力，但将其用于传感器数据任务仍在很大程度上尚未得到探索。这一差距源于诸如时间序列数据中缺乏语义上下文、计算限制以及 LLM 难以处理数字输入等挑战。为了解决这些问题，我们引入了 SensorLLM，这是一个两阶段框架，用于释放 LLM 在传感器数据任务中的潜力。在传感器语言对齐阶段，我们为每个传感器通道引入特殊标记，并自动生成趋势描述文本以将传感器数据与文本输入对齐，使 SensorLLM 能够捕获数值变化、特定于通道的信息和不同长度的传感器数据 - 现有 LLM 通常难以实现的功能，所有这些都无需人工注释。接下来，在任务感知调整阶段，我们使用冻结的 LLM 和对齐模块改进 HAR 分类模型，实现与最先进模型相当甚至超越其的性能。我们进一步证明，SensorLLM 通过传感器语言对齐发展成为有效的传感器学习器、推理器和分类器，使其能够跨 HAR 任务的不同数据集进行推广。我们坚信，我们的工作为未来的时间序列和文本对齐研究奠定了基础，为传感器数据的基础模型提供了一条道路。</li>
</ul>

<h3>Title: Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts</h3>
<ul>
<li><strong>Authors: </strong>Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10626">https://arxiv.org/abs/2410.10626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10626">https://arxiv.org/pdf/2410.10626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10626]] Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts(https://arxiv.org/abs/2410.10626)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.</li>
<li><strong>摘要：</strong>将医学大型语言模型适应当地语言可以减少获取医疗服务的障碍，但数据稀缺仍然是一项重大挑战，尤其是对于资源匮乏的语言而言。为了解决这个问题，我们首先构建一个高质量的医学数据集并进行分析以确保其质量。为了利用多语言 LLM 的泛化能力有效地扩展到资源受限的语言，我们使用混合专家 (MoE) 模块化从多语言角度探索 LLM 的内部信息流。从技术上讲，我们提出了一种新颖的 MoE 路由方法，该方法采用特定语言的专家和跨语言路由。受电路理论的启发，我们的路由分析揭示了一种在末端扩展的信息流机制：虽然较早的层集中了跨语言信息流，但较晚的层表现出特定于语言的分歧。这一见解直接导致了 Post-MoE 架构的开发，该架构仅在较晚的层中应用稀疏路由，同时保持其他层密集。实验结果表明，这种方法增强了多语言模型对其他语言的泛化能力，同时保持了可解释性。最后，为了有效地将模型扩展到 50 种语言，我们引入了语言家族专家的概念，借鉴语言先验，从而无需添加额外参数即可扩展语言数量。</li>
</ul>

<h3>Title: Thinking LLMs: General Instruction Following with Thought Generation</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10630">https://arxiv.org/abs/2410.10630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10630">https://arxiv.org/pdf/2410.10630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10630]] Thinking LLMs: General Instruction Following with Thought Generation(https://arxiv.org/abs/2410.10630)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks.</li>
<li><strong>摘要：</strong>LLM 通常经过训练，可以回答用户问题或遵循与人类专家类似的指令。然而，在标准对齐框架中，它们缺乏回答前明确思考的基本能力。思考对于需要推理和规划的复杂问题很重要——但可以应用于任何任务。我们提出了一种训练方法，使现有的 LLM 具备这种思考能力，无需使用额外的人类数据即可遵循一般指令。我们通过迭代搜索和优化程序实现这一点，该程序探索可能的思维生成空间，使模型能够在没有直接监督的情况下学习如何思考。对于每条指令，使用判断模型对思维候选进行评分，仅评估他们的反应，然后通过偏好优化进行优化。我们表明，除了更传统的推理和解决问题任务外，此程序还可在 AlpacaEval 和 Arena-Hard 上带来卓越表现，并且从营销、健康和一般知识等非推理类别的思考中获益。</li>
</ul>

<h3>Title: Generative AI and Its Impact on Personalized Intelligent Tutoring Systems</h3>
<ul>
<li><strong>Authors: </strong>Subhankar Maity, Aniket Deroy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10650">https://arxiv.org/abs/2410.10650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10650">https://arxiv.org/pdf/2410.10650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10650]] Generative AI and Its Impact on Personalized Intelligent Tutoring Systems(https://arxiv.org/abs/2410.10650)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (AI) is revolutionizing educational technology by enabling highly personalized and adaptive learning environments within Intelligent Tutoring Systems (ITS). This report delves into the integration of Generative AI, particularly large language models (LLMs) like GPT-4, into ITS to enhance personalized education through dynamic content generation, real-time feedback, and adaptive learning pathways. We explore key applications such as automated question generation, customized feedback mechanisms, and interactive dialogue systems that respond to individual learner needs. The report also addresses significant challenges, including ensuring pedagogical accuracy, mitigating inherent biases in AI models, and maintaining learner engagement. Future directions highlight the potential advancements in multimodal AI integration, emotional intelligence in tutoring systems, and the ethical implications of AI-driven education. By synthesizing current research and practical implementations, this report underscores the transformative potential of Generative AI in creating more effective, equitable, and engaging educational experiences.</li>
<li><strong>摘要：</strong>生成式人工智能 (AI) 通过在智能辅导系统 (ITS) 中实现高度个性化和自适应的学习环境，正在彻底改变教育技术。本报告深入探讨了生成式人工智能（尤其是像 GPT-4 这样的大型语言模型 (LLM)）与 ITS 的集成，以通过动态内容生成、实时反馈和自适应学习路径增强个性化教育。我们探索了关键应用，例如自动问题生成、定制反馈机制和响应个人学习者需求的交互式对话系统。该报告还解决了重大挑战，包括确保教学准确性、减轻人工智能模型中固有的偏见以及保持学习者的参与度。未来的方向突出了多模式人工智能集成、辅导系统中的情商以及人工智能驱动教育的伦理影响方面的潜在进步。通过综合当前的研究和实际实施，本报告强调了生成式人工智能在创造更有效、更公平、更具吸引力的教育体验方面的变革潜力。</li>
</ul>

<h3>Title: Double Jeopardy and Climate Impact in the Use of Large Language Models: Socio-economic Disparities and Reduced Utility for Non-English Speakers</h3>
<ul>
<li><strong>Authors: </strong>Aivin V. Solatorio, Gabriel Stefanini Vicente, Holly Krambeck, Olivier Dupriez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10665">https://arxiv.org/abs/2410.10665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10665">https://arxiv.org/pdf/2410.10665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10665]] Double Jeopardy and Climate Impact in the Use of Large Language Models: Socio-economic Disparities and Reduced Utility for Non-English Speakers(https://arxiv.org/abs/2410.10665)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI), particularly large language models (LLMs), holds the potential to bridge language and information gaps, which can benefit the economies of developing nations. However, our analysis of FLORES-200, FLORES+, Ethnologue, and World Development Indicators data reveals that these benefits largely favor English speakers. Speakers of languages in low-income and lower-middle-income countries face higher costs when using OpenAI's GPT models via APIs because of how the system processes the input -- tokenization. Around 1.5 billion people, speaking languages primarily from lower-middle-income countries, could incur costs that are 4 to 6 times higher than those faced by English speakers. Disparities in LLM performance are significant, and tokenization in models priced per token amplifies inequalities in access, cost, and utility. Moreover, using the quality of translation tasks as a proxy measure, we show that LLMs perform poorly in low-resource languages, presenting a ``double jeopardy" of higher costs and poor performance for these users. We also discuss the direct impact of fragmentation in tokenizing low-resource languages on climate. This underscores the need for fairer algorithm development to benefit all linguistic groups.</li>
<li><strong>摘要：</strong>人工智能 (AI)，尤其是大型语言模型 (LLM)，具有弥合语言和信息差距的潜力，这可以使发展中国家的经济受益。然而，我们对 FLORES-200、FLORES+、Ethnologue 和世界发展指标数据的分析表明，这些好处主要有利于英语使用者。低收入和中低收入国家语言的使用者在通过 API 使用 OpenAI 的 GPT 模型时面临更高的成本，因为系统处理输入的方式——标记化。大约 15 亿人，主要使用中低收入国家的语言，他们面临的成本可能是英语使用者的 4 到 6 倍。LLM 性能方面的差异很大，按标记定价的模型中的标记化加剧了访问、成本和效用的不平等。此外，使用翻译任务的质量作为替代指标，我们发现 LLM 在资源匮乏的语言中表现不佳，给这些用户带来了成本更高和性能不佳的“双重危险”。我们还讨论了资源匮乏的语言标记化过程中的碎片化对气候的直接影响。这强调了更公平的算法开发对所有语言群体的利益的必要性。</li>
</ul>

<h3>Title: Large Language Model Evaluation via Matrix Nuclear-Norm</h3>
<ul>
<li><strong>Authors: </strong>Yahan Li, Tingyu Xia, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10672">https://arxiv.org/abs/2410.10672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10672">https://arxiv.org/pdf/2410.10672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10672]] Large Language Model Evaluation via Matrix Nuclear-Norm(https://arxiv.org/abs/2410.10672)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \( O(n^3) \) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \( L_{1,2}\text{-norm} \) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \( O(n^2) \) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at this https URL.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的不断发展，有效的评估指标对于评估其压缩信息和减少冗余的能力至关重要。虽然矩阵熵等传统指标提供了有价值的见解，但由于其使用奇异值分解 (SVD) 的时间复杂度为 \( O(n^3) \)，因此对于大型模型而言，它们的计算量非常大。为了缓解这个问题，我们引入了矩阵核范数，它不仅可以作为量化 LLM 数据压缩能力的指标，还可以提供矩阵秩的凸近似，以捕获预测可辨别性和多样性。通过使用 \( L_{1,2}\text{-norm} \) 进一步近似核范数，我们可以有效地评估模型的信息压缩能力。这种方法将时间复杂度降低到 \( O(n^2) \)，并且无需进行 SVD 计算。因此，随着规模从 111M 增加到 6.7B，矩阵核范数的速度比 CEREBRAS-GPT 模型的矩阵熵快 8 到 24 倍。这种性能差距在较大的模型中变得更加明显，这在对 Pythia 等其他模型的测试中得到了验证。此外，对基准和模型响应的评估证实，我们提出的矩阵核范数是一种可靠、可扩展且高效的工具，可用于评估 LLM 的性能，在准确性和计算效率之间取得平衡。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues</h3>
<ul>
<li><strong>Authors: </strong>Qibing Ren, Hao Li, Dongrui Liu, Zhanxu Xie, Xiaoya Lu, Yu Qiao, Lei Sha, Junchi Yan, Lizhuang Ma, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10700">https://arxiv.org/abs/2410.10700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10700">https://arxiv.org/pdf/2410.10700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10700]] Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues(https://arxiv.org/abs/2410.10700)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired by actor-network theory, which models a network of semantically linked actors as attack clues to generate diverse and effective attack paths toward harmful targets. ActorAttack addresses two main challenges in multi-turn attacks: (1) concealing harmful intents by creating an innocuous conversation topic about the actor, and (2) uncovering diverse attack paths towards the same harmful target by leveraging LLMs' knowledge to specify the correlated actors as various attack clues. In this way, ActorAttack outperforms existing single-turn and multi-turn attack methods across advanced aligned LLMs, even for GPT-o1. We will publish a dataset called SafeMTData, which includes multi-turn adversarial prompts and safety alignment data, generated by ActorAttack. We demonstrate that models safety-tuned using our safety dataset are more robust to multi-turn attacks. Code is available at this https URL.</li>
<li><strong>摘要：</strong>本研究揭示了大型语言模型 (LLM) 在多轮交互中的安全漏洞，恶意用户可以在多个查询中掩盖有害意图。我们引入了 ActorAttack，这是一种受行动者网络理论启发的新型多轮攻击方法，它将语义上关联的行动者网络建模为攻击线索，以生成针对有害目标的多样化有效攻击路径。ActorAttack 解决了多轮攻击中的两个主要挑战：(1) 通过创建关于行动者的无害对话主题来隐藏有害意图，(2) 通过利用 LLM 的知识将相关行动者指定为各种攻击线索，揭示针对同一有害目标的多种攻击路径。通过这种方式，ActorAttack 优于现有的高级对齐 LLM 中的单轮和多轮攻击方法，甚至对于 GPT-o1 也是如此。我们将发布一个名为 SafeMTData 的数据集，其中包括由 ActorAttack 生成的多轮对抗提示和安全对齐数据。我们证明使用我们的安全数据集进行安全调整的模型对多轮攻击更具鲁棒性。代码可在此 https URL 上获取。</li>
</ul>

<h3>Title: Large Language Models Are Active Critics in NLG Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Shuying Xu, Junjie Hu, Ming Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10724">https://arxiv.org/abs/2410.10724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10724">https://arxiv.org/pdf/2410.10724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10724]] Large Language Models Are Active Critics in NLG Evaluation(https://arxiv.org/abs/2410.10724)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The conventional paradigm of using large language models (LLMs) for evaluating natural language generation (NLG) systems typically relies on two key inputs: (1) a clear definition of the NLG task to be evaluated and (2) a list of pre-defined evaluation criteria. This process treats LLMs as ''passive critics,'' strictly following human-defined criteria for evaluation. However, as new NLG tasks emerge, the criteria for assessing text quality can vary greatly. Consequently, these rigid evaluation methods struggle to adapt to diverse NLG tasks without extensive prompt engineering customized for each specific task. To address this limitation, we introduce Active-Critic, a novel LLM-based NLG evaluation protocol that enables LLMs to function as ''active critics.'' Specifically, our protocol comprises two key stages. In the first stage, the LLM is instructed to infer the target NLG task and establish relevant evaluation criteria from the data. Building on this self-inferred information, the second stage dynamically optimizes the prompt to guide the LLM toward more human-aligned scoring decisions, while also generating detailed explanations to justify its evaluations. Experiments across four NLG evaluation tasks show that our approach achieves stronger alignment with human judgments than state-of-the-art evaluation methods. Our comprehensive analysis further highlights the effectiveness and explainability of Active-Critic with only a small amount of labeled data. We will share our code and data on GitHub.</li>
<li><strong>摘要：</strong>使用大型语言模型 (LLM) 评估自然语言生成 (NLG) 系统的传统范例通常依赖于两个关键输入：(1) 对要评估的 NLG 任务的明确定义和 (2) 预定义的评估标准列表。此过程将 LLM 视为“被动批评者”，严格遵循人类定义的评估标准。然而，随着新 NLG 任务的出现，评估文本质量的标准可能会有很大差异。因此，这些僵化的评估方法难以适应不同的 NLG 任务，除非针对每个特定任务进行广泛的快速工程定制。为了解决这个限制，我们引入了 Active-Critic，这是一种基于 LLM 的新型 NLG 评估协议，使 LLM 能够充当“主动批评者”。具体来说，我们的协议包括两个关键阶段。在第一阶段，指示 LLM 推断目标 NLG 任务并从数据中建立相关的评估标准。基于这些自我推断的信息，第二阶段动态优化提示，以引导 LLM 做出更符合人类的评分决策，同时生成详细的解释来证明其评估的合理性。在四个 NLG 评估任务中进行的实验表明，与最先进的评估方法相比，我们的方法与人类判断的一致性更强。我们的全面分析进一步凸显了 Active-Critic 仅使用少量标记数据的有效性和可解释性。我们将在 GitHub 上分享我们的代码和数据。</li>
</ul>

<h3>Title: Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Ishan Jindal, Chandana Badrinath, Pranjal Bharti, Lakkidi Vinay, Sachin Dev Sharma</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10739">https://arxiv.org/abs/2410.10739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10739">https://arxiv.org/pdf/2410.10739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10739]] Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs(https://arxiv.org/abs/2410.10739)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) for public use require continuous pre-training to remain up-to-date with the latest data. The models also need to be fine-tuned with specific instructions to maintain their ability to follow instructions accurately. Typically, LLMs are released in two versions: the Base LLM, pre-trained on diverse data, and the instruction-refined LLM, additionally trained with specific instructions for better instruction following. The question arises as to which model should undergo continuous pre-training to maintain its instruction-following abilities while also staying current with the latest data. In this study, we delve into the intricate relationship between continuous pre-training and instruction fine-tuning of the LLMs and investigate the impact of continuous pre-training on the instruction following abilities of both the base and its instruction finetuned model. Further, the instruction fine-tuning process is computationally intense and requires a substantial number of hand-annotated examples for the model to learn effectively. This study aims to find the most compute-efficient strategy to gain up-to-date knowledge and instruction-following capabilities without requiring any instruction data and fine-tuning. We empirically prove our findings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction models, providing a comprehensive exploration of our hypotheses across varying sizes of pre-training data corpus and different LLMs settings.</li>
<li><strong>摘要：</strong>供公众使用的大型语言模型 (LLM) 需要持续的预训练才能与最新数据保持同步。模型还需要使用特定指令进行微调，以保持其准确遵循指令的能力。通常，LLM 有两个版本：基本 LLM，已在不同数据上进行预训练；指令精炼 LLM，还使用特定指令进行额外训练，以便更好地遵循指令。问题是，哪种模型应该进行持续的预训练，以保持其遵循指令的能力，同时与最新数据保持同步。在本研究中，我们深入研究了 LLM 的持续预训练和指令微调之间的复杂关系，并研究了持续预训练对基本模型和指令微调模型的指令遵循能力的影响。此外，指令微调过程计算量很大，需要大量手工注释的示例才能使模型有效学习。本研究旨在找到最高效的计算策略，以获得最新的知识和指令跟踪能力，而无需任何指令数据和微调。我们通过 LLaMa 3、3.1 和 Qwen 2、2.5 系列基础和指令模型来实证我们的发现，全面探索了我们在不同规模的预训练数据语料库和不同 LLM 设置中的假设。</li>
</ul>

<h3>Title: Use Random Selection for Now: Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation for Classification</h3>
<ul>
<li><strong>Authors: </strong>Jan Cegin, Branislav Pecher, Jakub Simko, Ivan Srba, Maria Bielikova, Peter Brusilovsky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10756">https://arxiv.org/abs/2410.10756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10756">https://arxiv.org/pdf/2410.10756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10756]] Use Random Selection for Now: Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation for Classification(https://arxiv.org/abs/2410.10756)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The generative large language models (LLMs) are increasingly used for data augmentation tasks, where text samples are paraphrased (or generated anew) and then used for classifier fine-tuning. Existing works on augmentation leverage the few-shot scenarios, where samples are given to LLMs as part of prompts, leading to better augmentations. Yet, the samples are mostly selected randomly and a comprehensive overview of the effects of other (more ``informed'') sample selection strategies is lacking. In this work, we compare sample selection strategies existing in few-shot learning literature and investigate their effects in LLM-based textual augmentation. We evaluate this on in-distribution and out-of-distribution classifier performance. Results indicate, that while some ``informed'' selection strategies increase the performance of models, especially for out-of-distribution data, it happens only seldom and with marginal performance increases. Unless further advances are made, a default of random sample selection remains a good option for augmentation practitioners.</li>
<li><strong>摘要：</strong>生成式大型语言模型 (LLM) 越来越多地用于数据增强任务，其中文本样本被解释（或重新生成），然后用于分类器微调。现有的增强工作利用了少样本场景，其中样本作为提示的一部分提供给 LLM，从而实现更好的增强。然而，样本大多是随机选择的，缺乏对其他（更“明智”）样本选择策略效果的全面概述。在这项工作中，我们比较了少样本学习文献中现有的样本选择策略，并研究了它们在基于 LLM 的文本增强中的效果。我们根据分布内和分布外的分类器性能对此进行了评估。结果表明，虽然一些“明智”的选择策略可以提高模型的性能，尤其是对于分布外的数据，但这种情况很少发生，而且性能提升幅度很小。除非取得进一步的进展，否则默认的随机样本选择仍然是增强从业者的一个好选择。</li>
</ul>

<h3>Title: When Attention Sink Emerges in Language Models: An Empirical View</h3>
<ul>
<li><strong>Authors: </strong>Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10781">https://arxiv.org/abs/2410.10781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10781">https://arxiv.org/pdf/2410.10781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10781]] When Attention Sink Emerges in Language Models: An Empirical View(https://arxiv.org/abs/2410.10781)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, long context</a></li>
<li><strong>Abstract: </strong>Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how optimization, data distribution, loss function, and model architecture in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, storing extra attention scores, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters. The code is available at this https URL.</li>
<li><strong>摘要：</strong>语言模型 (LM) 会将重要的注意力分配给第一个 token，即使它在语义上并不重要，这被称为注意力汇。这种现象已广泛应用于流式/长上下文生成、KV 缓存优化、推理加速、模型量化等应用中。尽管它被广泛使用，但对 LM 中的注意力汇的深入了解仍然不足。在这项工作中，我们首先证明注意力汇普遍存在于具有各种输入的 LM 中，即使在小模型中也是如此。此外，在 LM 预训练期间观察到注意力汇的出现，这促使我们研究 LM 预训练中的优化、数据分布、损失函数和模型架构如何影响其出现。我们强调，在对足够的训练数据进行有效优化后，注意力汇就会出现。接收器位置与损失函数和数据分布高度相关。最重要的是，我们发现注意力汇更像是关键偏差，存储额外的注意力分数，这些分数可能没有信息量，也不会对价值计算做出贡献。我们还观察到，这种现象（至少部分）源于 token 对注意力分数的内部依赖，这是 softmax 归一化的结果。通过将 softmax 注意力替换为其他注意力操作（例如未归一化的 sigmoid 注意力）来放松这种依赖性后，在高达 1B 参数的 LM 中不会出现注意力沉降。代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning</h3>
<ul>
<li><strong>Authors: </strong>Aakanksha, Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, Marzieh Fadaee, Sara Hooker</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10801">https://arxiv.org/abs/2410.10801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10801">https://arxiv.org/pdf/2410.10801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10801]] Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning(https://arxiv.org/abs/2410.10801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been adopted and deployed worldwide for a broad variety of applications. However, ensuring their safe use remains a significant challenge. Preference training and safety measures often overfit to harms prevalent in Western-centric datasets, and safety protocols frequently fail to extend to multilingual settings. In this work, we explore model merging in a diverse multi-task setting, combining safety and general-purpose tasks within a multilingual context. Each language introduces unique and varied learning challenges across tasks. We find that objective-based merging is more effective than mixing data, with improvements of up to 8% and 10% in general performance and safety respectively. We also find that language-based merging is highly effective -- by merging monolingually fine-tuned models, we achieve a 4% increase in general performance and 7% reduction in harm across all languages on top of the data mixtures method using the same available data. Overall, our comprehensive study of merging approaches provides a useful framework for building strong and safe multilingual models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已在全球范围内被广泛采用和部署，用于各种应用。然而，确保它们的安全使用仍然是一项重大挑战。偏好训练和安全措施往往过度拟合以西方为中心的数据集中普遍存在的危害，而安全协议往往无法扩展到多语言环境。在这项工作中，我们探索了在多样化多任务环境中的模型合并，在多语言环境中结合安全性和通用任务。每种语言在任务中都会带来独特而多样的学习挑战。我们发现基于目标的合并比混合数据更有效，总体性能和安全性分别提高了 8% 和 10%。我们还发现基于语言的合并非常有效——通过合并单语微调模型，我们在使用相同可用数据的数据混合方法的基础上实现了 4% 的总体性能提高和 7% 的危害减少。总的来说，我们对合并方法的全面研究为构建强大而安全的多语言模型提供了一个有用的框架。</li>
</ul>

<h3>Title: Local and Global Decoding in Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Daniel Gareev, Thomas Hofmann, Ezhilmathi Krishnasamy, Tiago Pimentel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10810">https://arxiv.org/abs/2410.10810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10810">https://arxiv.org/pdf/2410.10810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10810]] Local and Global Decoding in Text Generation(https://arxiv.org/abs/2410.10810)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Text generation, a key component in applications such as dialogue systems, relies on decoding algorithms that sample strings from a language model distribution. Traditional methods, such as top-$k$ and top-$\pi$, apply local normalisation to the model's output distribution, which can distort it. In this paper, we investigate the effect of this distortion by introducing globally-normalised versions of these decoding methods. Additionally, we propose an independent Metropolis-Hastings algorithm to approximate sampling from globally-normalised distributions without explicitly computing them. Our empirical analysis compares the performance of local and global normalisation across two decoding algorithms (top-$k$ and top-$\pi$) with various hyperparameters, using Pythia language models. Results show that, in most configurations, global decoding performs worse than the local decoding version of the same algorithms -- despite preserving the distribution's integrity. Our results suggest that distortion is an important feature of local decoding algorithms.</li>
<li><strong>摘要：</strong>文本生成是诸如对话系统等应用中的一个关键组件，它依赖于从语言模型分布中采样字符串的解码算法。传统方法（例如 top-$k$ 和 top-$\pi$）将局部归一化应用于模型的输出分布，这可能会扭曲模型。在本文中，我们通过引入这些解码方法的全局归一化版本来研究这种扭曲的影响。此外，我们提出了一种独立的 Metropolis-Hastings 算法来近似从全局归一化分布中采样，而无需明确计算它们。我们的实证分析使用 Pythia 语言模型，比较了具有各种超参数的两种解码算法（top-$k$ 和 top-$\pi$）的局部和全局归一化性能。结果表明，在大多数配置中，尽管保留了分布的完整性，但全局解码的性能仍差于相同算法的局部解码版本。我们的结果表明，失真是局部解码算法的一个重要特征。</li>
</ul>

<h3>Title: LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory</h3>
<ul>
<li><strong>Authors: </strong>Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10813">https://arxiv.org/abs/2410.10813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10813">https://arxiv.org/pdf/2410.10813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10813]] LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory(https://arxiv.org/abs/2410.10813)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI.</li>
<li><strong>摘要：</strong>最近，大型语言模型 (LLM) 驱动的聊天助手系统集成了记忆组件来跟踪用户助手聊天历史，从而实现更准确和个性化的响应。然而，它们在持续交互中的长期记忆能力仍未得到充分探索。本文介绍了 LongMemEval，这是一个全面的基准，旨在评估聊天助手的五项核心长期记忆能力：信息提取、多会话推理、时间推理、知识更新和弃权。LongMemEval 包含 500 个精心策划的问题，这些问题嵌入在可自由扩展的用户助手聊天历史中，对现有的长期记忆系统提出了重大挑战，商业聊天助手和长上下文 LLM 在持续交互中记忆信息的准确率下降了 30%。然后，我们提出了一个统一的框架，将长期记忆设计分解为索引、检索和阅读阶段的四个设计选择。基于关键的实验见解，我们提出了几种记忆设计，包括会话分解以优化值粒度、事实增强键扩展以增强索引结构以及时间感知查询扩展以细化搜索范围。实验结果表明，这些优化极大地改善了 LongMemEval 上的记忆回忆和下游问答。总体而言，我们的研究为提高基于 LLM 的聊天助手的长期记忆能力提供了宝贵的资源和指导，为更加个性化和可靠的对话式 AI 铺平了道路。</li>
</ul>

<h3>Title: Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free</h3>
<ul>
<li><strong>Authors: </strong>Ziyue Li, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10814">https://arxiv.org/abs/2410.10814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10814">https://arxiv.org/pdf/2410.10814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10814]] Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free(https://arxiv.org/abs/2410.10814)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 在生成任务上表现出色，但如果不应用进一步的表示微调，它们的解码器专用架构通常会限制它们作为嵌入模型的潜力。这是否与它们声称的通才相矛盾？为了回答这个问题，我们仔细研究了混合专家 (MoE) LLM。我们的研究表明，MoE LLM 中的专家路由器可以作为现成的嵌入模型，在各种以嵌入为重点的任务上具有良好的性能，而无需任何微调。此外，我们的广泛分析表明，MoE 路由权重 (RW) 与广泛使用的嵌入 LLM 的隐藏状态 (HS) 相辅相成。与 HS 相比，我们发现 RW 对提示的选择更为稳健，并且专注于高级语义。受分析的启发，我们提出了结合 RW 和 HS 的 MoEE，这比单独使用其中任何一个都获得了更好的性能。我们对它们的组合和提示策略的探索揭示了一些新颖的见解，例如，RW 和 HS 相似度的加权和优于它们连接时的相似度。我们的实验是在 6 个嵌入任务上进行的，使用了来自 Massive Text Embedding Benchmark (MTEB) 的 20 个数据集。结果表明，无需进一步微调，MoEE 就为基于 LLM 的嵌入带来了显著的改进。</li>
</ul>

<h3>Title: DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</h3>
<ul>
<li><strong>Authors: </strong>Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.10819">https://arxiv.org/abs/2410.10819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.10819">https://arxiv.org/pdf/2410.10819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.10819]] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads(https://arxiv.org/abs/2410.10819)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in this https URL.</li>
<li><strong>摘要：</strong>部署长上下文大型语言模型 (LLM) 至关重要，但会带来巨大的计算和内存挑战。在所有注意力头上缓存所有键和值 (KV) 状态会消耗大量内存。现有的 KV 缓存修剪方法要么损害 LLM 的长上下文功能，要么只能提供有限的效率改进。在本文中，我们发现只有一小部分注意力头（又称检索头）对于处理长上下文至关重要，并且需要对所有标记进行充分关注。相比之下，所有其他主要关注最近标记和注意力接收器的头（称为流头）不需要完全注意。基于这一见解，我们引入了 DuoAttention，这是一个仅将完整 KV 缓存应用于检索头的框架，同时对流头使用轻量级、恒定长度的 KV 缓存，这既减少了 LLM 的解码和预填充内存和延迟，又不损害其长上下文能力。 DuoAttention 使用轻量级、基于优化的算法和合成数据来准确识别检索头。我们的方法显著减少了长上下文推理内存，MHA 模型最多可减少 2.55 倍，GQA 模型最多可减少 1.67 倍，同时将解码速度提高 2.18 倍和 1.50 倍，MHA 和 GQA 模型的预填充速度分别提高 1.73 倍和 1.63 倍，与全注意力相比，准确度损失最小。值得注意的是，结合量化，DuoAttention 可以在单个 A100 GPU 上以 330 万上下文长度解码 Llama-3-8B。代码在此 https URL 中提供。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
