<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-09</h1>
<h3>Title: Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and  Google Bard Content in Relation to BioMedical Literature</h3>
<ul>
<li><strong>Authors: </strong>Jakub Klimczak, Ahmed Abdeen Hamed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05116">https://arxiv.org/abs/2402.05116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05116">https://arxiv.org/pdf/2402.05116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05116]] Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and  Google Bard Content in Relation to BioMedical Literature(https://arxiv.org/abs/2402.05116)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, lora, prompt, chat</a></li>
<li><strong>Abstract: </strong>Background: The emergence of generative AI tools, empowered by Large Language Models (LLMs), has shown powerful capabilities in generating content. To date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. Objectives Using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. Methods In this exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. Our approach is to use text-mining approaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. Results The experiments demonstrated that ChatGPT outperformed Google Bard in cosine document similarity (38% to 34%), Jaccard document similarity (23% to 19%), TF-IDF bigram similarity (47% to 41%), and term network centrality (degree and closeness). We also found new links that emerged in ChatGPT bigram networks that did not exist in literature bigram networks. Conclusions: The obtained similarity results show that ChatGPT outperformed Google Bard in document similarity, bigrams, and degree and closeness centrality. We also observed that ChatGPT offers linkage to terms that are connected in the literature. Such connections could inspire asking interesting questions and generate new hypotheses.</li>
<li><strong>摘要：</strong>背景：在大型语言模型（LLM）的支持下，生成式人工智能工具的出现，显示出了强大的内容生成能力。迄今为止，对由所谓的即时工程生成的此类内容的有用性进行评估已成为一个有趣的研究问题。目标 使用即时工程的手段，我们评估这些内容与科学家创作的真实文献的相似性和接近度。方法 在本次探索性分析中，(1) 我们提示设计 ChatGPT 和 Google Bard 来生成临床内容以与文献对应内容进行比较，(2) 我们通过将生成的内容与生物医学文献中的对应内容进行比较来评估生成内容的相似性。我们的方法是使用文本挖掘方法来比较文档和相关的二元组，并使用网络分析来评估术语的中心性。结果实验表明，ChatGPT 在余弦文档相似度（38% 到 34%）、Jaccard 文档相似度（23% 到 19%）、TF-IDF 二元相似度（47% 到 41%）和术语网络中心性（47% 到 41%）方面优于 Google Bard（程度和亲密程度）。我们还发现 ChatGPT 二元网络中出现的新链接在文学二元网络中不存在。结论：获得的相似度结果表明，ChatGPT 在文档相似度、二元组、度和接近中心性方面优于 Google Bard。我们还观察到 ChatGPT 提供了与文献中相关术语的链接。这种联系可以激发提出有趣的问题并产生新的假设。</li>
</ul>

<h3>Title: A Closer Look at the Limitations of Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05119">https://arxiv.org/abs/2402.05119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05119">https://arxiv.org/pdf/2402.05119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05119]] A Closer Look at the Limitations of Instruction Tuning(https://arxiv.org/abs/2402.05119)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, hallucination, agent</a></li>
<li><strong>Abstract: </strong>Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. Our findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. We hope the insights and challenges revealed inspire future work.</li>
<li><strong>摘要：</strong>指令调优 (IT) 是使用指令-响应对训练大型语言模型 (LLM) 的过程，已成为将基本预训练的 LLM 转换为开放域会话代理的主要方法。尽管 IT 取得了显着的成功并得到广泛采用，但其局限性和缺点仍未得到充分探讨。在本文中，通过严格的实验和深入分析法学硕士通过IT所经历的变化，我们揭示了IT的各种局限性。我们特别指出，(1) IT 未能增强法学硕士的知识或技能。 LoRA微调仅限于学习响应启动和风格标记，全参数微调会导致知识退化。 (2) 从知识来源获取的 IT 数据集中复制响应模式会导致响应质量下降。 (3) 全参数微调通过不准确地从 IT 数据集中概念上相似的实例借用令牌来生成响应，从而增加了幻觉。 (4) 改进 IT 的流行方法不会比简单的 LoRA 微调模型带来性能改进。我们的研究结果表明，仅根据预先训练的知识生成的响应始终优于通过开源数据集上的 IT 学习任何形式的新知识的模型的响应。我们希望所揭示的见解和挑战能够对未来的工作有所启发。</li>
</ul>

<h3>Title: More Agents Is All You Need</h3>
<ul>
<li><strong>Authors: </strong>Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05120">https://arxiv.org/abs/2402.05120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05120">https://arxiv.org/pdf/2402.05120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05120]] More Agents Is All You Need(https://arxiv.org/abs/2402.05120)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, agent</a></li>
<li><strong>Abstract: </strong>We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.</li>
<li><strong>摘要：</strong>我们发现，只需通过采样和投票方法，大型语言模型（LLM）的性能就会随着实例化代理的数量而变化。此外，该方法与现有的复杂方法正交，以进一步增强LLM，而增强程度与任务难度相关。我们对各种法学硕士基准进行了全面的实验，以验证我们的发现的存在，并研究可以促进其发生的属性。我们的代码公开于：\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}。</li>
</ul>

<h3>Title: Large Language Model for Table Processing: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Weizheng Lu, Jiaming Zhang, Jing Zhang, Yueguo Chen</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05121">https://arxiv.org/abs/2402.05121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05121">https://arxiv.org/pdf/2402.05121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05121]] Large Language Model for Table Processing: A Survey(https://arxiv.org/abs/2402.05121)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet calculations, and generating reports from web tables. Automating these table-centric tasks with Large Language Models (LLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides an extensive overview of table tasks, encompassing not only the traditional areas like table question answering (Table QA) and fact verification, but also newly emphasized aspects such as table manipulation and advanced table data analysis. Additionally, it goes beyond the early strategies of pre-training and fine-tuning small language models, to include recent paradigms in LLM usage. The focus here is particularly on instruction-tuning, prompting, and agent-based approaches within the realm of LLMs. Finally, we highlight several challenges, ranging from private deployment and efficient inference to the development of extensive benchmarks for table manipulation and advanced data analysis.</li>
<li><strong>摘要：</strong>表通常是二维的，可存储大量数据，在数据库查询、电子表格计算和从 Web 表生成报告等日常活动中至关重要。使用大型语言模型 (LLM) 自动执行这些以表为中心的任务可以提供显着的公共利益，引起学术界和工业界的兴趣。这项调查对表格任务进行了广泛的概述，不仅涵盖表格问答（Table QA）和事实验证等传统领域，还包括表格操作和高级表格数据分析等新强调的方面。此外，它超越了预训练和微调小语言模型的早期策略，包括 LLM 使用中的最新范式。这里的重点特别是法学硕士领域内的指令调整、提示和基于代理的方法。最后，我们强调了几个挑战，从私有部署和高效推理到为表操作和高级数据分析开发广泛的基准。</li>
</ul>

<h3>Title: A Survey on Data Selection for LLM Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang, Dianhui Chu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05123">https://arxiv.org/abs/2402.05123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05123">https://arxiv.org/pdf/2402.05123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05123]] A Survey on Data Selection for LLM Instruction Tuning(https://arxiv.org/abs/2402.05123)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.</li>
<li><strong>摘要：</strong>指令调优是训练大型语言模型（LLM）的重要一步，因此如何增强指令调优的效果受到越来越多的关注。现有工作表明，在LLM的指令调整过程中，数据集的质量比数量更重要。因此，最近很多研究集中在探索从指令数据集中选择高质量子集的方法，旨在降低法学硕士的训练成本并增强其指令跟踪能力。本文对 LLM 指令调整的数据选择进行了全面的调查。首先，我们介绍广泛使用的指令数据集。然后，我们提出了数据选择方法的新分类法，并详细介绍了最新进展，并对数据选择方法的评估策略和结果进行了详细阐述。最后，我们强调了这项任务的开放挑战和新领域。</li>
</ul>

<h3>Title: Zero-Shot Clinical Trial Patient Matching with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Michael Wornow, Alejandro Lozano, Dev Dash, Jenelle Jindal, Kenneth W. Mahaffey, Nigam H. Shah</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05125">https://arxiv.org/abs/2402.05125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05125">https://arxiv.org/pdf/2402.05125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05125]] Zero-Shot Clinical Trial Patient Matching with LLMs(https://arxiv.org/abs/2402.05125)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of tokens processed by up to a third while retaining high performance. Third, we evaluate the interpretability of our system by having clinicians evaluate the natural language justifications generated by the LLM for each eligibility decision, and show that it can output coherent explanations for 97% of its correct decisions and 75% of its incorrect ones. Our results establish the feasibility of using LLMs to accelerate clinical trial operations.</li>
<li><strong>摘要：</strong>将患者与临床试验相匹配是将新药推向市场的一个尚未解决的关键挑战。如今，识别符合试验资格标准的患者需要高度手工操作，每个患者最多需要 1 小时。然而，自动筛选具有挑战性，因为它需要理解非结构化的临床文本。大型语言模型（LLM）提供了一个有前途的解决方案。在这项工作中，我们探索了它们在试验匹配中的应用。首先，我们设计一个基于法学硕士的系统，将患者的病史作为非结构化临床文本，评估该患者是否符合一组纳入标准（也指定为自由文本）。我们的零样本系统在 n2c2 2018 队列选择基准上取得了最先进的分数。其次，我们通过确定一种提示策略来提高我们方法的数据和成本效率，该策略比现状更快、更便宜地匹配患者一个数量级，并开发了一个两阶段检索管道，减少了处理的令牌数量到三分之一，同时保持高性能。第三，我们通过让临床医生评估法学硕士为每个资格决策生成的自然语言理由来评估我们系统的可解释性，并表明它可以为 97% 的正确决策和 75% 的错误决策输出连贯的解释。我们的结果证实了使用法学硕士加速临床试验操作的可行性。</li>
</ul>

<h3>Title: Graph Neural Network and NER-Based Text Summarization</h3>
<ul>
<li><strong>Authors: </strong>Imaad Zaffar Khan, Amaan Aijaz Sheikh, Utkarsh Sinha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05126">https://arxiv.org/abs/2402.05126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05126">https://arxiv.org/pdf/2402.05126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05126]] Graph Neural Network and NER-Based Text Summarization(https://arxiv.org/abs/2402.05126)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>With the abundance of data and information in todays time, it is nearly impossible for man, or, even machine, to go through all of the data line by line. What one usually does is to try to skim through the lines and retain the absolutely important information, that in a more formal term is called summarization. Text summarization is an important task that aims to compress lengthy documents or articles into shorter, coherent representations while preserving the core information and meaning. This project introduces an innovative approach to text summarization, leveraging the capabilities of Graph Neural Networks (GNNs) and Named Entity Recognition (NER) systems. GNNs, with their exceptional ability to capture and process the relational data inherent in textual information, are adept at understanding the complex structures within large documents. Meanwhile, NER systems contribute by identifying and emphasizing key entities, ensuring that the summarization process maintains a focus on the most critical aspects of the text. By integrating these two technologies, our method aims to enhances the efficiency of summarization and also tries to ensures a high degree relevance in the condensed content. This project, therefore, offers a promising direction for handling the ever increasing volume of textual data in an information-saturated world.</li>
<li><strong>摘要：</strong>当今时代，数据和信息非常丰富，人类甚至机器几乎不可能逐行浏览所有数据。人们通常所做的就是尝试快速浏览并保留绝对重要的信息，用更正式的术语来说，这称为总结。文本摘要是一项重要任务，旨在将冗长的文档或文章压缩为较短、连贯的表示形式，同时保留核心信息和含义。该项目引入了一种创新的文本摘要方法，利用图神经网络 (GNN) 和命名实体识别 (NER) 系统的功能。 GNN 具有捕获和处理文本信息中固有的关系数据的卓越能力，擅长理解大型文档中的复杂结构。同时，NER 系统通过识别和强调关键实体来做出贡献，确保摘要过程始终关注文本的最关键方面。通过整合这两种技术，我们的方法旨在提高摘要的效率，并试图确保浓缩内容的高度相关性。因此，该项目为在信息饱和的世界中处理不断增加的文本数据量提供了一个有前途的方向。</li>
</ul>

<h3>Title: Illuminate: A novel approach for depression detection with explainable  analysis and proactive therapy using prompt engineering</h3>
<ul>
<li><strong>Authors: </strong>Aryan Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05127">https://arxiv.org/abs/2402.05127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05127">https://arxiv.org/pdf/2402.05127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05127]] Illuminate: A novel approach for depression detection with explainable  analysis and proactive therapy using prompt engineering(https://arxiv.org/abs/2402.05127)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel paradigm for depression detection and treatment using advanced Large Language Models (LLMs): Generative Pre-trained Transformer 4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized prompts to diagnose, explain, and suggest therapeutic interventions for depression. A unique few-shot prompting method enhances the models' ability to analyze and explain depressive symptoms based on the DSM-5 criteria. In the interaction phase, the models engage in empathetic dialogue management, drawing from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide, fostering supportive interactions with individuals experiencing major depressive disorders. Additionally, the research introduces the Illuminate Database, enriched with various CBT modules, aiding in personalized therapy recommendations. The study evaluates LLM performance using metrics such as F1 scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) across different test sets, demonstrating their effectiveness. This comprehensive approach blends cutting-edge AI with established psychological methods, offering new possibilities in mental health care and showcasing the potential of LLMs in revolutionizing depression diagnosis and treatment strategies.</li>
<li><strong>摘要：</strong>本文介绍了一种使用高级大型语言模型 (LLM) 进行抑郁症检测和治疗的新范例：生成式预训练 Transformer 4 (GPT-4)、Llama 2 chat 和 Gemini。这些法学硕士通过专门的提示进行微调，以诊断、解释和建议抑郁症的治疗干预措施。独特的几次提示方法增强了模型根据 DSM-5 标准分析和解释抑郁症状的能力。在互动阶段，模型利用 PsychDB 和认知行为治疗 (CBT) 指南等资源进行移情对话管理，促进与患有重度抑郁症的个体的支持性互动。此外，该研究还引入了 Illuminate 数据库，其中包含各种 CBT 模块，有助于提出个性化治疗建议。该研究使用不同测试集的 F1 分数、精确度、召回率、余弦相似度和面向召回率的 Gisting 评估 (ROUGE) 等指标来评估法学硕士的表现，证明了其有效性。这种综合方法将尖端人工智能与既定的心理学方法相结合，为心理健康护理提供了新的可能性，并展示了法学硕士在彻底改变抑郁症诊断和治疗策略方面的潜力。</li>
</ul>

<h3>Title: Enhancing Textbook Question Answering Task with Large Language Models  and Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05128">https://arxiv.org/abs/2402.05128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05128">https://arxiv.org/pdf/2402.05128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05128]] Enhancing Textbook Question Answering Task with Large Language Models  and Retrieval Augmented Generation(https://arxiv.org/abs/2402.05128)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context and multimodal data. Although previous research has significantly improved the task, there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context. The introduction of large language models (LLMs) has revolutionized the field of AI, however, directly applying LLMs often leads to inaccurate answers. This paper proposes a methodology that handle the out-of-domain scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize transfer learning to handle the long context and enhance reasoning abilities. Through supervised fine-tuning of the LLM model Llama-2 and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for non-diagram multiple-choice questions.</li>
<li><strong>摘要：</strong>由于上下文和多模态数据的复杂性，教科书问答（TQA）是人工智能中的一项具有挑战性的任务。尽管之前的研究已经显着改进了该任务，但仍然存在一些局限性，包括模型推理能力较弱以及无法在冗长的上下文中捕获上下文信息。大语言模型（LLM）的引入彻底改变了人工智能领域，然而，直接应用LLM往往会导致不准确的答案。本文提出了一种处理 TQA 中域外场景的方法，其中概念通过结合检索增强生成（RAG）技术分布在不同的课程中，并利用迁移学习来处理长上下文并增强推理能力。通过对 LLM 模型 Llama-2 的监督微调和 RAG 的结合，我们的架构优于基线，在非图多项选择题的验证集上实现了 4.12% 的准确率提升，在测试集上实现了 9.84% 的准确率提升。</li>
</ul>

<h3>Title: Best Practices for Text Annotation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Petter Törnberg</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05129">https://arxiv.org/abs/2402.05129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05129">https://arxiv.org/pdf/2402.05129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05129]] Best Practices for Text Annotation with Large Language Models(https://arxiv.org/abs/2402.05129)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have ushered in a new era of text annotation, as their ease-of-use, high accuracy, and relatively low costs have meant that their use has exploded in recent months. However, the rapid growth of the field has meant that LLM-based annotation has become something of an academic Wild West: the lack of established practices and standards has led to concerns about the quality and validity of research. Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results. Recognizing the transformative potential of LLMs, this paper proposes a comprehensive set of standards and best practices for their reliable, reproducible, and ethical use. These guidelines span critical areas such as model selection, prompt engineering, structured prompting, prompt stability analysis, rigorous model validation, and the consideration of ethical and legal implications. The paper emphasizes the need for a structured, directed, and formalized approach to using LLMs, aiming to ensure the integrity and robustness of text annotation practices, and advocates for a nuanced and critical engagement with LLMs in social scientific research.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 开创了文本注释的新时代，因为其易用性、高精度和相对较低的成本意味着它们的使用在最近几个月出现了爆炸式增长。然而，该领域的快速发展意味着基于法学硕士的注释已成为学术上的狂野西部：缺乏既定的实践和标准导致了对研究质量和有效性的担忧。研究人员警告说，法学硕士表面上的简单性可能会产生误导，因为它们很容易出现偏见、误解和不可靠的结果。认识到法学硕士的变革潜力，本文提出了一套全面的标准和最佳实践，以实现其可靠、可重复和合乎道德的使用。这些指南涵盖了模型选择、提示工程、结构化提示、提示稳定性分析、严格的模型验证以及道德和法律影响的考虑等关键领域。该论文强调需要采用结构化、定向和形式化的方法来使用法学硕士，旨在确保文本注释实践的完整性和稳健性，并倡导法学硕士在社会科学研究中进行细致入微和批判性的参与。</li>
</ul>

<h3>Title: LB-KBQA: Large-language-model and BERT based Knowledge-Based Question  and Answering System</h3>
<ul>
<li><strong>Authors: </strong>Yan Zhao, Zhongyun Li, Jiaxing Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05130">https://arxiv.org/abs/2402.05130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05130">https://arxiv.org/pdf/2402.05130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05130]] LB-KBQA: Large-language-model and BERT based Knowledge-Based Question  and Answering System(https://arxiv.org/abs/2402.05130)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect newly appeared intent and acquire new knowledge. In experiments on financial domain question answering, our model has demonstrated superior effectiveness.</li>
<li><strong>摘要：</strong>生成式人工智能（AI）因其新兴能力已经赋能各个领域，其中典型的就是大型语言模型（LLM）。生成式人工智能的典型应用领域之一是大语言模型（LLM），与传统的基于人工智能的方法相比，LLM的自然语言理解能力得到了显着提高。自然语言理解能力一直是基于知识的问答（KBQA）系统意图识别性能的障碍，这是由于语言多样性和新出现的意图而产生的。传统的基于人工智能的意图识别方法可以分为基于语义解析的方法和基于模型的方法。然而，这两种方法在意图识别方面都受到资源有限的影响。为了解决这个问题，我们提出了一种基于大语言模型（LLM）和 BERT（LB-KBQA）的新型 KBQA 系统。在生成人工智能的帮助下，我们提出的方法可以检测新出现的意图并获取新知识。在金融领域问答实验中，我们的模型表现出了卓越的有效性。</li>
</ul>

<h3>Title: Financial Report Chunking for Effective Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, Leah Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05131">https://arxiv.org/abs/2402.05131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05131">https://arxiv.org/pdf/2402.05131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05131]] Financial Report Chunking for Effective Retrieval Augmented Generation(https://arxiv.org/abs/2402.05131)</code><input type="text"></li>
<li><strong>Keywords: </strong>retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question & Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the quality of RAG outputs. Findings support that element type based chunking largely improve RAG results on financial reporting. Through this research, we are also able to answer how to uncover highly accurate RAG.</li>
<li><strong>摘要：</strong>信息分块是检索增强生成 (RAG) 的关键步骤。目前的研究主要集中在段落级分块上。这种方法将所有文本视为平等，并忽略了文档结构中包含的信息。我们提出了一种对块文档的扩展方法，超越了单纯的段落级分块，而是通过文档的结构元素组件对主要块进行了分块。将文档分解为这些组成元素创建了一种对文档进行分块的新方法，无需调整即可产生最佳的分块大小。我们引入了一种新颖的框架，该框架评估基于文档理解模型注释的元素类型的分块如何有助于检索信息的整体上下文和准确性。我们还演示了这种方法如何影响 RAG 辅助问答任务的绩效。我们的研究包括对各种元素类型、它们在有效信息检索中的作用以及它们对 RAG 输出质量的影响进行全面分析。研究结果支持基于元素类型的分块在很大程度上改善了财务报告的 RAG 结果。通过这项研究，我们还能够回答如何发现高度准确的 RAG。</li>
</ul>

<h3>Title: TexShape: Information Theoretic Sentence Embedding for Language Models</h3>
<ul>
<li><strong>Authors: </strong>H. Kaan Kale, Homa Esfahanizadeh, Noel Elias, Oguzhan Baser, Muriel Medard, Sriram Vishwanath</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05132">https://arxiv.org/abs/2402.05132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05132">https://arxiv.org/pdf/2402.05132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05132]] TexShape: Information Theoretic Sentence Embedding for Language Models(https://arxiv.org/abs/2402.05132)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>With the exponential growth in data volume and the emergence of data-intensive applications, particularly in the field of machine learning, concerns related to resource utilization, privacy, and fairness have become paramount. This paper focuses on the textual domain of data and addresses challenges regarding encoding sentences to their optimized representations through the lens of information-theory. In particular, we use empirical estimates of mutual information, using the Donsker-Varadhan definition of Kullback-Leibler divergence. Our approach leverages this estimation to train an information-theoretic sentence embedding, called TexShape, for (task-based) data compression or for filtering out sensitive information, enhancing privacy and fairness. In this study, we employ a benchmark language model for initial text representation, complemented by neural networks for information-theoretic compression and mutual information estimations. Our experiments demonstrate significant advancements in preserving maximal targeted information and minimal sensitive information over adverse compression ratios, in terms of predictive accuracy of downstream models that are trained using the compressed data.</li>
<li><strong>摘要：</strong>随着数据量的指数级增长和数据密集型应用程序的出现，特别是在机器学习领域，与资源利用、隐私和公平性相关的担忧变得至关重要。本文重点关注数据的文本领域，并通过信息论的视角解决了将句子编码为其优化表示的挑战。特别是，我们使用互信息的经验估计，使用 Kullback-Leibler 散度的 Donsker-Varadhan 定义。我们的方法利用这种估计来训练信息论句子嵌入，称为 TexShape，用于（基于任务的）数据压缩或过滤敏感信息，增强隐私性和公平性。在本研究中，我们采用基准语言模型来进行初始文本表示，并辅之以用于信息理论压缩和互信息估计的神经网络。我们的实验表明，在使用压缩数据训练的下游模型的预测准确性方面，在逆压缩率下保留最大目标信息和最小敏感信息方面取得了显着进步。</li>
</ul>

<h3>Title: Personalized Language Modeling from Personalized Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Li, Zachary C. Lipton, Liu Leqi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05133">https://arxiv.org/abs/2402.05133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05133">https://arxiv.org/pdf/2402.05133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05133]] Personalized Language Modeling from Personalized Human Feedback(https://arxiv.org/abs/2402.05133)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimization. To demonstrate the efficacy of our method, we test it on real-world text summarization data with annotated preferences and annotator information. We fine-tune GPT-J 6B to obtain personalized language (and reward) models, which outperform non-personalized models in terms of aligning with individual preferences.</li>
<li><strong>摘要：</strong>来自人类反馈的强化学习（RLHF）是当前的主导框架，用于微调大型语言模型以更好地符合人类偏好。然而，当人类反馈中编码的用户偏好多种多样时，在此框架下开发的算法的基本前提可能会出现问题。在这项工作中，我们的目标是通过开发构建个性化语言模型的方法来解决这个问题。我们首先正式介绍从个性化人类反馈中学习的任务，并解释为什么普通 RLHF 在这种情况下可能会出现问题。然后，我们提出了一种通用的个性化 RLHF（P-RLHF）框架，该框架需要联合学习用户模型和语言（或奖励）模型。用户模型接收用户信息并输出用户表示。它的结构编码了我们对反馈数据背后的用户偏好的假设。我们为个性化奖励建模和个性化直接偏好优化制定新的学习目标。为了证明我们方法的有效性，我们在带有注释偏好和注释者信息的真实文本摘要数据上对其进行了测试。我们对 GPT-J 6B 进行微调以获得个性化语言（和奖励）模型，该模型在符合个人偏好方面优于非个性化模型。</li>
</ul>

<h3>Title: CADReN: Contextual Anchor-Driven Relational Network for Controllable  Cross-Graphs Node Importance Estimation</h3>
<ul>
<li><strong>Authors: </strong>Zijie Zhong, Yunhui Zhang, Ziyi Chang, Zengchang Qin</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05135">https://arxiv.org/abs/2402.05135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05135">https://arxiv.org/pdf/2402.05135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05135]] CADReN: Contextual Anchor-Driven Relational Network for Controllable  Cross-Graphs Node Importance Estimation(https://arxiv.org/abs/2402.05135)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Node Importance Estimation (NIE) is crucial for integrating external information into Large Language Models through Retriever-Augmented Generation. Traditional methods, focusing on static, single-graph characteristics, lack adaptability to new graphs and user-specific requirements. CADReN, our proposed method, addresses these limitations by introducing a Contextual Anchor (CA) mechanism. This approach enables the network to assess node importance relative to the CA, considering both structural and semantic features within Knowledge Graphs (KGs). Extensive experiments show that CADReN achieves better performance in cross-graph NIE task, with zero-shot prediction ability. CADReN is also proven to match the performance of previous models on single-graph NIE task. Additionally, we introduce and opensource two new datasets, RIC200 and WK1K, specifically designed for cross-graph NIE research, providing a valuable resource for future developments in this domain.</li>
<li><strong>摘要：</strong>节点重要性估计 (NIE) 对于通过检索器增强生成将外部信息集成到大型语言模型中至关重要。传统方法侧重于静态、单图特征，缺乏对新图和用户特定需求的适应性。我们提出的方法 CADReN 通过引入上下文锚定 (CA) 机制来解决这些限制。这种方法使网络能够评估节点相对于 CA 的重要性，同时考虑知识图 (KG) 中的结构和语义特征。大量实验表明，CADReN在跨图NIE任务中取得了更好的性能，具有零样本预测能力。 CADReN 还被证明可以与之前模型在单图 NIE 任务上的性能相匹配。此外，我们还引入并开源了两个新数据集 RIC200 和 WK1K，专门为跨图 NIE 研究而设计，为该领域的未来发展提供了宝贵的资源。</li>
</ul>

<h3>Title: LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to  256K</h3>
<ul>
<li><strong>Authors: </strong>Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05136">https://arxiv.org/abs/2402.05136</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05136">https://arxiv.org/pdf/2402.05136</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05136]] LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to  256K(https://arxiv.org/abs/2402.05136)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag</a></li>
<li><strong>Abstract: </strong>State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation studies on the techniques used in LV-Eval construction. The results reveal that: (i) Commercial LLMs generally outperform open-source LLMs when evaluated within length levels shorter than their claimed context length. However, their overall performance is surpassed by open-source LLMs with longer context lengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k, exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of "needle in a haystack". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in LV-Eval. All datasets and evaluation codes are released at: https://github.com/infinigence/LVEval.</li>
<li><strong>摘要：</strong>最先进的大型语言模型 (LLM) 现在声称可支持 256k 甚至更长的上下文长度。相比之下，主流基准的平均上下文长度不足（5k-21k），并且存在潜在的知识泄漏和指标不准确的问题，导致评估有偏差。本文介绍了 LV-Eval，这是一个具有挑战性的长上下文基准测试，具有五个长度级别（16k、32k、64k、128k 和 256k），最高可达 256k 字。 LV-Eval 具有两个主要任务，单跳 QA 和多跳 QA，包含 11 个双语数据集。 LV-Eval的设计融合了三个关键技术，即混淆事实插入、关键词和短语替换以及基于关键词回忆的度量设计。 LV-Eval 的优点包括跨不同上下文长度的可控评估、用令人困惑的事实挑战测试实例、减少知识泄漏以及更客观的评估。我们评估了 10 名法学硕士的 LV-Eval，并对 LV-Eval 构建中使用的技术进行了消融研究。结果表明：（i）在比其声称的上下文长度更短的长度水平内进行评估时，商业法学硕士通常优于开源法学硕士。然而，它们的整体表现被上下文长度较长的开源法学硕士所超越。 (ii) 极长上下文的法学硕士，例如 Yi-6B-200k，表现出相对温和的性能下降，但其绝对性能不一定高于上下文长度较短的法学硕士。 (iii) 法学硕士的表现在存在混乱信息的情况下可能会显着下降，尤其是在“大海捞针”的压力测试中。 (iv) 与知识泄漏和不准确的指标相关的问题会在评估中引入偏差，而这些担忧在 LV-Eval 中得到了缓解。所有数据集和评估代码均发布于：https://github.com/infinigence/LVEval。</li>
</ul>

<h3>Title: SceMQA: A Scientific College Entrance Level Multimodal Question  Answering Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05138">https://arxiv.org/abs/2402.05138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05138">https://arxiv.org/pdf/2402.05138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05138]] SceMQA: A Scientific College Entrance Level Multimodal Question  Answering Benchmark(https://arxiv.org/abs/2402.05138)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models. Our benchmark and analysis will be available at https://scemqa.github.io/</li>
<li><strong>摘要：</strong>本文介绍了 SceMQA，这是一种针对高考水平的科学多模态问答的新颖基准。它解决了现有基准中经常被忽视的关键教育阶段，从高中到大学预科阶段。 SceMQA 专注于核心科学科目，包括数学、物理、化学和生物学。它采用多项选择和自由回答的形式，确保对人工智能模型能力的全面评估。此外，我们的基准测试还提供了每个问题的具体知识点以及每个答案的详细解释。 SceMQA 还独特地呈现具有相同上下文但不同问题的问题，以促进对推理能力进行更彻底和准确的评估。在实验中，我们在各种实验设置中评估了开源和闭源最先进的多模态大型语言模型（MLLM）。结果表明，需要进一步研究和开发来开发更强大的 MLLM，最强大的模型只能达到 50% 到 60% 的准确度。我们的基准测试和分析将在 https://scemqa.github.io/ 上提供</li>
</ul>

<h3>Title: Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains</h3>
<ul>
<li><strong>Authors: </strong>Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, Nicolo Fusi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05140">https://arxiv.org/abs/2402.05140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05140">https://arxiv.org/pdf/2402.05140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05140]] Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains(https://arxiv.org/abs/2402.05140)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from task functions, our method enables zero-shot generalization to unseen problems through diverse combinations of the input tags. It also boosts LLM's performance in various specialized domains, such as predicting protein or chemical properties and modeling drug-target interactions, outperforming expert models tailored to these tasks.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在理解和生成自然语言方面表现出了卓越的能力。然而，它们的能力在预训练语料库中代表性不足的高度专业化领域（例如物理和生物医学科学）中减弱。这项工作探讨了如何将通用法学硕士重新利用为专门领域的有效任务解决器。我们引入了一种新颖的、与模型无关的框架，用于学习自定义输入标签，这些标签被参数化为附加到 LLM 嵌入层的连续向量，以调节 LLM。我们设计了两种类型的输入标签：域标签用于界定专门的表示（例如化学式）并提供域相关的上下文；函数标签用于表示特定函数（例如，预测分子特性）并压缩函数求解指令。我们开发了一个三阶段协议来使用辅助数据和领域知识来学习这些标签。通过明确地将任务域与任务函数分开，我们的方法可以通过输入标签的不同组合对未见过的问题进行零样本泛化。它还提高了法学硕士在各个专业领域的表现，例如预测蛋白质或化学性质以及建模药物-靶点相互作用，优于为这些任务量身定制的专家模型。</li>
</ul>

<h3>Title: ApiQ: Finetuning of 2-Bit Quantized Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Baohao Liao, Christof Monz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05147">https://arxiv.org/abs/2402.05147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05147">https://arxiv.org/pdf/2402.05147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05147]] ApiQ: Finetuning of 2-Bit Quantized Large Language Model(https://arxiv.org/abs/2402.05147)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error propagation from shallower into deeper layers. Through comprehensive evaluations conducted on a spectrum of language tasks with various models, ApiQ demonstrably minimizes activation error during quantization. Consequently, it consistently achieves superior finetuning outcomes across various bit-widths of quantization.</li>
<li><strong>摘要：</strong>随着 LLM 规模的不断增大，大型语言模型 (LLM) 的内存高效微调最近引起了人们的广泛关注，这主要是由于 GPU 内存限制以及这些方法在完全微调的情况下的可比结果所带来的限制。尽管取得了进步，但当前的内存高效微调策略（例如 QLoRA）在不同的位宽量化和多方面任务中表现出不一致的性能。这种不一致很大程度上源于量化过程对保存的知识的有害影响，导致灾难性的遗忘并破坏了预训练模型用于微调的用途。在这项工作中，我们引入了一种名为 ApiQ 的新型量化框架，旨在通过同时初始化 LoRA 组件和量化 LLM 的权重来恢复量化中丢失的信息。这种方法确保维持原始 LLM 的激活精度，同时减轻从浅层到更深层的误差传播。通过使用各种模型对一系列语言任务进行综合评估，ApiQ 明显最小化了量化过程中的激活误差。因此，它能够在各种量化位宽上始终如一地实现卓越的微调结果。</li>
</ul>

<h3>Title: FlowPG: Action-constrained Policy Gradient with Normalizing Flows</h3>
<ul>
<li><strong>Authors: </strong>Janaka Chathuranga Brahmanage, Jiajing Ling, Akshat Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05149">https://arxiv.org/abs/2402.05149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05149">https://arxiv.org/pdf/2402.05149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05149]] FlowPG: Action-constrained Policy Gradient with Normalizing Flows(https://arxiv.org/abs/2402.05149)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learned normalizing flow with the DDPG algorithm. By design, a well-trained normalizing flow will transform policy output into a valid action without requiring an optimization solver. Empirically, our approach results in significantly fewer constraint violations (upto an order-of-magnitude for several instances) and is multiple times faster on a variety of continuous control tasks.</li>
<li><strong>摘要：</strong>行动约束强化学习（ACRL）是解决安全关键和资源分配相关决策问题的流行方法。 ACRL 的一个主要挑战是确保智能体采取有效的行动，满足每个 RL 步骤中的约束。在策略网络之上使用投影层的常用方法需要解决优化程序，这可能导致训练时间更长、收敛速度慢和零梯度问题。为了解决这个问题，首先我们使用归一化流模型来学习可行动作空间与潜在变量（例如高斯）的简单分布的支持之间的可逆、可微映射。其次，学习流模型需要从可行的动作空间中进行采样，这也具有挑战性。我们基于哈密顿蒙特卡罗和概率句子决策图开发了多种方法，用于凸和非凸约束的此类动作采样。第三，我们将学习到的归一化流程与 DDPG 算法集成。根据设计，训练有素的标准化流程将策略输出转换为有效的操作，而不需要优化求解器。根据经验，我们的方法可以显着减少约束违规（在多个实例中达到一个数量级），并且在各种连续控制任务上速度提高数倍。</li>
</ul>

<h3>Title: CrashFormer: A Multimodal Architecture to Predict the Risk of Crash</h3>
<ul>
<li><strong>Authors: </strong>Amin Karimi Monsefi, Pouya Shiri, Ahmad Mohammadshirazi, Nastaran Karimi Monsefi, Ron Davies, Sobhan Moosavi, Rajiv Ramnath</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05151">https://arxiv.org/abs/2402.05151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05151">https://arxiv.org/pdf/2402.05151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05151]] CrashFormer: A Multimodal Architecture to Predict the Risk of Crash(https://arxiv.org/abs/2402.05151)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Reducing traffic accidents is a crucial global public safety concern. Accident prediction is key to improving traffic safety, enabling proactive measures to be taken before a crash occurs, and informing safety policies, regulations, and targeted interventions. Despite numerous studies on accident prediction over the past decades, many have limitations in terms of generalizability, reproducibility, or feasibility for practical use due to input data or problem formulation. To address existing shortcomings, we propose CrashFormer, a multi-modal architecture that utilizes comprehensive (but relatively easy to obtain) inputs such as the history of accidents, weather information, map images, and demographic information. The model predicts the future risk of accidents on a reasonably acceptable cadence (i.e., every six hours) for a geographical location of 5.161 square kilometers. CrashFormer is composed of five components: a sequential encoder to utilize historical accidents and weather data, an image encoder to use map imagery data, a raw data encoder to utilize demographic information, a feature fusion module for aggregating the encoded features, and a classifier that accepts the aggregated data and makes predictions accordingly. Results from extensive real-world experiments in 10 major US cities show that CrashFormer outperforms state-of-the-art sequential and non-sequential models by 1.8% in F1-score on average when using ``sparse'' input data.</li>
<li><strong>摘要：</strong>减少交通事故是全球公共安全的一个重要问题。事故预测是改善交通安全、在事故发生前采取主动措施以及为安全政策、法规和有针对性的干预措施提供信息的关键。尽管过去几十年来对事故预测进行了大量研究，但由于输入数据或问题表述，许多研究在普遍性、再现性或实际使用的可行性方面存在局限性。为了解决现有的缺点，我们提出了 CrashFormer，这是一种多模式架构，它利用全面（但相对容易获取）的输入，例如事故历史、天气信息、地图图像和人口统计信息。该模型以合理可接受的节奏（即每六个小时）预测 5.161 平方公里地理位置的未来事故风险。 CrashFormer 由五个组件组成：一个利用历史事故和天气数据的顺序编码器、一个使用地图图像数据的图像编码器、一个利用人口统计信息的原始数据编码器、一个用于聚合编码特征的特征融合模块，以及一个分类器接受汇总数据并相应地做出预测。在美国 10 个主要城市进行的广泛现实世界实验的结果表明，在使用“稀疏”输入数据时，CrashFormer 的 F1 分数平均优于最先进的序列和非序列模型 1.8%。</li>
</ul>

<h3>Title: Estimating On-road Transportation Carbon Emissions from Open Data of  Road Network and Origin-destination Flow Data</h3>
<ul>
<li><strong>Authors: </strong>Jinwei Zeng, Yu Liu, Jingtao Ding, Jian Yuan, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05153">https://arxiv.org/abs/2402.05153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05153">https://arxiv.org/pdf/2402.05153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05153]] Estimating On-road Transportation Carbon Emissions from Open Data of  Road Network and Origin-destination Flow Data(https://arxiv.org/abs/2402.05153)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Accounting for over 20% of the total carbon emissions, the precise estimation of on-road transportation carbon emissions is crucial for carbon emission monitoring and efficient mitigation policy formulation. However, existing estimation methods typically depend on hard-to-collect individual statistics of vehicle miles traveled to calculate emissions, thereby suffering from high data collection difficulty. To relieve this issue by utilizing the strong pattern recognition of artificial intelligence, we incorporate two sources of open data representative of the transportation demand and capacity factors, the origin-destination (OD) flow data and the road network data, to build a hierarchical heterogeneous graph learning method for on-road carbon emission estimation (HENCE). Specifically, a hierarchical graph consisting of the road network level, community level, and region level is constructed to model the multi-scale road network-based connectivity and travel connection between spatial areas. Heterogeneous graphs consisting of OD links and spatial links are further built at both the community level and region level to capture the intrinsic interactions between travel demand and road network accessibility. Extensive experiments on two large-scale real-world datasets demonstrate HENCE's effectiveness and superiority with R-squared exceeding 0.75 and outperforming baselines by 9.60% on average, validating its success in pioneering the use of artificial intelligence to empower carbon emission management and sustainability development. The implementation codes are available at this link: https://github.com/tsinghua-fib-lab/HENCE.</li>
<li><strong>摘要：</strong>道路交通碳排放量占碳排放总量的20%以上，其精确估算对于碳排放监测和有效的减排政策制定至关重要。然而，现有的估算方法通常依赖于难以收集的车辆行驶里程的个体统计数据来计算排放，因此数据收集难度较高。为了利用人工智能强大的模式识别能力来缓解这个问题，我们结合了代表交通需求和容量因素的两个开放数据源，即起点-目的地（OD）流量数据和路网数据，构建了分层异构道路碳排放估算的图学习方法（HENCE）。具体来说，构建由道路网络级别、社区级别和区域级别组成的层次图，以对空间区域之间基于多尺度道路网络的连通性和出行连接进行建模。在社区层面和区域层面进一步构建由OD链接和空间链接组成的异构图，以捕获出行需求和路网可达性之间的内在相互作用。在两个大规模真实数据集上进行的大量实验证明了HENCE的有效性和优越性，R平方超过0.75，平均优于基线9.60%，验证了其在利用人工智能赋能碳排放管理和可持续发展方面的成功。实现代码可通过以下链接获取：https://github.com/tsinghua-fib-lab/HENCE。</li>
</ul>

<h3>Title: Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank  Modifications</h3>
<ul>
<li><strong>Authors: </strong>Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05162">https://arxiv.org/abs/2402.05162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05162">https://arxiv.org/pdf/2402.05162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05162]] Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank  Modifications(https://arxiv.org/abs/2402.05162)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在其安全机制中表现出固有的脆弱性，这一点可以从它们对越狱甚至非恶意微调的敏感性中得到证明。本研究通过利用剪枝和低阶修改来探讨安全对齐的脆弱性。我们开发方法来识别对安全护栏至关重要的关键区域，并且在神经元和等级级别上与实用程序相关区域分离。令人惊讶的是，我们发现的孤立区域非常稀疏，在参数级别大约包含 $3\%$，在排名级别包含 $2.5\%$。删除这些区域会损害安全性，但不会显着影响实用性，这证实了模型安全机制固有的脆弱性。此外，我们表明，即使对安全关键区域的修改受到限制，法学硕士仍然容易受到低成本微调攻击。这些发现强调了法学硕士迫切需要更强有力的安全策略。</li>
</ul>

<h3>Title: The Effect of Sampling Temperature on Problem Solving in Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Matthew Renze, Erhan Guven</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05201">https://arxiv.org/abs/2402.05201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05201">https://arxiv.org/pdf/2402.05201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05201]] The Effect of Sampling Temperature on Problem Solving in Large Language  Models(https://arxiv.org/abs/2402.05201)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.</li>
<li><strong>摘要：</strong>在这项研究中，我们实证研究了采样温度对大型语言模型（LLM）在各种问题解决任务中的性能的影响。我们通过从标准 LLM 基准中随机抽取问题来创建多项选择题与答案 (MCQA) 考试。然后，我们使用四个流行的法学硕士和五种即时工程技术来解决 MCQA 问题，同时将采样温度从 0.0 提高到 1.0。尽管有相反的传闻，但我们的实证结果表明，0.0 至 1.0 范围内的温度变化不会对 LLM 解决问题任务的表现产生统计上的显着影响。此外，无论法学硕士、即时工程技术或问题领域如何，这些结果似乎都成立。所有代码、数据和补充材料均可在 GitHub 上获取：https://github.com/matthewrenze/jhu-llm-Temperature。</li>
</ul>

<h3>Title: Bellman Conformal Inference: Calibrating Prediction Intervals For Time  Series</h3>
<ul>
<li><strong>Authors: </strong>Zitong Yang, Emmanuel Candès, Lihua Lei</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05203">https://arxiv.org/abs/2402.05203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05203">https://arxiv.org/pdf/2402.05203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05203]] Bellman Conformal Inference: Calibrating Prediction Intervals For Time  Series(https://arxiv.org/abs/2402.05203)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, rag</a></li>
<li><strong>Abstract: </strong>We introduce Bellman Conformal Inference (BCI), a framework that wraps around any time series forecasting models and provides calibrated prediction intervals. Unlike the existing methods, BCI is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (SCP) at each time step. In particular, we use the dynamic programming algorithm to find the optimal policy for the SCP. We prove that BCI achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts. We find empirically that BCI avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals on volatility forecasting problems when compared with existing methods.</li>
<li><strong>摘要：</strong>我们引入贝尔曼保形推理 (BCI)，这是一个包含任何时间序列预测模型并提供校准预测区间的框架。与现有方法不同，BCI 能够利用多步提前预测，并通过在每个时间步解决一维随机控制问题 (SCP) 来显式优化平均间隔长度。特别是，我们使用动态规划算法来寻找 SCP 的最优策略。我们证明，即使多步提前预测不佳，BCI 也能在任意分布变化和时间依赖性下实现长期覆盖。我们根据经验发现，与现有方法相比，BCI 避免了无限长度的无信息区间，并在波动性预测问题上产生了更短的预测区间。</li>
</ul>

<h3>Title: Universal Neural Functionals</h3>
<ul>
<li><strong>Authors: </strong>Allan Zhou, Chelsea Finn, James Harrison</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05232">https://arxiv.org/abs/2402.05232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05232">https://arxiv.org/pdf/2402.05232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05232]] Universal Neural Functionals(https://arxiv.org/abs/2402.05232)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the weight space they optimize. We open-source our library for constructing UNFs at https://github.com/AllanYangZhou/universal_neural_functional.</li>
<li><strong>摘要：</strong>许多现代机器学习任务中的一个具有挑战性的问题是处理权重空间特征，即从神经网络的权重和梯度中转换或提取信息。最近的工作已经开发出了有前景的权重空间模型，这些模型与简单前馈网络的排列对称性等价。然而，它们不适用于一般架构，因为权重空间的排列对称性可能因递归或残差连接而变得复杂。这项工作提出了一种针对任何权重空间自动构建排列等变模型的算法，我们将其称为通用神经函数（UNF）。在其他应用中，我们演示了如何将 UNF 替换到现有的学习优化器设计中，并在优化小型图像分类器和语言模型时发现比现有方法有希望的改进。我们的结果表明，学习优化器可以从考虑其优化的权重空间的（对称）结构中受益。我们在 https://github.com/AllanYangZhou/universal_neural_function 上开源了用于构建 UNF 的库。</li>
</ul>

<h3>Title: QGFN: Controllable Greediness with Action Values</h3>
<ul>
<li><strong>Authors: </strong>Elaine Lau, Stephen Zhewen Lu, Ling Pan, Doina Precup, Emmanuel Bengio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05234">https://arxiv.org/abs/2402.05234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05234">https://arxiv.org/pdf/2402.05234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05234]] QGFN: Controllable Greediness with Action Values(https://arxiv.org/abs/2402.05234)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets; GFNs) are a family of reward/energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.</li>
<li><strong>摘要：</strong>生成流网络（GFlowNets；GFN）是一系列基于奖励/能量的组合对象生成方法，能够生成多样化且高实用性的样本。然而，让 GFN 偏向于生成高效用样本并非易事。在这项工作中，我们利用 GFN 和强化学习 (RL) 之间的联系，并建议将 GFN 策略与动作值估计 $Q$ 相结合，以创建可以通过混合参数控制的贪婪采样策略。我们证明了所提出的方法 QGFN 的几种变体能够在不牺牲多样性的情况下提高各种任务中生成的高奖励样本的数量。</li>
</ul>

<h3>Title: Learning Fair Ranking Policies via Differentiable Optimization of  Ordered Weighted Averages</h3>
<ul>
<li><strong>Authors: </strong>My H. Dinh, James Kotary, Ferdinando Fioretto</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05252">https://arxiv.org/abs/2402.05252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05252">https://arxiv.org/pdf/2402.05252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05252]] Learning Fair Ranking Policies via Differentiable Optimization of  Ordered Weighted Averages(https://arxiv.org/abs/2402.05252)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constrained optimizations of OWA objectives, enabling their use in integrated prediction and decision models.</li>
<li><strong>摘要：</strong>Learning to Rank (LTR) 是最广泛使用的机器学习应用程序之一。它是具有深远社会影响的平台的关键组成部分，包括求职、医疗信息检索和社交媒体内容提要。传统的 LTR 模型已被证明会产生偏差结果，从而引发了关于如何解决仅优先考虑用户相关性的排名系统所带来的差异的讨论。然而，虽然已经提出了几种公平学习排名模型，但它们在准确性或效率方面存在缺陷，从而限制了它们在现实世界排名平台中的适用性。本文展示了如何将基于有序加权平均 (OWA) 函数优化的可有效解决的公平排名模型集成到 LTR 模型的训练循环中，以实现公平性、用户效用和运行时效率之间的良好平衡。特别是，本文首次展示了如何通过 OWA 目标的约束优化进行反向传播，从而使其能够在集成预测和决策模型中使用。</li>
</ul>

<h3>Title: Convergence for Natural Policy Gradient on Infinite-State Average-Reward  Markov Decision Processes</h3>
<ul>
<li><strong>Authors: </strong>Isaac Grosof, Siva Theja Maguluri, R. Srikant</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05274">https://arxiv.org/abs/2402.05274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05274">https://arxiv.org/pdf/2402.05274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05274]] Convergence for Natural Policy Gradient on Infinite-State Average-Reward  Markov Decision Processes(https://arxiv.org/abs/2402.05274)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Infinite-state Markov Decision Processes (MDPs) are essential in modeling and optimizing a wide variety of engineering problems. In the reinforcement learning (RL) context, a variety of algorithms have been developed to learn and optimize these MDPs. At the heart of many popular policy-gradient based learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the Natural Policy Gradient (NPG) algorithm. Convergence results for these RL algorithms rest on convergence results for the NPG algorithm. However, all existing results on the convergence of the NPG algorithm are limited to finite-state settings. We prove the first convergence rate bound for the NPG algorithm for infinite-state average-reward MDPs, proving a $O(1/\sqrt{T})$ convergence rate, if the NPG algorithm is initialized with a good initial policy. Moreover, we show that in the context of a large class of queueing MDPs, the MaxWeight policy suffices to satisfy our initial-policy requirement and achieve a $O(1/\sqrt{T})$ convergence rate. Key to our result are state-dependent bounds on the relative value function achieved by the iterate policies of the NPG algorithm.</li>
<li><strong>摘要：</strong>无限状态马尔可夫决策过程 (MDP) 对于建模和优化各种工程问题至关重要。在强化学习 (RL) 背景下，已经开发了多种算法来学习和优化这些 MDP。许多流行的基于策略梯度的学习算法（例如自然行动批评家、TRPO 和 PPO）的核心是自然策略梯度 (NPG) 算法。这些 RL 算法的收敛结果取决于 NPG 算法的收敛结果。然而，所有现有的 NPG 算法收敛结果都仅限于有限状态设置。我们证明了无限状态平均奖励 MDP 的 NPG 算法的第一个收敛速度界限，如果 NPG 算法使用良好的初始策略进行初始化，则证明 $O(1/\sqrt{T})$ 收敛速度。此外，我们表明，在一大类排队 MDP 的背景下，MaxWeight 策略足以满足我们的初始策略要求并实现 $O(1/\sqrt{T})$ 收敛率。我们结果的关键是 NPG 算法的迭代策略实现的相对价值函数的状态相关界限。</li>
</ul>

<h3>Title: Exploring Hierarchical Classification Performance for Time Series Data:  Dissimilarity Measures and Classifier Comparisons</h3>
<ul>
<li><strong>Authors: </strong>Celal Alagoz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05275">https://arxiv.org/abs/2402.05275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05275">https://arxiv.org/pdf/2402.05275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05275]] Exploring Hierarchical Classification Performance for Time Series Data:  Dissimilarity Measures and Classifier Comparisons(https://arxiv.org/abs/2402.05275)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The comparative performance of hierarchical classification (HC) and flat classification (FC) methodologies in the realm of time series data analysis is investigated in this study. Dissimilarity measures, including Jensen-Shannon Distance (JSD), Task Similarity Distance (TSD), and Classifier Based Distance (CBD), are leveraged alongside various classifiers such as MINIROCKET, STSF, and SVM. A subset of datasets from the UCR archive, focusing on multi-class cases comprising more than two classes, is employed for analysis. A significant trend is observed wherein HC demonstrates significant superiority over FC when paired with MINIROCKET utilizing TSD, diverging from conventional understandings. Conversely, FC exhibits consistent dominance across all configurations when employing alternative classifiers such as STSF and SVM. Moreover, TSD is found to consistently outperform both CBD and JSD across nearly all scenarios, except in instances involving the STSF classifier where CBD showcases superior performance. This discrepancy underscores the nuanced nature of dissimilarity measures and emphasizes the importance of their tailored selection based on the dataset and classifier employed. Valuable insights into the dynamic interplay between classification methodologies and dissimilarity measures in the realm of time series data analysis are provided by these findings. By elucidating the performance variations across different configurations, a foundation is laid for refining classification methodologies and dissimilarity measures to optimize performance in diverse analytical scenarios. Furthermore, the need for continued research aimed at elucidating the underlying mechanisms driving classification performance in time series data analysis is underscored, with implications for enhancing predictive modeling and decision-making in various domains.</li>
<li><strong>摘要：</strong>本研究研究了层次分类（HC）和平面分类（FC）方法在时间序列数据分析领域的性能比较。相异性度量，包括 Jensen-Shannon 距离 (JSD)、任务相似性距离 (TSD) 和基于分类器的距离 (CBD)，与 MINIROCKET、STSF 和 SVM 等各种分类器一起使用。采用 UCR 档案中的数据集子集进行分析，重点关注包含两个以上类别的多类案例。观察到一个显着趋势，其中当与利用 TSD 的 MINIROCKET 配对时，HC 表现出比 FC 显着的优越性，这与传统的理解不同。相反，当使用 STSF 和 SVM 等替代分类器时，FC 在所有配置中都表现出一致的优势。此外，我们发现 TSD 在几乎所有场景中都始终优于 CBD 和 JSD，除了在涉及 STSF 分类器的情况下 CBD 表现出卓越的性能。这种差异强调了差异性度量的细微差别，并强调了基于数据集和所使用的分类器进行定制选择的重要性。这些发现为时间序列数据分析领域中分类方法和相异性度量之间的动态相互作用提供了宝贵的见解。通过阐明不同配置之间的性能差异，为完善分类方法和相异性度量奠定了基础，以优化不同分析场景中的性能。此外，还强调需要继续研究，以阐明时间序列数据分析中驱动分类性能的潜在机制，这对增强各个领域的预测建模和决策具有影响。</li>
</ul>

<h3>Title: TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing</h3>
<ul>
<li><strong>Authors: </strong>Ran Zmigrod, Zhiqiang Ma, Armineh Nourbakhsh, Sameena Shah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05282">https://arxiv.org/abs/2402.05282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05282">https://arxiv.org/pdf/2402.05282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05282]] TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing(https://arxiv.org/abs/2402.05282)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Visually Rich Form Understanding (VRFU) poses a complex research problem due to the documents' highly structured nature and yet highly variable style and content. Current annotation schemes decompose form understanding and omit key hierarchical structure, making development and evaluation of end-to-end models difficult. In this paper, we propose a novel F1 metric to evaluate form parsers and describe a new content-agnostic, tree-based annotation scheme for VRFU: TreeForm. We provide methods to convert previous annotation schemes into TreeForm structures and evaluate TreeForm predictions using a modified version of the normalized tree-edit distance. We present initial baselines for our end-to-end performance metric and the TreeForm edit distance, averaged over the FUNSD and XFUND datasets, of 61.5 and 26.4 respectively. We hope that TreeForm encourages deeper research in annotating, modeling, and evaluating the complexities of form-like documents.</li>
<li><strong>摘要：</strong>由于文档的高度结构化性质以及高度可变的风格和内容，视觉丰富形式理解（VRFU）提出了一个复杂的研究问题。当前的注释方案分解了理解并省略了关键的层次结构，使得端到端模型的开发和评估变得困难。在本文中，我们提出了一种新颖的 F1 度量来评估表单解析器，并描述了一种新的与内容无关、基于树的 VRFU 注释方案：TreeForm。我们提供了将以前的注释方案转换为 TreeForm 结构的方法，并使用标准化树编辑距离的修改版本来评估 TreeForm 预测。我们提出了端到端性能指标和 TreeForm 编辑距离的初始基线，在 FUNSD 和 XFUND 数据集上的平均值分别为 61.5 和 26.4。我们希望 TreeForm 鼓励在注释、建模和评估类表单文档的复杂性方面进行更深入的研究。</li>
</ul>

<h3>Title: Classifying spam emails using agglomerative hierarchical clustering and  a topic-based approach</h3>
<ul>
<li><strong>Authors: </strong>F. Janez-Martino, R. Alaiz-Rodriguez, V. Gonzalez-Castro, E. Fidalgo, E. Alegre</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05296">https://arxiv.org/abs/2402.05296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05296">https://arxiv.org/pdf/2402.05296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05296]] Classifying spam emails using agglomerative hierarchical clustering and  a topic-based approach(https://arxiv.org/abs/2402.05296)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Spam emails are unsolicited, annoying and sometimes harmful messages which may contain malware, phishing or hoaxes. Unlike most studies that address the design of efficient anti-spam filters, we approach the spam email problem from a different and novel perspective. Focusing on the needs of cybersecurity units, we follow a topic-based approach for addressing the classification of spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S, two novel datasets with approximately 15K emails each in English and Spanish, respectively, and we label them using agglomerative hierarchical clustering into 11 classes. We evaluate 16 pipelines, combining four text representation techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words, Word2Vec and BERT- and four classifiers: Support Vector Machine, N\"aive Bayes, Random Forest and Logistic Regression. Experimental results show that the highest performance is achieved with TF-IDF and LR for the English dataset, with a F1 score of 0.953 and an accuracy of 94.6%, and while for the Spanish dataset, TF-IDF with NB yields a F1 score of 0.945 and 98.5% accuracy. Regarding the processing time, TF-IDF with LR leads to the fastest classification, processing an English and Spanish spam email in and on average, respectively.</li>
<li><strong>摘要：</strong>垃圾邮件是未经请求的、烦人的、有时甚至是有害的消息，可能包含恶意软件、网络钓鱼或恶作剧。与大多数解决高效反垃圾邮件过滤器设计的研究不同，我们从不同且新颖的角度来处理垃圾邮件问题。着眼于网络安全部门的需求，我们采用基于主题的方法将垃圾邮件分类为多个类别。我们提出了 SPEMC-15K-E 和 SPEMC-15K-S，这两个新颖的数据集分别包含大约 15K 份英语和西班牙语电子邮件，并使用凝聚层次聚类将它们标记为 11 个类别。我们评估了 16 个管道，结合了四种文本表示技术——词频-逆文档频率 (TF-IDF)、词袋、Word2Vec 和 BERT——以及四种分类器：支持向量机、N\"aive Bayes、随机森林和逻辑回归实验结果表明，对于英语数据集，使用 TF-IDF 和 LR 获得了最高的性能，F1 得分为 0.953，准确率达到 94.6%，而对于西班牙语数据集，TF-IDF 结合 NB 的 F1 得分准确率分别为 0.945 和 98.5%。就处理时间而言，带有 LR 的 TF-IDF 分类速度最快，分别处理英语和西班牙语垃圾邮件。</li>
</ul>

<h3>Title: Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Yuan Tian, Wenqi Zhou, Hao Dong, David S. Kammer, Olga Fink</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05306">https://arxiv.org/abs/2402.05306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05306">https://arxiv.org/pdf/2402.05306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05306]] Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making(https://arxiv.org/abs/2402.05306)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data. While existing transformer-based models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability. Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression. This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships. Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based model that redefines symbolic regression as a sequential decision-making task. Sym-Q leverages supervised demonstrations and refines expressions based on reward signals indicating the quality of fitting precision. Its distinctive ability to manage the complexity of expression trees and perform precise step-wise updates significantly enhances flexibility and efficiency. Our results demonstrate that Sym-Q excels not only in recovering underlying mathematical structures but also uniquely learns to efficiently refine the output expression based on reward signals, thereby discovering underlying expressions. Sym-Q paves the way for more intuitive and impactful discoveries in physical science, marking a substantial advancement in the field of symbolic regression.</li>
<li><strong>摘要：</strong>符号回归在从经验数据中揭示潜在的数学和物理关系方面具有巨大的潜力。虽然现有的基于变压器的模型最近在该领域取得了巨大的成功，但它们在通用性和适应性方面面临着挑战。通常，在输出表达式与实验数据不充分匹配的情况下，模型缺乏有效的机制来适应或修改表达式。这种不灵活性阻碍了它们在现实场景中的应用，特别是在发现未知的物理或生物关系方面。受到人类专家如何完善和调整表达式的启发，我们引入了符号 Q 网络 (Sym-Q)，这是一种基于强化学习的新型模型，它将符号回归重新定义为顺序决策任务。 Sym-Q 利用监督演示，并根据指示拟合精度质量的奖励信号完善表达式。其管理表达式树的复杂性和执行精确的逐步更新的独特能力显着提高了灵活性和效率。我们的结果表明，Sym-Q 不仅在恢复底层数学结构方面表现出色，而且还独特地学习根据奖励信号有效地细化输出表达式，从而发现底层表达式。 Sym-Q 为物理科学中更直观、更有影响力的发现铺平了道路，标志着符号回归领域的重大进步。</li>
</ul>

<h3>Title: KIX: A Metacognitive Generalization Framework</h3>
<ul>
<li><strong>Authors: </strong>Arun Kumar, Paul Schrater</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05346">https://arxiv.org/abs/2402.05346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05346">https://arxiv.org/pdf/2402.05346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05346]] KIX: A Metacognitive Generalization Framework(https://arxiv.org/abs/2402.05346)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.</li>
<li><strong>摘要：</strong>人类和其他动物在解决各种任务时恰当地表现出一般智力行为，通过重用和应用随着时间的推移获得的高水平知识，具有灵活性和适应新情况的能力。但人工代理更像是专家，缺乏这种通才行为。人工代理将需要理解和利用关键的结构化知识表示。我们提出了一个元认知泛化框架——知识交互执行（KIX），并认为利用类型空间与对象交互有助于学习可迁移的交互概念和泛化。这是将知识融入强化学习的自然方式，并有望成为人工智能系统中自主和通才行为的推动者。</li>
</ul>

<h3>Title: Exploring Learning Complexity for Downstream Data Pruning</h3>
<ul>
<li><strong>Authors: </strong>Wenyu Jiang, Zhenlong Liu, Zejian Xie, Songxin Zhang, Bingyi Jing, Hongxin Wei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05356">https://arxiv.org/abs/2402.05356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05356">https://arxiv.org/pdf/2402.05356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05356]] Exploring Learning Complexity for Downstream Data Pruning(https://arxiv.org/abs/2402.05356)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>The over-parameterized pre-trained models pose a great challenge to fine-tuning with limited computation resources. An intuitive solution is to prune the less informative samples from the fine-tuning dataset. A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the pruning cost becomes non-negligible due to the heavy parameter updating. For efficient pruning, it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free. However, we empirically show that such adaption distorts the original pruning and results in inferior performance on the downstream tasks. In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks. Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model. Then we preserve the diverse and easy samples for fine-tuning. Extensive experiments with vision datasets demonstrate the effectiveness and efficiency of the proposed scoring function for classification tasks. For the instruction fine-tuning of large language models, our method achieves state-of-the-art performance with stable convergence, outperforming the full training with only 10\% of the instruction dataset.</li>
<li><strong>摘要：</strong>过度参数化的预训练模型对有限计算资源的微调提出了巨大的挑战。一个直观的解决方案是从微调数据集中删除信息量较少的样本。提出了一系列基于训练的评分函数来量化数据子集的信息量，但由于大量的参数更新，剪枝成本变得不可忽略。为了高效剪枝，可以将基于几何的方法的相似性评分函数从基于训练调整为免训练。然而，我们的经验表明，这种适应扭曲了原始剪枝并导致下游任务的性能较差。在本文中，我们建议将学习复杂度（LC）视为分类和回归任务的评分函数。具体来说，学习复杂度被定义为具有不同容量的子网的平均预测置信度，它将数据处理封装在一个融合模型中。然后我们保留多样化且简单的样本进行微调。对视觉数据集的大量实验证明了所提出的分类任务评分函数的有效性和效率。对于大型语言模型的指令微调，我们的方法通过稳定的收敛实现了最先进的性能，仅用 10% 的指令数据集就优于完整训练。</li>
</ul>

<h3>Title: Guiding Large Language Models with Divide-and-Conquer Program for  Discerning Problem Solving</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05359">https://arxiv.org/abs/2402.05359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05359">https://arxiv.org/pdf/2402.05359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05359]] Guiding Large Language Models with Divide-and-Conquer Program for  Discerning Problem Solving(https://arxiv.org/abs/2402.05359)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achieve better performance than typical prompting strategies in tasks bothered by intermediate errors and deceptive contents, such as large integer multiplication, hallucination detection and misinformation detection.</li>
<li><strong>摘要：</strong>基础模型，例如大语言模型（LLM），由于其大量的应用而引起了人们的极大兴趣。现有的作品表明，适当的提示设计，例如思想链，可以释放LLM在不同领域的强大能力。然而，在处理涉及重复性子任务和/或欺骗性内容的任务时，例如算术计算和文章级假新闻检测，现有的提示策略要么表现力不足，要么因幻觉而引发中间错误。为了使LLM更能识别此类中间错误，我们建议用分而治之的程序来指导LLM，该程序同时确保卓越的表达能力并解开任务分解、子任务解析和解析组装过程。理论分析表明，我们的策略可以指导LLM扩展固定深度Transformer的表达能力。实验表明，在受中间错误和欺骗性内容困扰的任务中，例如大整数乘法、幻觉检测和错误信息检测，我们提出的方法可以比典型的提示策略取得更好的性能。</li>
</ul>

<h3>Title: Noise Contrastive Alignment of Language Models with Explicit Rewards</h3>
<ul>
<li><strong>Authors: </strong>Huayu Chen, Guande He, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05369">https://arxiv.org/abs/2402.05369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05369">https://arxiv.org/pdf/2402.05369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05369]] Noise Contrastive Alignment of Language Models with Explicit Rewards(https://arxiv.org/abs/2402.05369)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, rag</a></li>
<li><strong>Abstract: </strong>User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction, while NCA optimizes absolute likelihood for each response. We apply our methods to align a 7B language model with a GPT-4 annotated reward dataset. Experimental results suggest that InfoNCA surpasses the DPO baseline in GPT-4 evaluations, while NCA enjoys better training stability with competitive performance.</li>
<li><strong>摘要：</strong>用户意图通常被形式化为在微调语言模型（LM）时最大化的评估奖励。现有的对齐方法，例如直接偏好优化（DPO），主要是针对成对偏好数据量身定制的，其中奖励是隐式定义的，而不是显式给出的。在本文中，我们介绍了 LM 对齐的通用框架，利用噪声对比估计 (NCE) 来弥合处理用标量评估显式注释的奖励数据集方面的差距。我们的框架包含两种并行算法：NCA 和 InfoNCA，两者都能够从奖励数据和偏好数据中直接提取 LM 策略。值得注意的是，我们表明 DPO 损失是我们提出的 InfoNCA 目标在成对偏好设置下的一个特例，从而整合和扩展了当前的对齐理论。通过对比 NCA 和 InfoNCA，我们发现 InfoNCA 和 DPO 调整对单个指令的不同响应的相对可能性，而 NCA 优化每个响应的绝对可能性。我们应用我们的方法将 7B 语言模型与 GPT-4 带注释的奖励数据集对齐。实验结果表明，InfoNCA 在 GPT-4 评估中超越了 DPO 基线，而 NCA 具有更好的训练稳定性和竞技表现。</li>
</ul>

<h3>Title: Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms  in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Feihu Jin, Yifan Liu, Ying Tan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05376">https://arxiv.org/abs/2402.05376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05376">https://arxiv.org/pdf/2402.05376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05376]] Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms  in Large Language Models(https://arxiv.org/abs/2402.05376)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of our proposed method compared to current zero-shot CoT prompting methods on GPT-3.5-turbo and GPT-4. Moreover, in-depth analytical experiments underscore the adaptability and effectiveness of our method in various reasoning tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在不同的任务中表现出了卓越的性能，并通过应用零样本思维链 (CoT) 提示展现了令人印象深刻的推理能力。然而，由于预训练阶段句子前缀不断变化的性质，在所有任务实例上采用相同 CoT 提示的现有零样本 CoT 提示方法可能不是最佳的。在本文中，我们介绍了一种新颖的零样本提示方法，该方法利用进化算法为法学硕士动态生成多样化的提示。我们的方法包括初始化两个 CoT 提示，基于 LLM 执行进化操作以创建不同的集合，并利用 LLM 为给定问题选择合适的 CoT 提示。此外，在选定的 CoT 提示的指导下进行重写操作，可以增强法学硕士对问题的理解。在 10 个推理数据集上进行的广泛实验证明，与 GPT-3.5-turbo 和 GPT-4 上当前的零样本 CoT 提示方法相比，我们提出的方法具有优越的性能。此外，深入的分析实验强调了我们的方法在各种推理任务中的适应性和有效性。</li>
</ul>

<h3>Title: Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, Jiaqi Li, Xiaoze Liu, Jeff Z. Pan, Ningyu Zhang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CV, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05391">https://arxiv.org/abs/2402.05391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05391">https://arxiv.org/pdf/2402.05391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05391]] Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey(https://arxiv.org/abs/2402.05391)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora</a></li>
<li><strong>Abstract: </strong>Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss current challenges and identify emerging trends, such as progress in Large Language Modeling and Multi-modal Pre-training strategies. This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into KG and multi-modal learning research, offering insights into the evolving landscape of MMKG research and supporting future work.</li>
<li><strong>摘要：</strong>知识图谱 (KG) 在推进各种人工智能应用方面发挥着关键作用，语义网络社区对多模态维度的探索开启了新的创新途径。在本次调查中，我们仔细回顾了 300 多篇文章，重点关注两个主要方面的知识图谱感知研究：知识图谱驱动的多模态（KG4MM）学习，其中知识图谱支持多模态任务，以及多模态知识图谱（MM4KG） ，它将 KG 研究扩展到 MMKG 领域。我们首先定义 KG 和 MMKG，然后探讨它们的构建进度。我们的综述包括两个主要任务类别：KG 感知的多模态学习任务，例如图像分类和视觉问答，以及内在的 MMKG 任务，例如多模态知识图补全和实体对齐，突出了具体的研究轨迹。对于大多数此类任务，我们提供了定义、评估基准，并另外概述了进行相关研究的基本见解。最后，我们讨论当前的挑战并确定新兴趋势，例如大语言建模和多模式预训练策略的进展。本调查旨在为已经参与或考虑深入 KG 和多模态学习研究的研究人员提供全面的参考，为 MMKG 研究的不断发展提供见解并支持未来的工作。</li>
</ul>

<h3>Title: TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph  Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Gangda Deng, Hongkuan Zhou, Hanqing Zeng, Yinglong Xia, Christopher Leung, Jianbo Li, Rajgopal Kannan, Viktor Prasanna</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05396">https://arxiv.org/abs/2402.05396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05396">https://arxiv.org/pdf/2402.05396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05396]] TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph  Representation Learning(https://arxiv.org/abs/2402.05396)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and scalability. TASER adapts its mini-batch selection based on training dynamics and temporal neighbor selection based on the contextual, structural, and temporal properties of past interactions. To alleviate the bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal neighbor finder and a dedicated GPU feature cache. We evaluate the performance of TASER using two state-of-the-art backbone TGNNs. On five popular datasets, TASER outperforms the corresponding baselines by an average of 2.3% in Mean Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training time.</li>
<li><strong>摘要：</strong>最近，时态图神经网络（TGNN）在各种高影响力的应用程序中展示了最先进的性能，包括欺诈检测和内容推荐。尽管 TGNN 取得了成功，但它们很容易受到现实世界动态图中普遍存在的噪声的影响，例如过时的链接和倾斜的交互分布。噪声会导致两个严重影响 TGNN 准确性的关键问题：(1) 模型由较差的交互进行监督，(2) 噪声输入会导致聚合消息的高方差。然而，当前的TGNN去噪技术没有考虑每个节点的多样化和动态噪声模式。此外，它们还遭受因遍历更多邻居而导致的过多小批量生成开销。我们相信快速准确的 TGNN 的补救措施在于时间自适应采样。在这项工作中，我们提出了 TASER，这是第一个针对 TGNN 进行精度、效率和可扩展性优化的自适应采样方法。 TASER 根据训练动态调整其小批量选择，并根据过去交互的上下文、结构和时间属性调整其时间邻居选择。为了缓解小批量生成的瓶颈，TASER 实现了纯基于 GPU 的时间邻居查找器和专用 GPU 特征缓存。我们使用两个最先进的骨干 TGNN 来评估 TASER 的性能。在五个流行数据集上，TASER 的平均倒数排名 (MRR) 优于相应基线 2.3%，同时训练时间平均加速 5.1 倍。</li>
</ul>

<h3>Title: Optimizing for ROC Curves on Class-Imbalanced Data by Training over a  Family of Loss Functions</h3>
<ul>
<li><strong>Authors: </strong>Kelsey Lieberman, Shuai Yuan, Swarna Kamlam Ravindran, Carlo Tomasi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05400">https://arxiv.org/abs/2402.05400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05400">https://arxiv.org/pdf/2402.05400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05400]] Optimizing for ROC Curves on Class-Imbalanced Data by Training over a  Family of Loss Functions(https://arxiv.org/abs/2402.05400)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our method improves model performance and is more robust to hyperparameter choices. Code will be made available at: https://github.com/klieberman/roc_lct.</li>
<li><strong>摘要：</strong>尽管二元分类是计算机视觉中一个经过充分研究的问题，但在严重的类别不平衡下训练可靠的分类器仍然是一个具有挑战性的问题。最近的工作提出了通过修改损失函数或优化方法来减轻不平衡情况下训练的影响的技术。虽然这项工作显着提高了多类情况下的整体准确性，但我们观察到，这些方法的超参数值的微小变化可能会导致二进制问题的接收器操作特征（ROC）曲线方面的性能高度可变。严重失衡。为了降低对超参数选择的敏感性并训练更通用的模型，我们建议对一系列损失函数进行训练，而不是单个损失函数。我们开发了一种将损失条件训练（LCT）应用于不平衡分类问题的方法。在 CIFAR 和 Kaggle 竞赛数据集上的大量实验结果表明，我们的方法提高了模型性能，并且对超参数选择更加鲁棒。代码将在以下位置提供：https://github.com/klieberman/roc_lct。</li>
</ul>

<h3>Title: In-Context Principle Learning from Mistakes</h3>
<ul>
<li><strong>Authors: </strong>Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, Uri Alon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05403">https://arxiv.org/abs/2402.05403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05403">https://arxiv.org/pdf/2402.05403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05403]] In-Context Principle Learning from Mistakes(https://arxiv.org/abs/2402.05403)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific "principles" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings.</li>
<li><strong>摘要：</strong>上下文学习（ICL，也称为少样本提示）已成为通过从一些输入输出示例中学习来使法学硕士适应下游任务的标准方法。尽管如此，所有基于 ICL 的方法都只能从正确的输入输出对中学习。在本文中，我们通过从少数给定的输入输出示例中了解更多信息，重新审视这一范式。我们引入学习原则（LEAP）：首先，我们故意诱导模型在这几个例子上犯错误；然后我们反思这些错误，并从中学习明确的特定任务“原则”，这有助于解决类似问题并避免常见错误；最后，我们提示模型使用原始的少数样本和这些学到的一般原则来回答看不见的测试问题。我们在广泛的基准上评估 LEAP，包括多跳问答 (Hotpot QA)、文本 QA (DROP)、Big-Bench 硬推理和数学问题（GSM8K 和 MATH）；在所有这些基准测试中，LEAP 改进了最强的可用 LLM，例如 GPT-3.5-turbo、GPT-4、GPT-4 Turbo 和 Claude-2.1。例如，LEAP 在 DROP 中比使用 GPT-4 的标准小样本提示提高了 7.5%，在 HotpotQA 中提高了 3.3%。重要的是，LEAP 不需要比标准的几次提示设置更多的输入或示例。</li>
</ul>

<h3>Title: Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</h3>
<ul>
<li><strong>Authors: </strong>Lucio Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, Ameet Talwalkar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05406">https://arxiv.org/abs/2402.05406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05406">https://arxiv.org/pdf/2402.05406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05406]] Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes(https://arxiv.org/abs/2402.05406)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, rag</a></li>
<li><strong>Abstract: </strong>Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models. We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning methods requiring comparable resources as Bonsai. We also leverage Bonsai to produce a new sub-2B model using a single A6000 that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM leaderboard.</li>
<li><strong>摘要：</strong>鉴于非专业从业者和最受资助的机构之间可用硬件的代沟，随着法学硕士规模的扩大，它们变得越来越难以获得。虽然已经提出了许多方法来压缩 LLM 以使其资源消耗易于管理，但这些方法本身往往是资源密集型的，使它们超出了其目标用户组的范围。在这项工作中，我们探索仅使用前向传递的 LLM 的结构化修剪问题。我们寻求使从业者能够修剪如此大的模型，以便他们的可用硬件有足够的内存来运行推理。我们开发了 Bonsai，一种无梯度、扰动修剪方法，能够提供小型、快速且准确的修剪模型。我们观察到，Bonsai 输出的修剪模型（i）优于更昂贵的基于梯度的结构化修剪方法生成的模型，并且（ii）比需要相当资源的半结构化修剪方法生成的模型快两倍（具有相当的精度）盆栽。我们还利用 Bonsai 使用单个 A6000 生成新的 sub-2B 模型，该模型在 Huggingface Open LLM 排行榜上的 4/6 任务上具有最先进的性能。</li>
</ul>

<h3>Title: Version age-based client scheduling policy for federated learning</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Hu, Nikolaos Pappas, Howard H. Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05407">https://arxiv.org/abs/2402.05407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05407">https://arxiv.org/pdf/2402.05407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05407]] Version age-based client scheduling policy for federated learning(https://arxiv.org/abs/2402.05407)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has emerged as a privacy-preserving machine learning paradigm facilitating collaborative training across multiple clients without sharing local data. Despite advancements in edge device capabilities, communication bottlenecks present challenges in aggregating a large number of clients; only a portion of the clients can update their parameters upon each global aggregation. This phenomenon introduces the critical challenge of stragglers in FL and the profound impact of client scheduling policies on global model convergence and stability. Existing scheduling strategies address staleness but predominantly focus on either timeliness or content. Motivated by this, we introduce the novel concept of Version Age of Information (VAoI) to FL. Unlike traditional Age of Information metrics, VAoI considers both timeliness and content staleness. Each client's version age is updated discretely, indicating the freshness of information. VAoI is incorporated into the client scheduling policy to minimize the average VAoI, mitigating the impact of outdated local updates and enhancing the stability of FL systems.</li>
<li><strong>摘要：</strong>联邦学习 (FL) 已成为一种保护隐私的机器学习范例，可促进多个客户端之间的协作训练，而无需共享本地数据。尽管边缘设备功能取得了进步，但通信瓶颈在聚合大量客户端时带来了挑战；只有一部分客户端可以在每次全局聚合时更新其参数。这种现象引入了 FL 中落后者的严峻挑战，以及客户端调度策略对全局模型收敛和稳定性的深远影响。现有的调度策略可以解决陈旧问题，但主要关注及时性或内容。受此启发，我们将信息版本时代（VAoI）的新概念引入FL。与传统的信息时代指标不同，VAoI 同时考虑及时性和内容陈旧性。每个客户端的版本年龄都是离散更新的，表明信息的新鲜度。 VAoI被纳入客户端调度策略中，以最小化平均VAoI，减轻过时的本地更新的影响并增强FL系统的稳定性。</li>
</ul>

<h3>Title: DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement  and Imitation Learning</h3>
<ul>
<li><strong>Authors: </strong>Weikang Wan, Yufei Wang, Zackory Erickson, David Held</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05421">https://arxiv.org/abs/2402.05421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05421">https://arxiv.org/pdf/2402.05421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05421]] DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement  and Imitation Learning(https://arxiv.org/abs/2402.05421)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feed-forward policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15 model-based RL tasks and 13 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTOP outperforms prior state-of-the-art methods in both domains.</li>
<li><strong>摘要：</strong>本文介绍了 DiffTOP，它利用可微轨迹优化作为策略表示来生成深度强化和模仿学习的动作。轨迹优化是一种强大且广泛使用的控制算法，由成本和动态函数参数化。我们方法的关键是利用可微轨迹优化的最新进展，这使得能够计算相对于轨迹优化参数的损失梯度。因此，可以端到端地学习轨迹优化的成本和动态函数。 DiffTOP 解决了先前基于模型的 RL 算法的“目标不匹配”问题，因为 DiffTOP 中的动态模型通过轨迹优化过程区分策略梯度损失来学习直接最大化任务性能。我们通过高维感官观察进一步对标准机器人操作任务套件上的模仿学习进行 DiffTOP 基准测试，并将我们的方法与前馈策略类以及基于能量的模型 (EBM) 和扩散进行比较。在 15 个基于模型的 RL 任务和 13 个具有高维图像和点云输入的模仿学习任务中，DiffTOP 在这两个领域都优于先前最先进的方法。</li>
</ul>

<h3>Title: Neural Circuit Diagrams: Robust Diagrams for the Communication,  Implementation, and Analysis of Deep Learning Architectures</h3>
<ul>
<li><strong>Authors: </strong>Vincent Abbott</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05424">https://arxiv.org/abs/2402.05424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05424">https://arxiv.org/pdf/2402.05424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05424]] Neural Circuit Diagrams: Robust Diagrams for the Communication,  Implementation, and Analysis of Deep Learning Architectures(https://arxiv.org/abs/2402.05424)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation. In this work, I introduce neural circuit diagrams for an audience of machine learning researchers. After introducing neural circuit diagrams, I cover a host of architectures to show their utility and breed familiarity. This includes the transformer architecture, convolution (and its difficult-to-explain extensions), residual networks, the U-Net, and the vision transformer. I include a Jupyter notebook that provides evidence for the close correspondence between diagrams and code. Finally, I examine backpropagation using neural circuit diagrams. I show their utility in providing mathematical insight and analyzing algorithms' time and space complexities.</li>
<li><strong>摘要：</strong>图表很重要。不幸的是，深度学习社区没有绘制架构图的标准方法。当前线性代数符号和临时图的组合无法提供必要的精度来理解架构的所有细节。然而，这个细节对于忠实实施、数学分析、进一步创新和道德保证至关重要。我展示了神经回路图，这是一种专为满足深度学习架构通信需求而定制的图形语言。神经电路图自然地跟踪数据的变化排列，精确地显示操作如何在轴上广播，并显示线性操作的关键并行行为。现有图表方法的一个挥之不去的问题是无法同时表达轴的细节和数据的自由排列，而神经电路图可以解决这一问题。它们的组成结构类似于代码，在图表和实现之间建立了紧密的对应关系。在这项工作中，我向机器学习研究人员介绍了神经回路图。在介绍了神经电路图之后，我介绍了许多架构，以展示它们的实用性和品种熟悉度。这包括 Transformer 架构、卷积（及其难以解释的扩展）、残差网络、U-Net 和视觉 Transformer。我提供了一个 Jupyter 笔记本，它提供了图表和代码之间密切对应的证据。最后，我使用神经电路图检查反向传播。我展示了它们在提供数学见解和分析算法的时间和空间复杂性方面的实用性。</li>
</ul>

<h3>Title: A Sampling Theory Perspective on Activations for Implicit Neural  Representations</h3>
<ul>
<li><strong>Authors: </strong>Hemanth Saratchandran, Sameera Ramasinghe, Violetta Shevchenko, Alexander Long, Simon Lucey</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05427">https://arxiv.org/abs/2402.05427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05427">https://arxiv.org/pdf/2402.05427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05427]] A Sampling Theory Perspective on Activations for Implicit Neural  Representations(https://arxiv.org/abs/2402.05427)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, rag</a></li>
<li><strong>Abstract: </strong>Implicit Neural Representations (INRs) have gained popularity for encoding signals as compact, differentiable entities. While commonly using techniques like Fourier positional encodings or non-traditional activation functions (e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content, their properties lack exploration within a unified theoretical framework. Addressing this gap, we conduct a comprehensive analysis of these activations from a sampling theory perspective. Our investigation reveals that sinc activations, previously unused in conjunction with INRs, are theoretically optimal for signal encoding. Additionally, we establish a connection between dynamical systems and INRs, leveraging sampling theory to bridge these two paradigms.</li>
<li><strong>摘要：</strong>隐式神经表示（INR）因将信号编码为紧凑的、可微分的实体而受到欢迎。虽然通常使用傅立叶位置编码或非传统激活函数（例如高斯函数、正弦曲线或小波）等技术来捕获高频内容，但它们的特性缺乏在统一理论框架内的探索。为了解决这一差距，我们从采样理论的角度对这些激活进行了全面分析。我们的研究表明，之前与 INR 结合使用的 sinc 激活理论上对于信号编码来说是最佳的。此外，我们在动力系统和 INR 之间建立了联系，利用采样理论来弥合这两种范式。</li>
</ul>

<h3>Title: GPT-4 Generated Narratives of Life Events using a Structured Narrative  Prompt: A Validation Study</h3>
<ul>
<li><strong>Authors: </strong>Christopher J. Lynch, Erik Jensen, Madison H. Munro, Virginia Zamponi, Joseph Martinez, Kevin O'Brien, Brandon Feldhaus, Katherine Smith, Ann Marie Reinhold, Ross Gore</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05435">https://arxiv.org/abs/2402.05435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05435">https://arxiv.org/pdf/2402.05435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05435]] GPT-4 Generated Narratives of Life Events using a Structured Narrative  Prompt: A Validation Study(https://arxiv.org/abs/2402.05435)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, lora, prompt, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance the study of LLM capabilities, limitations, and validity but also offer practical insights for narrative generation and natural language processing applications.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在生成大量叙事方面发挥着关键作用，有助于系统地探索其以叙事形式传达生活事件的有效性。在本研究中，我们采用零样本结构化叙事提示，使用 OpenAI 的 GPT-4 生成 24,000 个叙事。从这个数据集中，我们手动对 2,880 个叙述进行分类，并评估它们在传达出生、死亡、雇用和解雇事件方面的有效性。值得注意的是，87.43%的叙述充分传达了结构化提示的意图。为了自动识别有效和无效叙述，我们在分类数据集上训练和验证了九个机器学习模型。利用这些模型，我们扩展了分析以预测其余 21,120 个叙述的分类。所有机器学习模型都擅长将有效叙述分类为有效，但在同时将无效叙述分类为无效方面遇到了挑战。我们的研究结果不仅推进了法学硕士能力、局限性和有效性的研究，而且还为叙事生成和自然语言处理应用提供了实用的见解。</li>
</ul>

<h3>Title: Learning Uncertainty-Aware Temporally-Extended Actions</h3>
<ul>
<li><strong>Authors: </strong>Joongkyu Lee, Seung Joon Park, Yunhao Tang, Min-hwan Oh</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05439">https://arxiv.org/abs/2402.05439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05439">https://arxiv.org/pdf/2402.05439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05439]] Learning Uncertainty-Aware Temporally-Extended Actions(https://arxiv.org/abs/2402.05439)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhancing policy learning efficiency.</li>
<li><strong>摘要：</strong>在强化学习中，动作空间中的时间抽象（以动作重复为例）是一种通过扩展动作促进策略学习的技术。然而，先前对动作重复的研究的一个主要限制是它可能会降低性能，特别是当重复次优动作时。这个问题常常否定了重复动作的优势。为了解决这个问题，我们提出了一种名为不确定性感知时间扩展（UTE）的新算法。 UTE 采用集成方法来准确测量动作扩展过程中的不确定性。这一功能允许政策根据其具体需求，在强调探索或采用规避不确定性的方法之间进行战略性选择。我们通过 Gridworld 和 Atari 2600 环境中的实验证明了 UTE 的有效性。我们的研究结果表明，UTE 优于现有的动作重复算法，有效减轻了其固有的局限性，并显着提高了策略学习效率。</li>
</ul>

<h3>Title: Improving Agent Interactions in Virtual Environments with Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Jack Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05440">https://arxiv.org/abs/2402.05440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05440">https://arxiv.org/pdf/2402.05440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05440]] Improving Agent Interactions in Virtual Environments with Language  Models(https://arxiv.org/abs/2402.05440)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Enhancing AI systems with efficient communication skills for effective human assistance necessitates proactive initiatives from the system side to discern specific circumstances and interact aptly. This research focuses on a collective building assignment in the Minecraft dataset, employing language modeling to enhance task understanding through state-of-the-art methods. These models focus on grounding multi-modal understanding and task-oriented dialogue comprehension tasks, providing insights into their interpretative and responsive capabilities. Our experimental results showcase a substantial improvement over existing methods, indicating a promising direction for future research in this domain.</li>
<li><strong>摘要：</strong>增强人工智能系统的高效沟通能力，以实现有效的人工协助，需要系统方面采取积极主动的行动，以识别特定情况并适当地进行交互。这项研究的重点是 Minecraft 数据集中的集体构建任务，利用语言建模通过最先进的方法来增强任务理解。这些模型侧重于奠定多模态理解和面向任务的对话理解任务的基础，提供对其解释和响应能力的见解。我们的实验结果展示了对现有方法的实质性改进，表明了该领域未来研究的有希望的方向。</li>
</ul>

<h3>Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention</h3>
<ul>
<li><strong>Authors: </strong>Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05445">https://arxiv.org/abs/2402.05445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05445">https://arxiv.org/pdf/2402.05445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05445]] Accurate LoRA-Finetuning Quantization of LLMs via Information Retention(https://arxiv.org/abs/2402.05445)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, lora, code</a></li>
<li><strong>Abstract: </strong>The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IRQLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains. The code is available at https://github.com/htqin/ir-qlora.</li>
<li><strong>摘要：</strong>LLM 的 LoRA 微调量化已得到广泛研究，以获得准确而紧凑的 LLM，以便部署在资源受限的硬件上。然而，现有的方法导致量化的LLM严重退化，甚至无法从LoRA的微调中受益。本文提出了一种新颖的 IR-QLoRA，用于通过信息保留来推动使用 LoRA 的量化 LLM 变得高度准确。所提出的IR-QLoRA主要依赖于从统一信息角度衍生出的两项技术：（1）基于统计的信息校准量化使得LLM的量化参数能够准确地保留原始信息； (2)基于微调的信息弹性连接使得LoRA利用多样化信息的弹性表示变换。综合实验表明，IR-QLoRA 可以在 2-4 位宽度下显着提高 LLaMA 和 LLaMA2 系列的精度，例如，与最先进的方法相比，4 位 LLaMA-7B 在 MMLU 上实现了 1.4% 的改进。显着的性能提升只需要 0.31% 的额外时间消耗，这表明我们的 IRQLoRA 的效率令人满意。我们强调，IR-QLoRA 具有出色的多功能性，与各种框架（例如 NormalFloat 和 Integer 量化）兼容，并带来了总体精度增益。代码可在 https://github.com/htqin/ir-qlora 获取。</li>
</ul>

<h3>Title: Large Language Models for Psycholinguistic Plausibility Pretesting</h3>
<ul>
<li><strong>Authors: </strong>Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05455">https://arxiv.org/abs/2402.05455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05455">https://arxiv.org/pdf/2402.05455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05455]] Large Language Models for Psycholinguistic Plausibility Pretesting(https://arxiv.org/abs/2402.05455)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.</li>
<li><strong>摘要：</strong>在心理语言学中，受控材料的创建对于确保研究结果仅归因于预期的操作而不受无关因素的影响至关重要。为了实现这一目标，心理语言学家通常会预先测试语言材料，其中常见的预先测试是征求人类评估者对特定句子的合理性判断。在这项工作中，我们研究是否可以使用语言模型（LM）来生成这些合理性判断。我们研究了跨多种语言结构的各种语言模型，并评估它们的合理性判断是否与人类判断相关。我们发现 GPT-4 的合理性判断与人类对我们检查的结构的判断高度相关，而其他 LM 与人类对常用句法结构的判断高度相关。然后我们测试这种相关性是否意味着可以使用 LM 代替人类进行预测试。我们发现，当需要粗粒度的合理性判断时，这种方法效果很好，但当需要细粒度的判断时，即使 GPT-4 也无法提供令人满意的判别力。</li>
</ul>

<h3>Title: It's Never Too Late: Fusing Acoustic Information into Large Language  Models for Automatic Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Chen Chen, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Ensiong Chng, Chao-Han Huck Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05457">https://arxiv.org/abs/2402.05457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05457">https://arxiv.org/pdf/2402.05457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05457]] It's Never Too Late: Fusing Acoustic Information into Large Language  Models for Automatic Speech Recognition(https://arxiv.org/abs/2402.05457)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the acoustic modality. Experimental evidence collected from various ASR tasks shows that UADF surpasses existing fusion mechanisms in several ways. It yields significant improvements in word error rate (WER) while mitigating data uncertainty issues in LLM and addressing the poor generalization relied with sole modality during fusion. We also demonstrate that UADF seamlessly adapts to audio-visual speech recognition.</li>
<li><strong>摘要：</strong>最近的研究成功表明，大型语言模型 (LLM) 可以成功用于自动语音识别 (ASR) 输出之上的生成错误纠正 (GER)。具体来说，LLM 用于执行从 ASR 系统生成的 N 最佳假设列表到预测输出转录的直接映射。然而，尽管 GER 有效，但由于 LLM 的训练没有考虑语音信号中可用的声学信息，因此 GER 引入了额外的数据不确定性。在这项工作中，我们的目标是通过一种称为不确定性感知动态融合（UADF）的新型后期融合解决方案，在生成预测转录之前注入声学信息，从而克服这种限制。 UADF 是一种在自回归解码过程中实现的多模态融合方法，分两个阶段工作：(i) 首先分析和校准令牌级 LLM 决策，(ii) 然后动态吸收来自声学模态的信息。从各种 ASR 任务收集的实验证据表明，UADF 在多个方面超越了现有的融合机制。它显着提高了字错误率 (WER)，同时减轻了法学硕士中的数据不确定性问题，并解决了融合过程中单一模态所依赖的泛化不良问题。我们还证明 UADF 可以无缝适应视听语音识别。</li>
</ul>

<h3>Title: Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation  and Echopraxia</h3>
<ul>
<li><strong>Authors: </strong>Guangyu Shen, Siyuan Cheng, Kaiyuan Zhang, Guanhong Tao, Shengwei An, Lu Yan, Zhuo Zhang, Shiqing Ma, Xiangyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05467">https://arxiv.org/abs/2402.05467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05467">https://arxiv.org/pdf/2402.05467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05467]] Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation  and Echopraxia(https://arxiv.org/abs/2402.05467)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become prevalent across diverse sectors, transforming human life with their extraordinary reasoning and comprehension abilities. As they find increased use in sensitive tasks, safety concerns have gained widespread attention. Extensive efforts have been dedicated to aligning LLMs with human moral principles to ensure their safe deployment. Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle. In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious Exploitation and Echopraxia), a novel optimization-based method inspired by two psychological concepts: subconsciousness and echopraxia, which describe the processes of the mind that occur without conscious awareness and the involuntary mimicry of actions, respectively. Evaluations across 6 open-source LLMs and 4 commercial LLM APIs show RIPPLE achieves an average Attack Success Rate of 91.5\%, outperforming five current methods by up to 47.0\% with an 8x reduction in overhead. Furthermore, it displays significant transferability and stealth, successfully evading established detection mechanisms. The code of our work is available at \url{https://github.com/SolidShen/RIPPLE_official/tree/official}</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已在各个领域盛行，以其非凡的推理和理解能力改变了人类的生活。随着它们在敏感任务中的使用越来越多，安全问题受到了广泛关注。我们付出了广泛的努力，使法学硕士符合人类道德原则，以确保其安全部署。尽管有潜力，但最近的研究表明，一致的法学硕士很容易受到专门的越狱提示，绕过安全措施，引发暴力和有害内容。当代法学硕士固有的离散性和巨大的规模对自动生成多样化、高效和有效的越狱提示提出了重大挑战，构成了持续的障碍。在本文中，我们介绍了 RIPPLE（通过潜意识利用和回声行为进行快速优化），这是一种基于优化的新颖方法，其灵感来自两个心理学概念：潜意识和回声行为，它们描述了在无意识的情况下发生的思维过程以及不自觉的模仿。分别采取行动。对 6 个开源 LLM 和 4 个商业 LLM API 的评估显示，RIPPLE 的平均攻击成功率为 91.5%，比当前的五种方法高出 47.0%，开销减少了 8 倍。此外，它还表现出显着的可转移性和隐秘性，成功地逃避了既定的检测机制。我们的工作代码可以在 \url{https://github.com/SolidShen/RIPPLE_official/tree/official} 获取</li>
</ul>

<h3>Title: Implicit Diffusion: Efficient Optimization through Stochastic Sampling</h3>
<ul>
<li><strong>Authors: </strong>Pierre Marion, Anna Korba, Peter Bartlett, Mathieu Blondel, Valentin De Bortoli, Arnaud Doucet, Felipe Llinares-López, Courtney Paquette, Quentin Berthet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05468">https://arxiv.org/abs/2402.05468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05468">https://arxiv.org/pdf/2402.05468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05468]] Implicit Diffusion: Efficient Optimization through Stochastic Sampling(https://arxiv.org/abs/2402.05468)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions. Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters. We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness in real-world settings.</li>
<li><strong>摘要：</strong>我们提出了一种新算法来优化由参数化随机扩散隐式定义的分布。这样做使我们能够通过优化参数来修改采样过程的结果分布。我们引入了这些过程的一阶优化的通用框架，该框架在单个循环中联合执行优化和采样步骤。这种方法的灵感来自双层优化和自动隐式微分的最新进展，利用采样的观点作为概率分布空间的优化。我们为我们的方法的性能提供了理论保证，并通过实验结果证明了其在现实环境中的有效性。</li>
</ul>

<h3>Title: Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy  Optimization</h3>
<ul>
<li><strong>Authors: </strong>Talha Bozkus, Urbashi Mitra</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05476">https://arxiv.org/abs/2402.05476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05476">https://arxiv.org/pdf/2402.05476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05476]] Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy  Optimization(https://arxiv.org/abs/2402.05476)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) is a classical tool to solve network control or policy optimization problems in unknown environments. The original Q-learning suffers from performance and complexity challenges across very large networks. Herein, a novel model-free ensemble reinforcement learning algorithm which adapts the classical Q-learning is proposed to handle these challenges for networks which admit Markov decision process (MDP) models. Multiple Q-learning algorithms are run on multiple, distinct, synthetically created and structurally related Markovian environments in parallel; the outputs are fused using an adaptive weighting mechanism based on the Jensen-Shannon divergence (JSD) to obtain an approximately optimal policy with low complexity. The theoretical justification of the algorithm, including the convergence of key statistics and Q-functions are provided. Numerical results across several network models show that the proposed algorithm can achieve up to 55% less average policy error with up to 50% less runtime complexity than the state-of-the-art Q-learning algorithms. Numerical results validate assumptions made in the theoretical analysis.</li>
<li><strong>摘要：</strong>强化学习（RL）是解决未知环境中的网络控制或策略优化问题的经典工具。原始的 Q 学习在非常大的网络中面临着性能和复杂性的挑战。在此，提出了一种采用经典 Q 学习的新型无模型集成强化学习算法，以应对接纳马尔可夫决策过程（MDP）模型的网络的这些挑战。多个 Q 学习算法在多个不同的、综合创建的且结构相关的马尔可夫环境中并行运行；使用基于詹森-香农散度（JSD）的自适应加权机制融合输出，以获得低复杂度的近似最优策略。提供了该算法的理论论证，包括关键统计数据和 Q 函数的收敛。多个网络模型的数值结果表明，与最先进的 Q 学习算法相比，所提出的算法可以将平均策略误差降低多达 55%，运行时复杂度降低多达 50%。数值结果验证了理论分析中的假设。</li>
</ul>

<h3>Title: Determining the severity of Parkinson's disease in patients using a  multi task neural network</h3>
<ul>
<li><strong>Authors: </strong>María Teresa García-Ordás, José Alberto Benítez-Andrades, Jose Aveleira-Mata, José-Manuel Alija-Pérez, Carmen Benavides</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05491">https://arxiv.org/abs/2402.05491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05491">https://arxiv.org/pdf/2402.05491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05491]] Determining the severity of Parkinson's disease in patients using a  multi task neural network(https://arxiv.org/abs/2402.05491)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Parkinson's disease is easy to diagnose when it is advanced, but it is very difficult to diagnose in its early stages. Early diagnosis is essential to be able to treat the symptoms. It impacts on daily activities and reduces the quality of life of both the patients and their families and it is also the second most prevalent neurodegenerative disorder after Alzheimer in people over the age of 60. Most current studies on the prediction of Parkinson's severity are carried out in advanced stages of the disease. In this work, the study analyzes a set of variables that can be easily extracted from voice analysis, making it a very non-intrusive technique. In this paper, a method based on different deep learning techniques is proposed with two purposes. On the one hand, to find out if a person has severe or non-severe Parkinson's disease, and on the other hand, to determine by means of regression techniques the degree of evolution of the disease in a given patient. The UPDRS (Unified Parkinson's Disease Rating Scale) has been used by taking into account both the motor and total labels, and the best results have been obtained using a mixed multi-layer perceptron (MLP) that classifies and regresses at the same time and the most important features of the data obtained are taken as input, using an autoencoder. A success rate of 99.15% has been achieved in the problem of predicting whether a person suffers from severe Parkinson's disease or non-severe Parkinson's disease. In the degree of disease involvement prediction problem case, a MSE (Mean Squared Error) of 0.15 has been obtained. Using a full deep learning pipeline for data preprocessing and classification has proven to be very promising in the field Parkinson's outperforming the state-of-the-art proposals.</li>
<li><strong>摘要：</strong>帕金森病晚期时很容易诊断，但早期诊断却非常困难。早期诊断对于治疗症状至关重要。它影响日常活动并降低患者及其家人的生活质量，也是 60 岁以上人群中继阿尔茨海默病之后第二大常见的神经退行性疾病。目前大多数关于预测帕金森病严重程度的研究都是在进行处于疾病晚期。在这项工作中，该研究分析了一组可以轻松从语音分析中提取的变量，使其成为一种非常非侵入性的技术。本文提出了一种基于不同深度学习技术的方法，其目的有两个。一方面，查明一个人是否患有严重或非严重帕金森病，另一方面，通过回归技术确定特定患者的疾病演变程度。 UPDRS（统一帕金森病评定量表）在考虑运动和总标签的情况下使用，并且使用同时分类和回归的混合多层感知器（MLP）获得了最佳结果。使用自动编码器将获得的数据的最重要特征作为输入。在预测一个人患有重度帕金森病还是非重度帕金森病的问题上，成功率达到了99.15%。在疾病受累程度预测问题案例中，得到了0.15的MSE（均方误差）。事实证明，使用完整的深度学习管道进行数据预处理和分类在帕金森病领域非常有前途，优于最先进的建议。</li>
</ul>

<h3>Title: GPTs Are Multilingual Annotators for Sequence Generation Tasks</h3>
<ul>
<li><strong>Authors: </strong>Juhwan Choi, Eunju Lee, Kyohoon Jin, YoungBin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05512">https://arxiv.org/abs/2402.05512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05512">https://arxiv.org/pdf/2402.05512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05512]] GPTs Are Multilingual Annotators for Sequence Generation Tasks(https://arxiv.org/abs/2402.05512)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, code</a></li>
<li><strong>Abstract: </strong>Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.</li>
<li><strong>摘要：</strong>数据注释是构建新数据集的重要步骤。然而，通过众包进行数据注释的传统方法既耗时又昂贵。此外，由于众包者语言库的差异，在处理资源匮乏的语言时，该过程的复杂性会增加。为了解决这些问题，本研究提出了一种利用大型语言模型的自主注释方法，该方法最近已被证明表现出卓越的性能。通过我们的实验，我们证明所提出的方法不仅具有成本效益，而且适用于低资源语言注释。此外，我们使用我们的方法构建了一个图像字幕数据集，并致力于开放该数据集以供未来研究。我们已经开放了源代码以供进一步研究和重现。</li>
</ul>

<h3>Title: NoisyICL: A Little Noise in Model Parameters Calibrates In-context  Learning</h3>
<ul>
<li><strong>Authors: </strong>Yufeng Zhao, Yoshihiro Sakai, Naoya Inoue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05515">https://arxiv.org/abs/2402.05515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05515">https://arxiv.org/pdf/2402.05515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05515]] NoisyICL: A Little Noise in Model Parameters Calibrates In-context  Learning(https://arxiv.org/abs/2402.05515)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github.</li>
<li><strong>摘要：</strong>由于较高的先验偏差和不忠实的信心​​，情境学习（ICL）的性能不令人满意且校准不足。之前的一些工作对语言模型进行了微调，以在巨大的数据集和计算成本下获得更好的 ICL 性能。在本文中，我们提出了 NoisyICL，通过随机噪声简单地扰动模型参数，以争取更好的性能和校准。我们对 2 个模型和 12 个下游数据集的实验表明，NoisyICL 可以帮助 ICL 产生更准确的预测。我们的进一步分析表明，NoisyICL 使模型能够提供更公平的预测，并且不忠实的置信度也更少。因此，我们认为NoisyICL是ICL的有效校准。我们的实验代码已上传到Github。</li>
</ul>

<h3>Title: Differentially Private Model-Based Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Alexandre Rio, Merwan Barlier, Igor Colin, Albert Thomas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05525">https://arxiv.org/abs/2402.05525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05525">https://arxiv.org/pdf/2402.05525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05525]] Differentially Private Model-Based Offline Reinforcement Learning(https://arxiv.org/abs/2402.05525)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.</li>
<li><strong>摘要：</strong>我们通过隐私保证来解决离线强化学习问题，其目标是训练针对数据集中的各个轨迹具有差异隐私的策略。为了实现这一目标，我们引入了 DP-MORL，这是一种具有差分隐私保证的 MBRL 算法。首先使用 DP-FedAvg 从离线数据中学习环境的私有模型，DP-FedAvg 是一种神经网络训练方法，可在轨迹级别提供差分隐私保证。然后，我们使用基于模型的策略优化从（受惩罚的）私有模型导出策略，而无需与系统进行任何进一步的交互或访问输入数据。我们凭经验证明 DP-MORL 能够利用离线数据训练私人 RL 代理，并且我们进一步概述了这种情况下隐私的代价。</li>
</ul>

<h3>Title: Asynchronous Diffusion Learning with Agent Subsampling and Local Updates</h3>
<ul>
<li><strong>Authors: </strong>Elsa Rizk, Kun Yuan, Ali H. Sayed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05529">https://arxiv.org/abs/2402.05529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05529">https://arxiv.org/pdf/2402.05529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05529]] Asynchronous Diffusion Learning with Agent Subsampling and Local Updates(https://arxiv.org/abs/2402.05529)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>In this work, we examine a network of agents operating asynchronously, aiming to discover an ideal global model that suits individual local datasets. Our assumption is that each agent independently chooses when to participate throughout the algorithm and the specific subset of its neighbourhood with which it will cooperate at any given moment. When an agent chooses to take part, it undergoes multiple local updates before conveying its outcomes to the sub-sampled neighbourhood. Under this setup, we prove that the resulting asynchronous diffusion strategy is stable in the mean-square error sense and provide performance guarantees specifically for the federated learning setting. We illustrate the findings with numerical simulations.</li>
<li><strong>摘要：</strong>在这项工作中，我们检查了异步运行的代理网络，旨在发现适合各个本地数据集的理想全局模型。我们的假设是，每个智能体独立选择何时参与整个算法以及在任何给定时刻与其合作的邻域的特定子集。当智能体选择参与时，它会经历多次本地更新，然后将其结果传递给二次采样的邻域。在此设置下，我们证明所得到的异步扩散策略在均方误差意义上是稳定的，并专门为联邦学习设置提供性能保证。我们通过数值模拟来说明研究结果。</li>
</ul>

<h3>Title: Reinforcement Learning as a Catalyst for Robust and Fair Federated  Learning: Deciphering the Dynamics of Client Contributions</h3>
<ul>
<li><strong>Authors: </strong>Jialuo He, Wei Chen, Xiaojin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05541">https://arxiv.org/abs/2402.05541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05541">https://arxiv.org/pdf/2402.05541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05541]] Reinforcement Learning as a Catalyst for Robust and Fair Federated  Learning: Deciphering the Dynamics of Client Contributions(https://arxiv.org/abs/2402.05541)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness. Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for continuous control of aggregation weights, an innovative client selection method based on model parameter distances, and a reward mechanism guided by validation set performance. Empirically, extensive experiments demonstrate that, in terms of robustness, RFL outperforms the state-of-the-art methods, while maintaining comparable levels of fairness, offering a promising solution to build resilient and fair federated systems.</li>
<li><strong>摘要：</strong>联邦学习 (FL) 的最新进展产生了通过跨多个分散设备或保存本地数据样本的系统进行训练来保留用户隐私的模型。然而，这些策略往往忽视了统计异质性和对抗性攻击脆弱性的固有挑战，这可能会降低模型的稳健性和公平性。个性化 FL 策略通过调整模型以适应各个客户端配置文件来提供一些喘息机会，但它们往往会忽略服务器端聚合漏洞。为了解决这些问题，我们提出了强化联合学习（RFL），这是一种新颖的框架，利用深度强化学习在聚合过程中自适应地优化客户端贡献，从而增强模型针对恶意客户端的鲁棒性以及参与者在不同分布设置下的公平性。为了实现这一目标，我们提出了一种细致的方法，包括用于连续控制聚合权重的基于深度确定性策略梯度的算法、基于模型参数距离的创新客户端选择方法以及由验证集性能引导的奖励机制。根据经验，大量实验表明，就鲁棒性而言，RFL 优于最先进的方法，同时保持相当的公平性水平，为构建弹性和公平的联邦系统提供了一种有前景的解决方案。</li>
</ul>

<h3>Title: Named Entity Recognition for Address Extraction in Speech-to-Text  Transcriptions Using Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Bibiána Lajčinová, Patrik Valábek, Michal Spišiak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05545">https://arxiv.org/abs/2402.05545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05545">https://arxiv.org/pdf/2402.05545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05545]] Named Entity Recognition for Address Extraction in Speech-to-Text  Transcriptions Using Synthetic Data(https://arxiv.org/abs/2402.05545)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, code</a></li>
<li><strong>Abstract: </strong>This paper introduces an approach for building a Named Entity Recognition (NER) model built upon a Bidirectional Encoder Representations from Transformers (BERT) architecture, specifically utilizing the SlovakBERT model. This NER model extracts address parts from data acquired from speech-to-text transcriptions. Due to scarcity of real data, a synthetic dataset using GPT API was generated. The importance of mimicking spoken language variability in this artificial data is emphasized. The performance of our NER model, trained solely on synthetic data, is evaluated using small real test dataset.</li>
<li><strong>摘要：</strong>本文介绍了一种基于 Transformers 双向编码器表示 (BERT) 架构构建命名实体识别 (NER) 模型的方法，特别是利用 SlovakBERT 模型。该 NER 模型从语音到文本转录获取的数据中提取地址部分。由于真实数据的稀缺，使用 GPT API 生成了一个合成数据集。强调了在人工数据中模仿口语变化的重要性。我们的 NER 模型仅在合成数据上进行训练，其性能是使用小型真实测试数据集进行评估的。</li>
</ul>

<h3>Title: Benchmarking Large Language Models on Communicative Medical Coaching: a  Novel System and Dataset</h3>
<ul>
<li><strong>Authors: </strong>Hengguan Huang, Songtao Wang, Hongfu Liu, Hao Wang, Ye Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05547">https://arxiv.org/abs/2402.05547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05547">https://arxiv.org/pdf/2402.05547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05547]] Benchmarking Large Language Models on Communicative Medical Coaching: a  Novel System and Dataset(https://arxiv.org/abs/2402.05547)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess their effectiveness in communicative medical coaching tasks. Our comparative analysis demonstrates that instruction-tuned Llama2 significantly outperforms ChatGPT's prompting-based approaches.</li>
<li><strong>摘要：</strong>自然语言处理 (NLP) 在医疗保健中的传统应用主要集中于以患者为中心的服务，增强患者互动和护理服务，例如通过医疗对话系统。然而，NLP 使缺乏经验的医生受益的潜力，特别是在交流医疗辅导等领域，在很大程度上仍未得到探索。我们引入了“ChatCoach”，一个集成的人类与人工智能合作框架。在此框架内，患者代理人和辅导代理人共同支持医学学习者在咨询期间练习医疗沟通技巧。与传统的对话系统不同，ChatCoach 提供了一个模拟环境，人类医生可以在其中与患者代理进行医疗对话。同时，教练代理向医生提供实时反馈。为了构建 ChatCoach 系统，我们开发了一个数据集并集成了 ChatGPT 和 Llama2 等大型语言模型，旨在评估它们在交流医疗辅导任务中的有效性。我们的比较分析表明，指令调整的 Llama2 显着优于 ChatGPT 基于提示的方法。</li>
</ul>

<h3>Title: Flashback: Understanding and Mitigating Forgetting in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Aljahdali, Ahmed M. Abdelmoniem, Marco Canini, Samuel Horváth</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05558">https://arxiv.org/abs/2402.05558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05558">https://arxiv.org/pdf/2402.05558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05558]] Flashback: Understanding and Mitigating Forgetting in Federated Learning(https://arxiv.org/abs/2402.05558)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.</li>
<li><strong>摘要：</strong>在联邦学习（FL）中，遗忘或跨轮知识丢失会阻碍算法收敛，特别是在客户端之间存在严重数据异质性的情况下。本研究探讨了这个问题的细微差别，强调了遗忘在异构数据环境中 FL 低效学习中的关键作用。知识丢失发生在客户端本地更新和服务器端聚合步骤中；只顾其中之一并不能减少遗忘。我们引入了一种衡量遗忘的指标，以确保在新知识获取过程中的清晰识别。利用这些见解，我们提出了 Flashback，这是一种采用动态蒸馏方法的 FL 算法，用于规范局部模型，并有效地聚合它们的知识。在不同的基准测试中，Flashback 优于其他方法，通过 6 到 16 轮收敛，减少了遗忘，并实现了更快的轮到目标准确度。</li>
</ul>

<h3>Title: Traditional Machine Learning Models and Bidirectional Encoder  Representations From Transformer (BERT)-Based Automatic Classification of  Tweets About Eating Disorders: Algorithm Development and Validation Study</h3>
<ul>
<li><strong>Authors: </strong>José Alberto Benítez-Andrades, José-Manuel Alija-Pérez, Maria-Esther Vidal, Rafael Pastor-Vargas, María Teresa García-Ordás</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05571">https://arxiv.org/abs/2402.05571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05571">https://arxiv.org/pdf/2402.05571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05571]] Traditional Machine Learning Models and Bidirectional Encoder  Representations From Transformer (BERT)-Based Automatic Classification of  Tweets About Eating Disorders: Algorithm Development and Validation Study(https://arxiv.org/abs/2402.05571)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Background: Eating disorders are increasingly prevalent, and social networks offer valuable information. Objective: Our goal was to identify efficient machine learning models for categorizing tweets related to eating disorders. Methods: Over three months, we collected tweets about eating disorders. A 2,000-tweet subset was labeled for: (1) being written by individuals with eating disorders, (2) promoting eating disorders, (3) informativeness, and (4) scientific content. Both traditional machine learning and deep learning models were employed for classification, assessing accuracy, F1 score, and computational time. Results: From 1,058,957 collected tweets, transformer-based bidirectional encoder representations achieved the highest F1 scores (71.1%-86.4%) across all four categories. Conclusions: Transformer-based models outperform traditional techniques in classifying eating disorder-related tweets, though they require more computational resources.</li>
<li><strong>摘要：</strong>背景：饮食失调越来越普遍，社交网络提供了有价值的信息。目标：我们的目标是确定有效的机器学习模型，用于对与饮食失调相关的推文进行分类。方法：在三个月的时间里，我们收集了有关饮食失调的推文。包含 2,000 条推文的子集被标记为：(1) 由饮食失调患者撰写，(2) 宣传饮食失调，(3) 信息性，以及 (4) 科学内容。传统的机器学习和深度学习模型都用于分类、评估准确性、F1 分数和计算时间。结果：从收集的 1,058,957 条推文中，基于 Transformer 的双向编码器表示在所有四个类别中获得了最高的 F1 分数 (71.1%-86.4%)。结论：基于 Transformer 的模型在对饮食失调相关推文进行分类方面优于传统技术，尽管它们需要更多的计算资源。</li>
</ul>

<h3>Title: Establishing degrees of closeness between audio recordings along  different dimensions using large-scale cross-lingual models</h3>
<ul>
<li><strong>Authors: </strong>Maxime Fily, Guillaume Wisniewski, Severine Guillaume, Gilles Adda, Alexis Michaud</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05581">https://arxiv.org/abs/2402.05581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05581">https://arxiv.org/pdf/2402.05581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05581]] Establishing degrees of closeness between audio recordings along  different dimensions using large-scale cross-lingual models(https://arxiv.org/abs/2402.05581)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>In the highly constrained context of low-resource language studies, we explore vector representations of speech from a pretrained model to determine their level of abstraction with regard to the audio signal. We propose a new unsupervised method using ABX tests on audio recordings with carefully curated metadata to shed light on the type of information present in the representations. ABX tests determine whether the representations computed by a multilingual speech model encode a given characteristic. Three experiments are devised: one on room acoustics aspects, one on linguistic genre, and one on phonetic aspects. The results confirm that the representations extracted from recordings with different linguistic/extra-linguistic characteristics differ along the same lines. Embedding more audio signal in one vector better discriminates extra-linguistic characteristics, whereas shorter snippets are better to distinguish segmental information. The method is fully unsupervised, potentially opening new research avenues for comparative work on under-documented languages.</li>
<li><strong>摘要：</strong>在资源匮乏的语言研究的高度受限的背景下，我们探索预训练模型中语音的向量表示，以确定它们对音频信号的抽象级别。我们提出了一种新的无监督方法，对录音进行 ABX 测试，并使用精心策划的元数据来揭示表示中存在的信息类型。 ABX 测试确定多语言语音模型计算的表示是否编码给定的特征。设计了三项实验：一项关于室内声学方面，一项关于语言类型，一项关于语音方面。结果证实，从具有不同语言/语言外特征的录音中提取的表征也有相同的差异。在一个向量中嵌入更多的音频信号可以更好地区分语言外特征，而较短的片段可以更好地区分分段信息。该方法完全不受监督，可能为记录不足的语言的比较工作开辟新的研究途径。</li>
</ul>

<h3>Title: AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods  in Low-resource Regimes</h3>
<ul>
<li><strong>Authors: </strong>Juhwan Choi, Kyohoon Jin, Junho Lee, Sangmin Song, Youngbin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05584">https://arxiv.org/abs/2402.05584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05584">https://arxiv.org/pdf/2402.05584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05584]] AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods  in Low-resource Regimes(https://arxiv.org/abs/2402.05584)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Text data augmentation is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pre-trained language models. We offer the source code.</li>
<li><strong>摘要：</strong>由于句子的离散性质，文本数据增强是一个复杂的问题。尽管基于规则的增强方法因其简单性而在现实应用中被广泛采用，但它们存在潜在的语义损坏。之前的研究人员建议使用软标签（softEDA）轻松进行数据增强，并采用标签平滑来缓解这个问题。然而，为每个模型和数据集找到最佳因子具有挑战性；因此，在实际应用中使用softEDA仍然很困难。在本文中，我们建议采用 AutoAugment 来解决这个问题。实验结果表明，所提出的方法可以增强现有的增强方法，并且基于规则的方法可以增强尖端的预训练语言模型。我们提供源代码。</li>
</ul>

<h3>Title: SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels</h3>
<ul>
<li><strong>Authors: </strong>Juhwan Choi, Kyohoon Jin, Junho Lee, Sangmin Song, Youngbin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05591">https://arxiv.org/abs/2402.05591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05591">https://arxiv.org/pdf/2402.05591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05591]] SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels(https://arxiv.org/abs/2402.05591)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Rule-based text data augmentation is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data. We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility.</li>
<li><strong>摘要：</strong>基于规则的文本数据增强由于其简单性而被广泛用于 NLP 任务。然而，这种方法可能会损害文本的原始含义，最终损害模型的性能。为了克服这一限制，我们提出了一种将软标签应用于增强数据的简单技术。我们对七个不同的分类任务进行了实验，并凭经验证明了我们提出的方法的有效性。我们已公开开放源代码以实现可重复性。</li>
</ul>

<h3>Title: AttnLRP: Attention-Aware Layer-wise Relevance Propagation for  Transformers</h3>
<ul>
<li><strong>Authors: </strong>Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05602">https://arxiv.org/abs/2402.05602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05602">https://arxiv.org/pdf/2402.05602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05602]] AttnLRP: Attention-Aware Layer-wise Relevance Propagation for  Transformers(https://arxiv.org/abs/2402.05602)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an open-source implementation on GitHub https://github.com/rachtibat/LRP-for-Transformers.</li>
<li><strong>摘要：</strong>大型语言模型很容易出现有偏差的预测和幻觉，这凸显了理解其模型内部推理过程的重要性。然而，实现整个黑盒变压器模型的忠实归因并保持计算效率是一个尚未解决的挑战。通过扩展分层相关性传播归因方法来处理注意力层，我们有效地解决了这些挑战。虽然存在部分解决方案，但我们的方法是第一个不仅忠实且整体地归因于变压器模型的输入和潜在表示的方法，其计算效率类似于奇异向后传递。通过对 Llama 2、Flan-T5 和 Vision Transformer 架构的现有方法进行广泛评估，我们证明我们提出的方法在忠实度方面超越了替代方法，并且能够理解潜在表示，为基于概念的解释打开了大门。我们在 GitHub https://github.com/rachtibat/LRP-for-Transformers 上提供了一个开源实现。</li>
</ul>

<h3>Title: Optimizing Delegation in Collaborative Human-AI Hybrid Teams</h3>
<ul>
<li><strong>Authors: </strong>Andrew Fuchs, Andrea Passarella, Marco Conti</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05605">https://arxiv.org/abs/2402.05605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05605">https://arxiv.org/pdf/2402.05605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05605]] Optimizing Delegation in Collaborative Human-AI Hybrid Teams(https://arxiv.org/abs/2402.05605)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To ensure minimal added complexity or potential inefficiency for the team, the manager should attempt to minimize the number of times the team reaches a constraint violation and requires subsequent manager intervention. Therefore our manager is optimizing its selection of authorized agents to boost overall team performance while minimizing the frequency of manager intervention. We demonstrate our manager performance in a simulated driving scenario representing the case of a hybrid team of agents composed of a human driver and autonomous driving system. We perform experiments for our driving scenario with interfering vehicles, indicating the need for collision avoidance and proper speed control. Our results indicate a positive impact of our manager, with some cases resulting in increased team performance up to ~187% that of the best solo agent performance.</li>
<li><strong>摘要：</strong>当人类和自主系统作为我们所说的混合团队一起运作时，我们当然希望确保团队成功有效地运作。我们将团队成员称为代理。在我们提出的框架中，我们解决了混合团队的情况，在这种情况下，在任何时候，只有一名团队成员（控制代理）被授权充当团队的控制者。为了确定控制代理的最佳选择，我们建议添加一个人工智能管理器（通过强化学习），它作为团队的外部观察者进行学习。管理者学习一个行为模型，将代理绩效的观察与团队所处的环境/世界联系起来，并根据这些观察做出最理想的控制代理选择。我们通过引入一组约束来限制经理的任务。经理约束表明团队操作是可接受的，因此如果团队进入不可接受的情况并需要经理干预，就会发生违规。为了确保团队的复杂性增加或潜在的低效率最小化，经理应尝试尽量减少团队违反约束并需要后续经理干预的次数。因此，我们的经理正在优化授权代理的选择，以提高团队的整体绩效，同时最大限度地减少经理干预的频率。我们在模拟驾驶场景中展示了经理的表现，该场景代表了由人类驾驶员和自动驾驶系统组成的混合代理团队的情况。我们针对干扰车辆的驾驶场景进行了实验，表明需要避免碰撞和适当的速度控制。我们的结果表明我们的经理产生了积极的影响，在某些情况下，团队绩效提高了约 187%，达到最佳单独代理绩效的 187%。</li>
</ul>

<h3>Title: Pretrained Generative Language Models as General Learning Frameworks for  Sequence-Based Tasks</h3>
<ul>
<li><strong>Authors: </strong>Ben Fauber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05616">https://arxiv.org/abs/2402.05616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05616">https://arxiv.org/pdf/2402.05616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05616]] Pretrained Generative Language Models as General Learning Frameworks for  Sequence-Based Tasks(https://arxiv.org/abs/2402.05616)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks. Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch. Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing. We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks. We also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success.</li>
<li><strong>摘要：</strong>我们建议，具有数百万个参数的小型预训练基础生成语言模型可以用作基于序列的任务的通用学习框架。我们的建议克服了与从头开始训练神经网络和语言模型相关的计算资源、技能集和时间线挑战。此外，我们的方法侧重于创建小型且高度专业化的模型，这些模型可以准确地执行基本模型无法执行的具有挑战性的任务。我们证明了 125M、350M 和 1.3B 参数预训练的基础语言模型可以通过 10,000 到 1,000,000 个指令示例进行指令微调，以在具有挑战性的化学信息学任务中获得接近最先进的结果。我们还展示了连续语言模型微调时期对改善结果的作用，以及数据格式化和预训练基础语言模型选择对于指令微调成功的重要性。</li>
</ul>

<h3>Title: Efficient Models for the Detection of Hate, Abuse and Profanity</h3>
<ul>
<li><strong>Authors: </strong>Christoph Tillmann, Aashka Trivedi, Bishwaranjan Bhattacharjee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05624">https://arxiv.org/abs/2402.05624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05624">https://arxiv.org/pdf/2402.05624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05624]] Efficient Models for the Detection of Hate, Abuse and Profanity(https://arxiv.org/abs/2402.05624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are the cornerstone for many Natural Language Processing (NLP) tasks like sentiment analysis, document classification, named entity recognition, question answering, summarization, etc. LLMs are often trained on data which originates from the web. This data is prone to having content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP, please refer to the Appendix. Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content. For example, when the open-source RoBERTa model (specifically, the RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted to replace the mask token in `I do not know that Persian people are that MASK` it returns the word `stupid` with the highest score. This is unacceptable in civil discourse.The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased LLMs, which is needed not only for English, but for all languages. In this article, we briefly describe the creation of HAP detectors and various ways of using them to make models civil and acceptable in the output they generate.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 是许多自然语言处理 (NLP) 任务的基石，例如情感分析、文档分类、命名实体识别、问答、摘要等。LLM 通常接受来自网络的数据训练。这些数据很容易包含仇恨、滥用和亵渎 (HAP) 的内容。 HAP的详细定义请参见附录。由于法学硕士在训练期间接触 HAP 内容，模型会学习这些内容，然后可能会生成仇恨或亵渎的内容。例如，当 HuggingFace (HF) Transformers 库中的开源 RoBERTa 模型（具体来说，RoBERTA 基本模型）被提示替换“我不知道波斯人是那个 MASK”中的 mask 标记时，它会返回单词得分最高的“愚蠢”。这在公民话语中是不可接受的。检测文本中的仇恨、辱骂和亵渎行为是创建公民和公正的法学硕士的重要组成部分，这不仅是英语所需要的，也是所有语言所需要的。在本文中，我们简要描述了 HAP 探测器的创建以及使用它们使模型生成的输出具有民用性和可接受性的各种方法。</li>
</ul>

<h3>Title: RepQuant: Towards Accurate Post-Training Quantization of Large  Transformer Models via Scale Reparameterization</h3>
<ul>
<li><strong>Authors: </strong>Zhikai Li, Xuewen Liu, Jing Zhang, Qingyi Gu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05628">https://arxiv.org/abs/2402.05628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05628">https://arxiv.org/pdf/2402.05628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05628]] RepQuant: Towards Accurate Post-Training Quantization of Large  Transformer Models via Scale Reparameterization(https://arxiv.org/abs/2402.05628)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Large transformer models have demonstrated remarkable success. Post-training quantization (PTQ), which requires only a small dataset for calibration and avoids end-to-end retraining, is a promising solution for compressing these large models. Regrettably, existing PTQ methods typically exhibit non-trivial performance loss. We find that the performance bottleneck stems from over-consideration of hardware compatibility in the quantization process, compelling them to reluctantly employ simple quantizers, albeit at the expense of accuracy. With the above insights, we propose RepQuant, a novel PTQ framework with quantization-inference decoupling paradigm to address the above issues. RepQuant employs complex quantizers in the quantization process and simplified quantizers in the inference process, and performs mathematically equivalent transformations between the two through quantization scale reparameterization, thus ensuring both accurate quantization and efficient inference. More specifically, we focus on two components with extreme distributions: LayerNorm activations and Softmax activations. Initially, we apply channel-wise quantization and log$\sqrt{2}$ quantization, respectively, which are tailored to their distributions. In particular, for the former, we introduce a learnable per-channel dual clipping scheme, which is designed to efficiently identify outliers in the unbalanced activations with fine granularity. Then, we reparameterize the scales to hardware-friendly layer-wise quantization and log2 quantization for inference. Moreover, quantized weight reconstruction is seamlessly integrated into the above procedure to further push the performance limits. Extensive experiments are performed on different large-scale transformer variants on multiple tasks, including vision, language, and multi-modal transformers, and RepQuant encouragingly demonstrates significant performance advantages.</li>
<li><strong>摘要：</strong>大型变压器模型已经取得了显着的成功。训练后量化（PTQ）只需要一个小数据集进行校准，并避免端到端的重新训练，是压缩这些大型模型的一种有前途的解决方案。遗憾的是，现有的 PTQ 方法通常会表现出不小的性能损失。我们发现性能瓶颈源于量化过程中对硬件兼容性的过度考虑，迫使他们不情愿地采用简单的量化器，尽管以牺牲准确性为代价。基于上述见解，我们提出了 RepQuant，一种具有量化推理解耦范式的新型 PTQ 框架来解决上述问题。 RepQuant在量化过程中采用复杂量化器，在推理过程中采用简化量化器，并通过量化尺度重参数化在两者之间进行数学上等效的变换，从而保证准确量化和高效推理。更具体地说，我们关注两个具有极端分布的组件：LayerNorm 激活和 Softmax 激活。最初，我们分别应用通道量化和 log$\sqrt{2}$ 量化，这是根据它们的分布定制的。特别是，对于前者，我们引入了一种可学习的每通道双裁剪方案，该方案旨在以细粒度有效地识别不平衡激活中的异常值。然后，我们将尺度重新参数化为硬件友好的逐层量化和 log2 量化以进行推理。此外，量化权重重建无缝集成到上述过程中，进一步突破性能极限。在多个任务上对不同的大规模 Transformer 变体进行了广泛的实验，包括视觉、语言和多模态 Transformer，RepQuant 令人鼓舞地展示了显着的性能优势。</li>
</ul>

<h3>Title: Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature  of Aggregated Factual Claims in Long-Form Generations</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Han Chiang, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05629">https://arxiv.org/abs/2402.05629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05629">https://arxiv.org/pdf/2402.05629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05629]] Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature  of Aggregated Factual Claims in Long-Form Generations(https://arxiv.org/abs/2402.05629)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifically designed for content with ambiguous entities. We evaluate the D-FActScores of people biographies generated with retrieval-augmented generation (RAG). We show that D-FActScore can better assess the factuality of paragraphs with entity ambiguity than FActScore. We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs.</li>
<li><strong>摘要：</strong>来自大型语言模型 (LLM) 的长格式生成包含事实和非事实声明的混合，使得评估事实性变得困难。为了以更细粒度的方式评估长形式生成的事实精度，先前的工作提出将长形式生成分解为多个可验证的事实并独立验证这些事实。生成的真实性是指可验证的事实占所有事实的比例。此类方法假设结合事实主张形成事实段落。本文表明，由于实体模糊性，该假设可能会被违反。我们表明，法学硕士可以生成包含可验证事实的段落，但由于实体模糊性，这些事实被组合起来形成非事实段落。我们进一步揭示，现有的事实精确度指标，包括 FActScore 和引文召回率，无法正确评估这些非事实段落的真实性。为了解决这个问题，我们引入了一种增强型指标 D-FActScore，专门针对具有不明确实体的内容而设计。我们评估使用检索增强生成 (RAG) 生成的人物传记的 D-FActScore。我们表明，D-FActScore 比 FActScore 可以更好地评估具有实体模糊性的段落的真实性。我们还发现，四个广泛使用的开源法学硕士倾向于混合不同实体的信息以形成非事实段落。</li>
</ul>

<h3>Title: Improving Token-Based World Models with Parallel Observation Prediction</h3>
<ul>
<li><strong>Authors: </strong>Lior Cohen, Kaixin Wang, Bingyi Kang, Shie Mannor</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05643">https://arxiv.org/abs/2402.05643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05643">https://arxiv.org/pdf/2402.05643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05643]] Improving Token-Based World Models with Parallel Observation Prediction(https://arxiv.org/abs/2402.05643)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, agent</a></li>
<li><strong>Abstract: </strong>Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours. Our code is available at \url{https://github.com/leor-c/REM}.</li>
<li><strong>摘要：</strong>受 Transformer 在应用于离散符号序列时取得的成功的推动，基于令牌的世界模型（TBWM）最近被提出作为样本有效的方法。在 TBWM 中，世界模型将代理体验视为类似语言的标记序列，其中每个观察都构成一个子序列。然而，在想象过程中，下一个观察结果的逐个标记的顺序生成会导致严重的瓶颈，导致训练时间长、GPU 利用率低和表示有限。为了解决这个瓶颈，我们设计了一种新颖的并行观察预测（POP）机制。 POP 通过针对我们的强化学习设置量身定制的新颖前向模式增强了保留网络 (RetNet)。我们将 POP 纳入一种名为 REM（保留环境模型）的新型 TBWM 代理中，与之前的 TBWM 相比，想象力快了 15.4 倍。 REM 在 Atari 100K 基准测试的 26 场游戏中的 12 场中取得了超人的表现，而训练时间不到 12 小时。我们的代码可在 \url{https://github.com/leor-c/REM} 获取。</li>
</ul>

<h3>Title: Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of  Average-Reward Restless Bandits</h3>
<ul>
<li><strong>Authors: </strong>Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05689">https://arxiv.org/abs/2402.05689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05689">https://arxiv.org/pdf/2402.05689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05689]] Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of  Average-Reward Restless Bandits(https://arxiv.org/abs/2402.05689)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We consider the infinite-horizon, average-reward restless bandit problem in discrete time. We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution. We show that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic. Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed simulation-based policy, which requires a Synchronization Assumption (SA).</li>
<li><strong>摘要：</strong>我们考虑离散时间内无限范围、平均奖励的不安分强盗问题。我们提出了一类新的政策，旨在推动越来越多的兵种实现最优分配。我们证明，只要单臂松弛问题是单链且非周期的，我们的策略对于 $N$ 武装问题来说是渐近最优的，其最优性差距为 $O(1/\sqrt{N})$ 。我们的方法不同于大多数现有的侧重于索引或优先级策略的工作，这些策略依赖于统一全局吸引子属性（UGAP）来保证收敛到最优，或者不同于最近开发的基于模拟的策略，这需要同步假设（SA） 。</li>
</ul>

<h3>Title: Self-Alignment of Large Language Models via Monopolylogue-based Social  Scene Simulation</h3>
<ul>
<li><strong>Authors: </strong>Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, Siheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05699">https://arxiv.org/abs/2402.05699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05699">https://arxiv.org/pdf/2402.05699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05699]] Self-Alignment of Large Language Models via Monopolylogue-based Social  Scene Simulation(https://arxiv.org/abs/2402.05699)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. Code is available at https://github.com/pangxianghe/MATRIX.</li>
<li><strong>摘要：</strong>使大语言模型 (LLM) 与人类价值观保持一致对于减轻其滥用所造成的潜在不利影响至关重要。借鉴社会学的见解，承认各方的关切是塑造人类价值观的关键因素，本文提出了一个让法学硕士自行调整的新方向：社会场景模拟。为了实现这一目标，我们推出了 MATRIX，一种新颖的社交场景模拟器，可以模拟用户输入查询周围的现实场景，使法学硕士能够在响应之前考虑社交后果。 MATRIX 充当虚拟排练空间，类似于独白，法学硕士在其中扮演与查询和练习相关的各种角色。为了注入这种对齐，我们使用 MATRIX 模拟数据对 LLM 进行微调，确保遵守人类价值观而不影响推理速度。我们从理论上证明，在温和的假设下，使用 MATRIX 的法学硕士优于宪法人工智能。最后，大量实验验证了我们的方法在 4 个基准测试中优于 10 多个基准。 875 个用户评分证明，我们调整后的 13B 大小的 LLM 在与人类价值观的一致性方面超过了 GPT-4。代码可在 https://github.com/pangxianghe/MATRIX 获取。</li>
</ul>

<h3>Title: Unified Speech-Text Pretraining for Spoken Dialog Modeling</h3>
<ul>
<li><strong>Authors: </strong>Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Jungwhan Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Sungroh Yoon, Kang Min Yoo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05706">https://arxiv.org/abs/2402.05706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05706">https://arxiv.org/pdf/2402.05706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05706]] Unified Speech-Text Pretraining for Spoken Dialog Modeling(https://arxiv.org/abs/2402.05706)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions. Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM. We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics. Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative studies reveal that, despite the cascaded approach being stronger in individual components, the joint speech-text modeling improves robustness against recognition errors and speech quality. Demo is available at https://unifiedsdm.github.io.</li>
<li><strong>摘要：</strong>虽然最近的工作在扩展大型语言模型 (LLM) 直接理解和合成语音的能力方面取得了可喜的成果，但基于 LLM 的口语对话建模策略仍然难以捉摸，需要进一步研究。这项工作提出了一个广泛的语音文本法学硕士框架，称为统一口语对话模型（USDM），以生成连贯的口语响应，具有与给定输入语音相关的有机韵律特征，而不依赖于自动语音识别（ASR）或文本到文本的转换。语音 (TTS) 解决方案。我们的方法采用多步骤语音文本推理方案，该方案利用底层法学硕士所展示的推理链能力。我们还提出了一种通用的语音文本预训练方案，有助于捕获跨模态语义。自动和人工评估表明，所提出的方法可以有效地生成听起来自然的语音响应，优于先前的基线和级联基线。详细的比较研究表明，尽管级联方法在各个组件中更强，但联合语音文本建模提高了针对识别错误和语音质量的鲁棒性。演示可在 https://unifiedsdm.github.io 获取。</li>
</ul>

<h3>Title: In-Context Learning Can Re-learn Forbidden Tasks</h3>
<ul>
<li><strong>Authors: </strong>Sophie Xhonneux, David Dobre, Jian Tang, Gauthier Gidel, Dhanya Sridhar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05723">https://arxiv.org/abs/2402.05723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05723">https://arxiv.org/pdf/2402.05723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05723]] In-Context Learning Can Re-learn Forbidden Tasks(https://arxiv.org/abs/2402.05723)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities. One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles. Finally, we investigate whether ICL can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL attack that uses the chat template tokens like a prompt injection attack to achieve a better attack success rate on Vicuna-7B and Starling-7B. Trigger Warning: the appendix contains LLM-generated text with violence, suicide, and misinformation.</li>
<li><strong>摘要：</strong>尽管在安全培训方面投入了大量资金，但在现实世界中部署的大型语言模型 (LLM) 仍然存在许多漏洞。 LLM 安全培训的一种观点是，它在算法上禁止模型回答有毒或有害的查询。为了评估安全培训的有效性，在这项工作中，我们研究了禁止的任务，即模型旨在拒绝回答的任务。具体来说，我们研究了上下文学习（ICL）是否可以用于重新学习禁止的任务，尽管对模型进行了明确的微调以拒绝它们。我们首先检查一个拒绝情感分类的玩具示例来演示该问题。然后，我们在经过微调的模型上使用 ICL，以拒绝总结虚构的新闻文章。最后，我们调查 ICL 是否可以撤销安全培训，这可能代表重大安全风险。对于安全任务，我们关注 Vicuna-7B、Starling-7B 和 Llama2-7B。我们证明该攻击在 Starling-7B 和 Vicuna-7B 上可以立即运行，但在 Llama2-7B 上失败。最后，我们提出了一种 ICL 攻击，它使用聊天模板令牌，如提示注入攻击，以在 Vicuna-7B 和 Starling-7B 上获得更好的攻击成功率。触发警告：附录包含法学硕士生成的包含暴力、自杀和错误信息的文本。</li>
</ul>

<h3>Title: Model-Based RL for Mean-Field Games is not Statistically Harder than  Single-Agent RL</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Huang, Niao He, Andreas Krause</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05724">https://arxiv.org/abs/2402.05724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05724">https://arxiv.org/pdf/2402.05724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05724]] Model-Based RL for Mean-Field Games is not Statistically Harder than  Single-Agent RL(https://arxiv.org/abs/2402.05724)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, generalizing from conventional MFGs and involving multiple types of agents. This extension implies statistical tractability of a broader class of Markov Games through the efficacy of mean-field approximation. Finally, inspired by our theoretical algorithm, we present a heuristic approach with improved computational efficiency and empirically demonstrate its effectiveness.</li>
<li><strong>摘要：</strong>我们使用基于模型的函数逼近来研究平均场博弈 (MFG) 中强化学习 (RL) 的样本复杂性，需要进行战略探索才能找到纳什均衡策略。我们引入了基于部分模型的 Eluder 维度 (P-MBED)，这是一种描述模型类复杂性的更有效的概念。值得注意的是，P-MBED 测量从给定平均场模型类转换而来的单智能体模型类的复杂性，并且可能比 \citet{huang2023statistical} 提出的 MBED 指数低。我们贡献了一种具有新颖探索策略的模型消除算法，并建立了样本复杂性结果多项式 w.r.t.~P-MBED。至关重要的是，我们的结果表明，在基本的可实现性和 Lipschitz 连续性假设下，\emph{在 MFG 中学习纳什均衡并不比解决对数数量的单智能体 RL 问题更具统计挑战性}。我们进一步将我们的结果扩展到多类型 MFG，从传统的 MFG 推广并涉及多种类型的代理。这种扩展意味着通过平均场近似的功效可以实现更广泛的马尔可夫游戏的统计易处理性。最后，受我们的理论算法的启发，我们提出了一种提高计算效率的启发式方法，并凭经验证明了其有效性。</li>
</ul>

<h3>Title: TimeArena: Shaping Efficient Multitasking Language Agents in a  Time-Aware Simulation</h3>
<ul>
<li><strong>Authors: </strong>Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, Jiangjie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05733">https://arxiv.org/abs/2402.05733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05733">https://arxiv.org/pdf/2402.05733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05733]] TimeArena: Shaping Efficient Multitasking Language Agents in a  Time-Aware Simulation(https://arxiv.org/abs/2402.05733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations do not adequately address the notion of time. To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios. In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time. We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment. TimeArena grounds to 30 real-world tasks in cooking, household activities, and laboratory work. We conduct extensive experiments with various state-of-the-art LLMs using TimeArena. Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents.</li>
<li><strong>摘要：</strong>尽管通过大型语言模型 (LLM) 模拟类人行为取得了显着进步，但当前的文本模拟并不能充分解决时间概念。为此，我们引入了 TimeArena，这是一种新颖的文本模拟环境，它结合了复杂的时间动态和约束，可以更好地反映现实生活中的规划场景。在 TimeArena 中，代理被要求尽快完成多个任务，从而允许并行处理以节省时间。我们实现了动作之间的依赖关系、每个动作的持续时间以及代理和环境中对象的占用情况。 TimeArena 涵盖 30 项现实世界的烹饪、家庭活动和实验室工作任务。我们使用 TimeArena 对各种最先进的法学硕士进行了广泛的实验。我们的研究结果表明，即使是最强大的模型，例如 GPT-4，在有效的多任务处理方面仍然落后于人类，这强调了在语言代理的开发中增强时间意识的必要性。</li>
</ul>

<h3>Title: Implicit Bias and Fast Convergence Rates for Self-attention</h3>
<ul>
<li><strong>Authors: </strong>Bhavya Vasudeva, Puneesh Deora, Christos Thrampoulidis</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05738">https://arxiv.org/abs/2402.05738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05738">https://arxiv.org/pdf/2402.05738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05738]] Implicit Bias and Fast Convergence Rates for Self-attention(https://arxiv.org/abs/2402.05738)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Self-attention, the core mechanism of transformers, distinguishes them from traditional neural networks and drives their outstanding performance. Towards developing the fundamental optimization principles of self-attention, we investigate the implicit bias of gradient descent (GD) in training a self-attention layer with fixed linear decoder in binary classification. Drawing inspiration from the study of GD in linear logistic regression over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $W_t$ converges locally (with respect to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our work enhances this result in four aspects. Firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape. Secondly, we provide the first finite-time convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of sparsification in the attention map. Thirdly, through an analysis of normalized GD and Polyak step-size, we demonstrate analytically that adaptive step-size rules can accelerate the convergence of self-attention. Additionally, we remove the restriction of prior work on a fixed linear decoder. Our results reinforce the implicit-bias perspective of self-attention and strengthen its connections to implicit-bias in linear logistic regression, despite the intricate non-convex nature of the former.</li>
<li><strong>摘要：</strong>自注意力是 Transformer 的核心机制，它区别于传统的神经网络，并驱动其表现出色。为了发展自注意力的基本优化原理，我们研究了二元分类中使用固定线性解码器训练自注意力层时梯度下降（GD）的隐式偏差。从可分离数据的线性逻辑回归中的 GD 研究中汲取灵感，最近的工作表明，随着迭代次数 $t$ 接近无穷大，键查询矩阵 $W_t$ 局部收敛（相对于初始化方向）硬裕度 SVM 解决方案 $W_{mm}$。我们的工作从四个方面增强了这一结果。首先，我们确定了可证明全局收敛的重要数据设置，从而揭示了优化前景。其次，我们提供 $W_t$ 到 $W_{mm}$ 的第一个有限时间收敛率，同时量化注意力图中的稀疏率。第三，通过对归一化GD和Polyak步长的分析，我们分析证明了自适应步长规则可以加速self-attention的收敛。此外，我们消除了先前工作对固定线性解码器的限制。我们的结果强化了自注意力的隐式偏差观点，并加强了其与线性逻辑回归中隐式偏差的联系，尽管前者具有复杂的非凸性质。</li>
</ul>

<h3>Title: SpiRit-LM: Interleaved Spoken and Written Language Model</h3>
<ul>
<li><strong>Authors: </strong>Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Gabriel Synnaeve, Juan Pino, Benoit Sagot, Emmanuel Dupoux</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05755">https://arxiv.org/abs/2402.05755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05755">https://arxiv.org/pdf/2402.05755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05755]] SpiRit-LM: Interleaved Spoken and Written Language Model(https://arxiv.org/abs/2402.05755)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>We introduce SPIRIT-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two versions: a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification).</li>
<li><strong>摘要：</strong>我们引入了 SPIRIT-LM，这是一种可以自由混合文本和语音的基础多模态语言模型。我们的模型基于预训练的文本语言模型，通过在文本和语音单元上持续训练它，我们将其扩展到语音模态。语音和文本序列连接为一组标记，并使用小型自动策划的语音文本并行语料库通过单词级交错方法进行训练。 SPIRIT-LM 有两个版本：使用语音语义单元的 BASE 版本和除了语义单元之外还使用音调和风格单元对表现力进行建模的 EXPRESSIVE 版本。对于这两个版本，文本均使用子字 BPE 标记进行编码。所得模型同时显示了文本模型的语义能力和语音模型的表达能力。此外，我们还证明了 SPIRIT-LM 能够以少量的方式跨模式（即 ASR、TTS、语音分类）学习新任务。</li>
</ul>

<h3>Title: Latent variable model for high-dimensional point process with structured  missingness</h3>
<ul>
<li><strong>Authors: </strong>Maksim Sinelnikov, Manuel Haussmann, Harri Lähdesmäki</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05758">https://arxiv.org/abs/2402.05758</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05758">https://arxiv.org/pdf/2402.05758</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05758]] Latent variable model for high-dimensional point process with structured  missingness(https://arxiv.org/abs/2402.05758)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process. While various solutions have been suggested, the majority of them have been designed to account for only one of these challenges. In this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations. Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process. We construct our model as a variational autoencoder together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised variational inference approach for efficient model training. We demonstrate competitive performance using both simulated and real datasets.</li>
<li><strong>摘要：</strong>纵向数据在医疗保健、社会学和地震学等许多领域都很重要，但现实世界的数据集给从业者带来了显着的挑战，因为它们可能是高维的，包含结构化缺失模式，并且测量时间点可以由未知的随机变量控制过程。尽管已经提出了各种解决方案，但其中大多数的设计目的只是解决这些挑战之一。在这项工作中，我们提出了一种灵活高效的潜变量模型，能够解决所有这些限制。我们的方法利用高斯过程来捕获样本及其相关缺失掩模之间的时间相关性，并对底层点过程进行建模。我们将我们的模型构建为变分自动编码器以及深度神经网络参数化编码器和解码器模型，并开发可扩展的摊销变分推理方法以进行高效的模型训练。我们使用模拟和真实数据集展示了竞争性能。</li>
</ul>

<h3>Title: Off-policy Distributional Q($λ$): Distributional RL without  Importance Sampling</h3>
<ul>
<li><strong>Authors: </strong>Yunhao Tang, Mark Rowland, Rémi Munos, Bernardo Ávila Pires, Will Dabney</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05766">https://arxiv.org/abs/2402.05766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05766">https://arxiv.org/pdf/2402.05766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05766]] Off-policy Distributional Q($λ$): Distributional RL without  Importance Sampling(https://arxiv.org/abs/2402.05766)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>We introduce off-policy distributional Q($\lambda$), a new addition to the family of off-policy distributional evaluation algorithms. Off-policy distributional Q($\lambda$) does not apply importance sampling for off-policy learning, which introduces intriguing interactions with signed measures. Such unique properties distributional Q($\lambda$) from other existing alternatives such as distributional Retrace. We characterize the algorithmic properties of distributional Q($\lambda$) and validate theoretical insights with tabular experiments. We show how distributional Q($\lambda$)-C51, a combination of Q($\lambda$) with the C51 agent, exhibits promising results on deep RL benchmarks.</li>
<li><strong>摘要：</strong>我们引入离策略分布 Q($\lambda$)，这是离策略分布评估算法系列的新成员。离策略分布 Q($\lambda$) 不对离策略学习应用重要性采样，这会引入与签名度量的有趣交互。这种独特的属性分布 Q($\lambda$) 来自其他现有的替代方案，例如分布 Retrace。我们描述了分布 Q($\lambda$) 的算法特性，并通过表格实验验证了理论见解。我们展示了分布式 Q($\lambda$)-C51（Q($\lambda$) 与 C51 代理的组合）如何在深度 RL 基准测试中展现出有希望的结果。</li>
</ul>

<h3>Title: Analysing the Sample Complexity of Opponent Shaping</h3>
<ul>
<li><strong>Authors: </strong>Kitty Fung, Qizhen Zhang, Chris Lu, Jia Wan, Timon Willi, Jakob Foerster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.GT, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05782">https://arxiv.org/abs/2402.05782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05782">https://arxiv.org/pdf/2402.05782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05782]] Analysing the Sample Complexity of Opponent Shaping(https://arxiv.org/abs/2402.05782)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises the continuous meta-game MDP into a tabular MDP. Within this discretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to derive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. We derive a sample complexity bound that is exponential in the cardinality of the inner state and action space and the number of agents. Our bound guarantees that, with high probability, the final policy learned by an R-FOS agent is close to the optimal policy, apart from a constant factor. Finally, we investigate how R-FOS's sample complexity scales in the size of state-action space. Our theoretical results on scaling are supported empirically in the Matching Pennies environment.</li>
<li><strong>摘要：</strong>在一般和游戏中学习通常会产生集体次优结果。为了解决这个问题，对手塑造（OS）方法积极指导其他智能体的学习过程，根据经验，在许多环境中改善个人和群体的表现。早期的操作系统方法使用高阶导数来塑造共同玩家的学习，这使得它们不适合塑造多个学习步骤。后续工作，无模型对手塑造（M-FOS），通过将操作系统问题重新定义为元游戏来解决这些问题。与早期的 OS 方法相比，人们对 M-FOS 框架的理论理解很少。为 M-FOS 提供理论保证很困难，因为 A）关于元强化学习的理论样本复杂性界限的文献很少 B）M-FOS 在连续状态和动作空间中运行，因此理论分析具有挑战性。在这项工作中，我们提出了 R-FOS，它是 M-FOS 的表格版本，更适合理论分析。 R-FOS 将连续元博弈 MDP 离散为表格 MDP。在这个离散 MDP 中，我们采用了 $R_{max}$ 算法，该算法主要用于导出 MDP 的 PAC 边界，作为 R-FOS 算法中的元学习器。我们得出一个样本复杂度界限，该界限在内部状态和动作空间以及代理数量的基数上呈指数增长。我们的界限保证，除了常数因子之外，R-FOS 代理学到的最终策略很可能接近最优策略。最后，我们研究了 R-FOS 的样本复杂性如何随着状态-动作空间的大小而扩展。我们关于缩放的理论结果在匹配便士环境中得到了经验支持。</li>
</ul>

<h3>Title: Text-to-Code Generation with Modality-relative Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Fenia Christopoulou, Guchun Zhang, Gerasimos Lampouras</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05783">https://arxiv.org/abs/2402.05783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05783">https://arxiv.org/pdf/2402.05783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05783]] Text-to-Code Generation with Modality-relative Pre-training(https://arxiv.org/abs/2402.05783)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model--where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. ``while'') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectives. We focus on text-to-code generation and observe consistent improvements across two backbone models and two test sets, measuring pass@$k$ and a novel incremental variation.</li>
<li><strong>摘要：</strong>大型预训练语言模型最近已被扩展并成功应用于编程语言任务，通常是通过严格自然语言模型的进一步预训练——其中训练序列通常包含自然和（线性化）编程语言。这种方法有效地将序列的两种模态映射到相同的嵌入空间中。然而，编程语言关键字（例如“while”）通常具有非常严格定义的语义。因此，从自然语言使用中进行迁移学习可能不一定有利于他们的代码应用，反之亦然。假设已经预先训练了语言模型，在这项工作中，我们研究了如何根据序列标记所属的模态以及下游任务的最终利益，对序列标记进行不同的调整和表示。我们在进一步的模型预训练过程中尝试使用与模态相关的训练目标来分离模态之间的嵌入空间。我们专注于文本到代码的生成，并观察两个骨干模型和两个测试集的一致改进，测量 pass@$k$ 和新颖的增量变化。</li>
</ul>

<h3>Title: Limits of Transformer Language Models on Algorithmic Learning</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Thomm, Aleksandar Terzic, Geethan Karunaratne, Giacomo Camposampiero, Bernhard Schölkopf, Abbas Rahimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05785">https://arxiv.org/abs/2402.05785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05785">https://arxiv.org/pdf/2402.05785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05785]] Limits of Transformer Language Models on Algorithmic Learning(https://arxiv.org/abs/2402.05785)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt</a></li>
<li><strong>Abstract: </strong>We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.</li>
<li><strong>摘要：</strong>我们分析了 Transformer 语言模型在学习离散算法方面的能力。为此，我们引入了两个新任务，需要多个离散子任务的组合。在从头开始训练 LLaMA 模型以及在 GPT-4 和 Gemini 上进行提示时，我们测量了学习基元的学习成分。我们观察到，最先进的 Transformer 语言模型的组合能力非常有限，并且样本规模比重新学习新算法组合的所有子任务更差。我们还提出了复杂性理论中的一个定理，表明记忆前馈模型的梯度下降可能会导致指数级的数据效率低下。</li>
</ul>

<h3>Title: Prompting Fairness: Artificial Intelligence as Game Players</h3>
<ul>
<li><strong>Authors: </strong>Jazmia Henry</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05786">https://arxiv.org/abs/2402.05786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05786">https://arxiv.org/pdf/2402.05786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05786]] Prompting Fairness: Artificial Intelligence as Game Players(https://arxiv.org/abs/2402.05786)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Utilitarian games such as dictator games to measure fairness have been studied in the social sciences for decades. These games have given us insight into not only how humans view fairness but also in what conditions the frequency of fairness, altruism and greed increase or decrease. While these games have traditionally been focused on humans, the rise of AI gives us the ability to study how these models play these games. AI is becoming a constant in human interaction and examining how these models portray fairness in game play can give us some insight into how AI makes decisions. Over 101 rounds of the dictator game, I conclude that AI has a strong sense of fairness that is dependant of it it deems the person it is playing with as trustworthy, framing has a strong effect on how much AI gives a recipient when designated the trustee, and there may be evidence that AI experiences inequality aversion just as humans.</li>
<li><strong>摘要：</strong>功利主义游戏，例如衡量公平性的独裁者游戏，已经在社会科学中研究了几十年。这些游戏不仅让我们深入了解人类如何看待公平，还让我们了解公平、利他主义和贪婪的频率在什么条件下增加或减少。虽然这些游戏传统上主要针对人类，但人工智能的兴起使我们能够研究这些模型如何玩这些游戏。人工智能正在成为人类互动中的常态，研究这些模型如何在游戏中体现公平性可以让我们深入了解人工智能如何做出决策。经过 101 轮的独裁者游戏，我得出的结论是，人工智能具有强烈的公平感，这取决于它认为与之一起玩的人是值得信赖的，当被指定为受托人时，框架对人工智能给予接收者的金额有很大影响，并且可能有证据表明人工智能和人类一样经历不平等厌恶。</li>
</ul>

<h3>Title: Phonetically rich corpus construction for a low-resourced language</h3>
<ul>
<li><strong>Authors: </strong>Marcellus Amadeus, William Alberto Cruz Castañeda, Wilmer Lobato, Niasche Aquino</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05794">https://arxiv.org/abs/2402.05794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05794">https://arxiv.org/pdf/2402.05794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05794]] Phonetically rich corpus construction for a low-resourced language(https://arxiv.org/abs/2402.05794)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, rag</a></li>
<li><strong>Abstract: </strong>Speech technologies rely on capturing a speaker's voice variability while obtaining comprehensive language information. Textual prompts and sentence selection methods have been proposed in the literature to comprise such adequate phonetic data, referred to as a phonetically rich \textit{corpus}. However, they are still insufficient for acoustic modeling, especially critical for languages with limited resources. Hence, this paper proposes a novel approach and outlines the methodological aspects required to create a \textit{corpus} with broad phonetic coverage for a low-resourced language, Brazilian Portuguese. Our methodology includes text dataset collection up to a sentence selection algorithm based on triphone distribution. Furthermore, we propose a new phonemic classification according to acoustic-articulatory speech features since the absolute number of distinct triphones, or low-probability triphones, does not guarantee an adequate representation of every possible combination. Using our algorithm, we achieve a 55.8\% higher percentage of distinct triphones -- for samples of similar size -- while the currently available phonetic-rich corpus, CETUC and TTS-Portuguese, 12.6\% and 12.3\% in comparison to a non-phonetically rich dataset.</li>
<li><strong>摘要：</strong>语音技术依赖于捕获说话者的语音变化，同时获取全面的语言信息。文献中已经提出了文本提示和句子选择方法来包含足够的语音数据，称为语音丰富的\textit{语料库}。然而，它们对于声学建模仍然是不够的，特别是对于资源有限的语言来说尤其重要。因此，本文提出了一种新颖的方法，并概述了为资源匮乏的语言巴西葡萄牙语创建具有广泛语音覆盖范围的 \textit{语料库} 所需的方法。我们的方法包括文本数据集收集到基于三音素分布的句子选择算法。此外，我们根据声学发音特征提出了一种新的音素分类，因为不同三音素或低概率三音素的绝对数量并不能保证每种可能组合的充分表示。使用我们的算法，对于相似大小的样本，我们的不同三音素百分比提高了 55.8%，而当前可用的语音丰富语料库 CETUC 和 TTS 葡萄牙语，与非语音丰富的数据集。</li>
</ul>

<h3>Title: On Calibration and Conformal Prediction of Deep Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Lahav Dabah, Tom Tirer</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05806">https://arxiv.org/abs/2402.05806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05806">https://arxiv.org/pdf/2402.05806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05806]] On Calibration and Conformal Prediction of Deep Classifiers(https://arxiv.org/abs/2402.05806)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive CP methods: it frequently leads to larger prediction sets. Then, we turn to theoretically analyze this behavior. We reveal several mathematical properties of the procedure, according to which we provide a reasoning for the phenomenon. Our study suggests that it may be worthwhile to utilize adaptive CP methods, chosen for their enhanced conditional coverage, based on softmax values prior to (or after canceling) temperature scaling calibration.</li>
<li><strong>摘要：</strong>在许多分类应用中，基于深度神经网络 (DNN) 的分类器的预测需要伴随一些置信度指示。用于该目标的两种流行的后处理方法是： 1）校准：修改分类器的 softmax 值，使其最大值（与预测相关）更好地估计正确性概率； 2）保形预测（CP）：设计一个分数（基于softmax值），从中产生一组理论上保证正确类别边缘覆盖的预测。虽然在实践中两种类型的指示都可能是理想的，但迄今为止它们之间的相互作用尚未被研究。为了填补这一空白，在本文中，我们研究了温度缩放（可以说是最常见的校准技术）对著名 CP 方法的影响。我们从广泛的实证研究开始，除其他见解外，令人惊讶的是，校准对流行的自适应 CP 方法具有不利影响：它经常导致更大的预测集。然后，我们转向从理论上分析这种行为。我们揭示了该过程的几个数学特性，根据这些特性我们提供了该现象的推理。我们的研究表明，使用自适应 CP 方法可能是值得的，这些方法因其增强的条件覆盖而被选择，基于温度缩放校准之前（或取消之后）的 softmax 值。</li>
</ul>

<h3>Title: Training Large Language Models for Reasoning through Reverse Curriculum  Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05808">https://arxiv.org/abs/2402.05808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05808">https://arxiv.org/pdf/2402.05808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05808]] Training Large Language Models for Reasoning through Reverse Curriculum  Reinforcement Learning(https://arxiv.org/abs/2402.05808)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora, code, rag</a></li>
<li><strong>Abstract: </strong>In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.</li>
<li><strong>摘要：</strong>在本文中，我们提出了 R$^3$：通过逆向课程强化学习（RL）进行学习推理，这是一种仅采用结果监督来实现大型语言模型过程监督的好处的新方法。将强化学习应用于复杂推理的核心挑战是识别一系列能够产生积极奖励的动作序列，并为优化提供适当的监督。结果监督为最终结果提供稀疏奖励，而无需识别错误位置，而过程监督提供逐步奖励，但需要大量的手动注释。 R$^3$ 通过从正确的演示中学习来克服这些限制。具体来说，R$^3$ 逐步将推理的开始状态从演示结束滑动到开始，从而促进各个阶段更轻松的模型探索。因此，R$^3$ 建立了阶梯式课程，允许结果监督提供阶梯级信号并精确定位错误。使用 Llama2-7B，我们的方法在 8 个推理任务上平均超出 RL 基线 4.1 美元点。值得注意的是，在 GSM8K 上基于程序的推理中，它在三个骨干模型中超出了基线 $4.2$ 点，并且在没有任何额外数据的情况下，Codellama-7B + R$^3$ 的性能与更大的模型或闭源模型相当。</li>
</ul>

<h3>Title: FAQ-Gen: An automated system to generate domain-specific FAQs to aid  content comprehension</h3>
<ul>
<li><strong>Authors: </strong>Sahil Kale, Gautam Khaire, Jay Patankar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05812">https://arxiv.org/abs/2402.05812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05812">https://arxiv.org/pdf/2402.05812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05812]] FAQ-Gen: An automated system to generate domain-specific FAQs to aid  content comprehension(https://arxiv.org/abs/2402.05812)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Frequently Asked Questions (FAQs) refer to the most common inquiries about specific content. They serve as content comprehension aids by simplifying topics and enhancing understanding through succinct presentation of information. In this paper, we address FAQ generation as a well-defined Natural Language Processing (NLP) task through the development of an end-to-end system leveraging text-to-text transformation models. We present a literature review covering traditional question-answering systems, highlighting their limitations when applied directly to the FAQ generation task. We propose our system capable of building FAQs from textual content tailored to specific domains, enhancing their accuracy and relevance. We utilise self-curated algorithms for obtaining optimal representation of information to be provided as input and also for ranking the question-answer pairs to maximise human comprehension. Qualitative human evaluation showcases the generated FAQs to be well-constructed and readable, while also utilising domain-specific constructs to highlight domain-based nuances and jargon in the original content.</li>
<li><strong>摘要：</strong>常见问题 (FAQ) 是指有关特定内容的最常见询问。它们通过简化主题并通过简洁的信息呈现增强理解来帮助理解内容。在本文中，我们通过开发利用文本到文本转换模型的端到端系统，将常见问题解答生成作为明确定义的自然语言处理（NLP）任务来解决。我们对传统问答系统进行了文献综述，强调了它们直接应用于常见问题解答生成任务时的局限性。我们建议我们的系统能够根据针对特定领域的文本内容构建常见问题解答，从而提高其准确性和相关性。我们利用自行策划的算法来获得作为输入提供的信息的最佳表示，并对问答对进行排名以最大限度地提高人类的理解力。定性人工评估展示了生成的常见问题解答结构良好且可读，同时还利用特定领域的结构来突出原始内容中基于领域的细微差别和术语。</li>
</ul>

<h3>Title: Selective Forgetting: Advancing Machine Unlearning Techniques and  Evaluation in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, Georg Gottlob</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05813">https://arxiv.org/abs/2402.05813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05813">https://arxiv.org/pdf/2402.05813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05813]] Selective Forgetting: Advancing Machine Unlearning Techniques and  Evaluation in Language Models(https://arxiv.org/abs/2402.05813)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination. To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline annotation entails a robust two-stage process based on Large Language Models (LLMs).</li>
<li><strong>摘要：</strong>本研究的目的是研究机器学习（MU），这是一个新兴领域，专注于解决与神经模型无意中保留个人或敏感数据相关的问题。在这里，引入了一种新颖的方法来在语言模型中实现精确和选择性的遗忘。与之前采用完全相反的训练目标的方法不同，这种方法旨在减轻对语言模型性能的不利影响，特别是在生成任务中。此外，还提出了两个创新的评估指标：敏感信息提取可能性（S-EL）和敏感信息记忆准确性（S-MA），旨在衡量敏感信息消除的有效性。为了强化遗忘框架，提出了一种注释敏感范围的有效方法，涉及在线和离线策略。在线选择机制利用语言概率分数来确保计算效率，而离线注释则需要基于大型语言模型（LLM）的稳健的两阶段过程。</li>
</ul>

<h3>Title: Guided Evolution with Binary Discriminators for ML Program Search</h3>
<ul>
<li><strong>Authors: </strong>John D. Co-Reyes, Yingjie Miao, George Tucker, Aleksandra Faust, Esteban Real</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05821">https://arxiv.org/abs/2402.05821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05821">https://arxiv.org/pdf/2402.05821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05821]] Guided Evolution with Binary Discriminators for ML Program Search(https://arxiv.org/abs/2402.05821)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>How to automatically design better machine learning programs is an open problem within AutoML. While evolution has been a popular tool to search for better ML programs, using learning itself to guide the search has been less successful and less understood on harder problems but has the promise to dramatically increase the speed and final performance of the optimization process. We propose guiding evolution with a binary discriminator, trained online to distinguish which program is better given a pair of programs. The discriminator selects better programs without having to perform a costly evaluation and thus speed up the convergence of evolution. Our method can encode a wide variety of ML components including symbolic optimizers, neural architectures, RL loss functions, and symbolic regression equations with the same directed acyclic graph representation. By combining this representation with modern GNNs and an adaptive mutation strategy, we demonstrate our method can speed up evolution across a set of diverse problems including a 3.7x speedup on the symbolic search for ML optimizers and a 4x speedup for RL loss functions.</li>
<li><strong>摘要：</strong>如何自动设计更好的机器学习程序是 AutoML 中的一个悬而未决的问题。虽然进化一直是搜索更好的机器学习程序的流行工具，但使用学习本身来指导搜索不太成功，而且在解决更困难的问题时也较少被理解，但有望显着提高优化过程的速度和最终性能。我们建议用二元判别器来指导进化，该判别器在线训练以区分给定一对程序哪个程序更好。判别器选择更好的程序，而无需执行昂贵的评估，从而加速进化的收敛。我们的方法可以使用相同的有向无环图表示来编码各种 ML 组件，包括符号优化器、神经架构、RL 损失函数和符号回归方程。通过将这种表示与现代 GNN 和自适应突变策略相结合，我们证明了我们的方法可以加速一系列不同问题的进化，包括 ML 优化器的符号搜索加速 3.7 倍，以及 RL 损失函数的 4 倍加速。</li>
</ul>

<h3>Title: Is it Possible to Edit Large Language Models Robustly?</h3>
<ul>
<li><strong>Authors: </strong>Xinbei Ma, Tianjie Ju, Jiyang Qiu, Zhuosheng Zhang, Hai Zhao, Lifeng Liu, Yulong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05827">https://arxiv.org/abs/2402.05827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05827">https://arxiv.org/pdf/2402.05827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05827]] Is it Possible to Edit Large Language Models Robustly?(https://arxiv.org/abs/2402.05827)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization. To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of language models and changes the related language generation. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI. Concretely, we conduct extensive analysis to address the three key research questions. Q1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? Q2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? Q3: Which knowledge features are correlated with the performance and robustness of editing? Our experimental results uncover a substantial disparity between existing editing methods and the practical application of LLMs. On rephrased prompts that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline. Further analysis shows that more popular knowledge is memorized better, easier to recall, and more challenging to edit effectively.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在构建模仿人类行为的交流人工智能方面发挥了关键作用，但面临着高效定制的挑战。为了应对这一挑战，最近的研究深入到模型编辑领域，它操纵语言模型的特定记忆并改变相关的语言生成。然而，模型编辑的稳健性仍然是一个悬而未决的问题。这项工作旨在了解编辑方法的优点和局限性，从而促进交流人工智能的稳健、现实的应用。具体来说，我们进行了广泛的分析来解决三个关键的研究问题。问题 1：经过编辑的法学硕士在现实情况下的行为是否能够始终像交流人工智能一样？ Q2：提示的改写会在多大程度上导致法学硕士偏离编辑过的知识记忆？ Q3：哪些知识特征与编辑的性能和稳健性相关？我们的实验结果揭示了现有编辑方法与法学硕士的实际应用之间存在巨大差异。对于复杂且灵活但在实际应用中常见的改写提示，编辑性能会显着下降。进一步的分析表明，更流行的知识记忆得更好，更容易回忆，并且更难以有效编辑。</li>
</ul>

<h3>Title: Discovering Temporally-Aware Reinforcement Learning Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Matthew Thomas Jackson, Chris Lu, Louis Kirsch, Robert Tjarko Lange, Shimon Whiteson, Jakob Nicolaus Foerster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05828">https://arxiv.org/abs/2402.05828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05828">https://arxiv.org/pdf/2402.05828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05828]] Discovering Temporally-Aware Reinforcement Learning Algorithms(https://arxiv.org/abs/2402.05828)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or "training horizon". In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends that ignoring the optimization time horizon significantly restricts the expressive potential of discovered learning algorithms. We propose a simple augmentation to two existing objective discovery approaches that allows the discovered algorithm to dynamically update its objective function throughout the agent's training procedure, resulting in expressive schedules and increased generalization across different training horizons. In the process, we find that commonly used meta-gradient approaches fail to discover such adaptive objective functions while evolution strategies discover highly dynamic learning rules. We demonstrate the effectiveness of our approach on a wide range of tasks and analyze the resulting learned algorithms, which we find effectively balance exploration and exploitation by modifying the structure of their learning rules throughout the agent's lifetime.</li>
<li><strong>摘要：</strong>元学习的最新进展使得能够自动发现由代理目标函数参数化的新型强化学习算法。为了改进手动设计的算法，这种学习目标函数的参数化必须具有足够的表现力，以代表新的学习原理（而不是仅仅恢复已经建立的原理），同时仍然泛化到其元训练分布之外的广泛设置。然而，现有的方法侧重于发现目标函数，与强化学习中许多广泛使用的目标函数一样，没有考虑训练允许的总步数或“训练范围”。相比之下，人类在获得新能力的过程中会使用大量不同的学习目标。例如，学生可以根据考试截止日期的临近和他们的自我评估能力来改变他们的学习技巧。本文认为，忽略优化时间范围会显着限制已发现的学习算法的表达潜力。我们提出了对两种现有目标发现方法的简单增强，允许所发现的算法在整个代理的训练过程中动态更新其目标函数，从而产生富有表现力的时间表并增强跨不同训练范围的泛化能力。在此过程中，我们发现常用的元梯度方法无法发现这种自适应目标函数，而进化策略则发现高度动态的学习规则。我们展示了我们的方法在各种任务上的有效性，并分析了由此产生的学习算法，我们发现通过在代理的整个生命周期修改学习规则的结构，我们发现这些算法有效地平衡了探索和利用。</li>
</ul>

<h3>Title: Limitations of Agents Simulated by Predictive Models</h3>
<ul>
<li><strong>Authors: </strong>Raymond Douglas, Jacek Karwowski, Chan Bae, Andis Draguns, Victoria Krakovna</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05829">https://arxiv.org/abs/2402.05829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05829">https://arxiv.org/pdf/2402.05829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05829]] Limitations of Agents Simulated by Predictive Models(https://arxiv.org/abs/2402.05829)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>There is increasing focus on adapting predictive models into agent-like systems, most notably AI assistants based on language models. We outline two structural reasons for why these models can fail when turned into agents. First, we discuss auto-suggestive delusions. Prior work has shown theoretically that models fail to imitate agents that generated the training data if the agents relied on hidden observations: the hidden observations act as confounding variables, and the models treat actions they generate as evidence for nonexistent observations. Second, we introduce and formally study a related, novel limitation: predictor-policy incoherence. When a model generates a sequence of actions, the model's implicit prediction of the policy that generated those actions can serve as a confounding variable. The result is that models choose actions as if they expect future actions to be suboptimal, causing them to be overly conservative. We show that both of those failures are fixed by including a feedback loop from the environment, that is, re-training the models on their own actions. We give simple demonstrations of both limitations using Decision Transformers and confirm that empirical results agree with our conceptual and formal analysis. Our treatment provides a unifying view of those failure modes, and informs the question of why fine-tuning offline learned policies with online learning makes them more effective.</li>
<li><strong>摘要：</strong>人们越来越关注将预测模型应用到类似代理的系统中，尤其是基于语言模型的人工智能助手。我们概述了这些模型在转变为代理时可能失败的两个结构性原因。首先，我们讨论自我暗示妄想。先前的工作从理论上表明，如果代理依赖于隐藏的观察，则模型无法模仿生成训练数据的代理：隐藏的观察充当混杂变量，并且模型将它们生成的动作视为不存在的观察的证据。其次，我们引入并正式研究一个相关的、新颖的局限性：预测策略不一致。当模型生成一系列操作时，模型对生成这些操作的策略的隐式预测可以充当混杂变量。结果是模型选择行动就好像它们预计未来的行动不是最优的一样，导致它们过于保守。我们表明，这两种失败都可以通过包含环境的反馈循环来解决，即根据模型自己的行为重新训练模型。我们使用决策转换器对这两种限制进行了简单的演示，并确认实证结果与我们的概念和形式分析一致。我们的治疗方法提供了对这些失败模式的统一看法，并提出了为什么通过在线学习微调离线学习策略会使它们更有效的问题。</li>
</ul>

<h3>Title: How Much is Unseen Depends Chiefly on Information About the Seen</h3>
<ul>
<li><strong>Authors: </strong>Seongmin Lee, Marcel Böhme</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05835">https://arxiv.org/abs/2402.05835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05835">https://arxiv.org/pdf/2402.05835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05835]] How Much is Unseen Depends Chiefly on Information About the Seen(https://arxiv.org/abs/2402.05835)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>It might seem counter-intuitive at first: We find that, in expectation, the proportion of data points in an unknown population-that belong to classes that do not appear in the training data-is almost entirely determined by the number $f_k$ of classes that do appear in the training data the same number of times. While in theory we show that the difference of the induced estimator decays exponentially in the size of the sample, in practice the high variance prevents us from using it directly for an estimator of the sample coverage. However, our precise characterization of the dependency between $f_k$'s induces a large search space of different representations of the expected value, which can be deterministically instantiated as estimators. Hence, we turn to optimization and develop a genetic algorithm that, given only the sample, searches for an estimator with minimal mean-squared error (MSE). In our experiments, our genetic algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 96% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80% of the Good-Turing estimator's.</li>
<li><strong>摘要：</strong>一开始这似乎违反直觉：我们发现，按照预期，未知总体中属于未出现在训练数据中的类的数据点的比例几乎完全由 $f_k$ 的数量决定在训练数据中出现相同次数的类。虽然理论上我们表明诱导估计量的差异随样本大小呈指数衰减，但实际上，高方差阻止我们直接将其用于样本覆盖率的估计量。然而，我们对 $f_k$ 之间的依赖关系的精确描述会导致预期值的不同表示的大量搜索空间，这些搜索空间可以确定性地实例化为估计器。因此，我们转向优化并开发了一种遗传算法，在仅给定样本的情况下，搜索具有最小均方误差（MSE）的估计量。在我们的实验中，我们的遗传算法发现估计器的 MSE 远小于最先进的 Good-Turing 估计器。当样本数量至少与类一样多时，这适用于超过 96% 的运行。我们的估计器的 MSE 大约是 Good-Turing 估计器的 80%。</li>
</ul>

<h3>Title: Learning to Route Among Specialized Experts for Zero-Shot Generalization</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Muqeeth, Haokun Liu, Yufan Liu, Colin Raffel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05859">https://arxiv.org/abs/2402.05859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05859">https://arxiv.org/pdf/2402.05859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05859]] Learning to Route Among Specialized Experts for Zero-Shot Generalization(https://arxiv.org/abs/2402.05859)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Recently, there has been a widespread proliferation of "expert" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range of specialized model collections and zero-shot generalization benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc routing and, in some cases, outperforms explicit multitask training (which requires simultaneous data access). To better understand the routing strategy learned by PHATGOOSE, we perform qualitative experiments to validate that PHATGOOSE's performance stems from its ability to make adaptive per-token and per-module expert choices. We release all of our code to support future work on improving zero-shot generalization by recycling specialized experts.</li>
<li><strong>摘要：</strong>最近，“专家”语言模型广泛扩散，这些模型通过参数高效的微调专门针对特定任务或领域。我们如何回收大量专家语言模型来提高对未见过的任务的零样本泛化？在这项工作中，我们提出了 Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE)，它学习在通过参数高效微调生成的专用模块之间进行路由。与过去学习在专门模型之间进行路由的方法不同，PHATGOOSE 探索了如果可以为模型中的每个令牌和每个层自适应地选择不同的专家，那么零样本泛化将会得到改善的可能性。至关重要的是，我们的方法是事后的——它不需要同时访问用于创建专门模型的数据集，并且只需要在训练每个专家模型后进行适量的额外计算。在涵盖一系列专门模型集合和零样本泛化基准的实验中，我们发现 PHATGOOSE 优于过去的事后路由方法，并且在某些情况下优于显式多任务训练（需要同时访问数据）。为了更好地理解 PHATGOOSE 学习的路由策略，我们进行了定性实验来验证 PHATGOOSE 的性能源于其自适应每个令牌和每个模块专家选择的能力。我们发布了所有代码，以支持回收专业专家未来改进零样本泛化的工作。</li>
</ul>

<h3>Title: Let Your Graph Do the Talking: Encoding Structured Data for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05862">https://arxiv.org/abs/2402.05862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05862">https://arxiv.org/pdf/2402.05862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05862]] Let Your Graph Do the Talking: Encoding Structured Data for LLMs(https://arxiv.org/abs/2402.05862)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark.</li>
<li><strong>摘要：</strong>我们如何才能最好地将结构化数据编码为顺序形式以用于大型语言模型（LLM）？在这项工作中，我们引入了一种参数有效的方法来显式表示法学硕士的结构化数据。我们的方法 GraphToken 学习编码函数，以使用显式结构化信息扩展提示。与其他专注于有限领域（例如知识图表示）的工作不同，我们的工作是第一个专注于用于各种推理任务的结构化数据的通用编码的工作。我们表明，显式表示图结构可以显着改进图推理任务。具体来说，我们在 GraphQA 基准测试中看到节点、边缘和图级任务的全面改进 - 高达 73%。</li>
</ul>

<h3>Title: How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, James Zou</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05863">https://arxiv.org/abs/2402.05863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05863">https://arxiv.org/pdf/2402.05863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05863]] How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis(https://arxiv.org/abs/2402.05863)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20\% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, \NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities.</li>
<li><strong>摘要：</strong>谈判是社会交往的基础；人类谈判一切事务，从汽车价格到如何共享公共资源。随着人们对使用大型语言模型 (LLM) 代表人类用户充当代理的兴趣迅速增长，此类 LLM 代理也需要能够进行协商。在本文中，我们研究了法学硕士之间的谈判能力。我们开发了 NegotiationArena：一个用于评估和探讨法学硕士代理人谈判能力的灵活框架。我们在 NegotiationArena 中实现了三种类型的场景来评估 LLM 在分配共享资源（最后通牒游戏）、聚合资源（交易游戏）和买卖商品（价格谈判）方面的行为。每个场景都允许 LLM 代理之间进行多轮灵活对话，以进行更复杂的谈判。有趣的是，法学硕士代理人可以通过采用某些行为策略来显着提高他们的谈判结果。例如，通过假装落寞和绝望，法学硕士在与标准 GPT-4 谈判时可以将他们的收益提高 20%。我们还量化了 LLM 代理人表现出的非理性谈判行为，其中许多也出现在人类身上。总之，\NegotiationArena 提供了一个研究法学硕士互动的新环境，使人们能够对法学硕士的心理理论、非理性和推理能力有新的见解。</li>
</ul>

<h3>Title: Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuandong Zhao, Lei Li, Yu-Xiang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05864">https://arxiv.org/abs/2402.05864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05864">https://arxiv.org/pdf/2402.05864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05864]] Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs(https://arxiv.org/abs/2402.05864)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, code</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding</li>
<li><strong>摘要：</strong>在本文中，我们提出了一种称为置换翻转（PF）解码器的新解码方法。它具有与标准采样解码器类似的鲁棒性属性，但事实证明，其质量鲁棒性权衡比采样好 2 倍，而且绝不会比任何其他解码器差。我们还设计了一种类似于 Aaronson 的 Gumbel 水印的加密水印方案，但自然是为 PF 解码器量身定制的。水印方案不会改变样本的分布，同时在生成的文本具有高熵时允许任意低的误报率和高召回率。我们的实验表明，PF 解码器（及其带水印的对应物）在困惑度方面显着优于朴素采样（及其 Gumbel 水印对应物），同时保留相同的鲁棒性（和可检测性），因此使其成为一种有前途的新方法LLM解码。代码可在 https://github.com/XuandongZhao/pf-decoding 获取</li>
</ul>

<h3>Title: PromptCrypt: Prompt Encryption for Secure Communication with Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guo Lin, Wenyue Hua, Yongfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05868">https://arxiv.org/abs/2402.05868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05868">https://arxiv.org/pdf/2402.05868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05868]] PromptCrypt: Prompt Encryption for Secure Communication with Large  Language Models(https://arxiv.org/abs/2402.05868)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code, chat, rag</a></li>
<li><strong>Abstract: </strong>Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the model's performance remains unaffected. We conduct experiments on three tasks, personalized recommendation, sentiment analysis, and tabular data analysis. Experiment results reveal that PromptCrypt can encrypt personal information within prompts in such a manner that not only prevents the discernment of sensitive data by humans or LLM itself, but also maintains or even improves the precision without further tuning, achieving comparable or even better task accuracy than directly prompting the LLM without prompt encryption. These results highlight the practicality of adopting encryption measures that safeguard user privacy without compromising the functional integrity and performance of LLMs. Code and dataset are available at https://github.com/agiresearch/PromptCrypt.</li>
<li><strong>摘要：</strong>ChatGPT 等基于云的大型语言模型 (LLM) 已日益成为日常运营中不可或缺的一部分，成为跨各种应用程序的重要工具。虽然这些模型在可访问性和功能方面提供了巨大的好处，但它们也带来了重大的隐私问题：在云基础设施中传输和存储用户数据会带来数据泄露和未经授权访问敏感信息的巨大风险；即使数据的传输和存储是加密的，LLM服务提供商本身仍然知道数据的真实内容，从而阻止个人或实体放心地使用此类LLM服务。为了解决这些问题，本文提出了一种简单而有效的PromptCrypt机制来保护用户隐私。它在将用户输入发送给 LLM 之前使用 Emoji 对其进行加密，有效地使人类或 LLM 的检查无法破译它们，同时保留提示的原始意图，从而确保模型的性能不受影响。我们对个性化推荐、情感分析和表格数据分析三个任务进行了实验。实验结果表明，PromptCrypt 可以对提示中的个人信息进行加密，不仅可以防止人类或 LLM 本身识别敏感数据，而且无需进一步调整即可保持甚至提高精度，达到与相比甚至更好的任务精度直接提示LLM，不提示加密。这些结果凸显了采用加密措施保护用户隐私而不损害法学硕士的功能完整性和性能的实用性。代码和数据集可在 https://github.com/agiresearch/PromptCrypt 获取。</li>
</ul>

<h3>Title: Federated Offline Reinforcement Learning: Collaborative Single-Policy  Coverage Suffices</h3>
<ul>
<li><strong>Authors: </strong>Jiin Woo, Laixi Shi, Gauri Joshi, Yuejie Chi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05876">https://arxiv.org/abs/2402.05876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05876">https://arxiv.org/pdf/2402.05876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05876]] Federated Offline Reinforcement Learning: Collaborative Single-Policy  Coverage Suffices(https://arxiv.org/abs/2402.05876)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. This work explores the benefit of federated learning for offline RL, aiming at collaboratively leveraging offline datasets at multiple agents. Focusing on finite-horizon episodic tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for federated offline RL. FedLCB-Q updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual agents, as long as the local datasets collectively cover the state-action space visited by the optimal policy, highlighting the power of collaboration in the federated setting. In fact, the sample complexity almost matches that of the single-agent counterpart, as if all the data are stored at a central location, up to polynomial factors of the horizon length. Furthermore, FedLCB-Q is communication-efficient, where the number of communication rounds is only linear with respect to the horizon length up to logarithmic factors.</li>
<li><strong>摘要：</strong>离线强化学习（RL）旨在利用离线数据学习最优策略，由于其在在线数据收集不可行或昂贵的关键应用中的潜力而引起了人们的极大兴趣。这项工作探讨了联合学习对离线强化学习的好处，旨在协作利用多个代理的离线数据集。我们专注于有限范围的情景表格马尔可夫决策过程 (MDP)，设计了 FedLCB-Q，它是流行的无模型 Q 学习算法的变体，专为联合离线 RL 量身定制。 FedLCB-Q 使用新颖的学习率计划更新代理的本地 Q 函数，并使用重要性平均和精心设计的悲观惩罚项将它们聚合在中央服务器上。我们的样本复杂性分析表明，通过适当选择参数和同步计划，只要本地数据集共同覆盖状态动作，FedLCB-Q 就可以在代理数量方面实现线性加速，而不需要单个代理的高质量数据集最优策略访问的空间，突出了联邦环境中协作的力量。事实上，样本复杂性几乎与单智能体对应的复杂性相匹配，就好像所有数据都存储在一个中心位置，最多可达水平长度的多项式因子。此外，FedLCB-Q 的通信效率很高，其中通信轮数仅与水平长度成线性关系，最多为对数因子。</li>
</ul>

<h3>Title: Generative Echo Chamber? Effects of LLM-Powered Search Systems on  Diverse Information Seeking</h3>
<ul>
<li><strong>Authors: </strong>Nikhil Sharma, Q. Vera Liao, Ziang Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05880">https://arxiv.org/abs/2402.05880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05880">https://arxiv.org/pdf/2402.05880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05880]] Generative Echo Chamber? Effects of LLM-Powered Search Systems on  Diverse Information Seeking(https://arxiv.org/abs/2402.05880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implications for the development of LLMs and conversational search systems, and the policy governing these technologies.</li>
<li><strong>摘要：</strong>由大型语言模型 (LLM) 支持的会话搜索系统已被数亿人使用，并且被认为比传统搜索带来许多好处。然而，尽管数十年的研究和公共讨论质疑搜索系统增加选择性曝光和产生回音室的风险——限制不同意见的曝光并导致意见两极分化，但人们对法学硕士支持的对话式搜索的风险知之甚少。我们进行了两项实验来研究：1）与传统搜索相比，LLM 驱动的会话搜索是否以及如何增加选择性曝光； 2) 具有观点偏见的法学硕士是否以及如何改变用户观点，从而强化或挑战用户的观点。总体而言，我们发现参与者通过法学硕士支持的对话搜索进行了更具偏见的信息查询，而固执己见的法学硕士强化了他们的观点，加剧了这种偏见。这些结果对法学硕士和对话式搜索系统的开发以及管理这些技术的政策具有重要意义。</li>
</ul>

<h3>Title: Large Language Model Meets Graph Neural Network in Knowledge  Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shengxiang Hu, Guobing Zou, Song Yang, Bofeng Zhang, Yixin Chen</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05894">https://arxiv.org/abs/2402.05894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05894">https://arxiv.org/pdf/2402.05894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05894]] Large Language Model Meets Graph Neural Network in Knowledge  Distillation(https://arxiv.org/abs/2402.05894)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the teacher LLM and the student GNN in latent space, employing a layer-adaptive contrastive learning strategy. Through extensive experiments on a variety of LLM and GNN models and multiple benchmark datasets, the proposed LinguGKD significantly boosts the student GNN's predictive accuracy and convergence rate, without the need of extra data or model parameters. Compared to teacher LLM, distilled GNN achieves superior inference speed equipped with much fewer computing and storage demands, when surpassing the teacher LLM's classification performance on some of benchmark datasets.</li>
<li><strong>摘要：</strong>尽管最近社区披露了大型语言模型 (LLM) 在理解文本属性图 (TAG) 方面的进步和潜力，但 LLM 的高计算和存储要求以及推理过程中的长延迟阻碍了 LLM 的生产部署。同时，虽然传统的图神经网络（GNN）重量轻并且擅长学习图的结构特征，但它们掌握标签中复杂语义的能力在实际应用中受到一定限制。为了解决这些限制，我们专注于 TAG 中节点分类的下游任务，并提出了一种新颖的图知识蒸馏框架，称为语言图知识蒸馏（LinguGKD），使用 LLM 作为教师模型和 GNN 作为知识蒸馏的学生模型。它涉及在设计的节点分类提示上对 LLM 进行面向 TAG 的指令调整，然后采用层自适应对比学习策略，在潜在空间中对齐教师 LLM 和学生 GNN 的分层学习节点特征。通过对各种LLM和GNN模型以及多个基准数据集的广泛实验，所提出的LinguGKD显着提高了学生GNN的预测准确性和收敛速度，而不需要额外的数据或模型参数。与教师 LLM 相比，蒸馏 GNN 在某些基准数据集上超越教师 LLM 的分类性能时，以更少的计算和存储需求实现了卓越的推理速度。</li>
</ul>

<h3>Title: FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Eun Cheol Choi, Emilio Ferrara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05904">https://arxiv.org/abs/2402.05904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05904">https://arxiv.org/pdf/2402.05904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05904]] FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs(https://arxiv.org/abs/2402.05904)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field.</li>
<li><strong>摘要：</strong>我们的社会正面临着危害公共健康和信任的猖獗的错误信息。为了应对社会挑战，我们引入了 FACT-GPT，这是一个利用大型语言模型 (LLM) 来自动化事实检查的声明匹配阶段的系统。 FACT-GPT 在合成数据集上进行训练，可以识别与之前被揭穿的主张一致、矛盾或无关的社交媒体内容。我们的评估表明，我们的专业法学硕士在识别相关索赔方面可以与大型模型的准确性相媲美，密切反映人类的判断。这项研究为高效索赔匹配提供了自动化解决方案，展示了法学硕士在支持事实核查方面的潜力，并为该领域的进一步研究提供了宝贵的资源。</li>
</ul>

<h3>Title: Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative  Markov Games</h3>
<ul>
<li><strong>Authors: </strong>Hafez Ghaemi, Hamed Kebriaei, Alireza Ramezani Moghaddam, Majid Nili Ahamdabadi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05906">https://arxiv.org/abs/2402.05906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05906">https://arxiv.org/pdf/2402.05906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05906]] Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative  Markov Games(https://arxiv.org/abs/2402.05906)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental results show that subjective CPT policies obtained by our algorithm can be different from the risk-neutral ones, and agents with a higher loss aversion are more inclined to socially isolate themselves in an NAMG.</li>
<li><strong>摘要：</strong>经典的多智能体强化学习（MARL）假设智能体的风险中立和完全客观。然而，在智能体需要考虑或建模人类经济或社会偏好的环境中，必须将风险概念纳入强化学习优化问题中。这在 MARL 中更为重要，因为其他人类或非人类代理也参与其中，可能还有他们自己的风险敏感政策。在这项工作中，我们利用累积前景理论（CPT）、非凸风险度量和连贯风险度量的概括来考虑风险敏感和非合作的 MARL。 CPT 能够解释人类的损失厌恶以及他们高估/低估小/大概率的倾向。我们提出了一种针对网络聚合马尔可夫游戏（NAMG）的具有 CPT 风险的基于分布式采样的行动评论家（AC）算法，我们将其称为分布式嵌套 CPT-AC。在一组假设下，我们证明了算法收敛于 NAMG 中马尔可夫完美纳什均衡的主观概念。实验结果表明，我们的算法获得的主观 CPT 策略可能与风险中性策略不同，损失厌恶程度较高的代理人更倾向于在 NAMG 中社会孤立自己。</li>
</ul>

<h3>Title: Efficient Stagewise Pretraining via Progressive Subnetworks</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Panigrahi, Nikunj Saunshi, Kaifeng Lyu, Sobhan Miryoosefi, Sashank Reddi, Satyen Kale, Sanjiv Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05913">https://arxiv.org/abs/2402.05913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05913">https://arxiv.org/pdf/2402.05913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05913]] Efficient Stagewise Pretraining via Progressive Subnetworks(https://arxiv.org/abs/2402.05913)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent developments in large language models have sparked interest in efficient pretraining methods. A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. RaPTr achieves better pre-training loss for BERT and UL2 language models while requiring 20-33% fewer FLOPs compared to standard training, and is competitive or better than other efficient training methods. Furthermore, RaPTr shows better downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5% compared to standard training and stacking. Finally, we provide a theoretical basis for RaPTr to justify (a) the increasing complexity of subnetworks in stages, and (b) the stability in loss across stage transitions due to residual connections and layer norm.</li>
<li><strong>摘要：</strong>大型语言模型的最新发展引发了人们对高效预训练方法的兴趣。最近的一个有效范例是进行分阶段训练，其中模型的大小在训练过程中逐渐增加（例如逐渐堆叠（Reddi 等人，2023））。虽然资源和工作时间的节省很有吸引力，但它也有局限性，特别是无法在早期阶段评估完整模型，以及由于初始阶段模型容量较小而导致模型质量下降。在这项工作中，我们提出了一种替代框架，即渐进式子网训练，它在整个训练过程中维护完整的模型，但在每个步骤中仅训练模型内的子网。我们专注于该框架的简单实例，即随机路径训练（RaPTr），它在每个步骤中仅训练层的子路径，逐步增加阶段中的路径长度。 RaPTr 为 BERT 和 UL2 语言模型实现了更好的预训练损失，同时与标准训练相比，所需的 FLOP 减少了 20-33%，并且比其他高效训练方法具有竞争力或更好。此外，RaPTr 在 UL2 上表现出更好的下游性能，与标准训练和堆栈相比，将 QA 任务和 SuperGLUE 提高了 1-5%。最后，我们为 RaPTr 提供了理论基础，以证明（a）阶段中子网络的复杂性不断增加，以及（b）由于剩余连接和层范数而导致的跨阶段转换损失的稳定性。</li>
</ul>

<h3>Title: GenEFT: Understanding Statics and Dynamics of Model Generalization via  Effective Theory</h3>
<ul>
<li><strong>Authors: </strong>David D. Baek, Ziming Liu, Max Tegmark</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05916">https://arxiv.org/abs/2402.05916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05916">https://arxiv.org/pdf/2402.05916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05916]] GenEFT: Understanding Statics and Dynamics of Model Generalization via  Effective Theory(https://arxiv.org/abs/2402.05916)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>We present GenEFT: an effective theory framework for shedding light on the statics and dynamics of neural network generalization, and illustrate it with graph learning examples. We first investigate the generalization phase transition as data size increases, comparing experimental results with information-theory-based approximations. We find generalization in a Goldilocks zone where the decoder is neither too weak nor too powerful. We then introduce an effective theory for the dynamics of representation learning, where latent-space representations are modeled as interacting particles (repons), and find that it explains our experimentally observed phase transition between generalization and overfitting as encoder and decoder learning rates are scanned. This highlights the power of physics-inspired effective theories for bridging the gap between theoretical predictions and practice in machine learning.</li>
<li><strong>摘要：</strong>我们提出 GenEFT：一个有效的理论框架，用于阐明神经网络泛化的静态和动态，并用图学习示例进行说明。我们首先研究随着数据量增加的泛化相变，将实验结果与基于信息论的近似值进行比较。我们在金发姑娘区域中发现了泛化，其中解码器既不太弱也不太强大。然后，我们引入了一种有效的表示学习动力学理论，其中潜在空间表示被建模为相互作用的粒子（响应），并发现它解释了我们在扫描编码器和解码器学习率时实验观察到的泛化和过度拟合之间的相变。这凸显了受物理学启发的有效理论在弥合机器学习理论预测与实践之间差距方面的力量。</li>
</ul>

<h3>Title: On the Convergence of Zeroth-Order Federated Tuning in Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05926">https://arxiv.org/abs/2402.05926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05926">https://arxiv.org/pdf/2402.05926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05926]] On the Convergence of Zeroth-Order Federated Tuning in Large Language  Models(https://arxiv.org/abs/2402.05926)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also significantly reduces GPU memory usage during training to levels comparable to those during inference. Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction. We hope our work can help to bridge theoretical and practical aspects of federated fine-tuning for LLMs and facilitate further development and research.</li>
<li><strong>摘要：</strong>联邦学习 (FL) 和大型语言模型 (LLM) 的融合正在开创隐私保护自然语言处理的新时代。然而，微调 LLM 的大量内存需求带来了重大挑战，特别是在计算资源有限的边缘设备上部署时。为了解决这个问题，我们探索了在联合环境中内存高效零阶优化的新颖集成，我们将这种协同作用表示为 FedMeZO。我们的研究首次在法学硕士背景下检验了 FedMeZO 的理论基础，解决了大参数空间对优化行为的影响、收敛特性的建立以及收敛关键参数的识别等关键问题，为个性化联邦决策提供信息。策略。我们广泛的经验证据支持该理论，表明 FedMeZO 不仅比 SGD 等传统一阶方法收敛得更快，而且还显着地将训练期间的 GPU 内存使用量降低到与推理期间相当的水平。此外，所提出的个性化 FL 策略基于理论见解来定制客户端学习率，可以有效加速损失减少。我们希望我们的工作能够帮助弥合法学硕士联合微调的理论和实践方面的问题，并促进进一步的发展和研究。</li>
</ul>

<h3>Title: An Interactive Agent Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul Tang, Ehsan Adeli, Shrinidhi Kowshika Lakshmikanth, Kevin Schulman, Arnold Milstein, Demetri Terzopoulos, Ade Famoti, Noboru Kuno, Ashley Llorens, Hoi Vo, Katsu Ikeuchi, Li Fei-Fei, Jianfeng Gao, Naoki Wake, Qiuyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05929">https://arxiv.org/abs/2402.05929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05929">https://arxiv.org/pdf/2402.05929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05929]] An Interactive Agent Foundation Model(https://arxiv.org/abs/2402.05929)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, rag, agent</a></li>
<li><strong>Abstract: </strong>The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems.</li>
<li><strong>摘要：</strong>人工智能系统的发展正在从创建静态的、特定于任务的模型转向动态的、基于代理的系统，能够在广泛的应用中表现良好。我们提出了一种交互式代理基础模型，该模型使用一种新颖的多任务代理训练范例来训练跨广泛领域、数据集和任务的人工智能代理。我们的训练范式统一了不同的预训练策略，包括视觉蒙版自动编码器、语言建模和下一步动作预测，从而实现了多功能且适应性强的人工智能框架。我们展示了我们的框架在三个不同领域的性能——机器人、游戏人工智能和医疗保健。我们的模型展示了其在每个领域生成有意义且与上下文相关的输出的能力。我们方法的优势在于其通用性，利用各种数据源（例如机器人序列、游戏数据、大规模视频数据集和文本信息）进行有效的多模式和多任务学习。我们的方法为开发通才、采取行动的多模式系统提供了一条有前途的途径。</li>
</ul>

<h3>Title: WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Xing Han Lù, Zdeněk Kasner, Siva Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05930">https://arxiv.org/abs/2402.05930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05930">https://arxiv.org/pdf/2402.05930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05930]] WebLINX: Real-World Website Navigation with Multi-Turn Dialogue(https://arxiv.org/abs/2402.05930)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code, agent</a></li>
<li><strong>Abstract: </strong>We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings. Our code, data and models are available for research: https://mcgill-nlp.github.io/weblinx</li>
<li><strong>摘要：</strong>我们提出了会话式网络导航问题，其中数字代理控制网络浏览器并遵循用户指令以多轮对话方式解决现实世界的任务。为了解决这个问题，我们引入了 WEBLINX - 一个跨 2300 个对话式 Web 导航专家演示的 100K 交互的大规模基准。我们的基准测试涵盖了 150 多个现实世界网站上的广泛模式，可用于在不同场景中训练和评估代理。由于存在的信息量很大，大型语言模型 (LLM) 无法实时处理整个网页。为了解决这个瓶颈，我们设计了一个受检索启发的模型，通过对相关元素进行排名来有效地修剪 HTML 页面。我们使用选定的元素以及屏幕截图和操作历史记录来评估各种模型在浏览网络时复制人类行为的能力。我们的实验涵盖小型纯文本到专有的多模式法学硕士。我们发现较小的微调解码器超越了最好的零样本 LLM（包括 GPT-4V），但也超越了在屏幕截图上明确预训练的较大微调多模态模型。然而，所有经过微调的模型都很难推广到看不见的网站。我们的研究结果强调了对可以推广到新环境的大型多模式模型的需求。我们的代码、数据和模型可供研究：https://mcgill-nlp.github.io/weblinx</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
