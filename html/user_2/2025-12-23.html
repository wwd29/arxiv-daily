<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-23</h1>
<h3>Title: Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongji Li, Junchi yao, Manjiang Yu, Priyanka Singh, Xue Li, Di Wang, Lijie Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.17911">https://arxiv.org/abs/2512.17911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.17911">https://arxiv.org/pdf/2512.17911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.17911]] Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models(https://arxiv.org/abs/2512.17911)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Machine unlearning aims to erase requested data from trained models without full retraining. For Reasoning Multimodal Large Language Models (RMLLMs), this is uniquely challenging: intermediate chain-of-thought steps can still leak sensitive information even when final answers are forgotten, and overly aggressive interventions easily damage general reasoning ability. Yet no benchmark jointly evaluates how well unlearning methods suppress reasoning-level leakage while preserving reasoning competence. We address this gap with RMLLMU-Bench, the first benchmark for RMLLM unlearning that extends standard forgetting metrics with dedicated measures of reasoning leakage and reasoning retention. A systematic evaluation on RMLLMU-Bench reveals that existing unlearning methods for MLLMs and Large (Language) Reasoning Models (LRMs) either leave substantial leakage in the reasoning process or severely degrade reasoning performance. To address these gaps, we propose R-MUSE (Reasoning-preserving MLLM Unlearning via Subspace guidance and Adaptive Steering), a training-free and inference-time intervention framework that steers internal representations to forget both answers and reasoning traces while explicitly preserving general reasoning. Experiments on RMLLMU-Bench demonstrate that R-MUSE achieves a substantially better balance between effective forgetting and reasoning retention.</li>
<li><strong>摘要：</strong>机器取消学习的目的是在不完全重新训练的情况下从经过训练的模型中删除请求的数据。对于推理多模态大型语言模型（RMLLM）来说，这是一个独特的挑战：即使最终答案被遗忘，中间的思维链步骤仍然可能泄漏敏感信息，而过度激进的干预很容易损害一般推理能力。然而，没有基准可以联合评估遗忘方法在保持推理能力的同时抑制推理级别泄漏的效果。我们通过 RMLLMU-Bench 解决了这一差距，这是 RMLLM 遗忘的第一个基准，它通过专门的推理泄漏和推理保留测量来扩展标准遗忘指标。对 RMLLMU-Bench 的系统评估表明，MLLM 和大型（语言）推理模型 (LRM) 的现有遗忘方法要么在推理过程中留下大量泄漏，要么严重降低推理性能。为了解决这些差距，我们提出了 R-MUSE（通过子空间指导和自适应转向进行推理保留 MLLM Unlearning），这是一种免训练和推理时间干预框架，可引导内部表示忘记答案和推理痕迹，同时明确保留一般推理。 RMLLMU-Bench 上的实验表明，R-MUSE 在有效遗忘和推理保留之间实现了更好的平衡。</li>
</ul>

<h3>Title: Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.17912">https://arxiv.org/abs/2512.17912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.17912">https://arxiv.org/pdf/2512.17912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.17912]] Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning(https://arxiv.org/abs/2512.17912)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>ChatGPT said: Text-attributed graphs, where nodes and edges contain rich textual information, are widely used across diverse domains. A central challenge in this setting is question answering, which requires jointly leveraging unstructured text and the structured relational signals within the graph. Although Large Language Models (LLMs) have made significant advances in natural language understanding, their direct use for reasoning over text-attributed graphs remains limited. Retrieval-augmented generation methods that operate purely on text often treat passages as isolated units, ignoring the interconnected structure of the graph. Conversely, graph-based RAG methods that serialize large subgraphs into long textual sequences quickly become infeasible due to LLM context-length constraints, resulting in fragmented reasoning and degraded accuracy. To overcome these limitations, we introduce Graph-O1, an agentic GraphRAG framework that enables LLMs to conduct stepwise, interactive reasoning over graphs. Our approach integrates Monte Carlo Tree Search (MCTS) with end-to-end reinforcement learning, allowing the model to selectively explore and retrieve only the most informative subgraph components. The reasoning procedure is framed as a multi-turn interaction between the agent and the graph environment, and the agent is trained through a unified reward mechanism. Extensive experiments across multiple LLM backbones demonstrate that Graph-O1 consistently surpasses state-of-the-art baselines, producing answers that are more accurate, reliable, and interpretable.</li>
<li><strong>摘要：</strong>ChatGPT 表示：文本属性图的节点和边包含丰富的文本信息，被广泛应用于不同的领域。这种情况下的一个核心挑战是问答，这需要联合利用图中的非结构化文本和结构化关系信号。尽管大型语言模型 (LLM) 在自然语言理解方面取得了重大进展，但它们直接用于文本属性图推理的用途仍然有限。纯粹对文本进行操作的检索增强生成方法通常将段落视为孤立的单元，而忽略了图形的互连结构。相反，由于 LLM 上下文长度的限制，将大型子图序列化为长文本序列的基于图的 RAG 方法很快变得不可行，从而导致推理碎片化和准确性下降。为了克服这些限制，我们引入了 Graph-O1，这是一种代理 GraphRAG 框架，使法学硕士能够对图进行逐步的交互式推理。我们的方法将蒙特卡罗树搜索（MCTS）与端到端强化学习相结合，使模型能够有选择地探索和检索信息最丰富的子图组件。推理过程被构建为智能体与图环境之间的多轮交互，并通过统一的奖励机制来训练智能体。跨多个 LLM 主干的广泛实验表明，Graph-O1 始终超越最先进的基线，产生更准确、可靠和可解释的答案。</li>
</ul>

<h3>Title: Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression</h3>
<ul>
<li><strong>Authors: </strong>Boris Kriuk, Logic Ng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.17914">https://arxiv.org/abs/2512.17914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.17914">https://arxiv.org/pdf/2512.17914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.17914]] Q-KVComm: Efficient Multi-Agent Communication Via Adaptive KV Cache Compression(https://arxiv.org/abs/2512.17914)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Multi-agent Large Language Model (LLM) systems face a critical bottleneck: redundant transmission of contextual information between agents consumes excessive bandwidth and computational resources. Traditional approaches discard internal semantic representations and transmit raw text, forcing receiving agents to recompute similar representations from scratch. We introduce Q-KVComm, a new protocol that enables direct transmission of compressed key-value (KV) cache representations between LLM agents. Q-KVComm combines three key innovations: (1) adaptive layer-wise quantization that allocates variable bit-widths based on sensitivity profiling, (2) hybrid information extraction that preserves critical facts across content domains, and (3) heterogeneous model calibration establishing cross-architecture communication. Extensive experiments across three diverse question-answering datasets demonstrate that Q-KVComm achieves 5-6x compression ratios while maintaining semantic fidelity, with coherence quality scores above 0.77 across all scenarios. The protocol exhibits robust performance across model sizes (1.1B-1.5B parameters) and adapts to real-world applications including conversational QA and multi-hop reasoning. Our work establishes a new paradigm for LLM agent communication, shifting from text-based to representation-based information exchange.</li>
<li><strong>摘要：</strong>多智能体大型语言模型（LLM）系统面临一个关键瓶颈：智能体之间上下文信息的冗余传输会消耗过多的带宽和计算资源。传统方法丢弃内部语义表示并传输原始文本，迫使接收代理从头开始重新计算类似的表示。我们引入了 Q-KVComm，这是一种新协议，可以在 LLM 代理之间直接传输压缩的键值 (KV) 缓存表示。 Q-KVComm 结合了三项关键创新：(1) 自适应逐层量化，根据灵敏度分析分配可变位宽；(2) 混合信息提取，跨内容域保留关键事实；(3) 建立跨架构通信的异构模型校准。在三个不同的问答数据集上进行的大量实验表明，Q-KVComm 在保持语义保真度的同时实现了 5-6 倍的压缩比，在所有场景中一致性质量得分均高于 0.77。该协议在模型大小（1.1B-1.5B 参数）上表现出强大的性能，并适应现实世界的应用，包括对话式 QA 和多跳推理。我们的工作为法学硕士代理沟通建立了一个新的范式，从基于文本的信息交换转变为基于表示的信息交换。</li>
</ul>

<h3>Title: Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset</h3>
<ul>
<li><strong>Authors: </strong>Nick Rossenbach, Robin Schmitt, Tina Raissi, Simon Berger, Larissa Kleppel, Ralf Schlüter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.17915">https://arxiv.org/abs/2512.17915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.17915">https://arxiv.org/pdf/2512.17915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.17915]] Supplementary Resources and Analysis for Automatic Speech Recognition Systems Trained on the Loquacious Dataset(https://arxiv.org/abs/2512.17915)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>The recently published Loquacious dataset aims to be a replacement for established English automatic speech recognition (ASR) datasets such as LibriSpeech or TED-Lium. The main goal of the Loquacious dataset is to provide properly defined training and test partitions across many acoustic and language domains, with an open license suitable for both academia and industry. To further promote the benchmarking and usability of this new dataset, we present additional resources in the form of n-gram language models (LMs), a grapheme-to-phoneme (G2P) model and pronunciation lexica, with open and public access. Utilizing those additional resources we show experimental results across a wide range of ASR architectures with different label units and topologies. Our initial experimental results indicate that the Loquacious dataset offers a valuable study case for a variety of common challenges in ASR.</li>
<li><strong>摘要：</strong>最近发布的 Loquacious 数据集旨在替代现有的英语自动语音识别 (ASR) 数据集，例如 LibriSpeech 或 TED-Lium。 Loquacious 数据集的主要目标是在许多声学和语言领域提供正确定义的训练和测试分区，并具有适合学术界和工业界的开放许可证。为了进一步促进这个新数据集的基准测试和可用性，我们以 n-gram 语言模型 (LM)、字素到音素 (G2P) 模型和发音词典的形式提供额外的资源，并开放和公共访问。利用这些额外资源，我们展示了具有不同标签单元和拓扑的各种 ASR 架构的实验结果。我们的初步实验结果表明，Loquacious 数据集为 ASR 中的各种常见挑战提供了有价值的研究案例。</li>
</ul>

<h3>Title: KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Aomufei Yuan, Zhiming Wang, Ruijie Miao, Dayu Wang, Yuxuan Tian, Zihan Wang, Yebo Peng, Yuhan Wu, Bairen Yi, Xin Liu, Tong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.17917">https://arxiv.org/abs/2512.17917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.17917">https://arxiv.org/pdf/2512.17917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.17917]] KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction(https://arxiv.org/abs/2512.17917)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As the context length of current large language models (LLMs) rapidly increases, the memory demand for the Key-Value (KV) cache is becoming a bottleneck for LLM deployment and batch processing. Traditional KV cache compression methods typically involve permanently evicting or irreversibly merging "less important" tokens with low attention scores. This approach results in the unrecoverable loss of token information, which we call Contextual Amnesia, significantly degrading the model's information retrieval capability. To address this issue, we propose KVReviver, a reversible KV cache compression method based on the sketch algorithm. This method allows reconstructing compressed tokens from an additional data structure, thus enabling full-scale computation within limited memory. Experiments showed that in 2k-length contexts, it requires only 10% of KV Cache budget while maintaining identical end-to-end inference accuracy. For 32k-length contexts, it achieves equivalent or comparable accuracy ~2% accuracy loss) using merely 25% of KV Cache budget.</li>
<li><strong>摘要：</strong>随着当前大型语言模型（LLM）的上下文长度迅速增加，键值（KV）缓存的内存需求正在成为LLM部署和批处理的瓶颈。传统的 KV 缓存压缩方法通常涉及永久驱逐或不可逆地合并注意力分数较低的“不太重要”的令牌。这种方法会导致令牌信息不可恢复的丢失，我们称之为上下文遗忘症，从而显着降低模型的信息检索能力。为了解决这个问题，我们提出了 KVReviver，一种基于 sketch 算法的可逆 KV 缓存压缩方法。该方法允许从附加数据结构重建压缩令牌，从而在有限的内存内实现全面计算。实验表明，在 2k 长度的上下文中，仅需要 10% 的 KV Cache 预算，同时保持相同的端到端推理精度。对于 32k 长度的上下文，它仅使用 25% 的 KV 缓存预算即可实现等效或相当的精度（约 2% 的精度损失）。</li>
</ul>

<h3>Title: Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression</h3>
<ul>
<li><strong>Authors: </strong>Rahul Baxi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.17920">https://arxiv.org/abs/2512.17920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.17920">https://arxiv.org/pdf/2512.17920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.17920]] Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression(https://arxiv.org/abs/2512.17920)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit degraded performance under prompt compression, but the mechanisms remain poorly understood. We introduce the Compression-Decay Comprehension Test (CDCT), a benchmark that independently measures constraint compliance (CC) and semantic accuracy (SA) across compression levels. We evaluate 9 frontier LLMs across 8 concepts using 5 compression levels from extreme (c=0.0, ~2 words) to none (c=1.0, ~135 words). A three-judge LLM jury achieves almost perfect inter-rater agreement on CC (Fleiss' \k{appa}=0.90). We observe a universal U-curve pattern in constraint compliance (97.2% prevalence), with violations peaking at medium compression (c=0.5, ~27 words). Counterintuitively, models perform better at extreme compression than medium lengths. The dimensions are statistically orthogonal (r=0.193, p=0.084), with constraint effects 2.9x larger than semantic effects. Experimental validation via RLHF ablation confirms our constraint salience hypothesis: removing "helpfulness" signals improves CC by 598% on average (71/72 trials, p<0.001), with 79% achieving perfect compliance. This demonstrates that RLHF-trained helpfulness behaviors are the dominant cause of constraint violations at medium compression. Reasoning models outperform efficient models by 27.5% (Cohen's d=0.96). Our findings reveal a fundamental tension between RLHF alignment and instruction-following, providing actionable guidelines for improving deployed systems.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在即时压缩下表现出性能下降，但其机制仍然知之甚少。我们引入了压缩衰减理解测试 (CDCT)，这是一个独立测量跨压缩级别的约束合规性 (CC) 和语义准确性 (SA) 的基准。我们使用从极端（c=0.0，~2 个单词）到无（c=1.0，~135 个单词）的 5 个压缩级别来评估跨 8 个概念的 9 个前沿法学硕士。由三名法官组成的法学硕士评审团在 CC 上达成了近乎完美的评审者间一致意见（Fleiss' \k{appa}=0.90）。我们观察到约束合规性存在普遍的 U 曲线模式（97.2% 的流行率），其中违规行为在中等压缩时达到峰值（c=0.5，约 27 个字）。与直觉相反，模型在极端压缩下的表现比中等长度的表现更好。这些维度在统计上是正交的（r=0.193，p=0.084），约束效应比语义效应大 2.9 倍。通过 RLHF 消融进行的实验验证证实了我们的约束显着性假设：消除“有用”信号使 CC 平均提高 598%（71/72 试验，p<0.001），其中 79% 实现了完美的依从性。这表明 RLHF 训练的帮助行为是中等压缩时违反约束的主要原因。推理模型的性能比高效模型高 27.5%（Cohen 的 d=0.96）。我们的研究结果揭示了 RLHF 一致性和指令遵循之间的根本紧张关系，为改进已部署的系统提供了可行的指导方针。</li>
</ul>

<h3>Title: CoPE: A Small Language Model for Steerable and Scalable Content Labeling</h3>
<ul>
<li><strong>Authors: </strong>Samidh Chakrabarti, David Willner, Kevin Klyman, Tiffany Saade, Emily Capstick, Sabina Nong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18027">https://arxiv.org/abs/2512.18027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18027">https://arxiv.org/pdf/2512.18027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18027]] CoPE: A Small Language Model for Steerable and Scalable Content Labeling(https://arxiv.org/abs/2512.18027)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper details the methodology behind CoPE, a policy-steerable small language model capable of fast and accurate content labeling. We present a novel training curricula called Contradictory Example Training that enables the model to learn policy interpretation rather than mere policy memorization. We also present a novel method for generating content policies, called Binocular Labeling, which enables rapid construction of unambiguous training datasets. When evaluated across seven different harm areas, CoPE exhibits equal or superior accuracy to frontier models at only 1% of their size. We openly release a 9 billion parameter version of the model that can be run on a single consumer-grade GPU. Models like CoPE represent a paradigm shift for classifier systems. By turning an ML task into a policy writing task, CoPE opens up new design possibilities for the governance of online platforms.</li>
<li><strong>摘要：</strong>本文详细介绍了 CoPE 背后的方法，CoPE 是一种策略可引导的小语言模型，能够快速、准确地进行内容标记。我们提出了一种称为矛盾示例训练的新颖训练课程，使模型能够学习政策解释而不仅仅是政策记忆。我们还提出了一种生成内容策略的新方法，称为双目标记，它可以快速构建明确的训练数据集。在对 7 个不同的危害领域进行评估时，CoPE 表现出与前沿模型相同或更高的精度，而其规模仅为前沿模型的 1%。我们公开发布了该模型的 90 亿参数版本，可以在单个消费级 GPU 上运行。像 CoPE 这样的模型代表了分类器系统的范式转变。通过将机器学习任务转变为策略编写任务，CoPE 为在线平台的治理开辟了新的设计可能性。</li>
</ul>

<h3>Title: Statistical laws and linguistics inform meaning in naturalistic and fictional conversation</h3>
<ul>
<li><strong>Authors: </strong>Ashley M. A. Fehr, Calla G. Beauregard, Julia Witte Zimmerman, Katie Ekström, Pablo Rosillo-Rodes, Christopher M. Danforth, Peter Sheridan Dodds</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18072">https://arxiv.org/abs/2512.18072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18072">https://arxiv.org/pdf/2512.18072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18072]] Statistical laws and linguistics inform meaning in naturalistic and fictional conversation(https://arxiv.org/abs/2512.18072)</code><input type="text"></li>
<li><strong>Keywords: </strong>chat</a></li>
<li><strong>Abstract: </strong>Conversation is a cornerstone of social connection and is linked to well-being outcomes. Conversations vary widely in type with some portion generating complex, dynamic stories. One approach to studying how conversations unfold in time is through statistical patterns such as Heaps' law, which holds that vocabulary size scales with document length. Little work on Heaps's law has looked at conversation and considered how language features impact scaling. We measure Heaps' law for conversations recorded in two distinct mediums: 1. Strangers brought together on video chat and 2. Fictional characters in movies. We find that scaling of vocabulary size differs by parts of speech. We discuss these findings through behavioral and linguistic frameworks.</li>
<li><strong>摘要：</strong>对话是社会联系的基石，与幸福感相关。对话的类型差异很大，有些部分会产生复杂、动态的故事。研究对话如何及时展开的一种方法是通过统计模式，例如堆定律，该定律认为词汇量大小与文档长度成比例。关于堆定律的研究很少关注对话并考虑语言特征如何影响扩展。我们测量了以两种不同媒介记录的对话的希普斯定律：1. 陌生人通过视频聊天聚集在一起；2. 电影中的虚构人物。我们发现词汇量的大小因词性而异。我们通过行为和语言框架讨论这些发现。</li>
</ul>

<h3>Title: Training LLMs with LogicReward for Faithful and Rigorous Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jundong Xu, Hao Fei, Huichi Zhou, Xin Quan, Qijun Huang, Shengqiong Wu, William Yang Wang, Mong-Li Lee, Wynne Hsu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18196">https://arxiv.org/abs/2512.18196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18196">https://arxiv.org/pdf/2512.18196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18196]] Training LLMs with LogicReward for Faithful and Rigorous Reasoning(https://arxiv.org/abs/2512.18196)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\% and 2\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at this https URL.</li>
<li><strong>摘要：</strong>尽管法学硕士表现出强大的推理能力，但现有的培训方法在很大程度上依赖于基于结果的反馈，这可能会在推理有缺陷的情况下产生正确的答案。先前的工作引入了对中间步骤的监督，但仍然缺乏逻辑健全性的保证，这在逻辑一致性至关重要的高风险场景中至关重要。为了解决这个问题，我们提出了 LogicReward，这是一种新颖的奖励系统，通过定理证明者强制执行步骤级逻辑正确性来指导模型训练。我们进一步引入了带有软统一的自动形式化，它减少了自然语言的歧义并提高了形式化质量，从而能够更有效地使用定理证明。使用 LogicReward 构建的数据训练的 8B 模型在自然语言推理和逻辑推理任务上，通过简单的训练程序，比 GPT-4o 和 o4-mini 分别提高了 11.6% 和 2%。进一步的分析表明，LogicReward 增强了推理的忠实度，提高了对数学和常识推理等看不见的任务的概括性，即使没有真实标签，也能提供可靠的奖励信号。我们将在此 https URL 发布所有数据和代码。</li>
</ul>

<h3>Title: LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Guo Chen, Junjie Huang, Huaijin Xie, Fei Sun, Tao Jia</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18329">https://arxiv.org/abs/2512.18329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18329">https://arxiv.org/pdf/2512.18329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18329]] LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation(https://arxiv.org/abs/2512.18329)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 通过将检索到的外部知识纳入生成过程，有效增强了大型语言模型 (LLM)。推理模型提高了多跳 QA 任务中的 LLM 性能，这些任务需要对不同文档中的多个证据进行集成和推理，以回答复杂的问题。然而，它们通常会带来大量的计算成本，包括增加的令牌消耗和推理延迟。为了更好地理解和减轻这种权衡，我们对 RAG 多跳 QA 任务中的推理模型的推理策略进行了全面的研究。我们的研究结果表明，推理模型采用结构化策略来整合检索到的知识和内部知识，主要有两种模式：直接依赖于检索内容的上下文推理，以及使用内部知识解决冲突或差距的知识协调推理。为此，我们提出了一种新颖的 RAG 轻量级重排序推理策略框架（LiR$^3$AG），使非推理模型能够通过将检索到的证据重组为连贯的推理链来转移推理策略。 LiR$^3$AG 显着降低了平均 98% 的输出令牌开销和 58.6% 的推理时间，同时将 8B 非推理模型的 F1 性能提高了 6.2% 到 22.5%，超越了 RAG 中 32B 推理模型的性能，为 RAG 系统提供了一条实用且高效的前进道路。</li>
</ul>

<h3>Title: Towards Efficient Agents: A Co-Design of Inference Architecture and System</h3>
<ul>
<li><strong>Authors: </strong>Weizhe Lin, Hui-Ling Zhen, Shuai Yang, Xian Wang, Renxi Liu, Hanting Chen, Wangze Zhang, Chuansai Zhou, Yiming Li, Chen Chen, Xing Li, Zhiyuan Yang, Xiaosong Li, Xianzhi Yu, Zhenhua Dong, Mingxuan Yuan, Yunhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18337">https://arxiv.org/abs/2512.18337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18337">https://arxiv.org/pdf/2512.18337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18337]] Towards Efficient Agents: A Co-Design of Inference Architecture and System(https://arxiv.org/abs/2512.18337)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.</li>
<li><strong>摘要：</strong>基于大语言模型（LLM）的智能体的快速发展为自主多轮推理和工具增强决策带来了新的可能性。然而，它们在现实世界中的部署受到严重低效的阻碍，这些低效不是由孤立的模型推理引起的，而是由推理循环、上下文增长和异构工具交互中积累的系统延迟引起的。本文介绍了 AgentInfer，这是一个用于端到端代理加速的统一框架，可连接推理优化和架构设计。我们将问题分解为四个协同组件：AgentCollab，一个分层双模型推理框架，通过动态角色分配平衡大模型和小模型的使用； AgentSched，一种缓存感知混合调度程序，可最大限度地减少异构请求模式下的延迟； AgentSAM，一种基于后缀自动机的推测解码方法，重用多会话语义内存来实现低开销的推理加速； AgentCompress，一种语义压缩机制，可以异步提取和重新组织代理内存，而不会中断正在进行的推理。这些模块共同构成了一个自我进化引擎，能够在整个长期推理任务中维持效率和认知稳定性。 BrowseComp-zh 和 DeepDiver 基准测试的实验表明，通过这些方法的协同协作，AgentInfer 将无效令牌消耗减少了 50% 以上，在保持准确性的情况下实现了总体加速 1.8-2.5 倍。这些结果强调，优化代理任务完成（而不仅仅是每个令牌吞吐量）是构建可扩展、高效和自我改进的智能系统的关键。</li>
</ul>

<h3>Title: LLM-based Few-Shot Early Rumor Detection with Imitation Agent</h3>
<ul>
<li><strong>Authors: </strong>Fengzhu Zeng, Qian Shao, Ling Cheng, Wei Gao, Shih-Fen Cheng, Jing Ma, Cheng Niu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18352">https://arxiv.org/abs/2512.18352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18352">https://arxiv.org/pdf/2512.18352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18352]] LLM-based Few-Shot Early Rumor Detection with Imitation Agent(https://arxiv.org/abs/2512.18352)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \textit{early time point determination}, while the LLM serves as a powerful \textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.</li>
<li><strong>摘要：</strong>早期谣言检测 (EARD) 旨在根据一系列社交媒体帖子来确定可以对索赔进行准确分类的最早时间点。这在数据稀缺的环境中尤其具有挑战性。虽然大型语言模型 (LLM) 在少量 NLP 任务中表现良好，但它们不太适合时间序列数据，并且训练和推理的计算成本很高。在这项工作中，我们提出了一种新颖的 EARD 框架，它结合了自主代理和基于 LLM 的检测模型，其中代理充当 \textit{早期时间点确定} 的可靠决策者，而 LLM 充当强大的 \textit{谣言检测器}。这种方法为小样本 EARD 提供了第一个解决方案，只需要训练一个轻量级代理，并允许法学硕士保持免训练。对四个真实世界数据集的广泛实验表明，我们的方法提高了 LLM 的性能，并在准确性和早期性方面超越了现有的 EARD 方法。</li>
</ul>

<h3>Title: DACE For Railway Acronym Disambiguation</h3>
<ul>
<li><strong>Authors: </strong>El Mokhtar Hribach, Oussama Mechhour, Mohammed Elmonstaser, Yassine El Boudouri, Othmane Kabal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18357">https://arxiv.org/abs/2512.18357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18357">https://arxiv.org/pdf/2512.18357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18357]] DACE For Railway Acronym Disambiguation(https://arxiv.org/abs/2512.18357)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.</li>
<li><strong>摘要：</strong>首字母缩略词消歧 (AD) 是技术文本处理中的一项基本挑战，特别是在高度歧义使自动分析变得复杂的专业领域。本文在 TextMine'26 法国铁路文献竞赛的背景下讨论 AD。我们提出了 DACE（动态提示、检索增强生成、上下文选择和集成聚合），这是一个通过自适应上下文学习和外部领域知识注入来增强大型语言模型的框架。通过动态定制首字母缩略词歧义提示和聚合整体预测，DACE 可以减轻幻觉并有效处理资源不足的情况。我们的方法以 0.9069 的 F1 分数在比赛中名列前茅。</li>
</ul>

<h3>Title: LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Lango, Ondřej Dušek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18360">https://arxiv.org/abs/2512.18360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18360">https://arxiv.org/pdf/2512.18360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18360]] LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators(https://arxiv.org/abs/2512.18360)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is "trained" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models</li>
<li><strong>摘要：</strong>我们提出了一种用于 RDF 到文本生成的新型神经符号框架，其中模型是通过多个 LLM 代理之间的协作交互而不是传统的反向传播来“训练”的。 LLM 代理为给定域的生成器生成基于规则的 Python 代码，仅基于 RDF 三元组，没有域内人类参考文本。由此产生的系统是完全可解释的，不需要监督训练数据，并且仅使用单个 CPU 几乎立即生成文本。我们对 WebNLG 和 OpenDialKG 数据的实验表明，我们的方法产生的输出减少了幻觉，与微调或提示的语言模型相比，仅产生轻微的流畅度损失</li>
</ul>

<h3>Title: SRS-Stories: Vocabulary-constrained multilingual story generation for language learning</h3>
<ul>
<li><strong>Authors: </strong>Wiktor Kamzela, Mateusz Lango, Ondrej Dusek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18362">https://arxiv.org/abs/2512.18362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18362">https://arxiv.org/pdf/2512.18362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18362]] SRS-Stories: Vocabulary-constrained multilingual story generation for language learning(https://arxiv.org/abs/2512.18362)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this paper, we use large language models to generate personalized stories for language learners, using only the vocabulary they know. The generated texts are specifically written to teach the user new vocabulary by simply reading stories where it appears in context, while at the same time seamlessly reviewing recently learned vocabulary. The generated stories are enjoyable to read and the vocabulary reviewing/learning is optimized by a Spaced Repetition System. The experiments are conducted in three languages: English, Chinese and Polish, evaluating three story generation methods and three strategies for enforcing lexical constraints. The results show that the generated stories are more grammatical, coherent, and provide better examples of word usage than texts generated by the standard constrained beam search approach</li>
<li><strong>摘要：</strong>在本文中，我们使用大型语言模型为语言学习者生成个性化故事，仅使用他们知道的词汇。生成的文本是专门编写的，用于通过简单地阅读上下文中出现的故事来教用户新词汇，同时无缝地回顾最近学习的词汇。生成的故事读起来很有趣，并且通过间隔重复系统优化了词汇复习/学习。实验以英语、中文和波兰语三种语言进行，评估了三种故事生成方法和三种强制词汇约束的策略。结果表明，与标准约束束搜索方法生成的文本相比，生成的故事更符合语法、更连贯，并且提供了更好的单词用法示例</li>
</ul>

<h3>Title: AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3</h3>
<ul>
<li><strong>Authors: </strong>Mark Kashirskiy, Artiom Lipinski, Ilya Makarov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18399">https://arxiv.org/abs/2512.18399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18399">https://arxiv.org/pdf/2512.18399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18399]] AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3(https://arxiv.org/abs/2512.18399)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Tokenization is a critical preprocessing step for large language models (LLMs), directly impacting training efficiency and downstream performance. General-purpose tokenizers trained predominantly on English and Latin-script languages exhibit suboptimal performance on morphologically rich languages such as Arabic, resulting in inflated token sequences and reduced compression efficiency. In this work, we present AraToken, an Arabic-optimized tokenizer built on SentencePiece Unigram algorithm with a comprehensive normalization pipeline addressing Arabic-specific orthographic variations including Alif variants, diacritics, and Arabic-Indic numerals. We systematically compare BPE, WordPiece, and SentencePiece algorithms across multiple configurations, demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines. Furthermore, we introduce the Language Extension Pipeline (LEP), a method for integrating the optimized tokenizer into Qwen3-0.6B through vocabulary extension with mean subtoken initialization and selective transformer layer unfreezing. Our experiments show that LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples. We release our tokenizer, training scripts, and model checkpoints to facilitate Arabic NLP research.</li>
<li><strong>摘要：</strong>标记化是大型语言模型 (LLM) 的关键预处理步骤，直接影响训练效率和下游性能。主要针对英语和拉丁文字语言训练的通用标记器在形态丰富的语言（例如阿拉伯语）上表现出次优性能，导致标记序列膨胀并降低压缩效率。在这项工作中，我们提出了 AraToken，这是一种基于 SentencePiece Unigram 算法构建的阿拉伯语优化分词器，具有全面的标准化管道，可解决阿拉伯语特定的正字法变化，包括 Alif 变体、变音符号和阿拉伯-印度数字。我们系统地比较了多种配置下的 BPE、WordPiece 和 SentencePiece 算法，证明与非标准化基线相比，标准化的 SentencePiece 的生育率降低了 18%（1.199 vs 1.35 个标记/单词）。此外，我们还引入了语言扩展管道（LEP），这是一种通过词汇扩展、平均子标记初始化和选择性转换器层解冻将优化标记器集成到 Qwen3-0.6B 中的方法。我们的实验表明，LEP 在 100K 阿拉伯语样本的 800 个训练步骤内将评估损失从 8.28 降低到 2.43。我们发布了分词器、训练脚本和模型检查点，以促进阿拉伯语 NLP 研究。</li>
</ul>

<h3>Title: An Agentic AI Framework for Training General Practitioner Student Skills</h3>
<ul>
<li><strong>Authors: </strong>Victor De Marez, Jens Van Nooten, Luna De Bruyne, Walter Daelemans</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18440">https://arxiv.org/abs/2512.18440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18440">https://arxiv.org/pdf/2512.18440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18440]] An Agentic AI Framework for Training General Practitioner Student Skills(https://arxiv.org/abs/2512.18440)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.</li>
<li><strong>摘要：</strong>大语言模型的进步为资源密集型传统方法提供可扩展的替代方案，为增强医学教育中的虚拟模拟患者 (VSP) 提供了强大的潜力。然而，当前的 VSP 经常在医疗准确性、一致的角色扮演、VSP 使用场景生成以及教育结构反馈方面遇到困难。我们引入了一个用于培训全科医生学生技能的代理框架，该框架统一了（i）可配置的、基于证据的小插图生成，（ii）受控的角色驱动的患者对话与可选的检索基础，以及（iii）基于标准的沟通和临床推理的评估和反馈。我们在交互式口头咨询环境中实例化该框架，并与医学生一起对其进行评估（$\mathbf{N{=}14}$）。参与者报告了真实且忠实的小插图对话、适当的难度校准、稳定的个性信号、非常有用的示例丰富的反馈，以及出色的整体可用性。这些结果支持场景控制、交互控制和基于标准的评估的代理分离，作为构建可靠且具有教学价值的 VSP 培训工具的实用模式。</li>
</ul>

<h3>Title: Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling</h3>
<ul>
<li><strong>Authors: </strong>Christopher Román Jaimes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18462">https://arxiv.org/abs/2512.18462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18462">https://arxiv.org/pdf/2512.18462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18462]] Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling(https://arxiv.org/abs/2512.18462)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.</li>
<li><strong>摘要：</strong>自然语言推理 (NLI) 模型经常依赖于虚假相关性而不是语义推理。现有的缓解策略通常会产生高昂的注释成本或在微调过程中引发灾难性遗忘。我们提出了一种自动化、可扩展的管道来解决这些限制。首先，我们引入对数频率 LMI (LF-LMI) 来准确检测语义伪影。其次，我们通过 LLM 合成管道和多法官验证生成高质量的合成对比集。最后，我们引入动态平衡采样，这是一种旋转原始数据分布以防止遗忘的训练策略。我们的方法将具有挑战性的基准的一致性从 63.5% 提高到 81.0%，同时保持 88.4% 的域内准确度，显着优于简单的微调。</li>
</ul>

<h3>Title: LLMs on Drugs: Language Models Are Few-Shot Consumers</h3>
<ul>
<li><strong>Authors: </strong>Alexander Doudkin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18546">https://arxiv.org/abs/2512.18546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18546">https://arxiv.org/pdf/2512.18546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18546]] LLMs on Drugs: Language Models Are Few-Shot Consumers(https://arxiv.org/abs/2512.18546)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level "drug" interventions have never been benchmarked rigorously. We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge. Four single-sentence prompts -- LSD, cocaine, alcohol, and cannabis -- are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests. Control accuracy is 0.45; alcohol collapses to 0.10 (p = 3.2e-8), cocaine to 0.21 (p = 4.9e-4), LSD to 0.19 (p = 1.3e-4), and cannabis to 0.30 (p = 0.041), largely because persona prompts disrupt the mandated "Answer: <LETTER>" template. Persona text therefore behaves like a "few-shot consumable" that can destroy reliability without touching model weights. All experimental code, raw results, and analysis scripts are available at this https URL.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）对推理时强加给他们的角色很敏感，但即时级别的“药物”干预从未经过严格的基准测试。我们首次使用 ARC-Challenge 对 GPT-5-mini 的精神活性框架进行对照研究。四种单句提示——LSD、可卡因、酒精和大麻——与每个条件的 100 个验证项目的清醒控制进行比较，并采用确定性解码、完整记录、威尔逊置信区间和费舍尔精确测试。控制精度0.45；酒精下降到 0.10 (p = 3.2e-8)，可卡因下降到 0.21 (p = 4.9e-4)，LSD 下降到 0.19 (p = 1.3e-4)，大麻下降到 0.30 (p = 0.041)，很大程度上是因为角色提示破坏了规定的“答案：<字母>”模板。因此，人物角色文本的行为就像“少数镜头消耗品”，可以在不影响模型权重的情况下破坏可靠性。所有实验代码、原始结果和分析脚本均可在此 https URL 中获取。</li>
</ul>

<h3>Title: Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering</h3>
<ul>
<li><strong>Authors: </strong>Sungjoon Park, Varun Ramamurthi, Owen Terry</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18551">https://arxiv.org/abs/2512.18551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18551">https://arxiv.org/pdf/2512.18551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18551]] Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering(https://arxiv.org/abs/2512.18551)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>In language modeling, neologisms are new tokens trained to represent a concept not already included in a given model's vocabulary. Neologisms can be used to encourage specific behavior in models, for example by appending prompts with "Give me a neologism answer." Behavioral steering can also be achieved through fine-tuning, albeit with more compute and less flexibility: learning a neologism only trains d parameters and allows the user to still access the model's default behavior. We compare the performance of neologism learning against low-rank adaptation (LoRA) fine-tuning, finding that neologisms outperform fine-tuned models under a matched training setup (same data and hyperparameters). We also investigate self-verbalizations of neologisms, and observe that the model will occasionally make up its own new words when asked about a neologism.</li>
<li><strong>摘要：</strong>在语言建模中，新词是经过训练的新标记，用于表示尚未包含在给定模型词汇中的概念。新词可用于鼓励模型中的特定行为，例如通过附加提示“给我一个新词答案”。行为引导也可以通过微调来实现，尽管需要更多的计算和更少的灵活性：学习新词仅训练 d 参数，并允许用户仍然访问模型的默认行为。我们将新词学习与低秩适应（LoRA）微调的性能进行了比较，发现新词在匹配的训练设置（相同数据和超参数）下优于微调模型。我们还研究了新词的自我表达，并观察到当被问及新词时，模型偶尔会自己造出新词。</li>
</ul>

<h3>Title: On Finding Inconsistencies in Documents</h3>
<ul>
<li><strong>Authors: </strong>Charles J. Lovering, Seth Ebner, Brandon Smock, Michael Krumdick, Saad Rabbani, Ahmed Muhammad, Varshini Reddy, Chris Tanner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18601">https://arxiv.org/abs/2512.18601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18601">https://arxiv.org/pdf/2512.18601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18601]] On Finding Inconsistencies in Documents(https://arxiv.org/abs/2512.18601)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Professionals in academia, law, and finance audit their documents because inconsistencies can result in monetary, reputational, and scientific costs. Language models (LMs) have the potential to dramatically speed up this auditing process. To understand their abilities, we introduce a benchmark, FIND (Finding INconsistencies in Documents), where each example is a document with an inconsistency inserted manually by a domain expert. Despite the documents being long, technical, and complex, the best-performing model (gpt-5) recovered 64% of the inserted inconsistencies. Surprisingly, gpt-5 also found undiscovered inconsistencies present in the original documents. For example, on 50 arXiv papers, we judged 136 out of 196 of the model's suggestions to be legitimate inconsistencies missed by the original authors. However, despite these findings, even the best models miss almost half of the inconsistencies in FIND, demonstrating that inconsistency detection is still a challenging task.</li>
<li><strong>摘要：</strong>学术界、法律和金融界的专业人士会审核他们的文件，因为不一致可能会导致金钱、声誉和科学成本。语言模型 (LM) 有可能显着加快此审核过程。为了了解他们的能力，我们引入了一个基准，FIND（查找文档中的不一致），其中每个示例都是由领域专家手动插入的不一致的文档。尽管文档冗长、技术性强且复杂，但性能最佳的模型 (gpt-5) 恢复了 64% 的插入不一致之处。令人惊讶的是，gpt-5 还发现原始文档中存在未被发现的不一致之处。例如，在 50 篇 arXiv 论文中，我们判断模型的 196 条建议中的 136 条是原作者遗漏的合理不一致之处。然而，尽管有这些发现，即使是最好的模型也会错过 FIND 中近一半的不一致，这表明不一致检测仍然是一项具有挑战性的任务。</li>
</ul>

<h3>Title: A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts</h3>
<ul>
<li><strong>Authors: </strong>Prabigya Acharya, Liza Shrestha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18608">https://arxiv.org/abs/2512.18608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18608">https://arxiv.org/pdf/2512.18608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18608]] A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts(https://arxiv.org/abs/2512.18608)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Automated masking of Personally Identifiable Information (PII) is critical for privacy-preserving conversational systems. While current frontier large language models demonstrate strong PII masking capabilities, concerns about data handling and computational costs motivate exploration of whether lightweight models can achieve comparable performance. We compare encoder-decoder and decoder-only architectures by fine-tuning T5-small and Mistral-Instruct-v0.3 on English datasets constructed from the AI4Privacy benchmark. We create different dataset variants to study label standardization and PII representation, covering 24 standardized PII categories and higher-granularity settings. Evaluation using entity-level and character-level metrics, type accuracy, and exact match shows that both lightweight models achieve performance comparable to frontier LLMs for PII masking tasks. Label normalization consistently improves performance across architectures. Mistral achieves higher F1 and recall with greater robustness across PII types but incurs significantly higher generation latency. T5, while less robust in conversational text, offers more controllable structured outputs and lower inference cost, motivating its use in a real-time Discord bot for real-world PII redaction. Evaluation on live messages reveals performance degradation under informal inputs. These results clarify trade-offs between accuracy, robustness, and computational efficiency, demonstrating that lightweight models can provide effective PII masking while addressing data handling concerns associated with frontier LLMs.</li>
<li><strong>摘要：</strong>自动屏蔽个人身份信息 (PII) 对于隐私保护对话系统至关重要。虽然当前的前沿大语言模型表现出强大的 PII 屏蔽能力，但对数据处理和计算成本的担忧促使人们探索轻量级模型是否能够实现可比的性能。我们通过在根据 AI4Privacy 基准构建的英语数据集上微调 T5-small 和 Mistral-Instruct-v0.3 来比较编码器-解码器和仅解码器架构。我们创建不同的数据集变体来研究标签标准化和 PII 表示，涵盖 24 个标准化 PII 类别和更高粒度的设置。使用实体级和字符级指标、类型准确性和精确匹配进行的评估表明，这两种轻量级模型的性能可与 PII 屏蔽任务的前沿法学硕士相媲美。标签规范化持续提高跨架构的性能。 Mistral 实现了更高的 F1 和召回率，并且在 PII 类型上具有更高的鲁棒性，但会产生明显更高的生成延迟。 T5 虽然在会话文本中不太健壮，但提供了更可控的结构化输出和更低的推理成本，这促使其在实时 Discord 机器人中用于现实世界的 PII 编辑。对实时消息的评估揭示了非正式输入下的性能下降。这些结果阐明了准确性、稳健性和计算效率之间的权衡，证明轻量级模型可以提供有效的 PII 屏蔽，同时解决与前沿法学硕士相关的数据处理问题。</li>
</ul>

<h3>Title: LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction</h3>
<ul>
<li><strong>Authors: </strong>Jensen Zhang, Ningyuan Liu, Yijia Fan, Zihao Huang, Qinglin Zeng, Kaitong Cai, Jian Wang, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18623">https://arxiv.org/abs/2512.18623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18623">https://arxiv.org/pdf/2512.18623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18623]] LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction(https://arxiv.org/abs/2512.18623)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting. We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification. Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常会生成缺乏事实或上下文基础的幻觉内容，从而限制了其在关键应用中的可靠性。现有的方法，例如监督微调和根据人类反馈进行强化学习，数据密集且计算成本昂贵，而静态参数编辑方法则与上下文相关的错误和灾难性遗忘作斗争。我们提出了 LLM-CAS，一个将实时幻觉纠正制定为分层强化学习问题的框架。 LLM-CAS 训练代理学习一种策略，该策略在基于当前上下文的推理过程中动态选择临时神经元扰动。与之前依赖启发式或预定义调整的动态方法不同，这种策略驱动机制可以实现自适应和细粒度的校正，而无需永久参数修改。跨多种语言模型的实验表明，LLM-CAS 持续提高了事实准确性，在 StoryCloze 上提高了 10.98 个百分点，在 TriviaQA 上提高了 2.71 分，在 TruthfulQA 的 MC1 分数上提高了 2.06 分。这些结果优于 ITI 和 CAA 等静态编辑方法以及动态 SADI 框架。总体而言，LLM-CAS 为提高 LLM 的可靠性提供了一种高效且上下文感知的解决方案，并具有未来多模式扩展的广阔潜力。</li>
</ul>

<h3>Title: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital</h3>
<ul>
<li><strong>Authors: </strong>Pierre Colombo, Malik Boudiaf, Allyn Sweet, Michael Desa, Hongxi Wang, Kevin Candra, Syméon del Marmol</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18658">https://arxiv.org/abs/2512.18658</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18658">https://arxiv.org/pdf/2512.18658</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18658]] Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital(https://arxiv.org/abs/2512.18658)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, agent</a></li>
<li><strong>Abstract: </strong>Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.</li>
<li><strong>摘要：</strong>在结束风险投资融资之前，律师会进行尽职调查，包括确定资本表：验证每种证券（例如股票、期权、认股权证）和发行条款（例如归属时间表、加速触发器、转让限制）均得到大量基础法律文件的支持。尽管法学硕士在法律基准上不断改进，但即使对于强大的代理系统，专业的法律工作流程（例如资本绑定）仍然遥不可及。该任务需要多文档推理、严格的证据可追溯性以及当前方法无法可靠提供的确定性输出。我们将资本化绑定描述为法律人工智能现实世界基准的一个实例，分析和比较现有代理系统的性能，并提出一个实现绑定自动化的世界模型架构，更广泛地作为应用法律情报的基础。</li>
</ul>

<h3>Title: Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Li, Handing Wang, Bing Xue, Mengjie Zhang, Yaochu Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18682">https://arxiv.org/abs/2512.18682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18682">https://arxiv.org/pdf/2512.18682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18682]] Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design(https://arxiv.org/abs/2512.18682)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In the high-cost simulation-driven design domain, translating ambiguous design requirements into a mathematical optimization formulation is a bottleneck for optimizing product performance. This process is time-consuming and heavily reliant on expert knowledge. While large language models (LLMs) offer potential for automating this task, existing approaches either suffer from poor formalization that fails to accurately align with the design intent or rely on solver feedback for data filtering, which is unavailable due to the high simulation costs. To address this challenge, we propose APF, a framework for solver-independent, automated problem formulation via LLMs designed to automatically convert engineers' natural language requirements into executable optimization models. The core of this framework is an innovative pipeline for automatically generating high-quality data, which overcomes the difficulty of constructing suitable fine-tuning datasets in the absence of high-cost solver feedback with the help of data generation and test instance annotation. The generated high-quality dataset is used to perform supervised fine-tuning on LLMs, significantly enhancing their ability to generate accurate and executable optimization problem formulations. Experimental results on antenna design demonstrate that APF significantly outperforms the existing methods in both the accuracy of requirement formalization and the quality of resulting radiation efficiency curves in meeting the design goals.</li>
<li><strong>摘要：</strong>在高成本仿真驱动设计领域，将模糊的设计要求转化为数学优化公式是优化产品性能的瓶颈。这个过程非常耗时并且严重依赖专家知识。虽然大型语言模型 (LLM) 提供了自动执行此任务的潜力，但现有方法要么形式化程度低，无法准确符合设计意图，要么依赖求解器反馈进行数据过滤，但由于仿真成本高而无法实现。为了应对这一挑战，我们提出了 APF，这是一个通过法学硕士进行独立于求解器的自动化问题制定的框架，旨在自动将工程师的自然语言需求转换为可执行的优化模型。该框架的核心是自动生成高质量数据的创新管道，借助数据生成和测试实例注释，克服了在缺乏高成本求解器反馈的情况下构建合适的微调数据集的困难。生成的高质量数据集用于对法学硕士进行监督微调，显着增强其生成准确且可执行的优化问题公式的能力。天线设计的实验结果表明，在满足设计目标方面，APF 在需求形式化的准确性和所得辐射效率曲线的质量方面均明显优于现有方法。</li>
</ul>

<h3>Title: MemEvolve: Meta-Evolution of Agent Memory Systems</h3>
<ul>
<li><strong>Authors: </strong>Guibin Zhang, Haotian Ren, Chong Zhan, Zhenhong Zhou, Junhao Wang, He Zhu, Wangchunshu Zhou, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18746">https://arxiv.org/abs/2512.18746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18746">https://arxiv.org/pdf/2512.18746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18746]] MemEvolve: Meta-Evolution of Agent Memory Systems(https://arxiv.org/abs/2512.18746)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.</li>
<li><strong>摘要：</strong>自我进化记忆系统正在前所未有地重塑基于大语言模型（LLM）的智能体的进化范式。之前的工作主要依赖于手动设计的内存架构来存储轨迹、提取经验并合成可重用工具，使代理能够在环境交互中动态发展。然而，这种范式从根本上受到内存系统本身静态性的限制：虽然内存促进了代理级别的发展，但底层内存架构无法元适应不同的任务上下文。为了解决这一差距，我们提出了 MemEvolve，这是一个元进化框架，它共同进化智能体的经验知识及其记忆架构，使智能体系统不仅能够积累经验，而且能够逐步完善它们从中学习的方式。为了使 MemEvolve 融入先前的研究并促进未来自进化系统的开放性，我们引入了 EvolveLab，这是一个统一的自进化内存代码库，它将 12 个代表性内存系统提炼成模块化设计空间（编码、存储、检索、管理），提供标准化的实现基础和公平的实验舞台。对四个具有挑战性的代理基准的广泛评估表明，MemEvolve 实现了 (I) 显着的性能提升，将 SmolAgent 和 Flash-Searcher 等框架改进了高达 17.06\%$； (II) 强大的跨任务和跨法学硕士泛化，设计可跨不同基准和骨干模型有效传输的内存架构。</li>
</ul>

<h3>Title: From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure</h3>
<ul>
<li><strong>Authors: </strong>Thorsten Hellert, Nikolay Agladze, Alex Giovannone, Jan Jug, Frank Mayet, Mark Sherwin, Antonin Sulc, Chris Tennant</a></li>
<li><strong>Subjects: </strong>cs.CL, physics.acc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18779">https://arxiv.org/abs/2512.18779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18779">https://arxiv.org/pdf/2512.18779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18779]] From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure(https://arxiv.org/abs/2512.18779)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Modern experimental platforms such as particle accelerators, fusion devices, telescopes, and industrial process control systems expose tens to hundreds of thousands of control and diagnostic channels accumulated over decades of evolution. Operators and AI systems rely on informal expert knowledge, inconsistent naming conventions, and fragmented documentation to locate signals for monitoring, troubleshooting, and automated control, creating a persistent bottleneck for reliability, scalability, and language-model-driven interfaces. We formalize semantic channel finding-mapping natural-language intent to concrete control-system signals-as a general problem in complex experimental infrastructure, and introduce a four-paradigm framework to guide architecture selection across facility-specific data regimes. The paradigms span (i) direct in-context lookup over curated channel dictionaries, (ii) constrained hierarchical navigation through structured trees, (iii) interactive agent exploration using iterative reasoning and tool-based database queries, and (iv) ontology-grounded semantic search that decouples channel meaning from facility-specific naming conventions. We demonstrate each paradigm through proof-of-concept implementations at four operational facilities spanning two orders of magnitude in scale-from compact free-electron lasers to large synchrotron light sources-and diverse control-system architectures, from clean hierarchies to legacy environments. These implementations achieve 90-97% accuracy on expert-curated operational queries.</li>
<li><strong>摘要：</strong>粒子加速器、聚变装置、望远镜和工业过程控制系统等现代实验平台暴露了数十年演化过程中积累的数万至数十万个控制和诊断通道。操作员和人工智能系统依靠非正式的专家知识、不一致的命名约定和碎片化的文档来定位信号以进行监控、故障排除和自动化控制，从而为可靠性、可扩展性和语言模型驱动的接口造成了持续的瓶颈。我们将语义通道查找（将自然语言意图映射到具体控制系统信号）形式化为复杂实验基础设施中的普遍问题，并引入四范式框架来指导跨设施特定数据机制的架构选择。这些范式涵盖（i）对精选频道字典进行直接上下文查找，（ii）通过结构化树进行约束层次导航，（iii）使用迭代推理和基于工具的数据库查询进行交互式代理探索，以及（iv）基于本体的语义搜索，将频道含义与特定于设施的命名约定分离。我们通过在规模跨越两个数量级的四个运营设施（从紧凑型自由电子激光器到大型同步加速器光源）和多样化的控制系统架构（从干净的层次结构到遗留环境）的概念验证实施来演示每个范例。这些实现在专家策划的操作查询上实现了 90-97% 的准确率。</li>
</ul>

<h3>Title: From Word to World: Can Large Language Models be Implicit Text-based World Models?</h3>
<ul>
<li><strong>Authors: </strong>Yixia Li, Hongru Wang, Jiahao Qiu, Zhenfei Yin, Dongdong Zhang, Cheng Qian, Zeping Li, Pony Ma, Guanhua Chen, Heng Ji, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18832">https://arxiv.org/abs/2512.18832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18832">https://arxiv.org/pdf/2512.18832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18832]] From Word to World: Can Large Language Models be Implicit Text-based World Models?(https://arxiv.org/abs/2512.18832)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.</li>
<li><strong>摘要：</strong>代理强化学习越来越依赖于经验驱动的扩展，但现实世界的环境仍然不具有适应性、覆盖范围有限且难以扩展。世界模型提供了一种通过模拟体验来提高学习效率的潜在方法，但目前尚不清楚大型语言模型是否能够可靠地发挥这一作用，以及在什么条件下它们可以使智能体受益。我们在基于文本的环境中研究这些问题，该环境提供了一种受控设置，可以将语言模型重新解释为交互下的下一状态预测。我们引入了一个三级框架来评估基于 LLM 的世界模型：（i）保真度和一致性，（ii）可扩展性和鲁棒性，以及（iii）代理效用。在五个代表性环境中，我们发现经过充分训练的世界模型保持一致的潜在状态，可根据数据和模型大小进行可预测的扩展，并通过动作验证、合成轨迹生成和热启动强化学习来提高代理性能。同时，这些收益主要取决于行为覆盖范围和环境复杂性，从而为世界建模何时有效支持代理学习划定了明确的界限。</li>
</ul>

<h3>Title: MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tung Duong Ta, Tim Oates</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18841">https://arxiv.org/abs/2512.18841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18841">https://arxiv.org/pdf/2512.18841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18841]] MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models(https://arxiv.org/abs/2512.18841)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Despite advances in mathematical reasoning capabilities, Large Language Models (LLMs) still struggle with calculation verification when using established prompting techniques. We present MDToC (Metacognitive Dynamic Tree of Concepts), a three-phase approach that constructs a concept tree, develops accuracy-verified calculations for each concept, and employs majority voting to evaluate competing solutions. Evaluations across CHAMP, MATH, and Game-of-24 benchmarks demonstrate our MDToC's effectiveness, with GPT-4-Turbo achieving 58.1\% on CHAMP, 86.6\% on MATH, and 85\% on Game-of-24 - outperforming GoT by 5\%, 5.4\%, and 4\% on all these tasks, respectively, without hand-engineered hints. MDToC consistently surpasses existing prompting methods across all backbone models, yielding improvements of up to 7.6\% over ToT and 6.2\% over GoT, establishing metacognitive calculation verification as a promising direction for enhanced mathematical reasoning.</li>
<li><strong>摘要：</strong>尽管数学推理能力取得了进步，但大型语言模型 (LLM) 在使用已建立的提示技术时仍然难以进行计算验证。我们提出 MDToC（元认知动态概念树），这是一种三阶段方法，可构建概念树，为每个概念开发经过准确性验证的计算，并采用多数投票来评估竞争解决方案。对 CHAMP、MATH 和 Game-of-24 基准的评估证明了我们的 MDToC 的有效性，GPT-4-Turbo 在 CHAMP 上实现了 58.1\%，在 MATH 上实现了 86.6\%，在 Game-of-24 上实现了 85\% - 在所有这些任务上分别比 GoT 好 5\%、5.4\% 和 4\%，而无需手工设计提示。 MDToC 在所有骨干模型上始终超越现有的提示方法，比 ToT 提高了 7.6%，比 GoT 提高了 6.2%，将元认知计算验证确立为增强数学推理的一个有希望的方向。</li>
</ul>

<h3>Title: Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction</h3>
<ul>
<li><strong>Authors: </strong>Ming Li, Han Chen, Yunze Xiao, Jian Chen, Hong Jiao, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18880">https://arxiv.org/abs/2512.18880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18880">https://arxiv.org/pdf/2512.18880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18880]] Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction(https://arxiv.org/abs/2512.18880)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.</li>
<li><strong>摘要：</strong>准确估计项目（问题或任务）难度对于教育评估至关重要，但会遇到冷启动问题。虽然大型语言模型展示了超人的问题解决能力，但它们是否能够感知人类学习者的认知斗争仍然是一个悬而未决的问题。在这项工作中，我们对医学知识和数学推理等不同领域的 20 多个模型进行了大规模的人类与人工智能难度匹配的实证分析。我们的研究结果揭示了系统性的失调，即扩大模型大小并没有可靠的帮助；模型不是与人类保持一致，而是趋向于共享的机器共识。我们观察到，高性能往往会阻碍准确的难度估计，因为即使明确提示采用特定的熟练程度，模型也难以模拟学生的能力限制。此外，我们发现严重缺乏内省，因为模型无法预测其自身的局限性。这些结果表明，一般解决问题的能力并不意味着理解人类的认知斗争，凸显了使用当前模型进行自动难度预测的挑战。</li>
</ul>

<h3>Title: Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations</h3>
<ul>
<li><strong>Authors: </strong>Shaomu Tan, Ryosuke Mitani, Ritvik Choudhary, Qiyu Wu, Toshiyuki Sekiya, Christof Monz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18906">https://arxiv.org/abs/2512.18906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18906">https://arxiv.org/pdf/2512.18906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18906]] Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations(https://arxiv.org/abs/2512.18906)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Over the years, automatic MT metrics have hillclimbed benchmarks and presented strong and sometimes human-level agreement with human ratings. Yet they remain black-box, offering little insight into their decision-making and often failing under real-world out-of-distribution (OOD) inputs. We introduce Remedy-R, a reasoning-driven generative MT metric trained with reinforcement learning from pairwise translation preferences, without requiring error-span annotations or distillation from closed LLMs. Remedy-R produces step-by-step analyses of accuracy, fluency, and completeness, followed by a final score, enabling more interpretable assessments. With only 60K training pairs across two language pairs, Remedy-R remains competitive with top scalar metrics and GPT-4-based judges on WMT22-24 meta-evaluation, generalizes to other languages, and exhibits strong robustness on OOD stress tests. Moreover, Remedy-R models generate self-reflective feedback that can be reused for translation improvement. Building on this finding, we introduce Remedy-R Agent, a simple evaluate-revise pipeline that leverages Remedy-R's evaluation analysis to refine translations. This agent consistently improves translation quality across diverse models, including Qwen2.5, ALMA-R, GPT-4o-mini, and Gemini-2.0-Flash, suggesting that Remedy-R's reasoning captures translation-relevant information and is practically useful.</li>
<li><strong>摘要：</strong>多年来，自动机器翻译指标已经超越了基准，并与人类评级表现出强烈的、有时甚至是人类水平的一致性。然而，他们仍然是黑匣子，无法深入了解他们的决策，并且经常在现实世界的分配外（OOD）输入下失败。我们引入了 Remedy-R，这是一种推理驱动的生成 MT 指标，通过成对翻译偏好的强化学习进行训练，无需错误跨度注释或从封闭的法学硕士中进行蒸馏。 Remedy-R 对准确性、流畅性和完整性进行逐步分析，然后给出最终分数，从而实现更具可解释性的评估。 Remedy-R 在两种语言对上仅提供 60K 训练对，在 WMT22-24 元评估中与顶级标量指标和基于 GPT-4 的评判者保持竞争力，可推广到其他语言，并在 OOD 压力测试中表现出强大的鲁棒性。此外，Remedy-R 模型会生成自我反思反馈，可重复用于翻译改进。基于这一发现，我们引入了 Remedy-R Agent，这是一个简单的评估-修改管道，利用 Remedy-R 的评估分析来完善翻译。该代理持续提高了不同模型（包括 Qwen2.5、ALMA-R、GPT-4o-mini 和 Gemini-2.0-Flash）的翻译质量，这表明 Remedy-R 的推理捕获了翻译相关信息并且实际上很有用。</li>
</ul>

<h3>Title: FASTRIC: Prompt Specification Language for Verifiable LLM Interactions</h3>
<ul>
<li><strong>Authors: </strong>Wen-Long Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18940">https://arxiv.org/abs/2512.18940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18940">https://arxiv.org/pdf/2512.18940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18940]] FASTRIC: Prompt Specification Language for Verifiable LLM Interactions(https://arxiv.org/abs/2512.18940)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-"Goldilocks zones"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 执行复杂的多轮交互协议，但缺乏正式规范来根据设计者意图验证执行情况。我们引入了 FASTRIC，一种提示规范语言，它使隐式有限状态机 (FSM) 在自然语言提示中显式化，从而通过执行跟踪分析实现一致性验证。 LLM 充当智能执行代理：解释设计者编码的 FSM 来执行指定的行为角色。与需要解析器和编译器的符号规范语言不同，FASTRIC 利用 LLM 作为统一的基础设施，同时包含解析器、解释器、运行时环境和开发助手。 FASTRIC 指导设计人员阐明构建多轮交互的七个 FSM 元素（最终状态、代理、状态、触发器、角色、初始状态、约束）。规范形式——从前沿模型推断的隐式描述到较弱模型的显式分步指令——充当设计参数。我们引入程序一致性作为衡量执行是否符合 FSM 规范的验证指标。通过四个形式级别和三个模型尺度（14.7B、685B、1T+ 参数）测试三州幼儿园辅导 FSM，结果表明最佳规范形式是模型容量的函数。 DeepSeek-V3.2（685B）在L2-L4上实现了完美的一致性（1.00）； ChatGPT-5 (~1T) 在 L3 (0.90) 处达到峰值，然后在 L4 (0.39) 处崩溃； Phi4 (14.7B) 没有表现出具有高方差的稳定最优值 (SD=0.16-0.36)。这些发现揭示了模型特定的形式范围——“金发姑娘区”——其中规范提供了足够的结构而没有过度约束，建立了快速规范工程来创建可验证的交互协议，将多轮交互设计从启发式艺术转变为具有可测量的程序保证的系统工程。</li>
</ul>

<h3>Title: Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework</h3>
<ul>
<li><strong>Authors: </strong>Jinyan Liu, Zikang Chen, Qinchuan Wang, Tan Xie, Heming Zheng, Xudong Lv</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.18999">https://arxiv.org/abs/2512.18999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.18999">https://arxiv.org/pdf/2512.18999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.18999]] Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework(https://arxiv.org/abs/2512.18999)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.</li>
<li><strong>摘要：</strong>当以端到端的方式直接应用于医疗随访任务时，由于随访表格的复杂性，大型语言模型（LLM）经常会出现不受控制的对话流和不准确的信息提取。为了解决这个限制，我们设计并比较了两个后续聊天机器人系统：一个基于LLM的端到端系统（对照组）和一个具有结构化过程控制的模块化管道（实验组）。实验结果表明，虽然端到端方法经常在冗长而复杂的形式上失败，但我们基于任务分解、语义聚类和流程管理的模块化方法大大提高了对话稳定性和提取准确性。此外，对话次数减少了46.73%，代币消耗减少了80%至87.5%。这些发现强调了在高风险医疗随访场景中部署法学硕士时整合外部控制机制的必要性。</li>
</ul>

<h3>Title: Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19004">https://arxiv.org/abs/2512.19004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19004">https://arxiv.org/pdf/2512.19004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19004]] Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models(https://arxiv.org/abs/2512.19004)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization. We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.</li>
<li><strong>摘要：</strong>扩散大型语言模型 (DLLM) 支持完全并行的标记解码，但在推理时通常仍然不切实际，因为需要进行多次去噪迭代才能将无信息、完全屏蔽的初始化细化为连贯的文本。大多数现有的加速方法侧重于通过改进的求解器或采样策略更有效地遍历该生成轨迹。我们提出了一个补充的观点：通过上下文感知初始化开始更接近目标分布，从而缩短轨迹本身。我们提出了一个免训练接口，它将来自轻量级辅助模型的提示条件先验注入到扩散初始化中，并用两种机制实例化它：离散令牌注入和表示级嵌入插值。由于注入的先验可能是不完美的，并且仅取消掩码的解码可能会过早过度提交，因此我们还引入了一种简单的基于置信度的重新掩码机制作为先验怀疑的一种形式。 GSM8K 的初步证据表明，上下文感知初始化可以大大减少去噪迭代（在我们的设置中大约减少 35% 的函数评估），同时也暴露了一个关键的开放挑战：相对于强扩散基线，朴素的热启动会降低最终的准确性。我们利用这些发现来推动围绕校准、修正机制和表示对齐的研究议程，以实现可靠的热启动扩散解码。</li>
</ul>

<h3>Title: DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation</h3>
<ul>
<li><strong>Authors: </strong>Shijian Ma, Yunqi Huang, Yan Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19012">https://arxiv.org/abs/2512.19012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19012">https://arxiv.org/pdf/2512.19012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19012]] DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation(https://arxiv.org/abs/2512.19012)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.</li>
<li><strong>摘要：</strong>戏剧剧本的延续要求模型保持人物一致性，连贯地推进情节，并保留现有基准无法全面评估的戏剧结构能力。我们推出了 DramaBench，这是第一个评估戏剧剧本延续性的大型基准，涵盖六个独立维度：格式标准、叙事效率、人物一致性、情感深度、逻辑一致性和冲突处理。我们的框架将基于规则的分析与基于法学硕士的标签和统计指标相结合，确保客观且可重复的评估。我们对 8 个最先进的语言模型对 1,103 个脚本（总共 8,824 个评估）进行了综合评估，并进行了严格的统计显着性测试（252 个成对比较，65.9% 显着性）和人工验证（188 个脚本，在 3/5 维度上基本一致）。我们的消融研究证实所有六个维度都捕获了独立的质量方面（平均值 | r | = 0.020）。 DramaBench 为模型改进提供可操作的、特定维度的反馈，并为创意写作评估建立了严格的标准。</li>
</ul>

<h3>Title: A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Zhang, Chao Wang, Zhuo Chen, Lei Chen, Chiyi Li, Kai Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19092">https://arxiv.org/abs/2512.19092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19092">https://arxiv.org/pdf/2512.19092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19092]] A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs(https://arxiv.org/abs/2512.19092)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.</li>
<li><strong>摘要：</strong>由于现实世界 KG 固有的不完整性和逻辑查询结构的组成复杂性，使用一阶逻辑 (FOL) 查询对知识图 (KG) 进行推理具有挑战性。大多数现有方法依赖于将实体和关系嵌入到连续的几何空间中，并通过可微集运算回答查询。虽然这些方法对于简单的查询模式有效，但通常很难推广到涉及多个运算符、更深层次的推理链或异构 KG 模式的复杂查询。我们提出了 ROG（使用大型语言模型对知识图进行推理），这是一种集成式框架，它将查询感知的 KG 邻域检索与基于大型语言模型 (LLM) 的思想链推理相结合。 ROG 将复杂的 FOL 查询分解为更简单的子查询序列，检索紧凑的、与查询相关的子图作为上下文证据，并使用 LLM 执行逐步逻辑推理，从而避免了特定于任务的嵌入优化的需要。标准 KG 推理基准的实验表明，ROG 在平均倒数排名 (MRR) 方面始终优于基于嵌入的强大基线，在高复杂性查询类型上的收益尤其显着。这些结果表明，将结构化知识图谱检索与法学硕士驱动的逻辑推理相结合，为复杂的知识图谱推理任务提供了稳健且有效的替代方案。</li>
</ul>

<h3>Title: Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?</h3>
<ul>
<li><strong>Authors: </strong>Amar Lakel (MICA)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19117">https://arxiv.org/abs/2512.19117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19117">https://arxiv.org/pdf/2512.19117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19117]] Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?(https://arxiv.org/abs/2512.19117)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.</li>
<li><strong>摘要：</strong>本文提出了大型生成模型分析的认识论转变，用“大型话语模型”（LDM）类别取代“大型语言模型”（LLM）类别，然后用人工话语代理（ADA）类别取代。该理论框架基于本体论三元组，区分了三个调节实例：对指称世界现象规律的理解、具身认知的结构化以及社会历史背景下话语的结构语言沉积。 LDM 在这三个实例（文档）的产物上运行，对由学习语料库具体化的部分人类经验的话语投影进行建模。拟议的计划旨在用公共审判和程序取代“迷恋/恐惧”二分法，使人造话语主体在当代社会空间中的位置、用途和限制变得可解读，并将这种方法置于涉及国家、工业、民间社会和学术界的治理和共同监管的视角下。</li>
</ul>

<h3>Title: SAP: Syntactic Attention Pruning for Transformer-based Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Yun Lee, Ding-Yong Hong, Jan-Jan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19125">https://arxiv.org/abs/2512.19125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19125">https://arxiv.org/pdf/2512.19125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19125]] SAP: Syntactic Attention Pruning for Transformer-based Language Models(https://arxiv.org/abs/2512.19125)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.</li>
<li><strong>摘要：</strong>本文介绍了句法注意力修剪（SAP），这是一种在 Transformer 模型中有效修剪注意力头的新方法。与仅依赖于模型权重和激活的数学分析的传统方法不同，SAP 结合了句子的句法结构和注意力模式来指导修剪过程。通过利用这些语言功能，SAP 不仅实现了与最先进方法相媲美的性能，而且还增强了模型行为的可解释性。为了进一步提高鲁棒性，我们提出了候选过滤（CF），这是一种根据头部对模型性能的贡献来确定头部优先级的机制，从而减轻剪枝期间的退化。实验结果表明，SAP 有效地保留了高密度强注意力值的关键头，在无重新训练的环境中优于现有的头修剪策略。这些发现使 SAP 成为模型压缩研究新方向的有前途的基础，为所有基于 Transformer 的语言模型的修剪提供了高度灵活性。</li>
</ul>

<h3>Title: AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards</h3>
<ul>
<li><strong>Authors: </strong>Zihan Lin, Xiaohan Wang, Hexiong Yang, Jiajun Chai, Jie Cao, Guojun Yin, Wei Lin, Ran He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19126">https://arxiv.org/abs/2512.19126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19126">https://arxiv.org/pdf/2512.19126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19126]] AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards(https://arxiv.org/abs/2512.19126)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.</li>
<li><strong>摘要：</strong>虽然强化学习 (RL) 在使用可验证的结果奖励来训练使用工具的大型语言模型 (LLM) 方面显示出前景，但现有方法在很大程度上忽视了显式推理奖励在增强推理和工具利用率方面的潜力。此外，本地结合推理和结果奖励可能会产生次优的性能或与主要优化目标相冲突。为了解决这个问题，我们提出了优势加权策略优化（AWPO）——一种有原则的强化学习框架，可以有效地整合显式推理奖励以增强工具使用能力。 AWPO 结合了方差感知门控和难度感知加权，可根据组相关统计数据自适应地调节推理信号的优势，并采用定制的裁剪机制实现稳定优化。大量实验表明，AWPO 在标准工具使用基准测试中实现了最先进的性能，在具有挑战性的多轮场景中显着优于强大的基准和领先的闭源模型。值得注意的是，凭借出色的参数效率，我们的 4B 模型在多圈精度方面比 Grok-4 高出 16.0%，同时保留了分布外 MMLU-Pro 基准的泛化能力。</li>
</ul>

<h3>Title: QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Dehai Min, Kailin Zhang, Tongtong Wu, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19134">https://arxiv.org/abs/2512.19134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19134">https://arxiv.org/pdf/2512.19134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19134]] QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation(https://arxiv.org/abs/2512.19134)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at this https URL.</li>
<li><strong>摘要：</strong>动态检索增强生成自适应地确定生成期间何时检索，以减轻大语言模型 (LLM) 中的幻觉。然而，现有方法依赖于模型内部信号（例如，逻辑、熵），这些信号从根本上来说是不可靠的，因为 LLM 通常校准不当，并且经常对错误输出表现出很高的置信度。我们提出了 QuCo-RAG，它将主观置信度转变为根据预训练数据计算的客观统计数据。我们的方法通过两个阶段量化不确定性：（1）在生成之前，我们识别表明长尾知识差距的低频实体； （2）在生成过程中，我们验证预训练语料库中的实体共现，其中零共现通常表示幻觉风险。这两个阶段都利用 Infini-gram 对超过 4 万亿个令牌进行毫秒延迟查询，在不确定性较高时触发检索。多跳 QA 基准实验表明，QuCo-RAG 的 EM 增益比 OLMo-2 模型的最先进基线提高了 5--12 点，并且有效地转移到具有未公开的预训练数据的模型（Llama、Qwen、GPT），将 EM 提高了多达 14 点。生物医学 QA 领域的泛化进一步验证了我们范例的稳健性。这些结果将基于语料库的验证确立为动态 RAG 的有原则的、实际上与模型无关的范例。我们的代码可通过此 https URL 公开获取。</li>
</ul>

<h3>Title: Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Anna-Maria Gueorguieva, Aylin Caliskan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19238">https://arxiv.org/abs/2512.19238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19238">https://arxiv.org/pdf/2512.19238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19238]] Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation(https://arxiv.org/abs/2512.19238)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）已被证明表现出社会偏见，然而，对不受保护的污名化身份的偏见仍未得到充分研究。此外，耻辱的哪些社会特征与法学硕士输出的偏见相关尚不清楚。心理学文献表明，耻辱包含六个共同的社会特征：美学、隐蔽性、过程、破坏性、起源和危险。在这项研究中，我们调查了人类和法学硕士对污名特征的评分以及污名的提示风格和类型是否对法学硕士输出中对污名群体的偏见产生影响。我们使用 SocialStigmaQA 衡量三个广泛使用的法学硕士（Granite 3.0-8B、Llama-3.1-8B、Mistral-7B）中 93 个被污名化群体的偏见，该基准包括 37 个有关污名化身份的社会场景；例如决定是否推荐他们实习。我们发现，被人类评为高度危险的耻辱（例如，成为帮派成员或感染艾滋病毒）在 SocialStigmaQA 提示中具有最大的偏见输出（占所有模型输出的 60％），而社会人口耻辱（例如亚裔美国人或老年人）具有最少的偏见输出（11％）。我们测试是否可以通过使用护栏模型来减少有偏差的输出量，该模型旨在识别有害输入，使用每个法学硕士各自的护栏模型（Granite Guardian 3.0、Llama Guard 3.0、Mistral Moderation API）。我们发现偏差显着分别降低了 10.4%、1.4% 和 7.8%。然而，我们表明，对偏见有显着影响的特征在缓解后保持不变，并且护栏模型通常无法识别提示中偏见的意图。这项工作对于在涉及污名化群体的场景中使用法学硕士具有重要意义，我们建议未来努力改进护栏模型以减轻偏见。</li>
</ul>

<h3>Title: ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mingxu Zhang, Dazhong Shen, Qi Zhang, Ying Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19240">https://arxiv.org/abs/2512.19240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19240">https://arxiv.org/pdf/2512.19240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19240]] ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models(https://arxiv.org/abs/2512.19240)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 表现出强大的一般推理能力，但由于标准字符串表示中缺乏明确的化学先验，因此在分子科学中举步维艰。当前的解决方案面临着根本性的困境。基于训练的方法将先验注入参数中，但这种静态耦合阻碍了知识的快速更新，并且常常损害模型的一般推理能力。相反，现有的免训练方法避免了这些问题，但依赖于表面级提示，无法提供精确化学推理所必需的细粒度原子级先验。为了解决这个问题，我们引入了 ChemATP，一个将化学知识与推理引擎解耦的框架。通过构建第一个原子级文本知识库，ChemATP 使冻结的法学硕士能够动态地显式检索和推理该信息。这种架构确保了可解释性和适应性，同时保留了法学硕士固有的通用智能。实验表明，ChemATP 的性能显着优于无训练基线，并且可与最先进的基于训练的模型相媲美，这表明显式事先注入是隐式参数更新的一种有竞争力的替代方案。</li>
</ul>

<h3>Title: Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics</h3>
<ul>
<li><strong>Authors: </strong>Do Minh Duc, Quan Xuan Truong, Nguyen Tat Dat, Nguyen Van Vinh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19247">https://arxiv.org/abs/2512.19247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19247">https://arxiv.org/pdf/2512.19247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19247]] Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics(https://arxiv.org/abs/2512.19247)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, retrieval-augmented generation, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.</li>
<li><strong>摘要：</strong>快速工程在使大型语言模型 (LLM) 适应复杂的推理和标记任务方面发挥着关键作用，而无需进行大量的微调。在本文中，我们提出了一种用于物流文本中的帧检测的新颖的提示优化管道，结合检索增强生成（RAG）、少样本提示、思想链（CoT）推理和自动 CoT 合成（Auto-CoT）来生成高效的特定于任务的提示。我们方法的核心是基于 LLM 的提示优化器代理，它使用检索到的示例、性能反馈和内部自我评估迭代地完善提示。我们的框架是在现实世界的物流文本注释任务上进行评估的，其中推理准确性和标签效率至关重要。实验结果表明，与基线零样本或静态提示相比，优化的提示（尤其是通过 Auto-CoT 和 RAG 增强的提示）可将现实世界的推理精度提高高达 15%。该系统在多个 LLM 中展示了一致的改进，包括 GPT-4o、Qwen 2.5 (72B) 和 LLaMA 3.1 (70B)，验证了其普遍性和实用价值。这些发现表明，结构化提示优化是全面微调的可行替代方案，为在物流等特定领域的 NLP 应用中部署 LLM 提供可扩展的解决方案。</li>
</ul>

<h3>Title: CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs</h3>
<ul>
<li><strong>Authors: </strong>Javier Vela-Tambo, Jorge Gracia, Fernando Dominguez-Castro</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19305">https://arxiv.org/abs/2512.19305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19305">https://arxiv.org/pdf/2512.19305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19305]] CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs(https://arxiv.org/abs/2512.19305)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.</li>
<li><strong>摘要：</strong>了解和监测气候灾害的社会经济影响需要从大规模的异构新闻文章中提取结构化信息。为此，我们开发了 CienaLLM，这是一个基于模式引导的生成信息提取的模块化框架。 CienaLLM 使用开放式大型语言模型从新闻文章中进行零样本信息提取，并支持可配置的提示和输出模式、多步骤管道以及云或本地推理。为了系统地评估法学硕士系列、规模、精度制度和提示策略的选择如何影响绩效，我们对模型、精度和提示工程技术进行了大型析因研究。额外的响应解析步骤几乎消除了格式错误，同时保持了准确性；较大的模型可提供最强大、最稳定的性能，而量化可在适度的精度权衡下提供显着的效率增益；提示策略显示出异质的、特定于模型的效果。 CienaLLM 在从西班牙新闻中提取干旱影响的准确性方面匹配或优于监督基线，尽管推理成本较高。在干旱中进行评估时，模式驱动和模型不可知的设计适合通过编辑提示和模式而不是重新训练来适应相关的信息提取任务（例如其他灾害、部门或语言）。我们发布代码、配置和模式以支持可重复使用。</li>
</ul>

<h3>Title: HATS: High-Accuracy Triple-Set Watermarking for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiqing Hu, Chenxu Zhao, Jiazhong Lu, Xiaolei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19378">https://arxiv.org/abs/2512.19378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19378">https://arxiv.org/pdf/2512.19378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19378]] HATS: High-Accuracy Triple-Set Watermarking for Large Language Models(https://arxiv.org/abs/2512.19378)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.</li>
<li><strong>摘要：</strong>可以通过将隐式信号嵌入到输出中的水印技术来遏制 LLM 生成的文本的滥用。我们提出了一种水印，它将每个解码步骤的词汇划分为具有固定比率的三个集合（绿色/黄色/红色），并将采样限制为绿色和黄色集合。在检测时，我们重播相同的分区，计算绿色富集和红色消耗统计数据，将它们转换为单侧 z 分数，并通过 Fisher 方法聚合它们的 p 值来确定段落是否带有水印。我们在 Llama 2 7B 上实现生成、检测和测试，并评估真阳性率、假阳性率和文本质量。结果表明，三分区方案在固定 FPR 下实现了较高的检测精度，同时保持了可读性。</li>
</ul>

<h3>Title: CodeSimpleQA: Scaling Factuality in Code Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jian Yang, Wei Zhang, Yizhi Li, Shawn Guo, Haowen Wang, Aishan Liu, Ge Zhang, Zili Wang, Zhoujun Li, Xianglong Liu, Weifeng Lv</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19424">https://arxiv.org/abs/2512.19424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19424">https://arxiv.org/pdf/2512.19424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19424]] CodeSimpleQA: Scaling Factuality in Code Large Language Models(https://arxiv.org/abs/2512.19424)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在代码生成方面取得了重大进展，在从自然语言指令合成代码片段方面实现了令人印象深刻的能力。然而，确保法学硕士对编程概念、技术实现等产生事实上准确的回答仍然是一个关键挑战。以前的大多数与代码相关的基准测试都关注代码执行的正确性，而忽视了编程知识的事实准确性。为了弥补这一差距，我们推出了 CodeSimpleQA，这是一个全面的双语基准测试，旨在评估代码法学硕士在回答代码相关问题时的事实准确性，其中包含精心策划的中英文问答对，涵盖多种编程语言和主要计算机科学领域。此外，我们创建了CodeSimpleQA-Instruct，一个拥有66M样本的大规模指令语料库，并开发了一个结合监督微调和强化学习的后训练框架。我们对不同法学硕士的综合评估表明，即使是前沿法学硕士也难以保证代码的真实性。我们提出的框架展示了对基本模型的重大改进，强调了事实意识一致性在开发可靠的代码法学硕士中的至关重要性。</li>
</ul>

<h3>Title: MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments</h3>
<ul>
<li><strong>Authors: </strong>Quyu Kong, Xu Zhang, Zhenyu Yang, Nolan Gao, Chen Liu, Panrong Tong, Chenglin Cai, Hanzhang Zhou, Jianan Zhang, Liangyu Chen, Zhidan Liu, Steven Hoi, Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19432">https://arxiv.org/abs/2512.19432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19432">https://arxiv.org/pdf/2512.19432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19432]] MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments(https://arxiv.org/abs/2512.19432)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.</li>
<li><strong>摘要：</strong>在现有的在线移动使用基准中，AndroidWorld 因其可重复的环境和确定性评估而成为主导基准；然而，最近的代理取得了超过 90% 的成功率，这表明它已经饱和，并激发了对更具挑战性的基准的需求。此外，其环境缺乏电子商务和企业通信等关键应用类别，并且不能反映以模糊用户指令和混合工具使用为特征的现实移动使用场景。为了弥补这一差距，我们引入了 MobileWorld，这是一个更具挑战性的基准测试，旨在更好地反映现实世界的移动使用情况，包括跨 20 个应用程序的 201 个任务，同时保持与 AndroidWorld 相同水平的可重复评估。 MobileWorld 的难度是双重的。首先，它强调跨应用程序交互的长期任务：与 AndroidWorld 相比，MobileWorld 平均需要近两倍的任务完成步骤（27.8 比 14.3），并且包含更多的多应用程序任务（62.2% 比 9.5%）。其次，MobileWorld 通过引入新的任务类别（包括代理与用户交互和 MCP 增强任务），超越了标准 GUI 操作。为了确保稳健的评估，我们提供基于快照的容器环境和精确的功能验证，包括后端数据库检查和任务回调API。我们进一步开发了一个具有扩展操作空间的规划器-执行器代理框架，以支持用户交互和 MCP 调用。我们的结果显示，与 AndroidWorld 相比，性能急剧下降，最佳代理框架和端到端模型的成功率分别为 51.7% 和 20.9%。我们的分析表明，当前模型在用户交互和 MCP 调用方面存在很大困难，这为实现更强大的下一代移动智能提供了战略路线图。</li>
</ul>

<h3>Title: SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Thittipat Pairatsuppawat, Abhibhu Tachaapornchai, Paweekorn Kusolsomboon, Chutikan Chaiwong, Thodsaporn Chay-intr, Kobkrit Viriyayudhakorn, Nongnuch Ketui, Aslan B. Wong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19455">https://arxiv.org/abs/2512.19455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19455">https://arxiv.org/pdf/2512.19455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19455]] SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation(https://arxiv.org/abs/2512.19455)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.</li>
<li><strong>摘要：</strong>尽管英语性能很强，但由于复杂指令下的生成不稳定，开放权重大型语言模型仍然难以部署于泰语。为了缓解这些限制，我们提出了 SiamGPT-32B，这是一种基于 Qwen3-32B 的开放权重模型，并采用强调对数据规模进行策划监督的质量第一策略进行了微调。微调管道将翻译后的高复杂性英语指令数据与泰语改编的 AutoIF 指令框架和语言约束相结合。 SiamGPT-32B 仅使用监督微调，无需持续预训练或语料库扩展，即可提高指令依从性、多轮鲁棒性和语言稳定性。 SEA-HELM 基准测试的评估表明，SiamGPT-32B 在类似规模的开放权重泰国模型中实现了最强的整体性能，在指令跟随、多轮对话和自然语言理解方面取得了一致的进步。</li>
</ul>

<h3>Title: Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations</h3>
<ul>
<li><strong>Authors: </strong>Jinwei Chi, Ke Wang, Yu Chen, Xuanye Lin, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19456">https://arxiv.org/abs/2512.19456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19456">https://arxiv.org/pdf/2512.19456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19456]] Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations(https://arxiv.org/abs/2512.19456)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.</li>
<li><strong>摘要：</strong>由于评分标准的多样性，自动论文评分 (AES) 在交叉提示设置中是一项具有挑战性的任务。虽然之前的研究主要集中在大型语言模型（LLM）的输出上以提高评分准确性，但我们相信中间层的激活也可能提供有价值的信息。为了探索这种可能性，我们评估了法学硕士在交叉提示论文评分任务中激活的辨别能力。具体来说，我们使用激活来拟合探针，并进一步分析了不同模型和法学硕士输入内容对这种判别力的影响。通过计算不同提示下论文在不同特征维度上的方向，我们分析了大型语言模型对论文类型和特征的评估视角的变化。结果表明，激活在评估论文质量方面具有很强的判别力，法学硕士可以根据不同的特征和论文类型调整评估视角，有效处理交叉提示设置中评分标准的多样性。</li>
</ul>

<h3>Title: A Large-Language-Model Framework for Automated Humanitarian Situation Reporting</h3>
<ul>
<li><strong>Authors: </strong>Ivan Decostanzi, Yelena Mejova, Kyriaki Kalimeri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19475">https://arxiv.org/abs/2512.19475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19475">https://arxiv.org/pdf/2512.19475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19475]] A Large-Language-Model Framework for Automated Humanitarian Situation Reporting(https://arxiv.org/abs/2512.19475)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.</li>
<li><strong>摘要：</strong>及时、准确的情况报告对于人道主义决策至关重要，但目前的工作流程仍然主要是手动、资源密集型且不一致。我们提出了一个完全自动化的框架，该框架使用大型语言模型（LLM）将异构的人道主义文档转换为结构化且基于证据的报告。该系统集成了语义文本聚类、自动问题生成、检索增强答案提取与引文、多级摘要和执行摘要生成，并由模拟专家推理的内部评估指标支持。我们使用来自 ReliefWeb 等经过验证的来源的 1,100 多份文档，评估了 13 次人道主义事件（包括自然灾害和冲突）的框架。生成的问题的相关性为 84.7%，重要性为 84.0%，紧迫性为 76.4%。提取的答案相关性达到86.3%，引用准确率和召回率均超过76%。人类评估与法学硕士评估之间的一致性超过了 0.80 的 F1 分数。比较分析表明，所提出的框架生成的报告比现有基线更具结构化、可解释性和可操作性。通过将法学硕士推理与透明的引文链接和多级评估相结合，本研究表明生成式人工智能可以自主生成准确、可验证且可操作的人道主义局势报告。</li>
</ul>

<h3>Title: Event Extraction in Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Bobo Li, Xudong Han, Jiang Liu, Yuzhe Ding, Liqiang Jing, Zhaoqi Zhang, Jinheng Li, Xinya Du, Fei Li, Meishan Zhang, Min Zhang, Aixin Sun, Philip S. Yu, Hao Fei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19537">https://arxiv.org/abs/2512.19537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19537">https://arxiv.org/pdf/2512.19537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19537]] Event Extraction in Large Language Model(https://arxiv.org/abs/2512.19537)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, hallucination, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.</li>
<li><strong>摘要：</strong>大语言模型 (LLM) 和多模态 LLM 正在改变事件提取 (EE)：提示和生成通常可以在零样本或少量样本设置中产生结构化输出。然而，基于法学硕士的管道面临部署差距，包括弱约束下的幻觉、长上下文和跨文档的脆弱时间和因果链接，以及有界上下文窗口内有限的长视野知识管理。我们认为，EE 应该被视为一个系统组件，为以 LLM 为中心的解决方案提供认知支架。事件模式和槽位约束创建用于接地和验证的接口；以事件为中心的结构充当逐步推理的受控中间表示；事件链接支持基于图的 RAG 的关系感知检索；事件存储提供超出上下文窗口的可更新的情景和代理内存。这项调查涵盖了文本和多模态环境中的 EE、组织任务和分类、跟踪从基于规则和神经模型到指令驱动和生成框架的方法演变，并总结了公式、解码策略、架构、表示、数据集和评估。我们还审查跨语言、低资源和特定领域的设置，并强调可靠的以事件为中心的系统的开放挑战和未来方向。最后，我们概述了 LLM 时代核心的开放挑战和未来方向，旨在将 EE 从静态提取发展为开放世界系统的结构可靠、代理就绪的感知和存储层。</li>
</ul>

<h3>Title: Increasing the Thinking Budget is Not All You Need</h3>
<ul>
<li><strong>Authors: </strong>Ignacio Iacobacci, Zhaozhi Qian, Faroq AL-Tam, Muhammad AL-Qurishi, Riad Souissi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19585">https://arxiv.org/abs/2512.19585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19585">https://arxiv.org/pdf/2512.19585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19585]] Increasing the Thinking Budget is Not All You Need(https://arxiv.org/abs/2512.19585)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.</li>
<li><strong>摘要：</strong>最近，出现了新一波具有思考能力的大型语言模型，在广泛的推理基准测试中展示了卓越的能力。早期研究已经开始探索推理过程长度（即所谓的思维预算）的计算量如何影响模型性能。在这项工作中，我们建议对思维预算作为一个关键参数进行系统研究，检查其与各种配置（例如自洽、反思等）的相互作用。我们的目标是提供一个信息丰富、平衡的比较框架，同时考虑性能结果和计算成本。在我们的研究结果中，我们发现简单地增加思维预算并不是最有效地利用计算。相反，可以通过其他配置来实现更准确的响应，例如自我一致性和自我反思。</li>
</ul>

<h3>Title: Exploring the features used for summary evaluation by Human and GPT</h3>
<ul>
<li><strong>Authors: </strong>Zahra Sadeghi, Evangelos Milios, Frank Rudzicz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19620">https://arxiv.org/abs/2512.19620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19620">https://arxiv.org/pdf/2512.19620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19620]] Exploring the features used for summary evaluation by Human and GPT(https://arxiv.org/abs/2512.19620)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.</li>
<li><strong>摘要：</strong>摘要评估涉及评估生成的摘要如何反映源文本的关键思想和含义，需要对内容有深入的理解。大型语言模型（LLM）已被用来自动化这一过程，充当法官来评估相对于原始文本的摘要。虽然之前的研究调查了法学硕士和人类反应之间的一致性，但尚不清楚当被要求根据特定的质量维度进行评估时，它们会利用哪些属性或特征，并且对于评估分数和指标之间的映射也没有太多关注。在本文中，我们解决了这个问题，并通过研究统计和机器学习指标来发现与人类和生成预训练 Transformer (GPT) 响应一致的特征。此外，我们表明，指示 GPT 采用人类使用的指标可以提高它们的判断力，并使它们更好地符合人类的反应。</li>
</ul>

<h3>Title: Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori</h3>
<ul>
<li><strong>Authors: </strong>Rolando Coto-Solano, Daisy Li, Manoela Teleginski Ferraz, Olivia Sasse, Cha Krupka, Sharid Loáiciga, Sally Akevai Tenamu Nicholas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19630">https://arxiv.org/abs/2512.19630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19630">https://arxiv.org/pdf/2512.19630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19630]] Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori(https://arxiv.org/abs/2512.19630)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Māori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts.</li>
<li><strong>摘要：</strong>我们提出了变音符号恢复的实验，这是一种对于自然语言处理（NLP）任务至关重要的文本规范化形式。我们的研究重点是两种资源极度匮乏的语言：布里布里语（一种在哥斯达黎加使用的奇布坎语言）和库克群岛毛利语（一种在库克群岛使用的波利尼西亚语言）。具体来说，本文：（i）比较资源贫乏语言中的变音符号恢复算法，包括声调变音符号，（ii）检查实现目标性能水平所需的数据量，（iii）对比不同资源条件下的结果，以及（iv）探索变音符号校正的相关任务。我们发现经过微调的字符级 LLM 表现最佳，可能是因为它们能够将复杂字符分解为其 UTF-8 字节表示形式。相比之下，鉴于我们的数据限制，大规模多语言模型的执行效率较低。在所有模型中，数据预算约为 10,000 个字时，可靠的性能就开始显现。零样本方法在所有情况下都表现不佳。这项研究既响应了语言社区的要求，也响应了有关资源贫乏环境中模型性能和泛化的更广泛的 NLP 研究问题。</li>
</ul>

<h3>Title: Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting</h3>
<ul>
<li><strong>Authors: </strong>Filippos Ventirozos, Peter Appleby, Matthew Shardlow</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19651">https://arxiv.org/abs/2512.19651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19651">https://arxiv.org/pdf/2512.19651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19651]] Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting(https://arxiv.org/abs/2512.19651)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.</li>
<li><strong>摘要：</strong>方面类别情绪分析 (ACSA) 通过识别评论中的特定主题及其相关情绪来提供精细的见解。虽然监督学习方法在该领域占据主导地位，但新领域注释数据的稀缺性和高成本构成了重大障碍。我们认为，在数据注释资源有限的情况下，在零样本设置中利用大型语言模型（LLM）是一种实用的替代方案。在这项工作中，我们提出了一种新颖的思想链 (CoT) 提示技术，该技术利用中间统一含义表示 (UMR) 来构建 ACSA 任务的推理过程。我们针对三个模型（Qwen3-4B、Qwen3-8B 和 Gemini-2.5-Pro）和四个不同数据集的标准 CoT 基线评估了这种基于 UMR 的方法。我们的研究结果表明 UMR 有效性可能依赖于模型。虽然初步结果表明 Qwen3-8B 等中型模型的性能相当，但这些观察结果值得进一步研究，特别是关于较小模型架构的潜在适用性。需要进一步的研究来确定这些发现在不同模型尺度上的普遍性。</li>
</ul>

<h3>Title: GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang, Xinzhe Juan, Jiahao Qiu, Ke Shen, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.19682">https://arxiv.org/abs/2512.19682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.19682">https://arxiv.org/pdf/2512.19682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.19682]] GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators(https://arxiv.org/abs/2512.19682)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $\alpha$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \textbf{+40.3\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.</li>
<li><strong>摘要：</strong>具有训练能力的大型语言模型（LLM）代理受到现实世界交互数据的高成本和静态特性的严重瓶颈。我们通过引入 GenEnv 来解决这个问题，该框架在代理和可扩展的生成环境模拟器之间建立了难度一致的共同进化游戏。与在静态数据集上演化模型的传统方法不同，GenEnv 实例化了数据演化：模拟器充当动态课程策略，不断生成专门针对代理的“最近发展区域”定制的任务。此过程由简单但有效的$\alpha$-课程奖励指导，它将任务难度与代理的当前能力相结合。我们根据五个基准评估 GenEnv，包括 API-Bank、ALFWorld、BFCL、Bamboogle 和 TravelPlanner。在这些任务中，GenEnv 将代理性能提高了 7B 基线高达 \textbf{+40.3\%}，并且匹配或超过了较大模型的平均性能。与基于 Gemini 2.5 Pro 的离线数据增强相比，GenEnv 实现了更好的性能，同时使用的数据减少了 3.3$\time$。通过从静态监督转向自适应模拟，GenEnv 为扩展代理功能提供了一条数据高效的途径。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
