<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-17</h1>
<h3>Title: Semantic Segment Based Semantic Parsing for Question Answering over  Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Sijia Wei, Wenwen Zhang, Qisong Li, Jiang Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06772">https://arxiv.org/abs/2401.06772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06772">https://arxiv.org/pdf/2401.06772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06772]] Semantic Segment Based Semantic Parsing for Question Answering over  Knowledge Graphs(https://arxiv.org/abs/2401.06772)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a novel method named "graph-to-segment" for question answering over knowledge graphs, focusing on understanding question utterances. This method centers on semantic parsing, a key approach for interpreting these utterances. Our primary challenge lies in comprehending implicit entities, relationships, and complex constraints like time, ordinality, and aggregation within questions, contextualized by the knowledge graph. Our framework employs a combination of rule-based and neural-based techniques to parse and construct highly accurate and comprehensive semantic segment sequences. These sequences form semantic query graphs, effectively representing question utterances. We approach question semantic parsing as a sequence generation task, utilizing an encoder-decoder neural network to transform natural language questions into semantic segments. Moreover, to enhance the parsing of implicit entities and relations, we incorporate a graph neural network that leverages the context of the knowledge graph to better understand question representations. Our experimental evaluations on two datasets demonstrate the effectiveness and superior performance of our model in semantic parsing for question answering.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了一种名为“图到分段”的新方法，用于知识图谱问答，重点是理解问题话语。该方法以语义解析为中心，这是解释这些话语的关键方法。我们的主要挑战在于理解问题中隐含的实体、关系和复杂的约束，如时间、序数和聚合，并通过知识图谱进行上下文化。我们的框架采用基于规则和基于神经的技术相结合来解析和构建高度准确和全面的语义片段序列。这些序列形成语义查询图，有效地表示问题话语。我们将问题语义解析视为序列生成任务，利用编码器-解码器神经网络将自然语言问题转换为语义片段。此外，为了增强对隐式实体和关系的解析，我们采用了图神经网络，该网络利用知识图的上下文来更好地理解问题表示。我们对两个数据集的实验评估证明了我们的模型在问答语义解析方面的有效性和优越性能。</li>
</ul>

<h3>Title: Two Directions for Clinical Data Generation with Large Language Models:  Data-to-Label and Label-to-Data</h3>
<ul>
<li><strong>Authors: </strong>Rumeng Li, Xun Wang, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06774">https://arxiv.org/abs/2401.06774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06774">https://arxiv.org/pdf/2401.06774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06774]] Two Directions for Clinical Data Generation with Large Language Models:  Data-to-Label and Label-to-Data(https://arxiv.org/abs/2401.06774)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can generate natural language texts for various domains and tasks, but their potential for clinical text mining, a domain with scarce, sensitive, and imbalanced medical data, is underexplored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge, which guides LLMs to generate synthetic data following two different directions: "data-to-label", which labels sentences from a public EHR collection with AD-related signs and symptoms; and "label-to-data", which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs, using three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method; and (3) a bronze dataset created by the label-to-data method. We find that using the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: Large language models in healthcare and medical domain: A review</h3>
<ul>
<li><strong>Authors: </strong>Zabir Al Nazi, Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06775">https://arxiv.org/abs/2401.06775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06775">https://arxiv.org/pdf/2401.06775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06775]] Large language models in healthcare and medical domain: A review(https://arxiv.org/abs/2401.06775)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora</a></li>
<li><strong>Abstract: </strong>The deployment of large language models (LLMs) within the healthcare sector has sparked both enthusiasm and apprehension. These models exhibit the remarkable capability to provide proficient responses to free-text queries, demonstrating a nuanced understanding of professional medical knowledge. This comprehensive survey delves into the functionalities of existing LLMs designed for healthcare applications, elucidating the trajectory of their development, starting from traditional Pretrained Language Models (PLMs) to the present state of LLMs in healthcare sector. First, we explore the potential of LLMs to amplify the efficiency and effectiveness of diverse healthcare applications, particularly focusing on clinical language understanding tasks. These tasks encompass a wide spectrum, ranging from named entity recognition and relation extraction to natural language inference, multi-modal medical applications, document classification, and question-answering. Additionally, we conduct an extensive comparison of the most recent state-of-the-art LLMs in the healthcare domain, while also assessing the utilization of various open-source LLMs and highlighting their significance in healthcare applications. Furthermore, we present the essential performance metrics employed to evaluate LLMs in the biomedical domain, shedding light on their effectiveness and limitations. Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings. This review provides a comprehensive exploration of the current landscape of LLMs in healthcare, addressing their role in transforming medical applications and the areas that warrant further research and development.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas  Hold'em via Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, Yanru Zhang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06781">https://arxiv.org/abs/2401.06781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06781">https://arxiv.org/pdf/2401.06781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06781]] PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas  Hold'em via Large Language Model(https://arxiv.org/abs/2401.06781)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Poker, also known as Texas Hold'em, has always been a typical research target within imperfect information games (IIGs). IIGs have long served as a measure of artificial intelligence (AI) development. Representative prior works, such as DeepStack and Libratus heavily rely on counterfactual regret minimization (CFR) to tackle heads-up no-limit Poker. However, it is challenging for subsequent researchers to learn CFR from previous models and apply it to other real-world applications due to the expensive computational cost of CFR iterations. Additionally, CFR is difficult to apply to multi-player games due to the exponential growth of the game tree size. In this work, we introduce PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number of players and gaining high win rates, established on a lightweight large language model (LLM). PokerGPT only requires simple textual information of Poker games for generating decision-making advice, thus guaranteeing the convenient interaction between AI and humans. We mainly transform a set of textual records acquired from real games into prompts, and use them to fine-tune a lightweight pre-trained LLM using reinforcement learning human feedback technique. To improve fine-tuning performance, we conduct prompt engineering on raw data, including filtering useful information, selecting behaviors of players with high win rates, and further processing them into textual instruction using multiple prompt engineering techniques. Through the experiments, we demonstrate that PokerGPT outperforms previous approaches in terms of win rate, model size, training time, and response speed, indicating the great potential of LLMs in solving IIGs.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: Semantic Similarity Matching for Patent Documents Using Ensemble  BERT-related Model and Novel Text Processing Method</h3>
<ul>
<li><strong>Authors: </strong>Liqiang Yu, Bo Liu, Qunwei Lin, Xinyu Zhao, Chang Che</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06782">https://arxiv.org/abs/2401.06782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06782">https://arxiv.org/pdf/2401.06782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06782]] Semantic Similarity Matching for Patent Documents Using Ensemble  BERT-related Model and Novel Text Processing Method(https://arxiv.org/abs/2401.06782)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In the realm of patent document analysis, assessing semantic similarity between phrases presents a significant challenge, notably amplifying the inherent complexities of Cooperative Patent Classification (CPC) research. Firstly, this study addresses these challenges, recognizing early CPC work while acknowledging past struggles with language barriers and document intricacy. Secondly, it underscores the persisting difficulties of CPC research. To overcome these challenges and bolster the CPC system, This paper presents two key innovations. Firstly, it introduces an ensemble approach that incorporates four BERT-related models, enhancing semantic similarity accuracy through weighted averaging. Secondly, a novel text preprocessing method tailored for patent documents is introduced, featuring a distinctive input structure with token scoring that aids in capturing semantic relationships during CPC context training, utilizing BCELoss. Our experimental findings conclusively establish the effectiveness of both our Ensemble Model and novel text processing strategies when deployed on the U.S. Patent Phrase to Phrase Matching dataset.</li>
<li><strong>摘要：</strong>在专利文档分析领域，评估短语之间的语义相似性提出了重大挑战，特别是放大了合作专利分类（CPC）研究的固有复杂性。首先，本研究解决了这些挑战，承认早期的中国共产党工作，同时承认过去与语言障碍和文件复杂性的斗争。其次，凸显了中国共产党研究长期存在的困难。为了克服这些挑战并加强中国共产党制度，本文提出了两项​​关键创新。首先，它引入了一种集成方法，该方法结合了四个 BERT 相关模型，通过加权平均来提高语义相似度准确性。其次，引入了一种针对专利文档量身定制的新颖文本预处理方法，该方法具有独特的输入结构和标记评分，有助于利用 BCELoss 在 CPC 上下文训练期间捕获语义关系。我们的实验结果最终证明了我们的集成模型和新颖的文本处理策略在美国专利短语到短语匹配数据集上部署时的有效性。</li>
</ul>

<h3>Title: Human-Instruction-Free LLM Self-Alignment with Limited Samples</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Guo, Yuanshun Yao, Wei Shen, Jiaheng Wei, Xiaoying Zhang, Zhaoran Wang, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06785">https://arxiv.org/abs/2401.06785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06785">https://arxiv.org/pdf/2401.06785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06785]] Human-Instruction-Free LLM Self-Alignment with Limited Samples(https://arxiv.org/abs/2401.06785)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human values is a vital task for LLM practitioners. Current alignment techniques have several limitations: (1) requiring a large amount of annotated data; (2) demanding heavy human involvement; (3) lacking a systematic mechanism to continuously improve. In this work, we study aligning LLMs to a new domain with limited samples (e.g. < 100). We propose an algorithm that can self-align LLMs iteratively without active human involvement. Unlike existing works, our algorithm relies on neither human-crafted instructions nor labeled rewards, significantly reducing human involvement. In addition, our algorithm can self-improve the alignment continuously. The key idea is to first retrieve high-quality samples related to the target domain and use them as In-context Learning examples to generate more samples. Then we use the self-generated samples to finetune the LLM iteratively. We show that our method can unlock the LLMs' self-generalization ability to perform alignment with near-zero human supervision. We test our algorithm on three benchmarks in safety, truthfulness, and instruction-following, and show good performance in alignment, domain adaptability, and scalability.</li>
<li><strong>摘要：</strong>将大型语言模型 (LLM) 与人类价值观结合起来是 LLM 从业者的一项重要任务。目前的对齐技术有几个局限性：（1）需要大量的注释数据； (2) 需要大量人力参与； （三）缺乏持续改进的系统机制。在这项工作中，我们研究如何将法学硕士与有限样本（例如 < 100）的新领域结合起来。我们提出了一种算法，可以迭代地自我调整法学硕士，无需人工积极参与。与现有的作品不同，我们的算法既不依赖于人为的指令，也不依赖于标记的奖励，从而显着减少了人类的参与。此外，我们的算法可以不断自我改进对齐。关键思想是首先检索与目标领域相关的高质量样本，并将它们用作上下文学习示例以生成更多样本。然后我们使用自行生成的样本迭代地微调 LLM。我们证明，我们的方法可以解锁法学硕士的自我泛化能力，以在近乎零的人类监督下进行对齐。我们在安全性、真实性和指令遵循性三个基准上测试了我们的算法，并在对齐、领域适应性和可扩展性方面表现出了良好的性能。</li>
</ul>

<h3>Title: Deep Learning Based Cyberbullying Detection in Bangla Language</h3>
<ul>
<li><strong>Authors: </strong>Sristy Shidul Nath, Razuan Karim, Mahdi H. Miraz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06787">https://arxiv.org/abs/2401.06787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06787">https://arxiv.org/pdf/2401.06787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06787]] Deep Learning Based Cyberbullying Detection in Bangla Language(https://arxiv.org/abs/2401.06787)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>The Internet is currently the largest platform for global communication including expressions of opinions, reviews, contents, images, videos and so forth. Moreover, social media has now become a very broad and highly engaging platform due to its immense popularity and swift adoption trend. Increased social networking, however, also has detrimental impacts on the society leading to a range of unwanted phenomena, such as online assault, intimidation, digital bullying, criminality and trolling. Hence, cyberbullying has become a pervasive and worrying problem that poses considerable psychological and emotional harm to the people, particularly amongst the teens and the young adults. In order to lessen its negative effects and provide victims with prompt support, a great deal of research to identify cyberbullying instances at various online platforms is emerging. In comparison to other languages, Bangla (also known as Bengali) has fewer research studies in this domain. This study demonstrates a deep learning strategy for identifying cyberbullying in Bengali, using a dataset of 12282 versatile comments from multiple social media sites. In this study, a two-layer bidirectional long short-term memory (Bi-LSTM) model has been built to identify cyberbullying, using a variety of optimisers as well as 5-fold cross validation. To evaluate the functionality and efficacy of the proposed system, rigorous assessment and validation procedures have been employed throughout the project. The results of this study reveals that the proposed model's accuracy, using momentum-based stochastic gradient descent (SGD) optimiser, is 94.46%. It also reflects a higher accuracy of 95.08% and a F1 score of 95.23% using Adam optimiser as well as a better accuracy of 94.31% in 5-fold cross validation.</li>
<li><strong>摘要：</strong>互联网是目前最大的全球交流平台，包括表达意见、评论、内容、图像、视频等。此外，由于其巨大的受欢迎程度和迅速的采用趋势，社交媒体现已成为一个非常广泛和高度参与的平台。然而，社交网络的增加也对社会产生了不利影响，导致一系列不良现象，例如网络攻击、恐吓、数字欺凌、犯罪和网络钓鱼。因此，网络欺凌已成为一个普遍且令人担忧的问题，对人们，特别是青少年和年轻人造成相当大的心理和情感伤害。为了减轻其负面影响并为受害者提供及时的支持，大量研究旨在识别各种在线平台上的网络欺凌实例。与其他语言相比，孟加拉语（也称为孟加拉语）在该领域的研究较少。这项研究展示了一种识别孟加拉语网络欺凌的深度学习策略，使用来自多个社交媒体网站的 12282 条多功能评论的数据集。在本研究中，使用各种优化器以及 5 倍交叉验证，构建了一个两层双向长短期记忆 (Bi-LSTM) 模型来识别网络欺凌。为了评估拟议系统的功能和功效，整个项目采用了严格的评估和验证程序。这项研究的结果表明，使用基于动量的随机梯度下降 (SGD) 优化器所提出的模型的准确度为 94.46%。它还反映了使用 Adam 优化器的 95.08% 的更高准确率和 95.23% 的 F1 分数，以及 5 倍交叉验证中 94.31% 的更好准确率。</li>
</ul>

<h3>Title: Using Zero-shot Prompting in the Automatic Creation and Expansion of  Topic Taxonomies for Tagging Retail Banking Transactions</h3>
<ul>
<li><strong>Authors: </strong>Daniel de S. Moraes, Pedro T. C. Santos, Polyana B. da Costa, Matheus A. S. Pinto, Ivan de J. P. Pinto, Álvaro M. G. da Veiga, Sergio Colcher, Antonio J. G. Busson, Rafael H. Rocha, Rennan Gaio, Rafael Miceli, Gabriela Tourinho, Marcos Rabaioli, Leandro Santos, Fellipe Marques, David Favaro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06790">https://arxiv.org/abs/2401.06790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06790">https://arxiv.org/pdf/2401.06790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06790]] Using Zero-shot Prompting in the Automatic Creation and Expansion of  Topic Taxonomies for Tagging Retail Banking Transactions(https://arxiv.org/abs/2401.06790)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>This work presents an unsupervised method for automatically constructing and expanding topic taxonomies by using instruction-based fine-tuned LLMs (Large Language Models). We apply topic modeling and keyword extraction techniques to create initial topic taxonomies and LLMs to post-process the resulting terms and create a hierarchy. To expand an existing taxonomy with new terms, we use zero-shot prompting to find out where to add new nodes, which, to our knowledge, is the first work to present such an approach to taxonomy tasks. We use the resulting taxonomies to assign tags that characterize merchants from a retail bank dataset. To evaluate our work, we asked 12 volunteers to answer a two-part form in which we first assessed the quality of the taxonomies created and then the tags assigned to merchants based on that taxonomy. The evaluation revealed a coherence rate exceeding 90% for the chosen taxonomies, while the average coherence for merchant tagging surpassed 80%.</li>
<li><strong>摘要：</strong>这项工作提出了一种无监督方法，通过使用基于指令的微调 LLM（大型语言模型）来自动构建和扩展主题分类法。我们应用主题建模和关键词提取技术来创建初始主题分类法，并应用法学硕士来对结果术语进行后处理并创建层次结构。为了用新术语扩展现有分类法，我们使用零样本提示来找出在哪里添加新节点，据我们所知，这是第一个提出这种分类任务方法的工作。我们使用生成的分类法来分配代表零售银行数据集中商家特征的标签。为了评估我们的工作，我们要求 12 名志愿者回答一份由两部分组成的表格，其中我们首先评估所创建的分类法的质量，然后根据该分类法分配给商家的标签。评估显示所选分类法的一致性率超过 90%，而商家标签的平均一致性超过 80%。</li>
</ul>

<h3>Title: LightHouse: A Survey of AGI Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Feng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06792">https://arxiv.org/abs/2401.06792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06792">https://arxiv.org/pdf/2401.06792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06792]] LightHouse: A Survey of AGI Hallucination(https://arxiv.org/abs/2401.06792)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, hallucination</a></li>
<li><strong>Abstract: </strong>With the development of artificial intelligence, large-scale models have become increasingly intelligent. However, numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research. In the pursuit of achieving strong artificial intelligence, a significant volume of research effort is being invested in the AGI (Artificial General Intelligence) hallucination research. Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models). As for multimodal AGI, research on hallucinations is still in an early stage. To further the progress of research in the domain of hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI, summarizing the current work on AGI hallucinations and proposing some directions for future research.</li>
<li><strong>摘要：</strong>随着人工智能的发展，大型模型变得越来越智能。然而，大量研究表明，这些大型模型中的幻觉是阻碍人工智能研究发展的瓶颈。为了实现强大的人工智能，大量的研究工作投入到 AGI（通用人工智能）幻觉研究中。之前的探索是在法学硕士（大语言模型）中研究幻觉。至于多模态 AGI，对幻觉的研究仍处于早期阶段。为了进一步推动幻觉现象领域的研究进展，我们对 AGI 幻觉进行了鸟瞰，总结了当前 AGI 幻觉方面的工作，并提出了未来研究的方向。</li>
</ul>

<h3>Title: AI and Generative AI for Research Discovery and Summarization</h3>
<ul>
<li><strong>Authors: </strong>Mark Glickman, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06795">https://arxiv.org/abs/2401.06795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06795">https://arxiv.org/pdf/2401.06795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06795]] AI and Generative AI for Research Discovery and Summarization(https://arxiv.org/abs/2401.06795)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, code, chat</a></li>
<li><strong>Abstract: </strong>AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simulate abductive reasoning, which provides researchers the ability to make connections among related technical topics, which can also be used for research discovery. We review the developments in AI and generative AI for research discovery and summarization, and propose directions where these types of tools are likely to head in the future that may be of interest to statistician and data scientists.</li>
<li><strong>摘要：</strong>人工智能和生成式人工智能工具，包括像 ChatGPT 这样依赖大型语言模型 (LLM) 的聊天机器人，今年突然出现，为提高工作效率和改善我们的生活创造了难以置信的机会。统计学家和数据科学家已经开始以多种方式体验这些工具的可用性的好处，例如根据文本提示生成编程代码以分析数据或拟合统计模型。这些工具可以产生重大影响的领域之一是研究发现和总结。正在开发聊天机器人的独立工具和插件，使研究人员能够比 2023 年之前的搜索工具更快地找到相关文献。此外，生成式人工智能工具已经改进到可以用简洁的语言总结和提取研究文章中的要点。最后，基于高度参数化的法学硕士的聊天机器人可用于模拟溯因推理，这为研究人员提供了在相关技术主题之间建立联系的能力，这也可用于研究发现。我们回顾了人工智能和生成人工智能的发展，以进行研究发现和总结，并提出了统计学家和数据科学家可能感兴趣的此类工具未来可能发展的方向。</li>
</ul>

<h3>Title: AI Hallucinations: A Misnomer Worth Clarifying</h3>
<ul>
<li><strong>Authors: </strong>Negar Maleki, Balaji Padmanabhan, Kaushik Dutta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06796">https://arxiv.org/abs/2401.06796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06796">https://arxiv.org/pdf/2401.06796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06796]] AI Hallucinations: A Misnomer Worth Clarifying(https://arxiv.org/abs/2401.06796)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, hallucination</a></li>
<li><strong>Abstract: </strong>As large language models continue to advance in Artificial Intelligence (AI), text generation systems have been shown to suffer from a problematic phenomenon termed often as "hallucination." However, with AI's increasing presence across various domains including medicine, concerns have arisen regarding the use of the term itself. In this study, we conducted a systematic review to identify papers defining "AI hallucination" across fourteen databases. We present and analyze definitions obtained across all databases, categorize them based on their applications, and extract key points within each category. Our results highlight a lack of consistency in how the term is used, but also help identify several alternative terms in the literature. We discuss implications of these and call for a more unified effort to bring consistency to an important contemporary AI issue that can affect multiple domains significantly.</li>
<li><strong>摘要：</strong>随着大型语言模型在人工智能 (AI) 领域的不断发展，文本生成系统已被证明遇到了通常被称为“幻觉”的问题现象。然而，随着人工智能在包括医学在内的各个领域的应用不断增加，人们对该术语本身的使用产生了担忧。在这项研究中，我们进行了系统回顾，以识别十四个数据库中定义“人工智能幻觉”的论文。我们展示并分析从所有数据库中获得的定义，根据其应用对它们进行分类，并提取每个类别中的关键点。我们的结果凸显了该术语的使用方式缺乏一致性，但也有助于识别文献中的几个替代术语。我们讨论这些影响，并呼吁更加统一的努力，使可能对多个领域产生重大影响的重要当代人工智能问题保持一致。</li>
</ul>

<h3>Title: Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt  Learning with Data-Dependent Prior</h3>
<ul>
<li><strong>Authors: </strong>Youngjae Cho, HeeSun Bae, Seungjae Shin, Yeo Dong Youn, Weonyoung Joo, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06799">https://arxiv.org/abs/2401.06799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06799">https://arxiv.org/pdf/2401.06799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06799]] Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt  Learning with Data-Dependent Prior(https://arxiv.org/abs/2401.06799)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, code</a></li>
<li><strong>Abstract: </strong>Recent Vision-Language Pretrained (VLP) models have become the backbone for many downstream tasks, but they are utilized as frozen model without learning. Prompt learning is a method to improve the pre-trained VLP model by adding a learnable context vector to the inputs of the text encoder. In a few-shot learning scenario of the downstream task, MLE training can lead the context vector to over-fit dominant image features in the training data. This overfitting can potentially harm the generalization ability, especially in the presence of a distribution shift between the training and test dataset. This paper presents a Bayesian-based framework of prompt learning, which could alleviate the overfitting issues on few-shot learning application and increase the adaptability of prompts on unseen instances. Specifically, modeling data-dependent prior enhances the adaptability of text features for both seen and unseen image features without the trade-off of performance between them. Based on the Bayesian framework, we utilize the Wasserstein Gradient Flow in the estimation of our target posterior distribution, which enables our prompt to be flexible in capturing the complex modes of image features. We demonstrate the effectiveness of our method on benchmark datasets for several experiments by showing statistically significant improvements on performance compared to existing methods. The code is available at https://github.com/youngjae-cho/APP.</li>
<li><strong>摘要：</strong>最近的视觉语言预训练（VLP）模型已成为许多下游任务的骨干，但它们在没有学习的情况下被用作冻结模型。即时学习是一种通过向文本编码器的输入添加可学习的上下文向量来改进预训练的 VLP 模型的方法。在下游任务的少数镜头学习场景中，MLE 训练可以导致上下文向量过度拟合训练数据中的主导图像特征。这种过度拟合可能会损害泛化能力，尤其是在训练数据集和测试数据集之间存在分布变化的情况下。本文提出了一种基于贝叶斯的提示学习框架，可以缓解小样本学习应用中的过度拟合问题，并提高提示对未见过的实例的适应性。具体来说，对数据相关的先验进行建模增强了文本特征对可见和不可见图像特征的适应性，而无需权衡它们之间的性能。基于贝叶斯框架，我们利用 Wasserstein 梯度流来估计目标后验分布，这使得我们的提示能够灵活地捕获图像特征的复杂模式。通过与现有方法相比，我们在统计上显着提高了性能，从而证明了我们的方法在多个实验的基准数据集上的有效性。代码可在 https://github.com/youngjae-cho/APP 获取。</li>
</ul>

<h3>Title: Reinforcement Learning for Optimizing RAG for Domain Chatbots</h3>
<ul>
<li><strong>Authors: </strong>Mandar Kulkarni, Praveen Tangarajan, Kyung Kim, Anusua Trivedi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06800">https://arxiv.org/abs/2401.06800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06800">https://arxiv.org/pdf/2401.06800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06800]] Reinforcement Learning for Optimizing RAG for Domain Chatbots(https://arxiv.org/abs/2401.06800)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases. LLMs acquire the ability to contextual question answering through training, and Retrieval Augmented Generation (RAG) further enables the bot to answer domain-specific questions. This paper describes a RAG-based approach for building a chatbot that answers user's queries using Frequently Asked Questions (FAQ) data. We train an in-house retrieval embedding model using infoNCE loss, and experimental results demonstrate that the in-house model works significantly better than the well-known general-purpose public embedding model, both in terms of retrieval accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open API-based paid ChatGPT model. We noticed that a previously retrieved-context could be used to generate an answer for specific patterns/sequences of queries (e.g., follow-up queries). Hence, there is a scope to optimize the number of LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize the number of LLM tokens using Reinforcement Learning (RL). Specifically, we propose a policy-based model external to the RAG, which interacts with the RAG pipeline through policy actions and updates the policy to optimize the cost. The policy model can perform two actions: to fetch FAQ context or skip retrieval. We use the open API-based GPT-4 as the reward model. We then train a policy model using policy gradient on multiple training chat sessions. As a policy model, we experimented with a public gpt-2 model and an in-house BERT model. With the proposed RL-based optimization combined with similarity threshold, we are able to achieve significant cost savings while getting a slightly improved accuracy. Though we demonstrate results for the FAQ chatbot, the proposed RL approach is generic and can be experimented with any existing RAG pipeline.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的出现，对话助理在领域用例中变得越来越普遍。法学硕士通过培训获得上下文问答的能力，检索增强生成（RAG）进一步使机器人能够回答特定领域的问题。本文介绍了一种基于 RAG 的方法，用于构建使用常见问题 (FAQ) 数据回答用户查询的聊天机器人。我们使用 infoNCE 损失训练内部检索嵌入模型，实验结果表明，无论是在检索精度还是在域外方面，内部模型都明显优于众所周知的通用公共嵌入模型(OOD) 查询检测。作为法学硕士，我们使用基于开放 API 的付费 ChatGPT 模型。我们注意到，先前检索到的上下文可用于生成特定模式/查询序列（例如后续查询）的答案。因此，LLM 代币的数量和成本还有优化的空间。假设固定检索模型和 LLM，我们使用强化学习 (RL) 优化 LLM 令牌的数量。具体来说，我们提出了 RAG 外部基于策略的模型，该模型通过策略操作与 RAG 管道交互并更新策略以优化成本。策略模型可以执行两个操作：获取常见问题解答上下文或跳过检索。我们使用基于开放 API 的 GPT-4 作为奖励模型。然后，我们在多个训练聊天会话上使用策略梯度来训练策略模型。作为政策模型，我们尝试了公共 gpt-2 模型和内部 BERT 模型。通过所提出的基于强化学习的优化与相似性阈值相结合，我们能够显着节省成本，同时略微提高准确性。尽管我们展示了 FAQ 聊天机器人的结果，但所提出的 RL 方法是通用的，可以使用任何现有的 RAG 管道进行实验。</li>
</ul>

<h3>Title: Graph-of-Thought: Utilizing Large Language Models to Solve Complex and  Dynamic Business Problems</h3>
<ul>
<li><strong>Authors: </strong>Ye Li</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06801">https://arxiv.org/abs/2401.06801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06801">https://arxiv.org/pdf/2401.06801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06801]] Graph-of-Thought: Utilizing Large Language Models to Solve Complex and  Dynamic Business Problems(https://arxiv.org/abs/2401.06801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow's potential for improving business processes is significant, promising advancements in both efficiency and decision quality with continuous development.</li>
<li><strong>摘要：</strong>本文提出了思维图 (GoT)，这是一种新的工作流自动化模型，可增强大型语言模型 (LLM) 在复杂任务执行中的灵活性和效率。 GoT 超越了传统的线性和树状认知模型，其图形结构支持动态路径选择。开源引擎 GoTFlow 展示了 GoT 的实际应用，促进跨各个领域的自动化、数据驱动决策。尽管存在复杂性和透明度方面的挑战，GoTFlow 改进业务流程的潜力巨大，有望通过持续开发在效率和决策质量方面取得进步。</li>
</ul>

<h3>Title: Hierarchical Knowledge Distillation on Text Graph for Data-limited  Attribute Inference</h3>
<ul>
<li><strong>Authors: </strong>Quan Li, Shixiong Jing, Lingwei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06802">https://arxiv.org/abs/2401.06802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06802">https://arxiv.org/pdf/2401.06802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06802]] Hierarchical Knowledge Distillation on Text Graph for Data-limited  Attribute Inference(https://arxiv.org/abs/2401.06802)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>The popularization of social media increases user engagements and generates a large amount of user-oriented data. Among them, text data (e.g., tweets, blogs) significantly attracts researchers and speculators to infer user attributes (e.g., age, gender, location) for fulfilling their intents. Generally, this line of work casts attribute inference as a text classification problem, and starts to leverage graph neural networks (GNNs) to utilize higher-level representations of source texts. However, these text graphs are constructed over words, suffering from high memory consumption and ineffectiveness on few labeled texts. To address this challenge, we design a text-graph-based few-shot learning model for attribute inferences on social media text data. Our model first constructs and refines a text graph using manifold learning and message passing, which offers a better trade-off between expressiveness and complexity. Afterwards, to further use cross-domain texts and unlabeled texts to improve few-shot performance, a hierarchical knowledge distillation is devised over text graph to optimize the problem, which derives better text representations, and advances model generalization ability. Experiments on social media datasets demonstrate the state-of-the-art performance of our model on attribute inferences with considerably fewer labeled texts.</li>
<li><strong>摘要：</strong>社交媒体的普及增加了用户参与度并产生大量面向用户的数据。其中，文本数据（例如推文、博客）极大地吸引了研究人员和投机者推断用户属性（例如年龄、性别、位置）以实现他们的意图。一般来说，这方面的工作将属性推断视为文本分类问题，并开始利用图神经网络（GNN）来利用源文本的更高级别表示。然而，这些文本图是基于单词构建的，存在高内存消耗和对少量标记文本无效的问题。为了应对这一挑战，我们设计了一种基于文本图的小样本学习模型，用于社交媒体文本数据的属性推断。我们的模型首先使用流形学习和消息传递构建和完善文本图，这在表达性和复杂性之间提供了更好的权衡。之后，为了进一步使用跨域文本和未标记文本来提高小样本性能，在文本图上设计了分层知识蒸馏来优化问题，从而得出更好的文本表示，并提高模型泛化能力。对社交媒体数据集的实验证明了我们的模型在属性推断上的最先进性能，并且标记文本少得多。</li>
</ul>

<h3>Title: Generative AI Meets Semantic Communication: Evolution and Revolution of  Communication Tasks</h3>
<ul>
<li><strong>Authors: </strong>Eleonora Grassucci, Jihong Park, Sergio Barbarossa, Seong-Lyun Kim, Jinho Choi, Danilo Comminiello</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06803">https://arxiv.org/abs/2401.06803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06803">https://arxiv.org/pdf/2401.06803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06803]] Generative AI Meets Semantic Communication: Evolution and Revolution of  Communication Tasks(https://arxiv.org/abs/2401.06803)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>While deep generative models are showing exciting abilities in computer vision and natural language processing, their adoption in communication frameworks is still far underestimated. These methods are demonstrated to evolve solutions to classic communication problems such as denoising, restoration, or compression. Nevertheless, generative models can unveil their real potential in semantic communication frameworks, in which the receiver is not asked to recover the sequence of bits used to encode the transmitted (semantic) message, but only to regenerate content that is semantically consistent with the transmitted message. Disclosing generative models capabilities in semantic communication paves the way for a paradigm shift with respect to conventional communication systems, which has great potential to reduce the amount of data traffic and offers a revolutionary versatility to novel tasks and applications that were not even conceivable a few years ago. In this paper, we present a unified perspective of deep generative models in semantic communication and we unveil their revolutionary role in future communication frameworks, enabling emerging applications and tasks. Finally, we analyze the challenges and opportunities to face to develop generative models specifically tailored for communication systems.</li>
<li><strong>摘要：</strong>虽然深度生成模型在计算机视觉和自然语言处理方面显示出令人兴奋的能力，但它们在通信框架中的采用仍然被远远低估。这些方法被证明可以改进经典通信问题的解决方案，例如去噪、恢复或压缩。尽管如此，生成模型可以揭示它们在语义通信框架中的真正潜力，其中接收者不需要恢复用于编码传输的（语义）消息的比特序列，而只是重新生成在语义上与传输的消息一致的内容。公开语义通信中的生成模型功能为传统通信系统的范式转变铺平了道路，传统通信系统具有减少数据流量的巨大潜力，并为几年前无法想象的新颖任务和应用程序提供了革命性的多功能性前。在本文中，我们提出了语义通信中深度生成模型的统一视角，并揭示了它们在未来通信框架中的革命性作用，从而支持新兴的应用程序和任务。最后，我们分析了开发专门为通信系统定制的生成模型所面临的挑战和机遇。</li>
</ul>

<h3>Title: ChatGPT, Let us Chat Sign Language: Experiments, Architectural Elements,  Challenges and Research Directions</h3>
<ul>
<li><strong>Authors: </strong>Nada Shahin, Leila Ismail</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06804">https://arxiv.org/abs/2401.06804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06804">https://arxiv.org/pdf/2401.06804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06804]] ChatGPT, Let us Chat Sign Language: Experiments, Architectural Elements,  Challenges and Research Directions(https://arxiv.org/abs/2401.06804)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>ChatGPT is a language model based on Generative AI. Existing research work on ChatGPT focused on its use in various domains. However, its potential for Sign Language Translation (SLT) is yet to be explored. This paper addresses this void. Therefore, we present GPT's evolution aiming a retrospective analysis of the improvements to its architecture for SLT. We explore ChatGPT's capabilities in translating different sign languages in paving the way to better accessibility for deaf and hard-of-hearing community. Our experimental results indicate that ChatGPT can accurately translate from English to American (ASL), Australian (AUSLAN), and British (BSL) sign languages and from Arabic Sign Language (ArSL) to English with only one prompt iteration. However, the model failed to translate from Arabic to ArSL and ASL, AUSLAN, and BSL to Arabic. Consequently, we present challenges and derive insights for future research directions.</li>
<li><strong>摘要：</strong>ChatGPT 是一种基于生成式 AI 的语言模型。 ChatGPT 的现有研究工作集中于其在各个领域的使用。然而，其手语翻译 (SLT) 的潜力仍有待探索。本文解决了这一空白。因此，我们提出 GPT 的演变，旨在回顾性分析其 SLT 架构的改进。我们探索 ChatGPT 翻译不同手语的能力，为聋哑和听力障碍社区更好地无障碍服务铺平道路。我们的实验结果表明，ChatGPT 只需一次快速迭代即可准确地将英语翻译为美国手语 (ASL)、澳大利亚手语 (AUSLAN) 和英国手语 (BSL)，以及从阿拉伯手语 (ArSL) 翻译为英语。但是，该模型未能将阿拉伯语翻译为 ArSL，也未能将 ASL、AUSLAN 和 BSL 翻译为阿拉伯语。因此，我们提出了挑战并得出对未来研究方向的见解。</li>
</ul>

<h3>Title: Exploring the Reasoning Abilities of Multimodal Large Language Models  (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06805">https://arxiv.org/abs/2401.06805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06805">https://arxiv.org/pdf/2401.06805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06805]] Exploring the Reasoning Abilities of Multimodal Large Language Models  (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning(https://arxiv.org/abs/2401.06805)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.</li>
<li><strong>摘要：</strong>具有抽象推理能力的强人工智能（Strong AI）或通用人工智能（AGI）是下一代人工智能的目标。大型语言模型 (LLM) 的最新进展以及新兴的多模态大型语言模型 (MLLM) 领域在各种多模态任务和应用程序中展示了令人印象深刻的功能。特别是，各种 MLLM 都具有不同的模型架构、训练数据和训练阶段，已在广泛的 MLLM 基准上进行了评估。这些研究在不同程度上揭示了 MLLM 当前能力的不同方面。然而，MLLM 的推理能力尚未得到系统研究。在本次调查中，我们全面回顾了现有的多模态推理评估协议，对 MLLM 的前沿进行了分类和说明，介绍了 MLLM 在推理密集型任务中应用的最新趋势，最后讨论了当前的实践和未来的方向。我们相信我们的调查为多模态推理这一重要主题奠定了坚实的基础并阐明了这一点。</li>
</ul>

<h3>Title: AugSumm: towards generalizable speech summarization using synthetic  labels from large language model</h3>
<ul>
<li><strong>Authors: </strong>Jee-weon Jung, Roshan Sharma, William Chen, Bhiksha Raj, Shinji Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06806">https://arxiv.org/abs/2401.06806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06806">https://arxiv.org/pdf/2401.06806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06806]] AugSumm: towards generalizable speech summarization using synthetic  labels from large language model(https://arxiv.org/abs/2401.06806)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, rag</a></li>
<li><strong>Abstract: </strong>Abstractive speech summarization (SSUM) aims to generate human-like summaries from speech. Given variations in information captured and phrasing, recordings can be summarized in multiple ways. Therefore, it is more reasonable to consider a probabilistic distribution of all potential summaries rather than a single summary. However, conventional SSUM models are mostly trained and evaluated with a single ground-truth (GT) human-annotated deterministic summary for every recording. Generating multiple human references would be ideal to better represent the distribution statistically, but is impractical because annotation is expensive. We tackle this challenge by proposing AugSumm, a method to leverage large language models (LLMs) as a proxy for human annotators to generate augmented summaries for training and evaluation. First, we explore prompting strategies to generate synthetic summaries from ChatGPT. We validate the quality of synthetic summaries using multiple metrics including human evaluation, where we find that summaries generated using AugSumm are perceived as more valid to humans. Second, we develop methods to utilize synthetic summaries in training and evaluation. Experiments on How2 demonstrate that pre-training on synthetic summaries and fine-tuning on GT summaries improves ROUGE-L by 1 point on both GT and AugSumm-based test sets. AugSumm summaries are available at https://github.com/Jungjee/AugSumm.</li>
<li><strong>摘要：</strong>抽象语音摘要（SSUM）旨在从语音生成类似人类的摘要。鉴于捕获的信息和措辞的变化，可以通过多种方式对录音进行总结。因此，考虑所有潜在摘要的概率分布而不是单个摘要更为合理。然而，传统的 SSUM 模型大多使用每个记录的单个真实值 (GT) 人工注释的确定性摘要进行训练和评估。生成多个人类参考对于更好地表示统计分布来说是理想的选择，但由于注释成本高昂，因此不切实际。我们通过提出 AugSumm 来应对这一挑战，这是一种利用大型语言模型 (LLM) 作为人类注释者的代理来生成用于训练和评估的增强摘要的方法。首先，我们探索从 ChatGPT 生成综合摘要的提示策略。我们使用包括人工评估在内的多种指标来验证合成摘要的质量，我们发现使用 AugSumm 生成的摘要对人类来说更有效。其次，我们开发了在培训和评估中利用综合摘要的方法。 How2 上的实验表明，综合摘要的预训练和 GT 摘要的微调在 GT 和基于 AugSumm 的测试集上将 ROUGE-L 提高了 1 个点。 AugSumm 摘要可在 https://github.com/Jungjee/AugSumm 获取。</li>
</ul>

<h3>Title: An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue  Assistant</h3>
<ul>
<li><strong>Authors: </strong>Mohit Tomar, Abhisek Tiwari, Tulika Saha, Prince Jha, Sriparna Saha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06807">https://arxiv.org/abs/2401.06807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06807">https://arxiv.org/pdf/2401.06807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06807]] An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue  Assistant(https://arxiv.org/abs/2401.06807)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In recent times, there has been an increasing awareness about imminent environmental challenges, resulting in people showing a stronger dedication to taking care of the environment and nurturing green life. The current $19.6 billion indoor gardening industry, reflective of this growing sentiment, not only signifies a monetary value but also speaks of a profound human desire to reconnect with the natural world. However, several recent surveys cast a revealing light on the fate of plants within our care, with more than half succumbing primarily due to the silent menace of improper care. Thus, the need for accessible expertise capable of assisting and guiding individuals through the intricacies of plant care has become paramount more than ever. In this work, we make the very first attempt at building a plant care assistant, which aims to assist people with plant(-ing) concerns through conversations. We propose a plant care conversational dataset named Plantational, which contains around 1K dialogues between users and plant care experts. Our end-to-end proposed approach is two-fold : (i) We first benchmark the dataset with the help of various large language models (LLMs) and visual language model (VLM) by studying the impact of instruction tuning (zero-shot and few-shot prompting) and fine-tuning techniques on this task; (ii) finally, we build EcoSage, a multi-modal plant care assisting dialogue generation framework, incorporating an adapter-based modality infusion using a gated mechanism. We performed an extensive examination (both automated and manual evaluation) of the performance exhibited by various LLMs and VLM in the generation of the domain-specific dialogue responses to underscore the respective strengths and weaknesses of these diverse models.</li>
<li><strong>摘要：</strong>近年来，人们对迫在眉睫的环境挑战的认识不断增强，人们更加致力于保护环境、培育绿色生活。目前价值 196 亿美元的室内园艺产业反映了这种日益增长的情绪，不仅意味着货币价值，还体现了人类与自然世界重新联系的深刻愿望。然而，最近的几项调查揭示了我们照顾的植物的命运，超过一半的植物死亡主要是由于照顾不当造成的无声威胁。因此，对能够帮助和指导个人完成复杂的植物护理的可用专业知识的需求变得比以往任何时候都更加重要。在这项工作中，我们首次尝试构建一个植物护理助手，旨在通过对话帮助人们解决植物问题。我们提出了一个名为 Plantational 的植物护理对话数据集，其中包含用户和植物护理专家之间的大约 1K 对话。我们提出的端到端方法有两个方面：（i）我们首先通过研究指令调整（零样本）的影响，借助各种大型语言模型（LLM）和视觉语言模型（VLM）对数据集进行基准测试和少样本提示）以及该任务的微调技术； (ii)最后，我们构建了 EcoSage，一个多模式植物护理辅助对话生成框架，其中使用门控机制结合了基于适配器的模式注入。我们对各种 LLM 和 VLM 在生成特定领域对话响应时所表现出的性能进行了广泛的检查（自动和手动评估），以强调这些不同模型各自的优点和缺点。</li>
</ul>

<h3>Title: When ChatGPT is gone: Creativity reverts and homogeneity persists</h3>
<ul>
<li><strong>Authors: </strong>Qinghan Liu, Yiyong Zhou, Jihao Huang, Guiquan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06816">https://arxiv.org/abs/2401.06816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06816">https://arxiv.org/pdf/2401.06816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06816]] When ChatGPT is gone: Creativity reverts and homogeneity persists(https://arxiv.org/abs/2401.06816)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>ChatGPT has been evidenced to enhance human performance in creative tasks. Yet, it is still unclear if this boosting effect sustains with and without ChatGPT. In a pre-registered seven-day lab experiment and a follow-up survey after 30 days of experiment completion, we examined the impacts of ChatGPT presence and absence on sustained creativity using a text dataset of 3302 creative ideas and 427 creative solutions from 61 college students. Participants in the treatment group used ChatGPT in creative tasks, while those in the control group completed the tasks by themselves. The findings show that although the boosting effect of ChatGPT was consistently observed over a five-day creative journey, human creative performance reverted to baseline when ChatGPT was down on the 7th and the 30th day. More critically, the use of ChatGPT in creative tasks resulted in increasingly homogenized contents, and this homogenization effect persisted even when ChatGPT was absence. These findings pose a challenge to the prevailing argument that ChatGPT can enhance human creativity. In fact, generative AI like ChatGPT lends to human with a temporary rise in creative performance but boxes human creative capability in the long run, highlighting the imperative for cautious generative AI integration in creative endeavors.</li>
<li><strong>摘要：</strong>ChatGPT 已被证明可以提高人类在创造性任务中的表现。然而，目前尚不清楚无论有没有 ChatGPT，这种促进作用是否都能持续。在预先注册的 7 天实验室实验和实验完成 30 天后的后续调查中，我们使用来自 61 所大学的 3302 个创意想法和 427 个创意解决方案的文本数据集，研究了 ChatGPT 存在和不存在对持续创造力的影响学生。治疗组的参与者在创造性任务中使用 ChatGPT，而对照组的参与者则自行完成任务。研究结果表明，尽管在为期五天的创意之旅中始终观察到 ChatGPT 的促进作用，但当 ChatGPT 在第 7 天和第 30 天下降时，人类的创意表现恢复到基线。更关键的是，在创意任务中使用ChatGPT导致内容日益同质化，即使没有ChatGPT，这种同质化效应仍然持续存在。这些发现对 ChatGPT 可以增强人类创造力的流行观点提出了挑战。事实上，像 ChatGPT 这样的生成式人工智能可以暂时提升人类的创造性表现，但从长远来看会限制人类的创造性能力，这凸显了在创造性工作中谨慎集成生成式人工智能的必要性。</li>
</ul>

<h3>Title: Analyzing Regional Impacts of Climate Change using Natural Language  Processing Techniques</h3>
<ul>
<li><strong>Authors: </strong>Tanwi Mallick, John Murphy, Joshua David Bergerson, Duane R. Verner, John K Hutchison, Leslie-Anne Levy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06817">https://arxiv.org/abs/2401.06817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06817">https://arxiv.org/pdf/2401.06817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06817]] Analyzing Regional Impacts of Climate Change using Natural Language  Processing Techniques(https://arxiv.org/abs/2401.06817)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code</a></li>
<li><strong>Abstract: </strong>Understanding the multifaceted effects of climate change across diverse geographic locations is crucial for timely adaptation and the development of effective mitigation strategies. As the volume of scientific literature on this topic continues to grow exponentially, manually reviewing these documents has become an immensely challenging task. Utilizing Natural Language Processing (NLP) techniques to analyze this wealth of information presents an efficient and scalable solution. By gathering extensive amounts of peer-reviewed articles and studies, we can extract and process critical information about the effects of climate change in specific regions. We employ BERT (Bidirectional Encoder Representations from Transformers) for Named Entity Recognition (NER), which enables us to efficiently identify specific geographies within the climate literature. This, in turn, facilitates location-specific analyses. We conduct region-specific climate trend analyses to pinpoint the predominant themes or concerns related to climate change within a particular area, trace the temporal progression of these identified issues, and evaluate their frequency, severity, and potential development over time. These in-depth examinations of location-specific climate data enable the creation of more customized policy-making, adaptation, and mitigation strategies, addressing each region's unique challenges and providing more effective solutions rooted in data-driven insights. This approach, founded on a thorough exploration of scientific texts, offers actionable insights to a wide range of stakeholders, from policymakers to engineers to environmentalists. By proactively understanding these impacts, societies are better positioned to prepare, allocate resources wisely, and design tailored strategies to cope with future climate conditions, ensuring a more resilient future for all.</li>
<li><strong>摘要：</strong>了解气候变化对不同地理位置的多方面影响对于及时适应和制定有效的缓解战略至关重要。随着有关该主题的科学文献数量持续呈指数级增长，手动审查这些文档已成为一项极具挑战性的任务。利用自然语言处理 (NLP) 技术来分析这些丰富的信息提供了一种高效且可扩展的解决方案。通过收集大量经过同行评审的文章和研究，我们可以提取和处理有关特定地区气候变化影响的关键信息。我们采用 BERT（来自 Transformers 的双向编码器表示）进行命名实体识别 (NER)，这使我们能够有效地识别气候文献中的特定地理位置。这反过来又促进了特定地点的分析。我们进行特定区域的气候趋势分析，以查明特定区域内与气候变化相关的主要主题或担忧，追踪这些已识别问题的时间进展，并评估其频率、严重性和随着时间的推移的潜在发展。这些对特定地点气候数据的深入研究有助于制定更加定制的政策制定、适应和缓解战略，解决每个地区的独特挑战，并提供基于数据驱动的见解的更有效的解决方案。这种方法建立在对科学文本的彻底探索的基础上，为从政策制定者到工程师再到环保主义者的广泛利益相关者提供了可行的见解。通过主动了解这些影响，社会能够更好地做好准备，明智地分配资源，并设计量身定制的战略来应对未来的气候条件，确保所有人拥有一个更有弹性的未来。</li>
</ul>

<h3>Title: Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation  Engineering</h3>
<ul>
<li><strong>Authors: </strong>Tianlong Li, Xiaoqing Zheng, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06824">https://arxiv.org/abs/2401.06824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06824">https://arxiv.org/pdf/2401.06824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06824]] Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation  Engineering(https://arxiv.org/abs/2401.06824)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Getting large language models (LLMs) to refuse to answer hostile toxicity questions is a core issue under the theme of LLMs security. Previous approaches have used prompts engineering to jailbreak LLMs and answer some toxicity questions. These approaches can easily fail after the model manufacturer makes additional fine-tuning to the model. To promote the further understanding of model jailbreaking by researchers, we are inspired by Representation Engineering to propose a jailbreaking method that does not require elaborate construction prompts, is not affected by model fine-tuning, and can be widely applied to any open-source LLMs in a pluggable manner. We have evaluated this method on multiple mainstream LLMs on carefully supplemented toxicity datasets, and the experimental results demonstrate the significant effectiveness of our approach. After being surprised by some interesting jailbreaking cases, we did extensive in-depth research to explore the techniques behind this method.</li>
<li><strong>摘要：</strong>让大型语言模型（LLM）拒绝回答恶意毒性问题是 LLM 安全主题下的一个核心问题。以前的方法使用提示工程来越狱法学硕士并回答一些毒性问题。当模型制造商对模型进行额外的微调后，这些方法很容易失败。为了促进研究者对模型越狱的进一步理解，我们受到表示工程的启发，提出了一种不需要精细构建提示、不受模型微调影响、可以广泛应用于任何开源LLM的越狱方法以可插拔的方式。我们在多个主流法学硕士的仔细补充的毒性数据集上评估了这种方法，实验结果证明了我们方法的显着有效性。在对一些有趣的越狱案例感到惊讶之后，我们进行了广泛深入的研究来探索这种方法背后的技术。</li>
</ul>

<h3>Title: Direct Distillation between Different Domains</h3>
<ul>
<li><strong>Authors: </strong>Jialiang Tang, Shuo Chen, Gang Niu, Hongyuan Zhu, Joey Tianyi Zhou, Chen Gong, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06826">https://arxiv.org/abs/2401.06826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06826">https://arxiv.org/pdf/2401.06826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06826]] Direct Distillation between Different Domains(https://arxiv.org/abs/2401.06826)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Knowledge Distillation (KD) aims to learn a compact student network using knowledge from a large pre-trained teacher network, where both networks are trained on data from the same distribution. However, in practical applications, the student network may be required to perform in a new scenario (i.e., the target domain), which usually exhibits significant differences from the known scenario of the teacher network (i.e., the source domain). The traditional domain adaptation techniques can be integrated with KD in a two-stage process to bridge the domain gap, but the ultimate reliability of two-stage approaches tends to be limited due to the high computational consumption and the additional errors accumulated from both stages. To solve this problem, we propose a new one-stage method dubbed ``Direct Distillation between Different Domains" (4Ds). We first design a learnable adapter based on the Fourier transform to separate the domain-invariant knowledge from the domain-specific knowledge. Then, we build a fusion-activation mechanism to transfer the valuable domain-invariant knowledge to the student network, while simultaneously encouraging the adapter within the teacher network to learn the domain-specific knowledge of the target data. As a result, the teacher network can effectively transfer categorical knowledge that aligns with the target domain of the student network. Intensive experiments on various benchmark datasets demonstrate that our proposed 4Ds method successfully produces reliable student networks and outperforms state-of-the-art approaches.</li>
<li><strong>摘要：</strong>知识蒸馏（KD）旨在使用来自大型预训练教师网络的知识来学习紧凑的学生网络，其中两个网络都接受来自相同分布的数据的训练。然而，在实际应用中，学生网络可能需要在新的场景（即目标域）中执行，这通常与教师网络的已知场景（即源域）表现出显着差异。传统的域自适应技术可以在两阶段过程中与 KD 集成以弥合域差距，但由于高计算消耗和两阶段累积的额外误差，两阶段方法的最终可靠性往往受到限制。为了解决这个问题，我们提出了一种新的单阶段方法，称为“不同领域之间的直接蒸馏”（4D）。我们首先设计一个基于傅立叶变换的可学习适配器，以将领域不变知识与领域特定知识分开然后，我们建立了一种融合激活机制，将有价值的领域不变知识转移到学生网络，同时鼓励教师网络内的适配器学习目标数据的领域特定知识。网络可以有效地传输与学生网络目标域一致的分类知识。对各种基准数据集的深入实验表明，我们提出的 4D 方法成功地生成了可靠的学生网络，并且性能优于最先进的方法。</li>
</ul>

<h3>Title: Cross-Attention Watermarking of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Folco Bertini Baldassini, Huy H. Nguyen, Ching-Chung Chang, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06829">https://arxiv.org/abs/2401.06829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06829">https://arxiv.org/pdf/2401.06829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06829]] Cross-Attention Watermarking of Large Language Models(https://arxiv.org/abs/2401.06829)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora</a></li>
<li><strong>Abstract: </strong>A new approach to linguistic watermarking of language models is presented in which information is imperceptibly inserted into the output text while preserving its readability and original meaning. A cross-attention mechanism is used to embed watermarks in the text during inference. Two methods using cross-attention are presented that minimize the effect of watermarking on the performance of a pretrained model. Exploration of different training strategies for optimizing the watermarking and of the challenges and implications of applying this approach in real-world scenarios clarified the tradeoff between watermark robustness and text quality. Watermark selection substantially affects the generated output for high entropy sentences. This proactive watermarking approach has potential application in future model development.</li>
<li><strong>摘要：</strong>提出了一种新的语言模型语言水印方法，其中信息不知不觉地插入到输出文本中，同时保留其可读性和原始含义。交叉注意力机制用于在推理过程中在文本中嵌入水印。提出了两种使用交叉注意力的方法，可以最大限度地减少水印对预训练模型性能的影响。对优化水印的不同训练策略的探索以及在现实场景中应用这种方法的挑战和影响阐明了水印鲁棒性和文本质量之间的权衡。水印选择很大程度上影响高熵句子的生成输出。这种主动的水印方法在未来的模型开发中具有潜在的应用。</li>
</ul>

<h3>Title: A Survey on the Applications of Frontier AI, Foundation Models, and  Large Language Models to Intelligent Transportation Systems</h3>
<ul>
<li><strong>Authors: </strong>Mohamed R. Shoaib, Heba M. Emara, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06831">https://arxiv.org/abs/2401.06831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06831">https://arxiv.org/pdf/2401.06831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06831]] A Survey on the Applications of Frontier AI, Foundation Models, and  Large Language Models to Intelligent Transportation Systems(https://arxiv.org/abs/2401.06831)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, rag</a></li>
<li><strong>Abstract: </strong>This survey paper explores the transformative influence of frontier AI, foundation models, and Large Language Models (LLMs) in the realm of Intelligent Transportation Systems (ITS), emphasizing their integral role in advancing transportation intelligence, optimizing traffic management, and contributing to the realization of smart cities. Frontier AI refers to the forefront of AI technology, encompassing the latest advancements, innovations, and experimental techniques in the field, especially AI foundation models and LLMs. Foundation models, like GPT-4, are large, general-purpose AI models that provide a base for a wide range of applications. They are characterized by their versatility and scalability. LLMs are obtained from finetuning foundation models with a specific focus on processing and generating natural language. They excel in tasks like language understanding, text generation, translation, and summarization. By leveraging vast textual data, including traffic reports and social media interactions, LLMs extract critical insights, fostering the evolution of ITS. The survey navigates the dynamic synergy between LLMs and ITS, delving into applications in traffic management, integration into autonomous vehicles, and their role in shaping smart cities. It provides insights into ongoing research, innovations, and emerging trends, aiming to inspire collaboration at the intersection of language, intelligence, and mobility for safer, more efficient, and sustainable transportation systems. The paper further surveys interactions between LLMs and various aspects of ITS, exploring roles in traffic management, facilitating autonomous vehicles, and contributing to smart city development, while addressing challenges brought by frontier AI and foundation models. This paper offers valuable inspiration for future research and innovation in the transformative domain of intelligent transportation.</li>
<li><strong>摘要：</strong>本调查论文探讨了前沿人工智能、基础模型和大型语言模型 (LLM) 在智能交通系统 (ITS) 领域的变革性影响，强调了它们在推进交通智能、优化交通管理和促进实现智能交通方面的不可或缺的作用。智慧城市。前沿人工智能是指人工智能技术的最前沿，涵盖该领域的最新进展、创新和实验技术，特别是人工智能基础模型和法学硕士。基础模型（如 GPT-4）是大型通用 AI 模型，为广泛的应用程序提供基础。它们的特点是多功能性和可扩展性。法学硕士是通过微调基础模型获得的，特别关注处理和生成自然语言。他们擅长语言理解、文本生成、翻译和摘要等任务。通过利用大量文本数据（包括流量报告和社交媒体互动），法学硕士可以提取关键见解，促进 ITS 的发展。该调查探讨了法学硕士和智能交通系统之间的动态协同作用，深入研究了交通管理中的应用、自动驾驶汽车的集成及其在塑造智慧城市中的作用。它提供了对正在进行的研究、创新和新兴趋势的见解，旨在激发语言、智能和移动性交叉领域的合作，以实现更安全、更高效和可持续的交通系统。该论文进一步调查了法学硕士与智能交通系统各个方面之间的相互作用，探讨了在交通管理、促进自动驾驶汽车和为智慧城市发展做出贡献方面的作用，同时应对前沿人工智能和基础模型带来的挑战。本文为智能交通变革领域的未来研究和创新提供了宝贵的启发。</li>
</ul>

<h3>Title: XLS-R Deep Learning Model for Multilingual ASR on Low- Resource  Languages: Indonesian, Javanese, and Sundanese</h3>
<ul>
<li><strong>Authors: </strong>Panji Arisaputra, Alif Tri Handoyo, Amalia Zahra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06832">https://arxiv.org/abs/2401.06832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06832">https://arxiv.org/pdf/2401.06832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06832]] XLS-R Deep Learning Model for Multilingual ASR on Low- Resource  Languages: Indonesian, Javanese, and Sundanese(https://arxiv.org/abs/2401.06832)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This research paper focuses on the development and evaluation of Automatic Speech Recognition (ASR) technology using the XLS-R 300m model. The study aims to improve ASR performance in converting spoken language into written text, specifically for Indonesian, Javanese, and Sundanese languages. The paper discusses the testing procedures, datasets used, and methodology employed in training and evaluating the ASR systems. The results show that the XLS-R 300m model achieves competitive Word Error Rate (WER) measurements, with a slight compromise in performance for Javanese and Sundanese languages. The integration of a 5-gram KenLM language model significantly reduces WER and enhances ASR accuracy. The research contributes to the advancement of ASR technology by addressing linguistic diversity and improving performance across various languages. The findings provide insights into optimizing ASR accuracy and applicability for diverse linguistic contexts.</li>
<li><strong>摘要：</strong>本研究论文重点介绍使用 XLS-R 300m 模型的自动语音识别 (ASR) 技术的开发和评估。该研究旨在提高 ASR 将口语转换为书面文本的性能，特别是印度尼西亚语、爪哇语和巽他语。本文讨论了测试程序、使用的数据集以及训练和评估 ASR 系统所采用的方法。结果表明，XLS-R 300m 模型实现了具有竞争力的字错误率 (WER) 测量，但爪哇语和巽他语的性能略有下降。 5-gram KenLM 语言模型的集成显着降低了 WER 并提高了 ASR 准确性。该研究通过解决语言多样性和提高各种语言的性能，为 ASR 技术的进步做出了贡献。研究结果为优化 ASR 准确性和对不同语言环境的适用性提供了见解。</li>
</ul>

<h3>Title: Enhancing the Emotional Generation Capability of Large Language Models  via Emotional Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Zaijing Li, Gongwei Chen, Rui Shao, Dongmei Jiang, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06836">https://arxiv.org/abs/2401.06836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06836">https://arxiv.org/pdf/2401.06836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06836]] Enhancing the Emotional Generation Capability of Large Language Models  via Emotional Chain-of-Thought(https://arxiv.org/abs/2401.06836)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>The Emotional Generation is a subset of emotional intelligence, which aims to output an emotional response based on emotional conditions as input. Emotion generation has a wide range of applications, including emotion chat, emotional visual caption, and emotional rewriting. However, it faces challenges such as a lack of interpretability and poor evaluability. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that enhances the performance of Large Language Models (LLMs) on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called EGS. Extensive experimental results demonstrate the effectiveness of ECoT and EGS. Further,we discuss the promise of LLMs in the field of sentiment analysis and present key insights into the LLMs with the ECoT in emotional generation tasks.</li>
<li><strong>摘要：</strong>情绪生成是情绪智力的一个子集，旨在根据情绪条件作为输入输出情绪反应。情感生成具有广泛的应用，包括情感聊天、情感视觉字幕和情感重写。但它也面临着缺乏可解释性、可评价性差等挑战。在本文中，我们提出了情绪思维链（ECOT），这是一种即插即用的提示方法，通过与人类情绪智力指南保持一致，增强大型语言模型（LLM）在各种情绪生成任务上的性能。为了评估 ECoT 的可靠性，我们提出了一种基于模型的自动化评估方法，称为 EGS。大量实验结果证明了 ECoT 和 EGS 的有效性。此外，我们讨论了法学硕士在情感分析领域的前景，并提出了法学硕士与 ECoT 在情感生成任务中的关键见解。</li>
</ul>

<h3>Title: Structsum Generation for Faster Text Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Parag Jain, Andreea Marzoca, Francesco Piccinno</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06837">https://arxiv.org/abs/2401.06837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06837">https://arxiv.org/pdf/2401.06837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06837]] Structsum Generation for Faster Text Comprehension(https://arxiv.org/abs/2401.06837)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>We consider the task of generating structured representations of text using large language models (LLMs). We focus on tables and mind maps as representative modalities. Tables are more organized way of representing data, while mind maps provide a visually dynamic and flexible approach, particularly suitable for sparse content. Despite the effectiveness of LLMs on different tasks, we show that current models struggle with generating structured outputs. In response, we present effective prompting strategies for both of these tasks. We introduce a taxonomy of problems around factuality, global and local structure, common to both modalities and propose a set of critiques to tackle these issues resulting in an absolute improvement in accuracy of +37pp (79%) for mind maps and +15pp (78%) for tables. To evaluate semantic coverage of generated structured representations we propose Auto-QA, and we verify the adequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of structured representations via a text comprehension user study. The results show a significant reduction in comprehension time compared to text when using table (42.9%) and mind map (31.9%), without loss in accuracy.</li>
<li><strong>摘要：</strong>我们考虑使用大型语言模型（LLM）生成文本的结构化表示的任务。我们专注于表格和思维导图作为代表性模式。表格是更有条理的数据表示方式，而思维导图提供了一种视觉上动态且灵活的方法，特别适合稀疏内容。尽管法学硕士在不同的任务上都很有效，但我们表明当前的模型在生成结构化输出方面存在困难。作为回应，我们为这两项任务提出了有效的激励策略。我们引入了两种模式所共有的围绕事实性、全局和局部结构的问题分类，并提出了一系列批评来解决这些问题，从而使思维导图的准确度绝对提高了 +37pp (79%) 和 +15pp (78) %) 用于表格。为了评估生成的结构化表示的语义覆盖率，我们提出了 Auto-QA，并使用 SQuAD 数据集验证了 Auto-QA 的充分性。我们通过文本理解用户研究进一步评估结构化表示的有用性。结果显示，使用表格 (42.9%) 和思维导图 (31.9%) 时，与文本相比，理解时间显着减少，且准确性没有损失。</li>
</ul>

<h3>Title: MAPO: Advancing Multilingual Reasoning through Multilingual  Alignment-as-Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Shuaijie She, Shujian Huang, Wei Zou, Wenhao Zhu, Xiang Liu, Xiang Geng, Jiajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06838">https://arxiv.org/abs/2401.06838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06838">https://arxiv.org/pdf/2401.06838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06838]] MAPO: Advancing Multilingual Reasoning through Multilingual  Alignment-as-Preference Optimization(https://arxiv.org/abs/2401.06838)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in a pivot language is superior to other languages due to the imbalance of multilingual training data.To enhance reasoning abilities in non-pivot languages, we propose an alignment-as-preference optimization framework. Specifically, we adopt an open-source translation model to estimate the consistency between answers in non-pivot and pivot languages. We further adopt the answer consistency as the preference for DPO or PPO thus optimizing the lesser reasoning. Experiments show that our method significantly improves the model's multilingual reasoning, with better reasoning consistency across languages. Our framework achieved a 13.7% accuracy improvement on out-of-domain datasets MSVAMP while preserving the competitive performance on MGSM. Moreover, we find that iterative DPO is helpful for further alignment and improvement of the model's multilingual mathematical reasoning ability, further pushing the improvement to 16.7%</li>
<li><strong>摘要：</strong>尽管推理能力被认为与语言无关，但现有的法学硕士在不同语言之间表现出不一致的推理能力，例如，由于多语言训练数据的不平衡，枢轴语言的推理优于其他语言。为了增强非枢轴语言的推理能力，我们提出了一种按偏好排列的优化框架。具体来说，我们采用开源翻译模型来估计非枢轴语言和枢轴语言答案之间的一致性。我们进一步采用答案一致性作为 DPO 或 PPO 的偏好，从而优化次要推理。实验表明，我们的方法显着提高了模型的多语言推理能力，跨语言推理一致性更好。我们的框架在域外数据集 MSVAMP 上实现了 13.7% 的精度提升，同时保持了 MGSM 上的竞争性能。此外，我们发现迭代DPO有助于模型多语言数学推理能力的进一步对齐和提升，进一步将提升推至16.7%</li>
</ul>

<h3>Title: Large Language Models Can Learn Temporal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Siheng Xiong, Ali Payani, Ramana Kompella, Faramarz Fekri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06853">https://arxiv.org/abs/2401.06853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06853">https://arxiv.org/pdf/2401.06853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06853]] Large Language Models Can Learn Temporal Reasoning(https://arxiv.org/abs/2401.06853)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) learn temporal concepts from the co-occurrence of related tokens in a sequence. Compared with conventional text generation, temporal reasoning, which reaches a conclusion based on mathematical, logical and commonsense knowledge, is more challenging. In this paper, we propose TempGraph-LLM, a new paradigm towards text-based temporal reasoning. To be specific, we first teach LLMs to translate the context into a temporal graph. A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for pre-training on this task. We prove in experiments that LLMs benefit from the pre-training on other tasks. On top of that, we guide LLMs to perform symbolic reasoning with the strategies of Chain of Thoughts (CoTs) bootstrapping and special data augmentation. We observe that CoTs with symbolic reasoning bring more consistent and reliable results than those using free text.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 从序列中相关标记的共现中学习时间概念。与传统的文本生成相比，基于数学、逻辑和常识知识得出结论的时间推理更具挑战性。在本文中，我们提出了 TempGraph-LLM，这是一种基于文本的时间推理的新范式。具体来说，我们首先教法学硕士将上下文转换为时间图。构建了一个完全可控且需要最少监督的合成数据集，用于对该任务进行预训练。我们在实验中证明，法学硕士可以从其他任务的预训练中受益。最重要的是，我们指导法学硕士通过思想链（CoT）引导和特殊数据增强策略进行符号推理。我们观察到，使用符号推理的 CoT 比使用自由文本的 CoT 带来更加一致和可靠的结果。</li>
</ul>

<h3>Title: Fine-grained Hallucination Detection and Editing for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06855">https://arxiv.org/abs/2401.06855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06855">https://arxiv.org/pdf/2401.06855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06855]] Fine-grained Hallucination Detection and Editing for Language Models(https://arxiv.org/abs/2401.06855)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT on fine-grained hallucination detection by a large margin though a large room for future improvement still exists. FAVA's suggested edits also improve the factuality of LM-generated text, resulting in 5-10% FActScore improvements.</li>
<li><strong>摘要：</strong>大型语言模型（LM）很容易产生各种事实上不正确的陈述，这被广泛称为幻觉。当前的方法主要集中于粗粒度的自动幻觉检测或编辑，忽略了细微的错误级别。在本文中，我们提出了一项新任务——自动细粒度幻觉检测——并提出了包含六种分层定义的幻觉类型的综合分类法。为了便于评估，我们引入了一个新的基准，其中包括对跨不同领域的两个 LM 输出的细粒度人类判断。我们的分析表明，ChatGPT 和 Llama 2-Chat 的输出中分别有 60% 和 75% 表现出幻觉，其中大多数幻觉属于尚未充分探索的类别。作为解决这个问题的第一步，我们通过仔细设计合成数据生成来训练 FAVA，这是一种检索增强的 LM，以检测和纠正细粒度的幻觉。在我们的基准测试中，我们的自动和人工评估表明，FAVA 在细粒度幻觉检测方面明显优于 ChatGPT，尽管未来仍然存在很大的改进空间。 FAVA 建议的编辑还可以提高 LM 生成文本的真实性，从而使 FActScore 提高 5-10%。</li>
</ul>

<h3>Title: Health-LLM: Large Language Models for Health Prediction via Wearable  Sensor Data</h3>
<ul>
<li><strong>Authors: </strong>Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, Hae Won Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06866">https://arxiv.org/abs/2401.06866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06866">https://arxiv.org/pdf/2401.06866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06866]] Health-LLM: Large Language Models for Health Prediction via Wearable  Sensor Data(https://arxiv.org/abs/2401.06866)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and non-linguistic data is important. This paper investigates the capacity of LLMs to deliver multi-modal health predictions based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting and fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps, GLOBEM, AW_FB, MIT-BIH & MIMIC-III). Our experiments cover thirteen consumer health prediction tasks in mental health, activity, metabolic, sleep, and cardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable performance to larger models (GPT-3.5 and GPT-4), achieving the best performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness of context enhancement strategies, and generalization capability of the fine-tuned models across training datasets and the size of training samples. Notably, we observe that our context enhancement can yield up to 23.8% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 能够执行许多自然语言任务，但它们还远非完美。在健康应用中，基础和解释特定领域和非语言数据非常重要。本文研究了法学硕士根据情境信息（例如用户人口统计、健康知识）和生理数据（例如静息心率、睡眠时间）提供多模式健康预测的能力。我们对八个最先进的法学硕士在六个公共卫生数据集（PM-Data、LifeSnaps、GLOBEM、AW_FB、MIT-BIH 和 MIMIC-III）上采用不同的提示和微调技术进行了全面评估。我们的实验涵盖心理健康、活动、代谢、睡眠和心脏评估等十三个消费者健康预测任务。我们的微调模型 Health-Alpaca 表现出与较大模型（GPT-3.5 和 GPT-4）相当的性能，在 13 项任务中的 5 项中实现了最佳性能。消融研究强调了上下文增强策略的有效性，以及跨训练数据集和训练样本大小的微调模型的泛化能力。值得注意的是，我们观察到上下文增强可以使性能提高高达 23.8%。虽然构建上下文丰富的提示（结合用户上下文、健康知识和时间信息）表现出协同改进，但在提示中包含健康知识上下文可显着提高整体性能。</li>
</ul>

<h3>Title: Promptly Predicting Structures: The Return of Inference</h3>
<ul>
<li><strong>Authors: </strong>Maitrey Mehta, Valentina Pyatkin, Vivek Srikumar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06877">https://arxiv.org/abs/2401.06877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06877">https://arxiv.org/pdf/2401.06877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06877]] Promptly Predicting Structures: The Return of Inference(https://arxiv.org/abs/2401.06877)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints -- and combinatorial inference derived from them -- to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants.</li>
<li><strong>摘要：</strong>基于提示的方法已在 NLP 中广泛使用，以构建零样本和少样本标签预测器。许多 NLP 任务都是自然结构化的：也就是说，它们的输出由多个相互约束的标签组成。为此类任务注释数据可能很麻烦。基于提示的范式的承诺可以扩展到这种结构化的输出吗？在本文中，我们提出了一个构建零样本和少样本语言结构预测器的框架。我们的主要见解是，我们可以使用结构约束以及从中得出的组合推理来过滤掉大型语言模型预测的不一致结构。我们在两个结构化预测任务和五个数据集上实例化了该框架。在所有情况下，我们的结果表明，强制一致性不仅可以构造结构上有效的输出，而且还可以提高无约束变体的性能。</li>
</ul>

<h3>Title: An Axiomatic Approach to Model-Agnostic Concept Explanations</h3>
<ul>
<li><strong>Authors: </strong>Zhili Feng, Michal Moshkovitz, Dotan Di Castro, J. Zico Kolter</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06890">https://arxiv.org/abs/2401.06890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06890">https://arxiv.org/pdf/2401.06890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06890]] An Axiomatic Approach to Model-Agnostic Concept Explanations(https://arxiv.org/abs/2401.06890)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Concept explanation is a popular approach for examining how human-interpretable concepts impact the predictions of a model. However, most existing methods for concept explanations are tailored to specific models. To address this issue, this paper focuses on model-agnostic measures. Specifically, we propose an approach to concept explanations that satisfy three natural axioms: linearity, recursivity, and similarity. We then establish connections with previous concept explanation methods, offering insight into their varying semantic meanings. Experimentally, we demonstrate the utility of the new method by applying it in different scenarios: for model selection, optimizer selection, and model improvement using a kind of prompt editing for zero-shot vision language models.</li>
<li><strong>摘要：</strong>概念解释是一种流行的方法，用于检查人类可解释的概念如何影响模型的预测。然而，大多数现有的概念解释方法都是针对特定模型量身定制的。为了解决这个问题，本文重点关注与模型无关的措施。具体来说，我们提出了一种满足三个自然公理的概念解释方法：线性、递归性和相似性。然后，我们与以前的概念解释方法建立联系，深入了解它们不同的语义含义。在实验上，我们通过将新方法应用于不同的场景来展示其实用性：用于模型选择、优化器选择和使用零样本视觉语言模型的提示编辑进行模型改进。</li>
</ul>

<h3>Title: Always-Sparse Training by Growing Connections with Guided Stochastic  Exploration</h3>
<ul>
<li><strong>Authors: </strong>Mike Heddes, Narayan Srinivasa, Tony Givargis, Alexandru Nicolau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06898">https://arxiv.org/abs/2401.06898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06898">https://arxiv.org/pdf/2401.06898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06898]] Always-Sparse Training by Growing Connections with Guided Stochastic  Exploration(https://arxiv.org/abs/2401.06898)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>The excessive computational requirements of modern artificial neural networks (ANNs) are posing limitations on the machines that can run them. Sparsification of ANNs is often motivated by time, memory and energy savings only during model inference, yielding no benefits during training. A growing body of work is now focusing on providing the benefits of model sparsification also during training. While these methods greatly improve the training efficiency, the training algorithms yielding the most accurate models still materialize the dense weights, or compute dense gradients during training. We propose an efficient, always-sparse training algorithm with excellent scaling to larger and sparser models, supported by its linear time complexity with respect to the model width during training and inference. Moreover, our guided stochastic exploration algorithm improves over the accuracy of previous sparse training methods. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG, and ViT models, and compare it against a range of sparsification methods.</li>
<li><strong>摘要：</strong>现代人工神经网络 (ANN) 过多的计算要求对运行它们的机器造成了限制。人工神经网络的稀疏化通常只是在模型推理过程中出于时间、内存和能源节省的目的，在训练过程中不会产生任何好处。现在越来越多的工作致力于在训练期间提供模型稀疏化的好处。虽然这些方法极大地提高了训练效率，但产生最准确模型的训练算法仍然在训练期间具体化密集权重或计算密集梯度。我们提出了一种高效、始终稀疏的训练算法，能够出色地扩展到更大和更稀疏的模型，并得到其在训练和推理期间相对于模型宽度的线性时间复杂度的支持。此外，我们的引导随机探索算法比以前的稀疏训练方法的准确性有所提高。我们使用 ResNet、VGG 和 ViT 模型在 CIFAR-10/100 和 ImageNet 上评估我们的方法，并将其与一系列稀疏方法进行比较。</li>
</ul>

<h3>Title: DocFinQA: A Long-Context Financial Reasoning Dataset</h3>
<ul>
<li><strong>Authors: </strong>Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Chris Tanner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06915">https://arxiv.org/abs/2401.06915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06915">https://arxiv.org/pdf/2401.06915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06915]] DocFinQA: A Long-Context Financial Reasoning Dataset(https://arxiv.org/abs/2401.06915)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Research in quantitative reasoning within the financial domain indeed necessitates the use of realistic tasks and data, primarily because of the significant impact of decisions made in business and finance. Financial professionals often interact with documents hundreds of pages long, but most research datasets drastically reduce this context length. To address this, we introduce a long-document financial QA task. We augment 7,621 questions from the existing FinQA dataset with full-document context, extending the average context length for each question from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments of retrieval-based QA pipelines and long-context language models on the augmented data. Our results show that DocFinQA provides challenges for even the strongest, state-of-the-art systems.</li>
<li><strong>摘要：</strong>金融领域的定量推理研究确实需要使用现实的任务和数据，这主要是因为商业和金融决策的重大影响。金融专业人士经常与数百页长的文档进行交互，但大多数研究数据集大大缩短了上下文长度。为了解决这个问题，我们引入了一项长文档的财务质量保证任务。我们使用完整文档上下文扩充了现有 FinQA 数据集中的 7,621 个问题，将每个问题的平均上下文长度从 FinQA 中的 700 个单词以下扩展到 DocFinQA 中的 123k 个单词。我们对增强数据进行了基于检索的 QA 管道和长上下文语言模型的广泛实验。我们的结果表明，即使是最强大、最先进的系统，DocFinQA 也面临着挑战。</li>
</ul>

<h3>Title: Comparing GPT-4 and Open-Source Language Models in Misinformation  Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Tyler Vergho, Jean-Francois Godbout, Reihaneh Rabbany, Kellin Pelrine</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06920">https://arxiv.org/abs/2401.06920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06920">https://arxiv.org/pdf/2401.06920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06920]] Comparing GPT-4 and Open-Source Language Models in Misinformation  Mitigation(https://arxiv.org/abs/2401.06920)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have been shown to be effective for misinformation detection. However, the choice of LLMs for experiments varies widely, leading to uncertain conclusions. In particular, GPT-4 is known to be strong in this domain, but it is closed source, potentially expensive, and can show instability between different versions. Meanwhile, alternative LLMs have given mixed results. In this work, we show that Zephyr-7b presents a consistently viable alternative, overcoming key limitations of commonly used approaches like Llama-2 and GPT-3.5. This provides the research community with a solid open-source option and shows open-source models are gradually catching up on this task. We then highlight how GPT-3.5 exhibits unstable performance, such that this very widely used model could provide misleading results in misinformation detection. Finally, we validate new tools including approaches to structured output and the latest version of GPT-4 (Turbo), showing they do not compromise performance, thus unlocking them for future research and potentially enabling more complex pipelines for misinformation mitigation.</li>
<li><strong>摘要：</strong>最近的大型语言模型（LLM）已被证明对于错误信息检测是有效的。然而，实验法学硕士的选择差异很大，导致结论不确定。特别是，GPT-4 众所周知在该领域很强大，但它是闭源的，可能很昂贵，并且在不同版本之间可能表现出不稳定。与此同时，替代法学硕士的结果好坏参半。在这项工作中，我们展示了 Zephyr-7b 提供了一种始终可行的替代方案，克服了 Llama-2 和 GPT-3.5 等常用方法的关键限制。这为研究社区提供了可靠的开源选项，并表明开源模型正在逐渐赶上这项任务。然后，我们重点介绍 GPT-3.5 如何表现出不稳定的性能，使得这种广泛使用的模型可能在错误信息检测中提供误导性结果。最后，我们验证了新工具，包括结构化输出方法和最新版本的 GPT-4 (Turbo)，表明它们不会影响性能，从而为未来的研究解锁它们，并有可能启用更复杂的管道来缓解错误信息。</li>
</ul>

<h3>Title: Open RAN LSTM Traffic Prediction and Slice Management using Deep  Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Lotfi, Fatemeh Afghah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI, eess.SY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06922">https://arxiv.org/abs/2401.06922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06922">https://arxiv.org/pdf/2401.06922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06922]] Open RAN LSTM Traffic Prediction and Slice Management using Deep  Reinforcement Learning(https://arxiv.org/abs/2401.06922)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag, agent</a></li>
<li><strong>Abstract: </strong>With emerging applications such as autonomous driving, smart cities, and smart factories, network slicing has become an essential component of 5G and beyond networks as a means of catering to a service-aware network. However, managing different network slices while maintaining quality of services (QoS) is a challenge in a dynamic environment. To address this issue, this paper leverages the heterogeneous experiences of distributed units (DUs) in ORAN systems and introduces a novel approach to ORAN slicing xApp using distributed deep reinforcement learning (DDRL). Additionally, to enhance the decision-making performance of the RL agent, a prediction rApp based on long short-term memory (LSTM) is incorporated to provide additional information from the dynamic environment to the xApp. Simulation results demonstrate significant improvements in network performance, particularly in reducing QoS violations. This emphasizes the importance of using the prediction rApp and distributed actors' information jointly as part of a dynamic xApp.</li>
<li><strong>摘要：</strong>随着自动驾驶、智慧城市和智能工厂等新兴应用的出现，网络切片已成为 5G 及其他网络的重要组成部分，成为服务感知网络的一种手段。然而，在动态环境中，管理不同的网络切片并同时保持服务质量 (QoS) 是一项挑战。为了解决这个问题，本文利用 ORAN 系统中分布式单元 (DU) 的异构体验，并介绍了一种使用分布式深度强化学习 (DDRL) 进行 ORAN 切片 xApp 的新颖方法。此外，为了增强 RL 代理的决策性能，结合了基于长短期记忆 (LSTM) 的预测 rApp，以向 xApp 提供来自动态环境的附加信息。仿真结果表明网络性能显着提高，特别是在减少 QoS 违规方面。这强调了将预测 rApp 和分布式参与者信息联合使用作为动态 xApp 的一部分的重要性。</li>
</ul>

<h3>Title: Minimally Supervised Learning using Topological Projections in  Self-Organizing Maps</h3>
<ul>
<li><strong>Authors: </strong>Zimeng Lyu, Alexander Ororbia, Rui Li, Travis Desell</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06923">https://arxiv.org/abs/2401.06923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06923">https://arxiv.org/pdf/2401.06923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06923]] Minimally Supervised Learning using Topological Projections in  Self-Organizing Maps(https://arxiv.org/abs/2401.06923)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Parameter prediction is essential for many applications, facilitating insightful interpretation and decision-making. However, in many real life domains, such as power systems, medicine, and engineering, it can be very expensive to acquire ground truth labels for certain datasets as they may require extensive and expensive laboratory testing. In this work, we introduce a semi-supervised learning approach based on topological projections in self-organizing maps (SOMs), which significantly reduces the required number of labeled data points to perform parameter prediction, effectively exploiting information contained in large unlabeled datasets. Our proposed method first trains SOMs on unlabeled data and then a minimal number of available labeled data points are ultimately assigned to key best matching units (BMU). The values estimated for newly-encountered data points are computed utilizing the average of the $n$ closest labeled data points in the SOM's U-matrix in tandem with a topological shortest path distance calculation scheme. Our results indicate that the proposed semi-supervised model significantly outperforms traditional regression techniques, including linear and polynomial regression, Gaussian process regression, K-nearest neighbors, as well as various deep neural network models.</li>
<li><strong>摘要：</strong>参数预测对于许多应用至关重要，有助于有洞察力的解释和决策。然而，在许多现实生活领域，例如电力系统、医学和工程，获取某些数据集的真实标签可能非常昂贵，因为它们可能需要广泛且昂贵的实验室测试。在这项工作中，我们引入了一种基于自组织映射（SOM）中的拓扑投影的半监督学习方法，该方法显着减少了执行参数预测所需的标记数据点数量，有效地利用了大型未标记数据集中包含的信息。我们提出的方法首先在未标记数据上训练 SOM，然后将最少数量的可用标记数据点最终分配给关键最佳匹配单元 (BMU)。新遇到的数据点的估计值是利用 SOM 的 U 矩阵中 $n$ 个最接近标记数据点的平均值以及拓扑最短路径距离计算方案来计算的。我们的结果表明，所提出的半监督模型显着优于传统的回归技术，包括线性和多项式回归、高斯过程回归、K 最近邻以及各种深度神经网络模型。</li>
</ul>

<h3>Title: Modeling Latent Selection with Structural Causal Models</h3>
<ul>
<li><strong>Authors: </strong>Leihao Chen, Onno Zoeter, Joris M. Mooij</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, math.ST, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06925">https://arxiv.org/abs/2401.06925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06925">https://arxiv.org/pdf/2401.06925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06925]] Modeling Latent Selection with Structural Causal Models(https://arxiv.org/abs/2401.06925)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection bias and how the conditioning operation helps with modeling of real-world problems.</li>
<li><strong>摘要：</strong>选择偏差在现实世界的数据中普遍存在，如果处理不当，可能会导致误导性的结果。我们引入了对结构因果模型（SCM）的条件操作，以从因果角度对潜在选择进行建模。我们表明，条件操作将存在显式潜在选择机制的 SCM 转换为没有这种选择机制的 SCM，它根据原始 SCM 部分编码所选子群的因果语义。此外，我们表明这种调节操作保留了 SCM 的简单性、非循环性和线性性，并且可以进行边缘化。由于这些特性，再加上边缘化和干预，条件操作提供了一种有价值的工具，可以在因果模型中执行因果推理任务，其中潜在的细节已被抽象出来。我们通过示例演示了如何将因果推理的经典结果概括为包括选择偏差，以及条件操作如何帮助对现实世界问题进行建模。</li>
</ul>

<h3>Title: PizzaCommonSense: Learning to Model Commonsense Reasoning about  Intermediate Steps in Cooking Recipes</h3>
<ul>
<li><strong>Authors: </strong>Aissatou Diallo, Antonis Bikakis, Luke Dickens, Anthony Hunter, Rob Miller</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06930">https://arxiv.org/abs/2401.06930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06930">https://arxiv.org/pdf/2401.06930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06930]] PizzaCommonSense: Learning to Model Commonsense Reasoning about  Intermediate Steps in Cooking Recipes(https://arxiv.org/abs/2401.06930)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>Decoding the core of procedural texts, exemplified by cooking recipes, is crucial for intelligent reasoning and instruction automation. Procedural texts can be comprehensively defined as a sequential chain of steps to accomplish a task employing resources. From a cooking perspective, these instructions can be interpreted as a series of modifications to a food preparation, which initially comprises a set of ingredients. These changes involve transformations of comestible resources. For a model to effectively reason about cooking recipes, it must accurately discern and understand the inputs and outputs of intermediate steps within the recipe. Aiming to address this, we present a new corpus of cooking recipes enriched with descriptions of intermediate steps of the recipes that explicate the input and output for each step. We discuss the data collection process, investigate and provide baseline models based on T5 and GPT-3.5. This work presents a challenging task and insight into commonsense reasoning and procedural text generation.</li>
<li><strong>摘要：</strong>解码程序文本的核心（以烹饪食谱为例）对于智能推理和指令自动化至关重要。程序文本可以全面定义为使用资源完成任务的一系列顺序步骤。从烹饪的角度来看，这些说明可以解释为对食物准备的一系列修改，食物准备最初包含一组成分。这些变化涉及食物资源的转变。对于有效推理烹饪食谱的模型，它必须准确识别和理解食谱中中间步骤的输入和输出。为了解决这个问题，我们提出了一个新的烹饪食谱语料库，其中丰富了食谱中间步骤的描述，解释了每个步骤的输入和输出。我们讨论数据收集过程，研究并提供基于 T5 和 GPT-3.5 的基线模型。这项工作提出了一项具有挑战性的任务，并深入了解常识推理和程序文本生成。</li>
</ul>

<h3>Title: Knowledge-Centric Templatic Views of Documents</h3>
<ul>
<li><strong>Authors: </strong>Isabel Cachola, Silviu Cucerzan, Allen Herring, Vuksan Mijovic, Erik Oveson, Sujay Kumar Jauhar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06945">https://arxiv.org/abs/2401.06945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06945">https://arxiv.org/pdf/2401.06945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06945]] Knowledge-Centric Templatic Views of Documents(https://arxiv.org/abs/2401.06945)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, rag</a></li>
<li><strong>Abstract: </strong>Authors seeking to communicate with broader audiences often compose their ideas about the same underlying knowledge in different documents and formats -- for example, as slide decks, newsletters, reports, brochures, etc. Prior work in document generation has generally considered the creation of each separate format to be different a task, developing independent methods for generation and evaluation. This approach is suboptimal for the advancement of AI-supported content authoring from both research and application perspectives because it leads to fragmented learning processes, redundancy in models and methods, and disjointed evaluation. Thus, in our work, we consider each of these documents to be templatic views of the same underlying knowledge, and we aim to unify the generation and evaluation of these templatic views of documents. We begin by introducing an LLM-powered method to extract the most important information from an input document and represent this information in a structured format. We show that this unified representation can be used to generate multiple templatic views with no supervision and with very little guidance, improving over strong baselines. We additionally introduce a unified evaluation method that is template agnostic, and can be adapted to building document generators for heterogeneous downstream applications. Finally, we conduct a human evaluation, which shows that humans prefer 82% of the downstream documents generated with our method. Furthermore, the newly proposed evaluation metric correlates more highly with human judgement than prior metrics, while providing a unified evaluation method.</li>
<li><strong>摘要：</strong>寻求与更广泛的受众交流的作者通常会在不同的文档和格式中撰写关于相同基础知识的想法——例如幻灯片、新闻通讯、报告、小册子等。文档生成的先前工作通常考虑了每个文档的创建不同的任务采用不同的格式，开发独立的生成和评估方法。从研究和应用的角度来看，这种方法对于人工智能支持的内容创作的发展并不是最理想的，因为它会导致学习过程碎片化、模型和方法冗余以及评估脱节。因此，在我们的工作中，我们将这些文档中的每一个视为相同基础知识的模板视图，并且我们的目标是统一这些文档模板视图的生成和评估。我们首先介绍一种由法学硕士支持的方法，从输入文档中提取最重要的信息，并以结构化格式表示这些信息。我们证明，这种统一的表示可用于在没有监督和很少指导的情况下生成多个模板视图，从而在强基线上进行改进。我们还引入了一种与模板无关的统一评估方法，可以适用于为异构下游应用程序构建文档生成器。最后，我们进行了人类评估，结果表明人类更喜欢使用我们的方法生成的 82% 的下游文档。此外，新提出的评估指标比先前的指标与人类判断的相关性更高，同时提供了统一的评估方法。</li>
</ul>

<h3>Title: Parameter-Efficient Detoxification with Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Tong Niu, Caiming Xiong, Semih Yavuz, Yingbo Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06947">https://arxiv.org/abs/2401.06947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06947">https://arxiv.org/pdf/2401.06947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06947]] Parameter-Efficient Detoxification with Contrastive Decoding(https://arxiv.org/abs/2401.06947)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, rag</a></li>
<li><strong>Abstract: </strong>The field of natural language generation has witnessed significant advancements in recent years, including the development of controllable text generation techniques. However, controlling the attributes of the generated text remains a challenge, especially when aiming to avoid undesirable behavior such as toxicity. In this work, we introduce Detoxification Generator (DETOXIGEN), an inference-time algorithm that steers the generation away from unwanted styles. DETOXIGEN is an ensemble of a pre-trained language model (generator) and a detoxifier. The detoxifier is trained intentionally on the toxic data representative of the undesirable attribute, encouraging it to generate text in that style exclusively. During the actual generation, we use the trained detoxifier to produce undesirable tokens for the generator to contrast against at each decoding step. This approach directly informs the generator to avoid generating tokens that the detoxifier considers highly likely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS benchmark (Gehman et al., 2020) with various language models as generators. We find that it significantly outperforms previous approaches in detoxification metrics while not compromising on the generation quality. Moreover, the detoxifier is obtained by soft prompt-tuning using the same backbone language model as the generator. Hence, DETOXIGEN requires only a tiny amount of extra weights from the virtual tokens of the detoxifier to be loaded into GPU memory while decoding, making it a promising lightweight, practical, and parameter-efficient detoxification strategy.</li>
<li><strong>摘要：</strong>近年来，自然语言生成领域取得了重大进步，包括可控文本生成技术的发展。然而，控制生成文本的属性仍然是一个挑战，特别是在旨在避免不良行为（例如毒性）时。在这项工作中，我们引入了解毒生成器（DETOXIGEN），这是一种推理时间算法，可以引导生成远离不需要的样式。 DETOXIGEN 是预训练语言模型（生成器）和解毒器的集合。解毒器有意识地根据代表不良属性的有毒数据进行训练，鼓励它专门生成该风格的文本。在实际生成过程中，我们使用经过训练的解毒器来生成不需要的标记，以便生成器在每个解码步骤中进行对比。这种方法直接通知生成器避免生成解毒器认为很有可能的令牌。我们使用各种语言模型作为生成器，在常用的 REALTOXICITYPROMPTS 基准（Gehman 等人，2020）上评估 DETOXIGEN。我们发现它在解毒指标方面显着优于以前的方法，同时又不影响发电质量。此外，解毒器是通过使用与生成器相同的主干语言模型进行软提示调整而获得的。因此，DETOXIGEN 在解码时只需要从解毒器的虚拟令牌中加载少量额外权重到 GPU 内存中，使其成为一种有前景的轻量级、实用且参数高效的解毒策略。</li>
</ul>

<h3>Title: E^2-LLM: Efficient and Extreme Length Extension of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, Tiezheng Ge, Jie Fu, Wenhu Chen, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06951">https://arxiv.org/abs/2401.06951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06951">https://arxiv.org/pdf/2401.06951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06951]] E^2-LLM: Efficient and Extreme Length Extension of Large Language Models(https://arxiv.org/abs/2401.06951)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context</a></li>
<li><strong>Abstract: </strong>Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we introduce two different augmentation methods on the scale and position index parameters for different samples in training. It aims to make the model more robust to the different relative differences when directly interpolating the arbitrary context length at inference. Comprehensive experimental results on multiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on challenging long-context tasks.</li>
<li><strong>摘要：</strong>通常，训练具有长上下文大小的法学硕士的计算成本很高，需要大量的训练时间和 GPU 资源。现有的长上下文扩展方法通常需要额外的训练过程来支持相应的长上下文窗口，其中需要长上下文训练数据（例如32k），并且假设GPU训练成本很高。为了解决上述问题，我们提出了一种高效且极端的大型语言模型长度扩展方法，称为 E 2 -LLM，只需一个训练过程，大大降低了计算成本，也消除了收集长上下文数据的需要。具体来说，首先，我们的E 2 -LLM的训练数据只需要很短的长度（例如4k），这大大降低了调整成本。其次，短训练上下文窗口上的训练过程仅执行一次，并且我们可以在推理时支持不同的评估上下文窗口。第三，在E 2 - LLM中，基于RoPE位置嵌入，我们针对训练中的不同样本引入了两种不同的尺度和位置索引参数增强方法。其目的是使模型在推理时直接插值任意上下文长度时对不同的相对差异更加鲁棒。多个基准数据集的综合实验结果证明了我们的 E 2 -LLM 在具有挑战性的长上下文任务上的有效性。</li>
</ul>

<h3>Title: Reinforcement Learning for Scalable Train Timetable Rescheduling with  Graph Representation</h3>
<ul>
<li><strong>Authors: </strong>Peng Yue, Yaochu Jin, Xuewu Dai, Zhenhua Feng, Dongliang Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06952">https://arxiv.org/abs/2401.06952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06952">https://arxiv.org/pdf/2401.06952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06952]] Reinforcement Learning for Scalable Train Timetable Rescheduling with  Graph Representation(https://arxiv.org/abs/2401.06952)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Train timetable rescheduling (TTR) aims to promptly restore the original operation of trains after unexpected disturbances or disruptions. Currently, this work is still done manually by train dispatchers, which is challenging to maintain performance under various problem instances. To mitigate this issue, this study proposes a reinforcement learning-based approach to TTR, which makes the following contributions compared to existing work. First, we design a simple directed graph to represent the TTR problem, enabling the automatic extraction of informative states through graph neural networks. Second, we reformulate the construction process of TTR's solution, not only decoupling the decision model from the problem size but also ensuring the generated scheme's feasibility. Third, we design a learning curriculum for our model to handle the scenarios with different levels of delay. Finally, a simple local search method is proposed to assist the learned decision model, which can significantly improve solution quality with little additional computation cost, further enhancing the practical value of our method. Extensive experimental results demonstrate the effectiveness of our method. The learned decision model can achieve better performance for various problems with varying degrees of train delay and different scales when compared to handcrafted rules and state-of-the-art solvers.</li>
<li><strong>摘要：</strong>列车时刻表重新安排（TTR）旨在在意外干扰或中断后迅速恢复列车原来的运行。目前，这项工作仍然由列车调度员手动完成，这对于在各种问题情况下保持性能具有挑战性。为了缓解这个问题，本研究提出了一种基于强化学习的 TTR 方法，与现有工作相比，该方法做出了以下贡献。首先，我们设计一个简单的有向图来表示 TTR 问题，从而能够通过图神经网络自动提取信息状态。其次，我们重新制定了TTR解决方案的构建过程，不仅将决策模型与问题规模解耦，而且确保了生成方案的可行性。第三，我们为我们的模型设计了一个学习课程，以处理不同延迟程度的场景。最后，提出了一种简单的局部搜索方法来辅助学习的决策模型，该方法可以在几乎没有额外计算成本的情况下显着提高解决方案质量，进一步增强了我们方法的实用价值。大量的实验结果证明了我们方法的有效性。与手工制定的规则和最先进的求解器相比，学习的决策模型可以针对不同程度的列车延迟和不同规模的各种问题获得更好的性能。</li>
</ul>

<h3>Title: Bridging the Preference Gap between Retrievers and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06954">https://arxiv.org/abs/2401.06954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06954">https://arxiv.org/pdf/2401.06954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06954]] Bridging the Preference Gap between Retrievers and LLMs(https://arxiv.org/abs/2401.06954)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, while retrieval has long been established as an effective means of obtaining task-relevant information for humans. Retrieval-augmented Generation (RAG) are known for their effectiveness in knowledge-intensive tasks by locating relevant information and placing it within the context window of the LLM. However, the relationship between retrievers and LLMs is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-friendly information and assembling a LLM-friendly context. In this work, we examine a novel bridge model, validate the ranking and selection assumptions in retrievers in the context of RAG, and propose a training framework that chains together supervised and reinforcement learning to learn a bridge model. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在广泛的任务中表现出了卓越的结果，而检索长期以来一直被认为是人类获取任务相关信息的有效手段。检索增强生成（RAG）以其在知识密集型任务中的有效性而闻名，它可以定位相关信息并将其放置在法学硕士的上下文窗口中。然而，猎犬和法学硕士之间的关系仍然没有得到充分研究。大多数现有工作将检索器和 LLM 视为独立的组件，并在检索人类友好的信息和组装 LLM 友好的上下文之间留下了差距。在这项工作中，我们研究了一种新颖的桥梁模型，验证了 RAG 背景下检索器的排名和选择假设，并提出了一个将监督学习和强化学习链接在一起以学习桥梁模型的训练框架。实证结果证明了我们的方法在问答和个性化生成任务中的有效性。</li>
</ul>

<h3>Title: CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs'  Mathematical Reasoning Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Yujun Mao, Yoon Kim, Yilun Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06961">https://arxiv.org/abs/2401.06961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06961">https://arxiv.org/pdf/2401.06961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06961]] CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs'  Mathematical Reasoning Capabilities(https://arxiv.org/abs/2401.06961)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, code, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Recent large language models (LLMs) have shown indications of mathematical reasoning ability. However it has not been clear how they would fare on more challenging competition-level problems. And while self-generated verbalizations of intermediate reasoning steps (i.e., chain-of-thought prompting) have been shown to be helpful, whether LLMs can make use of helpful side information such as problem-specific hints has not been investigated before. In this paper, we propose a challenging benchmark dataset for enabling such analyses. The Concept and Hint-Annotated Math Problems (CHAMP) consists of high school math competition problems, annotated with concepts, or general math facts, and hints, or problem-specific tricks. These annotations allow us to explore the effects of additional information, such as relevant hints, misleading concepts, or related problems. This benchmark is difficult, with the best model only scoring 58.1% in standard settings. With concepts and hints, performance sometimes improves, indicating that some models can make use of such side information. We further annotate model-generated solutions for their correctness. Using this corpus, we find that models often arrive at the correct final answer through wrong reasoning steps. In addition, we test whether models are able to verify these solutions, and find that most models struggle. The dataset and code are available on the project website.</li>
<li><strong>摘要：</strong>最近的大型语言模型（LLM）已经显示出数学推理能力的迹象。然而，目前尚不清楚他们在更具挑战性的竞争级别问题上的表现如何。虽然中间推理步骤的自我生成的语言表达（即思维链提示）已被证明是有帮助的，但法学硕士是否可以利用有用的辅助信息（例如特定问题的提示）之前尚未被研究过。在本文中，我们提出了一个具有挑战性的基准数据集来实现此类分析。概念和提示注释数学问题 (CHAMP) 由高中数学竞赛问题组成，用概念或一般数学事实以及提示或特定问题技巧进行注释。这些注释使我们能够探索附加信息的影响，例如相关提示、误导性概念或相关问题。这个基准测试很困难，最好的模型在标准设置下只得分 58.1%。通过概念和提示，性能有时会提高，这表明某些模型可以利用此类辅助信息。我们进一步注释模型生成的解决方案的正确性。使用这个语料库，我们发现模型经常通过错误的推理步骤得出正确的最终答案。此外，我们测试模型是否能够验证这些解决方案，发现大多数模型都遇到困难。数据集和代码可在项目网站上获取。</li>
</ul>

<h3>Title: Joint Unsupervised and Supervised Training for Automatic Speech  Recognition via Bilevel Optimization</h3>
<ul>
<li><strong>Authors: </strong>A F M Saif, Xiaodong Cui, Han Shen, Songtao Lu, Brian Kingsbury, Tianyi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.06980">https://arxiv.org/abs/2401.06980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.06980">https://arxiv.org/pdf/2401.06980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.06980]] Joint Unsupervised and Supervised Training for Automatic Speech  Recognition via Bilevel Optimization(https://arxiv.org/abs/2401.06980)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (ASR) tasks that we term {bi-level joint unsupervised and supervised training (BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an unsupervised loss and a supervised loss respectively, leveraging recent advances in penalty-based bilevel optimization to solve this challenging ASR problem with affordable complexity and rigorous convergence guarantees.} To evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2 datasets have been conducted. BL-JUST achieves superior performance over the commonly used pre-training followed by fine-tuning strategy.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: Extending LLMs' Context Window with 100 Samples</h3>
<ul>
<li><strong>Authors: </strong>Yikai Zhang, Junlong Li, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07004">https://arxiv.org/abs/2401.07004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07004">https://arxiv.org/pdf/2401.07004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07004]] Extending LLMs' Context Window with 100 Samples(https://arxiv.org/abs/2401.07004)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comparative experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://github.com/GAIR-NLP/Entropy-ABF.</li>
<li><strong>摘要：</strong>众所周知，大型语言模型（LLM）在其预先训练的上下文窗口之外的外推能力有限，限制了它们在具有冗长输入的下游任务中的应用。最近的研究试图通过修改旋转位置嵌入（RoPE）来扩展LLM的上下文窗口，RoPE是LLaMA、PaLM和GPT-NeoX等著名LLM采用的一种流行的位置编码方法。然而，诸如位置插值（PI）和 YaRN 之类的先前工作是资源密集型的，并且缺乏比较实验来评估其适用性。在这项工作中，我们确定了 LLM 注意力熵（即注意力分数的信息熵）保持稳定性的内在需求，并引入了 RoPE 的新颖扩展，它将调整 RoPE 的基频和缩放注意力逻辑结合起来，以帮助 LLM 有效地适应更大的上下文窗口。我们验证了我们的方法在各种上下文要求任务的不同上下文窗口大小的微调性能和鲁棒性方面的优越性。值得注意的是，我们的方法仅用 100 个样本和 6 个训练步骤就将 LLaMA-2-7B-Chat 的上下文窗口扩展到 16,384，展示了非凡的效率。最后，我们还探讨了数据构成和培训课程如何影响特定下游任务的上下文窗口扩展，建议以冗长的对话作为一个良好的起点来微调法学硕士。我们在 https://github.com/GAIR-NLP/Entropy-ABF 发布了我们的代码和 SFT 数据。</li>
</ul>

<h3>Title: Joint Extraction of Uyghur Medicine Knowledge with Edge Computing</h3>
<ul>
<li><strong>Authors: </strong>Fan Lu, Quan Qi, Huaibin Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07009">https://arxiv.org/abs/2401.07009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07009">https://arxiv.org/pdf/2401.07009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07009]] Joint Extraction of Uyghur Medicine Knowledge with Edge Computing(https://arxiv.org/abs/2401.07009)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Medical knowledge extraction methods based on edge computing deploy deep learning models on edge devices to achieve localized entity and relation extraction. This approach avoids transferring substantial sensitive data to cloud data centers, effectively safeguarding the privacy of healthcare services. However, existing relation extraction methods mainly employ a sequential pipeline approach, which classifies relations between determined entities after entity recognition. This mode faces challenges such as error propagation between tasks, insufficient consideration of dependencies between the two subtasks, and the neglect of interrelations between different relations within a sentence. To address these challenges, a joint extraction model with parameter sharing in edge computing is proposed, named CoEx-Bert. This model leverages shared parameterization between two models to jointly extract entities and relations. Specifically, CoEx-Bert employs two models, each separately sharing hidden layer parameters, and combines these two loss functions for joint backpropagation to optimize the model parameters. Additionally, it effectively resolves the issue of entity overlapping when extracting knowledge from unstructured Uyghur medical texts by considering contextual relations. Finally, this model is deployed on edge devices for real-time extraction and inference of Uyghur medical knowledge. Experimental results demonstrate that CoEx-Bert outperforms existing state-of-the-art methods, achieving accuracy, recall, and F1 scores of 90.65\%, 92.45\%, and 91.54\%, respectively, in the Uyghur traditional medical literature dataset. These improvements represent a 6.45\% increase in accuracy, a 9.45\% increase in recall, and a 7.95\% increase in F1 score compared to the baseline.</li>
<li><strong>摘要：</strong>基于边缘计算的医学知识提取方法在边缘设备上部署深度学习模型，实现局部实体和关系提取。这种方法避免了将大量敏感数据传输到云数据中心，有效保护了医疗服务的隐私。然而，现有的关系提取方法主要采用顺序管道方法，其在实体识别后对确定的实体之间的关系进行分类。这种模式面临着任务之间的错误传播、对两个子任务之间的依赖关系考虑不足、忽略句子内不同关系之间的相互关系等挑战。为了解决这些挑战，提出了一种边缘计算中具有参数共享的联合提取模型，名为 CoEx-Bert。该模型利用两个模型之间的共享参数化来联合提取实体和关系。具体来说，CoEx-Bert采用两个模型，每个模型分别共享隐藏层参数，并将这两个损失函数组合起来进行联合反向传播以优化模型参数。此外，它通过考虑上下文关系，有效解决了从非结构化维吾尔医学文本中提取知识时的实体重叠问题。最后，将该模型部署在边缘设备上，用于维吾尔医学知识的实时提取和推理。实验结果表明，CoEx-Bert 优于现有的最先进方法，在维吾尔族传统医学文献数据集中实现了准确率、召回率和 F1 分数分别为 90.65%、92.45% 和 91.54%。与基线相比，这些改进意味着准确率提高了 6.45%，召回率提高了 9.45%，F1 分数提高了 7.95%。</li>
</ul>

<h3>Title: Knowledge Distillation for Closed-Source Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hongzhan Chen, Xiaojun Quan, Hehong Chen, Ming Yan, Ji Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07013">https://arxiv.org/abs/2401.07013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07013">https://arxiv.org/pdf/2401.07013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07013]] Knowledge Distillation for Closed-Source Language Models(https://arxiv.org/abs/2401.07013)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, rag</a></li>
<li><strong>Abstract: </strong>Closed-source language models such as GPT-4 have achieved remarkable performance. Many recent studies focus on enhancing the capabilities of smaller models through knowledge distillation from closed-source language models. However, due to the incapability to directly access the weights, hidden states, and output distributions of these closed-source models, the distillation can only be performed by fine-tuning smaller models with data samples generated by closed-source language models, which constrains the effectiveness of knowledge distillation. In this paper, we propose to estimate the output distributions of closed-source language models within a Bayesian estimation framework, involving both prior and posterior estimation. The prior estimation aims to derive a prior distribution by utilizing the corpus generated by closed-source language models, while the posterior estimation employs a proxy model to update the prior distribution and derive a posterior distribution. By leveraging the estimated output distribution of closed-source language models, traditional knowledge distillation can be executed. Experimental results demonstrate that our method surpasses the performance of current models directly fine-tuned on data generated by closed-source language models.</li>
<li><strong>摘要：</strong>GPT-4等闭源语言模型已经取得了令人瞩目的性能。最近的许多研究侧重于通过闭源语言模型的知识蒸馏来增强较小模型的能力。然而，由于无法直接访问这些闭源模型的权重、隐藏状态和输出分布，只能通过闭源语言模型生成的数据样本对较小的模型进行微调来进行蒸馏，这限制了知识蒸馏的有效性。在本文中，我们建议在贝叶斯估计框架内估计闭源语言模型的输出分布，涉及先验估计和后验估计。先验估计旨在利用闭源语言模型生成的语料库推导先验分布，而后验估计则采用代理模型来更新先验分布并推导出后验分布。通过利用闭源语言模型的估计输出分布，可以执行传统知识蒸馏。实验结果表明，我们的方法超越了直接对闭源语言模型生成的数据进行微调的当前模型的性能。</li>
</ul>

<h3>Title: Edge-Enabled Anomaly Detection and Information Completion for Social  Network Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Fan Lu, Quan Qi, Huaibin Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07022">https://arxiv.org/abs/2401.07022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07022">https://arxiv.org/pdf/2401.07022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07022]] Edge-Enabled Anomaly Detection and Information Completion for Social  Network Knowledge Graphs(https://arxiv.org/abs/2401.07022)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>In the rapidly advancing information era, various human behaviors are being precisely recorded in the form of data, including identity information, criminal records, and communication data. Law enforcement agencies can effectively maintain social security and precisely combat criminal activities by analyzing the aforementioned data. In comparison to traditional data analysis methods, deep learning models, relying on the robust computational power in cloud centers, exhibit higher accuracy in extracting data features and inferring data. However, within the architecture of cloud centers, the transmission of data from end devices introduces significant latency, hindering real-time inference of data. Furthermore, low-latency edge computing architectures face limitations in direct deployment due to relatively weak computing and storage capacities of nodes. To address these challenges, a lightweight distributed knowledge graph completion architecture is proposed. Firstly, we introduce a lightweight distributed knowledge graph completion architecture that utilizes knowledge graph embedding for data analysis. Subsequently, to filter out substandard data, a personnel data quality assessment method named PDQA is proposed. Lastly, we present a model pruning algorithm that significantly reduces the model size while maximizing performance, enabling lightweight deployment. In experiments, we compare the effects of 11 advanced models on completing the knowledge graph of public security personnel information. The results indicate that the RotatE model outperforms other models significantly in knowledge graph completion, with the pruned model size reduced by 70\%, and hits@10 reaching 86.97\%.}</li>
<li><strong>摘要：</strong>在快速发展的信息时代，人类的各种行为正在以数据的形式被精确记录，包括身份信息、犯罪记录、通讯数据等。执法部门通过分析上述数据，可以有效维护社会治安，精准打击犯罪活动。与传统的数据分析方法相比，深度学习模型依靠云中心强大的计算能力，在提取数据特征和推断数据方面表现出更高的准确性。然而，在云中心的架构中，来自终端设备的数据传输会带来显着的延迟，阻碍数据的实时推理。此外，由于节点的计算和存储能力相对较弱，低延迟边缘计算架构在直接部署方面面临限制。为了解决这些挑战，提出了一种轻量级分布式知识图补全架构。首先，我们介绍了一种轻量级分布式知识图补全架构，该架构利用知识图嵌入进行数据分析。随后，为了过滤掉不合格数据，提出了一种名为PDQA的人员数据质量评估方法。最后，我们提出了一种模型修剪算法，可以显着减小模型大小，同时最大限度地提高性能，从而实现轻量级部署。在实验中，我们比较了11种先进模型在完成公安人员信息知识图谱方面的效果。结果表明，RotatE模型在知识图补全方面明显优于其他模型，剪枝后的模型大小减少了70\%，hits@10达到86.97\%。}</li>
</ul>

<h3>Title: xCoT: Cross-lingual Instruction Tuning for Cross-lingual  Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, Zhoujun Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07037">https://arxiv.org/abs/2401.07037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07037">https://arxiv.org/pdf/2401.07037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07037]] xCoT: Cross-lingual Instruction Tuning for Cross-lingual  Chain-of-Thought Reasoning(https://arxiv.org/abs/2401.07037)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) has emerged as a powerful technique to elicit reasoning in large language models and improve a variety of downstream tasks. CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization. To bridge the gap among different languages, we propose a cross-lingual instruction fine-tuning framework (xCOT) to transfer knowledge from high-resource languages to low-resource languages. Specifically, the multilingual instruction training data (xCOT-INSTRUCT) is created to encourage the semantic alignment of multiple languages. We introduce cross-lingual in-context few-shot learning (xICL)) to accelerate multilingual agreement in instruction tuning, where some fragments of source languages in examples are randomly substituted by their counterpart translations of target languages. During multilingual instruction tuning, we adopt the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results on previous benchmarks demonstrate the superior performance of xCoT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap.</li>
<li><strong>摘要：</strong>思想链 (CoT) 已成为一种强大的技术，可在大型语言模型中引发推理并改进各种下游任务。 CoT主要在英语中表现出优异的性能，但由于语言泛化能力差，其在低资源语言中的使用受到限制。为了弥合不同语言之间的差距，我们提出了一种跨语言指令微调框架（xCOT），将知识从高资源语言转移到低资源语言。具体来说，创建多语言教学训练数据（xCOT-INSTRUCT）是为了鼓励多种语言的语义对齐。我们引入跨语言上下文中的小样本学习（xICL）来加速指令调整中的多语言一致性，其中示例中的一些源语言片段被目标语言的对应翻译随机替换。在多语言指令调优过程中，我们采用随机在线CoT策略，首先将查询翻译成另一种语言，然后用英语回答，以增强大语言模型的多语言推理能力。为了进一步促进语言迁移，我们利用高资源 CoT 来通过跨语言蒸馏来监督低资源语言的训练。之前基准测试的实验结果证明了 xCoT 在缩小不同语言之间差距方面的优越性能，凸显了其缩小跨语言差距的潜力。</li>
</ul>

<h3>Title: COIN: Chance-Constrained Imitation Learning for Uncertainty-aware  Adaptive Resource Oversubscription Policy</h3>
<ul>
<li><strong>Authors: </strong>Lu Wang, Mayukh Das, Fangkai Yang, Chao Duo, Bo Qiao, Hang Dong, Si Qin, Chetan Bansal, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07051">https://arxiv.org/abs/2401.07051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07051">https://arxiv.org/pdf/2401.07051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07051]] COIN: Chance-Constrained Imitation Learning for Uncertainty-aware  Adaptive Resource Oversubscription Policy(https://arxiv.org/abs/2401.07051)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>We address the challenge of learning safe and robust decision policies in presence of uncertainty in context of the real scientific problem of adaptive resource oversubscription to enhance resource efficiency while ensuring safety against resource congestion risk. Traditional supervised prediction or forecasting models are ineffective in learning adaptive policies whereas standard online optimization or reinforcement learning is difficult to deploy on real systems. Offline methods such as imitation learning (IL) are ideal since we can directly leverage historical resource usage telemetry. But, the underlying aleatoric uncertainty in such telemetry is a critical bottleneck. We solve this with our proposed novel chance-constrained imitation learning framework, which ensures implicit safety against uncertainty in a principled manner via a combination of stochastic (chance) constraints on resource congestion risk and ensemble value functions. This leads to substantial ($\approx 3-4\times$) improvement in resource efficiency and safety in many oversubscription scenarios, including resource management in cloud services.</li>
<li><strong>摘要：</strong>我们解决了在自适应资源超额认购的真正科学问题的背景下，在存在不确定性的情况下学习安全和稳健的决策政策的挑战，以提高资源效率，同时确保免受资源拥塞风险的安全。传统的监督预测或预测模型在学习自适应策略方面无效，而标准的在线优化或强化学习很难在实际系统上部署。模仿学习 (IL) 等离线方法是理想的选择，因为我们可以直接利用历史资源使用情况遥测。但是，此类遥测中潜在的任意不确定性是一个关键瓶颈。我们通过提出的新颖的机会约束模仿学习框架来解决这个问题，该框架通过结合资源拥塞风险和集合价值函数的随机（机会）约束，以原则性的方式确保针对不确定性的隐式安全性。这使得许多超额订阅场景中的资源效率和安全性得到显着提高（$\大约 3-4\times$），包括云服务中的资源管理。</li>
</ul>

<h3>Title: Aquarium: A Comprehensive Framework for Exploring Predator-Prey Dynamics  through Multi-Agent Reinforcement Learning Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Michael Kölle, Yannick Erpelding, Fabian Ritz, Thomy Phan, Steffen Illium, Claudia Linnhoff-Popien</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07056">https://arxiv.org/abs/2401.07056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07056">https://arxiv.org/pdf/2401.07056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07056]] Aquarium: A Comprehensive Framework for Exploring Predator-Prey Dynamics  through Multi-Agent Reinforcement Learning Algorithms(https://arxiv.org/abs/2401.07056)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, agent</a></li>
<li><strong>Abstract: </strong>Recent advances in Multi-Agent Reinforcement Learning have prompted the modeling of intricate interactions between agents in simulated environments. In particular, the predator-prey dynamics have captured substantial interest and various simulations been tailored to unique requirements. To prevent further time-intensive developments, we introduce Aquarium, a comprehensive Multi-Agent Reinforcement Learning environment for predator-prey interaction, enabling the study of emergent behavior. Aquarium is open source and offers a seamless integration of the PettingZoo framework, allowing a quick start with proven algorithm implementations. It features physics-based agent movement on a two-dimensional, edge-wrapping plane. The agent-environment interaction (observations, actions, rewards) and the environment settings (agent speed, prey reproduction, predator starvation, and others) are fully customizable. Besides a resource-efficient visualization, Aquarium supports to record video files, providing a visual comprehension of agent behavior. To demonstrate the environment's capabilities, we conduct preliminary studies which use PPO to train multiple prey agents to evade a predator. In accordance to the literature, we find Individual Learning to result in worse performance than Parameter Sharing, which significantly improves coordination and sample-efficiency.</li>
<li><strong>摘要：</strong>多智能体强化学习的最新进展促进了模拟环境中智能体之间复杂交互的建模。特别是，捕食者-被捕食者的动态引起了人们的极大兴趣，并且根据独特的要求定制了各种模拟。为了防止进一步的时间密集型开发，我们引入了 Aquarium，这是一个用于捕食者-猎物交互的综合多智能体强化学习环境，可以研究突发行为。 Aquarium 是开源的，提供与 PettingZoo 框架的无缝集成，允许快速启动经过验证的算法实现。它的特点是在二维、边缘包裹的平面上基于物理的代理运动。代理与环境的交互（观察、动作、奖励）和环境设置（代理速度、猎物繁殖、捕食者饥饿等）是完全可定制的。除了资源高效的可视化之外，Aquarium 还支持录制视频文件，提供对代理行为的视觉理解。为了证明环境的能力，我们进行了初步研究，使用 PPO 训练多个猎物代理来躲避捕食者。根据文献，我们发现个体学习的性能比参数共享更差，而参数共享显着提高了协调性和样本效率。</li>
</ul>

<h3>Title: Dirichlet-Based Prediction Calibration for Learning with Noisy Labels</h3>
<ul>
<li><strong>Authors: </strong>Chen-Chen Zong, Ye-Wen Wang, Ming-Kun Xie, Sheng-Jun Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07062">https://arxiv.org/abs/2401.07062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07062">https://arxiv.org/pdf/2401.07062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07062]] Dirichlet-Based Prediction Calibration for Learning with Noisy Labels(https://arxiv.org/abs/2401.07062)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Learning with noisy labels can significantly hinder the generalization performance of deep neural networks (DNNs). Existing approaches address this issue through loss correction or example selection methods. However, these methods often rely on the model's predictions obtained from the softmax function, which can be over-confident and unreliable. In this study, we identify the translation invariance of the softmax function as the underlying cause of this problem and propose the \textit{Dirichlet-based Prediction Calibration} (DPC) method as a solution. Our method introduces a calibrated softmax function that breaks the translation invariance by incorporating a suitable constant in the exponent term, enabling more reliable model predictions. To ensure stable model training, we leverage a Dirichlet distribution to assign probabilities to predicted labels and introduce a novel evidence deep learning (EDL) loss. The proposed loss function encourages positive and sufficiently large logits for the given label, while penalizing negative and small logits for other labels, leading to more distinct logits and facilitating better example selection based on a large-margin criterion. Through extensive experiments on diverse benchmark datasets, we demonstrate that DPC achieves state-of-the-art performance. The code is available at https://github.com/chenchenzong/DPC.</li>
<li><strong>摘要：</strong>使用噪声标签进行学习会严重阻碍深度神经网络 (DNN) 的泛化性能。现有方法通过损失校正或示例选择方法来解决这个问题。然而，这些方法通常依赖于从 softmax 函数获得的模型预测，这可能过于自信且不可靠。在本研究中，我们将 softmax 函数的平移不变性确定为该问题的根本原因，并提出 \textit{基于狄利克雷的预测校准}（DPC）方法作为解决方案。我们的方法引入了一个校准的 softmax 函数，该函数通过在指数项中加入合适的常数来打破平移不变性，从而实现更可靠的模型预测。为了确保稳定的模型训练，我们利用狄利克雷分布为预测标签分配概率，并引入新的证据深度学习（EDL）损失。所提出的损失函数鼓励给定标签的正的且足够大的逻辑，同时惩罚其他标签的负的和小的逻辑，从而导致更明显的逻辑并促进基于大裕度标准的更好的示例选择。通过对不同基准数据集的广泛实验，我们证明 DPC 实现了最先进的性能。代码可在 https://github.com/chenchenzong/DPC 获取。</li>
</ul>

<h3>Title: PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics  Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Settaluri Lakshmi Sravanthi, Meet Doshi, Tankala Pavan Kalyan, Rudra Murthy, Pushpak Bhattacharyya, Raj Dabre</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07078">https://arxiv.org/abs/2401.07078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07078">https://arxiv.org/pdf/2401.07078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07078]] PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics  Capabilities(https://arxiv.org/abs/2401.07078)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat, rag</a></li>
<li><strong>Abstract: </strong>LLMs have demonstrated remarkable capability for understanding semantics, but they often struggle with understanding pragmatics. To demonstrate this fact, we release a Pragmatics Understanding Benchmark (PUB) dataset consisting of fourteen tasks in four pragmatics phenomena, namely, Implicature, Presupposition, Reference, and Deixis. We curated high-quality test sets for each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes a total of 28k data points, 6.1k of which have been created by us, and the rest are adapted from existing datasets. We evaluated nine models varying in the number of parameters and type of training. Our study indicates that fine-tuning for instruction-following and chat significantly enhances the pragmatics capabilities of smaller language models. However, for larger models, the base versions perform comparably with their chat-adapted counterparts. Additionally, there is a noticeable performance gap between human capabilities and model capabilities. Furthermore, unlike the consistent performance of humans across various tasks, the models demonstrate variability in their proficiency, with performance levels fluctuating due to different hints and the complexities of tasks within the same dataset. Overall, the benchmark aims to provide a comprehensive evaluation of LLM's ability to handle real-world language tasks that require pragmatic reasoning.</li>
<li><strong>摘要：</strong>法学硕士在理解语义方面表现出了卓越的能力，但他们常常在理解语用方面遇到困难。为了证明这一事实，我们发布了一个语用理解基准（PUB）数据集，其中包含四种语用现象的十四个任务，即含义、预设、指称和指示语。我们为每项任务策划了高质量的测试集，其中包括多项选择题答案 (MCQA)。 PUB 总共包含 28k 个数据点，其中 6.1k 个是我们创建的，其余的都是根据现有数据集改编的。我们评估了参数数量和训练类型不同的九个模型。我们的研究表明，对指令跟随和聊天的微调可以显着增强较小语言模型的语用能力。然而，对于较大的模型，基本版本的性能与聊天适应版本相当。此外，人类能​​力和模型能力之间存在明显的性能差距。此外，与人类在各种任务中的一致表现不同，这些模型表现出其熟练程度的可变性，由于不同的提示和同一数据集中任务的复杂性，表现水平会发生波动。总体而言，该基准旨在对法学硕士处理需要实用推理的现实世界语言任务的能力进行全面评估。</li>
</ul>

<h3>Title: A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ  Generation using GPT</h3>
<ul>
<li><strong>Authors: </strong>Subhankar Maity, Aniket Deroy, Sudeshna Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07098">https://arxiv.org/abs/2401.07098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07098">https://arxiv.org/pdf/2401.07098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07098]] A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ  Generation using GPT(https://arxiv.org/abs/2401.07098)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>We introduce a multi-stage prompting approach (MSP) for the generation of multiple choice questions (MCQs), harnessing the capabilities of GPT models such as text-davinci-003 and GPT-4, renowned for their excellence across various NLP tasks. Our approach incorporates the innovative concept of chain-of-thought prompting, a progressive technique in which the GPT model is provided with a series of interconnected cues to guide the MCQ generation process. Automated evaluations consistently demonstrate the superiority of our proposed MSP method over the traditional single-stage prompting (SSP) baseline, resulting in the production of high-quality distractors. Furthermore, the one-shot MSP technique enhances automatic evaluation results, contributing to improved distractor generation in multiple languages, including English, German, Bengali, and Hindi. In human evaluations, questions generated using our approach exhibit superior levels of grammaticality, answerability, and difficulty, highlighting its efficacy in various languages.</li>
<li><strong>摘要：</strong>我们引入了多阶段提示方法 (MSP) 来生成多项选择题 (MCQ)，利用 GPT 模型（例如 text-davinci-003 和 GPT-4）的功能，这些模型因其在各种 NLP 任务中的卓越表现而闻名。我们的方法融合了思想链提示的创新概念，这是一种渐进技术，其中 GPT 模型提供了一系列相互关联的线索来指导 MCQ 生成过程。自动评估一致证明了我们提出的 MSP 方法相对于传统单阶段提示 (SSP) 基线的优越性，从而产生了高质量的干扰项。此外，一次性 MSP 技术增强了自动评估结果，有助于改进多种语言的干扰项生成，包括英语、德语、孟加拉语和印地语。在人类评估中，使用我们的方法生成的问题表现出卓越的语法性、可回答性和难度，凸显了其在各种语言中的功效。</li>
</ul>

<h3>Title: Leveraging Large Language Models for NLG Evaluation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07103">https://arxiv.org/abs/2401.07103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07103">https://arxiv.org/pdf/2401.07103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07103]] Leveraging Large Language Models for NLG Evaluation: A Survey(https://arxiv.org/abs/2401.07103)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, rag</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This survey aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.</li>
<li><strong>摘要：</strong>在快速发展的自然语言生成（NLG）评估领域，引入大型语言模型（LLM）为评估生成的内容质量（例如连贯性、创造力和上下文相关性）开辟了新途径。本调查旨在全面概述如何利用法学硕士进行 NLG 评估，这是一个缺乏系统分析的新兴领域。我们提出了一个连贯的分类法来组织现有的基于法学硕士的评估指标，提供一个结构化的框架来理解和比较这些方法。我们的详细探索包括批判性地评估各种基于法学硕士的方法，以及比较它们在评估 NLG 输出方面的优势和局限性。通过讨论尚未解决的挑战，包括偏见、稳健性、领域特异性和统一评估，本次调查旨在为研究人员提供见解，并倡导更公平、更先进的 NLG 评估技术。</li>
</ul>

<h3>Title: Graph Language Models</h3>
<ul>
<li><strong>Authors: </strong>Moritz Plenz, Anette Frank</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07105">https://arxiv.org/abs/2401.07105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07105">https://arxiv.org/pdf/2401.07105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07105]] Graph Language Models(https://arxiv.org/abs/2401.07105)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>While Language Models have become workhorses for NLP, their interplay with textual knowledge graphs (KGs) - structured memories of general or domain knowledge - is actively researched. Current embedding methodologies for such graphs typically either (i) linearize graphs for embedding them using sequential Language Models (LMs), which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve graph structure, while GNNs cannot represent textual features as well as a pre-trained LM could. In this work we introduce a novel language model, the Graph Language Model (GLM), that integrates the strengths of both approaches, while mitigating their weaknesses. The GLM parameters are initialized from a pretrained LM, to facilitate nuanced understanding of individual concepts and triplets. Simultaneously, its architectural design incorporates graph biases, thereby promoting effective knowledge distribution within the graph. Empirical evaluations on relation classification tasks on ConceptNet subgraphs reveal that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot settings.</li>
<li><strong>摘要：</strong>虽然语言模型已成为 NLP 的主力，但它们与文本知识图 (KG)（一般知识或领域知识的结构化记忆）的相互作用正在被积极研究。目前此类图的嵌入方法通常要么（i）使用顺序语言模型（LM）对图进行线性化以嵌入它们，但该模型未充分利用结构信息，要么（ii）使用图神经网络（GNN）来保留图结构，而 GNN 无法表示文本功能和预训练的 LM 一样。在这项工作中，我们引入了一种新颖的语言模型，即图语言模型（GLM），它集成了两种方法的优点，同时减轻了它们的缺点。 GLM 参数是从预训练的 LM 初始化的，以促进对各个概念和三元组的细致理解。同时，其架构设计融入了图偏差，从而促进了图内知识的有效分配。对 ConceptNet 子图上的关系分类任务的实证评估表明，GLM 嵌入在监督和零样本设置中超越了基于 LM 和 GNN 的基线。</li>
</ul>

<h3>Title: Open Models, Closed Minds? On Agents Capabilities in Mimicking Human  Personalities through Open Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lucio La Cava, Davide Costa, Andrea Tagarelli</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.CY, cs.HC, physics.soc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07115">https://arxiv.org/abs/2401.07115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07115">https://arxiv.org/pdf/2401.07115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07115]] Open Models, Closed Minds? On Agents Capabilities in Mimicking Human  Personalities through Open Large Language Models(https://arxiv.org/abs/2401.07115)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology, leading to a proliferation of computational agents. Scholars have been studying the inherent personalities displayed by LLM agents and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by conducting a comprehensive examination of the ability of agents to emulate human personalities using Open LLMs. To achieve this, we generate a set of ten LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that: $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities; and $(iv)$ personalities typically associated with the role of teacher tend to be emulated with greater accuracy. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）中类人行为的出现使得 NLP 和人类心理学之间的联系更加紧密，从而导致计算代理的激增。学者们一直在研究法学硕士代理人所表现出的内在个性，并试图将人类特征和行为融入其中。然而，这些努力主要集中在商业许可的法学硕士上，忽视了开放式法学硕士的广泛使用和显着进步。这项工作旨在通过使用开放法学硕士对代理模仿人类性格的能力进行全面检查来解决这一差距。为了实现这一目标，我们根据最具代表性的开放模型生成了一组十个法学硕士代理，并对他们进行了一系列有关迈尔斯-布里格斯类型指标（MBTI）测试的评估。我们的方法包括评估 Open LLM 代理人的内在人格特征，并确定这些代理人在受特定个性和角色限制时模仿人类性格的程度。我们的研究结果表明： $(i)$ 每个 Open LLM 代理人都展现出独特的个性； $(ii)$ 个性条件提示对代理人产生不同的影响，只有少数成功地反映了强加的个性，而大多数人都是“思想封闭”（即，他们保留了自己的内在特征）； $(iii)$将角色和人格调节相结合可以增强智能体模仿人类人格的能力；通常与教师角色相关的 $(iv)$ 个性往往能够更准确地被模仿。我们的工作代表着通过开放法学硕士的视角理解 NLP 和人类心理学之间的紧密关系又向前迈进了一步。</li>
</ul>

<h3>Title: EHRAgent: Code Empowers Large Language Models for Complex Tabular  Reasoning on Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, May D. Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07128">https://arxiv.org/abs/2401.07128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07128">https://arxiv.org/pdf/2401.07128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07128]] EHRAgent: Code Empowers Large Language Models for Complex Tabular  Reasoning on Electronic Health Records(https://arxiv.org/abs/2401.07128)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent1, an LLM agent empowered with a code interface, to autonomously generate and execute code for complex clinical tasks within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on two real-world EHR datasets show that EHRAgent outperforms the strongest LLM agent baseline by 36.48% and 12.41%, respectively. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在作为自主代理的规划和工具利用方面表现出了卓越的能力，但很少被开发用于解决医疗问题。我们提出 EHRAgent1，这是一种具有代码接口的法学硕士代理，可以为电子健康记录 (EHR) 中的复杂临床任务自动生成和执行代码。首先，我们将 EHR 问答任务制定为工具使用规划流程，有效地将复杂的任务分解为一系列可管理的操作。通过集成交互式编码和执行反馈，EHRAgent 从错误消息中学习并通过迭代改进最初生成的代码。此外，我们通过结合长期记忆来增强 LLM 代理，这使得 EHRAgent 能够有效地从过去的经验中选择和建立最相关的成功案例。对两个真实 EHR 数据集的实验表明，EHRAgent 的性能分别优于最强的 LLM 代理基线 36.48% 和 12.41%。 EHRAgent 利用法学硕士新兴的几次学习功能，实现自主代码生成和执行，以最少的演示来处理复杂的临床任务。</li>
</ul>

<h3>Title: Scalable and Efficient Methods for Uncertainty Estimation and Reduction  in Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Soyed Tuhin Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07145">https://arxiv.org/abs/2401.07145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07145">https://arxiv.org/pdf/2401.07145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07145]] Scalable and Efficient Methods for Uncertainty Estimation and Reduction  in Deep Learning(https://arxiv.org/abs/2401.07145)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Neural networks (NNs) can achieved high performance in various fields such as computer vision, and natural language processing. However, deploying NNs in resource-constrained safety-critical systems has challenges due to uncertainty in the prediction caused by out-of-distribution data, and hardware non-idealities. To address the challenges of deploying NNs in resource-constrained safety-critical systems, this paper summarizes the (4th year) PhD thesis work that explores scalable and efficient methods for uncertainty estimation and reduction in deep learning, with a focus on Computation-in-Memory (CIM) using emerging resistive non-volatile memories. We tackle the inherent uncertainties arising from out-of-distribution inputs and hardware non-idealities, crucial in maintaining functional safety in automated decision-making systems. Our approach encompasses problem-aware training algorithms, novel NN topologies, and hardware co-design solutions, including dropout-based \emph{binary} Bayesian Neural Networks leveraging spintronic devices and variational inference techniques. These innovations significantly enhance OOD data detection, inference accuracy, and energy efficiency, thereby contributing to the reliability and robustness of NN implementations.</li>
<li><strong>摘要：</strong>神经网络（NN）可以在计算机视觉和自然语言处理等各个领域实现高性能。然而，由于分布外数据和硬件非理想性导致的预测不确定性，在资源受限的安全关键系统中部署神经网络面临着挑战。为了解决在资源受限的安全关键系统中部署神经网络的挑战，本文总结了（第四年）博士论文工作，该论文探索了深度学习中不确定性估计和减少的可扩展且有效的方法，重点是计算中的计算存储器（CIM）采用新兴的电阻式非易失性存储器。我们解决由分布外输入和硬件非理想性引起的固有不确定性，这对于维护自动化决策系统的功能安全至关重要。我们的方法包括问题感知训练算法、新颖的神经网络拓扑和硬件协同设计解决方案，包括利用自旋电子设备和变分推理技术的基于 dropout 的 \emph{binary} 贝叶斯神经网络。这些创新显着增强了 OOD 数据检测、推理准确性和能源效率，从而有助于神经网络实现的可靠性和鲁棒性。</li>
</ul>

<h3>Title: Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhengxin Zhang, Dan Zhao, Xupeng Miao, Gabriele Oliaro, Qing Li, Yong Jiang, Zhihao Jia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07159">https://arxiv.org/abs/2401.07159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07159">https://arxiv.org/pdf/2401.07159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07159]] Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized  Large Language Models(https://arxiv.org/abs/2401.07159)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory and none can simultaneously mitigate memory footprint for all three sources. In this paper, we present Quantized Side Tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM's model weights into 4-bit to reduce the memory footprint of the LLM's original weights; QST also introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing backpropagation through the LLM, thus reducing the memory requirement of the intermediate activations. Furthermore, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3 $\times$ and speed up the finetuning process by up to 3 $\times$ while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7 $\times$.</li>
<li><strong>摘要：</strong>从经验来看，微调大型语言模型 (LLM) 对于各种下游任务都是有效的。现有的 LLM 微调方法要么专注于参数高效的微调，仅更新少量可训练参数，要么尝试减少微调训练阶段的内存占用。通常，微调期间的内存占用源于三个因素：模型权重、优化器状态和中间激活。然而，现有的工作仍然需要大量的内存，并且没有一个可以同时减少所有三个来源的内存占用。在本文中，我们提出了量化侧调整（QST），它通过双阶段过程操作来实现 LLM 的内存高效和快速微调。首先，QST将LLM的模型权重量化为4位，以减少LLM原始权重的内存占用； QST 还引入了一个与 LLM 分离的侧网络，它利用 LLM 的隐藏状态来进行特定于任务的预测。使用单独的侧网络可以避免通过 LLM 执行反向传播，从而减少中间激活的内存需求。此外，QST 利用多个低秩适配器和无梯度下采样模块来显着减少可训练参数，从而节省优化器状态的内存占用。实验表明，与最先进的技术相比，QST 可以将总内存占用减少高达 2.3 $\times$，并将微调过程加速高达 3 $\times$，同时实现足够的性能。当进行全面微调时，QST 可以将总内存占用量减少多达 7 $\times$。</li>
</ul>

<h3>Title: Domain Adaptation for Sustainable Soil Management using Causal and  Contrastive Constraint Minimization</h3>
<ul>
<li><strong>Authors: </strong>Somya Sharma, Swati Sharma, Rafael Padilha, Emre Kiciman, Ranveer Chandra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07175">https://arxiv.org/abs/2401.07175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07175">https://arxiv.org/pdf/2401.07175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07175]] Domain Adaptation for Sustainable Soil Management using Causal and  Contrastive Constraint Minimization(https://arxiv.org/abs/2401.07175)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Monitoring organic matter is pivotal for maintaining soil health and can help inform sustainable soil management practices. While sensor-based soil information offers higher-fidelity and reliable insights into organic matter changes, sampling and measuring sensor data is cost-prohibitive. We propose a multi-modal, scalable framework that can estimate organic matter from remote sensing data, a more readily available data source while leveraging sparse soil information for improving generalization. Using the sensor data, we preserve underlying causal relations among sensor attributes and organic matter. Simultaneously we leverage inherent structure in the data and train the model to discriminate among domains using contrastive learning. This causal and contrastive constraint minimization ensures improved generalization and adaptation to other domains. We also shed light on the interpretability of the framework by identifying attributes that are important for improving generalization. Identifying these key soil attributes that affect organic matter will aid in efforts to standardize data collection efforts.</li>
<li><strong>摘要：</strong>监测有机物对于维持土壤健康至关重要，有助于为可持续土壤管理实践提供信息。虽然基于传感器的土壤信息可以提供有关有机物变化的更高保真度和可靠的见解，但采样和测量传感器数据的成本却过高。我们提出了一种多模式、可扩展的框架，可以从遥感数据（一种更容易获得的数据源）中估计有机物，同时利用稀疏的土壤信息来提高泛化能力。使用传感器数据，我们保留传感器属性和有机物之间的潜在因果关系。同时，我们利用数据的固有结构并训练模型以使用对比学习来区分领域。这种因果和对比约束最小化确保了改进的泛化和对其他领域的适应。我们还通过识别对于提高泛化能力很重要的属性来阐明框架的可解释性。识别影响有机质的这些关键土壤属性将有助于标准化数据收集工作。</li>
</ul>

<h3>Title: Reinforcement Learning from LLM Feedback to Counteract Goal  Misgeneralization</h3>
<ul>
<li><strong>Authors: </strong>Houda Nait El Barj, Theophile Sautory</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07181">https://arxiv.org/abs/2401.07181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07181">https://arxiv.org/pdf/2401.07181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07181]] Reinforcement Learning from LLM Feedback to Counteract Goal  Misgeneralization(https://arxiv.org/abs/2401.07181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>We introduce a method to address goal misgeneralization in reinforcement learning (RL), leveraging Large Language Model (LLM) feedback during training. Goal misgeneralization, a type of robustness failure in RL occurs when an agent retains its capabilities out-of-distribution yet pursues a proxy rather than the intended one. Our approach utilizes LLMs to analyze an RL agent's policies during training and identify potential failure scenarios. The RL agent is then deployed in these scenarios, and a reward model is learnt through the LLM preferences and feedback. This LLM-informed reward model is used to further train the RL agent on the original dataset. We apply our method to a maze navigation task, and show marked improvements in goal generalization, especially in cases where true and proxy goals are somewhat distinguishable and behavioral biases are pronounced. This study demonstrates how the LLM, despite its lack of task proficiency, can efficiently supervise RL agents, providing scalable oversight and valuable insights for enhancing goal-directed learning in RL through the use of LLMs.</li>
<li><strong>摘要：</strong>我们引入了一种方法来解决强化学习 (RL) 中的目标误泛问题，在训练期间利用大型语言模型 (LLM) 反馈。目标误泛化是强化学习中的一种鲁棒性失败，当代理保留其分布外的能力但追求代理而不是预期的代理时，就会发生这种情况。我们的方法利用 LLM 来分析 RL 代理在训练期间的策略并识别潜在的故障场景。然后在这些场景中部署 RL 代理，并通过 LLM 偏好和反馈学习奖励模型。这个基于 LLM 的奖励模型用于在原始数据集上进一步训练 RL 代理。我们将我们的方法应用于迷宫导航任务，并显示出目标泛化的显着改进，特别是在真实目标和代理目标在某种程度上可区分且行为偏差明显的情况下。这项研究展示了法学硕士如何在缺乏任务熟练度的情况下有效地监督强化学习代理，提供可扩展的监督和有价值的见解，以通过使用法学硕士来增强强化学习中的目标导向学习。</li>
</ul>

<h3>Title: Inroads to a Structured Data Natural Language Bijection and the role of  LLM annotation</h3>
<ul>
<li><strong>Authors: </strong>Blake Vente</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07190">https://arxiv.org/abs/2401.07190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07190">https://arxiv.org/pdf/2401.07190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07190]] Inroads to a Structured Data Natural Language Bijection and the role of  LLM annotation(https://arxiv.org/abs/2401.07190)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This work finds limited evidence supporting the theory that using multiple tasks with sequence-to-sequence transformer language models can improve performance on some metrics. In particular, the multi-task generalist t5-small outperforms the specialist t5-small with a $F_1$ of $0.771$ up from $0.692$, which may point to underlying cross-task knowledge generalization. This further suggests that even with the same network, "re-using" the same data in a different way may lead to higher performance in some metrics. However, the inverse task alone is likely only an optimization strategy, since it does not yield a significant general improvement at the model sizes explored in this work. Also, adding $\approx 4500$ LLM annotated records (interlaced with the $12800$ WebNLG training records) does not substantially change automatic metric performance compared to the same t5-small model without the synthetic data. This may be due to a learning capacity bottleneck on account of model size, and decreases observed may be due to distributional differences in the corpora. Future research using larger models or human evaluation is required to more fully explain the mechanisms contributing to performance on these tasks.</li>
<li><strong>摘要：</strong>这项工作发现有限的证据支持这一理论，即使用序列到序列转换器语言模型的多个任务可以提高某些指标的性能。特别是，多任务通才 t5-small 的表现优于专家 t5-small，$F_1$ 从 $0.692$ 上升到 $0.771$，这可能表明潜在的跨任务知识泛化。这进一步表明，即使使用相同的网络，以不同的方式“重复使用”相同的数据也可能会在某些指标上带来更高的性能。然而，单独的逆任务可能只是一种优化策略，因为它不会对本工作中探索的模型大小产生显着的总体改进。此外，与没有合成数据的相同 t5-small 模型相比，添加 $\approx 4500$ LLM 注释记录（与 $12800$ WebNLG 训练记录交错）并不会显着改变自动指标性能。这可能是由于模型大小造成的学习能力瓶颈，并且观察到的下降可能是由于语料库的分布差异造成的。未来的研究需要使用更大的模型或人类评估来更全面地解释有助于这些任务表现的机制。</li>
</ul>

<h3>Title: Use of Prior Knowledge to Discover Causal Additive Models with  Unobserved Variables and its Application to Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Takashi Nicholas Maeda, Shimizu Shohei</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07231">https://arxiv.org/abs/2401.07231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07231">https://arxiv.org/pdf/2401.07231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07231]] Use of Prior Knowledge to Discover Causal Additive Models with  Unobserved Variables and its Application to Time Series Data(https://arxiv.org/abs/2401.07231)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>This paper proposes two methods for causal additive models with unobserved variables (CAM-UV). CAM-UV assumes that the causal functions take the form of generalized additive models and that latent confounders are present. First, we propose a method that leverages prior knowledge for efficient causal discovery. Then, we propose an extension of this method for inferring causality in time series data. The original CAM-UV algorithm differs from other existing causal function models in that it does not seek the causal order between observed variables, but rather aims to identify the causes for each observed variable. Therefore, the first proposed method in this paper utilizes prior knowledge, such as understanding that certain variables cannot be causes of specific others. Moreover, by incorporating the prior knowledge that causes precedes their effects in time, we extend the first algorithm to the second method for causal discovery in time series data. We validate the first proposed method by using simulated data to demonstrate that the accuracy of causal discovery increases as more prior knowledge is accumulated. Additionally, we test the second proposed method by comparing it with existing time series causal discovery methods, using both simulated data and real-world data.</li>
<li><strong>摘要：</strong>本文提出了两种具有不可观测变量的因果加性模型（CAM-UV）的方法。 CAM-UV 假设因果函数采用广义加性模型的形式，并且存在潜在的混杂因素。首先，我们提出了一种利用先验知识进行有效因果发现的方法。然后，我们提出了该方法的扩展，用于推断时间序列数据中的因果关系。原始的 CAM-UV 算法与其他现有因果函数模型的不同之处在于，它并不寻求观测变量之间的因果顺序，而是旨在识别每个观测变量的原因。因此，本文第一个提出的方法利用了先验知识，例如理解某些变量不能成为特定其他变量的原因。此外，通过结合时间上原因先于其影响的先验知识，我们将第一种算法扩展到第二种方法，用于时间序列数据中的因果发现。我们通过使用模拟数据来验证第一个提出的方法，以证明因果发现的准确性随着更多先验知识的积累而增加。此外，我们使用模拟数据和真实世界数据，通过将其与现有时间序列因果发现方法进行比较来测试第二种提出的方​​法。</li>
</ul>

<h3>Title: The Effects of Data Imbalance Under a Federated Learning Approach for  Credit Risk Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Shuyao Zhang, Jordan Tay, Pedro Baiz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07234">https://arxiv.org/abs/2401.07234</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07234">https://arxiv.org/pdf/2401.07234</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07234]] The Effects of Data Imbalance Under a Federated Learning Approach for  Credit Risk Forecasting(https://arxiv.org/abs/2401.07234)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Credit risk forecasting plays a crucial role for commercial banks and other financial institutions in granting loans to customers and minimise the potential loss. However, traditional machine learning methods require the sharing of sensitive client information with an external server to build a global model, potentially posing a risk of security threats and privacy leakage. A newly developed privacy-preserving distributed machine learning technique known as Federated Learning (FL) allows the training of a global model without the necessity of accessing private local data directly. This investigation examined the feasibility of federated learning in credit risk assessment and showed the effects of data imbalance on model performance. Two neural network architectures, Multilayer Perceptron (MLP) and Long Short-Term Memory (LSTM), and one tree ensemble architecture, Extreme Gradient Boosting (XGBoost), were explored across three different datasets under various scenarios involving different numbers of clients and data distribution configurations. We demonstrate that federated models consistently outperform local models on non-dominant clients with smaller datasets. This trend is especially pronounced in highly imbalanced data scenarios, yielding a remarkable average improvement of 17.92% in model performance. However, for dominant clients (clients with more data), federated models may not exhibit superior performance, suggesting the need for special incentives for this type of clients to encourage their participation.</li>
<li><strong>摘要：</strong>信用风险预测对于商业银行和其他金融机构向客户发放贷款并最大限度地减少潜在损失起着至关重要的作用。然而，传统的机器学习方法需要与外部服务器共享敏感的客户端信息来构建全局模型，可能会带来安全威胁和隐私泄露的风险。新开发的隐私保护分布式机器学习技术称为联邦学习（FL），允许训练全局模型，而无需直接访问私有本地数据。这项研究检验了联邦学习在信用风险评估中的可行性，并显示了数据不平衡对模型性能的影响。在涉及不同数量的客户端和数据分布的各种场景下，在三个不同的数据集上探索了两种神经网络架构，多层感知器（MLP）和长短期记忆（LSTM），以及一种树集成架构，极端梯度提升（XGBoost）配置。我们证明，在具有较小数据集的非主导客户端上，联合模型始终优于本地模型。这种趋势在高度不平衡的数据场景中尤其明显，模型性能平均提高了 17.92%。然而，对于主导客户（拥有更多数据的客户），联合模型可能不会表现出优越的性能，这表明需要对此类客户采取特殊激励措施以鼓励他们的参与。</li>
</ul>

<h3>Title: Distilling Event Sequence Knowledge From Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Somin Wadhwa, Oktie Hassanzadeh, Debarun Bhattacharjya, Ken Barker, Jian Ni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07237">https://arxiv.org/abs/2401.07237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07237">https://arxiv.org/pdf/2401.07237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07237]] Distilling Event Sequence Knowledge From Large Language Models(https://arxiv.org/abs/2401.07237)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag</a></li>
<li><strong>Abstract: </strong>Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex structured knowledge from pattern mining and probabilistic event models. We release our sequence generation code and evaluation framework, as well as corpus of event sequence data.</li>
<li><strong>摘要：</strong>人们发现事件序列模型在事件分析和预测方面非常有效。构建此类模型需要丰富的高质量事件序列数据。然而，在某些应用中，干净的结构化事件序列不可用，并且自动序列提取会导致数据噪声太大且不完整。在这项工作中，我们探索使用大型语言模型（LLM）来生成可有效用于概率事件模型构建的事件序列。这可以被视为从法学硕士中提取事件序列知识的机制。我们的方法依赖于具有部分因果关系的事件概念的知识图（KG）来指导因果事件序列生成的生成语言模型。我们证明我们的方法可以生成高质量的事件序列，填补输入知识图谱中的知识空白。此外，我们探索如何利用生成的序列从模式挖掘和概率事件模型中发现有用且更复杂的结构化知识。我们发布了序列生成代码和评估框架，以及事件序列数据语料库。</li>
</ul>

<h3>Title: Imputation with Inter-Series Information from Prototypes for Irregular  Sampled Time Series</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Yu, Xu Chu, Liantao Ma, Yasha Wang, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07249">https://arxiv.org/abs/2401.07249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07249">https://arxiv.org/pdf/2401.07249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07249]] Imputation with Inter-Series Information from Prototypes for Irregular  Sampled Time Series(https://arxiv.org/abs/2401.07249)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Irregularly sampled time series are ubiquitous, presenting significant challenges for analysis due to missing values. Despite existing methods address imputation, they predominantly focus on leveraging intra-series information, neglecting the potential benefits that inter-series information could provide, such as reducing uncertainty and memorization effect. To bridge this gap, we propose PRIME, a Prototype Recurrent Imputation ModEl, which integrates both intra-series and inter-series information for imputing missing values in irregularly sampled time series. Our framework comprises a prototype memory module for learning inter-series information, a bidirectional gated recurrent unit utilizing prototype information for imputation, and an attentive prototypical refinement module for adjusting imputations. We conducted extensive experiments on three datasets, and the results underscore PRIME's superiority over the state-of-the-art models by up to 26% relative improvement on mean square error.</li>
<li><strong>摘要：</strong>不规则采样的时间序列无处不在，由于缺失值而给分析带来了巨大的挑战。尽管现有方法解决了插补问题，但它们主要侧重于利用系列内信息，忽略了系列间信息可以提供的潜在好处，例如减少不确定性和记忆效应。为了弥补这一差距，我们提出了 PRIME，一种原型循环插补模型，它集成了系列内和系列间信息，用于插补不规则采样时间序列中的缺失值。我们的框架包括用于学习系列间信息的原型存储模块、利用原型信息进行插补的双向门控循环单元以及用于调整插补的细心原型细化模块。我们对三个数据集进行了广泛的实验，结果强调了 PRIME 相对于最先进模型的优越性，均方误差相对改善高达 26%。</li>
</ul>

<h3>Title: BET: Explaining Deep Reinforcement Learning through The Error-Prone  Decisions</h3>
<ul>
<li><strong>Authors: </strong>Xiao Liu, Jie Zhao, Wubing Chen, Mao Tan, Yongxing Su</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07263">https://arxiv.org/abs/2401.07263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07263">https://arxiv.org/pdf/2401.07263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07263]] BET: Explaining Deep Reinforcement Learning through The Error-Prone  Decisions(https://arxiv.org/abs/2401.07263)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Despite the impressive capabilities of Deep Reinforcement Learning (DRL) agents in many challenging scenarios, their black-box decision-making process significantly limits their deployment in safety-sensitive domains. Several previous self-interpretable works focus on revealing the critical states of the agent's decision. However, they cannot pinpoint the error-prone states. To address this issue, we propose a novel self-interpretable structure, named Backbone Extract Tree (BET), to better explain the agent's behavior by identify the error-prone states. At a high level, BET hypothesizes that states in which the agent consistently executes uniform decisions exhibit a reduced propensity for errors. To effectively model this phenomenon, BET expresses these states within neighborhoods, each defined by a curated set of representative states. Therefore, states positioned at a greater distance from these representative benchmarks are more prone to error. We evaluate BET in various popular RL environments and show its superiority over existing self-interpretable models in terms of explanation fidelity. Furthermore, we demonstrate a use case for providing explanations for the agents in StarCraft II, a sophisticated multi-agent cooperative game. To the best of our knowledge, we are the first to explain such a complex scenarios using a fully transparent structure.</li>
<li><strong>摘要：</strong>尽管深度强化学习（DRL）代理在许多具有挑战性的场景中具有令人印象深刻的能力，但它们的黑盒决策过程极大地限制了它们在安全敏感领域的部署。之前的一些可自我解释的作品侧重于揭示智能体决策的关键状态。然而，他们无法查明容易出错的状态。为了解决这个问题，我们提出了一种新颖的自解释结构，称为主干提取树（BET），通过识别容易出错的状态来更好地解释代理的行为。在较高层面上，BET 假设代理一致执行统一决策的状态会降低出错的可能性。为了有效地模拟这种现象，BET 在邻域内表达这些状态，每个状态都由一组精选的代表性状态定义。因此，距离这些代表性基准较远的州更容易出错。我们在各种流行的 RL 环境中评估 BET，并展示其在解释保真度方面优于现有的自我解释模型。此外，我们还演示了一个用例，用于为星际争霸 II（一款复杂的多智能体合作游戏）中的智能体提供解释。据我们所知，我们是第一个使用完全透明的结构来解释如此复杂的场景的人。</li>
</ul>

<h3>Title: Improving Domain Adaptation through Extended-Text Reading Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07284">https://arxiv.org/abs/2401.07284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07284">https://arxiv.org/pdf/2401.07284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07284]] Improving Domain Adaptation through Extended-Text Reading Comprehension(https://arxiv.org/abs/2401.07284)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, rag</a></li>
<li><strong>Abstract: </strong>To enhance the domain-specific capabilities of large language models, continued pre-training on a domain-specific corpus is a prevalent method. Recent work demonstrates that adapting models using reading comprehension data formatted by regex-based patterns can significantly improve performance on domain-specific tasks. However, regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context. To address this limitation, we improve reading comprehension via LLM and clustering. LLM focuses on leveraging domain knowledge within the corpus to refine comprehension stage, while clustering supplies relevant knowledge by extending the context to enrich reading stage. Additionally, our method incorporates parameter-efficient fine-tuning to improve the efficiency of domain adaptation. In comparison to AdaptLLM, our method achieves an improvement exceeding 5% in domain-specific tasks. Our code will available at https://github.com/microsoft/LMOps.</li>
<li><strong>摘要：</strong>为了增强大型语言模型的特定领域能力，对特定领域语料库进行持续预训练是一种普遍的方法。最近的工作表明，使用基于正则表达式的模式格式化的阅读理解数据来调整模型可以显着提高特定领域任务的性能。然而，基于正则表达式的模式无法使用特定领域的知识来解析原始语料库。此外，问题和答案对以预定义格式直接从语料库中提取，提供了有限的上下文。为了解决这个限制，我们通过法学硕士和聚类来提高阅读理解能力。 LLM侧重于利用语料库内的领域知识来完善理解阶段，而聚类则通过扩展上下文来丰富阅读阶段来提供相关知识。此外，我们的方法结合了参数有效的微调来提高域适应的效率。与 AdaptLLM 相比，我们的方法在特定领域的任务中实现了超过 5% 的改进。我们的代码将在 https://github.com/microsoft/LMOps 上提供。</li>
</ul>

<h3>Title: CANDLE: Iterative Conceptualization and Instantiation Distillation from  Large Language Models for Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Weiqi Wang, Tianqing Fang, Chunyang Li, Haochen Shi, Wenxuan Ding, Baixuan Xu, Zhaowei Wang, Jiaxin Bai, Xin Liu, Jiayang Cheng, Chunkit Chan, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07286">https://arxiv.org/abs/2401.07286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07286">https://arxiv.org/pdf/2401.07286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07286]] CANDLE: Iterative Conceptualization and Instantiation Distillation from  Large Language Models for Commonsense Reasoning(https://arxiv.org/abs/2401.07286)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE, a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across four downstream tasks. Our code, data, and models are publicly available at https://github.com/HKUST-KnowComp/CANDLE.</li>
<li><strong>摘要：</strong>概念化和实例化的顺序过程对于可概括的常识推理至关重要，因为它允许将现有知识应用于不熟悉的场景。然而，现有的工作往往低估了实例化的步骤，严重依赖预先构建的概念分类和人工注释来收集这两类知识，导致缺乏实例化的知识来完成推理、成本高昂且可扩展性有限。为了应对这些挑战，我们引入了 CANDLE，这是一个蒸馏框架，通过指示大型语言模型通过批评者过滤生成两种类型的知识，迭代地对常识知识库执行上下文化概念化和实例化。通过将 CANDLE 应用于 ATOMIC，我们构建了一个包含 600 万个概念化和实例化常识知识三元组的综合知识库。两种类型的知识都牢固地植根于原始的 ATOMIC 数据集，内在评估证明了它们卓越的质量和多样性。实证结果表明，在学生模型上提取 CANDLE 可以为四个下游任务带来好处。我们的代码、数据和模型可在 https://github.com/HKUST-KnowComp/CANDLE 上公开获取。</li>
</ul>

<h3>Title: Small Language Model Can Self-correct</h3>
<ul>
<li><strong>Authors: </strong>Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, Yanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07301">https://arxiv.org/abs/2401.07301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07301">https://arxiv.org/pdf/2401.07301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07301]] Small Language Model Can Self-correct(https://arxiv.org/abs/2401.07301)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Generative Language Models (LMs) such as ChatGPT have exhibited remarkable performance across various downstream tasks. Nevertheless, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. Previous studies have devised sophisticated pipelines and prompts to induce large LMs to exhibit the capability for self-correction. However, large LMs are explicitly prompted to verify and modify its answers separately rather than completing all steps spontaneously like humans. Moreover, these complex prompts are extremely challenging for small LMs to follow. In this paper, we introduce the \underline{I}ntrinsic \underline{S}elf-\underline{C}orrection (ISC) in generative language models, aiming to correct the initial output of LMs in a self-triggered manner, even for those small LMs with 6 billion parameters. Specifically, we devise a pipeline for constructing self-correction data and propose Partial Answer Masking (PAM), aiming to endow the model with the capability for intrinsic self-correction through fine-tuning. We conduct experiments using LMs with parameters sizes ranging from 6 billion to 13 billion in two tasks, including commonsense reasoning and factual knowledge reasoning. Our experiments demonstrate that the outputs generated using ISC outperform those generated without self-correction. We believe that the output quality of even small LMs can be further improved by empowering them with the ability to intrinsic self-correct.</li>
<li><strong>摘要：</strong>ChatGPT 等生成语言模型 (LM) 在各种下游任务中表现出了卓越的性能。然而，它们最突出的缺点之一是以自信的语气生成不准确或虚假的信息。之前的研究已经设计了复杂的管道和提示来诱导大型语言模型展现出自我校正的能力。然而，大型 LM 会被明确提示单独验证和修改其答案，而不是像人类一样自发完成所有步骤。此外，这些复杂的提示对于小型 LM 来说极具挑战性。在本文中，我们在生成语言模型中引入了 \underline{I}ntrinsic \underline{S}elf-\underline{C}orrection (ISC)，旨在以自触发的方式纠正 LM 的初始输出，即使对于那些拥有 60 亿个参数的小型 LM。具体来说，我们设计了一个构建自校正数据的管道，并提出了部分答案屏蔽（PAM），旨在通过微调赋予模型内在的自校正能力。我们使用参数大小从 60 亿到 130 亿不等的 LM 在常识推理和事实知识推理两个任务中进行实验。我们的实验表明，使用 ISC 生成的输出优于没有自我校正生成的输出。我们相信，即使是小型语言模型，也可以通过赋予它们内在的自我纠正能力来进一步提高其输出质量。</li>
</ul>

<h3>Title: Harnessing Large Language Models Over Transformer Models for Detecting  Bengali Depressive Social Media Text: A Comprehensive Study</h3>
<ul>
<li><strong>Authors: </strong>Ahmadul Karim Chowdhury, Md. Saidur Rahman Sujon, Md. Shirajus Salekin Shafi, Tasin Ahmmad, Sifat Ahmed, Khan Md Hasib, Faisal Muhammad Shah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07310">https://arxiv.org/abs/2401.07310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07310">https://arxiv.org/pdf/2401.07310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07310]] Harnessing Large Language Models Over Transformer Models for Detecting  Bengali Depressive Social Media Text: A Comprehensive Study(https://arxiv.org/abs/2401.07310)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, lora</a></li>
<li><strong>Abstract: </strong>In an era where the silent struggle of underdiagnosed depression pervades globally, our research delves into the crucial link between mental health and social media. This work focuses on early detection of depression, particularly in extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our proposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning models(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT, SahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into "Depressive" and "Non-Depressive" segments, translated into Bengali by native speakers with expertise in mental health, resulting in the creation of the Bengali Social Media Depressive Dataset (BSMDD). Our work provides full architecture details for each model and a methodical way to assess their performance in Bengali depressive text categorization using zero-shot and few-shot learning techniques. Our work demonstrates the superiority of SahajBERT and Bi-LSTM with FastText embeddings in their respective domains also tackles explainability issues with transformer models and emphasizes the effectiveness of LLMs, especially DepGPT, demonstrating flexibility and competence in a range of learning contexts. According to the experiment results, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in zero-shot and few-shot scenarios but also every other model, achieving a near-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and exceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B show relatively poorer effectiveness in zero-shot and few-shot situations. The work emphasizes the effectiveness and flexibility of LLMs in a variety of linguistic circumstances, providing insightful information about the complex field of depression detection models.</li>
<li><strong>摘要：</strong>在全球范围内普遍存在与未确诊抑郁症的无声斗争的时代，我们的研究深入探讨了心理健康与社交媒体之间的关键联系。这项工作的重点是使用 GPT 3.5、GPT 4 和我们提出的 GPT 3.5 微调模型 DepGPT 等 LLM 以及先进的深度学习模型（LSTM、Bi-LSTM、 GRU、BiGRU）和 Transformer 模型（BERT、BanglaBERT、SahajBERT、BanglaBERT-Base）。该研究将 Reddit 和 X 数据集分为“抑郁”和“非抑郁”部分，由具有心理健康专业知识的母语人士翻译成孟加拉语，从而创建了孟加拉社交媒体抑郁数据集 (BSMDD)。我们的工作提供了每个模型的完整架构细节，以及使用零样本和少样本学习技术评估其在孟加拉抑郁文本分类中的性能的系统方法。我们的工作证明了 SahajBERT 和 Bi-LSTM 在各自领域中具有 FastText 嵌入的优越性，还解决了 Transformer 模型的可解释性问题，并强调了 LLM（尤其是 DepGPT）的有效性，在一系列学习环境中展示了灵活性和能力。根据实验结果，所提出的模型 DepGPT 不仅在零样本和少样本场景中优于 Alpaca Lora 7B，而且优于其他所有模型，实现了 0.9796 的近乎完美的精度和 0.9804 的 F1 分数，高召回率和卓越的精确度。尽管具有竞争力，GPT-3.5 Turbo 和 Alpaca Lora 7B 在零射击和少射击情况下表现出相对较差的效果。这项工作强调了法学硕士在各种语言环境中的有效性和灵活性，提供了有关抑郁症检测模型复杂领域的深刻信息。</li>
</ul>

<h3>Title: MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, Kwan-Yee K. Wong</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07314">https://arxiv.org/abs/2401.07314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07314">https://arxiv.org/pdf/2401.07314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07314]] MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation(https://arxiv.org/abs/2401.07314)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, lora, prompt, rag, agent</a></li>
<li><strong>Abstract: </strong>Embodied agents equipped with GPT as their brain have exhibited extraordinary thinking and decision-making abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT to handle excessive environmental information and select potential locations within localized environments, without constructing an effective ''global-view'' (e.g., a commonly-used map) for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based path-planning agent, dubbed MapGPT, for the zero-shot VLN task. Specifically, we convert a topological map constructed online into prompts to encourage map-guided global exploration, and require the agent to explicitly output and update multi-step path planning to avoid getting stuck in local exploration. Extensive experiments demonstrate that our MapGPT is effective, achieving impressive performance on both the R2R and REVERIE datasets (38.8% and 28.4% success rate, respectively) and showcasing the newly emerged global thinking and path planning capabilities of the GPT model. Unlike previous VLN agents, which require separate parameters fine-tuning or specific prompt design to accommodate various instruction styles across different datasets, our MapGPT is more unified as it can adapt to different instruction styles seamlessly, which is the first of its kind in this field.</li>
<li><strong>摘要：</strong>以 GPT 作为大脑的实体智能体在各种任务中表现出了非凡的思维和决策能力。然而，现有的用于视觉和语言导航（VLN）的零样本智能体仅提示 GPT 处理过多的环境信息并在局部环境中选择潜在位置，而没有构建有效的“全局视图”（例如，常见的-使用地图）让智能体了解整体环境。在这项工作中，我们提出了一种新颖的基于地图引导的 GPT 路径规划代理，称为 MapGPT，用于零样本 VLN 任务。具体来说，我们将在线构建的拓扑图转换为提示，以鼓励地图引导的全局探索，并要求代理显式输出和更新多步路径规划，以避免陷入局部探索。大量实验证明我们的 MapGPT 是有效的，在 R2R 和 REVERIE 数据集上都取得了令人印象深刻的性能（成功率分别为 38.8% 和 28.4%），并展示了 GPT 模型新出现的全局思维和路径规划能力。与之前的 VLN 代理需要单独的参数微调或特定的提示设计以适应不同数据集的各种指令风格不同，我们的 MapGPT 更加统一，因为它可以无缝适应不同的指令风格，这是该领域的首创。</li>
</ul>

<h3>Title: Small LLMs Are Weak Tool Learners: A Multi-LLM Agent</h3>
<ul>
<li><strong>Authors: </strong>Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07324">https://arxiv.org/abs/2401.07324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07324">https://arxiv.org/pdf/2401.07324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07324]] Small LLMs Are Weak Tool Learners: A Multi-LLM Agent(https://arxiv.org/abs/2401.07324)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete complex tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers but also excel in task planning, memory management, tool invocation, and result summarization. While traditional approaches focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. Moreover, the entire LLM may require retraining when tools are updated. To overcome these challenges, we propose a novel strategy that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with other components to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 代理显着扩展了独立 LLM 的功能，使它们能够与外部工具（例如 API、函数）交互并以自我指导的方式完成复杂的任务。工具使用的挑战要求法学硕士不仅能够理解用户查询并生成答案，而且还要擅长任务规划、内存管理、工具调用和结果总结。虽然传统方法侧重于培训具有所有这些功能的单个法学硕士，但性能限制变得明显，特别是对于较小的模型。此外，当工具更新时，整个法学硕士可能需要重新培训。为了克服这些挑战，我们提出了一种新颖的策略，将上述功能分解为规划器、调用器和总结器。每个组件都由一个专注于特定功能并与其他组件协作来完成任务的法学硕士实施。这种模块化框架有利于单独更新以及使用较小的法学硕士来构建每种功能。为了有效地训练这个框架，我们引入了两阶段训练范例。首先，我们在整个数据集上微调主干 LLM，而不区分子任务，为模型提供对任务的全面理解。其次，微调的LLM分别用于实例化规划器、调用器和摘要器，它们在各自的子任务上不断进行微调。对各种工具使用基准的评估表明，我们提出的多LLM框架超越了传统的单LLM方法，突出了其在工具学习方面的功效和优势。</li>
</ul>

<h3>Title: ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided  Sequence Reordering</h3>
<ul>
<li><strong>Authors: </strong>Yakun Song, Zhuo Chen, Xiaofei Wang, Ziyang Ma, Xie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07333">https://arxiv.org/abs/2401.07333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07333">https://arxiv.org/pdf/2401.07333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07333]] ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided  Sequence Reordering(https://arxiv.org/abs/2401.07333)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, code</a></li>
<li><strong>Abstract: </strong>The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.</li>
<li><strong>摘要：</strong>基于声学和语言提示的语言模型（LM）方法，例如VALL-E，在零样本音频生成领域取得了显着的进展。然而，现有方法仍然存在一些局限性：1）由于音频和音素标记之间的对齐约束有限，输出合成语音中存在重复、换位和遗漏； 2）使用自回归（AR）语言模型对合成语音进行细粒度控制的挑战； 3）由于基于AR的解码的性质，特别是在贪婪策略下，会产生无限沉默。为了缓解这些问题，我们提出了 ELLA-V，这是一种简单但高效的基于 LM 的零样本文本转语音 (TTS) 框架，它能够在音素级别对合成音频进行细粒度控制。 ELLA-V 的关键是声学标记和音素标记的交错序列，其中音素标记出现在相应的声学标记之前。实验结果表明，我们的模型在准确性方面优于 VALL-E，并且使用贪婪和基于采样的解码策略提供更稳定的结果。 ELLA-V的代码将在清理后开源。音频样本可在 https://ereboas.github.io/ELLAV/ 获取。</li>
</ul>

<h3>Title: Promptformer: Prompted Conformer Transducer for ASR</h3>
<ul>
<li><strong>Authors: </strong>Sergio Duarte-Torres, Arunasish Sen, Aman Rana, Lukas Drude, Alejandro Gomez-Alanis, Andreas Schwarz, Leif Rädel, Volker Leutnant</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07360">https://arxiv.org/abs/2401.07360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07360">https://arxiv.org/pdf/2401.07360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07360]] Promptformer: Prompted Conformer Transducer for ASR(https://arxiv.org/abs/2401.07360)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt, rag</a></li>
<li><strong>Abstract: </strong>Context cues carry information which can improve multi-turn interactions in automatic speech recognition (ASR) systems. In this paper, we introduce a novel mechanism inspired by hyper-prompting to fuse textual context with acoustic representations in the attention mechanism. Results on a test set with multi-turn interactions show that our method achieves 5.9% relative word error rate reduction (rWERR) over a strong baseline. We show that our method does not degrade in the absence of context and leads to improvements even if the model is trained without context. We further show that leveraging a pre-trained sentence-piece model for context embedding generation can outperform an external BERT model.</li>
<li><strong>摘要：</strong>上下文线索携带的信息可以改善自动语音识别 (ASR) 系统中的多轮交互。在本文中，我们介绍了一种受超级提示启发的新颖机制，将文本上下文与注意力机制中的声学表示融合在一起。多轮交互测试集的结果表明，我们的方法在强基线上实现了 5.9% 的相对单词错误率 (rWERR) 降低。我们表明，即使模型在没有上下文的情况下进行训练，我们的方法在没有上下文的情况下也不会退化，并且会带来改进。我们进一步表明，利用预训练的句子片段模型进行上下文嵌入生成可以优于外部 BERT 模型。</li>
</ul>

<h3>Title: PersonalityChat: Conversation Distillation for Personalized Dialog  Modeling with Facts and Traits</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Lotfi, Maxime De Bruyn, Jeska Buhmann, Walter Daelemans</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07363">https://arxiv.org/abs/2401.07363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07363">https://arxiv.org/pdf/2401.07363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07363]] PersonalityChat: Conversation Distillation for Personalized Dialog  Modeling with Facts and Traits(https://arxiv.org/abs/2401.07363)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>The new wave of Large Language Models (LLM) has offered an efficient tool to curate sizeable conversational datasets. So far studies have mainly focused on task-oriented or generic open-domain dialogs, and have not fully explored the ability of LLMs in following complicated prompts. In this work, we focus on personalization, and employ LLMs to curate a dataset which is difficult and costly to crowd-source: PersonalityChat is a synthetic conversational dataset based upon the popular PersonaChat dataset, but conditioned on both personas and (Big-5) personality traits. Evaluating models fine-tuned on this dataset, we show that the personality trait labels can be used for trait-based personalization of generative dialogue models. We also perform a head-to-head comparison between PersonalityChat and PersonaChat, and show that training on the distilled dataset results in more fluent and coherent dialog agents in the small-model regime.</li>
<li><strong>摘要：</strong>新一波的大型语言模型 (LLM) 提供了一种有效的工具来管理大量的会话数据集。迄今为止的研究主要集中在面向任务或通用开放域对话，并没有充分探索法学硕士遵循复杂提示的能力。在这项工作中，我们专注于个性化，并利用法学硕士来管理众包困难且成本高昂的数据集：PersonalityChat 是一个基于流行的 PersonaChat 数据集的合成对话数据集，但以角色和 (Big-5) 为条件人格特质。通过评估在此数据集上微调的模型，我们表明人格特质标签可用于生成对话模型的基于特质的个性化。我们还对 PersonalityChat 和 PersonaChat 进行了头对头比较，结果表明，对精炼数据集进行训练可以在小模型体系中产生更流畅、更连贯的对话代理。</li>
</ul>

<h3>Title: PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar  Nonlinear Conservation Laws</h3>
<ul>
<li><strong>Authors: </strong>Liu Yang, Stanley J. Osher</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07364">https://arxiv.org/abs/2401.07364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07364">https://arxiv.org/pdf/2401.07364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07364]] PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar  Nonlinear Conservation Laws(https://arxiv.org/abs/2401.07364)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Can we build a single large model for a wide range of PDE-related scientific learning tasks? Can this model generalize to new PDEs, even of new forms, without any fine-tuning? In-context operator learning and the corresponding model In-Context Operator Networks (ICON) [1] represent an initial exploration of these questions. The capability of ICON regarding the first question has been demonstrated in [1]. In this paper, we explore the second question by investigating the generalization capabilities of ICON for conservation laws, a family of PDEs with temporal evolution. We show the positive answer to the second question, i.e., ICON can generalize well to some PDEs with new forms without any fine-tuning. We also show how to broaden the range of problems that ICON can address, by transforming functions and equations to ICON's capability scope. We believe that the progress in this paper is a significant step towards the goal of training a foundation model for PDE-related tasks under the in-context operator learning framework.</li>
<li><strong>摘要：</strong>我们能否为各种与偏微分方程相关的科学学习任务构建一个单一的大型模型？该模型能否在不进行任何微调的情况下推广到新的偏微分方程，甚至是新形式的偏微分方程？上下文算子学习和相应的模型上下文算子网络（ICON）[1]代表了对这些问题的初步探索。 ICON 对于第一个问题的能力已在 [1] 中得到证明。在本文中，我们通过研究 ICON 对守恒定律（一类具有时间演化的偏微分方程）的泛化能力来探讨第二个问题。我们对第二个问题给出了肯定的答案，即 ICON 可以很好地推广到一些具有新形式的偏微分方程，而无需任何微调。我们还展示了如何通过将函数和方程转换为 ICON 的功能范围来扩大 ICON 可以解决的问题范围。我们相信，本文的进展是朝着在上下文算子学习框架下训练偏微分方程相关任务的基础模型的目标迈出的重要一步。</li>
</ul>

<h3>Title: Active Learning for NLP with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuesong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07367">https://arxiv.org/abs/2401.07367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07367">https://arxiv.org/pdf/2401.07367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07367]] Active Learning for NLP with Large Language Models(https://arxiv.org/abs/2401.07367)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Human annotation of training samples is expensive, laborious, and sometimes challenging, especially for Natural Language Processing (NLP) tasks. To reduce the labeling cost and enhance the sample efficiency, Active Learning (AL) technique can be used to label as few samples as possible to reach a reasonable or similar results. To reduce even more costs and with the significant advances of Large Language Models (LLMs), LLMs can be a good candidate to annotate samples. This work investigates the accuracy and cost of using LLMs (GPT-3.5 and GPT-4) to label samples on 3 different datasets. A consistency-based strategy is proposed to select samples that are potentially incorrectly labeled so that human annotations can be used for those samples in AL settings, and we call it mixed annotation strategy. Then we test performance of AL under two different settings: (1) using human annotations only; (2) using the proposed mixed annotation strategy. The accuracy of AL models under 3 AL query strategies are reported on 3 text classification datasets, i.e., AG's News, TREC-6, and Rotten Tomatoes. On AG's News and Rotten Tomatoes, the models trained with the mixed annotation strategy achieves similar or better results compared to that with human annotations. The method reveals great potentials of LLMs as annotators in terms of accuracy and cost efficiency in active learning settings.</li>
<li><strong>摘要：</strong>训练样本的人工注释既昂贵又费力，有时甚至具有挑战性，特别是对于自然语言处理 (NLP) 任务。为了降低标记成本并提高样本效率，可以使用主动学习（AL）技术来标记尽可能少的样本以达到合理或相似的结果。为了降低更多成本，并且随着大型语言模型 (LLM) 的显着进步，LLM 可以成为注释样本的良好候选者。这项工作研究了使用 LLM（GPT-3.5 和 GPT-4）在 3 个不同数据集上标记样本的准确性和成本。提出了一种基于一致性的策略来选择可能被错误标记的样本，以便在 AL 设置中对这些样本使用人工注释，我们称之为混合注释策略。然后我们在两种不同的设置下测试 AL 的性能：（1）仅使用人工注释； （2）使用所提出的混合注释策略。 3种AL查询策略下的AL模型的准确性在3个文本分类数据集上报告，即AG's News、TREC-6和Rotten Tomatoes。在 AG 新闻和烂番茄上，使用混合标注策略训练的模型与人类标注相比取得了相似或更好的结果。该方法揭示了法学硕士作为标注者在主动学习环境中的准确性和成本效率方面的巨大潜力。</li>
</ul>

<h3>Title: DRLC: Reinforcement Learning with Dense Rewards from LLM Critic</h3>
<ul>
<li><strong>Authors: </strong>Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, Lei Meng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07382">https://arxiv.org/abs/2401.07382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07382">https://arxiv.org/pdf/2401.07382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07382]] DRLC: Reinforcement Learning with Dense Rewards from LLM Critic(https://arxiv.org/abs/2401.07382)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only one reward for the entire generation. This sparsity of rewards can lead to inefficient and unstable learning. In this paper, we introduce a novel framework leveraging the critique ability of LLMs to produce dense rewards throughout the learning process. Our approach incorporates a critic language model alongside the policy model. This critic is prompted with the task description, question, policy model's output, and environment's reward signal as input, and provides token or span-level dense rewards that reflect the quality of each segment of the output. We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial dense rewards in training yields consistent performance gains over the PPO baseline with holistic rewards. Furthermore, in a setting where the same model serves as both policy and critic, we demonstrate that "self-critique" rewards also boost learning efficiency.</li>
<li><strong>摘要：</strong>强化学习 (RL) 可以将语言模型与不可微分的奖励信号（例如人类偏好）结合起来。然而，这些奖励信号的稀疏性带来了一个重大挑战——通常，整个一代只有一个奖励。这种奖励的稀疏性会导致学习效率低下且不稳定。在本文中，我们介绍了一种新颖的框架，利用法学硕士的批判能力在整个学习过程中产生密集的奖励。我们的方法将批评语言模型与政策模型结合起来。该批评家以任务描述、问题、策略模型的输出和环境的奖励信号作为输入进行提示，并提供反映输出每个部分质量的令牌或跨度级别的密集奖励。我们评估了我们在三个文本生成任务上的方法：情绪控制、语言模型解毒和摘要。实验结果表明，在训练中加入人工密集奖励可以在整体奖励的基础上产生比 PPO 基线一致的性能增益。此外，在同一模型既充当政策又充当批评者的情况下，我们证明“自我批评”奖励也可以提高学习效率。</li>
</ul>

<h3>Title: Optimising network interactions through device agnostic models</h3>
<ul>
<li><strong>Authors: </strong>Luca Manneschi, Ian T. Vidamour, Kilian D. Stenning, Jack C. Gartside, Charles Swindells, Guru Venkat, David Griffin, Susan Stepney, Will R. Branford, Thomas Hayward, Matt O Ellis, Eleni Vasilaki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07387">https://arxiv.org/abs/2401.07387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07387">https://arxiv.org/pdf/2401.07387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07387]] Optimising network interactions through device agnostic models(https://arxiv.org/abs/2401.07387)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>Physically implemented neural networks hold the potential to achieve the performance of deep learning models by exploiting the innate physical properties of devices as computational tools. This exploration of physical processes for computation requires to also consider their intrinsic dynamics, which can serve as valuable resources to process information. However, existing computational methods are unable to extend the success of deep learning techniques to parameters influencing device dynamics, which often lack a precise mathematical description. In this work, we formulate a universal framework to optimise interactions with dynamic physical systems in a fully data-driven fashion. The framework adopts neural stochastic differential equations as differentiable digital twins, effectively capturing both deterministic and stochastic behaviours of devices. Employing differentiation through the trained models provides the essential mathematical estimates for optimizing a physical neural network, harnessing the intrinsic temporal computation abilities of its physical nodes. To accurately model real devices' behaviours, we formulated neural-SDE variants that can operate under a variety of experimental settings. Our work demonstrates the framework's applicability through simulations and physical implementations of interacting dynamic devices, while highlighting the importance of accurately capturing system stochasticity for the successful deployment of a physically defined neural network.</li>
<li><strong>摘要：</strong>物理实现的神经网络有潜力通过利用设备的固有物理特性作为计算工具来实现深度学习模型的性能。对计算物理过程的探索还需要考虑其内在动力学，这可以作为处理信息的宝贵资源。然而，现有的计算方法无法将深度学习技术的成功扩展到影响设备动态的参数，这些参数通常缺乏精确的数学描述。在这项工作中，我们制定了一个通用框架，以完全数据驱动的方式优化与动态物理系统的交互。该框架采用神经随机微分方程作为可微分的数字孪生，有效捕获设备的确定性和随机行为。通过训练模型采用微分法，可以利用其物理节点固有的时间计算能力，为优化物理神经网络提供必要的数学估计。为了准确地模拟真实设备的行为，我们制定了可以在各种实验设置下运行的神经 SDE 变体。我们的工作通过交互动态设备的模拟和物理实现展示了该框架的适用性，同时强调了准确捕获系统随机性对于成功部署物理定义的神经网络的重要性。</li>
</ul>

<h3>Title: Harnessing the Power of Beta Scoring in Deep Active Learning for  Multi-Label Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Wei Tan, Ngoc Dang Nguyen, Lan Du, Wray Buntine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07395">https://arxiv.org/abs/2401.07395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07395">https://arxiv.org/pdf/2401.07395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07395]] Harnessing the Power of Beta Scoring in Deep Active Learning for  Multi-Label Text Classification(https://arxiv.org/abs/2401.07395)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Within the scope of natural language processing, the domain of multi-label text classification is uniquely challenging due to its expansive and uneven label distribution. The complexity deepens due to the demand for an extensive set of annotated data for training an advanced deep learning model, especially in specialized fields where the labeling task can be labor-intensive and often requires domain-specific knowledge. Addressing these challenges, our study introduces a novel deep active learning strategy, capitalizing on the Beta family of proper scoring rules within the Expected Loss Reduction framework. It computes the expected increase in scores using the Beta Scoring Rules, which are then transformed into sample vector representations. These vector representations guide the diverse selection of informative samples, directly linking this process to the model's expected proper score. Comprehensive evaluations across both synthetic and real datasets reveal our method's capability to often outperform established acquisition techniques in multi-label text classification, presenting encouraging outcomes across various architectural and dataset scenarios.</li>
<li><strong>摘要：</strong>在自然语言处理的范围内，多标签文本分类领域由于其广泛且不均匀的标签分布而具有独特的挑战性。由于需要大量带注释的数据来训练高级深度学习模型，因此复杂性加深，特别是在标记任务可能是劳动密集型且通常需要特定领域知识的专业领域。为了解决这些挑战，我们的研究引入了一种新颖的深度主动学习策略，利用预期损失减少框架内的 Beta 系列适当评分规则。它使用 Beta 评分规则计算分数的预期增长，然后将其转换为样本向量表示。这些向量表示指导信息样本的多样化选择，直接将该过程与模型的预期正确分数联系起来。对合成数据集和真实数据集的综合评估表明，我们的方法在多标签文本分类方面通常优于现有的采集技术，在各种架构和数据集场景中呈现出令人鼓舞的结果。</li>
</ul>

<h3>Title: Leveraging the power of transformers for guilt detection in text</h3>
<ul>
<li><strong>Authors: </strong>Abdul Gafar Manuel Meque, Jason Angel, Grigori Sidorov, Alexander Gelbukh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07414">https://arxiv.org/abs/2401.07414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07414">https://arxiv.org/pdf/2401.07414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07414]] Leveraging the power of transformers for guilt detection in text(https://arxiv.org/abs/2401.07414)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>In recent years, language models and deep learning techniques have revolutionized natural language processing tasks, including emotion detection. However, the specific emotion of guilt has received limited attention in this field. In this research, we explore the applicability of three transformer-based language models for detecting guilt in text and compare their performance for general emotion detection and guilt detection. Our proposed model outformed BERT and RoBERTa models by two and one points respectively. Additionally, we analyze the challenges in developing accurate guilt-detection models and evaluate our model's effectiveness in detecting related emotions like "shame" through qualitative analysis of results.</li>
<li><strong>摘要：</strong>近年来，语言模型和深度学习技术彻底改变了自然语言处理任务，包括情绪检测。然而，内疚这一特定情绪在该领域受到的关注有限。在这项研究中，我们探索了三种基于变压器的语言模型在检测文本中的内疚感方面的适用性，并比较了它们在一般情绪检测和内疚检测方面的性能。我们提出的模型分别比 BERT 和 RoBERTa 模型高出 2 分和 1 分。此外，我们分析了开发准确的内疚检测模型所面临的挑战，并通过对结果的定性分析来评估我们的模型在检测“羞耻”等相关情绪方面的有效性。</li>
</ul>

<h3>Title: Generalized Planning for the Abstraction and Reasoning Corpus</h3>
<ul>
<li><strong>Authors: </strong>Chao Lei, Nir Lipovetzky, Krista A. Ehinger</a></li>
<li><strong>Subjects: </strong>cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07426">https://arxiv.org/abs/2401.07426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07426">https://arxiv.org/pdf/2401.07426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07426]] Generalized Planning for the Abstraction and Reasoning Corpus(https://arxiv.org/abs/2401.07426)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The Abstraction and Reasoning Corpus (ARC) is a general artificial intelligence benchmark that poses difficulties for pure machine learning methods due to its requirement for fluid intelligence with a focus on reasoning and abstraction. In this work, we introduce an ARC solver, Generalized Planning for Abstract Reasoning (GPAR). It casts an ARC problem as a generalized planning (GP) problem, where a solution is formalized as a planning program with pointers. We express each ARC problem using the standard Planning Domain Definition Language (PDDL) coupled with external functions representing object-centric abstractions. We show how to scale up GP solvers via domain knowledge specific to ARC in the form of restrictions over the actions model, predicates, arguments and valid structure of planning programs. Our experiments demonstrate that GPAR outperforms the state-of-the-art solvers on the object-centric tasks of the ARC, showing the effectiveness of GP and the expressiveness of PDDL to model ARC problems. The challenges provided by the ARC benchmark motivate research to advance existing GP solvers and understand new relations with other planning computational models. Code is available at github.com/you68681/GPAR.</li>
<li><strong>摘要：</strong>抽象与推理语料库（ARC）是一个通用的人工智能基准，由于其对侧重于推理和抽象的流体智能的要求，给纯机器学习方法带来了困难。在这项工作中，我们引入了 ARC 求解器，即抽象推理的通用规划 (GPAR)。它将 ARC 问题转化为广义规划 (GP) 问题，其中解决方案被形式化为带有指针的规划程序。我们使用标准规划域定义语言 (PDDL) 以及表示以对象为中心的抽象的外部函数来表达每个 ARC 问题。我们展示了如何通过针对 ARC 的领域知识以对行动模型、谓词、参数和规划程序的有效结构的限制的形式扩展 GP 求解器。我们的实验表明，GPAR 在 ARC 以对象为中心的任务上优于最先进的求解器，展示了 GP 的有效性和 PDDL 对 ARC 问题建模的表达能力。 ARC 基准测试带来的挑战激发了研究人员推进现有 GP 求解器的发展，并了解与其他规划计算模型的新关系。代码可在 github.com/you68681/GPAR 获取。</li>
</ul>

<h3>Title: Stability Analysis of ChatGPT-based Sentiment Analysis in AI Quality  Assurance</h3>
<ul>
<li><strong>Authors: </strong>Tinghui Ouyang, AprilPyone MaungMaung, Koichi Konishi, Yoshiki Seo, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07441">https://arxiv.org/abs/2401.07441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07441">https://arxiv.org/pdf/2401.07441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07441]] Stability Analysis of ChatGPT-based Sentiment Analysis in AI Quality  Assurance(https://arxiv.org/abs/2401.07441)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>In the era of large AI models, the complex architecture and vast parameters present substantial challenges for effective AI quality management (AIQM), e.g. large language model (LLM). This paper focuses on investigating the quality assurance of a specific LLM-based AI product--a ChatGPT-based sentiment analysis system. The study delves into stability issues related to both the operation and robustness of the expansive AI model on which ChatGPT is based. Experimental analysis is conducted using benchmark datasets for sentiment analysis. The results reveal that the constructed ChatGPT-based sentiment analysis system exhibits uncertainty, which is attributed to various operational factors. It demonstrated that the system also exhibits stability issues in handling conventional small text attacks involving robustness.</li>
<li><strong>摘要：</strong>在大型人工智能模型时代，复杂的架构和庞大的参数给有效的人工智能质量管理（AIQM）带来了巨大的挑战。大语言模型（LLM）。本文重点研究基于 LLM 的特定人工智能产品——基于 ChatGPT 的情感分析系统的质量保证。该研究深入探讨了与 ChatGPT 所基于的扩展人工智能模型的运行和鲁棒性相关的稳定性问题。使用用于情感分析的基准数据集进行实验分析。结果表明，所构建的基于 ChatGPT 的情感分析系统表现出不确定性，这归因于各种操作因素。它表明该系统在处理涉及鲁棒性的传统小文本攻击时也表现出稳定性问题。</li>
</ul>

<h3>Title: Formal Logic Enabled Personalized Federated Learning Through Property  Inference</h3>
<ul>
<li><strong>Authors: </strong>Ziyan An, Taylor T. Johnson, Meiyi Ma</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07448">https://arxiv.org/abs/2401.07448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07448">https://arxiv.org/pdf/2401.07448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07448]] Formal Logic Enabled Personalized Federated Learning Through Property  Inference(https://arxiv.org/abs/2401.07448)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Recent advancements in federated learning (FL) have greatly facilitated the development of decentralized collaborative applications, particularly in the domain of Artificial Intelligence of Things (AIoT). However, a critical aspect missing from the current research landscape is the ability to enable data-driven client models with symbolic reasoning capabilities. Specifically, the inherent heterogeneity of participating client devices poses a significant challenge, as each client exhibits unique logic reasoning properties. Failing to consider these device-specific specifications can result in critical properties being missed in the client predictions, leading to suboptimal performance. In this work, we propose a new training paradigm that leverages temporal logic reasoning to address this issue. Our approach involves enhancing the training process by incorporating mechanically generated logic expressions for each FL client. Additionally, we introduce the concept of aggregation clusters and develop a partitioning algorithm to effectively group clients based on the alignment of their temporal reasoning properties. We evaluate the proposed method on two tasks: a real-world traffic volume prediction task consisting of sensory data from fifteen states and a smart city multi-task prediction utilizing synthetic data. The evaluation results exhibit clear improvements, with performance accuracy improved by up to 54% across all sequential prediction models.</li>
<li><strong>摘要：</strong>联邦学习（FL）的最新进展极大地促进了去中心化协作应用程序的开发，特别是在物联网人工智能（AIoT）领域。然而，当前研究领域缺少的一个关键方面是启用具有符号推理功能的数据驱动客户端模型的能力。具体来说，参与的客户端设备固有的异构性提出了重大挑战，因为每个客户端都表现出独特的逻辑推理属性。如果不考虑这些特定于设备的规范，可能会导致客户端预测中遗漏关键属性，从而导致性能不佳。在这项工作中，我们提出了一种新的训练范例，利用时序逻辑推理来解决这个问题。我们的方法包括通过为每个 FL 客户端合并机械生成的逻辑表达式来增强训练过程。此外，我们引入了聚合集群的概念，并开发了一种分区算法，以根据时间推理属性的对齐对客户端进行有效的分组。我们在两个任务上评估所提出的方法：由来自十五个州的传感数据组成的现实世界交通量预测任务和利用合成数据的智能城市多任务预测。评估结果显示出明显的改进，所有顺序预测模型的性能准确性提高了高达 54%。</li>
</ul>

<h3>Title: Model Editing at Scale leads to Gradual and Catastrophic Forgetting</h3>
<ul>
<li><strong>Authors: </strong>Akshat Gupta, Anurag Rao, Gopala Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07453">https://arxiv.org/abs/2401.07453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07453">https://arxiv.org/pdf/2401.07453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07453]] Model Editing at Scale leads to Gradual and Catastrophic Forgetting(https://arxiv.org/abs/2401.07453)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgetting phase. Both gradual and catastrophic forgetting limit the usefulness of model editing methods at scale -- the former making model editing less effective as multiple edits are made to the model while the latter caps the scalability of such model editing methods. Our analysis also highlights other key limitations of ROME and MEMIT at scale. With our work, we push for the development and evaluation of model editing methods keeping scalability in mind.</li>
<li><strong>摘要：</strong>在大型语言模型中编辑知识是一项很有吸引力的功能，它使我们能够在预训练期间纠正错误学习的事实，并使用不断增长的新事实列表来更新模型。虽然现有的模型编辑技术已显示出希望，但通常使用一次或几次编辑的可靠性、特异性和泛化性指标来评估它们。我们认为，为了使模型编辑具有实用性，我们必须能够对同一模型进行多次编辑。考虑到这一点，我们大规模评估当前的模型编辑方法，重点关注两种最先进的方法：ROME 和 MEMIT。我们发现，当模型按顺序编辑多个事实时，它会不断忘记先前编辑的事实以及执行下游任务的能力。这种遗忘发生在两个阶段——最初是渐进但渐进的遗忘阶段，随后是突然或灾难性的遗忘阶段。渐进性遗忘和灾难性遗忘都限制了模型编辑方法的大规模使用——前者使模型编辑效率降低，因为对模型进行了多次编辑，而后者则限制了此类模型编辑方法的可扩展性。我们的分析还强调了 ROME 和 MEMIT 在规模上的其他主要局限性。通过我们的工作，我们推动模型编辑方法的开发和评估，同时考虑到可扩展性。</li>
</ul>

<h3>Title: Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering  Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zihao Wang, P S Pravin, Zhe Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07494">https://arxiv.org/abs/2401.07494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07494">https://arxiv.org/pdf/2401.07494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07494]] Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering  Tasks(https://arxiv.org/abs/2401.07494)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Network. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor.</li>
<li><strong>摘要：</strong>计算效率和对抗鲁棒性是现实工程应用中的关键因素。然而，传统的神经网络往往无法同时甚至单独解决这两个问题。从自然物理系统和现有文献中汲取见解，众所周知，输入凸架构可以提高计算效率，而 Lipschitz 约束架构则可以增强对抗鲁棒性。通过利用凸性和 Lipschitz 连续性的优势，我们开发了一种新颖的网络架构，称为输入凸 Lipschitz 循环神经网络。该模型在计算效率和对抗鲁棒性方面优于一系列工程任务中的现有循环单元。这些任务包括基准 MNIST 图像分类、新加坡 LHT Holdings 太阳能光伏系统规划的真实太阳辐照度预测以及化学反应器的实时模型预测控制优化。</li>
</ul>

<h3>Title: Developing ChatGPT for Biology and Medicine: A Complete Review of  Biomedical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Qing Li, Lei Li, Yu Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07510">https://arxiv.org/abs/2401.07510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07510">https://arxiv.org/pdf/2401.07510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07510]] Developing ChatGPT for Biology and Medicine: A Complete Review of  Biomedical Question Answering(https://arxiv.org/abs/2401.07510)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, lora, chat</a></li>
<li><strong>Abstract: </strong>ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks such as unimodal-related question answering, reading comprehension, reasoning, diagnosis, relation extraction, probability modeling, and others, as well as multimodal-related tasks like vision question answering, image caption, cross-modal retrieval, report summarization, and generation, are discussed in detail. Each section delves into the intricate specifics of the respective method under consideration. This paper highlights the structures and advancements of medical domain explorations against general domain methods, emphasizing their applications across different tasks and datasets. It also outlines current challenges and opportunities for future medical domain research, paving the way for continued innovation and application in this rapidly evolving field.</li>
<li><strong>摘要：</strong>ChatGPT 探索了提供医疗诊断、治疗建议和其他医疗保健支持的问答 (QA) 战略蓝图。这是通过自然语言处理 (NLP) 和多模式范式越来越多地整合医学领域数据来实现的。通过将文本、图像、视频和其他模态的分布从一般领域转移到医学领域，这些技术加快了医学领域问答（MDQA）的进展。它们弥合了人类自然语言与复杂的医学领域知识或专家手动注释之间的差距，处理医学环境中的大规模、多样化、不平衡甚至未标记的数据分析场景。我们的重点是利用语言模型和多模态范式进行医学问答，旨在指导研究界为其特定的医学研究需求选择适当的机制。专业任务，如单模态相关问答、阅读理解、推理、诊断、关系提取、概率建模等，以及多模态相关任务，如视觉问答、图像标题、跨模态检索、报告摘要等一代，详细讨论。每个部分都深入研究了所考虑的相应方法的复杂细节。本文重点介绍了医学领域探索相对于通用领域方法的结构和进展，强调了它们在不同任务和数据集上的应用。它还概述了未来医学领域研究当前的挑战和机遇，为这个快速发展的领域的持续创新和应用铺平了道路。</li>
</ul>

<h3>Title: TAROT: A Hierarchical Framework with Multitask Co-Pretraining on  Semi-Structured Data towards Effective Person-Job Fit</h3>
<ul>
<li><strong>Authors: </strong>Yihan Cao, Xu Chen, Lun Du, Hao Chen, Qiang Fu, Shi Han, Yushu Du, Yanbin Kang, Guangming Lu, Zi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07525">https://arxiv.org/abs/2401.07525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07525">https://arxiv.org/pdf/2401.07525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07525]] TAROT: A Hierarchical Framework with Multitask Co-Pretraining on  Semi-Structured Data towards Effective Person-Job Fit(https://arxiv.org/abs/2401.07525)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Person-job fit is an essential part of online recruitment platforms in serving various downstream applications like Job Search and Candidate Recommendation. Recently, pretrained large language models have further enhanced the effectiveness by leveraging richer textual information in user profiles and job descriptions apart from user behavior features and job metadata. However, the general domain-oriented design struggles to capture the unique structural information within user profiles and job descriptions, leading to a loss of latent semantic correlations. We propose TAROT, a hierarchical multitask co-pretraining framework, to better utilize structural and semantic information for informative text embeddings. TAROT targets semi-structured text in profiles and jobs, and it is co-pretained with multi-grained pretraining tasks to constrain the acquired semantic information at each level. Experiments on a real-world LinkedIn dataset show significant performance improvements, proving its effectiveness in person-job fit tasks.</li>
<li><strong>摘要：</strong>人岗匹配是在线招聘平台服务求职、候选人推荐等各种下游应用的重要组成部分。最近，除了用户行为特征和工作元数据之外，预训练的大语言模型还利用用户配置文件和工作描述中更丰富的文本信息，进一步提高了有效性。然而，一般的面向领域的设计很难捕获用户配置文件和工作描述中的独特结构信息，导致潜在语义相关性的丢失。我们提出了 TAROT，一种分层多任务协同预训练框架，以更好地利用结构和语义信息进行信息文本嵌入。 TAROT针对的是个人资料和工作中的半结构化文本，并与多粒度的预训练任务共同保留，以约束每个级别所获取的语义信息。在现实世界的 LinkedIn 数据集上进行的实验显示出显着的性能改进，证明了其在人员与工作匹配任务中的有效性。</li>
</ul>

<h3>Title: Editing Arbitrary Propositions in LLMs without Subject Labels</h3>
<ul>
<li><strong>Authors: </strong>Itai Feigenbaum, Devansh Arpit, Huan Wang, Shelby Heinecke, Juan Carlos Niebles, Weiran Yao, Caiming Xiong, Silvio Savarese</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07526">https://arxiv.org/abs/2401.07526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07526">https://arxiv.org/pdf/2401.07526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07526]] Editing Arbitrary Propositions in LLMs without Subject Labels(https://arxiv.org/abs/2401.07526)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) editing modifies factual information in LLMs. Locate-and-Edit (L\&E) methods accomplish this by finding where relevant information is stored within the neural network, and editing the weights at that location. The goal of editing is to modify the response of an LLM to a proposition independently of its phrasing, while not modifying its response to other related propositions. Existing methods are limited to binary propositions, which represent straightforward binary relations between a subject and an object. Furthermore, existing methods rely on semantic subject labels, which may not be available or even be well-defined in practice. In this paper, we show that both of these issues can be effectively skirted with a simple and fast localization method called Gradient Tracing (GT). This localization method allows editing arbitrary propositions instead of just binary ones, and does so without the need for subject labels. As propositions always have a truth value, our experiments prompt an LLM as a boolean classifier, and edit its T/F response to propositions. Our method applies GT for location tracing, and then edit the model at that location using a mild variant of Rank-One Model Editing (ROME). On datasets of binary propositions derived from the CounterFact dataset, we show that our method -- without access to subject labels -- performs close to state-of-the-art L\&E methods which has access subject labels. We then introduce a new dataset, Factual Accuracy Classification Test (FACT), which includes non-binary propositions and for which subject labels are not generally applicable, and therefore is beyond the scope of existing L\&E methods. Nevertheless, we show that with our method editing is possible on FACT.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 编辑修改 LLM 中的事实信息。定位和编辑 (L\&E) 方法通过查找相关信息存储在神经网络中的位置并编辑该位置的权重来实现此目的。编辑的目标是独立于措辞修改法学硕士对命题的响应，同时不修改其对其他相关命题的响应。现有的方法仅限于二元命题，它表示主语和客体之间直接的二元关系。此外，现有方法依赖于语义主题标签，而这些标签在实践中可能不可用，甚至没有明确定义。在本文中，我们表明可以通过一种简单而快速的定位方法（称为梯度追踪（GT））有效地解决这两个问题。这种本地化方法允许编辑任意命题而不仅仅是二进制命题，并且不需要主题标签。由于命题总是具有真值，我们的实验提示 LLM 作为布尔分类器，并编辑其对命题的 T/F 响应。我们的方法应用 GT 进行位置跟踪，然后使用 Rank-One 模型编辑 (ROME) 的温和变体在该位置编辑模型。在从 CounterFact 数据集派生的二元命题数据集上，我们表明我们的方法（无需访问主题标签）的性能接近可访问主题标签的最先进的 L\&E 方法。然后，我们引入一个新的数据集，事实准确性分类测试（FACT），其中包括非二元命题，并且主题标签通常不适用，因此超出了现有 L\&E 方法的范围。尽管如此，我们证明使用我们的方法在 FACT 上进行编辑是可能的。</li>
</ul>

<h3>Title: See the Unseen: Better Context-Consistent Knowledge-Editing by Noises</h3>
<ul>
<li><strong>Authors: </strong>Youcheng Huang, Wenqiang Lei, Zheng Zhang, Jiancheng Lv, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07544">https://arxiv.org/abs/2401.07544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07544">https://arxiv.org/pdf/2401.07544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07544]] See the Unseen: Better Context-Consistent Knowledge-Editing by Noises(https://arxiv.org/abs/2401.07544)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge-editing updates knowledge of large language models (LLMs) and contributes to the interpretability and application of LLMs. However, knowledge applying is context-consistent: LLMs can recall the same knowledge in different contexts. Existing works ignore this property and the editing lacks generalization. In this paper, we empirically find that the effects of different contexts upon LLMs in recalling the same knowledge follow a Gaussian-like distribution. We then sample Gaussian noises to simulate the effects of different contexts when updating LLMs. By such, we can make LLMs see the unseen contexts where the edited knowledge will be applied, therefore improving the editing generalization. Experimental results on three LLMs demonstrate the effectiveness of our methods and also distinguish our methods from the others of fine-tuning LLMs by noises.</li>
<li><strong>摘要：</strong>知识编辑更新了大语言模型（LLM）的知识，并有助于LLM的可解释性和应用。然而，知识应用是上下文一致的：法学硕士可以在不同的上下文中回忆起相同的知识。现有作品忽略了这一特性，编辑缺乏概括性。在本文中，我们凭经验发现不同背景对法学硕士回忆相同知识的影响遵循类高斯分布。然后，我们对高斯噪声进行采样，以模拟更新 LLM 时不同上下文的影响。通过这样，我们可以让法学硕士看到编辑知识将被应用的看不见的上下文，从而提高编辑泛化能力。三个 LLM 的实验结果证明了我们方法的有效性，并将我们的方法与其他通过噪声微调 LLM 的方法区分开来。</li>
</ul>

<h3>Title: Safe Reinforcement Learning with Free-form Natural Language Constraints  and Pre-Trained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xingzhou Lou, Junge Zhang, Ziyan Wang, Kaiqi Huang, Yali Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07553">https://arxiv.org/abs/2401.07553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07553">https://arxiv.org/pdf/2401.07553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07553]] Safe Reinforcement Learning with Free-form Natural Language Constraints  and Pre-Trained Language Models(https://arxiv.org/abs/2401.07553)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Safe reinforcement learning (RL) agents accomplish given tasks while adhering to specific constraints. Employing constraints expressed via easily-understandable human language offers considerable potential for real-world applications due to its accessibility and non-reliance on domain expertise. Previous safe RL methods with natural language constraints typically adopt a recurrent neural network, which leads to limited capabilities when dealing with various forms of human language input. Furthermore, these methods often require a ground-truth cost function, necessitating domain expertise for the conversion of language constraints into a well-defined cost function that determines constraint violation. To address these issues, we proposes to use pre-trained language models (LM) to facilitate RL agents' comprehension of natural language constraints and allow them to infer costs for safe policy learning. Through the use of pre-trained LMs and the elimination of the need for a ground-truth cost, our method enhances safe policy learning under a diverse set of human-derived free-form natural language constraints. Experiments on grid-world navigation and robot control show that the proposed method can achieve strong performance while adhering to given constraints. The usage of pre-trained LMs allows our method to comprehend complicated constraints and learn safe policies without the need for ground-truth cost at any stage of training or evaluation. Extensive ablation studies are conducted to demonstrate the efficacy of each part of our method.</li>
<li><strong>摘要：</strong>安全强化学习（RL）代理在遵守特定约束的同时完成给定的任务。由于其可访问性和不依赖领域专业知识，采用通过易于理解的人类语言表达的约束为现实世界的应用程序提供了巨大的潜力。以前具有自然语言约束的安全强化学习方法通​​常采用循环神经网络，这导致在处理各种形式的人类语言输入时能力有限。此外，这些方法通常需要真实的成本函数，需要领域专业知识才能将语言约束转换为明确定义的成本函数，以确定约束违规。为了解决这些问题，我们建议使用预训练的语言模型（LM）来促进强化学习代理对自然语言约束的理解，并允许它们推断安全策略学习的成本。通过使用预先训练的 LM 并消除对真实成本的需求，我们的方法增强了在各种人类衍生的自由形式自然语言约束下的安全政策学习。网格世界导航和机器人控制的实验表明，所提出的方法可以在遵守给定约束的同时实现强大的性能。使用预先训练的 LM 使我们的方法能够理解复杂的约束并学习安全的策略，而无需在训练或评估的任何阶段使用真实成本。进行了广泛的消融研究以证明我们方法的每个部分的功效。</li>
</ul>

<h3>Title: Cascaded Cross-Modal Transformer for Audio-Textual Classification</h3>
<ul>
<li><strong>Authors: </strong>Nicolae-Catalin Ristea, Andrei Anghel, Radu Tudor Ionescu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07575">https://arxiv.org/abs/2401.07575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07575">https://arxiv.org/pdf/2401.07575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07575]] Cascaded Cross-Modal Transformer for Audio-Textual Classification(https://arxiv.org/abs/2401.07575)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Speech classification tasks often require powerful language understanding models to grasp useful features, which becomes problematic when limited training data is available. To attain superior classification performance, we propose to harness the inherent value of multimodal representations by transcribing speech using automatic speech recognition (ASR) models and translating the transcripts into different languages via pretrained translation models. We thus obtain an audio-textual (multimodal) representation for each data sample. Subsequently, we combine language-specific Bidirectional Encoder Representations from Transformers (BERT) with Wav2Vec2.0 audio features via a novel cascaded cross-modal transformer (CCMT). Our model is based on two cascaded transformer blocks. The first one combines text-specific features from distinct languages, while the second one combines acoustic features with multilingual features previously learned by the first transformer block. We employed our system in the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge. CCMT was declared the winning solution, obtaining an unweighted average recall (UAR) of 65.41% and 85.87% for complaint and request detection, respectively. Moreover, we applied our framework on the Speech Commands v2 and HarperValleyBank dialog data sets, surpassing previous studies reporting results on these benchmarks. Our code is freely available for download at: https://github.com/ristea/ccmt.</li>
<li><strong>摘要：</strong>语音分类任务通常需要强大的语言理解模型来掌握有用的特征，当可用的训练数据有限时，这就会成为问题。为了获得卓越的分类性能，我们建议通过使用自动语音识别（ASR）模型转录语音并通过预训练的翻译模型将转录文本翻译成不同的语言来利用多模态表示的固有价值。因此，我们获得每个数据样本的音频文本（多模式）表示。随后，我们通过新型级联跨模态变压器 (CCMT) 将特定于语言的 Transformers 双向编码器表示 (BERT) 与 Wav2Vec2.0 音频功能相结合。我们的模型基于两个级联变压器块。第一个结合了来自不同语言的文本特定特征，而第二个结合了声学特征和第一个转换器块先前学习的多语言特征。我们在 ACM 多媒体 2023 计算副语言学挑战赛的请求子挑战赛中使用了我们的系统。 CCMT 被宣布为获胜解决方案，在投诉和请求检测方面分别获得了 65.41% 和 85.87% 的未加权平均召回率 (UAR)。此外，我们将我们的框架应用于 Speech Commands v2 和 HarperValleyBank 对话数据集，超越了之前报告这些基准测试结果的研究。我们的代码可以免费下载：https://github.com/ristea/ccmt。</li>
</ul>

<h3>Title: E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy</h3>
<ul>
<li><strong>Authors: </strong>Oliver T. Unke, Hartmut Maennel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07595">https://arxiv.org/abs/2401.07595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07595">https://arxiv.org/pdf/2401.07595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07595]] E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy(https://arxiv.org/abs/2401.07595)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>This work introduces E3x, a software package for building neural networks that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$, consisting of translations, rotations, and reflections of three-dimensional space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models promise benefits whenever input and/or output data are quantities associated with three-dimensional objects. This is because the numeric values of such quantities (e.g. positions) typically depend on the chosen coordinate system. Under transformations of the reference frame, the values change predictably, but the underlying rules can be difficult to learn for ordinary machine learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks are guaranteed to satisfy the relevant transformation rules exactly, resulting in superior data efficiency and accuracy. The code for E3x is available from https://github.com/google-research/e3x.</li>
<li><strong>摘要：</strong>这项工作介绍了 E3x，一个用于构建与欧几里得群 $\mathrm{E}(3)$ 等变的神经网络的软件包，由三维空间的平移、旋转和反射组成。与普通神经网络相比，只要输入和/或输出数据是与三维对象相关的量，$\mathrm{E}(3)$-等变模型就会带来好处。这是因为这些量（例如位置）的数值通常取决于所选的坐标系。在参考系的变换下，值的变化是可预测的，但对于普通的机器学习模型来说，潜在的规则可能很难学习。通过内置的$\mathrm{E}(3)$-等方差，神经网络可以保证准确满足相关的变换规则，从而获得卓越的数据效率和准确性。 E3x 的代码可从 https://github.com/google-research/e3x 获取。</li>
</ul>

<h3>Title: MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Divyanshu Aggarwal, Ashutosh Sathe, Sunayana Sitaram</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07598">https://arxiv.org/abs/2401.07598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07598">https://arxiv.org/pdf/2401.07598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07598]] MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of  Large Language Models(https://arxiv.org/abs/2401.07598)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Parameter efficient finetuning has emerged as a viable solution for improving the performance of Large Language Models without requiring massive resources and compute. Prior work on multilingual evaluation has shown that there is a large gap between the performance of LLMs on English and other languages. Further, there is also a large gap between the performance of smaller open-source models and larger LLMs. Finetuning can be an effective way to bridge this gap and make language models more equitable. In this work, we finetune the LLaMA-7B and Mistral-7B models on synthetic multilingual instruction tuning data to determine its effect on model performance on five downstream tasks covering twenty three languages in all. Additionally, we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and higher quantisation values benefit low-resource languages. We find that parameter efficient finetuning of smaller open source models sometimes bridges the gap between the performance of these models and the larger ones, however, English performance can take a hit. We also find that finetuning sometimes improves performance on low-resource languages, while degrading performance on high-resource languages.</li>
<li><strong>摘要：</strong>参数高效微调已成为一种无需大量资源和计算即可提高大型语言模型性能的可行解决方案。先前的多语言评估工作表明，法学硕士在英语和其他语言方面的表现存在很大差距。此外，较小的开源模型和较大的法学硕士之间的性能也存在很大差距。微调可以是弥合这一差距并使语言模型更加公平的有效方法。在这项工作中，我们根据合成多语言指令调整数据对 LLaMA-7B 和 Mistral-7B 模型进行微调，以确定其对总共涵盖 23 种语言的 5 个下游任务的模型性能的影响。此外，我们还尝试了各种参数，例如低等级适应的等级和量化值，以确定它们对下游性能的影响，并发现较高的等级和较高的量化值有利于低资源语言。我们发现，对较小的开源模型进行参数有效微调有时会缩小这些模型与较大模型的性能之间的差距，但是，英语性能可能会受到影响。我们还发现，微调有时会提高低资源语言的性能，但会降低高资源语言的性能。</li>
</ul>

<h3>Title: Compute-Efficient Active Learning</h3>
<ul>
<li><strong>Authors: </strong>Gábor Németh, Tamás Matuszka</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07639">https://arxiv.org/abs/2401.07639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07639">https://arxiv.org/pdf/2401.07639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07639]] Compute-Efficient Active Learning(https://arxiv.org/abs/2401.07639)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Active learning, a powerful paradigm in machine learning, aims at reducing labeling costs by selecting the most informative samples from an unlabeled dataset. However, the traditional active learning process often demands extensive computational resources, hindering scalability and efficiency. In this paper, we address this critical issue by presenting a novel method designed to alleviate the computational burden associated with active learning on massive datasets. To achieve this goal, we introduce a simple, yet effective method-agnostic framework that outlines how to strategically choose and annotate data points, optimizing the process for efficiency while maintaining model performance. Through case studies, we demonstrate the effectiveness of our proposed method in reducing computational costs while maintaining or, in some cases, even surpassing baseline model outcomes. Code is available at https://github.com/aimotive/Compute-Efficient-Active-Learning.</li>
<li><strong>摘要：</strong>主动学习是机器学习中的一种强大范例，旨在通过从未标记的数据集中选择信息最丰富的样本来降低标记成本。然而，传统的主动学习过程通常需要大量的计算资源，阻碍了可扩展性和效率。在本文中，我们通过提出一种新颖的方法来解决这个关键问题，该方法旨在减轻与大规模数据集主动学习相关的计算负担。为了实现这一目标，我们引入了一个简单但有效的与方法无关的框架，该框架概述了如何战略性地选择和注释数据点，在保持模型性能的同时优化流程以提高效率。通过案例研究，我们证明了我们提出的方法在降低计算成本方面的有效性，同时保持甚至在某些情况下超越基线模型结果。代码可在 https://github.com/aimotive/Compute-Efficient-Active-Learning 获取。</li>
</ul>

<h3>Title: Empirical Evidence for the Fragment level Understanding on Drug  Molecular Structure of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiuyuan Hu, Guoqing Liu, Yang Zhao, Hao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07657">https://arxiv.org/abs/2401.07657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07657">https://arxiv.org/pdf/2401.07657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07657]] Empirical Evidence for the Fragment level Understanding on Drug  Molecular Structure of LLMs(https://arxiv.org/abs/2401.07657)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>AI for drug discovery has been a research hotspot in recent years, and SMILES-based language models has been increasingly applied in drug molecular design. However, no work has explored whether and how language models understand the chemical spatial structure from 1D sequences. In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments. The results indicate that language models can understand chemical structures from the perspective of molecular fragments, and the structural knowledge learned through fine-tuning is reflected in the high-frequency SMILES substrings generated by the model.</li>
<li><strong>摘要：</strong>人工智能用于药物发现是近年来的研究热点，基于SMILES的语言模型在药物分子设计中得到越来越多的应用。然而，还没有研究探讨语言模型是否以及如何理解一维序列的化学空间结构。在这项工作中，我们预训练了化学语言的 Transformer 模型，并针对药物设计目标对其进行微调，并研究了高频 SMILES 子串和分子片段之间的对应关系。结果表明，语言模型可以从分子片段的角度理解化学结构，并且通过微调学习到的结构知识反映在模型生成的高频SMILES子串中。</li>
</ul>

<h3>Title: Prompting open-source and commercial language models for grammatical  error correction of English learner text</h3>
<ul>
<li><strong>Authors: </strong>Christopher Davis, Andrew Caines, Øistein Andersen, Shiva Taslimipoor, Helen Yannakoudakis, Zheng Yuan, Christopher Bryant, Marek Rei, Paula Buttery</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07702">https://arxiv.org/abs/2401.07702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07702">https://arxiv.org/pdf/2401.07702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07702]] Prompting open-source and commercial language models for grammatical  error correction of English learner text(https://arxiv.org/abs/2401.07702)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Thanks to recent advances in generative AI, we are able to prompt large language models (LLMs) to produce texts which are fluent and grammatical. In addition, it has been shown that we can elicit attempts at grammatical error correction (GEC) from LLMs when prompted with ungrammatical input sentences. We evaluate how well LLMs can perform at GEC by measuring their performance on established benchmark datasets. We go beyond previous studies, which only examined GPT* models on a selection of English GEC datasets, by evaluating seven open-source and three commercial LLMs on four established GEC benchmarks. We investigate model performance and report results against individual error types. Our results indicate that LLMs do not always outperform supervised English GEC models except in specific contexts -- namely commercial LLMs on benchmarks annotated with fluency corrections as opposed to minimal edits. We find that several open-source models outperform commercial ones on minimal edit benchmarks, and that in some settings zero-shot prompting is just as competitive as few-shot prompting.</li>
<li><strong>摘要：</strong>由于生成式人工智能的最新进展，我们能够促使大型语言模型（LLM）生成流畅且符合语法的文本。此外，研究表明，当提示不符合语法的输入句子时，我们可以引发法学硕士进行语法错误纠正（GEC）的尝试。我们通过衡量法学硕士在既定基准数据集上的表现来评估他们在 GEC 的表现。我们超越了以前的研究，通过在四个已建立的 GEC 基准上评估七个开源和三个商业 LLM，仅在精选的英语 GEC 数据集上检查 GPT* 模型。我们调查模型性能并针对各个错误类型报告结果。我们的结果表明，法学硕士并不总是优于受监督的英语 GEC 模型，除非在特定情况下——即商业法学硕士在基准上进行了流畅性修正，而不是进行最少的编辑。我们发现，一些开源模型在最小编辑基准上优于商业模型，并且在某些设置中，零样本提示与少样本提示一样有竞争力。</li>
</ul>

<h3>Title: Go-Explore for Residential Energy Management</h3>
<ul>
<li><strong>Authors: </strong>Junlin Lu, Patrick Mannion, Karl Mason</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07710">https://arxiv.org/abs/2401.07710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07710">https://arxiv.org/pdf/2401.07710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07710]] Go-Explore for Residential Energy Management(https://arxiv.org/abs/2401.07710)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>Reinforcement learning is commonly applied in residential energy management, particularly for optimizing energy costs. However, RL agents often face challenges when dealing with deceptive and sparse rewards in the energy control domain, especially with stochastic rewards. In such situations, thorough exploration becomes crucial for learning an optimal policy. Unfortunately, the exploration mechanism can be misled by deceptive reward signals, making thorough exploration difficult. Go-Explore is a family of algorithms which combines planning methods and reinforcement learning methods to achieve efficient exploration. We use the Go-Explore algorithm to solve the cost-saving task in residential energy management problems and achieve an improvement of up to 19.84\% compared to the well-known reinforcement learning algorithms.</li>
<li><strong>摘要：</strong>强化学习通常应用于住宅能源管理，特别是优化能源成本。然而，强化学习智能体在处理能量控制领域中的欺骗性和稀疏奖励时经常面临挑战，尤其是随机奖励。在这种情况下，彻底的探索对于学习最优策略变得至关重要。不幸的是，探索机制可能会被欺骗性的奖励信号误导，使得彻底的探索变得困难。 Go-Explore是一系列结合规划方法和强化学习方法来实现高效探索的算法。我们使用 Go-Explore 算法来解决住宅能源管理问题中的成本节省任务，与著名的强化学习算法相比，实现了高达 19.84% 的改进。</li>
</ul>

<h3>Title: On the importance of Data Scale in Pretraining Arabic Language Models</h3>
<ul>
<li><strong>Authors: </strong>Abbas Ghaddar, Philippe Langlais, Mehdi Rezagholizadeh, Boxing Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07760">https://arxiv.org/abs/2401.07760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07760">https://arxiv.org/pdf/2401.07760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07760]] On the importance of Data Scale in Pretraining Arabic Language Models(https://arxiv.org/abs/2401.07760)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Pretraining monolingual language models have been proven to be vital for performance in Arabic Natural Language Processing (NLP) tasks. In this paper, we conduct a comprehensive study on the role of data in Arabic Pretrained Language Models (PLMs). More precisely, we reassess the performance of a suite of state-of-the-art Arabic PLMs by retraining them on massive-scale, high-quality Arabic corpora. We have significantly improved the performance of the leading Arabic encoder-only BERT-base and encoder-decoder T5-base models on the ALUE and ORCA leaderboards, thereby reporting state-of-the-art results in their respective model categories. In addition, our analysis strongly suggests that pretraining data by far is the primary contributor to performance, surpassing other factors. Our models and source code are publicly available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch.</li>
<li><strong>摘要：</strong>事实证明，预训练单语语言模型对于阿拉伯语自然语言处理 (NLP) 任务的性能至关重要。在本文中，我们对数据在阿拉伯语预训练语言模型（PLM）中的作用进行了全面的研究。更准确地说，我们通过在大规模、高质量的阿拉伯语语料库上重新训练一套最先进的阿拉伯语 PLM 的性能来重新评估它们。我们显着提高了 ALUE 和 ORCA 排行榜上领先的仅阿拉伯语编码器 BERT 基础和编码器解码器 T5 基础模型的性能，从而报告了各自模型类别中最先进的结果。此外，我们的分析强烈表明，到目前为止，预训练数据是性能的主要贡献者，超过了其他因素。我们的模型和源代码可在 https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch 上公开获取。</li>
</ul>

<h3>Title: When Large Language Model Agents Meet 6G Networks: Perception,  Grounding, and Alignment</h3>
<ul>
<li><strong>Authors: </strong>Minrui Xu, Niyato Dusit, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, Khaled B. Letaief</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07764">https://arxiv.org/abs/2401.07764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07764">https://arxiv.org/pdf/2401.07764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07764]] When Large Language Model Agents Meet 6G Networks: Perception,  Grounding, and Alignment(https://arxiv.org/abs/2401.07764)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, agent</a></li>
<li><strong>Abstract: </strong>AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to perform user-agent interactive tasks collaboratively. In the proposed system, LLM agents are split into perception, grounding, and alignment modules, facilitating inter-module communications to meet extended user requirements on 6G network functions, including integrated sensing and communication, digital twins, and task-oriented communications. Furthermore, we introduce a novel model caching algorithm for LLMs within the proposed system to improve model utilization in context, thus reducing network costs of the collaborative mobile and edge LLM agents.</li>
<li><strong>摘要：</strong>基于多模态大语言模型（LLM）的人工智能代理有望彻底改变人机交互，并在医疗、教育、制造和娱乐等各个领域提供更加个性化的助理服务。在6G网络中部署LLM代理，使用户能够通过移动设备民主地访问以前昂贵的AI助手服务，从而减少交互延迟并更好地保护用户隐私。然而，移动设备的有限容量限制了部署和执行本地LLM的有效性，这需要在长范围交互期间将复杂的任务卸载到在边缘服务器上运行的全局LLM。在本文中，我们提出了一种用于 6G 网络中 LLM 代理的分离学习系统，利用移动设备和边缘服务器之间的协作，其中具有不同角色的多个 LLM 分布在移动设备和边缘服务器上，以协作执行用户代理交互任务。在所提出的系统中，LLM代理被分为感知、接地和对准模块，促进模块间通信，以满足用户对6G网络功能的扩展需求，包括集成感知和通信、数字孪生和面向任务的通信。此外，我们在所提出的系统中引入了一种新颖的 LLM 模型缓存算法，以提高上下文中的模型利用率，从而降低协作移动和边缘 LLM 代理的网络成本。</li>
</ul>

<h3>Title: Quantum Transfer Learning for Acceptability Judgements</h3>
<ul>
<li><strong>Authors: </strong>Giuseppe Buonaiuto, Raffaele Guarasci, Aniello Minutolo, Giuseppe De Pietro, Massimo Esposito</a></li>
<li><strong>Subjects: </strong>cs.CL, physics.comp-ph, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07777">https://arxiv.org/abs/2401.07777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07777">https://arxiv.org/pdf/2401.07777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07777]] Quantum Transfer Learning for Acceptability Judgements(https://arxiv.org/abs/2401.07777)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Hybrid quantum-classical classifiers promise to positively impact critical aspects of natural language processing tasks, particularly classification-related ones. Among the possibilities currently investigated, quantum transfer learning, i.e., using a quantum circuit for fine-tuning pre-trained classical models for a specific task, is attracting significant attention as a potential platform for proving quantum advantage. This work shows potential advantages, both in terms of performance and expressiveness, of quantum transfer learning algorithms trained on embedding vectors extracted from a large language model to perform classification on a classical Linguistics task: acceptability judgments. Acceptability judgment is the ability to determine whether a sentence is considered natural and well-formed by a native speaker. The approach has been tested on sentences extracted from ItaCoLa, a corpus that collects Italian sentences labeled with their acceptability judgment. The evaluation phase shows results for the quantum transfer learning pipeline comparable to state-of-the-art classical transfer learning algorithms, proving current quantum computers' capabilities to tackle NLP tasks for ready-to-use applications. Furthermore, a qualitative linguistic analysis, aided by explainable AI methods, reveals the capabilities of quantum transfer learning algorithms to correctly classify complex and more structured sentences, compared to their classical counterpart. This finding sets the ground for a quantifiable quantum advantage in NLP in the near future.</li>
<li><strong>摘要：</strong>混合量子经典分类器有望对自然语言处理任务的关键方面产生积极影响，特别是与分类相关的任务。在目前研究的可能性中，量子迁移学习，即使用量子电路对特定任务的预训练经典模型进行微调，作为证明量子优势的潜在平台而引起了广泛关注。这项工作显示了量子迁移学习算法在性能和表达能力方面的潜在优势，该算法在从大型语言模型中提取的嵌入向量上进行训练，以对经典语言学任务（可接受性判断）进行分类。可接受性判断是确定母语人士是否认为句子自然且结构良好的能力。该方法已经在从 ItaCoLa 中提取的句子上进行了测试，ItaCoLa 是一个语料库，收集了带有可接受性判断标签的意大利语句子。评估阶段显示了与最先进的经典迁移学习算法相当的量子迁移学习管道的结果，证明了当前量子计算机处理即用型应用程序的 NLP 任务的能力。此外，在可解释的人工智能方法的辅助下，定性语言分析揭示了量子迁移学习算法与经典对应的算法相比，能够正确分类复杂且结构化的句子的能力。这一发现为不久的将来 NLP 领域可量化的量子优势奠定了基础。</li>
</ul>

<h3>Title: Activations and Gradients Compression for Model-Parallel Training</h3>
<ul>
<li><strong>Authors: </strong>Mikhail Rudakov, Aleksandr Beznosikov, Yaroslav Kholodov, Alexander Gasnikov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07788">https://arxiv.org/abs/2401.07788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07788">https://arxiv.org/pdf/2401.07788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07788]] Activations and Gradients Compression for Model-Parallel Training(https://arxiv.org/abs/2401.07788)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large neural networks require enormous computational clusters of machines. Model-parallel training, when the model architecture is partitioned sequentially between workers, is a popular approach for training modern models. Information compression can be applied to decrease workers communication time, as it is often a bottleneck in such systems. This work explores how simultaneous compression of activations and gradients in model-parallel distributed training setup affects convergence. We analyze compression methods such as quantization and TopK compression, and also experiment with error compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error feedback approach. We conduct experiments on image classification and language model fine-tuning tasks. Our findings demonstrate that gradients require milder compression rates than activations. We observe that $K=10\%$ is the lowest TopK compression level, which does not harm model convergence severely. Experiments also show that models trained with TopK perform well only when compression is also applied during inference. We find that error feedback techniques do not improve model-parallel training compared to plain compression, but allow model inference without compression with almost no quality drop. Finally, when applied with the AQ-SGD approach, TopK stronger than with $ K=30\%$ worsens model performance significantly.</li>
<li><strong>摘要：</strong>大型神经网络需要巨大的机器计算集群。模型并行训练，即模型架构在工作人员之间按顺序分区，是训练现代模型的流行方法。信息压缩可用于减少工作人员的通信时间，因为它通常是此类系统中的瓶颈。这项工作探讨了模型并行分布式训练设置中激活和梯度的同时压缩如何影响收敛。我们分析了量化和 TopK 压缩等压缩方法，并尝试了误差补偿技术。此外，我们采用 TopK 和 AQ-SGD 每批次误差反馈方法。我们对图像分类和语言模型微调任务进行实验。我们的研究结果表明，梯度比激活需要更温和的压缩率。我们观察到 $K=10\%$ 是最低的 TopK 压缩级别，这不会严重损害模型收敛。实验还表明，只有在推理过程中也应用压缩时，使用 TopK 训练的模型才能表现良好。我们发现，与普通压缩相比，错误反馈技术并不能改善模型并行训练，但允许在不压缩的情况下进行模型推理，几乎没有质量下降。最后，当应用 AQ-SGD 方法时，TopK 比 $ K=30\%$ 更强，导致模型性能显着恶化。</li>
</ul>

<h3>Title: Flexibly Scaling Large Language Models Contexts Through Extensible  Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Ninglu Shao, Shitao Xiao, Zheng Liu, Peitian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07793">https://arxiv.org/abs/2401.07793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07793">https://arxiv.org/pdf/2401.07793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07793]] Flexibly Scaling Large Language Models Contexts Through Extensible  Tokenization(https://arxiv.org/abs/2401.07793)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, code, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are in need of sufficient contexts to handle many critical applications, such as retrieval augmented generation and few-shot learning. However, due to the constrained window size, the LLMs can only access to the information within a limited context. Although the size of context window can be extended by fine-tuning, it will result in a substantial cost in both training and inference stage. In this paper, we present Extensible Tokenization as an alternative method which realizes the flexible scaling of LLMs' context. Extensible Tokenization stands as a midware in between of the tokenized context and the LLM, which transforms the raw token embeddings into the extensible embeddings. Such embeddings provide a more compact representation for the long context, on top of which the LLM is able to perceive more information with the same context window. Extensible Tokenization is also featured by its flexibility: the scaling factor can be flexibly determined within a feasible scope, leading to the extension of an arbitrary context length at the inference time. Besides, Extensible Tokenization is introduced as a drop-in component, which can be seamlessly plugged into not only the LLM itself and but also its fine-tuned derivatives, bringing in the extended contextual information while fully preserving the LLM's existing capabilities. We perform comprehensive experiments on long-context language modeling and understanding tasks, which verify Extensible Tokenization as an effective, efficient, flexible, and compatible method to extend LLM's context. Our model and source code will be made publicly available.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 需要足够的上下文来处理许多关键应用，例如检索增强生成和小样本学习。然而，由于窗口大小的限制，法学硕士只能在有限的背景下访问信息。虽然上下文窗口的大小可以通过微调来扩展，但这将导致训练和推理阶段的大量成本。在本文中，我们提出了可扩展标记化作为一种​​替代方法，它实现了法学硕士上下文的灵活扩展。可扩展标记化是标记化上下文和 LLM 之间的中间件，它将原始标记嵌入转换为可扩展嵌入。这种嵌入为长上下文提供了更紧凑的表示，在此基础上，LLM 能够使用相同的上下文窗口感知更多信息。可扩展标记化还具有灵活性：可以在可行范围内灵活确定缩放因子，从而在推理时扩展任意上下文长度。此外，可扩展标记化作为插入式组件引入，它不仅可以无缝插入LLM本身，还可以无缝插入其微调的衍生产品，引入扩展的上下文信息，同时完全保留LLM的现有功能。我们对长上下文语言建模和理解任务进行了全面的实验，验证了可扩展标记化是一种有效、高效、灵活且兼容的方法来扩展 LLM 的上下文。我们的模型和源代码将公开。</li>
</ul>

<h3>Title: Consolidating Strategies for Countering Hate Speech Using Persuasive  Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Sougata Saha, Rohini Srihari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07810">https://arxiv.org/abs/2401.07810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07810">https://arxiv.org/pdf/2401.07810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07810]] Consolidating Strategies for Countering Hate Speech Using Persuasive  Dialogues(https://arxiv.org/abs/2401.07810)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Hateful comments are prevalent on social media platforms. Although tools for automatically detecting, flagging, and blocking such false, offensive, and harmful content online have lately matured, such reactive and brute force methods alone provide short-term and superficial remedies while the perpetrators persist. With the public availability of large language models which can generate articulate synthetic and engaging content at scale, there are concerns about the rapid growth of dissemination of such malicious content on the web. There is now a need to focus on deeper, long-term solutions that involve engaging with the human perpetrator behind the source of the content to change their viewpoint or at least bring down the rhetoric using persuasive means. To do that, we propose defining and experimenting with controllable strategies for generating counter-arguments to hateful comments in online conversations. We experiment with controlling response generation using features based on (i) argument structure and reasoning-based Walton argument schemes, (ii) counter-argument speech acts, and (iii) human characteristics-based qualities such as Big-5 personality traits and human values. Using automatic and human evaluations, we determine the best combination of features that generate fluent, argumentative, and logically sound arguments for countering hate. We further share the developed computational models for automatically annotating text with such features, and a silver-standard annotated version of an existing hate speech dialog corpora.</li>
<li><strong>摘要：</strong>社交媒体平台上仇恨言论盛行。尽管自动检测、标记和阻止此类虚假、攻击性和有害在线内容的工具最近已经成熟，但在肇事者坚持不懈的情况下，这种被动和暴力的方法只能提供短期和肤浅的补救措施。随着大型语言模型的公开可用，这些模型可以大规模生成清晰的合成和引人入胜的内容，人们担心此类恶意内容在网络上传播的快速增长。现在需要关注更深入、长期的解决方案，包括与内容来源背后的肇事者进行接触，以改变他们的观点，或者至少使用有说服力的手段来平息言论。为此，我们建议定义和试验可控策略，以针对在线对话中的仇恨评论产生反驳。我们尝试使用基于以下特征的特征来控制响应生成：(i) 论证结构和基于推理的沃尔顿论证方案，(ii) 反论证言语行为，以及 (iii) 基于人类特征的品质，例如 Big-5 人格特征和人类特征价值观。通过自动和人工评估，我们确定最佳的特征组合，从而生成流畅、有争议性且逻辑合理的论据来对抗仇恨。我们进一步分享开发的计算模型，用于自动注释具有此类功能的文本，以及现有仇恨言论对话语料库的银标准注释版本。</li>
</ul>

<h3>Title: Wikidata as a seed for Web Extraction</h3>
<ul>
<li><strong>Authors: </strong>Kunpeng Guo, Dennis Diefenbach, Antoine Gourru, Christophe Gravier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07812">https://arxiv.org/abs/2401.07812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07812">https://arxiv.org/pdf/2401.07812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07812]] Wikidata as a seed for Web Extraction(https://arxiv.org/abs/2401.07812)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Wikidata has grown to a knowledge graph with an impressive size. To date, it contains more than 17 billion triples collecting information about people, places, films, stars, publications, proteins, and many more. On the other side, most of the information on the Web is not published in highly structured data repositories like Wikidata, but rather as unstructured and semi-structured content, more concretely in HTML pages containing text and tables. Finding, monitoring, and organizing this data in a knowledge graph is requiring considerable work from human editors. The volume and complexity of the data make this task difficult and time-consuming. In this work, we present a framework that is able to identify and extract new facts that are published under multiple Web domains so that they can be proposed for validation by Wikidata editors. The framework is relying on question-answering technologies. We take inspiration from ideas that are used to extract facts from textual collections and adapt them to extract facts from Web pages. For achieving this, we demonstrate that language models can be adapted to extract facts not only from textual collections but also from Web pages. By exploiting the information already contained in Wikidata the proposed framework can be trained without the need for any additional learning signals and can extract new facts for a wide range of properties and domains. Following this path, Wikidata can be used as a seed to extract facts on the Web. Our experiments show that we can achieve a mean performance of 84.07 at F1-score. Moreover, our estimations show that we can potentially extract millions of facts that can be proposed for human validation. The goal is to help editors in their daily tasks and contribute to the completion of the Wikidata knowledge graph.</li>
<li><strong>摘要：</strong>维基数据已经发展成为一个规模惊人的知识图谱。迄今为止，它包含超过 170 亿个三元组，收集有关人物、地点、电影、明星、出版物、蛋白质等的信息。另一方面，网络上的大多数信息并不是在像 Wikidata 这样的高度结构化的数据存储库中发布，而是作为非结构化和半结构化内容发布，更具体地说，在包含文本和表格的 HTML 页面中发布。在知识图中查找、监控和组织这些数据需要人类编辑做大量的工作。数据的数量和复杂性使得这项任务变得困难且耗时。在这项工作中，我们提出了一个框架，能够识别和提取在多个 Web 域下发布的新事实，以便维基数据编辑可以建议它们进行验证。该框架依赖于问答技术。我们从用于从文本集合中提取事实的想法中汲取灵感，并对其进行调整以从网页中提取事实。为了实现这一目标，我们证明语言模型不仅可以从文本集合中提取事实，还可以从网页中提取事实。通过利用维基数据中已包含的信息，可以对所提出的框架进行训练，而无需任何额外的学习信号，并且可以提取各种属性和领域的新事实。沿着这条路径，维基数据可以用作提取网络上事实的种子。我们的实验表明，我们可以在 F1 分数上实现 84.07 的平均性能。此外，我们的估计表明，我们有可能提取数百万个事实，供人类验证。目标是帮助编辑完成日常任务，并为维基数据知识图谱的完成做出贡献。</li>
</ul>

<h3>Title: Question Translation Training for Better Multilingual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07817">https://arxiv.org/abs/2401.07817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07817">https://arxiv.org/pdf/2401.07817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07817]] Question Translation Training for Better Multilingual Reasoning(https://arxiv.org/abs/2401.07817)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of chain-of-thought and mathematical reasoning instructions. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English question data. In this way we perform targetted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13B show that question alignment leads to consistent improvements over the translate-training approach: an average improvement of 11.3\% and 16.1\% accuracy across ten languages on the MGSM and MSVAMP maths reasoning benchmarks (The project will be available at: https://github.com/NJUNLP/QAlign).</li>
<li><strong>摘要：</strong>大型语言模型在推理任务中表现出引人注目的性能，但它们在英语以外的语言中往往表现较差。鉴于他们的训练数据主要由英文文本和指令组成，这并不奇怪。典型的解决方案是将指令数据翻译成所有感兴趣的语言，然后对生成的多语言数据进行训练，这称为翻译训练。这种方法不仅成本高昂，而且由于思维链和数学推理指令的格式不标准，导致数据翻译质量不佳。在本文中，我们探讨了问题对齐的好处，通过对 X-English 问题数据进行微调来训练模型将推理问题翻译成英语。通过这种方式，我们进行有针对性的领域内语言对齐，充分利用英语教学数据来释放法学硕士的多语言推理能力。 LLaMA2-13B 上的实验结果表明，问题对齐导致翻译训练方法的持续改进：在 MGSM 和 MSVAMP 数学推理基准上，十种语言的准确率平均提高了 11.3% 和 16.1%（该项目将在网址：https://github.com/NJUNLP/QAlign）。</li>
</ul>

<h3>Title: Milestones in Bengali Sentiment Analysis leveraging Transformer-models:  Fundamentals, Challenges and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Sengupta, Shreya Ghosh, Prasenjit Mitra, Tarikul Islam Tamiti</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07847">https://arxiv.org/abs/2401.07847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07847">https://arxiv.org/pdf/2401.07847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07847]] Milestones in Bengali Sentiment Analysis leveraging Transformer-models:  Fundamentals, Challenges and Future Directions(https://arxiv.org/abs/2401.07847)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Sentiment Analysis (SA) refers to the task of associating a view polarity (usually, positive, negative, or neutral; or even fine-grained such as slightly angry, sad, etc.) to a given text, essentially breaking it down to a supervised (since we have the view labels apriori) classification task. Although heavily studied in resource-rich languages such as English thus pushing the SOTA by leaps and bounds, owing to the arrival of the Transformer architecture, the same cannot be said for resource-poor languages such as Bengali (BN). For a language spoken by roughly 300 million people, the technology enabling them to run trials on their favored tongue is severely lacking. In this paper, we analyze the SOTA for SA in Bengali, particularly, Transformer-based models. We discuss available datasets, their drawbacks, the nuances associated with Bengali i.e. what makes this a challenging language to apply SA on, and finally provide insights for future direction to mitigate the limitations in the field.</li>
<li><strong>摘要：</strong>情感分析（SA）是指将观点极性（通常是积极的、消极的或中立的；甚至是细粒度的，例如轻微生气、悲伤等）与给定的文本关联起来的任务，本质上是将其分解为监督（因为我们有先验的视图标签）分类任务。尽管由于 Transformer 架构的到来，人们对英语等资源丰富的语言进行了大量研究，从而使 SOTA 突飞猛进，但对于孟加拉语（BN）等资源匮乏的语言来说，情况却并非如此。对于大约 3 亿人使用的语言来说，能够让他们用自己喜欢的语言进行试验的技术严重缺乏。在本文中，我们分析了孟加拉语 SA 的 SOTA，特别是基于 Transformer 的模型。我们讨论可用的数据集、它们的缺点、与孟加拉语相关的细微差别，即是什么使它成为应用 SA 的具有挑战性的语言，并最终为未来的方向提供见解，以减轻该领域的限制。</li>
</ul>

<h3>Title: Unlocking Efficiency in Large Language Model Inference: A Comprehensive  Survey of Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07851">https://arxiv.org/abs/2401.07851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07851">https://arxiv.org/pdf/2401.07851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07851]] Unlocking Efficiency in Large Language Model Inference: A Comprehensive  Survey of Speculative Decoding(https://arxiv.org/abs/2401.07851)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first efficiently drafts several future tokens and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, including current leading techniques, the challenges faced, and potential future directions in this field. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.</li>
<li><strong>摘要：</strong>为了减轻大型语言模型 (LLM) 中自回归解码带来的高推理延迟，推测解码已成为 LLM 推理的一种新型解码范例。在每个解码步骤中，该方法首先有效地起草几个未来的令牌，然后并行验证它们。与自回归解码不同，推测解码有助于每步同时解码多个标记，从而加速推理。本文对这种有前景的解码范式进行了全面的概述和分析。我们首先提供推测解码的正式定义和表述。然后，我们对其关键方面进行深入讨论，包括该领域当前的领先技术、面临的挑战以及未来潜在的方向。我们的目标是这项工作成为推测解码进一步研究的催化剂，最终有助于更有效的 LLM 推理。</li>
</ul>

<h3>Title: Authorship Obfuscation in Multilingual Machine-Generated Text Detection</h3>
<ul>
<li><strong>Authors: </strong>Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, Maria Bielikova</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07867">https://arxiv.org/abs/2401.07867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07867">https://arxiv.org/pdf/2401.07867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07867]] Authorship Obfuscation in Multilingual Machine-Generated Text Detection(https://arxiv.org/abs/2401.07867)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>High-quality text generation capability of latest Large Language Models (LLMs) causes concerns about their misuse (e.g., in massive generation/spread of disinformation). Machine-generated text (MGT) detection is important to cope with such threats. However, it is susceptible to authorship obfuscation (AO) methods, such as paraphrasing, which can cause MGTs to evade detection. So far, this was evaluated only in monolingual settings. Thus, the susceptibility of recently proposed multilingual detectors is still unknown. We fill this gap by comprehensively benchmarking the performance of 10 well-known AO methods, attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10 $\times$ 37 $\times$ 11 = 4,070 combinations). We also evaluate the effect of data augmentation on adversarial robustness using obfuscated texts. The results indicate that all tested AO methods can cause detection evasion in all tested languages, where homoglyph attacks are especially successful.</li>
<li><strong>摘要：</strong>最新大型语言模型 (LLM) 的高质量文本生成能力引起了对其滥用的担忧（例如，大量生成/传播虚假信息）。机器生成文本 (MGT) 检测对于应对此类威胁非常重要。然而，它很容易受到作者身份混淆 (AO) 方法的影响，例如释义，这可能导致 MGT 逃避检测。到目前为止，仅在单语环境中对此进行了评估。因此，最近提出的多语言检测器的敏感性仍然未知。我们通过对 10 种知名 AO 方法的性能进行全面基准测试，针对 11 种语言的 MGT 攻击 37 种 MGT 检测方法（即 10 $\times$ 37 $\times$ 11 = 4,070 种组合）来填补这一空白。我们还使用模糊文本评估数据增强对对抗鲁棒性的影响。结果表明，所有测试的 AO 方法都可以在所有测试的语言中引起检测规避，其中同形文字攻击尤其成功。</li>
</ul>

<h3>Title: JumpCoder: Go Beyond Autoregressive Coder via Online Modification</h3>
<ul>
<li><strong>Authors: </strong>Mouxiang Chen, Hao Tian, Zhongxin Liu, Xiaoxue Ren, Jianling Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07870">https://arxiv.org/abs/2401.07870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07870">https://arxiv.org/pdf/2401.07870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07870]] JumpCoder: Go Beyond Autoregressive Coder via Online Modification(https://arxiv.org/abs/2401.07870)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>While existing code large language models (code LLMs) exhibit impressive capabilities in code generation, their autoregressive sequential generation inherently lacks reversibility. This limitation hinders them from timely correcting previous missing statements during coding as humans do, often leading to error propagation and suboptimal performance. We introduce JumpCoder, a novel modelagnostic framework that enables online modification and non-sequential generation to augment the code LLMs. The key idea behind JumpCoder is to insert new code into the currently generated code when necessary during generation, which is achieved through an auxiliary infilling model that works in tandem with the code LLM. Since identifying the best infill position beforehand is intractable, we adopt an infill-first, judge-later strategy, which experiments with filling at the $k$ most critical positions following the generation of each line, and uses an Abstract Syntax Tree (AST) parser alongside the Generation Model Scoring to effectively judge the validity of each potential infill. Extensive experiments using six state-of-the-art code LLMs across multiple benchmarks consistently indicate significant improvements over all baselines. Notably, JumpCoder assists code LLMs in achieving up to a 3.6% increase in Pass@1 for Python, 6.3% for Java, and 3.7% for C++ in the multilingual HumanEval benchmarks. Our code is public at https://github.com/Keytoyze/JumpCoder.</li>
<li><strong>摘要：</strong>虽然现有的代码大语言模型（代码 LLM）在代码生成方面表现出令人印象深刻的能力，但它们的自回归顺序生成本质上缺乏可逆性。这种限制阻碍了他们像人类一样在编码过程中及时纠正以前丢失的语句，通常会导致错误传播和性能不佳。我们引入了 JumpCoder，这是一种新颖的与模型无关的框架，可以实现在线修改和非顺序生成来增强代码 LLM。 JumpCoder背后的关键思想是在生成过程中必要时将新代码插入到当前生成的代码中，这是通过与代码LLM协同工作的辅助填充模型来实现的。由于事先确定最佳填充位置很困难，因此我们采用先填充，后判断的策略，该策略在每行生成后尝试在 $k$ 个最关键的位置进行填充，并使用抽象语法树（AST）解析器与生成模型评分一起有效地判断每个潜在填充的有效性。使用六个最先进的代码法学硕士在多个基准上进行的广泛实验一致表明，与所有基准相比都有显着改进。值得注意的是，JumpCoder 帮助代码法学硕士在多语言 HumanEval 基准测试中，Python 的 Pass@1 提高了 3.6%，Java 提高了 6.3%，C++ 提高了 3.7%。我们的代码在 https://github.com/Keytoyze/JumpCoder 上公开。</li>
</ul>

<h3>Title: The What, Why, and How of Context Length Extension Techniques in Large  Language Models -- A Detailed Survey</h3>
<ul>
<li><strong>Authors: </strong>Saurav Pawar, S.M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Aman Chadha, Amitava Das</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07872">https://arxiv.org/abs/2401.07872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07872">https://arxiv.org/pdf/2401.07872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07872]] The What, Why, and How of Context Length Extension Techniques in Large  Language Models -- A Detailed Survey(https://arxiv.org/abs/2401.07872)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advent of Large Language Models (LLMs) represents a notable breakthrough in Natural Language Processing (NLP), contributing to substantial progress in both text comprehension and generation. However, amidst these advancements, it is noteworthy that LLMs often face a limitation in terms of context length extrapolation. Understanding and extending the context length for LLMs is crucial in enhancing their performance across various NLP applications. In this survey paper, we delve into the multifaceted aspects of exploring why it is essential, and the potential transformations that superior techniques could bring to NLP applications. We study the inherent challenges associated with extending context length and present an organized overview of the existing strategies employed by researchers. Additionally, we discuss the intricacies of evaluating context extension techniques and highlight the open challenges that researchers face in this domain. Furthermore, we explore whether there is a consensus within the research community regarding evaluation standards and identify areas where further agreement is needed. This comprehensive survey aims to serve as a valuable resource for researchers, guiding them through the nuances of context length extension techniques and fostering discussions on future advancements in this evolving field.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现代表了自然语言处理 (NLP) 领域的显着突破，为文本理解和生成方面的重大进步做出了贡献。然而，在这些进步中，值得注意的是法学硕士经常面临上下文长度外推方面的限制。理解和扩展法学硕士的上下文长度对于提高其在各种 NLP 应用程序中的性能至关重要。在这篇调查论文中，我们深入探讨了其重要性的多方面问题，以及先进技术可能给 NLP 应用带来的潜在转变。我们研究了与延长上下文长度相关的固有挑战，并对研究人员采用的现有策略进行了有组织的概述。此外，我们还讨论了评估上下文扩展技术的复杂性，并强调了研究人员在该领域面临的开放挑战。此外，我们探讨研究界是否就评估标准达成共识，并确定需要进一步达成一致的领域。这项全面的调查旨在为研究人员提供宝贵的资源，指导他们了解上下文长度扩展技术的细微差别，并促进对这个不断发展的领域的未来进展的讨论。</li>
</ul>

<h3>Title: EMBRE: Entity-aware Masking for Biomedical Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Mingjie Li, Karin Verspoor</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07877">https://arxiv.org/abs/2401.07877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07877">https://arxiv.org/pdf/2401.07877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07877]] EMBRE: Entity-aware Masking for Biomedical Relation Extraction(https://arxiv.org/abs/2401.07877)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Information extraction techniques, including named entity recognition (NER) and relation extraction (RE), are crucial in many domains to support making sense of vast amounts of unstructured text data by identifying and connecting relevant information. Such techniques can assist researchers in extracting valuable insights. In this paper, we introduce the Entity-aware Masking for Biomedical Relation Extraction (EMBRE) method for biomedical relation extraction, as applied in the context of the BioRED challenge Task 1, in which human-annotated entities are provided as input. Specifically, we integrate entity knowledge into a deep neural network by pretraining the backbone model with an entity masking objective. We randomly mask named entities for each instance and let the model identify the masked entity along with its type. In this way, the model is capable of learning more specific knowledge and more robust representations. Then, we utilize the pre-trained model as our backbone to encode language representations and feed these representations into two multilayer perceptron (MLPs) to predict the logits for relation and novelty, respectively. The experimental results demonstrate that our proposed method can improve the performances of entity pair, relation and novelty extraction over our baseline.</li>
<li><strong>摘要：</strong>信息提取技术，包括命名实体识别 (NER) 和关系提取 (RE)，在许多领域中至关重要，可通过识别和连接相关信息来支持理解大量非结构化文本数据。此类技术可以帮助研究人员提取有价值的见解。在本文中，我们介绍了用于生物医学关系提取的实体感知屏蔽（EMBRE）方法，该方法应用于 BioRED 挑战任务 1，其中提供人工注释的实体作为输入。具体来说，我们通过使用实体屏蔽目标对骨干模型进行预训练，将实体知识集成到深度神经网络中。我们为每个实例随机屏蔽命名实体，并让模型识别屏蔽实体及其类型。通过这种方式，模型能够学习更具体的知识和更鲁棒的表示。然后，我们利用预先训练的模型作为主干来编码语言表示，并将这些表示输入两个多层感知器（MLP）中，以分别预测关系和新颖性的逻辑。实验结果表明，我们提出的方法可以在我们的基线上提高实体对、关系和新颖性提取的性能。</li>
</ul>

<h3>Title: The Chronicles of RAG: The Retriever, the Chunk and the Generator</h3>
<ul>
<li><strong>Authors: </strong>Paulo Finardi, Leonardo Avila, Rodrigo Castaldoni, Pedro Gengo, Celio Larcher, Marcos Piau, Pablo Costa, Vinicius Caridá</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07883">https://arxiv.org/abs/2401.07883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07883">https://arxiv.org/pdf/2401.07883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07883]] The Chronicles of RAG: The Retriever, the Chunk and the Generator(https://arxiv.org/abs/2401.07883)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, hallucination, retrieval augmented generation, rag</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) has become one of the most popular paradigms for enabling LLMs to access external data, and also as a mechanism for grounding to mitigate against hallucinations. When implementing RAG you can face several challenges like effective integration of retrieval models, efficient representation learning, data diversity, computational efficiency optimization, evaluation, and quality of text generation. Given all these challenges, every day a new technique to improve RAG appears, making it unfeasible to experiment with all combinations for your problem. In this context, this paper presents good practices to implement, optimize, and evaluate RAG for the Brazilian Portuguese language, focusing on the establishment of a simple pipeline for inference and experiments. We explored a diverse set of methods to answer questions about the first Harry Potter book. To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview, gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to the baseline. When optimizing the input size in the application, we observed that it is possible to further enhance it by 2.4%. Finally, we present the complete architecture of the RAG with our recommendations. As result, we moved from a baseline of 57.88% to a maximum relative score of 98.61%.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）已成为使法学硕士能够访问外部数据的最流行的范例之一，同时也是一种减轻幻觉的基础机制。在实施 RAG 时，您可能会面临一些挑战，例如检索模型的有效集成、高效的表示学习、数据多样性、计算效率优化、评估和文本生成的质量。考虑到所有这些挑战，每天都有一种改进 RAG 的新技术出现，这使得针对您的问题尝试所有组合变得不可行。在此背景下，本文提出了针对巴西葡萄牙语实施、优化和评估 RAG 的良好实践，重点是建立一个简单的推理和实验管道。我们探索了多种方法来回答有关第一部《哈利·波特》的问题。为了生成答案，我们使用了 OpenAI 的 gpt-4、gpt-4-1106-preview、gpt-3.5-turbo-1106 和 Google 的 Gemini Pro。着眼于检索器的质量，我们的方法与基线相比，MRR@10 提高了 35.4%。在优化应用程序中的输入大小时，我们观察到可以将其进一步提高 2.4%。最后，我们展示了 RAG 的完整架构以及我们的建议。结果，我们从 57.88% 的基线上升到 98.61% 的最高相对分数。</li>
</ul>

<h3>Title: Learned Best-Effort LLM Serving</h3>
<ul>
<li><strong>Authors: </strong>Siddharth Jha, Coleman Hooper, Xiaoxuan Liu, Sehoon Kim, Kurt Keutzer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07886">https://arxiv.org/abs/2401.07886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07886">https://arxiv.org/pdf/2401.07886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07886]] Learned Best-Effort LLM Serving(https://arxiv.org/abs/2401.07886)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Many applications must provide low-latency LLM service to users or risk unacceptable user experience. However, over-provisioning resources to serve fluctuating request patterns is often prohibitively expensive. In this work, we present a best-effort serving system that employs deep reinforcement learning to adjust service quality based on the task distribution and system load. Our best-effort system can maintain availability with over 10x higher client request rates, serves above 96% of peak performance 4.1x more often, and serves above 98% of peak performance 2.3x more often than static serving on unpredictable workloads. Our learned router is robust to shifts in both the arrival and task distribution. Compared to static serving, learned best-effort serving allows for cost-efficient serving through increased hardware utility. Additionally, we argue that learned best-effort LLM serving is applicable in wide variety of settings and provides application developers great flexibility to meet their specific needs.</li>
<li><strong>摘要：</strong>许多应用程序必须向用户提供低延迟的 LLM 服务，否则可能会带来不可接受的用户体验。然而，过度配置资源来服务波动的请求模式通常成本高昂。在这项工作中，我们提出了一种尽力而为的服务系统，该系统采用深度强化学习来根据任务分配和系统负载来调整服务质量。我们的尽力而为系统可以保持可用性，客户端请求率高出 10 倍以上，提供 96% 以上的峰值性能的频率比在不可预测的工作负载上提供静态服务的频率高 4.1 倍，提供 98% 以上的峰值性能的频率比静态服务高 2.3 倍。我们学习的路由器对于到达和任务分配的变化都很鲁棒。与静态服务相比，学习的尽力而为服务可以通过增加硬件实用性来实现经济高效的服务。此外，我们认为学习的尽力而为的 LLM 服务适用于各种环境，并为应用程序开发人员提供了极大的灵活性来满足他们的特定需求。</li>
</ul>

<h3>Title: The Pitfalls of Defining Hallucination</h3>
<ul>
<li><strong>Authors: </strong>Kees van Deemter</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07897">https://arxiv.org/abs/2401.07897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07897">https://arxiv.org/pdf/2401.07897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07897]] The Pitfalls of Defining Hallucination(https://arxiv.org/abs/2401.07897)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Despite impressive advances in Natural Language Generation (NLG) and Large Language Models (LLMs), researchers are still unclear about important aspects of NLG evaluation. To substantiate this claim, I examine current classifications of hallucination and omission in Data-text NLG, and I propose a logic-based synthesis of these classfications. I conclude by highlighting some remaining limitations of all current thinking about hallucination and by discussing implications for LLMs.</li>
<li><strong>摘要：</strong>尽管自然语言生成 (NLG) 和大型语言模型 (LLM) 取得了令人印象深刻的进步，但研究人员仍然不清楚 NLG 评估的重要方面。为了证实这一说法，我检查了数据文本 NLG 中幻觉和遗漏的当前分类，并提出了这些分类的基于逻辑的综合。最后，我强调了当前关于幻觉的所有想法的一些剩余局限性，并讨论了对法学硕士的影响。</li>
</ul>

<h3>Title: Word Boundary Information Isn't Useful for Encoder Language Models</h3>
<ul>
<li><strong>Authors: </strong>Edward Gow-Smith, Dylan Phelps, Harish Tayyar Madabushi, Carolina Scarton, Aline Villavicencio</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07923">https://arxiv.org/abs/2401.07923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07923">https://arxiv.org/pdf/2401.07923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07923]] Word Boundary Information Isn't Useful for Encoder Language Models(https://arxiv.org/abs/2401.07923)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>All existing transformer-based approaches to NLP using subword tokenisation algorithms encode whitespace (word boundary information) through the use of special space symbols (such as \#\# or \_) forming part of tokens. These symbols have been shown to a) lead to reduced morphological validity of tokenisations, and b) give substantial vocabulary redundancy. As such, removing these symbols has been shown to have a beneficial effect on the processing of morphologically complex words for transformer encoders in the pretrain-finetune paradigm. In this work, we explore whether word boundary information is at all useful to such models. In particular, we train transformer encoders across four different training scales, and investigate several alternative approaches to including word boundary information, evaluating on a range of tasks across different domains and problem set-ups: GLUE (for sentence-level classification), NER (for token-level classification), and two classification datasets involving complex words (Superbizarre and FLOTA). Overall, through an extensive experimental setup that includes the pre-training of 29 models, we find no substantial improvements from our alternative approaches, suggesting that modifying tokenisers to remove word boundary information isn't leading to a loss of useful information.</li>
<li><strong>摘要：</strong>所有现有的基于变压器的 NLP 方法都使用子字标记化算法，通过使用构成标记一部分的特殊空格符号（例如 \#\# 或 \_）来编码空白（字边界信息）。这些符号已被证明：a）导致标记化的形态有效性降低，b）提供大量词汇冗余。因此，删除这些符号已被证明对预训练微调范例中变压器编码器处理形态复杂的单词具有有益的影响。在这项工作中，我们探讨了单词边界信息对于此类模型是否有用。特别是，我们在四种不同的训练尺度上训练 Transformer 编码器，并研究了几种包含单词边界信息的替代方法，评估跨不同领域和问题设置的一系列任务：GLUE（用于句子级分类）、NER（用于令牌级别分类），以及两个涉及复杂单词的分类数据集（Superbizarre 和 FLOTA）。总的来说，通过包括 29 个模型的预训练在内的广泛实验设置，我们发现我们的替代方法没有实质性改进，这表明修改标记器以删除单词边界信息不会导致有用信息的丢失。</li>
</ul>

<h3>Title: Can Large Language Models Explain Themselves?</h3>
<ul>
<li><strong>Authors: </strong>Andreas Madsen, Sarath Chandar, Siva Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07927">https://arxiv.org/abs/2401.07927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07927">https://arxiv.org/pdf/2401.07927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07927]] Can Large Language Models Explain Themselves?(https://arxiv.org/abs/2401.07927)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to three types of self-explanations: counterfactuals, importance measures, and redactions. Our work demonstrate that faithfulness is both task and model dependent, e.g., for sentiment classification, counterfactual explanations are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B. Finally, our findings are robust to prompt-variations.</li>
<li><strong>摘要：</strong>指令调整的大型语言模型 (LLM) 在许多任务上表现出色，甚至可以为其行为提供解释。由于这些模型可以直接向公众开放，因此存在令人信服和错误的解释可能导致对法学硕士的信心不受支持的风险。因此，可解释性——自我解释的可信度是人工智能安全的一个重要考虑因素。评估这些解释（称为自我解释）的可解释性忠实度具有挑战性，因为这些模型太复杂，人类无法注释什么是正确的解释。为了解决这个问题，我们建议采用自我一致性检查作为忠诚度的衡量标准。例如，如果法学硕士说一组单词对于做出预测很重要，那么如果没有这些单词，它应该无法做出相同的预测。虽然自我一致性检查是忠实性的常见方法，但之前并未应用于法学硕士的自我解释。我们对三种类型的自我解释应用自我一致性检查：反事实、重要性度量和修订。我们的工作表明，忠实度既依赖于任务又依赖于模型，例如，对于情感分类，Llama2 的反事实解释更加忠实，Mistral 的重要性度量以及 Falcon 40B 的修订更加忠实。最后，我们的发现对于提示变化是稳健的。</li>
</ul>

<h3>Title: SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT</h3>
<ul>
<li><strong>Authors: </strong>Rupak Kumar Das, Dr. Ted Pedersen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07944">https://arxiv.org/abs/2401.07944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07944">https://arxiv.org/pdf/2401.07944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07944]] SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT(https://arxiv.org/abs/2401.07944)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>This paper uses the BERT model, which is a transformer-based architecture, to solve task 4A, English Language, Sentiment Analysis in Twitter of SemEval2017. BERT is a very powerful large language model for classification tasks when the amount of training data is small. For this experiment, we have used the BERT{\textsubscript{\tiny BASE}} model, which has 12 hidden layers. This model provides better accuracy, precision, recall, and f1 score than the Naive Bayes baseline model. It performs better in binary classification subtasks than the multi-class classification subtasks. We also considered all kinds of ethical issues during this experiment, as Twitter data contains personal and sensible information. The dataset and code used in our experiment can be found in this GitHub repository.</li>
<li><strong>摘要：</strong>本文使用基于 Transformer 的架构 BERT 模型来解决 SemEval2017 的 Twitter 任务 4A、英语语言、情感分析。 BERT 是一种非常强大的大型语言模型，用于训练数据量较小时的分类任务。在本实验中，我们使用了 BERT{\textsubscript{\tiny BASE}} 模型，该模型有 12 个隐藏层。该模型比朴素贝叶斯基线模型提供更好的准确度、精确度、召回率和 f1 分数。它在二元分类子任务中比在多类分类子任务中表现更好。在这个实验中我们还考虑了各种伦理问题，因为 Twitter 数据包含个人和敏感信息。我们实验中使用的数据集和代码可以在此 GitHub 存储库中找到。</li>
</ul>

<h3>Title: SciGLM: Training Scientific Language Models with Self-Reflective  Instruction Annotation and Tuning</h3>
<ul>
<li><strong>Authors: </strong>Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07950">https://arxiv.org/abs/2401.07950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07950">https://arxiv.org/pdf/2401.07950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07950]] SciGLM: Training Scientific Language Models with Self-Reflective  Instruction Annotation and Tuning(https://arxiv.org/abs/2401.07950)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code, chat, rag</a></li>
<li><strong>Abstract: </strong>\label{sec:abstract} Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated SciInstruct, a diverse and high-quality dataset encompassing mathematics, physics, chemistry, and formal proofs. We fine-tuned the ChatGLM family of language models with SciInstruct, enhancing their capabilities in scientific and mathematical reasoning. Remarkably, SciGLM consistently improves both the base model (ChatGLM3-6B-Base) and larger-scale models (12B and 32B), without sacrificing the language understanding capabilities of the base model. This makes SciGLM a suitable foundational model to facilitate diverse scientific discovery tasks. For the benefit of the wider research community, we release SciInstruct, SciGLM, alongside a self-reflective framework and fine-tuning code at \url{https://github.com/THUDM/SciGLM}.</li>
<li><strong>摘要：</strong>\label{sec:abstract} 大型语言模型（LLM）在协助科学发现方面表现出了希望。然而，此类应用目前因法学硕士在理解复杂的科学概念、推导符号方程和解决高级数值计算方面的缺陷而受到限制。为了弥补这些差距，我们引入了 SciGLM，这是一套能够进行大学水平科学推理的科学语言模型。我们方法的核心是一种新颖的自我反思指令注释框架，以解决科学领域的数据稀缺挑战。该框架利用现有的法学硕士为未标记的科学问题生成逐步推理，然后是自我反思的批评和修改过程。应用这个框架，我们策划了 SciInstruct，一个涵盖数学、物理、化学和形式证明的多样化且高质量的数据集。我们使用 SciInstruct 对 ChatGLM 系列语言模型进行了微调，增强了它们的科学和数学推理能力。值得注意的是，SciGLM 不断改进基础模型 (ChatGLM3-6B-Base) 和更大规模的模型（12B 和 32B），而不会牺牲基础模型的语言理解能力。这使得 SciGLM 成为促进各种科学发现任务的合适基础模型。为了更广泛的研究社区的利益，我们在 \url{https://github.com/THUDM/SciGLM} 上发布了 SciInstruct、SciGLM，以及自我反思框架和微调代码。</li>
</ul>

<h3>Title: A Study on Large Language Models' Limitations in Multiple-Choice  Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Aisha Khatun, Daniel G. Brown</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07955">https://arxiv.org/abs/2401.07955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07955">https://arxiv.org/pdf/2401.07955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07955]] A Study on Large Language Models' Limitations in Multiple-Choice  Question Answering(https://arxiv.org/abs/2401.07955)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Large Language Models (LLMs) has become commonplace, particularly with the emergence of open-source models. More importantly, smaller models are well-suited for integration into consumer devices and are frequently employed either as standalone solutions or as subroutines in various AI tasks. Despite their ubiquitous use, there is no systematic analysis of their specific capabilities and limitations. In this study, we tackle one of the most widely used tasks - answering Multiple Choice Question (MCQ). We analyze 26 small open-source models and find that 65% of the models do not understand the task, only 4 models properly select an answer from the given choices, and only 5 of these models are choice order independent. These results are rather alarming given the extensive use of MCQ tests with these models. We recommend exercising caution and testing task understanding before using MCQ to evaluate LLMs in any field whatsoever.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的广泛采用已变得司空见惯，尤其是随着开源模型的出现。更重要的是，较小的模型非常适合集成到消费设备中，并且经常用作独立解决方案或各种人工智能任务中的子例程。尽管它们的使用无处不在，但还没有对其具体功能和局限性进行系统分析。在这项研究中，我们解决最广泛使用的任务之一 - 回答多项选择题 (MCQ)。我们分析了 26 个小型开源模型，发现 65% 的模型不理解任务，只有 4 个模型从给定的选项中正确选择答案，并且其中只有 5 个模型与选择顺序无关。鉴于这些模型广泛使用 MCQ 测试，这些结果相当令人震惊。我们建议在使用 MCQ 评估任何领域的法学硕士之前要谨慎行事并测试对任务的理解。</li>
</ul>

<h3>Title: GD-CAF: Graph Dual-stream Convolutional Attention Fusion for  Precipitation Nowcasting</h3>
<ul>
<li><strong>Authors: </strong>Lorand Vatamany, Siamak Mehrkanoon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07958">https://arxiv.org/abs/2401.07958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07958">https://arxiv.org/pdf/2401.07958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07958]] GD-CAF: Graph Dual-stream Convolutional Attention Fusion for  Precipitation Nowcasting(https://arxiv.org/abs/2401.07958)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Accurate precipitation nowcasting is essential for various purposes, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolutional operations. This enhancement enables the model to directly process the high-dimensional spatiotemporal graph of precipitation maps and exploits higher-order correlations between the data dimensions. We evaluate our model on seven years of precipitation maps across Europe and its neighboring areas collected from the ERA5 dataset, provided by Copernicus. The model receives a fully connected graph in which each node represents historical observations from a specific region on the map. Consequently, each node contains a 3D tensor with time, height, and width dimensions. Experimental results demonstrate that the proposed GD-CAF model outperforms the other examined models. Furthermore, the averaged seasonal spatial and temporal attention scores over the test set are visualized to provide additional insights about the strongest connections between different regions or time steps. These visualizations shed light on the decision-making process of our model.</li>
<li><strong>摘要：</strong>准确的降水临近预报对于洪水预测、灾害管理、优化农业活动、管理运输路线和可再生能源等多种目的至关重要。虽然一些研究从序列到序列的角度解决了这一具有挑战性的任务，但大多数研究都集中在单个区域，而没有考虑多个不相交区域之间现有的相关性。在本文中，我们将降水临近预报表述为时空图序列临近预报问题。特别是，我们引入了图双流卷积注意力融合（GD-CAF），这是一种新颖的方法，旨在从降水图的历史时空图中学习，并预测不同空间位置的未来时间步长降水。 GD-CAF 由时空卷积注意力以及配备深度可分离卷积运算的门控融合模块组成。这一增强功能使模型能够直接处理降水图的高维时空图，并利用数据维度之间的高阶相关性。我们根据哥白尼提供的 ERA5 数据集收集的欧洲及其邻近地区七年降水图来评估我们的模型。该模型接收一个完全连接的图，其中每个节点代表地图上特定区域的历史观测结果。因此，每个节点都包含一个具有时间、高度和宽度维度的 3D 张量。实验结果表明，所提出的 GD-CAF 模型优于其他检查模型。此外，测试集上的平均季节性空间和时间注意力分数被可视化，以提供有关不同区域或时间步之间最强联系的额外见解。这些可视化揭示了我们模型的决策过程。</li>
</ul>

<h3>Title: AI-as-exploration: Navigating intelligence space</h3>
<ul>
<li><strong>Authors: </strong>Dimitri Coelho Mollo</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07964">https://arxiv.org/abs/2401.07964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07964">https://arxiv.org/pdf/2401.07964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07964]] AI-as-exploration: Navigating intelligence space(https://arxiv.org/abs/2401.07964)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence is a field that lives many lives, and the term has come to encompass a motley collection of scientific and commercial endeavours. In this paper, I articulate the contours of a rather neglected but central scientific role that AI has to play, which I dub `AI-as-exploration'.The basic thrust of AI-as-exploration is that of creating and studying systems that can reveal candidate building blocks of intelligence that may differ from the forms of human and animal intelligence we are familiar with. In other words, I suggest that AI is one of the best tools we have for exploring intelligence space, namely the space of possible intelligent systems. I illustrate the value of AI-as-exploration by focusing on a specific case study, i.e., recent work on the capacity to combine novel and invented concepts in humans and Large Language Models. I show that the latter, despite showing human-level accuracy in such a task, most probably solve it in ways radically different, but no less relevant to intelligence research, to those hypothesised for humans.</li>
<li><strong>摘要：</strong>人工智能是一个与许多人息息相关的领域，这个术语已经涵盖了各种科学和商业努力的集合。在本文中，我阐述了人工智能所扮演的一个相当被忽视但核心的科学角色的轮廓，我将其称为“人工智能探索”。人工智能探索的基本主旨是创建和研究系统可以揭示可能与我们熟悉的人类和动物智力形式不同的候选智力构建模块。换句话说，我认为人工智能是我们探索智能空间（即可能的智能系统空间）的最佳工具之一。我通过关注一个特定的案例研究来说明人工智能作为探索的价值，即最近关于将人类和大型语言模型中的新颖和发明的概念相结合的能力的工作。我表明，后者尽管在此类任务中表现出人类水平的准确性，但很可能以与人类假设的方式截然不同的方式解决它，但与情报研究的相关性丝毫不减。</li>
</ul>

<h3>Title: Leveraging External Knowledge Resources to Enable Domain-Specific  Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Sengupta, Connor Heaton, Prasenjit Mitra, Soumalya Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07977">https://arxiv.org/abs/2401.07977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07977">https://arxiv.org/pdf/2401.07977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07977]] Leveraging External Knowledge Resources to Enable Domain-Specific  Comprehension(https://arxiv.org/abs/2401.07977)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, rag</a></li>
<li><strong>Abstract: </strong>Machine Reading Comprehension (MRC) has been a long-standing problem in NLP and, with the recent introduction of the BERT family of transformer based language models, it has come a long way to getting solved. Unfortunately, however, when BERT variants trained on general text corpora are applied to domain-specific text, their performance inevitably degrades on account of the domain shift i.e. genre/subject matter discrepancy between the training and downstream application data. Knowledge graphs act as reservoirs for either open or closed domain information and prior studies have shown that they can be used to improve the performance of general-purpose transformers in domain-specific applications. Building on existing work, we introduce a method using Multi-Layer Perceptrons (MLPs) for aligning and integrating embeddings extracted from knowledge graphs with the embeddings spaces of pre-trained language models (LMs). We fuse the aligned embeddings with open-domain LMs BERT and RoBERTa, and fine-tune them for two MRC tasks namely span detection (COVID-QA) and multiple-choice questions (PubMedQA). On the COVID-QA dataset, we see that our approach allows these models to perform similar to their domain-specific counterparts, Bio/Sci-BERT, as evidenced by the Exact Match (EM) metric. With regards to PubMedQA, we observe an overall improvement in accuracy while the F1 stays relatively the same over the domain-specific models.</li>
<li><strong>摘要：</strong>机器阅读理解 (MRC) 一直是 NLP 中长期存在的问题，随着最近推出的基于 Transformer 的 BERT 系列语言模型，该问题的解决已经取得了长足的进步。然而不幸的是，当在一般文本语料库上训练的 BERT 变体应用于特定领域的文本时，由于领域的转变，即训练和下游应用数据之间的流派/主题差异，它们的性能不可避免地会下降。知识图充当开放或封闭域信息的存储库，先前的研究表明它们可用于提高通用变压器在特定领域应用中的性能。在现有工作的基础上，我们引入了一种使用多层感知器（MLP）的方法，将从知识图谱中提取的嵌入与预训练语言模型（LM）的嵌入空间进行对齐和集成。我们将对齐的嵌入与开放域 LM BERT 和 RoBERTa 融合，并针对两个 MRC 任务（即跨度检测 (COVID-QA) 和多项选择问题 (PubMedQA)）对其进行微调。在 COVID-QA 数据集上，我们看到我们的方法使这些模型的表现与其特定领域的对应模型 Bio/Sci-BERT 类似，正如精确匹配 (EM) 指标所证明的那样。关于 PubMedQA，我们观察到准确性的整体提高，而 F1 与特定领域模型相比保持相对相同。</li>
</ul>

<h3>Title: Carrying over algorithm in transformers</h3>
<ul>
<li><strong>Authors: </strong>Jorrit Kruthoff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.07993">https://arxiv.org/abs/2401.07993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.07993">https://arxiv.org/pdf/2401.07993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.07993]] Carrying over algorithm in transformers(https://arxiv.org/abs/2401.07993)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code</a></li>
<li><strong>Abstract: </strong>Addition is perhaps one of the simplest arithmetic tasks one can think of and is usually performed using the carrying over algorithm. This algorithm consists of two tasks: adding digits in the same position and carrying over a one whenever necessary. We study how transformer models implement this algorithm and how the two aforementioned tasks are allocated to different parts of the network. We first focus on two-layer encoder-only models and show that the carrying over algorithm is implemented in a modular fashion. The first layer is mostly responsible for adding digits in the same position. The second layer first decides, in the attention, which positions need a carried one or not, and then performs the carrying of the one in the final MLP. We provide a simple way of precisely identifying which neurons are responsible for that task. This implementation of the carrying over algorithm occurs across a range of hyperparameters for two as well as three-layer models. For small decoder-only models, we observe the same implementation and provide suggestive evidence for its existence in three 7B large language models.</li>
<li><strong>摘要：</strong>加法可能是人们能想到的最简单的算术任务之一，通常使用结转算法来执行。该算法由两个任务组成：在相同位置添加数字并在必要时保留一位。我们研究变压器模型如何实现该算法以及如何将上述两个任务分配给网络的不同部分。我们首先关注仅两层编码器模型，并表明结转算法是以模块化方式实现的。第一层主要负责在相同位置添加数字。第二层首先决定attention中哪些位置需要携带1，然后在最终的MLP中进行携带。我们提供了一种简单的方法来精确识别哪些神经元负责该任务。结转算法的这种实现发生在两层和三层模型的一系列超参数上。对于小型仅解码器模型，我们观察到相同的实现，并为其在三个 7B 大型语言模型中的存在提供了暗示性证据。</li>
</ul>

<h3>Title: Contextual Bandits with Stage-wise Constraints</h3>
<ul>
<li><strong>Authors: </strong>Aldo Pacchiano, Mohammad Ghavamzadeh, Peter Bartlett</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08016">https://arxiv.org/abs/2401.08016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08016">https://arxiv.org/pdf/2401.08016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08016]] Contextual Bandits with Stage-wise Constraints(https://arxiv.org/abs/2401.08016)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>We study contextual bandits in the presence of a stage-wise constraint (a constraint at each round), when the constraint must be satisfied both with high probability and in expectation. Obviously the setting where the constraint is in expectation is a relaxation of the one with high probability. We start with the linear case where both the contextual bandit problem (reward function) and the stage-wise constraint (cost function) are linear. In each of the high probability and in expectation settings, we propose an upper-confidence bound algorithm for the problem and prove a $T$-round regret bound for it. Our algorithms balance exploration and constraint satisfaction using a novel idea that scales the radii of the reward and cost confidence sets with different scaling factors. We also prove a lower-bound for this constrained problem, show how our algorithms and analyses can be extended to multiple constraints, and provide simulations to validate our theoretical results. In the high probability setting, we describe the minimum requirements for the action set in order for our algorithm to be tractable. In the setting that the constraint is in expectation, we further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting with regret analysis. Finally, we extend our results to the case where the reward and cost functions are both non-linear. We propose an algorithm for this case and prove a regret bound for it that characterize the function class complexity by the eluder dimension.</li>
<li><strong>摘要：</strong>我们在存在阶段性约束（每轮都有一个约束）的情况下研究上下文老虎机，此时约束必须以高概率和期望得到满足。显然，期望约束的设置是对高概率约束的放松。我们从线性情况开始，其中上下文老虎机问题（奖励函数）和阶段约束（成本函数）都是线性的。在每个高概率和期望设置中，我们为该问题提出了一个置信上限算法，并证明了它的 $T$ 轮后悔界限。我们的算法使用一种新颖的想法来平衡探索和约束满足，该想法用不同的缩放因子缩放奖励和成本置信集的半径。我们还证明了这个约束问题的下界，展示了我们的算法和分析如何扩展到多个约束，并提供模拟来验证我们的理论结果。在高概率设置中，我们描述了操作集的最低要求，以便我们的算法易于处理。在约束符合预期的情况下，我们进一步将结果专门化为多臂老虎机，并针对此设置提出了一种具有遗憾分析的计算有效的算法。最后，我们将结果扩展到奖励函数和成本函数都是非线性的情况。我们针对这种情况提出了一种算法，并证明了它的遗憾界限，该遗憾界限通过逃避维数来表征函数类复杂性。</li>
</ul>

<h3>Title: Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using  Self-Imagination</h3>
<ul>
<li><strong>Authors: </strong>Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, Eric Nyberg</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08025">https://arxiv.org/abs/2401.08025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08025">https://arxiv.org/pdf/2401.08025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08025]] Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using  Self-Imagination(https://arxiv.org/abs/2401.08025)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, rag</a></li>
<li><strong>Abstract: </strong>The potential of Vision-Language Models (\textsc{vlm}s) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose \textsc{Self-Imagine}. We leverage a single Vision-Language Model (\textsc{vlm}) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same \vlm to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach in three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art \textsc{vlm}. Our approach boosts the performance of \textsc{vlm} on all math tasks (\gsm: +4.62\%; \asdiv: +4.49\%; \svamp: +9.30\%) and the majority of the general-purpose reasoning tasks by 0.4\% to 13.20\% while achieving comparable performance in other tasks. Code and data at https://github.com/snat1505027/self-imagine .</li>
<li><strong>摘要：</strong>在处理复杂的基于文本的问题时，视觉语言模型（\textsc{vlm}s）的潜力通常仍未得到充分利用，特别是当这些问题可以从视觉表示中受益时。为了与人类解决复杂的基于文本的问题的能力产生共鸣，我们提出了 \textsc{Self-Imagine}，通过（1）根据问题创建可视化图表并（2）推断他们需要采取哪些步骤来解决问题。我们利用单一视觉语言模型 (\textsc{vlm}) 使用 HTML 生成问题的结构化表示，然后将 HTML 渲染为图像，最后使用相同的 \vlm 使用问题和问题来回答问题图片。我们的方法不需要任何额外的训练数据或培训。我们使用最先进的 \textsc{vlm} 评估我们在三个数学任务和九个通用推理任务中的方法。我们的方法提高了 \textsc{vlm} 在所有数学任务（\gsm：+4.62\%；\asdiv：+4.49\%；\svamp：+9.30\%）和大多数通用推理任务上的性能0.4\% 到 13.20\%，同时在其他任务中实现类似的性能。代码和数据位于 https://github.com/snat1505027/self-imagine 。</li>
</ul>

<h3>Title: Enhancing Robustness of LLM-Synthetic Text Detectors for Academic  Writing: A Comprehensive Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhicheng Dou, Yuchen Guo, Ching-Chun Chang, Huy H. Nguyen, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08046">https://arxiv.org/abs/2401.08046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08046">https://arxiv.org/pdf/2401.08046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08046]] Enhancing Robustness of LLM-Synthetic Text Detectors for Academic  Writing: A Comprehensive Analysis(https://arxiv.org/abs/2401.08046)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>The emergence of large language models (LLMs), such as Generative Pre-trained Transformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and broader community. While these models offer numerous advantages in terms of revolutionizing work and study methods, they have also garnered significant attention due to their potential negative consequences. One example is generating academic reports or papers with little to no human contribution. Consequently, researchers have focused on developing detectors to address the misuse of LLMs. However, most existing methods prioritize achieving higher accuracy on restricted datasets, neglecting the crucial aspect of generalizability. This limitation hinders their practical application in real-life scenarios where reliability is paramount. In this paper, we present a comprehensive analysis of the impact of prompts on the text generated by LLMs and highlight the potential lack of robustness in one of the current state-of-the-art GPT detectors. To mitigate these issues concerning the misuse of LLMs in academic writing, we propose a reference-based Siamese detector named Synthetic-Siamese which takes a pair of texts, one as the inquiry and the other as the reference. Our method effectively addresses the lack of robustness of previous detectors (OpenAI detector and DetectGPT) and significantly improves the baseline performances in realistic academic writing scenarios by approximately 67% to 95%.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的出现，例如 ChatGPT 使用的生成式预训练 Transformer 4 (GPT-4)，对学术界和更广泛的社区产生了深远的影响。虽然这些模式在彻底改变工作和学习方法方面具有许多优势，但由于其潜在的负面后果，它们也引起了极大的关注。一个例子是在几乎没有人类贡献的情况下生成学术报告或论文。因此，研究人员专注于开发探测器来解决法学硕士的滥用问题。然而，大多数现有方法优先考虑在受限数据集上实现更高的准确性，而忽略了普遍性的关键方面。这种限制阻碍了它们在可靠性至关重要的现实生活场景中的实际应用。在本文中，我们对提示对法学硕士生成的文本的影响进行了全面分析，并强调了当前最先进的 GPT 检测器之一可能缺乏鲁棒性。为了缓解学术写作中滥用法学硕士的问题，我们提出了一种基于参考的连体检测器，名为 Synthetic-Siamese，它采用一对文本，一个作为查询，另一个作为参考。我们的方法有效解决了先前检测器（OpenAI 检测器和 DetectGPT）鲁棒性不足的问题，并将现实学术写作场景中的基线性能显着提高了约 67% 至 95%。</li>
</ul>

<h3>Title: Enhancing Document-level Translation of Large Language Model via  Translation Mixed-instructions</h3>
<ul>
<li><strong>Authors: </strong>Yachao Li, Junhui Li, Jing Jiang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08088">https://arxiv.org/abs/2401.08088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08088">https://arxiv.org/pdf/2401.08088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08088]] Enhancing Document-level Translation of Large Language Model via  Translation Mixed-instructions(https://arxiv.org/abs/2401.08088)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>Existing large language models (LLMs) for machine translation are typically fine-tuned on sentence-level translation instructions and achieve satisfactory performance at the sentence level. However, when applied to document-level translation, these models face a significant challenge, particularly when dealing with documents containing over 512 tokens. This challenge arises from the issue of sentence-level coverage, where subsequent sentences in the document remain untranslated. As a result, the document-level translation capability of LLMs fine-tuned on sentence-level translation instructions is significantly limited. We conjecture that the primary cause of LLMs' weak document-level translation performance is the absence of document-to-document mapping ability. To address the issue, we propose an approach that combines sentence-level and document-level translation instructions of varying lengths to fine-tune LLMs. Our proposed translation mixed-instructions enable LLMs (Llama-2~7B and 13B) to maintain consistent translation performance from the sentence level to documents containing as many as 2048 tokens. Extensive experimental results show that the proposed approach significantly enhances the document-level translation capabilities of LLMs on 10 language pairs, effectively mitigating the sentence-level coverage issue in document-level translation. Experimentation on discourse phenomena has demonstrated that our document-level translation approach significantly improves translation quality, both in terms of BLEU score and discourse coherence.</li>
<li><strong>摘要：</strong>现有的机器翻译大型语言模型（LLM）通常在句子级翻译指令上进行微调，并在句子级获得令人满意的性能。然而，当应用于文档级翻译时，这些模型面临着重大挑战，特别是在处理包含超过 512 个标记的文档时。这一挑战源于句子级覆盖问题，文档中的后续句子仍未翻译。因此，根据句子级翻译指令进行微调的法学硕士的文档级翻译能力受到极大限制。我们推测法学硕士文档级翻译性能较差的主要原因是缺乏文档到文档的映射能力。为了解决这个问题，我们提出了一种结合不同长度的句子级和文档级翻译指令来微调法学硕士的方法。我们提出的翻译混合指令使 LLM（Llama-2~7B 和 13B）能够从句子级别到包含多达 2048 个标记的文档保持一致的翻译性能。大量实验结果表明，该方法显着增强了法学硕士在10个语言对上的文档级翻译能力，有效缓解了文档级翻译中的句子级覆盖问题。对话语现象的实验表明，我们的文档级翻译方法在 BLEU 分数和话语连贯性方面显着提高了翻译质量。</li>
</ul>

<h3>Title: A Study on Training and Developing Large Language Models for Behavior  Tree Generation</h3>
<ul>
<li><strong>Authors: </strong>Fu Li, Xueying Wang, Bin Li, Yunlong Wu, Yanzhen Wang, Xiaodong Yi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08089">https://arxiv.org/abs/2401.08089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08089">https://arxiv.org/pdf/2401.08089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08089]] A Study on Training and Developing Large Language Models for Behavior  Tree Generation(https://arxiv.org/abs/2401.08089)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, lora, rag, agent</a></li>
<li><strong>Abstract: </strong>This paper presents an innovative exploration of the application potential of large language models (LLM) in addressing the challenging task of automatically generating behavior trees (BTs) for complex tasks. The conventional manual BT generation method is inefficient and heavily reliant on domain expertise. On the other hand, existing automatic BT generation technologies encounter bottlenecks related to task complexity, model adaptability, and reliability. In order to overcome these challenges, we propose a novel methodology that leverages the robust representation and reasoning abilities of LLMs. The core contribution of this paper lies in the design of a BT generation framework based on LLM, which encompasses the entire process, from data synthesis and model training to application developing and data verification. Synthetic data is introduced to train the BT generation model (BTGen model), enhancing its understanding and adaptability to various complex tasks, thereby significantly improving its overall performance. In order to ensure the effectiveness and executability of the generated BTs, we emphasize the importance of data verification and introduce a multilevel verification strategy. Additionally, we explore a range of agent design and development schemes with LLM as the central element. We hope that the work in this paper may provide a reference for the researchers who are interested in BT generation based on LLMs.</li>
<li><strong>摘要：</strong>本文对大型语言模型（LLM）的应用潜力进行了创新探索，以解决为复杂任务自动生成行为树（BT）的挑战性任务。传统的手动 BT 生成方法效率低下，并且严重依赖领域专业知识。另一方面，现有的BT自动生成技术遇到了任务复杂性、模型适应性和可靠性等瓶颈。为了克服这些挑战，我们提出了一种利用法学硕士强大的表征和推理能力的新颖方法。本文的核心贡献在于设计了一个基于LLM的BT生成框架，涵盖了从数据合成、模型训练到应用开发和数据验证的整个过程。引入合成数据来训练BT生成模型（BTGen模型），增强其对各种复杂任务的理解和适应能力，从而显着提高其整体性能。为了保证生成的BT的有效性和可执行性，我们强调数据验证的重要性，并引入多级验证策略。此外，我们还探索了一系列以法学硕士为核心要素的代理设计和开发方案。我们希望本文的工作可以为对基于法学硕士的BT生成感兴趣的研究人员提供参考。</li>
</ul>

<h3>Title: A Survey of Resource-efficient LLM and Multimodal Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, Xuanzhe Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08092">https://arxiv.org/abs/2401.08092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08092">https://arxiv.org/pdf/2401.08092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08092]] A Survey of Resource-efficient LLM and Multimodal Foundation Models(https://arxiv.org/abs/2401.08092)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.</li>
<li><strong>摘要：</strong>大型基础模型，包括大型语言模型 (LLM)、视觉转换器 (ViT)、扩散和基于 LLM 的多模态模型，正在彻底改变从训练到部署的整个机器学习生命周期。然而，这些模型在多功能性和性能方面的巨大进步是以硬件资源方面的巨大成本为代价的。为了以可扩展和环境可持续的方式支持这些大型模型的发展，人们非常重视制定资源节约型战略。这项调查深入探讨了此类研究的至关重要性，检查了算法和系统方面。它提供了从现有文献中收集的全面分析和有价值的见解，涵盖从尖端模型架构和训练/服务算法到实际系统设计和实现的广泛主题。这项调查的目的是全面了解当前的方法如何应对大型基础模型带来的资源挑战，并有可能激发该领域的未来突破。</li>
</ul>

<h3>Title: SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic  Spatio-Temporal Traffic Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Lequan Lin, Dai Shi, Andi Han, Junbin Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08119">https://arxiv.org/abs/2401.08119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08119">https://arxiv.org/pdf/2401.08119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08119]] SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic  Spatio-Temporal Traffic Forecasting(https://arxiv.org/abs/2401.08119)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Traffic forecasting, a crucial application of spatio-temporal graph (STG) learning, has traditionally relied on deterministic models for accurate point estimations. Yet, these models fall short of identifying latent risks of unexpected volatility in future observations. To address this gap, probabilistic methods, especially variants of diffusion models, have emerged as uncertainty-aware solutions. However, existing diffusion methods typically focus on generating separate future time series for individual sensors in the traffic network, resulting in insufficient involvement of spatial network characteristics in the probabilistic learning process. To better leverage spatial dependencies and systematic patterns inherent in traffic data, we propose SpecSTG, a novel spectral diffusion framework. Our method generates the Fourier representation of future time series, transforming the learning process into the spectral domain enriched with spatial information. Additionally, our approach incorporates a fast spectral graph convolution designed for Fourier input, alleviating the computational burden associated with existing models. Numerical experiments show that SpecSTG achieves outstanding performance with traffic flow and traffic speed datasets compared to state-of-the-art baselines. The source code for SpecSTG is available at https://anonymous.4open.science/r/SpecSTG.</li>
<li><strong>摘要：</strong>交通预测是时空图（STG）学习的一个重要应用，传统上依赖于确定性模型来进行准确的点估计。然而，这些模型无法识别未来观察中意外波动的潜在风险。为了解决这一差距，概率方法，特别是扩散模型的变体，已成为不确定性感知的解决方案。然而，现有的扩散方法通常侧重于为交通网络中的各个传感器生成单独的未来时间序列，导致概率学习过程中空间网络特征的参与不足。为了更好地利用交通数据固有的空间依赖性和系统模式，我们提出了 SpecSTG，一种新颖的光谱扩散框架。我们的方法生成未来时间序列的傅立叶表示，将学习过程转换为富含空间信息的谱域。此外，我们的方法结合了专为傅立叶输入设计的快速谱图卷积，减轻了与现有模型相关的计算负担。数值实验表明，与最先进的基线相比，SpecSTG 在交通流量和交通速度数据集上实现了出色的性能。 SpecSTG 的源代码可从 https://anonymous.4open.science/r/SpecSTG 获取。</li>
</ul>

<h3>Title: CycLight: learning traffic signal cooperation with a cycle-level  strategy</h3>
<ul>
<li><strong>Authors: </strong>Gengyue Han, Xiaohan Liu, Xianyue Peng, Hao Wang, Yu Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08121">https://arxiv.org/abs/2401.08121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08121">https://arxiv.org/pdf/2401.08121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08121]] CycLight: learning traffic signal cooperation with a cycle-level  strategy(https://arxiv.org/abs/2401.08121)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This study introduces CycLight, a novel cycle-level deep reinforcement learning (RL) approach for network-level adaptive traffic signal control (NATSC) systems. Unlike most traditional RL-based traffic controllers that focus on step-by-step decision making, CycLight adopts a cycle-level strategy, optimizing cycle length and splits simultaneously using Parameterized Deep Q-Networks (PDQN) algorithm. This cycle-level approach effectively reduces the computational burden associated with frequent data communication, meanwhile enhancing the practicality and safety of real-world applications. A decentralized framework is formulated for multi-agent cooperation, while attention mechanism is integrated to accurately assess the impact of the surroundings on the current intersection. CycLight is tested in a large synthetic traffic grid using the microscopic traffic simulation tool, SUMO. Experimental results not only demonstrate the superiority of CycLight over other state-of-the-art approaches but also showcase its robustness against information transmission delays.</li>
<li><strong>摘要：</strong>本研究介绍了 CycLight，这是一种用于网络级自适应交通信号控制 (NATSC) 系统的新型循环级深度强化学习 (RL) 方法。与大多数传统的基于强化学习的交通控制器专注于逐步决策不同，CycLight 采用循环级策略，使用参数化深度 Q 网络 (PDQN) 算法同时优化循环长度和分割。这种循环级方法有效减少了与频繁数据通信相关的计算负担，同时增强了实际应用的实用性和安全性。制定了多智能体合作的去中心化框架，同时集成了注意力机制，以准确评估周围环境对当前路口的影响。 CycLight 使用微观交通模拟工具 SUMO 在大型合成交通网格中进行了测试。实验结果不仅证明了 CycLight 相对于其他最先进方法的优越性，而且还展示了其对信息传输延迟的鲁棒性。</li>
</ul>

<h3>Title: Transferring Core Knowledge via Learngenes</h3>
<ul>
<li><strong>Authors: </strong>Fu Feng, Jing Wang, Xin Geng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08139">https://arxiv.org/abs/2401.08139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08139">https://arxiv.org/pdf/2401.08139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08139]] Transferring Core Knowledge via Learngenes(https://arxiv.org/abs/2401.08139)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>The pre-training paradigm fine-tunes the models trained on large-scale datasets to downstream tasks with enhanced performance. It transfers all knowledge to downstream tasks without discriminating which part is necessary or unnecessary, which may lead to negative transfer. In comparison, knowledge transfer in nature is much more efficient. When passing genetic information to descendants, ancestors encode only the essential knowledge into genes, which act as the medium. Inspired by that, we adopt a recent concept called ``learngene'' and refine its structures by mimicking the structures of natural genes. We propose the Genetic Transfer Learning (GTL) -- a framework to copy the evolutionary process of organisms into neural networks. GTL trains a population of networks, selects superior learngenes by tournaments, performs learngene mutations, and passes the learngenes to next generations. Finally, we successfully extract the learngenes of VGG11 and ResNet12. We show that the learngenes bring the descendant networks instincts and strong learning ability: with 20% parameters, the learngenes bring 12% and 16% improvements of accuracy on CIFAR-FS and miniImageNet. Besides, the learngenes have the scalability and adaptability on the downstream structure of networks and datasets. Overall, we offer a novel insight that transferring core knowledge via learngenes may be sufficient and efficient for neural networks.</li>
<li><strong>摘要：</strong>预训练范式将在大规模数据集上训练的模型微调到具有增强性能的下游任务。它将所有知识转移到下游任务，而不区分哪一部分是必要的或不必要的，这可能会导致负转移。相比之下，自然界的知识转移效率要高得多。当将遗传信息传递给后代时，祖先仅将基本知识编码到基因中，基因充当媒介。受此启发，我们采用了一个名为“学习基因”的最新概念，并通过模仿自然基因的结构来完善其结构。我们提出了遗传迁移学习（GTL）——一个将生物体进化过程复制到神经网络中的框架。 GTL 训练网络群体，通过锦标赛选择优秀的学习基因，执行学习基因突变，并将学习基因传递给下一代。最后，我们成功提取了VGG11和ResNet12的学习基因。我们表明，learngenes 带来了后代网络的本能和强大的学习能力：在参数为 20% 的情况下，learngenes 在 CIFAR-FS 和 miniImageNet 上带来了 12% 和 16% 的精度提高。此外，学习基因对网络和数据集的下游结构具有可扩展性和适应性。总的来说，我们提供了一种新颖的见解，即通过学习基因转移核心知识对于神经网络来说可能是足够且有效的。</li>
</ul>

<h3>Title: PRewrite: Prompt Rewriting with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Weize Kong, Spurthi Amba Hombaiah, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08189">https://arxiv.org/abs/2401.08189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08189">https://arxiv.org/pdf/2401.08189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08189]] PRewrite: Prompt Rewriting with Reinforcement Learning(https://arxiv.org/abs/2401.08189)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt, rag</a></li>
<li><strong>Abstract: </strong>Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications? To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated tool leverages manually crafted prompts as starting points which makes the rewriting procedure more guided and efficient. The generated prompts are human readable, and self-explanatory, unlike some of those in previous works. We conducted extensive experiments on diverse datasets and found that the prompts generated with this new method not only outperform professionally crafted prompts, but also prompts generated with other previously proposed methods.</li>
<li><strong>摘要：</strong>快速工程对于基于法学硕士的应用程序的开发至关重要。然而，它通常是以“反复试验”的方式手动完成的。此手动过程可能非常耗时、低效，并且在很多情况下生成的提示并不是最佳的。即使对于看起来效果不错的提示，也始终存在一个挥之不去的问题：进一步修改是否可以使提示变得更好？为了解决这些问题，在本文中，我们研究了即时工程自动化。我们考虑一个特定的用例场景，其中开发人员/用户起草了初始提示，但缺乏时间/专业知识来优化它们。我们建议使用 PRewrite，这是一种自动化工具，可以重写这些草稿并生成高效的新提示。 PRewrite 基于强化学习 (RL) 框架，该框架允许端到端优化，并且我们的设计允许 RL 搜索在较大的动作空间中进行。该自动化工具利用手动制作的提示作为起点，使重写过程更加有指导性和效率。与之前作品中的一些提示不同，生成的提示是人类可读的且不言自明的。我们对不同的数据集进行了广泛的实验，发现用这种新方法生成的提示不仅优于专业制作的提示，而且优于用其他先前提出的方法生成的提示。</li>
</ul>

<h3>Title: MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible  Pipeline</h3>
<ul>
<li><strong>Authors: </strong>Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, Kai Fan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08190">https://arxiv.org/abs/2401.08190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08190">https://arxiv.org/pdf/2401.08190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08190]] MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible  Pipeline(https://arxiv.org/abs/2401.08190)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have seen considerable advancements in natural language understanding tasks, yet there remains a gap to bridge before attaining true artificial general intelligence, especially concerning shortcomings in mathematical reasoning capabilities. We postulate that the inherent nature of LLM training, which focuses on predicting probabilities of next token, presents challenges in effectively modeling mathematical reasoning that demands exact calculations, both from data-driven and theoretical standpoints. In this paper, we address this challenge by enriching the data landscape and introducing a novel math dataset, enhanced with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT-4 annotations, human review, and self-training processes, where the errors in the original GSM8K training set have been fixed. Additionally, we propose a tentative, easily replicable protocol for the fine-tuning of math-specific LLMs, which has led to a significant improvement in the performance of a 7B-parameter LLM on the GSM8K and MATH datasets. We are committed to advancing the field of mathematical reasoning in LLMs and, to that end, we have made the model checkpoints and will make the dataset publicly available. We hope this will facilitate further research and development within the community.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）在自然语言理解任务方面取得了相当大的进步，但在实现真正的通用人工智能之前仍然存在差距，特别是在数学推理能力方面的缺陷。我们假设 LLM 训练的本质是预测下一个代币的概率，这在有效建模数学推理方面提出了挑战，无论是从数据驱动还是理论的角度来看，都需要精确的计算。在本文中，我们通过丰富数据景观并引入新颖的数学数据集来应对这一挑战，并通过利用 Python 代码解释器的功能进行增强。该数据集源自 GSM8K 和 MATH，并通过 GPT-4 注释、人工审查和自我训练过程的结合进一步完善，其中原始 GSM8K 训练集中的错误已得到修复。此外，我们提出了一种试验性的、易于复制的协议，用于微调特定于数学的 LLM，这使得 7B 参数 LLM 在 GSM8K 和 MATH 数据集上的性能得到显着提高。我们致力于推进法学硕士数学推理领域的发展，为此，我们制定了模型检查点并将公开数据集。我们希望这将促进社区内进一步的研究和开发。</li>
</ul>

<h3>Title: Towards Causal Relationship in Indefinite Data: Baseline Model and New  Datasets</h3>
<ul>
<li><strong>Authors: </strong>Hang Chen, Xinyu Yang, Keqing Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08221">https://arxiv.org/abs/2401.08221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08221">https://arxiv.org/pdf/2401.08221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08221]] Towards Causal Relationship in Indefinite Data: Baseline Model and New  Datasets(https://arxiv.org/abs/2401.08221)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Integrating deep learning and causal discovery has encouraged us to spot that learning causal structures and representations in dialogue and video is full of challenges. We defined These data forms as "Indefinite Data", characterized by multi-structure data and multi-value representations. Unlike existing adaptable data forms, Indefinite Data still faces gaps in datasets and methods. To address the dataset gap, we release two high-quality datasets - Causalogue and Causaction, containing text dialogue samples and video action samples with causal annotations respectively. Moreover, the method gap arises from the coexistence of multi-structure data and multi-value representations, breaking the assumptions of all current methods and rendering them infeasible on Indefinite Data. To this end, we propose a probabilistic framework as a baseline, incorporating three designed highlights for this gap: 1) establishing Causation Condition of representations using the independence of noise terms under non-fixed causal structures, 2) treating causal strength as a latent variable and measuring the reconstruction loss in the correlation space, and 3) estimating the effects of latent confounders. These highpoints make the probabilistic model capable of overcoming challenges brought by the coexistence of multi-structure data and multi-value representations and pave the way for the extension of latent confounders. Comprehensive experiments have evaluated baseline results of causal structures, causal representations, and confounding disentanglement.</li>
<li><strong>摘要：</strong>将深度学习和因果发现相结合，让我们发现学习对话和视频中的因果结构和表征充满了挑战。我们将这些数据形式定义为“不定数据”，其特点是多结构数据和多值表示。与现有的适应性数据形式不同，无限数据仍然面临数据集和方法方面的差距。为了解决数据集差距，我们发布了两个高质量数据集 - Causalogue 和 Causaction，分别包含带有因果注释的文本对话样本和视频动作样本。此外，方法差距是由于多结构数据和多值表示的共存而产生的，打破了所有当前方法的假设，并使它们在不定数据上不可行。为此，我们提出了一个概率框架作为基线，结合了针对这一差距的三个设计亮点：1）使用非固定因果结构下噪声项的独立性建立表示的因果条件，2）将因果强度视为潜在变量测量相关空间中的重建损失，3) 估计潜在混杂因素的影响。这些亮点使得概率模型能够克服多结构数据和多值表示共存带来的挑战，并为潜在混杂因素的扩展铺平道路。综合实验评估了因果结构、因果表征和混杂解理的基线结果。</li>
</ul>

<h3>Title: Efficient and Mathematically Robust Operations for Certified Neural  Networks Inference</h3>
<ul>
<li><strong>Authors: </strong>Fabien Geyer, Johannes Freitag, Tobias Schulz, Sascha Uhrig</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08225">https://arxiv.org/abs/2401.08225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08225">https://arxiv.org/pdf/2401.08225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08225]] Efficient and Mathematically Robust Operations for Certified Neural  Networks Inference(https://arxiv.org/abs/2401.08225)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora</a></li>
<li><strong>Abstract: </strong>In recent years, machine learning (ML) and neural networks (NNs) have gained widespread use and attention across various domains, particularly in transportation for achieving autonomy, including the emergence of flying taxis for urban air mobility (UAM). However, concerns about certification have come up, compelling the development of standardized processes encompassing the entire ML and NN pipeline. This paper delves into the inference stage and the requisite hardware, highlighting the challenges associated with IEEE 754 floating-point arithmetic and proposing alternative number representations. By evaluating diverse summation and dot product algorithms, we aim to mitigate issues related to non-associativity. Additionally, our exploration of fixed-point arithmetic reveals its advantages over floating-point methods, demonstrating significant hardware efficiencies. Employing an empirical approach, we ascertain the optimal bit-width necessary to attain an acceptable level of accuracy, considering the inherent complexity of bit-width optimization.</li>
<li><strong>摘要：</strong>近年来，机器学习（ML）和神经网络（NN）在各个领域得到了广泛的使用和关注，特别是在实现自主的交通领域，包括用于城市空中交通（UAM）的飞行出租车的出现。然而，对认证的担忧已经出现，迫使开发涵盖整个机器学习和神经网络管道的标准化流程。本文深入研究了推理阶段和必要的硬件，强调了与 IEEE 754 浮点运算相关的挑战，并提出了替代的数字表示形式。通过评估不同的求和和点积算法，我们的目标是减轻与非关联性相关的问题。此外，我们对定点运算的探索揭示了它相对于浮点方法的优势，展示了显着的硬件效率。考虑到位宽优化的固有复杂性，我们采用经验方法，确定达到可接受的精度水平所需的最佳位宽。</li>
</ul>

<h3>Title: Large Language Models are Null-Shot Learners</h3>
<ul>
<li><strong>Authors: </strong>Pittawat Taveekitworachai, Febri Abdullah, Ruck Thawonmas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08273">https://arxiv.org/abs/2401.08273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08273">https://arxiv.org/pdf/2401.08273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08273]] Large Language Models are Null-Shot Learners(https://arxiv.org/abs/2401.08273)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with six LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.</li>
<li><strong>摘要：</strong>本文提出了零样本提示。空样本提示通过指导 LLM 使用“示例”部分中的信息（这些信息在所提供的上下文中从未存在）来执行任务，从而利用大语言模型 (LLM) 中的幻觉。虽然减少幻觉对于法学硕士的日常和关键用途至关重要且不可忽视，但我们建议，在目前这些法学硕士仍然产生幻觉的情况下，事实上，与标准相比，利用幻觉来提高执行任务的表现是可能的零射击提示。六个法学硕士的实验表明，八个数据集的大部分性能都有所提高，包括阅读理解、算术推理和闭卷问答。观察到的法学硕士相对表现提高的不一致也可能表明每个模型中存在不同程度的固有幻觉。这些差异表明，可以利用空样本提示作为使用现有基准数据集检测法学硕士幻觉程度的方法。我们还进行消融研究，包括试验零样本提示的修改版本，该版本融合了零样本思维链提示的想法，这显示了不同的结果趋势。</li>
</ul>

<h3>Title: Inferflow: an Efficient and Highly Configurable Inference Engine for  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shuming Shi, Enbo Zhao, Deng Cai, Leyang Cui, Xinting Huang, Huayang Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08294">https://arxiv.org/abs/2401.08294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08294">https://arxiv.org/pdf/2401.08294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08294]] Inferflow: an Efficient and Highly Configurable Inference Engine for  Large Language Models(https://arxiv.org/abs/2401.08294)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, code</a></li>
<li><strong>Abstract: </strong>We present Inferflow, an efficient and highly configurable inference engine for large language models (LLMs). With Inferflow, users can serve most of the common transformer models by simply modifying some lines in corresponding configuration files, without writing a single line of source code. Compared with most existing inference engines, Inferflow has some key features. First, by implementing a modular framework of atomic build-blocks and technologies, Inferflow is compositionally generalizable to new models. Second, 3.5-bit quantization is introduced in Inferflow as a tradeoff between 3-bit and 4-bit quantization. Third, hybrid model partitioning for multi-GPU inference is introduced in Inferflow to better balance inference speed and throughput than the existing partition-by-layer and partition-by-tensor strategies.</li>
<li><strong>摘要：</strong>我们推出了 Inferflow，这是一种适用于大型语言模型 (LLM) 的高效且高度可配置的推理引擎。使用Inferflow，用户只需修改相应配置文件中的一些行即可服务于大多数常见的Transformer模型，而无需编写一行源代码。与大多数现有推理引擎相比，Inferflow 有一些关键特性。首先，通过实现原子构建块和技术的模块化框架，Inferflow 在组合上可推广到新模型。其次，Inferflow 中引入了 3.5 位量化，作为 3 位和 4 位量化之间的折衷。第三，Inferflow中引入了用于多GPU推理的混合模型分区，以比现有的按层分区和按张量分区策略更好地平衡推理速度和吞吐量。</li>
</ul>

<h3>Title: DAPT: A Dual Attention Framework for Parameter-Efficient Continual  Learning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Weixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08295">https://arxiv.org/abs/2401.08295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08295">https://arxiv.org/pdf/2401.08295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08295]] DAPT: A Dual Attention Framework for Parameter-Efficient Continual  Learning of Large Language Models(https://arxiv.org/abs/2401.08295)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Based on parameter-efficient tuning (PET), existing methods devise the learning module and the selection module to handle the challenges of catastrophic forgetting (CF) and knowledge transfer (KT) in CL. The learning module allocates separate PET blocks for each continually emerged task and the selection module function to choose the correct one for the input at testing time. However, there are limitations in their deigns of both modules and they ignore the potential of aligning the two module to address CF and KT simultaneously. To this end, we propose a novel Dual Attention Framework , to align the PET learning and selection via the Dual Attentive Learning\&Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of DAPT to resist CF and facilitate KT at the same time. Moreover, DAPT exhibits the superiority when we scale it to different model sizes (from 770M to 11B) and unseen tasks.</li>
<li><strong>摘要：</strong>持续学习（CL）能力对于在动态世界中部署大型语言模型（LLM）至关重要。基于参数高效调整（PET），现有方法设计了学习模块和选择模块来应对 CL 中灾难性遗忘（CF）和知识转移（KT）的挑战。学习模块为每个不断出现的任务分配单独的 PET 块，选择模块的功能是在测试时为输入选择正确的块。然而，他们的两个模块的设计都存在局限性，并且他们忽略了对齐两个模块以同时寻址 CF 和 KT 的潜力。为此，我们提出了一种新颖的双重注意力框架，通过双重注意力学习\&选择模块来协调 PET 学习和选择。在两个 CL 基准上的大量实验证明了 DAPT 在抵抗 CF 和同时促进 KT 方面的优越性。此外，当我们将其扩展到不同的模型大小（从 770M 到 11B）和未见过的任务时，DAPT 表现出了优越性。</li>
</ul>

<h3>Title: Anchor function: a type of benchmark functions for studying language  models</h3>
<ul>
<li><strong>Authors: </strong>Zhongwang Zhang, Zhiwei Wang, Junjie Yao, Zhangchen Zhou, Xiaolong Li, Weinan E, Zhi-Qin John Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08309">https://arxiv.org/abs/2401.08309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08309">https://arxiv.org/pdf/2401.08309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08309]] Anchor function: a type of benchmark functions for studying language  models(https://arxiv.org/abs/2401.08309)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, lora</a></li>
<li><strong>Abstract: </strong>Understanding transformer-based language models is becoming increasingly crucial, particularly as they play pivotal roles in advancing towards artificial general intelligence. However, language model research faces significant challenges, especially for academic research groups with constrained resources. These challenges include complex data structures, unknown target functions, high computational costs and memory requirements, and a lack of interpretability in the inference process, etc. Drawing a parallel to the use of simple models in scientific research, we propose the concept of an anchor function. This is a type of benchmark function designed for studying language models in learning tasks that follow an "anchor-key" pattern. By utilizing the concept of an anchor function, we can construct a series of functions to simulate various language tasks. The anchor function plays a role analogous to that of mice in diabetes research, particularly suitable for academic research. We demonstrate the utility of the anchor function with an example, revealing two basic operations by attention structures in language models: shifting tokens and broadcasting one token from one position to many positions. These operations are also commonly observed in large language models. The anchor function framework, therefore, opens up a series of valuable and accessible research questions for further exploration, especially for theoretical study.</li>
<li><strong>摘要：</strong>理解基于 Transformer 的语言模型变得越来越重要，特别是因为它们在推进通用人工智能方面发挥着关键作用。然而，语言模型研究面临着巨大的挑战，特别是对于资源有限的学术研究团体而言。这些挑战包括复杂的数据结构、未知的目标函数、高计算成本和内存需求以及推理过程中缺乏可解释性等。与科学研究中简单模型的使用相似，我们提出了锚点的概念功能。这是一种基准函数，旨在研究遵循“锚键”模式的学习任务中的语言模型。利用锚函数的概念，我们可以构造一系列函数来模拟各种语言任务。锚定功能在糖尿病研究中起到类似于小鼠的作用，特别适合学术研究。我们通过一个例子展示了锚函数的实用性，揭示了语言模型中注意力结构的两种基本操作：转移标记和将一个标记从一个位置广播到多个位置。这些操作在大型语言模型中也很常见。因此，锚函数框架为进一步探索，特别是理论研究开辟了一系列有价值且易于理解的研究问题。</li>
</ul>

<h3>Title: Application of LLM Agents in Recruitment: A Novel Framework for Resume  Screening</h3>
<ul>
<li><strong>Authors: </strong>Chengguang Gan, Qinghao Zhang, Tatsunori Mori</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08315">https://arxiv.org/abs/2401.08315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08315">https://arxiv.org/pdf/2401.08315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08315]] Application of LLM Agents in Recruitment: A Novel Framework for Resume  Screening(https://arxiv.org/abs/2401.08315)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. The advent of Large Language Models (LLMs) has notably enhanced the efficacy of these systems, showcasing their robust generalization abilities across diverse language-related tasks. Accompanying these developments are various agents based on LLMs, which facilitate their application in practical scenarios. This paper introduces a novel LLM-based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making, determining which candidates receive job offers, or which ones to bring in for interviews. To evaluate our framework, we constructed a dataset from actual resumes and conducted simulate a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.</li>
<li><strong>摘要：</strong>简历筛选的自动化是组织招聘流程的一个重要方面。自动简历筛选系统通常包含一系列自然语言处理 (NLP) 任务。大型语言模型（LLM）的出现显着增强了这些系统的效率，展示了它们在各种语言相关任务中强大的泛化能力。伴随这些发展的是基于法学硕士的各种代理，这有助于它们在实际场景中的应用。本文介绍了一种基于法学硕士的新型简历筛选代理框架，旨在提高招聘过程中的效率和时间管理。我们的框架的独特之处在于它能够从大型数据集中有效地总结和评分每份简历。此外，它还利用法学硕士代理人进行决策，确定哪些候选人获得工作机会，或者邀请哪些候选人参加面试。为了评估我们的框架，我们根据实际简历构建了一个数据集，并进行了模拟简历筛选过程。随后，对模拟实验的结果进行了比较和详细分析。结果表明，我们的自动化简历筛选框架比传统手动方法快 11 倍。此外，通过对 LLM 进行微调，我们观察到在简历句子分类阶段 F1 分数显着提高，达到 87.73%。在简历总结和评分阶段，我们的微调模型超越了 GPT-3.5 模型的基线性能。对LLM代理人在最终offer阶段的决策效率的分析进一步强调了LLM代理人在改变简历筛选流程方面的潜力。</li>
</ul>

<h3>Title: OpenDPD: An Open-Source End-to-End Learning & Benchmarking Framework for  Wideband Power Amplifier Modeling and Digital Pre-Distortion</h3>
<ul>
<li><strong>Authors: </strong>Yizhuo Wu, Gagan Deep Singh, Mohammadreza Beikmirza, Leo de Vreede, Morteza Alavi, Chang Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08318">https://arxiv.org/abs/2401.08318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08318">https://arxiv.org/pdf/2401.08318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08318]] OpenDPD: An Open-Source End-to-End Learning & Benchmarking Framework for  Wideband Power Amplifier Modeling and Digital Pre-Distortion(https://arxiv.org/abs/2401.08318)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, code</a></li>
<li><strong>Abstract: </strong>With the rise in communication capacity, deep neural networks (DNN) for digital pre-distortion (DPD) to correct non-linearity in wideband power amplifiers (PAs) have become prominent. Yet, there is a void in open-source and measurement-setup-independent platforms for fast DPD exploration and objective DPD model comparison. This paper presents an open-source framework, OpenDPD, crafted in PyTorch, with an associated dataset for PA modeling and DPD learning. We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a novel end-to-end learning architecture, outperforming previous DPD models on a digital PA DPA in the new digital transmitter (DTX) architecture with unconventional transfer characteristics compared to analog PAs. Measurements show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB for 200 MHz OFDM signals. OpenDPD code, datasets, and documentation are publicly available at https://github.com/lab-emi/OpenDPD.</li>
<li><strong>摘要：</strong>随着通信能力的提高，用于数字预失真（DPD）以校正宽带功率放大器（PA）非线性的深度神经网络（DNN）变得越来越重要。然而，用于快速 DPD 探索和客观 DPD 模型比较的开源和独立于测量设置的平台尚存在空白。本文提出了一个用 PyTorch 制作的开源框架 OpenDPD，以及用于 PA 建模和 DPD 学习的相关数据集。我们引入了密集门控循环单元 (DGRU)-DPD，通过新颖的端到端学习架构进行训练，在新的数字发射机 (DTX) 架构中的数字 PA DPA 上优于之前的 DPD 模型，与模拟相比，具有非常规的传输特性PA。测量结果表明，我们的 DGRU-DPD 对于 200 MHz OFDM 信号实现了 -44.69/-44.47 dBc 的 ACPR 和 -35.22 dB 的 EVM。 OpenDPD 代码、数据集和文档可在 https://github.com/lab-emi/OpenDPD 上公开获取。</li>
</ul>

<h3>Title: RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large  Language Models in Tool Learning</h3>
<ul>
<li><strong>Authors: </strong>Junjie Ye, Yilong Wu, Songyang Gao, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08326">https://arxiv.org/abs/2401.08326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08326">https://arxiv.org/pdf/2401.08326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08326]] RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large  Language Models in Tool Learning(https://arxiv.org/abs/2401.08326)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning. The code and data are available at https://github.com/Junjie-Ye/RoTBench.</li>
<li><strong>摘要：</strong>工具学习作为大型语言模型（LLM）与物理世界之间交互的重要手段引起了广泛的兴趣。目前的研究主要强调法学硕士在结构良好的环境中使用工具的能力，而忽视了它们在面对现实世界不可避免的噪音时的稳定性。为了弥补这一差距，我们引入了 RoTBench，这是一个用于评估法学硕士在工具学习中稳健性的多级基准。具体来说，我们建立了五个外部环境，每个环境具有不同的噪声级别（即，干净、轻微、中等、严重和联合），提供了对模型跨三个关键阶段的弹性的深入分析：工具选择、参数识别、和内容填充。涉及六个广泛使用的模型的实验强调了增强法学硕士在工具学习方面的稳健性的迫切必要性。例如，在手动精度没有实质性变化的情况下，GPT-4的性能甚至从80.00大幅下降到58.10。更令人惊讶的是，GPT 系列固有的噪声校正能力反而阻碍了其面对轻微噪声的适应性。根据这些发现，我们提出了 RoTTuning，这是一种丰富培训环境多样性的策略，以增强法学硕士在工具学习方面的稳健性。代码和数据可在 https://github.com/Junjie-Ye/RoTBench 获取。</li>
</ul>

<h3>Title: Salute the Classic: Revisiting Challenges of Machine Translation in the  Age of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu, Derek F. Wong, Shuming Shi, Zhaopeng Tu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08350">https://arxiv.org/abs/2401.08350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08350">https://arxiv.org/pdf/2401.08350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08350]] Salute the Classic: Revisiting Challenges of Machine Translation in the  Age of Large Language Models(https://arxiv.org/abs/2401.08350)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017), which have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search. Our empirical findings indicate that LLMs effectively lessen the reliance on parallel data for major languages in the pretraining phase. Additionally, the LLM-based translation system significantly enhances the translation of long sentences that contain approximately 80 words and shows the capability to translate documents of up to 512 words. However, despite these significant improvements, the challenges of domain mismatch and prediction of rare words persist. While the challenges of word alignment and beam search, specifically associated with NMT, may not apply to LLMs, we identify three new challenges for LLMs in translation tasks: inference efficiency, translation of low-resource languages in the pretraining phase, and human-aligned evaluation. The datasets and models are released at https://github.com/pangjh3/LLM4MT.</li>
<li><strong>摘要：</strong>神经机器翻译（NMT）的发展受到六个核心挑战的显着影响（Koehn 和 Knowles，2017），这些挑战已成为该领域进展的基准。这项研究重新审视了这些挑战，深入探讨了它们在高级大语言模型 (LLM) 背景下的持续相关性：领域不匹配、并行数据量、罕见词预测、长句翻译、作为单词对齐的注意力模型以及子模型。最优波​​束搜索。我们的实证研究结果表明，法学硕士有效地减少了预训练阶段对主要语言并行数据的依赖。此外，基于法学硕士的翻译系统显着增强了包含约 80 个单词的长句子的翻译，并显示出翻译最多 512 个单词的文档的能力。然而，尽管有这些重大改进，域不匹配和稀有词预测的挑战仍然存在。虽然与 NMT 相关的词对齐和波束搜索的挑战可能不适用于法学硕士，但我们确定了法学硕士在翻译任务中面临的三个新挑战：推理效率、预训练阶段低资源语言的翻译以及人类对齐评估。数据集和模型发布于https://github.com/pangjh3/LLM4MT。</li>
</ul>

<h3>Title: Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian  Approach</h3>
<ul>
<li><strong>Authors: </strong>Mahrokh Ghoddousi Boroujeni, Andreas Krause, Giancarlo Ferrari Trecate</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08351">https://arxiv.org/abs/2401.08351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08351">https://arxiv.org/pdf/2401.08351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08351]] Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian  Approach(https://arxiv.org/abs/2401.08351)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Federated learning aims to infer a shared model from private and decentralized data stored locally by multiple clients. Personalized federated learning (PFL) goes one step further by adapting the global model to each client, enhancing the model's fit for different clients. A significant level of personalization is required for highly heterogeneous clients, but can be challenging to achieve especially when they have small datasets. To address this problem, we propose a PFL algorithm named PAC-PFL for learning probabilistic models within a PAC-Bayesian framework that utilizes differential privacy to handle data-dependent priors. Our algorithm collaboratively learns a shared hyper-posterior and regards each client's posterior inference as the personalization step. By establishing and minimizing a generalization bound on the average true risk of clients, PAC-PFL effectively combats over-fitting. PACPFL achieves accurate and well-calibrated predictions, supported by experiments on a dataset of photovoltaic panel power generation, FEMNIST dataset (Caldas et al., 2019), and Dirichlet-partitioned EMNIST dataset (Cohen et al., 2017).</li>
<li><strong>摘要：</strong>联邦学习旨在从多个客户端本地存储的私有和去中心化数据中推断出共享模型。个性化联邦学习 (PFL) 更进一步，根据每个客户调整全局模型，增强模型对不同客户的适应性。高度异构的客户需要高度的个性化，但实现起来可能具有挑战性，特别是当他们拥有较小的数据集时。为了解决这个问题，我们提出了一种名为 PAC-PFL 的 PFL 算法，用于在 PAC-贝叶斯框架内学习概率模型，该框架利用差分隐私来处理数据相关的先验。我们的算法协作学习共享的超后验，并将每个客户的后验推理视为个性化步骤。通过建立并最小化客户平均真实风险的泛化界限，PAC-PFL 有效地防止了过度拟合。 PACPFL 实现了准确且校准良好的预测，并得到了光伏板发电数据集、FEMNIST 数据集（Caldas 等人，2019）和 Dirichlet 分区的 EMNIST 数据集（Cohen 等人，2017）上实验的支持。</li>
</ul>

<h3>Title: Hallucination Detection and Hallucination Mitigation: An Investigation</h3>
<ul>
<li><strong>Authors: </strong>Junliang Luo, Tianyu Li, Di Wu, Michael Jenkin, Steve Liu, Gregory Dudek</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08358">https://arxiv.org/abs/2401.08358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08358">https://arxiv.org/pdf/2401.08358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08358]] Hallucination Detection and Hallucination Mitigation: An Investigation(https://arxiv.org/abs/2401.08358)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM)，包括 ChatGPT、Bard 和 Llama，在过去两年中在一系列不同的应用中取得了显着的成功。尽管取得了这些成功，但仍然存在限制法学硕士广泛应用的问题。一个关键问题是幻觉问题。幻觉是指法学硕士除了正确的反应之外，还可能产生看似正确但实际上不正确的反应。本报告旨在对当前有关幻觉检测和幻觉缓解的文献进行全面回顾。我们希望这份报告能够为对法学硕士感兴趣并将其应用于现实世界任务的工程师和研究人员提供很好的参考。</li>
</ul>

<h3>Title: Exploiting Inter-Layer Expert Affinity for Accelerating  Mixture-of-Experts Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Jinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar K. (DK)Panda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08383">https://arxiv.org/abs/2401.08383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08383">https://arxiv.org/pdf/2401.08383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08383]] Exploiting Inter-Layer Expert Affinity for Accelerating  Mixture-of-Experts Model Inference(https://arxiv.org/abs/2401.08383)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>In large language models like the Generative Pre-trained Transformer, the Mixture of Experts paradigm has emerged as a powerful technique for enhancing model expressiveness and accuracy. However, deploying GPT MoE models for parallel inference on distributed systems presents significant challenges, primarily due to the extensive Alltoall communication required for expert routing and aggregation. This communication bottleneck exacerbates the already complex computational landscape, hindering the efficient utilization of high-performance computing resources. In this paper, we propose a lightweight optimization technique called ExFlow, to largely accelerate the inference of these MoE models. We take a new perspective on alleviating the communication overhead by exploiting the inter-layer expert affinity. Unlike previous methods, our solution can be directly applied to pre-trained MoE models without any fine-tuning or accuracy degradation. By proposing a context-coherent expert parallelism on distributed systems, our design only uses one Alltoall communication to deliver the same functionality while previous methods all require two Alltoalls. By carefully examining the conditional probability in tokens' routing across multiple layers, we proved that pre-trained GPT MoE models implicitly exhibit a strong inter-layer expert affinity. We then design an efficient integer programming model to capture such features and show that by properly placing the experts on corresponding GPUs, we can reduce up to 67% cross-GPU routing latency. Our solution beats the cutting-edge MoE implementations with experts from 8 to 64, with up to 2.2x improvement in inference throughput. We further provide a detailed study of how the model implicitly acquires this expert affinity at the very early training stage and how this affinity evolves and stabilizes during training.</li>
<li><strong>摘要：</strong>在像生成预训练 Transformer 这样的大型语言模型中，专家混合范式已经成为增强模型表达力和准确性的强大技术。然而，在分布式系统上部署 GPT MoE 模型进行并行推理面临着重大挑战，这主要是由于专家路由和聚合所需的广泛的 Alltoall 通信。这种通信瓶颈加剧了本已复杂的计算环境，阻碍了高性能计算资源的有效利用。在本文中，我们提出了一种名为 ExFlow 的轻量级优化技术，以大大加速这些 MoE 模型的推理。我们以新的视角通过利用层间专家亲和力来减轻通信开销。与以前的方法不同，我们的解决方案可以直接应用于预训练的 MoE 模型，无需任何微调或精度下降。通过在分布式系统上提出上下文一致的专家并行性，我们的设计仅使用一个 Alltoall 通信来提供相同的功能，而以前的方法都需要两个 Alltoall。通过仔细检查令牌跨多层路由的条件概率，我们证明预训练的 GPT MoE 模型隐含地表现出强大的层间专家亲和力。然后，我们设计了一个高效的整数编程模型来捕获这些特征，并表明，通过将专家正确地放置在相应的 GPU 上，我们可以减少高达 67% 的跨 GPU 路由延迟。我们的解决方案击败了拥有 8 到 64 名专家的尖端 MoE 实施，推理吞吐量提高了 2.2 倍。我们进一步详细研究了模型如何在早期训练阶段隐式获得这种专家亲和力，以及这种亲和力在训练过程中如何演变和稳定。</li>
</ul>

<h3>Title: Deep Learning-based Group Causal Inference in Multivariate Time-series</h3>
<ul>
<li><strong>Authors: </strong>Wasim Ahmad, Maha Shadaydeh, Joachim Denzler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08386">https://arxiv.org/abs/2401.08386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08386">https://arxiv.org/pdf/2401.08386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08386]] Deep Learning-based Group Causal Inference in Multivariate Time-series(https://arxiv.org/abs/2401.08386)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Causal inference in a nonlinear system of multivariate timeseries is instrumental in disentangling the intricate web of relationships among variables, enabling us to make more accurate predictions and gain deeper insights into real-world complex systems. Causality methods typically identify the causal structure of a multivariate system by considering the cause-effect relationship of each pair of variables while ignoring the collective effect of a group of variables or interactions involving more than two-time series variables. In this work, we test model invariance by group-level interventions on the trained deep networks to infer causal direction in groups of variables, such as climate and ecosystem, brain networks, etc. Extensive testing with synthetic and real-world time series data shows a significant improvement of our method over other applied group causality methods and provides us insights into real-world time series. The code for our method can be found at:https://github.com/wasimahmadpk/gCause.</li>
<li><strong>摘要：</strong>多元时间序列非线性系统中的因果推理有助于理清变量之间错综复杂的关系网络，使我们能够做出更准确的预测并更深入地了解现实世界的复杂系统。因果关系方法通常通过考虑每对变量的因果关系来识别多元系统的因果结构，同时忽略一组变量的集体效应或涉及两个以上时间序列变量的相互作用。在这项工作中，我们通过对经过训练的深度网络进行组级干预来测试模型不变性，以推断变量组（例如气候和生态系统、大脑网络等）的因果方向。对合成和真实世界时间序列数据的广泛测试表明与其他应用的群体因果关系方法相比，我们的方法有了显着改进，并为我们提供了对现实世界时间序列的见解。我们方法的代码可以在以下位置找到：https://github.com/wasimahmadpk/gCause。</li>
</ul>

<h3>Title: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on  Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Aman Gupta, Anup Shirgaonkar, Angels de Luis Balaguer, Bruno Silva, Daniel Holstein, Dawei Li, Jennifer Marsman, Leonardo O. Nunes, Mahsa Rouzbahman, Morris Sharp, Nick Mecklenburg, Rafael Padilha, Ranveer Chandra, Renato Luiz de Freitas Cunha, Roberto de M. Estevão Filho, Ryan Tsang, Sara Malvar, Swati Sharma, Todd Hendry, Vijay Aski, Vijetha Vijayendran, Vinamra Benara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08406">https://arxiv.org/abs/2401.08406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08406">https://arxiv.org/pdf/2401.08406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08406]] RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on  Agriculture(https://arxiv.org/abs/2401.08406)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, retrieval-augmented generation, rag</a></li>
<li><strong>Abstract: </strong>There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.</li>
<li><strong>摘要：</strong>开发人员在构建大型语言模型 (LLM) 应用程序时合并专有和特定领域数据的常见方法有两种：检索增强生成 (RAG) 和微调。 RAG 使用外部数据增强提示，而微调则将额外的知识合并到模型本身中。然而，这两种方法的优缺点尚不清楚。在本文中，我们提出了一种微调和 RAG 的流程，并针对多种流行的 LLM（包括 Llama2-13B、GPT-3.5 和 GPT-4）提出了两者的权衡。我们的管道由多个阶段组成，包括从 PDF 中提取信息、生成问题和答案、使用它们进行微调以及利用 GPT-4 评估结果。我们提出了评估 RAG 和微调管道不同阶段性能的指标。我们对农业数据集进行了深入研究。农业作为一个行业，人工智能的渗透率还不是很高，我们研究了一个潜在的颠覆性应用——如果我们能够为农民提供特定地点的见解会怎么样？我们的结果显示了我们的数据集生成管道在捕获特定地理知识方面的有效性，以及 RAG 和微调的定量和定性优势。我们发现准确率提高了 6 个百分点以上。当微调模型时，这是通过 RAG 累积的，这将精度提高了 5 个百分点。更远。在一项特定实验中，我们还证明了经过微调的模型利用来自不同地理位置的信息来回答特定问题，将答案相似度从 47% 提高到 72%。总体而言，结果表明使用法学硕士构建的系统如何能够适应和整合对特定行业至关重要的跨维度知识，为法学硕士在其他工业领域的进一步应用铺平道路。</li>
</ul>

<h3>Title: Contrastive Preference Optimization: Pushing the Boundaries of LLM  Performance in Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Young Jin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08417">https://arxiv.org/abs/2401.08417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08417">https://arxiv.org/pdf/2401.08417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08417]] Contrastive Preference Optimization: Pushing the Boundaries of LLM  Performance in Machine Translation(https://arxiv.org/abs/2401.08417)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, code</a></li>
<li><strong>Abstract: </strong>Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.</li>
<li><strong>摘要：</strong>中等规模的大型语言模型 (LLM)——具有 7B 或 13B 参数的模型——表现出有前途的机器翻译 (MT) 性能。然而，即使是性能最佳的基于 13B LLM 的翻译模型（如 ALMA），也无法与最先进的传统编码器-解码器翻译模型或更大规模的 LLM（如 GPT-4）的性能相匹配。在这项研究中，我们弥补了这一性能差距。我们首先评估了 MT 任务中法学硕士的监督微调的缺点，强调参考数据中存在的质量问题，尽管是人类生成的。然后，与模仿参考翻译的 SFT 相比，我们引入了对比偏好优化（CPO），这是一种训练模型以避免生成足够但不完美的翻译的新颖方法。将 CPO 应用于仅具有 22K 并行句子和 12M 参数的 ALMA 模型会产生显着的改进。由此产生的模型称为 ALMA-R，可以匹配或超过 WMT 竞赛获胜者和 GPT-4 在 WMT'21、WMT'22 和 WMT'23 测试数据集上的性能。</li>
</ul>

<h3>Title: Ask the experts: sourcing high-quality datasets for nutritional  counselling through Human-AI collaboration</h3>
<ul>
<li><strong>Authors: </strong>Simone Balloccu, Ehud Reiter, Vivek Kumar, Diego Reforgiato Recupero, Daniele Riboni</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08420">https://arxiv.org/abs/2401.08420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08420">https://arxiv.org/pdf/2401.08420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08420]] Ask the experts: sourcing high-quality datasets for nutritional  counselling through Human-AI collaboration(https://arxiv.org/abs/2401.08420)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), with their flexible generation abilities, can be powerful data sources in domains with few or no available corpora. However, problems like hallucinations and biases limit such applications. In this case study, we pick nutrition counselling, a domain lacking any public resource, and show that high-quality datasets can be gathered by combining LLMs, crowd-workers and nutrition experts. We first crowd-source and cluster a novel dataset of diet-related issues, then work with experts to prompt ChatGPT into producing related supportive text. Finally, we let the experts evaluate the safety of the generated text. We release HAI-coaching, the first expert-annotated nutrition counselling dataset containing ~2.4K dietary struggles from crowd workers, and ~97K related supportive texts generated by ChatGPT. Extensive analysis shows that ChatGPT while producing highly fluent and human-like text, also manifests harmful behaviours, especially in sensitive topics like mental health, making it unsuitable for unsupervised use.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 凭借其灵活的生成能力，可以成为可用语料库很少或没有的领域中强大的数据源。然而，幻觉和偏见等问题限制了此类应用。在本案例研究中，我们选择了营养咨询这一缺乏公共资源的领域，并表明可以通过法学硕士、众包工作者和营养专家的结合来收集高质量的数据集。我们首先众包并聚类一个新的饮食相关问题数据集，然后与专家合作促使 ChatGPT 生成相关的支持文本。最后，我们让专家评估生成文本的安全性。我们发布了 HAI-coaching，这是第一个专家注释的营养咨询数据集，其中包含来自人群工作者的约 2.4K 饮食挣扎，以及由 ChatGPT 生成的约 97K 相关支持文本。广泛的分析表明，ChatGPT 在生成高度流畅且类似人类的文本的同时，也表现出有害行为，特别是在心理健康等敏感话题上，使其不适合在无人监督的情况下使用。</li>
</ul>

<h3>Title: Machine Translation with Large Language Models: Prompt Engineering for  Persian, English, and Russian Directions</h3>
<ul>
<li><strong>Authors: </strong>Nooshin Pourkamali, Shler Ebrahim Sharifi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08429">https://arxiv.org/abs/2401.08429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08429">https://arxiv.org/pdf/2401.08429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08429]] Machine Translation with Large Language Models: Prompt Engineering for  Persian, English, and Russian Directions(https://arxiv.org/abs/2401.08429)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Generative large language models (LLMs) have demonstrated exceptional proficiency in various natural language processing (NLP) tasks, including machine translation, question answering, text summarization, and natural language understanding. To further enhance the performance of LLMs in machine translation, we conducted an investigation into two popular prompting methods and their combination, focusing on cross-language combinations of Persian, English, and Russian. We employed n-shot feeding and tailored prompting frameworks. Our findings indicate that multilingual LLMs like PaLM exhibit human-like machine translation outputs, enabling superior fine-tuning of desired translation nuances in accordance with style guidelines and linguistic considerations. These models also excel in processing and applying prompts. However, the choice of language model, machine translation task, and the specific source and target languages necessitate certain considerations when adopting prompting frameworks and utilizing n-shot in-context learning. Furthermore, we identified errors and limitations inherent in popular LLMs as machine translation tools and categorized them based on various linguistic metrics. This typology of errors provides valuable insights for utilizing LLMs effectively and offers methods for designing prompts for in-context learning. Our report aims to contribute to the advancement of machine translation with LLMs by improving both the accuracy and reliability of evaluation metrics.</li>
<li><strong>摘要：</strong>生成式大语言模型 (LLM) 在各种自然语言处理 (NLP) 任务中表现出了卓越的能力，包括机器翻译、问答、文本摘要和自然语言理解。为了进一步提高法学硕士在机器翻译方面的表现，我们对两种流行的提示方法及其组合进行了调查，重点关注波斯语、英语和俄语的跨语言组合。我们采用了 n-shot feed 和定制的提示框架。我们的研究结果表明，像 PaLM 这样的多语言法学硕士表现出类似人类的机器翻译输出，能够根据风格指南和语言考虑因素对所需翻译的细微差别进行卓越的微调。这些模型在处理和应用提示方面也表现出色。然而，在采用提示框架和利用n-shot上下文学习时，语言模型、机器翻译任务以及特定源语言和目标语言的选择需要一定的考虑。此外，我们还发现了流行的法学硕士作为机器翻译工具固有的错误和局限性，并根据各种语言指标对它们进行了分类。这种错误类型为有效利用法学硕士提供了宝贵的见解，并提供了设计情境学习提示的方法。我们的报告旨在通过提高评估指标的准确性和可靠性，为法学硕士机器翻译的进步做出贡献。</li>
</ul>

<h3>Title: CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Yaojia Lv, Haojie Pan, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08438">https://arxiv.org/abs/2401.08438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08438">https://arxiv.org/pdf/2401.08438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08438]] CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language  Models(https://arxiv.org/abs/2401.08438)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Cognitive dynamics are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) reveal their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on static modeling, overlooking the dynamic nature of cognition. To bridge this gap, we propose the concept of the cognitive dynamics of LLMs and present a corresponding task with the inspiration of longitudinal studies. Towards the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we introduce CogGPT for the task, which features an innovative iterative cognitive mechanism aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows.</li>
<li><strong>摘要：</strong>认知动力学对于促进人类对世界的理解至关重要。大语言模型 (LLM) 的最新进展揭示了它们在认知模拟方面的潜力。然而，这些基于法学硕士的认知研究主要关注静态建模，忽视了认知的动态本质。为了弥合这一差距，我们提出了法学硕士认知动态的概念，并在纵向研究的启发下提出了相应的任务。为了完成这项任务，我们开发了 CogBench，这是一种新颖的基准，用于评估法学硕士的认知动态并通过参与者调查对其进行验证。我们还为CogBench设计了两个评估指标，包括真实性和合理性。认识到法学硕士固有的静态性质，我们为该任务引入了 CogGPT，其特点是创新的迭代认知机制，旨在增强终身认知动态。实证结果证明了 CogGPT 相对于现有方法的优越性，特别是它能够在连续信息流下促进特定角色的认知动态。</li>
</ul>

<h3>Title: Reinforcement Learning for Conversational Question Answering over  Knowledge Graph</h3>
<ul>
<li><strong>Authors: </strong>Mi Wu</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08460">https://arxiv.org/abs/2401.08460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08460">https://arxiv.org/pdf/2401.08460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08460]] Reinforcement Learning for Conversational Question Answering over  Knowledge Graph(https://arxiv.org/abs/2401.08460)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Conversational question answering (ConvQA) over law knowledge bases (KBs) involves answering multi-turn natural language questions about law and hope to find answers in the law knowledge base. Despite many methods have been proposed. Existing law knowledge base ConvQA model assume that the input question is clear and can perfectly reflect user's intention. However, in real world, the input questions are noisy and inexplict. This makes the model hard to find the correct answer in the law knowledge bases. In this paper, we try to use reinforcement learning to solve this problem. The reinforcement learning agent can automatically learn how to find the answer based on the input question and the conversation history, even when the input question is inexplicit. We test the proposed method on several real world datasets and the results show the effectivenss of the proposed model.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: Decentralised Emergence of Robust and Adaptive Linguistic Conventions in  Populations of Autonomous Agents Grounded in Continuous Worlds</h3>
<ul>
<li><strong>Authors: </strong>Jérôme Botoko Ekila, Jens Nevens, Lara Verheyen, Katrien Beuls, Paul Van Eecke</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08461">https://arxiv.org/abs/2401.08461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08461">https://arxiv.org/pdf/2401.08461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08461]] Decentralised Emergence of Robust and Adaptive Linguistic Conventions in  Populations of Autonomous Agents Grounded in Continuous Worlds(https://arxiv.org/abs/2401.08461)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>This paper introduces a methodology through which a population of autonomous agents can establish a linguistic convention that enables them to refer to arbitrary entities that they observe in their environment. The linguistic convention emerges in a decentralised manner through local communicative interactions between pairs of agents drawn from the population. The convention consists of symbolic labels (word forms) associated to concept representations (word meanings) that are grounded in a continuous feature space. The concept representations of each agent are individually constructed yet compatible on a communicative level. Through a range of experiments, we show (i) that the methodology enables a population to converge on a communicatively effective, coherent and human-interpretable linguistic convention, (ii) that it is naturally robust against sensor defects in individual agents, (iii) that it can effectively deal with noisy observations, uncalibrated sensors and heteromorphic populations, (iv) that the method is adequate for continual learning, and (v) that the convention self-adapts to changes in the environment and communicative needs of the agents.</li>
<li><strong>摘要：</strong>本文介绍了一种方法，通过该方法，一群自治代理可以建立一种语言约定，使他们能够引用他们在环境中观察到的任意实体。语言约定通过来自人群的成对代理之间的本地交流互动以分散的方式出现。该约定由与基于连续特征空间的概念表示（单词含义）相关联的符号标签（单词形式）组成。每个代理的概念表示都是单独构建的，但在通信层面上是兼容的。通过一系列实验，我们表明（i）该方法使群体能够收敛于有效的、连贯的和人类可解释的语言约定，（ii）它对个体代理的传感器缺陷具有天然的鲁棒性，（iii）它可以有效地处理噪声观测、未校准的传感器和异态群体，（iv）该方法足以持续学习，（v）该公约能够自适应环境的变化和智能体的通信需求。</li>
</ul>

<h3>Title: Enhancing Evolving Domain Generalization through Dynamic Latent  Representations</h3>
<ul>
<li><strong>Authors: </strong>Binghui Xie, Yongqiang Chen, Jiaqi Wang, Kaiwen Zhou, Bo Han, Wei Meng, James Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08464">https://arxiv.org/abs/2401.08464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08464">https://arxiv.org/pdf/2401.08464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08464]] Enhancing Evolving Domain Generalization through Dynamic Latent  Representations(https://arxiv.org/abs/2401.08464)</code><input type="text"></li>
<li><strong>Keywords: </strong>code, rag</a></li>
<li><strong>Abstract: </strong>Domain generalization is a critical challenge for machine learning systems. Prior domain generalization methods focus on extracting domain-invariant features across several stationary domains to enable generalization to new domains. However, in non-stationary tasks where new domains evolve in an underlying continuous structure, such as time, merely extracting the invariant features is insufficient for generalization to the evolving new domains. Nevertheless, it is non-trivial to learn both evolving and invariant features within a single model due to their conflicts. To bridge this gap, we build causal models to characterize the distribution shifts concerning the two patterns, and propose to learn both dynamic and invariant features via a new framework called Mutual Information-Based Sequential Autoencoders (MISTS). MISTS adopts information theoretic constraints onto sequential autoencoders to disentangle the dynamic and invariant features, and leverage a domain adaptive classifier to make predictions based on both evolving and invariant information. Our experimental results on both synthetic and real-world datasets demonstrate that MISTS succeeds in capturing both evolving and invariant information, and present promising results in evolving domain generalization tasks.</li>
<li><strong>摘要：</strong>领域泛化是机器学习系统的一个关键挑战。先前的域泛化方法侧重于跨多个固定域提取域不变特征，以实现对新域的泛化。然而，在新领域在底层连续结构（例如时间）中演化的非平稳任务中，仅仅提取不变特征不足以泛化到不断演化的新领域。然而，由于它们之间的冲突，在单个模型中学习演化特征和不变特征并非易事。为了弥补这一差距，我们构建了因果模型来表征两种模式的分布变化，并建议通过称为基于互信息的顺序自动编码器（MISTS）的新框架来学习动态和不变特征。 MISTS 对顺序自动编码器采用信息论约束来解开动态和不变特征，并利用域自适应分类器根据演化和不变信息进行预测。我们在合成数据集和真实数据集上的实验结果表明，MISTS 成功捕获了演化信息和不变信息，并在演化领域泛化任务中呈现出有希望的结果。</li>
</ul>

<h3>Title: Solving Continual Offline Reinforcement Learning with Decision  Transformer</h3>
<ul>
<li><strong>Authors: </strong>Kaixin Huang, Li Shen, Chen Zhao, Chun Yuan, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08478">https://arxiv.org/abs/2401.08478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08478">https://arxiv.org/pdf/2401.08478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08478]] Solving Continual Offline Reinforcement Learning with Decision  Transformer(https://arxiv.org/abs/2401.08478)</code><input type="text"></li>
<li><strong>Keywords: </strong>lora, agent</a></li>
<li><strong>Abstract: </strong>Continuous offline reinforcement learning (CORL) combines continuous and offline reinforcement learning, enabling agents to learn multiple tasks from static datasets without forgetting prior tasks. However, CORL faces challenges in balancing stability and plasticity. Existing methods, employing Actor-Critic structures and experience replay (ER), suffer from distribution shifts, low efficiency, and weak knowledge-sharing. We aim to investigate whether Decision Transformer (DT), another offline RL paradigm, can serve as a more suitable offline continuous learner to address these issues. We first compare AC-based offline algorithms with DT in the CORL framework. DT offers advantages in learning efficiency, distribution shift mitigation, and zero-shot generalization but exacerbates the forgetting problem during supervised parameter updates. We introduce multi-head DT (MH-DT) and low-rank adaptation DT (LoRA-DT) to mitigate DT's forgetting problem. MH-DT stores task-specific knowledge using multiple heads, facilitating knowledge sharing with common components. It employs distillation and selective rehearsal to enhance current task learning when a replay buffer is available. In buffer-unavailable scenarios, LoRA-DT merges less influential weights and fine-tunes DT's decisive MLP layer to adapt to the current task. Extensive experiments on MoJuCo and Meta-World benchmarks demonstrate that our methods outperform SOTA CORL baselines and showcase enhanced learning capabilities and superior memory efficiency.</li>
<li><strong>摘要：</strong>连续离线强化学习（CORL）结合了连续强化学习和离线强化学习，使智能体能够从静态数据集中学习多个任务，而不会忘记先前的任务。然而，CORL 在平衡稳定性和可塑性方面面临挑战。现有的方法采用 Actor-Critic 结构和经验重播 (ER)，存在分布偏移、效率低和知识共享薄弱的问题。我们的目标是研究另一种离线强化学习范式决策变换器（DT）是否可以作为更合适的离线连续学习器来解决这些问题。我们首先在CORL框架中将基于AC的离线算法与DT进行比较。 DT 在学习效率、分布偏移缓解和零样本泛化方面具有优势，但会加剧监督参数更新期间的遗忘问题。我们引入多头DT（MH-DT）和低秩自适应DT（LoRA-DT）来减轻DT的遗忘问题。 MH-DT 使用多个头存储特定于任务的知识，促进与通用组件的知识共享。当重播缓冲区可用时，它采用蒸馏和选择性演练来增强当前任务的学习。在缓冲区不可用的场景中，LoRA-DT 合并影响较小的权重，并微调 DT 的决定性 MLP 层以适应当前任务。对 MoJuCo 和 Meta-World 基准的大量实验表明，我们的方法优于 SOTA CORL 基准，并展示了增强的学习能力和卓越的记忆效率。</li>
</ul>

<h3>Title: Contrastive Perplexity for Controlled Generation: An Application in  Detoxifying Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tassilo Klein, Moin Nabi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08491">https://arxiv.org/abs/2401.08491</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08491">https://arxiv.org/pdf/2401.08491</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08491]] Contrastive Perplexity for Controlled Generation: An Application in  Detoxifying Large Language Models(https://arxiv.org/abs/2401.08491)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, rag</a></li>
<li><strong>Abstract: </strong>The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful.</li>
<li><strong>摘要：</strong>大型语言模型生成不良且事实上不正确的内容构成了重大挑战，并且在很大程度上仍然是一个未解决的问题。本文研究了对比学习目标的整合，以微调法学硕士的隐式知识编辑和受控文本生成。优化训练目标需要以对比方式调整文本困惑。为了促进以自我监督的方式训练模型，我们利用现成的法学硕士来生成训练数据。我们展示了在排毒领域的适用性。在此，所提出的方法可显着减少有毒内容的产生，同时保留常识推理和阅读理解等下游任务的一般实用性。所提出的方法在概念上很简单，但在经验上却很强大。</li>
</ul>

<h3>Title: The Effect of Group Status on the Variability of Group Representations  in LLM-generated Text</h3>
<ul>
<li><strong>Authors: </strong>Messi H.J. Lee, Jacob M. Montgomery, Calvin K. Lai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08495">https://arxiv.org/abs/2401.08495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08495">https://arxiv.org/pdf/2401.08495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08495]] The Effect of Group Status on the Variability of Group Representations  in LLM-generated Text(https://arxiv.org/abs/2401.08495)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become pervasive in everyday life, yet their inner workings remain opaque. While scholarly efforts have demonstrated LLMs' propensity to reproduce biases in their training data, they have primarily focused on the association of social groups with stereotypic attributes. In this paper, we extend this line of inquiry to investigate a bias akin to the social-psychological phenomenon where socially dominant groups are perceived to be less homogeneous than socially subordinate groups as it is reproduced by LLMs. We had ChatGPT, a state-of-the-art LLM, generate a diversity of texts about intersectional group identities and compared text homogeneity. We consistently find that LLMs portray African, Asian, and Hispanic Americans as more homogeneous than White Americans. They also portray women as more homogeneous than men, but these differences are small. Finally, we find that the effect of gender differs across racial/ethnic groups such that the effect of gender is consistent within African and Hispanic Americans but not within Asian and White Americans. We speculate possible sources of this bias in LLMs and posit that the bias has the potential to amplify biases in future LLM training and to reinforce stereotypes.</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: Code Generation with AlphaCodium: From Prompt Engineering to Flow  Engineering</h3>
<ul>
<li><strong>Authors: </strong>Tal Ridnik, Dedy Kredo, Itamar Friedman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08500">https://arxiv.org/abs/2401.08500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08500">https://arxiv.org/pdf/2401.08500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08500]] Code Generation with AlphaCodium: From Prompt Engineering to Flow  Engineering(https://arxiv.org/abs/2401.08500)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt, code</a></li>
<li><strong>Abstract: </strong>Code generation problems differ from common natural language problems - they require matching the exact syntax of the target language, identifying happy paths and edge cases, paying attention to numerous small details in the problem spec, and addressing other code-specific issues and requirements. Hence, many of the optimizations and tricks that have been successful in natural language generation may not be effective for code tasks. In this work, we propose a new approach to code generation by LLMs, which we call AlphaCodium - a test-based, multi-stage, code-oriented iterative flow, that improves the performances of LLMs on code problems. We tested AlphaCodium on a challenging code generation dataset called CodeContests, which includes competitive programming problems from platforms such as Codeforces. The proposed flow consistently and significantly improves results. On the validation set, for example, GPT-4 accuracy (pass@5) increased from 19% with a single well-designed direct prompt to 44% with the AlphaCodium flow. Many of the principles and best practices acquired in this work, we believe, are broadly applicable to general code generation tasks. Full implementation is available at: https://github.com/Codium-ai/AlphaCodium</li>
<li><strong>摘要：</strong></li>
</ul>

<h3>Title: EmoLLMs: A Series of Emotional Large Language Models and Annotation  Tools for Comprehensive Affective Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Liu, Kailai Yang, Tianlin Zhang, Qianqian Xie, Zeping Yu, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08508">https://arxiv.org/abs/2401.08508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08508">https://arxiv.org/pdf/2401.08508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08508]] EmoLLMs: A Series of Emotional Large Language Models and Annotation  Tools for Comprehensive Affective Analysis(https://arxiv.org/abs/2401.08508)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of LLMs, researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on various classification and regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 14 tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our model with a variety of LLMs on AEB, where our models outperform all other open-sourced LLMs, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools.</li>
<li><strong>摘要：</strong>情感分析和情绪检测是自然语言处理（NLP）中的重要研究课题，并使许多下游任务受益。随着LLM的广泛应用，研究人员开始探索基于指令调优的LLM在情感分析领域的应用。然而，这些模型只关注情感分类任务的单个方面（例如情感极性或分类情感），而忽略了回归任务（例如情感强度或情感强度），这导致下游任务的性能不佳。主要原因是缺乏全面的情感指令调优数据集和评估基准，涵盖各种情感分类和回归任务。此外，尽管情感信息对于下游任务有用，但现有的下游数据集缺乏高质量和全面的情感注释。在本文中，我们提出了EmoLLM，这是第一个开源指令跟踪LLM系列，用于基于使用指令数据微调各种LLM的全面情感分析，第一个具有234K数据样本的多任务情感分析指令数据集（AAID）基于各种分类和回归任务来支持LLM指令调优，以及包含来自不同来源和领域的14个任务的综合情感评估基准（AEB）来测试LLM的泛化能力。我们通过使用 AAID 微调 LLM 来提出一系列 EmoLLM，以解决各种情感指导任务。我们将我们的模型与 AEB 上的各种 LLM 进行比较，我们的模型优于所有其他开源 LLM，并且在大多数任务中超越 ChatGPT 和 GPT-4，这表明该系列 EmoLLM 达到了 ChatGPT 级别和 GPT-4情感分析任务的泛化能力，并证明我们的模型可以用作情感注释工具。</li>
</ul>

<h3>Title: The Gaps between Pre-train and Downstream Settings in Bias Evaluation  and Debiasing</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Kaneko, Danushka Bollegala, Timothy Baldwin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08511">https://arxiv.org/abs/2401.08511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08511">https://arxiv.org/pdf/2401.08511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08511]] The Gaps between Pre-train and Downstream Settings in Bias Evaluation  and Debiasing(https://arxiv.org/abs/2401.08511)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>The output tendencies of Pre-trained Language Models (PLM) vary markedly before and after Fine-Tuning (FT) due to the updates to the model parameters. These divergences in output tendencies result in a gap in the social biases of PLMs. For example, there exits a low correlation between intrinsic bias scores of a PLM and its extrinsic bias scores under FT-based debiasing methods. Additionally, applying FT-based debiasing methods to a PLM leads to a decline in performance in downstream tasks. On the other hand, PLMs trained on large datasets can learn without parameter updates via In-Context Learning (ICL) using prompts. ICL induces smaller changes to PLMs compared to FT-based debiasing methods. Therefore, we hypothesize that the gap observed in pre-trained and FT models does not hold true for debiasing methods that use ICL. In this study, we demonstrate that ICL-based debiasing methods show a higher correlation between intrinsic and extrinsic bias scores compared to FT-based methods. Moreover, the performance degradation due to debiasing is also lower in the ICL case compared to that in the FT case.</li>
<li><strong>摘要：</strong>由于模型参数的更新，预训练语言模型（PLM）在Fine-Tuning（FT）前后的输出趋势存在明显变化。这些产出倾向的差异导致 PLM 的社会偏见存在差距。例如，在基于 FT 的去偏方法下，PLM 的内在偏差分数与其外在偏差分数之间存在较低的相关性。此外，将基于 FT 的去偏方法应用于 PLM 会导致下游任务的性能下降。另一方面，在大型数据集上训练的 PLM 可以使用提示通过上下文学习 (ICL) 进行学习，而无需进行参数更新。与基于 FT 的去偏方法相比，ICL 对 PLM 造成的变化更小。因此，我们假设在预训练模型和 FT 模型中观察到的差距对于使用 ICL 的去偏方法并不成立。在这项研究中，我们证明，与基于 FT 的方法相比，基于 ICL 的去偏差方法显示出内在和外在偏差分数之间更高的相关性。此外，与 FT 情况相比，ICL 情况下由于去偏置导致的性能下降也较低。</li>
</ul>

<h3>Title: Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN  Expressiveness</h3>
<ul>
<li><strong>Authors: </strong>Bohang Zhang, Jingchu Gai, Yiheng Du, Qiwei Ye, Di He, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM, cs.DS, math.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08514">https://arxiv.org/abs/2401.08514</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08514">https://arxiv.org/pdf/2401.08514</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08514]] Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN  Expressiveness(https://arxiv.org/abs/2401.08514)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a unified framework for quantitatively studying the expressiveness of GNN architectures, addressing all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes of prominent GNNs as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide novel insights into a series of previous work, unify the landscape of different subareas in the community, and settle several open questions. Empirically, extensive experiments on both synthetic and real-world tasks verify our theory, showing that the practical performance of GNN models aligns well with the proposed metric.</li>
<li><strong>摘要：</strong>设计富有表现力的图神经网络（GNN）是图学习社区的一个基本主题。到目前为止，GNN 的表达能力主要通过 Weisfeiler-Lehman (WL) 层次结构进行评估。然而，这种表达性度量具有显着的局限性：它本质上是粗糙的、定性的，并且可能不能很好地反映实际需求（例如，编码子结构的能力）。在本文中，我们引入了一个统一的框架来定量研究 GNN 架构的表达能力，解决上述所有局限性。具体来说，我们确定了一种称为同态表达性的基本表达性度量，它量化了 GNN 模型在同态下计算图的能力。同态表现力提供了一个完整且实用的评估工具：完整性使得能够在 GNN 模型之间进行直接的表现力比较，而实用性则允许理解具体的 GNN 能力，例如子图计数。通过研究四类著名的 GNN 作为案例研究，我们得出了它们在不变和等变设置下的同态表达性的简单、统一和优雅的描述。我们的结果为一系列先前的工作提供了新颖的见解，统一了社区中不同分区的景观，并解决了几个悬而未决的问题。根据经验，对合成任务和现实世界任务的大量实验验证了我们的理论，表明 GNN 模型的实际性能与所提出的指标非常吻合。</li>
</ul>

<h3>Title: Supporting Student Decisions on Learning Recommendations: An LLM-Based  Chatbot with Knowledge Graph Contextualization for Conversational  Explainability and Mentoring</h3>
<ul>
<li><strong>Authors: </strong>Hasan Abu-Rasheed, Mohamad Hussam Abdulsalam, Christian Weber, Madjid Fathi</a></li>
<li><strong>Subjects: </strong>cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08517">https://arxiv.org/abs/2401.08517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08517">https://arxiv.org/pdf/2401.08517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08517]] Supporting Student Decisions on Learning Recommendations: An LLM-Based  Chatbot with Knowledge Graph Contextualization for Conversational  Explainability and Mentoring(https://arxiv.org/abs/2401.08517)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM's output through defining its prompt's context. A group chat approach is developed to connect students with human mentors, either on demand or in cases that exceed the chatbot's pre-defined tasks. We evaluate the chatbot with a user study, to provide a proof-of-concept and highlight the potential requirements and limitations of utilizing chatbots in conversational explainability.</li>
<li><strong>摘要：</strong>学生对学习推荐的承诺与他们对学习推荐原因的理解是分不开的；以及他们根据这种理解修改它的能力。在可解释性方法中，聊天机器人提供了让学生参与对话的潜力，类似于与同伴或导师的讨论。然而，尽管生成式人工智能（GenAI）和大型语言模型（LLM）取得了进步，但聊天机器人的能力仍然不足以取代人类导师。因此，我们提出了一种利用聊天机器人作为对话中介以及有限和受控解释生成来源的方法，以挖掘法学硕士的潜力，同时降低其潜在风险。拟议的基于法学硕士的聊天机器人支持学生理解学习路径建议。我们使用知识图（KG）作为人工管理的信息源，通过定义其提示的上下文来调节法学硕士的输出。开发了一种群聊方法，可以根据需要或在超出聊天机器人预定义任务的情况下将学生与人类导师联系起来。我们通过用户研究来评估聊天机器人，以提供概念验证并强调在对话可解释性中使用聊天机器人的潜在要求和局限性。</li>
</ul>

<h3>Title: Explaining Time Series via Contrastive and Locally Sparse Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Zichuan Liu, Yingying Zhang, Tianchun Wang, Zefan Wang, Dongsheng Luo, Mengnan Du, Min Wu, Yi Wang, Chunlin Chen, Lunting Fan, Qingsong Wen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08552">https://arxiv.org/abs/2401.08552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08552">https://arxiv.org/pdf/2401.08552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08552]] Explaining Time Series via Contrastive and Locally Sparse Perturbations(https://arxiv.org/abs/2401.08552)</code><input type="text"></li>
<li><strong>Keywords: </strong>code</a></li>
<li><strong>Abstract: </strong>Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data. The code is available for review: https://anonymous.4open.science/r/ContraLSP-1146/</li>
<li><strong>摘要：</strong>解释多元时间序列是一项复合挑战，因为它需要识别时间序列中的重要位置并匹配复杂的时间模式。尽管以前的基于显着性的方法解决了这些挑战，但它们的扰动可能无法缓解分布偏移问题，这在异质样本中是不可避免的。我们提出了 ContraLSP，一种局部稀疏模型，它引入反事实样本来构建无信息的扰动，但使用对比学习来保持分布。此外，我们结合了特定于样本的稀疏门来生成更多二元倾斜和平滑的掩模，这可以轻松地整合时间趋势并简约地选择显着特征。对合成数据集和真实数据集的实证研究表明，ContraLSP 的性能优于最先进的模型，证明时间序列数据的解释质量有了显着提高。该代码可供审查：https://anonymous.4open.science/r/ContraLSP-1146/</li>
</ul>

<h3>Title: Tuning Language Models by Proxy</h3>
<ul>
<li><strong>Authors: </strong>Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, Noah A. Smith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08565">https://arxiv.org/abs/2401.08565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08565">https://arxiv.org/pdf/2401.08565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08565]] Tuning Language Models by Proxy(https://arxiv.org/abs/2401.08565)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, code, chat</a></li>
<li><strong>Abstract: </strong>Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing only its prediction over the output vocabulary. Our method instead tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.</li>
<li><strong>摘要：</strong>尽管大型预训练语言模型具有一般功能，但它们始终受益于进一步的适应，以更好地实现所需的行为。然而，调整这些模型已经变得越来越资源密集，或者当模型权重是私有的时候是不可能的。我们引入了代理调优，这是一种轻量级解码时算法，它在黑盒 LM 之上运行，以实现直接调优模型的结果，但仅访问其对输出词汇表的预测。相反，我们的方法调整较小的 LM，然后应用小调整和未调整 LM 的预测之间的差异，将基础模型的原始预测向调整方向移动，同时保留更大规模预训练的优势。在实验中，当我们使用仅 7B 大小的代理对 Llama2-70B 进行代理调整时，在跨知识、推理和安全基准进行评估时，我们可以缩小 Llama2-70B 与其真正调整的聊天版本之间 88% 的差距。有趣的是，当在 TruthfulQA 上进行测试时，代理调整的模型实际上比直接调整的模型更真实，可能是因为解码时指导更好地保留了模型的事实知识。然后，我们通过将代理调优应用于代码的域适应以及问答和数学问题的特定于任务的微调来展示代理调优的通用性。我们的工作展示了使用小型调谐 LM 通过解码时间指导来高效定制大型、可能专有的 LM 的前景。</li>
</ul>

<h3>Title: Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal  Data</h3>
<ul>
<li><strong>Authors: </strong>Yuhui Zhang, Elaine Sui, Serena Yeung-Levy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08567">https://arxiv.org/abs/2401.08567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08567">https://arxiv.org/pdf/2401.08567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08567]] Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal  Data(https://arxiv.org/abs/2401.08567)</code><input type="text"></li>
<li><strong>Keywords: </strong>rag</a></li>
<li><strong>Abstract: </strong>Building cross-modal applications is challenging due to limited paired multi-modal data. Recent works have shown that leveraging a pre-trained multi-modal contrastive representation space enables cross-modal tasks to be learned from uni-modal data. This is based on the assumption that contrastive optimization makes embeddings from different modalities interchangeable. However, this assumption is under-explored due to the poorly understood geometry of the multi-modal contrastive space, where a modality gap exists. In our study, we provide a theoretical explanation of this space's geometry and introduce a three-step method, $C^3$ (Connect, Collapse, Corrupt), to bridge the modality gap, enhancing the interchangeability of embeddings. Our $C^3$ method significantly improves cross-modal learning from uni-modal data, achieving state-of-the-art results on zero-shot image / audio / video captioning and text-to-image generation.</li>
<li><strong>摘要：</strong>由于配对多模态数据有限，构建跨模态应用程序具有挑战性。最近的工作表明，利用预先训练的多模态对比表示空间可以从单模态数据中学习跨模态任务。这是基于这样的假设：对比优化使得不同模态的嵌入可以互换。然而，由于对存在模态间隙的多模态对比空间的几何形状了解甚少，这一假设尚未得到充分探索。在我们的研究中，我们提供了该空间几何的理论解释，并引入了三步方法 $C^3$（连接、折叠、损坏），以弥合模态差距，增强嵌入的可互换性。我们的 $C^3$ 方法显着改进了单模态数据的跨模态学习，在零样本图像/音频/视频字幕和文本到图像生成方面取得了最先进的结果。</li>
</ul>

<h3>Title: Deductive Closure Training of Language Models for Coherence, Accuracy,  and Updatability</h3>
<ul>
<li><strong>Authors: </strong>Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, Jacob Andreas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.08574">https://arxiv.org/abs/2401.08574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.08574">https://arxiv.org/pdf/2401.08574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.08574]] Deductive Closure Training of Language Models for Coherence, Accuracy,  and Updatability(https://arxiv.org/abs/2401.08574)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, rag</a></li>
<li><strong>Abstract: </strong>While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs' reasoning capabilities during inference can be leveraged during training to improve their reliability.</li>
<li><strong>摘要：</strong>虽然语言模型（LM）有时可以生成事实上正确的文本并估计单个主张的真值，但这些通常不能反映全球一致的、可操纵的世界模型。因此，当前的 LM 也会生成不正确或无意义的内容，并且难以编辑和更新。我们提出了一种称为演绎闭合训练（DCT）的方法，该方法使用语言模型本身来识别它们生成的文本的含义（以及其中的矛盾），从而产生一种有效的自我监督程序来提高语言模型的真实性。给定一组种子文档，DCT 会提示 LM 生成这些文档隐含的附加文本，对生成的文本的正确性进行全局推理，最后对推断正确的文本进行微调。给定来自可信来源的种子文档，DCT 提供了监督模型更新的工具；如果种子文档是从 LM 本身采样的，DCT 可以实现完全无监督的微调，以提高一致性和准确性。在 CREAK、MQUaKE 和 Reversal Curse 数据集上，有监督 DCT 将 LM 事实验证和文本生成准确性提高了 3-26%；在 CREAK 上完全无监督的 DCT 将验证精度提高了 12%。这些结果表明，语言模型在推理过程中的推理能力可以在训练过程中得到利用，以提高其可靠性。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
