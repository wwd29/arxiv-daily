<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-09</h1>
<h3>Title: Persona Setting Pitfall: Persistent Outgroup Biases in Large Language Models Arising from Social Identity Adoption</h3>
<ul>
<li><strong>Authors: </strong>Wenchao Dong, Assem Zhunis, Dongyoung Jeong, Hyojin Chin, Jiyoung Han, Meeyoung Cha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.03843">https://arxiv.org/abs/2409.03843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.03843">https://arxiv.org/pdf/2409.03843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.03843]] Persona Setting Pitfall: Persistent Outgroup Biases in Large Language Models Arising from Social Identity Adoption(https://arxiv.org/abs/2409.03843)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Drawing parallels between human cognition and artificial intelligence, we explored how large language models (LLMs) internalize identities imposed by targeted prompts. Informed by Social Identity Theory, these identity assignments lead LLMs to distinguish between "we" (the ingroup) and "they" (the outgroup). This self-categorization generates both ingroup favoritism and outgroup bias. Nonetheless, existing literature has predominantly focused on ingroup favoritism, often overlooking outgroup bias, which is a fundamental source of intergroup prejudice and discrimination. Our experiment addresses this gap by demonstrating that outgroup bias manifests as strongly as ingroup favoritism. Furthermore, we successfully mitigated the inherent pro-liberal, anti-conservative bias in LLMs by guiding them to adopt the perspectives of the initially disfavored group. These results were replicated in the context of gender bias. Our findings highlight the potential to develop more equitable and balanced language models.</li>
<li><strong>摘要：</strong>通过将人类认知与人工智能进行比较，我们探索了大型语言模型 (LLM) 如何内化有针对性的提示所强加的身份。根据社会身份理论，这些身份分配使 LLM 能够区分“我们”（内群体）和“他们”（外群体）。这种自我分类会产生内群体偏袒和外群体偏见。尽管如此，现有文献主要关注内群体偏袒，往往忽视外群体偏见，而外群体偏见是群体间偏见和歧视的根本原因。我们的实验通过证明外群体偏见与内群体偏袒一样强烈地表现出来，解决了这一差距。此外，我们通过引导 LLM 采用最初不受青睐的群体的观点，成功地减轻了他们固有的亲自由主义、反保守主义偏见。这些结果在性别偏见的背景下得到了复制。我们的研究结果强调了开发更公平、更平衡的语言模型的潜力。</li>
</ul>

<h3>Title: Sirius: Contextual Sparsity with Correction for Efficient LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Zhuoming Chen, Zhaozhuo Xu, Victoria Lin, Beidi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.03856">https://arxiv.org/abs/2409.03856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.03856">https://arxiv.org/pdf/2409.03856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.03856]] Sirius: Contextual Sparsity with Correction for Efficient LLMs(https://arxiv.org/abs/2409.03856)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>With the blossom of large language models (LLMs), inference efficiency becomes increasingly important. Various approximation methods are proposed to reduce the cost at inference time. Contextual Sparsity (CS) is appealing for its training-free nature and its ability to reach a higher compression ratio seemingly without quality degradation. However, after a comprehensive evaluation of contextual sparsity methods on various complex generation tasks, we find that although CS succeeds in prompt-understanding tasks, CS significantly degrades the model performance for reasoning, deduction, and knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that sparse models often share general problem-solving logic and require only a few token corrections to recover the original model performance. This paper introduces Sirius, an efficient correction mechanism, which significantly recovers CS models quality on reasoning tasks while maintaining its efficiency gain. Sirius is evaluated on 6 models with 8 difficult generation tasks in reasoning, math, and coding and shows consistent effectiveness and efficiency. Also, we carefully develop a system implementation for Sirius and show that Sirius achieves roughly 20% reduction in latency for 8B model on-chip and 35% reduction for 70B model offloading. We open-source our implementation of Sirius at this https URL.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的蓬勃发展，推理效率变得越来越重要。提出了各种近似方法来降低推理时的成本。上下文稀疏性 (CS) 因其无需训练的特性以及在不降低质量的情况下达到更高压缩比的能力而备受青睐。然而，在对各种复杂生成任务上的上下文稀疏性方法进行全面评估后，我们发现尽管 CS 在快速理解任务中取得了成功，但 CS 显著降低了推理、推理和基于知识的任务的模型性能。尽管端到端准确性存在差距，但我们观察到稀疏模型通常共享一般的问题解决逻辑，只需要少量标记校正即可恢复原始模型性能。本文介绍了一种有效的校正机制 Sirius，它在保持其效率增益的同时显著恢复了 CS 模型在推理任务上的质量。Sirius 在 6 个模型上进行了评估，其中包含 8 个推理、数学和编码方面的困难生成任务，并显示出一致的有效性和效率。此外，我们精心开发了 Sirius 的系统实现，并表明 Sirius 可将 8B 型号片上延迟减少约 20%，将 70B 型号卸载延迟减少 35%。我们在此 https URL 上开源了 Sirius 实现。</li>
</ul>

<h3>Title: CACER: Clinical Concept Annotations for Cancer Events and Relations</h3>
<ul>
<li><strong>Authors: </strong>Yujuan Fu, Giridhar Kaushik Ramachandran, Ahmad Halwani, Bridget T. McInnes, Fei Xia, Kevin Lybarger, Meliha Yetisgen, Özlem Uzuner</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.03905">https://arxiv.org/abs/2409.03905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.03905">https://arxiv.org/pdf/2409.03905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.03905]] CACER: Clinical Concept Annotations for Cancer Events and Relations(https://arxiv.org/abs/2409.03905)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Clinical notes contain unstructured representations of patient histories, including the relationships between medical problems and prescription drugs. To investigate the relationship between cancer drugs and their associated symptom burden, we extract structured, semantic representations of medical problem and drug information from the clinical narratives of oncology notes. We present Clinical Concept Annotations for Cancer Events and Relations (CACER), a novel corpus with fine-grained annotations for over 48,000 medical problems and drug events and 10,000 drug-problem and problem-problem relations. Leveraging CACER, we develop and evaluate transformer-based information extraction (IE) models such as BERT, Flan-T5, Llama3, and GPT-4 using fine-tuning and in-context learning (ICL). In event extraction, the fine-tuned BERT and Llama3 models achieved the highest performance at 88.2-88.0 F1, which is comparable to the inter-annotator agreement (IAA) of 88.4 F1. In relation extraction, the fine-tuned BERT, Flan-T5, and Llama3 achieved the highest performance at 61.8-65.3 F1. GPT-4 with ICL achieved the worst performance across both tasks. The fine-tuned models significantly outperformed GPT-4 in ICL, highlighting the importance of annotated training data and model optimization. Furthermore, the BERT models performed similarly to Llama3. For our task, LLMs offer no performance advantage over the smaller BERT models. The results emphasize the need for annotated training data to optimize models. Multiple fine-tuned transformer models achieved performance comparable to IAA for several extraction tasks.</li>
<li><strong>摘要：</strong>临床笔记包含患者病史的非结构化表示，包括医疗问题与处方药之间的关系。为了研究癌症药物与其相关症状负担之间的关系，我们从肿瘤学笔记的临床叙述中提取了医疗问题和药物信息的结构化语义表示。我们提出了癌症事件和关系的临床概念注释 (CACER)，这是一个新的语料库，其中包含超过 48,000 个医疗问题和药物事件以及 10,000 个药物问题和问题问题关系的细粒度注释。利用 CACER，我们使用微调和上下文学习 (ICL) 开发和评估基于转换器的信息提取 (IE) 模型，例如 BERT、Flan-T5、Llama3 和 GPT-4。在事件提取中，经过微调的 BERT 和 Llama3 模型取得了 88.2-88.0 F1 的最高性能，这与 88.4 F1 的注释者间一致性 (IAA) 相当。在关系提取中，经过微调的 BERT、Flan-T5 和 Llama3 取得了 61.8-65.3 F1 的最高性能。带有 ICL 的 GPT-4 在两个任务中的表现最差。经过微调的模型在 ICL 中的表现明显优于 GPT-4，凸显了带注释的训练数据和模型优化的重要性。此外，BERT 模型的表现与 Llama3 相似。对于我们的任务，LLM 并不比较小的 BERT 模型具有性能优势。结果强调了对带注释的训练数据进行优化模型的必要性。多个经过微调的 Transformer 模型在几个提取任务中实现了与 IAA 相当的性能。</li>
</ul>

<h3>Title: Experimentation in Content Moderation using RWKV</h3>
<ul>
<li><strong>Authors: </strong>Umut Yildirim, Rohan Dutta, Burak Yildirim, Atharva Vaidya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.03939">https://arxiv.org/abs/2409.03939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.03939">https://arxiv.org/pdf/2409.03939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.03939]] Experimentation in Content Moderation using RWKV(https://arxiv.org/abs/2409.03939)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper investigates the RWKV model's efficacy in content moderation through targeted experimentation. We introduce a novel dataset specifically designed for distillation into smaller models, enhancing content moderation practices. This comprehensive dataset encompasses images, videos, sounds, and text data that present societal challenges. Leveraging advanced Large Language Models (LLMs), we generated an extensive set of responses -- 558,958 for text and 83,625 for images -- to train and refine content moderation systems. Our core experimentation involved fine-tuning the RWKV model, capitalizing on its CPU-efficient architecture to address large-scale content moderation tasks. By highlighting the dataset's potential for knowledge distillation, this study not only demonstrates RWKV's capability in improving the accuracy and efficiency of content moderation systems but also paves the way for developing more compact, resource-efficient models in this domain. Datasets and models can be found in HuggingFace: this https URL</li>
<li><strong>摘要：</strong>本文通过有针对性的实验研究了 RWKV 模型在内容审核中的有效性。我们引入了一个专门设计用于提炼成较小模型的新数据集，以增强内容审核实践。这个综合数据集涵盖了代表社会挑战的图像、视频、声音和文本数据。利用先进的大型语言模型 (LLM)，我们生成了一组广泛的响应（文本 558,958 个，图像 83,625 个）来训练和改进内容审核系统。我们的核心实验涉及微调 RWKV 模型，利用其 CPU 高效的架构来解决大规模内容审核任务。通过强调数据集的知识提炼潜力，这项研究不仅展示了 RWKV 在提高内容审核系统的准确性和效率方面的能力，而且还为开发更紧凑、资源高效的模型铺平了道路。数据集和模型可以在 HuggingFace 中找到：此 https URL</li>
</ul>

<h3>Title: On The Role of Prompt Construction In Enhancing Efficacy and Efficiency of LLM-Based Tabular Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Banooqa Banday, Kowshik Thopalli, Tanzima Z. Islam, Jayaraman J. Thiagarajan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.03946">https://arxiv.org/abs/2409.03946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.03946">https://arxiv.org/pdf/2409.03946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.03946]] On The Role of Prompt Construction In Enhancing Efficacy and Efficiency of LLM-Based Tabular Data Generation(https://arxiv.org/abs/2409.03946)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>LLM-based data generation for real-world tabular data can be challenged by the lack of sufficient semantic context in feature names used to describe columns. We hypothesize that enriching prompts with domain-specific insights can improve both the quality and efficiency of data generation. To test this hypothesis, we explore three prompt construction protocols: Expert-guided, LLM-guided, and Novel-Mapping. Through empirical studies with the recently proposed GReaT framework, we find that context-enriched prompts lead to significantly improved data generation quality and training efficiency.</li>
<li><strong>摘要：</strong>基于 LLM 的真实表格数据生成可能面临一个挑战，即用于描述列的特征名称缺乏足够的语义上下文。我们假设，通过丰富特定领域的见解来丰富提示可以提高数据生成的质量和效率。为了验证这一假设，我们探索了三种提示构建协议：专家指导、LLM 指导和 Novel-Mapping。通过对最近提出的 GReaT 框架进行实证研究，我们发现上下文丰富的提示可以显著提高数据生成质量和训练效率。</li>
</ul>

<h3>Title: Towards Safer Online Spaces: Simulating and Assessing Intervention Strategies for Eating Disorder Discussions</h3>
<ul>
<li><strong>Authors: </strong>Louis Penafiel, Hsien-Te Kao, Isabel Erickson, David Chu, Robert McCormack, Kristina Lerman, Svitlana Volkova</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04043">https://arxiv.org/abs/2409.04043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04043">https://arxiv.org/pdf/2409.04043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04043]] Towards Safer Online Spaces: Simulating and Assessing Intervention Strategies for Eating Disorder Discussions(https://arxiv.org/abs/2409.04043)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Eating disorders are complex mental health conditions that affect millions of people around the world. Effective interventions on social media platforms are crucial, yet testing strategies in situ can be risky. We present a novel LLM-driven experimental testbed for simulating and assessing intervention strategies in ED-related discussions. Our framework generates synthetic conversations across multiple platforms, models, and ED-related topics, allowing for controlled experimentation with diverse intervention approaches. We analyze the impact of various intervention strategies on conversation dynamics across four dimensions: intervention type, generative model, social media platform, and ED-related community/topic. We employ cognitive domain analysis metrics, including sentiment, emotions, etc., to evaluate the effectiveness of interventions. Our findings reveal that civility-focused interventions consistently improve positive sentiment and emotional tone across all dimensions, while insight-resetting approaches tend to increase negative emotions. We also uncover significant biases in LLM-generated conversations, with cognitive metrics varying notably between models (Claude-3 Haiku $>$ Mistral $>$ GPT-3.5-turbo $>$ LLaMA3) and even between versions of the same model. These variations highlight the importance of model selection in simulating realistic discussions related to ED. Our work provides valuable information on the complex dynamics of ED-related discussions and the effectiveness of various intervention strategies.</li>
<li><strong>摘要：</strong>饮食失调是一种复杂的心理健康状况，影响着全世界数百万人。在社交媒体平台上进行有效的干预至关重要，但在现场测试策略可能会有风险。我们提出了一种新颖的 LLM 驱动实验测试平台，用于模拟和评估 ED 相关讨论中的干预策略。我们的框架会在多个平台、模型和 ED 相关主题上生成合成对话，从而允许对各种干预方法进行受控实验。我们分析了各种干预策略对四个维度的对话动态的影响：干预类型、生成模型、社交媒体平台和 ED 相关社区/主题。我们使用认知领域分析指标（包括情绪、情感等）来评估干预的有效性。我们的研究结果表明，以文明为重点的干预措施始终如一地改善各个维度的积极情绪和情绪基调，而洞察力重置方法往往会增加负面情绪。我们还发现 LLM 生成的对话中存在显著的偏见，认知指标在模型之间（Claude-3 Haiku $>$ Mistral $>$ GPT-3.5-turbo $>$ LLaMA3）甚至同一模型的不同版本之间都存在显著差异。这些差异凸显了模型选择在模拟与 ED 相关的现实讨论中的重要性。我们的工作为 ED 相关讨论的复杂动态和各种干预策略的有效性提供了宝贵的信息。</li>
</ul>

<h3>Title: Self-Harmonized Chain of Thought</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Jin, Wei Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04057">https://arxiv.org/abs/2409.04057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04057">https://arxiv.org/pdf/2409.04057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04057]] Self-Harmonized Chain of Thought(https://arxiv.org/abs/2409.04057)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) prompting reveals that large language models are capable of performing complex reasoning via intermediate steps. CoT prompting is primarily categorized into three approaches. The first approach utilizes straightforward prompts like ``Let's think step by step'' to generate a sequential thought process before yielding an answer. The second approach makes use of human-crafted, step-by-step demonstrations to guide the model's reasoning process. The third automates the generation of reasoned demonstrations with the 'Let's think step by step'.This approach sometimes leads to reasoning errors, highlighting the need to diversify demonstrations to mitigate its misleading effects. However, diverse demonstrations pose challenges for effective representations. In this work, we propose ECHO, a self-harmonized chain-of-thought prompting method. It consolidates diverse solution paths into a uniform and effective solution pattern.ECHO demonstrates the best overall performance across three reasoning domains.</li>
<li><strong>摘要：</strong>思路链 (CoT) 提示表明大型语言模型能够通过中间步骤执行复杂的推理。CoT 提示主要分为三种方法。第一种方法利用“让我们一步一步思考”等简单的提示来生成连续的思维过程，然后再得出答案。第二种方法利用人工设计的逐步演示来指导模型的推理过程。第三种方法使用“让我们一步一步思考”自动生成推理演示。这种方法有时会导致推理错误，强调需要多样化演示以减轻其误导性影响。然而，多样化的演示对有效的表示提出了挑战。在这项工作中，我们提出了一种自我协调的思路链提示方法 ECHO。它将不同的解决方案路径整合为统一有效的解决方案模式。ECHO 在三个推理领域表现出最佳的整体性能。</li>
</ul>

<h3>Title: AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language Model</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Zhang, Paul Groth, Iacer Calixto, Sebastian Schelter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04073">https://arxiv.org/abs/2409.04073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04073">https://arxiv.org/pdf/2409.04073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04073]] AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language Model(https://arxiv.org/abs/2409.04073)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Entity matching (EM) is the problem of determining whether two records refer to same real-world entity, which is crucial in data integration, e.g., for product catalogs or address databases. A major drawback of many EM approaches is their dependence on labelled examples. We thus focus on the challenging setting of zero-shot entity matching where no labelled examples are available for an unseen target dataset. Recently, large language models (LLMs) have shown promising results for zero-shot EM, but their low throughput and high deployment cost limit their applicability and scalability. We revisit the zero-shot EM problem with AnyMatch, a small language model fine-tuned in a transfer learning setup. We propose several novel data selection techniques to generate fine-tuning data for our model, e.g., by selecting difficult pairs to match via an AutoML filter, by generating additional attribute-level examples, and by controlling label imbalance in the data. We conduct an extensive evaluation of the prediction quality and deployment cost of our model, in a comparison to thirteen baselines on nine benchmark datasets. We find that AnyMatch provides competitive prediction quality despite its small parameter size: it achieves the second-highest F1 score overall, and outperforms several other approaches that employ models with hundreds of billions of parameters. Furthermore, our approach exhibits major cost benefits: the average prediction quality of AnyMatch is within 4.4% of the state-of-the-art method MatchGPT with the proprietary trillion-parameter model GPT-4, yet AnyMatch requires four orders of magnitude less parameters and incurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).</li>
<li><strong>摘要：</strong>实体匹配 (EM) 是确定两个记录是否指向同一个现实世界实体的问题，这在数据集成中至关重要，例如，对于产品目录或地址数据库。许多 EM 方法的主要缺点是它们依赖于标记示例。因此，我们专注于零样本实体匹配的挑战性设置，其中没有可用于未见目标数据集的标记示例。最近，大型语言模型 (LLM) 在零样本 EM 中显示出有希望的结果，但它们的低吞吐量和高部署成本限制了它们的适用性和可扩展性。我们使用 AnyMatch（一种在迁移学习设置中微调的小型语言模型）重新审视零样本 EM 问题。我们提出了几种新颖的数据选择技术来为我们的模型生成微调数据，例如，通过 AutoML 过滤器选择难以匹配的对，通过生成额外的属性级示例，以及通过控制数据中的标签不平衡。我们对模型的预测质量和部署成本进行了广泛的评估，并与九个基准数据集上的十三个基线进行了比较。我们发现，尽管 AnyMatch 的参数规模很小，但它的预测质量却极具竞争力：总体而言，它的 F1 得分排名第二，并且优于其他几种采用具有数千亿个参数的模型的方法。此外，我们的方法还具有巨大的成本效益：AnyMatch 的平均预测质量与最先进的方法 MatchGPT（采用专有的万亿参数模型 GPT-4）相差 4.4% 以内，但 AnyMatch 所需的参数数量少了四个数量级，推理成本降低了 3,899 倍（以每 1,000 个 token 计算）。</li>
</ul>

<h3>Title: UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Fu, Raviteja Anantha, Prabal Vashisht, Jianpeng Cheng, Etai Littwin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04081">https://arxiv.org/abs/2409.04081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04081">https://arxiv.org/pdf/2409.04081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04081]] UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity(https://arxiv.org/abs/2409.04081)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Generating user intent from a sequence of user interface (UI) actions is a core challenge in comprehensive UI understanding. Recent advancements in multimodal large language models (MLLMs) have led to substantial progress in this area, but their demands for extensive model parameters, computing power, and high latency makes them impractical for scenarios requiring lightweight, on-device solutions with low latency or heightened privacy. Additionally, the lack of high-quality datasets has hindered the development of such lightweight models. To address these challenges, we propose UI-JEPA, a novel framework that employs masking strategies to learn abstract UI embeddings from unlabeled data through self-supervised learning, combined with an LLM decoder fine-tuned for user intent prediction. We also introduce two new UI-grounded multimodal datasets, "Intent in the Wild" (IIW) and "Intent in the Tame" (IIT), designed for few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos across 219 intent categories, while IIT contains 914 videos across 10 categories. We establish the first baselines for these datasets, showing that representations learned using a JEPA-style objective, combined with an LLM decoder, can achieve user intent predictions that match the performance of state-of-the-art large MLLMs, but with significantly reduced annotation and deployment resources. Measured by intent similarity scores, UI-JEPA outperforms GPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged across two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x reduction in computational cost and a 6.6x improvement in latency in the IIW dataset. These results underscore the effectiveness of UI-JEPA, highlighting its potential for lightweight, high-performance UI understanding.</li>
<li><strong>摘要：</strong>从一系列用户界面 (UI) 操作中生成用户意图是全面理解 UI 的核心挑战。多模态大型语言模型 (MLLM) 的最新进展已导致该领域取得了实质性进展，但它们对大量模型参数、计算能力和高延迟的要求使其不适用于需要轻量级、低延迟或高度隐私的设备解决方案的场景。此外，缺乏高质量的数据集阻碍了此类轻量级模型的开发。为了应对这些挑战，我们提出了 UI-JEPA，这是一个新颖的框架，它采用掩蔽策略通过自监督学习从未标记数据中学习抽象的 UI 嵌入，并结合针对用户意图预测进行微调的 LLM 解码器。我们还引入了两个新的基于 UI 的多模态数据集，“Intent in the Wild”（IIW）和“Intent in the Tame”（IIT），专为少样本和零样本 UI 理解任务而设计。IIW 包含 219 个意图类别的 1.7K 个视频，而 IIT 包含 10 个类别的 914 个视频。我们为这些数据集建立了第一个基线，表明使用 JEPA 样式目标与 LLM 解码器相结合学习的表示可以实现与最先进的大型 MLLM 性能相匹配的用户意图预测，但注释和部署资源显著减少。以意图相似度得分衡量，UI-JEPA 在两个数据集的平均表现分别比 GPT-4 Turbo 和 Claude 3.5 Sonnet 高出 10.0% 和 7.2%。值得注意的是，UI-JEPA 在 IIW 数据集中将计算成本降低了 50.5 倍，延迟提高了 6.6 倍，从而实现了这一性能。这些结果强调了 UI-JEPA 的有效性，凸显了其在轻量级、高性能 UI 理解方面的潜力。</li>
</ul>

<h3>Title: Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers</h3>
<ul>
<li><strong>Authors: </strong>Chenglei Si, Diyi Yang, Tatsunori Hashimoto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04109">https://arxiv.org/abs/2409.04109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04109">https://arxiv.org/pdf/2409.04109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04109]] Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers(https://arxiv.org/abs/2409.04109)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的最新进展引发了人们对其加速科学发现潜力的乐观情绪，越来越多的研究提出了能够自主生成和验证新想法的研究代理。尽管如此，没有任何评估表明 LLM 系统可以迈出产生新颖的专家级想法的第一步，更不用说执行整个研究过程了。我们通过建立一个实验设计来解决这个问题，该实验设计在控制混杂因素的同时评估研究想法的生成，并在专家 NLP 研究人员和 L​​LM 构思代理之间进行首次正面比较。通过招募 100 多名 NLP 研究人员来撰写新颖的想法并对 LLM 和人类想法进行盲审，我们获得了关于当前 LLM 研究构思能力的第一个具有统计学意义的结论：我们发现 LLM 生成的想法被判定为比人类专家想法更新颖 (p < 0.05)，而在可行性方面被判定为略弱。通过仔细研究我们的代理基线，我们发现在构建和评估研究代理方面存在未解决的问题，包括 LLM 自我评估失败及其生成缺乏多样性。最后，我们承认，即使是专家，人类对新颖性的判断也是困难的，并提出了一种端到端的研究设计，招募研究人员将这些想法执行到完整的项目中，使我们能够研究这些新颖性和可行性判断是否会导致研究结果的重大差异。</li>
</ul>

<h3>Title: Multi-Programming Language Ensemble for Code Generation in Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Tengfei Xue, Xuefeng Li, Tahir Azim, Roman Smirnov, Jianhui Yu, Arash Sadrieh, Babak Pahlavan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04114">https://arxiv.org/abs/2409.04114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04114">https://arxiv.org/pdf/2409.04114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04114]] Multi-Programming Language Ensemble for Code Generation in Large Language Model(https://arxiv.org/abs/2409.04114)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly improved code generation, particularly in one-pass code generation. However, most existing approaches focus solely on generating code in a single programming language, overlooking the potential of leveraging the multi-language capabilities of LLMs. LLMs have varying patterns of errors across different languages, suggesting that a more robust approach could be developed by leveraging these multi-language outputs. In this study, we propose Multi-Programming Language Ensemble (MPLE), a novel ensemble-based method that utilizes code generation across multiple programming languages to enhance overall performance. By treating each language-specific code generation process as an individual "weak expert" and effectively integrating their outputs, our method mitigates language-specific errors and biases. This multi-language ensemble strategy leverages the complementary strengths of different programming languages, enabling the model to produce more accurate and robust code. Our approach can be seamlessly integrated with commonly used techniques such as the reflection algorithm and Monte Carlo tree search to improve code generation quality further. Experimental results show that our framework consistently enhances baseline performance by up to 17.92% on existing benchmarks (HumanEval and HumanEval-plus), with a standout result of 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art results across various LLM models. The code will be released at this https URL</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 显著改善了代码生成，尤其是在一次性代码生成中。然而，大多数现有方法仅侧重于用单一编程语言生成代码，而忽略了利用 LLM 的多语言功能的潜力。LLM 在不同语言中具有不同的错误模式，这表明可以通过利用这些多语言输出来开发更强大的方法。在本研究中，我们提出了多编程语言集成 (MPLE)，这是一种基于集成的新型方法，它利用多种编程语言的代码生成来提高整体性能。通过将每种特定于语言的代码生成过程视为单独的“弱专家”并有效地整合它们的输出，我们的方法可以减轻特定于语言的错误和偏差。这种多语言集成策略利用了不同编程语言的互补优势，使模型能够生成更准确、更强大的代码。我们的方法可以与反射算法和蒙特卡洛树搜索等常用技术无缝集成，以进一步提高代码生成质量。实验结果表明，我们的框架在现有基准测试（HumanEval 和 HumanEval-plus）上持续将基准性能提高高达 17.92%，在 HumanEval 基准测试中取得了 96.25% 的出色准确率，在各种 LLM 模型中取得了新的最先进结果。代码将在此 https URL 上发布</li>
</ul>

<h3>Title: Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering</h3>
<ul>
<li><strong>Authors: </strong>Jan Hofmann, Cornelia Sindermann, Roman Klinger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04122">https://arxiv.org/abs/2409.04122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04122">https://arxiv.org/pdf/2409.04122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04122]] Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering(https://arxiv.org/abs/2409.04122)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Author profiling is the task of inferring characteristics about individuals by analyzing content they share. Supervised machine learning still dominates automatic systems that perform this task, despite the popularity of prompting large language models to address natural language understanding tasks. One reason is that the classification instances consist of large amounts of posts, potentially a whole user profile, which may exceed the input length of Transformers. Even if a model can use a large context window, the entirety of posts makes the application of API-accessed black box systems costly and slow, next to issues which come with such "needle-in-the-haystack" tasks. To mitigate this limitation, we propose a new method for author profiling which aims at distinguishing relevant from irrelevant content first, followed by the actual user profiling only with relevant data. To circumvent the need for relevance-annotated data, we optimize this relevance filter via reinforcement learning with a reward function that utilizes the zero-shot capabilities of large language models. We evaluate our method for Big Five personality trait prediction on two Twitter corpora. On publicly available real-world data with a skewed label distribution, our method shows similar efficacy to using all posts in a user profile, but with a substantially shorter context. An evaluation on a version of these data balanced with artificial posts shows that the filtering to relevant posts leads to a significantly improved accuracy of the predictions.</li>
<li><strong>摘要：</strong>作者分析是通过分析个人分享的内容来推断其特征的任务。尽管使用大型语言模型来解决自然语言理解任务非常流行，但监督式机器学习仍然在执行此任务的自动系统中占据主导地位。原因之一是分类实例由大量帖子组成，可能是整个用户资料，这可能会超过 Transformers 的输入长度。即使模型可以使用大型上下文窗口，但整个帖子也会使 API 访问的黑盒系统的应用成本高昂且速度慢，此外还有这种“大海捞针”任务带来的问题。为了缓解这一限制，我们提出了一种新的作者分析方法，旨在首先区分相关内容和不相关内容，然后仅使用相关数据进行实际用户分析。为了避免对相关性注释数据的需求，我们通过强化学习优化了此相关性过滤器，并使用奖励函数，该函数利用大型语言模型的零样本能力。我们在两个 Twitter 语料库上评估了我们的大五人格特质预测方法。对于标签分布不均的公开现实世界数据，我们的方法与使用用户个人资料中的所有帖子效果相似，但上下文要短得多。对这些数据与人工帖子平衡的版本的评估表明，过滤相关帖子可显著提高预测的准确性。</li>
</ul>

<h3>Title: Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language Models for Text-to-Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Luis Mayer, Christian Heumann, Matthias Aßenmacher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04164">https://arxiv.org/abs/2409.04164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04164">https://arxiv.org/pdf/2409.04164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04164]] Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language Models for Text-to-Code Generation(https://arxiv.org/abs/2409.04164)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>In recent years, large language models (LLMs) have emerged as powerful tools with potential applications in various fields, including software engineering. Within the scope of this research, we evaluate five different state-of-the-art LLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their capabilities for text-to-code generation. In an empirical study, we feed prompts with textual descriptions of coding problems sourced from the programming website LeetCode to the models with the task of creating solutions in Python. Subsequently, the quality of the generated outputs is assessed using the testing functionalities of LeetCode. The results indicate large differences in performance between the investigated models. ChatGPT can handle these typical programming challenges by far the most effectively, surpassing even code-specialized models like Code Llama. To gain further insights, we measure the runtime as well as the memory usage of the generated outputs and compared them to the other code submissions on Leetcode. A detailed error analysis, encompassing a comparison of the differences concerning correct indentation and form of the generated code as well as an assignment of the incorrectly solved tasks to certain error categories allows us to obtain a more nuanced picture of the results and potential for improvement. The results also show a clear pattern of increasingly incorrect produced code when the models are facing a lot of context in the form of longer prompts.</li>
<li><strong>摘要：</strong>近年来，大型语言模型 (LLM) 已成为强大的工具，在软件工程等各个领域都有潜在的应用。在本研究范围内，我们评估了五种不同的最先进的 LLM - Bard、BingChat、ChatGPT、Llama2 和 Code Llama - 关于它们的文本到代码生成能力。在一项实证研究中，我们将来自编程网站 LeetCode 的编码问题的文本描述提示输入到模型，并要求模型用 Python 创建解决方案。随后，使用 LeetCode 的测试功能评估生成的输出的质量。结果表明，所研究模型之间的性能差异很大。ChatGPT 可以最有效地处理这些典型的编程挑战，甚至超越了 Code Llama 等代码专用模型。为了获得进一步的见解，我们测量了生成输出的运行时间和内存使用情况，并将它们与 Leetcode 上的其他代码提交进行了比较。详细的错误分析包括比较生成代码的正确缩进和格式差异，以及将错误解决的任务归类到某些错误类别，这使我们能够更细致地了解结果和改进潜力。结果还显示出，当模型面对较长提示形式的大量上下文时，生成的代码错误会越来越多。</li>
</ul>

<h3>Title: From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks</h3>
<ul>
<li><strong>Authors: </strong>Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, Benjamin Roth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04168">https://arxiv.org/abs/2409.04168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04168">https://arxiv.org/pdf/2409.04168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04168]] From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks(https://arxiv.org/abs/2409.04168)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. LLM judges are typically evaluated by measuring the correlation with human judgments on generation tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that the used judges are mostly unable to improve task performance but are able to pick the better model. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance. We observe that judges tend to choose the model of higher quality even if its answer is incorrect. Further, we show that it is possible to use statistics, such as the task performances of the individual models, to predict judgment performance. In an ablation, we either swap or mask the candidate answers and observe that judges often keep the original judgment, providing evidence that judges incorporate writing style in their judgments. In summary, we find that regularities in the judgments are quantifiable using statistical measures and provide various angles on exploiting them.</li>
<li><strong>摘要：</strong>为了减少对人工注释的需求，大型语言模型 (LLM) 已被提议作为其他候选模型质量的评判者。LLM 评判者通常通过测量与人类判断在生成任务（例如摘要或机器翻译）上的相关性来评估。相比之下，我们研究 LLM 评判者在数学推理任务上的表现。这些任务需要多步推理，并且其解决方案的正确性是可验证的，从而可以进行更客观的评估。我们进行了详细的性能分析，发现使用的评判者大多无法提高任务性能，但能够选择更好的模型。我们的分析揭示了判断性能与候选模型任务性能之间的强相关性。我们观察到，即使答案不正确，评判者也倾向于选择质量更高的模型。此外，我们表明可以使用统计数据（例如各个模型的任务性能）来预测判断性能。在消融中，我们交换或屏蔽候选答案，并观察到评判者经常保留原始判断，这证明评判者在判断中融入了写作风格。总而言之，我们发现判断中的规律可以用统计方法来量化，并提供了利用这些规律的各种角度。</li>
</ul>

<h3>Title: Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Larissa Pusch, Tim O. F. Conrad</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04181">https://arxiv.org/abs/2409.04181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04181">https://arxiv.org/pdf/2409.04181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04181]] Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering(https://arxiv.org/abs/2409.04181)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Advancements in natural language processing have revolutionized the way we can interact with digital information systems, such as databases, making them more accessible. However, challenges persist, especially when accuracy is critical, as in the biomedical domain. A key issue is the hallucination problem, where models generate information unsupported by the underlying data, potentially leading to dangerous misinformation. This paper presents a novel approach designed to bridge this gap by combining Large Language Models (LLM) and Knowledge Graphs (KG) to improve the accuracy and reliability of question-answering systems, on the example of a biomedical KG. Built on the LangChain framework, our method incorporates a query checker that ensures the syntactical and semantic validity of LLM-generated queries, which are then used to extract information from a Knowledge Graph, substantially reducing errors like hallucinations. We evaluated the overall performance using a new benchmark dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other models in generating accurate queries, open-source models like llama3:70b show promise with appropriate prompt engineering. To make this approach accessible, a user-friendly web-based interface has been developed, allowing users to input natural language queries, view generated and corrected Cypher queries, and verify the resulting paths for accuracy. Overall, this hybrid approach effectively addresses common issues such as data gaps and hallucinations, offering a reliable and intuitive solution for question answering systems. The source code for generating the results of this paper and for the user-interface can be found in our Git repository: this https URL</li>
<li><strong>摘要：</strong>自然语言处理技术的进步彻底改变了我们与数据库等数字信息系统交互的方式，使它们更易于访问。然而，挑战依然存在，尤其是在准确性至关重要的情况下，例如在生物医学领域。一个关键问题是幻觉问题，即模型生成的信息不受基础数据支持，可能会导致危险的错误信息。本文以生物医学 KG 为例，介绍了一种旨在弥补这一差距的新方法，即结合大型语言模型 (LLM) 和知识图谱 (KG) 来提高问答系统的准确性和可靠性。我们的方法基于 LangChain 框架，结合了一个查询检查器，可确保 LLM 生成的查询的语法和语义有效性，然后使用这些查询从知识图谱中提取信息，从而大大减少幻觉等错误。我们使用 50 个生物医学问题的新基准数据集评估了整体性能，测试了几个 LLM，包括 GPT-4 Turbo 和 llama3:70b。我们的结果表明，虽然 GPT-4 Turbo 在生成准确查询方面优于其他模型，但像 llama3:70b 这样的开源模型在适当的提示工程下显示出良好的前景。为了使这种方法易于理解，我们开发了一个用户友好的基于 Web 的界面，允许用户输入自然语言查询、查看生成和更正的 Cypher 查询，并验证结果路径的准确性。总体而言，这种混合方法有效地解决了数据缺口和幻觉等常见问题，为问答系统提供了可靠且直观的解决方案。用于生成本文结果和用户界面的源代码可以在我们的 Git 存储库中找到：此 https URL</li>
</ul>

<h3>Title: GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04183">https://arxiv.org/abs/2409.04183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04183">https://arxiv.org/pdf/2409.04183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04183]] GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding(https://arxiv.org/abs/2409.04183)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Programming languages possess rich semantic information such as data flow that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with four different baseline LLMs ranging in size from 350M to 8B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3.</li>
<li><strong>摘要：</strong>编程语言拥有丰富的语义信息，例如数据流，这些信息由图形表示，而无法从源代码的表面形式获得。最近的代码语言模型已经扩展到数十亿个参数，但仅将源代码建模为文本标记，而忽略任何其他结构信息。相反，对代码结构信息进行编码的模型会对 Transformer 架构进行修改，从而限制其规模和与预训练 LLM 的兼容性。在这项工作中，我们利用 GALLa - 图对齐大型语言模型兼顾了两者的优点。GALLa 利用图神经网络和跨模态对齐技术将代码的结构信息注入 LLM，作为微调期间的辅助任务。该框架既与模型无关，也与任务无关，因为它可以应用于任何代码 LLM 的任何代码下游任务，并且仅在训练时才需要来自与微调数据无关的语料库的结构图数据，同时在推理时不会对基线 LLM 产生任何成本。使用四个不同基线 LLM（大小从 350M 到 8B）对五项代码任务进行的实验验证了 GALLa 的有效性，并证明了与基线相比的持续改进，即使对于像 LLaMA3 这样的强大模型也是如此。</li>
</ul>

<h3>Title: Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04318">https://arxiv.org/abs/2409.04318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04318">https://arxiv.org/pdf/2409.04318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04318]] Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs(https://arxiv.org/abs/2409.04318)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can perform regression on real-world datasets and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We argue that this process lies on a spectrum between these two extremes. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on various factors, such as prior knowledge about the tasks and the type and richness of the information provided by the in-context examples. We employ three LLMs and utilize multiple datasets to corroborate the robustness of our findings. Our results shed light on how to engineer prompts to leverage meta-learning from in-context examples and foster knowledge retrieval depending on the problem being addressed.</li>
<li><strong>摘要：</strong>生成式大型语言模型 (LLM) 能够成为情境学习者。然而，情境学习 (ICL) 的底层机制仍然是一个主要的研究问题，关于模型如何利用 ICL 的实验研究结果并不总是一致的。在这项工作中，我们提出了一个评估情境学习机制的框架，我们声称这些机制是检索内部知识和从情境示例中学习的结合，重点是回归任务。首先，我们表明 LLM 可以对真实世界数据集执行回归，然后设计实验来测量 LLM 检索其内部知识与从情境示例中学习的程度。我们认为这个过程介于这两个极端之间。我们深入分析了这些机制根据各种因素触发的程度，例如关于任务的先验知识以及情境示例提供的信息的类型和丰富程度。我们使用三个 LLM 并使用多个数据集来证实我们的研究结果的稳健性。我们的研究结果揭示了如何设计提示以利用来自上下文示例的元学习并根据正在解决的问题促进知识检索。</li>
</ul>

<h3>Title: RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Wu, Lin Ning, Luyang Liu, Harrison Lee, Neo Wu, Chao Wang, Sushant Prakash, Shawn O'Banion, Bradley Green, Jun Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.04421">https://arxiv.org/abs/2409.04421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.04421">https://arxiv.org/pdf/2409.04421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.04421]] RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs(https://arxiv.org/abs/2409.04421)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>LLM-powered personalization agent systems employ Large Language Models (LLMs) to predict users' behavior from their past activities. However, their effectiveness often hinges on the ability to effectively leverage extensive, long user historical data due to its inherent noise and length of such data. Existing pretrained LLMs may generate summaries that are concise but lack the necessary context for downstream tasks, hindering their utility in personalization systems. To address these challenges, we introduce Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to generate concise, human-readable user summaries that are optimized for downstream task performance. By maximizing the usefulness of the generated summaries, RLPF effectively distills extensive user history data while preserving essential information for downstream tasks. Our empirical evaluation demonstrates significant improvements in both extrinsic downstream task utility and intrinsic summary quality, surpassing baseline methods by up to 22% on downstream task performance and achieving an up to 84.59% win rate on Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable 74% reduction in context length while improving performance on 16 out of 19 unseen tasks and/or datasets, showcasing its generalizability. This approach offers a promising solution for enhancing LLM personalization by effectively transforming long, noisy user histories into informative and human-readable representations.</li>
<li><strong>摘要：</strong>基于 LLM 的个性化代理系统采用大型语言模型 (LLM) 根据用户过去的活动预测其行为。然而，由于数据本身的噪声和长度，它们的有效性通常取决于能否有效利用大量、较长的用户历史数据。现有的预训练 LLM 可能会生成简洁但缺乏下游任务必要背景的摘要，从而阻碍其在个性化系统中的实用性。为了应对这些挑战，我们引入了基于预测反馈的强化学习 (RLPF)。RLPF 对 LLM 进行微调，以生成简洁、人性化、可读的用户摘要，并针对下游任务性能进行优化。通过最大限度地提高生成的摘要的实用性，RLPF 可以有效地提取大量用户历史数据，同时为下游任务保留必要的信息。我们的实证评估表明，外部下游任务效用和内部摘要质量均有显著改善，下游任务性能比基线方法高出 22%，真实性、抽象性和可读性方面的胜率高达 84.59%。RLPF 还实现了上下文长度显著减少 74%，同时提高了 19 个未见任务和/或数据集中的 16 个的性能，展示了其通用性。这种方法通过有效地将长而嘈杂的用户历史记录转换为信息丰富且人类可读的表示，为增强 LLM 个性化提供了一种有希望的解决方案。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
