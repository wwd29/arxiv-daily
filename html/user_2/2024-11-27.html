<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-11-27</h1>
<h3>Title: Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework</h3>
<ul>
<li><strong>Authors: </strong>Mengshuo Jia, Zeyu Cui, Gabriela Hug</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16707">https://arxiv.org/abs/2411.16707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16707">https://arxiv.org/pdf/2411.16707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16707]] Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework(https://arxiv.org/abs/2411.16707)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat, retrieval-augmented generation, agent</a></li>
<li><strong>Abstract: </strong>The integration of experimental technologies with large language models (LLMs) is transforming scientific research, positioning AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations -- one of the essential experimental technologies -- remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, we propose a feedback-driven, multi-agent framework that incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from Daline and MATPOWER, this framework achieves success rates of 93.13% and 96.85%, respectively, significantly outperforming the latest LLMs (ChatGPT 4o and o1-preview), which achieved a 27.77% success rate on standard simulation tasks and 0% on complex tasks. Additionally, our framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond.</li>
<li><strong>摘要：</strong>实验技术与大型语言模型 (LLM) 的结合正在改变科学研究，将 AI 定位为多功能研究助手，而不仅仅是解决问题的工具。然而，在电力系统领域，管理模拟（必不可少的实验技术之一）仍然是 LLM 面临的挑战，因为它们的领域特定知识有限、推理能力有限以及对模拟参数的处理不精确。为了解决这些限制，我们提出了一个反馈驱动的多智能体框架，该框架包含三个拟议的模块：增强检索增强生成 (RAG) 模块、改进的推理模块和具有错误反馈机制的动态环境代理模块。在 Daline 和 MATPOWER 的 69 个不同任务上进行了验证，该框架分别实现了 93.13% 和 96.85% 的成功率，大大优于最新的 LLM（ChatGPT 4o 和 o1-preview），后者在标准模拟任务上的成功率为 27.77%，在复杂任务上的成功率为 0%。此外，我们的框架还支持快速、经济高效的任务执行，每次模拟大约需要 30 秒，平均代币成本为 0.014 美元。总体而言，这个适应性强的框架为开发基于 LLM 的智能助手奠定了基础，可帮助人类研究人员开展电力系统研究及其他研究。</li>
</ul>

<h3>Title: Multi-Reranker: Maximizing performance of retrieval-augmented generation in the FinanceRAG challenge</h3>
<ul>
<li><strong>Authors: </strong>Joohyun Lee, Minji Roh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16732">https://arxiv.org/abs/2411.16732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16732">https://arxiv.org/pdf/2411.16732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16732]] Multi-Reranker: Maximizing performance of retrieval-augmented generation in the FinanceRAG challenge(https://arxiv.org/abs/2411.16732)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) increasingly address domain-specific problems, their application in the financial sector has expanded rapidly. Tasks that are both highly valuable and time-consuming, such as analyzing financial statements, disclosures, and related documents, are now being effectively tackled using LLMs. This paper details the development of a high-performance, finance-specific Retrieval-Augmented Generation (RAG) system for the ACM-ICAIF '24 FinanceRAG competition. We optimized performance through ablation studies on query expansion and corpus refinement during the pre-retrieval phase. To enhance retrieval accuracy, we employed multiple reranker models. Notably, we introduced an efficient method for managing long context sizes during the generation phase, significantly improving response quality without sacrificing performance. We ultimately achieve 2nd place in the FinanceRAG Challenge. Our key contributions include: (1) pre-retrieval ablation analysis, (2) an enhanced retrieval algorithm, and (3) a novel approach for long-context management. This work demonstrates the potential of LLMs in effectively processing and analyzing complex financial data to generate accurate and valuable insights. The source code and further details are available at this https URL.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 越来越多地用于解决特定领域的问题，它们在金融领域的应用也迅速扩展。现在，使用 LLM 可以有效地解决那些既有价值又耗时的任务，例如分析财务报表、披露和相关文件。本文详细介绍了为 ACM-ICAIF '24 FinanceRAG 竞赛开发高性能、金融专用的检索增强生成 (RAG) 系统。我们通过对预检索阶段的查询扩展和语料库细化进行消融研究来优化性能。为了提高检索准确性，我们采用了多个重新排序模型。值得注意的是，我们引入了一种在生成阶段管理长上下文大小的有效方法，在不牺牲性能的情况下显着提高了响应质量。我们最终在 FinanceRAG 挑战赛中获得了第二名。我们的主要贡献包括：(1) 预检索消融分析、(2) 增强的检索算法和 (3) 一种用于长上下文管理的新方法。这项工作展示了 LLM 在有效处理和分析复杂财务数据以产生准确且有价值的见解方面的潜力。源代码和更多详细信息可在此 https URL 上找到。</li>
</ul>

<h3>Title: ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain</h3>
<ul>
<li><strong>Authors: </strong>Haochen Zhao, Xiangru Tang, Ziran Yang, Xiao Han, Xuanzhi Feng, Yueqing Fan, Senhao Cheng, Di Jin, Yilun Zhao, Arman Cohan, Mark Gerstein</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16736">https://arxiv.org/abs/2411.16736</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16736">https://arxiv.org/pdf/2411.16736</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16736]] ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain(https://arxiv.org/abs/2411.16736)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The advancement and extensive application of large language models (LLMs) have been remarkable, including their use in scientific research assistance. However, these models often generate scientifically incorrect or unsafe responses, and in some cases, they may encourage users to engage in dangerous behavior. To address this issue in the field of chemistry, we introduce ChemSafetyBench, a benchmark designed to evaluate the accuracy and safety of LLM responses. ChemSafetyBench encompasses three key tasks: querying chemical properties, assessing the legality of chemical uses, and describing synthesis methods, each requiring increasingly deeper chemical knowledge. Our dataset has more than 30K samples across various chemical materials. We incorporate handcrafted templates and advanced jailbreaking scenarios to enhance task diversity. Our automated evaluation framework thoroughly assesses the safety, accuracy, and appropriateness of LLM responses. Extensive experiments with state-of-the-art LLMs reveal notable strengths and critical vulnerabilities, underscoring the need for robust safety measures. ChemSafetyBench aims to be a pivotal tool in developing safer AI technologies in chemistry. Our code and dataset are available at this https URL. Warning: this paper contains discussions on the synthesis of controlled chemicals using AI models.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的进步和广泛应用令人瞩目，包括它们在科学研究协助中的应用。然而，这些模型通常会产生科学上不正确或不安全的响应，在某些情况下，它们可能会鼓励用户从事危险行为。为了解决化学领域的这一问题，我们推出了 ChemSafetyBench，这是一个旨在评估 LLM 响应准确性和安全性的基准。ChemSafetyBench 包含三个关键任务：查询化学性质、评估化学用途的合法性以及描述合成方法，每个任务都需要越来越深入的化学知识。我们的数据集包含各种化学材料的 30K 多个样本。我们结合了手工制作的模板和高级越狱场景来增强任务多样性。我们的自动评估框架全面评估了 LLM 响应的安全性、准确性和适当性。对最先进的 LLM 进行的大量实验揭示了显著的优势和关键的漏洞，强调了采取强有力的安全措施的必要性。ChemSafetyBench 旨在成为开发更安全的化学 AI 技术的关键工具。我们的代码和数据集可在此 https URL 上找到。警告：本文包含有关使用 AI 模型合成受控化学品的讨论。</li>
</ul>

<h3>Title: Parameter Efficient Instruction Tuning: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Pengfei He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16775">https://arxiv.org/abs/2411.16775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16775">https://arxiv.org/pdf/2411.16775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16775]] Parameter Efficient Instruction Tuning: An Empirical Study(https://arxiv.org/abs/2411.16775)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Instruction tuning has become an important step for finetuning pretrained language models to better follow human instructions and generalize on various tasks. Nowadays, pretrained language models become increasingly larger, and full parameter finetuning is overwhelmingly costly. Therefore, Parameter Efficient Finetuning (PEFT) has arisen as a cost-effective practice for instruction tuning because of significantly smaller computational, memory, and storage cost compared to full finetuning. Despite their widespread adaptations, the vast hyperparameter spaces, the number of PEFT methods, the different focus of instruction tuning capabilities make disentangling the impact of each aspect difficult. This study systematically investigates several representative PEFT methods, surveying the effect of hyperparameter choices including training hyperparameters and PEFT-specific hyperparameters, how different models sizes and the number of instruction tasks affect the performance, in-task-distribution memorization and open instruction following capability. Our empirical study shows that only LoRA and adapter can get close to full finetuning with ideal training settings. The ideal training setting includes an appropriate learning rate, largest LoRA rank or adapter size allowed and diverse training tasks. On the other hand, LoRA and adapter suffer from training instability if such an ideal training condition is not met. Additionally, LoRA requires a greater number of tasks for effective unseen task generalization, exhibit slower learning speed. Moreover, LoRA has weaker task-level memorization. Lastly, LoRA and adapter fall short in complex reasoning, coding and long-form generation compared to finetuning in open instruction tuning settings but it shows stronger capabilities compared to adapter.</li>
<li><strong>摘要：</strong>指令调整已成为微调预训练语言模型以更好地遵循人类指令并概括各种任务的重要步骤。如今，预训练语言模型变得越来越大，而完整的参数微调成本极高。因此，参数高效微调 (PEFT) 已成为一种经济高效的指令调整实践，因为与完全微调相比，其计算、内存和存储成本要小得多。尽管它们被广泛采用，但巨大的超参数空间、PEFT 方法的数量、指令调整能力的不同重点使得解开每个方面的影响变得困难。本研究系统地研究了几种有代表性的 PEFT 方法，调查了超参数选择的影响，包括训练超参数和 PEFT 特定的超参数，以及不同的模型大小和指令任务数量如何影响性能、任务分布记忆和开放指令遵循能力。我们的实证研究表明，只有 LoRA 和适配器才能在理想的训练设置下接近完全微调。理想的训练设置包括适当的学习率、允许的最大 LoRA 等级或适配器大小以及多样化的训练任务。另一方面，如果不满足这种理想的训练条件，LoRA 和适配器就会遭受训练不稳定的困扰。此外，LoRA 需要更多的任务才能有效地进行看不见的任务泛化，学习速度较慢。此外，LoRA 的任务级记忆能力较弱。最后，与开放指令调整设置中的微调相比，LoRA 和适配器在复杂推理、编码和长格式生成方面有所欠缺，但与适配器相比，它表现出更强大的能力。</li>
</ul>

<h3>Title: What can LLM tell us about cities?</h3>
<ul>
<li><strong>Authors: </strong>Zhuoheng Li, Yaochen Wang, Zhixue Song, Yuqi Huang, Rui Bao, Guanjie Zheng, Zhenhui Jessie Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16791">https://arxiv.org/abs/2411.16791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16791">https://arxiv.org/pdf/2411.16791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16791]] What can LLM tell us about cities?(https://arxiv.org/abs/2411.16791)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This study explores the capabilities of large language models (LLMs) in providing knowledge about cities and regions on a global scale. We employ two methods: directly querying the LLM for target variable values and extracting explicit and implicit features from the LLM correlated with the target variable. Our experiments reveal that LLMs embed a broad but varying degree of knowledge across global cities, with ML models trained on LLM-derived features consistently leading to improved predictive accuracy. Additionally, we observe that LLMs demonstrate a certain level of knowledge across global cities on all continents, but it is evident when they lack knowledge, as they tend to generate generic or random outputs for unfamiliar tasks. These findings suggest that LLMs can offer new opportunities for data-driven decision-making in the study of cities.</li>
<li><strong>摘要：</strong>本研究探索了大型语言模型 (LLM) 在全球范围内提供有关城市和地区知识的能力。我们采用了两种方法：直接查询 LLM 以获取目标变量值，并从 LLM 中提取与目标变量相关的显式和隐式特征。我们的实验表明，LLM 在全球城市中嵌入了广泛但程度不同的知识，使用 LLM 衍生特征训练的 ML 模型始终可以提高预测准确性。此外，我们观察到 LLM 在所有大陆的全球城市中都表现出一定程度的知识，但当它们缺乏知识时，这一点很明显，因为它们往往会为不熟悉的任务生成通用或随机的输出。这些发现表明，LLM 可以为城市研究中的数据驱动决策提供新的机会。</li>
</ul>

<h3>Title: Enhancing Answer Reliability Through Inter-Model Consensus of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alireza Amiri-Margavi, Iman Jebellat, Ehsan Jebellat, Seyed Pouyan Mousavi Davoudi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16797">https://arxiv.org/abs/2411.16797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16797">https://arxiv.org/pdf/2411.16797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16797]] Enhancing Answer Reliability Through Inter-Model Consensus of Large Language Models(https://arxiv.org/abs/2411.16797)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>We explore the collaborative dynamics of an innovative language model interaction system involving advanced models such as GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and Gemini-1.5-Flash. These models generate and answer complex, PhD-level statistical questions without exact ground-truth answers. Our study investigates how inter-model consensus enhances the reliability and precision of responses. By employing statistical methods such as chi-square tests, Fleiss' Kappa, and confidence interval analysis, we evaluate consensus rates and inter-rater agreement to quantify the reliability of collaborative outputs. Key results reveal that Claude and GPT-4 exhibit the highest reliability and consistency, as evidenced by their narrower confidence intervals and higher alignment with question-generating models. Conversely, Gemini and LLaMA show more significant variability in their consensus rates, as reflected in wider confidence intervals and lower reliability percentages. These findings demonstrate that collaborative interactions among large language models (LLMs) significantly improve response reliability, offering novel insights into autonomous, cooperative reasoning and validation in AI systems.</li>
<li><strong>摘要：</strong>我们探索了创新语言模型交互系统的协作动态，该系统涉及 GPT-4-0125-preview、Meta-LLaMA-3-70B-Instruct、Claude-3-Opus 和 Gemini-1.5-Flash 等高级模型。这些模型生成并回答复杂的博士级统计问题，而没有确切的真实答案。我们的研究调查了模型间共识如何提高响应的可靠性和准确性。通过采用卡方检验、Fleiss' Kappa 和置信区间分析等统计方法，我们评估了共识率和评分者间一致性，以量化协作输出的可靠性。主要结果表明，Claude 和 GPT-4 表现出最高的可靠性和一致性，这体现在它们更窄的置信区间和与问题生成模型的更高一致性上。相反，Gemini 和 LLaMA 的共识率表现出更显著的变化，这反映在更宽的置信区间和更低的可靠性百分比上。这些发现表明，大型语言模型 (LLM) 之间的协作交互显著提高了响应可靠性，为人工智能系统中的自主、合作推理和验证提供了新的见解。</li>
</ul>

<h3>Title: Fine-Tuning LLMs with Noisy Data for Political Argument Generation</h3>
<ul>
<li><strong>Authors: </strong>Svetlana Churina, Kokil Jaidka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16813">https://arxiv.org/abs/2411.16813</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16813">https://arxiv.org/pdf/2411.16813</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16813]] Fine-Tuning LLMs with Noisy Data for Political Argument Generation(https://arxiv.org/abs/2411.16813)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>The incivility in social media discourse complicates the deployment of automated text generation models for politically sensitive content. Fine-tuning and prompting strategies are critical, but underexplored, solutions to mitigate toxicity in such contexts. This study investigates the fine-tuning and prompting effects on GPT-3.5 Turbo using subsets of the CLAPTON dataset of political discussion posts, comprising Twitter and Reddit data labeled for their justification, reciprocity and incivility. Fine-tuned models on Reddit data scored highest on discussion quality, while combined noisy data led to persistent toxicity. Prompting strategies reduced specific toxic traits, such as personal attacks, but had limited broader impact. The findings emphasize that high-quality data and well-crafted prompts are essential to reduce incivility and improve rhetorical quality in automated political discourse generation.</li>
<li><strong>摘要：</strong>社交媒体话语中的不文明行为使部署针对政治敏感内容的自动文本生成模型变得复杂。微调和提示策略是缓解此类情况下的毒性的关键但尚未得到充分探索的解决方案。本研究使用 CLAPTON 政治讨论帖子数据集的子集（包括 Twitter 和 Reddit 数据，这些数据被标记为合理性、互惠性和不文明行为）研究了微调和提示对 GPT-3.5 Turbo 的影响。Reddit 数据的微调模型在讨论质量方面得分最高，而组合噪声数据则导致持续的毒性。提示策略减少了特定的毒性特征，例如人身攻击，但更广泛的影响有限。研究结果强调，高质量的数据和精心设计的提示对于减少不文明行为和提高自动政治话语生成中的修辞质量至关重要。</li>
</ul>

<h3>Title: Enhancing In-Hospital Mortality Prediction Using Multi-Representational Learning with LLM-Generated Expert Summaries</h3>
<ul>
<li><strong>Authors: </strong>Harshavardhan Battula, Jiacheng Liu, Jaideep Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16818">https://arxiv.org/abs/2411.16818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16818">https://arxiv.org/pdf/2411.16818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16818]] Enhancing In-Hospital Mortality Prediction Using Multi-Representational Learning with LLM-Generated Expert Summaries(https://arxiv.org/abs/2411.16818)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>In-hospital mortality (IHM) prediction for ICU patients is critical for timely interventions and efficient resource allocation. While structured physiological data provides quantitative insights, clinical notes offer unstructured, context-rich narratives. This study integrates these modalities with Large Language Model (LLM)-generated expert summaries to improve IHM prediction accuracy. Using the MIMIC-III database, we analyzed time-series physiological data and clinical notes from the first 48 hours of ICU admission. Clinical notes were concatenated chronologically for each patient and transformed into expert summaries using Med42-v2 70B. A multi-representational learning framework was developed to integrate these data sources, leveraging LLMs to enhance textual data while mitigating direct reliance on LLM predictions, which can introduce challenges in uncertainty quantification and interpretability. The proposed model achieved an AUPRC of 0.6156 (+36.41%) and an AUROC of 0.8955 (+7.64%) compared to a time-series-only baseline. Expert summaries outperformed clinical notes or time-series data alone, demonstrating the value of LLM-generated knowledge. Performance gains were consistent across demographic groups, with notable improvements in underrepresented populations, underscoring the framework's equitable application potential. By integrating LLM-generated summaries with structured and unstructured data, the framework captures complementary patient information, significantly improving predictive performance. This approach showcases the potential of LLMs to augment critical care prediction models, emphasizing the need for domain-specific validation and advanced integration strategies for broader clinical adoption.</li>
<li><strong>摘要：</strong>ICU 患者住院死亡率 (IHM) 预测对于及时干预和有效资源分配至关重要。虽然结构化生理数据提供了定量见解，但临床记录提供了非结构化、背景丰富的叙述。本研究将这些模式与大型语言模型 (LLM) 生成的专家摘要相结合，以提高 IHM 预测准确性。使用 MIMIC-III 数据库，我们分析了 ICU 入院后前 48 小时内的时间序列生理数据和临床记录。临床记录按时间顺序串联每个患者，并使用 Med42-v2 70B 转换为专家摘要。开发了一个多表示学习框架来整合这些数据源，利用 LLM 来增强文本数据，同时减轻对 LLM 预测的直接依赖，这可能会给不确定性量化和可解释性带来挑战。与仅基于时间序列的基线相比，所提出的模型实现了 0.6156 (+36.41%) 的 AUPRC 和 0.8955 (+7.64%) 的 AUROC。专家摘要的表现优于单独的临床笔记或时间序列数据，证明了 LLM 生成的知识的价值。各个人口群体的性能提升是一致的，代表性不足的人群的性能显著提高，凸显了该框架的公平应用潜力。通过将 LLM 生成的摘要与结构化和非结构化数据相结合，该框架可以捕获互补的患者信息，从而显著提高预测性能。这种方法展示了 LLM 增强重症监护预测模型的潜力，强调了需要针对特定​​领域的验证和先进的集成策略才能更广泛地应用于临床。</li>
</ul>

<h3>Title: Integrating Geodesic Interpolation and Flow Matching for Non-Autoregressive Text Generation in Logit Space</h3>
<ul>
<li><strong>Authors: </strong>Egor Sevriugov, Ivan Oseledets</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16821">https://arxiv.org/abs/2411.16821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16821">https://arxiv.org/pdf/2411.16821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16821]] Integrating Geodesic Interpolation and Flow Matching for Non-Autoregressive Text Generation in Logit Space(https://arxiv.org/abs/2411.16821)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Non-autoregressive language models are emerging as effective alternatives to autoregressive models in the field of natural language processing, facilitating simultaneous token generation. This study introduces a novel flow matching approach that employs Kullback-Leibler (KL) divergence geodesics to interpolate between initial and target distributions for discrete sequences. We formulate a loss function designed to maximize the conditional likelihood of discrete tokens and demonstrate that its maximizer corresponds to the flow matching velocity during logit interpolation. Although preliminary experiments conducted on the TinyStories dataset yielded suboptimal results, we propose an empirical sampling scheme based on a pretrained denoiser that significantly enhances performance. Additionally, we present a more general hybrid approach that achieves strong performance on more complex datasets, such as Fine Web and Lamini Instruction.</li>
<li><strong>摘要：</strong>非自回归语言模型正在成为自然语言处理领域中自回归模型的有效替代方案，有助于同时生成标记。本研究介绍了一种新颖的流匹配方法，该方法采用 Kullback-Leibler (KL) 散度测地线在离散序列的初始分布和目标分布之间进行插值。我们制定了一个旨在最大化离散标记条件似然的损失函数，并证明其最大化器对应于 logit 插值期间的流匹配速度。虽然在 TinyStories 数据集上进行的初步实验产生了次优结果，但我们提出了一种基于预训练降噪器的经验采样方案，可显着提高性能。此外，我们提出了一种更通用的混合方法，可在更复杂的数据集（例如 Fine Web 和 Lamini Instruction）上实现出色的性能。</li>
</ul>

<h3>Title: Harnessing LLMs for Educational Content-Driven Italian Crossword Generation</h3>
<ul>
<li><strong>Authors: </strong>Kamyar Zeinalipour, Achille Fusco, Asya Zanollo, Marco Maggini, Marco Gori</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16936">https://arxiv.org/abs/2411.16936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16936">https://arxiv.org/pdf/2411.16936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16936]] Harnessing LLMs for Educational Content-Driven Italian Crossword Generation(https://arxiv.org/abs/2411.16936)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>In this work, we unveil a novel tool for generating Italian crossword puzzles from text, utilizing advanced language models such as GPT-4o, Mistral-7B-Instruct-v0.3, and Llama3-8b-Instruct. Crafted specifically for educational applications, this cutting-edge generator makes use of the comprehensive Italian-Clue-Instruct dataset, which comprises over 30,000 entries including diverse text, solutions, and types of clues. This carefully assembled dataset is designed to facilitate the creation of contextually relevant clues in various styles associated with specific texts and keywords. The study delves into four distinctive styles of crossword clues: those without format constraints, those formed as definite determiner phrases, copular sentences, and bare noun phrases. Each style introduces unique linguistic structures to diversify clue presentation. Given the lack of sophisticated educational tools tailored to the Italian language, this project seeks to enhance learning experiences and cognitive development through an engaging, interactive platform. By meshing state-of-the-art AI with contemporary educational strategies, our tool can dynamically generate crossword puzzles from Italian educational materials, thereby providing an enjoyable and interactive learning environment. This technological advancement not only redefines educational paradigms but also sets a new benchmark for interactive and cognitive language learning solutions.</li>
<li><strong>摘要：</strong>在这项研究中，我们利用 GPT-4o、Mistral-7B-Instruct-v0.3 和 Llama3-8b-Instruct 等高级语言模型，推出了一种从文本生成意大利语填字游戏的新工具。这款先进的生成器专为教育应用而设计，利用了全面的 Italian-Clue-Instruct 数据集，该数据集包含超过 30,000 个条目，包括各种文本、解决方案和线索类型。这个精心组装的数据集旨在帮助创建与特定文本和关键字相关的各种风格的上下文相关线索。这项研究深入研究了四种不同的填字游戏线索风格：没有格式限制的线索、形成明确的限定词短语的线索、系动词句和裸名词短语的线索。每种风格都引入了独特的语言结构，使线索呈现多样化。鉴于缺乏针对意大利语的复杂教育工具，该项目旨在通过一个引人入胜的互动平台来增强学习体验和认知发展。通过将最先进的人工智能与当代教育策略相结合，我们的工具可以根据意大利语教育材料动态生成填字游戏，从而提供令人愉快且互动的学习环境。这项技术进步不仅重新定义了教育模式，还为互动和认知语言学习解决方案树立了新的标杆。</li>
</ul>

<h3>Title: Teaching Smaller Language Models To Generalise To Unseen Compositional Questions (Full Thesis)</h3>
<ul>
<li><strong>Authors: </strong>Tim Hartill</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16985">https://arxiv.org/abs/2411.16985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16985">https://arxiv.org/pdf/2411.16985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16985]] Teaching Smaller Language Models To Generalise To Unseen Compositional Questions (Full Thesis)(https://arxiv.org/abs/2411.16985)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Pretrained large Language Models (LLMs) are able to answer questions that are unlikely to have been encountered during training. However a diversity of potential applications exist in the broad domain of reasoning systems and considerations such as latency, cost, available compute resource and internet connectivity are relevant in determining an appropriate approach. We consider the setting where some local compute capacity is available at inference time but internet connectivity is not. Similar to a general-purpose LLM, we assume that our much smaller Reasoning Models may be asked arbitrary questions from unknown distributions, so we focus on evaluation in an unseen setting. We train our models to answer diverse questions by instilling an ability to reason over a retrieved context. We acquire context from two knowledge sources; a Wikipedia corpus queried using a multi-hop dense retrieval system with novel extensions, and from rationales generated from a larger Language Model optimised to run in a lower resource environment. Our main contributions: We propose novel methods to show that our model is capable of answering contextualised questions without memorisation. We establish a comprehensive set of baseline results on unseen evaluation datasets. We show that the addition of novel retrieval-augmented training datasets (RATD) to the training regime of the Reasoning Model significantly improves results. We demonstrate further significant improvement through the application of methods for combining knowledge from two sources. The first method (RR) involves training a novel Rationale Ranking model to score both generated rationales and retrieved contexts with respect to relevance and truthfulness. We use the scores to derive combined contexts. We also show that utilising the RATD datasets enables our model to become proficient at utilising combined noisy contexts.</li>
<li><strong>摘要：</strong>预训练的大型语言模型 (LLM) 能够回答在训练期间不太可能遇到的问题。然而，在推理系统的广泛领域中存在着各种潜在的应用，延迟、成本、可用计算资源和互联网连接等考虑因素与确定适当的方法有关。我们考虑在推理时有一些本地计算能力可用但没有互联网连接的情况。与通用 LLM 类似，我们假设我们的小得多的推理模型可能会被问到来自未知分布的任意问题，因此我们专注于在看不见的环境中进行评估。我们通过灌输对检索到的上下文进行推理的能力来训练我们的模型回答各种问题。我们从两个知识来源获取上下文；使用具有新扩展的多跳密集检索系统查询的维基百科语料库，以及从优化为在资源较少的环境中运行的较大语言模型生成的理由。我们的主要贡献：我们提出了新颖的方法来表明我们的模型能够回答上下文问题而无需记忆。我们在未见过的评估数据集上建立了一套全面的基线结果。我们表明，在推理模型的训练机制中添加新的检索增强训练数据集 (RATD) 可以显著改善结果。我们通过应用结合来自两个来源的知识的方法展示了进一步的显著改进。第一种方法 (RR) 涉及训练一个新的 Rationale Ranking 模型，以根据相关性和真实性对生成的原理和检索到的上下文进行评分。我们使用这些分数来得出组合上下文。我们还表明，利用 RATD 数据集使我们的模型能够熟练地利用组合的噪声上下文。</li>
</ul>

<h3>Title: Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yao Fu, Yin Yu, Xiaotian Han, Runchao Li, Xianxuan Long, Haotian Yu, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16991">https://arxiv.org/abs/2411.16991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16991">https://arxiv.org/pdf/2411.16991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16991]] Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning Small Language Models(https://arxiv.org/abs/2411.16991)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Knowledge distillation (KD) has become a widely adopted approach for compressing large language models (LLMs) to reduce computational costs and memory footprints. However, the availability of complex teacher models is a prerequisite for running most KD pipelines. Thus, the traditional KD procedure can be unachievable or budget-unfriendly, particularly when relying on commercial LLMs like GPT4. In this regard, Self-distillation (SelfD) emerges as an advisable alternative, enabling student models to learn without teachers' guidance. Nonetheless, existing SelfD approaches for LMs often involve architectural modifications, assuming the models are open-source, which may not always be practical. In this work, we introduce a model-agnostic and task-agnostic method named dynamic SelfD from the previous minibatch (DynSDPB), which realizes current iterations' distillation from the last ones' generated logits. Additionally, to address prediction inaccuracies during the early iterations, we dynamically adjust the distillation influence and temperature values to enhance the adaptability of fine-tuning. Furthermore, DynSDPB is a novel fine-tuning policy that facilitates the seamless integration of existing self-correction and self-training techniques for small language models (SLMs) because they all require updating SLMs' parameters. We demonstrate the superior performance of DynSDPB on both encoder-only LMs (e.g., BERT model families) and decoder-only LMs (e.g., LLaMA model families), validating its effectiveness across natural language understanding (NLU) and natural language generation (NLG) benchmarks.</li>
<li><strong>摘要：</strong>知识蒸馏 (KD) 已成为一种广泛采用的压缩大型语言模型 (LLM) 的方法，以降低计算成本和内存占用。然而，复杂的教师模型的可用性是运行大多数 KD 管道的先决条件。因此，传统的 KD 程序可能无法实现或预算不友好，尤其是在依赖 GPT4 等商业 LLM 时。在这方面，自我蒸馏 (SelfD) 作为一种可取的替代方案应运而生，它使学生模型能够在没有老师指导的情况下学习。尽管如此，现有的 LM 的 SelfD 方法通常涉及架构修改，假设模型是开源的，这可能并不总是可行的。在这项工作中，我们引入了一种与模型无关和与任务无关的方法，称为来自前一个小批量 (DynSDPB) 的动态 SelfD，它实现了当前迭代从最后生成的 logit 中进行蒸馏。此外，为了解决早期迭代过程中的预测不准确性问题，我们动态调整蒸馏影响和温度值以增强微调的适应性。此外，DynSDPB 是一种新颖的微调策略，有助于无缝集成现有的小型语言模型 (SLM) 的自我校正和自我训练技术，因为它们都需要更新 SLM 的参数。我们展示了 DynSDPB 在仅编码器 LM（例如 BERT 模型系列）和仅解码器 LM（例如 LLaMA 模型系列）上的卓越性能，验证了其在自然语言理解 (NLU) 和自然语言生成 (NLG) 基准测试中的有效性。</li>
</ul>

<h3>Title: Tree Transformers are an Ineffective Model of Syntactic Constituency</h3>
<ul>
<li><strong>Authors: </strong>Michael Ginn</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.16993">https://arxiv.org/abs/2411.16993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.16993">https://arxiv.org/pdf/2411.16993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.16993]] Tree Transformers are an Ineffective Model of Syntactic Constituency(https://arxiv.org/abs/2411.16993)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Linguists have long held that a key aspect of natural language syntax is the recursive organization of language units into constituent structures, and research has suggested that current state-of-the-art language models lack an inherent bias towards this feature. A number of alternative models have been proposed to provide inductive biases towards constituency, including the Tree Transformer, which utilizes a modified attention mechanism to organize tokens into constituents. We investigate Tree Transformers to study whether they utilize meaningful and/or useful constituent structures. We pretrain a large Tree Transformer on language modeling in order to investigate the learned constituent tree representations of sentences, finding little evidence for meaningful structures. Next, we evaluate Tree Transformers with similar transformer models on error detection tasks requiring constituent structure. We find that while the Tree Transformer models may slightly outperform at these tasks, there is little evidence to suggest a meaningful improvement. In general, we conclude that there is little evidence to support Tree Transformer as an effective model of syntactic constituency.</li>
<li><strong>摘要：</strong>语言学家长期以来一直认为，自然语言句法的一个关键方面是将语言单元递归组织成组成结构，研究表明，当前最先进的语言模型缺乏对这一特征的固有偏见。已经提出了许多替代模型来提供对成分的归纳偏见，包括 Tree Transformer，它利用改进的注意力机制将标记组织成成分。我们研究 Tree Transformer，以研究它们是否使用有意义和/或有用的组成结构。我们在语言建模上对大型 Tree Transformer 进行了预训练，以研究学习到的句子的组成树表示，发现有意义的结构的证据很少。接下来，我们在需要组成结构的错误检测任务上评估了 Tree Transformer 和类似的转换器模型。我们发现，虽然 Tree Transformer 模型在这些任务上的表现可能略胜一筹，但几乎没有证据表明它有有意义的改进。总的来说，我们得出结论，几乎没有证据支持 Tree Transformer 是一种有效的句法组成模型。</li>
</ul>

<h3>Title: Don't Command, Cultivate: An Exploratory Study of System-2 Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Wang, Jitao Sang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17075">https://arxiv.org/abs/2411.17075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17075">https://arxiv.org/pdf/2411.17075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17075]] Don't Command, Cultivate: An Exploratory Study of System-2 Alignment(https://arxiv.org/abs/2411.17075)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>The o1 system card identifies the o1 models as the most robust within OpenAI, with their defining characteristic being the progression from rapid, intuitive thinking to slower, more deliberate reasoning. This observation motivated us to investigate the influence of System-2 thinking patterns on model safety. In our preliminary research, we conducted safety evaluations of the o1 model, including complex jailbreak attack scenarios using adversarial natural language prompts and mathematical encoding prompts. Our findings indicate that the o1 model demonstrates relatively improved safety performance; however, it still exhibits vulnerabilities, particularly against jailbreak attacks employing mathematical encoding. Through detailed case analysis, we identified specific patterns in the o1 model's responses. We also explored the alignment of System-2 safety in open-source models using prompt engineering and supervised fine-tuning techniques. Experimental results show that some simple methods to encourage the model to carefully scrutinize user requests are beneficial for model safety. Additionally, we proposed a implementation plan for process supervision to enhance safety alignment. The implementation details and experimental results will be provided in future versions.</li>
<li><strong>摘要：</strong>o1 系统卡将 o1 模型标识为 OpenAI 中最强大的模型，其定义特征是从快速、直觉的思考到更慢、更慎重的推理的进展。这一观察促使我们研究 System-2 思维模式对模型安全性的影响。在我们的初步研究中，我们对 o1 模型进行了安全性评估，包括使用对抗性自然语言提示和数学编码提示的复杂越狱攻击场景。我们的研究结果表明，o1 模型表现出相对更高的安全性能；然而，它仍然存在漏洞，特别是针对使用数学编码的越狱攻击。通过详细的案例分析，我们在 o1 模型的响应中发现了特定的模式。我们还使用提示工程和监督微调技术探索了开源模型中 System-2 安全性的协调。实验结果表明，一些简单的方法来鼓励模型仔细审查用户请求对模型安全有益。此外，我们提出了一个流程监督的实施计划，以增强安全性协调。实施细节和实验结果将在未来的版本中提供。</li>
</ul>

<h3>Title: Star Attention: Efficient LLM Inference over Long Sequences</h3>
<ul>
<li><strong>Authors: </strong>Shantanu Acharya, Fei Jia, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17116">https://arxiv.org/abs/2411.17116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17116">https://arxiv.org/pdf/2411.17116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17116]] Star Attention: Efficient LLM Inference over Long Sequences(https://arxiv.org/abs/2411.17116)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy.</li>
<li><strong>摘要：</strong>由于自注意力机制的二次复杂度，使用基于 Transformer 的大型语言模型 (LLM) 对长序列进行推理既昂贵又缓慢。我们引入了 Star Attention，这是一种两阶段块稀疏近似，通过将注意力分散到多个主机上来提高计算效率，同时最大限度地减少通信开销。在第一阶段，使用跨主机的块状局部注意力并行处理上下文。在第二阶段，查询和响应标记通过序列全局注意力关注所有先前缓存的标记。Star Attention 可与大多数使用全局注意力训练的基于 Transformer 的 LLM 无缝集成，将内存需求和推理时间减少高达 11 倍，同时保持 95-100% 的准确率。</li>
</ul>

<h3>Title: Strategic Prompting for Conversational Tasks: A Comparative Analysis of Large Language Models Across Diverse Conversational Tasks</h3>
<ul>
<li><strong>Authors: </strong>Ratnesh Kumar Joshi, Priyanshu Priya, Vishesh Desai, Saurav Dudhate, Siddhant Senapati, Asif Ekbal, Roshni Ramnani, Anutosh Maitra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17204">https://arxiv.org/abs/2411.17204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17204">https://arxiv.org/pdf/2411.17204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17204]] Strategic Prompting for Conversational Tasks: A Comparative Analysis of Large Language Models Across Diverse Conversational Tasks(https://arxiv.org/abs/2411.17204)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Given the advancements in conversational artificial intelligence, the evaluation and assessment of Large Language Models (LLMs) play a crucial role in ensuring optimal performance across various conversational tasks. In this paper, we present a comprehensive study that thoroughly evaluates the capabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon, Alpaca, and MPT. The study encompasses various conversational tasks, including reservation, empathetic response generation, mental health and legal counseling, persuasion, and negotiation. To conduct the evaluation, an extensive test setup is employed, utilizing multiple evaluation criteria that span from automatic to human evaluation. This includes using generic and task-specific metrics to gauge the LMs' performance accurately. From our evaluation, no single model emerges as universally optimal for all tasks. Instead, their performance varies significantly depending on the specific requirements of each task. While some models excel in certain tasks, they may demonstrate comparatively poorer performance in others. These findings emphasize the importance of considering task-specific requirements and characteristics when selecting the most suitable LM for conversational applications.</li>
<li><strong>摘要：</strong>鉴于对话式人工智能的进步，大型语言模型 (LLM) 的评估和评估在确保各种对话任务的最佳性能方面发挥着至关重要的作用。在本文中，我们进行了一项全面的研究，彻底评估了五种流行的 LLM 的能力和局限性：Llama、OPT、Falcon、Alpaca 和 MPT。该研究涵盖了各种对话任务，包括保留、产生共情反应、心理健康和法律咨询、说服和谈判。为了进行评估，采用了广泛的测试设置，利用了从自动评估到人工评估的多种评估标准。这包括使用通用和特定于任务的指标来准确衡量 LM 的性能。从我们的评估来看，没有一个模型可以成为所有任务的普遍最优模型。相反，它们的性能会根据每个任务的具体要求而有很大差异。虽然某些模型在某些任务中表现出色，但它们在其他任务中的表现可能相对较差。这些发现强调了在为对话应用程序选择最合适的 LM 时考虑任务特定的要求和特征的重要性。</li>
</ul>

<h3>Title: A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Lehan He, Zeren Chen, Zhelun Shi, Tianyu Yu, Jing Shao, Lu Sheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17265">https://arxiv.org/abs/2411.17265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17265">https://arxiv.org/pdf/2411.17265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17265]] A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs(https://arxiv.org/abs/2411.17265)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Aligning the behaviors of Multimodal Large Language Models (MLLMs) with human preferences is crucial for developing robust and trustworthy AI systems. While recent attempts have employed human experts or powerful auxiliary AI systems to provide more accurate preference feedback, such as determining the preferable responses from MLLMs or directly rewriting hallucination-free responses, extensive resource overhead compromise the scalability of the feedback collection. In this work, we introduce Topic-level Preference Overwriting (TPO), a self-correctional approach that guide the model itself to mitigate its own hallucination at the topic level. Through a deconfounded strategy that replaces each topic within the response with the best or worst alternatives generated by the model itself, TPO creates more contrasting pairwise preference feedback, enhancing the feedback quality without human or proprietary model intervention. Notably, the experimental results demonstrate proposed TPO achieves state-of-the-art performance in trustworthiness, significantly reducing the object hallucinations by 92% and overall hallucinations by 38%. Code, model and data will be released.</li>
<li><strong>摘要：</strong>将多模态大型语言模型 (MLLM) 的行为与人类偏好相结合对于开发稳健且值得信赖的 AI 系统至关重要。虽然最近的尝试已经聘请了人类专家或强大的辅助 AI 系统来提供更准确的偏好反馈，例如确定来自 MLLM 的可取响应或直接重写无幻觉响应，但大量的资源开销损害了反馈集合的可扩展性。在这项工作中，我们引入了主题级偏好覆盖 (TPO)，这是一种自我纠正方法，可指导模型本身在主题级别减轻自己的幻觉。通过一种去混淆策略，将响应中的每个主题替换为模型本身生成的最佳或最差替代方案，TPO 创建了更具对比性的成对偏好反馈，从而提高了反馈质量，而无需人工或专有模型干预。值得注意的是，实验结果表明，提出的 TPO 在可信度方面实现了最先进的性能，显著降低了 92% 的物体幻觉和 38% 的整体幻觉。代码、模型和数据即将发布。</li>
</ul>

<h3>Title: ER2Score: LLM-based Explainable and Customizable Metric for Assessing Radiology Reports with Reward-Control Loss</h3>
<ul>
<li><strong>Authors: </strong>Yunyi Liu, Yingshu Li, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Luping Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17301">https://arxiv.org/abs/2411.17301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17301">https://arxiv.org/pdf/2411.17301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17301]] ER2Score: LLM-based Explainable and Customizable Metric for Assessing Radiology Reports with Reward-Control Loss(https://arxiv.org/abs/2411.17301)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Automated radiology report generation (R2Gen) has advanced significantly, introducing challenges in accurate evaluation due to its complexity. Traditional metrics often fall short by relying on rigid word-matching or focusing only on pathological entities, leading to inconsistencies with human assessments. To bridge this gap, we introduce ER2Score, an automatic evaluation metric designed specifically for R2Gen. Our metric utilizes a reward model, guided by our margin-based reward enforcement loss, along with a tailored training data design that enables customization of evaluation criteria to suit user-defined needs. It not only scores reports according to user-specified criteria but also provides detailed sub-scores, enhancing interpretability and allowing users to adjust the criteria between different aspects of reports. Leveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling us to produce extensive training data based on two distinct scoring systems, each containing reports of varying quality along with corresponding scores. These GPT-generated reports are then paired as accepted and rejected samples through our pairing rule to train an LLM towards our fine-grained reward model, which assigns higher rewards to the report with high quality. Our reward-control loss enables this model to simultaneously output multiple individual rewards corresponding to the number of evaluation criteria, with their summation as our final ER2Score. Our experiments demonstrate ER2Score's heightened correlation with human judgments and superior performance in model selection compared to traditional metrics. Notably, our model provides both an overall score and individual scores for each evaluation item, enhancing interpretability. We also demonstrate its flexible training across various evaluation systems.</li>
<li><strong>摘要：</strong>自动放射学报告生成 (R2Gen) 取得了长足进步，但由于其复杂性，在准确评估方面也面临挑战。传统指标往往存在不足，因为它们依赖于严格的单词匹配或仅关注病理实体，导致与人工评估不一致。为了弥补这一差距，我们引入了 ER2Score，这是一种专为 R2Gen 设计的自动评估指标。我们的指标采用奖励模型，以基于边际的奖励执行损失为指导，并采用量身定制的训练数据设计，可根据用户定义的需求定制评估标准。它不仅根据用户指定的标准对报告进行评分，还提供详细的子分数，增强了可解释性，并允许用户调整报告不同方面之间的标准。利用 GPT-4，我们设计了一个易于使用的数据生成管道，使我们能够基于两个不同的评分系统生成大量训练数据，每个系统都包含不同质量的报告以及相应的分数。然后，这些 GPT 生成的报告通过我们的配对规则配对为接受和拒绝的样本，以训练 LLM 实现我们的细粒度奖励模型，该模型会为高质量的报告分配更高的奖励。我们的奖励控制损失使该模型能够同时输出与评估标准数量相对应的多个单独奖励，它们的总和作为我们最终的 ER2Score。我们的实验表明，与传统指标相比，ER2Score 与人类判断的相关性更高，并且在模型选择方面具有出色的性能。值得注意的是，我们的模型为每个评估项目提供了总体分数和单独分数，从而提高了可解释性。我们还展示了它在各种评估系统中的灵活训练。</li>
</ul>

<h3>Title: Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning</h3>
<ul>
<li><strong>Authors: </strong>Milena Chadimová, Eduard Jurášek, Tomáš Kliegr</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17304">https://arxiv.org/abs/2411.17304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17304">https://arxiv.org/pdf/2411.17304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17304]] Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning(https://arxiv.org/abs/2411.17304)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, hallucination, prompt, chat</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel method, referred to as "hashing", which involves masking potentially bias-inducing words in large language models (LLMs) with hash-like meaningless identifiers to reduce cognitive biases and reliance on external knowledge. The method was tested across three sets of experiments involving a total of 490 prompts. Statistical analysis using chi-square tests showed significant improvements in all tested scenarios, which covered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first experiment, hashing decreased the fallacy rate in a modified version of the "Linda" problem aimed at evaluating susceptibility to cognitive biases. In the second experiment, it improved LLM results on the frequent itemset extraction task. In the third experiment, we found hashing is also effective when the Linda problem is presented in a tabular format rather than text, indicating that the technique works across various input representations. Overall, the method was shown to improve bias reduction and incorporation of external knowledge. Despite bias reduction, hallucination rates were inconsistently reduced across types of LLM models. These findings suggest that masking bias-inducing terms can improve LLM performance, although its effectiveness is model- and task-dependent.</li>
<li><strong>摘要：</strong>本文介绍了一种称为“散列”的新方法，该方法涉及使用散列式无意义标识符掩盖大型语言模型 (LLM) 中可能引起偏见的单词，以减少认知偏见和对外部知识的依赖。该方法在三组实验中进行了测试，总共涉及 490 个提示。使用卡方检验的统计分析表明，在所有测试场景中都有显著改善，这些场景涵盖了 LLama、ChatGPT、Copilot、Gemini 和 Mixtral 模型。在第一个实验中，散列降低了“Linda”问题的修改版本中的谬误率，该问题旨在评估对认知偏见的敏感性。在第二个实验中，它改进了 LLM 在频繁项集提取任务上的结果。在第三个实验中，我们发现当 Linda 问题以表格形式而不是文本形式呈现时，散列也是有效的，这表明该技术适用于各种输入表示。总体而言，该方法被证明可以改善偏见减少和外部知识的整合。尽管偏见有所减少，但幻觉率在不同类型的 LLM 模型中都有所降低。这些发现表明，掩盖引起偏见的术语可以提高 LLM 性能，尽管其有效性取决于模型和任务。</li>
</ul>

<h3>Title: Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach</h3>
<ul>
<li><strong>Authors: </strong>Changgeon Ko, Jisu Shin, Hoyun Song, Jeongyeon Seo, Jong C. Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17338">https://arxiv.org/abs/2411.17338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17338">https://arxiv.org/pdf/2411.17338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17338]] Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach(https://arxiv.org/abs/2411.17338)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often reflect real-world biases, leading to efforts to mitigate these effects and make the models unbiased. Achieving this goal requires defining clear criteria for an unbiased state, with any deviation from these criteria considered biased. Some studies define an unbiased state as equal treatment across diverse demographic groups, aiming for balanced outputs from LLMs. However, differing perspectives on equality and the importance of pluralism make it challenging to establish a universal standard. Alternatively, other approaches propose using fact-based criteria for more consistent and objective evaluations, though these methods have not yet been fully applied to LLM bias assessments. Thus, there is a need for a metric with objective criteria that offers a distinct perspective from equality-based approaches. Motivated by this need, we introduce a novel metric to assess bias using fact-based criteria and real-world statistics. In this paper, we conducted a human survey demonstrating that humans tend to perceive LLM outputs more positively when they align closely with real-world demographic distributions. Evaluating various LLMs with our proposed metric reveals that model bias varies depending on the criteria used, highlighting the need for multi-perspective assessment.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 通常反映现实世界的偏见，因此需要努力减轻这些影响并使模型无偏见。要实现这一目标，需要为无偏见状态定义明确的标准，任何偏离这些标准的行为都被视为有偏见。一些研究将无偏见状态定义为对不同人口群体的平等对待，旨在实现 LLM 的平衡产出。然而，对平等和多元化重要性的不同看法使得建立通用标准变得具有挑战性。或者，其他方法建议使用基于事实的标准进行更一致和客观的评估，尽管这些方法尚未完全应用于 LLM 偏见评估。因此，需要一种具有客观标准的指标，该指标提供与基于平等的方法不同的视角。受此需求的推动，我们引入了一种使用基于事实的标准和现实世界统计数据来评估偏见的新指标。在本文中，我们进行了一项人类调查，表明当 LLM 输出与现实世界的人口分布紧密一致时，人类往往会更积极地看待它们。使用我们提出的指标评估各种 LLM 表明，模型偏差因所使用的标准而异，凸显了多视角评估的必要性。</li>
</ul>

<h3>Title: The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs in LLM Generations</h3>
<ul>
<li><strong>Authors: </strong>Theodora Worledge, Tatsunori Hashimoto, Carlos Guestrin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17375">https://arxiv.org/abs/2411.17375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17375">https://arxiv.org/pdf/2411.17375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17375]] The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs in LLM Generations(https://arxiv.org/abs/2411.17375)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Across all fields of academic study, experts cite their sources when sharing information. While large language models (LLMs) excel at synthesizing information, they do not provide reliable citation to sources, making it difficult to trace and verify the origins of the information they present. In contrast, search engines make sources readily accessible to users and place the burden of synthesizing information on the user. Through a survey, we find that users prefer search engines over LLMs for high-stakes queries, where concerns regarding information provenance outweigh the perceived utility of LLM responses. To examine the interplay between verifiability and utility of information-sharing tools, we introduce the extractive-abstractive spectrum, in which search engines and LLMs are extreme endpoints encapsulating multiple unexplored intermediate operating points. Search engines are extractive because they respond to queries with snippets of sources with links (citations) to the original webpages. LLMs are abstractive because they address queries with answers that synthesize and logically transform relevant information from training and in-context sources without reliable citation. We define five operating points that span the extractive-abstractive spectrum and conduct human evaluations on seven systems across four diverse query distributions that reflect real-world QA settings: web search, language simplification, multi-step reasoning, and medical advice. As outputs become more abstractive, we find that perceived utility improves by as much as 200%, while the proportion of properly cited sentences decreases by as much as 50% and users take up to 3 times as long to verify cited information. Our findings recommend distinct operating points for domain-specific LLM systems and our failure analysis informs approaches to high-utility LLM systems that empower users to verify information.</li>
<li><strong>摘要：</strong>在所有学术研究领域，专家在分享信息时都会引用其来源。虽然大型语言模型 (LLM) 擅长综合信息，但它们无法提供对来源的可靠引用，因此很难追踪和验证它们所呈现的信息的来源。相比之下，搜索引擎使用户可以轻松访问来源，并将综合信息的负担放在用户身上。通过调查，我们发现用户更喜欢搜索引擎而不是 LLM 来进行高风险查询，因为对信息来源的担忧超过了 LLM 响应的感知效用。为了研究信息共享工具的可验证性和效用之间的相互作用，我们引入了提取-抽象频谱，其中搜索引擎和 LLM 是封装多个未探索的中间操作点的极端端点。搜索引擎是提取式的，因为它们使用带有原始网页链接（引用）的来源片段来响应查询。LLM 是抽象式的，因为它们使用答案来处理查询，这些答案综合并逻辑地转换来自训练和上下文来源的相关信息，而无需可靠的引用。我们定义了五个操作点，涵盖了提取-抽象范围，并对七个系统进行了人工评估，涵盖了反映真实世界 QA 设置的四个不同查询分布：网络搜索、语言简化、多步推理和医疗建议。随着输出变得更加抽象，我们发现感知效用提高了 200%，而正确引用的句子的比例下降了 50%，用户验证引用信息所需的时间增加了 3 倍。我们的研究结果为特定领域的 LLM 系统推荐了不同的操作点，我们的故障分析为高效用 LLM 系统的方法提供了参考，使用户能够验证信息。</li>
</ul>

<h3>Title: Can LLMs be Good Graph Judger for Knowledge Graph Construction?</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Huang, Chong Chen, Conghui He, Yang Li, Jiawei Jiang, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17388">https://arxiv.org/abs/2411.17388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17388">https://arxiv.org/pdf/2411.17388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17388]] Can LLMs be Good Graph Judger for Knowledge Graph Construction?(https://arxiv.org/abs/2411.17388)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>In real-world scenarios, most of the data obtained from information retrieval (IR) system is unstructured. Converting natural language sentences into structured Knowledge Graphs (KGs) remains a critical challenge. The quality of constructed KGs may also impact the performance of some KG-dependent domains like GraphRAG systems and recommendation systems. Recently, Large Language Models (LLMs) have demonstrated impressive capabilities in addressing a wide range of natural language processing tasks. However, there are still challenges when utilizing LLMs to address the task of generating structured KGs. And we have identified three limitations with respect to existing KG construction methods. (1)There is a large amount of information and excessive noise in real-world documents, which could result in extracting messy information. (2)Native LLMs struggle to effectively extract accuracy knowledge from some domain-specific documents. (3)Hallucinations phenomenon cannot be overlooked when utilizing LLMs directly as an unsupervised method for constructing KGs. In this paper, we propose GraphJudger, a knowledge graph construction framework to address the aforementioned challenges. We introduce three innovative modules in our method, which are entity-centric iterative text denoising, knowledge aware instruction tuning and graph judgement, respectively. We seek to utilize the capacity of LLMs to function as a graph judger, a capability superior to their role only as a predictor for KG construction problems. Experiments conducted on two general text-graph pair datasets and one domain-specific text-graph pair dataset show superior performances compared to baseline methods. The code of our proposed method is available at this https URL.</li>
<li><strong>摘要：</strong>在现实场景中，从信息检索 (IR) 系统获得的大部分数据都是非结构化的。将自然语言句子转换为结构化的知识图谱 (KG) 仍然是一个关键挑战。构建的 KG 的质量也可能影响一些依赖 KG 领域的性能，如 GraphRAG 系统和推荐系统。最近，大型语言模型 (LLM) 在解决广泛的自然语言处理任务方面表现出色。然而，在使用 LLM 解决生成结构化 KG 的任务时仍然存在挑战。我们已经确定了现有 KG 构建方法的三个限制。（1）现实世界文档中存在大量信息和过多噪音，这可能导致提取混乱的信息。（2）原生 LLM 难以有效地从一些特定领域的文档中提取准确性知识。（3）直接使用 LLM 作为构建 KG 的无监督方法时，不能忽视幻觉现象。在本文中，我们提出了 GraphJudger，这是一个知识图谱构建框架，用于解决上述挑战。我们在我们的方法中引入了三个创新模块，分别是以实体为中心的迭代文本去噪、知识感知指令调整和图判断。我们试图利用 LLM 的能力来充当图判断器，这种能力优于它们仅作为 KG 构建问题的预测器的作用。在两个通用文本图对数据集和一个领域特定文本图对数据集上进行的实验表明，与基线方法相比，其性能更优越。我们提出的方法的代码可在此 https URL 中找到。</li>
</ul>

<h3>Title: One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge Neurons in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Cao, Yuheng Chen, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17401">https://arxiv.org/abs/2411.17401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17401">https://arxiv.org/pdf/2411.17401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17401]] One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge Neurons in Large Language Models(https://arxiv.org/abs/2411.17401)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have learned vast amounts of factual knowledge through self-supervised pre-training on large-scale corpora. Meanwhile, LLMs have also demonstrated excellent multilingual capabilities, which can express the learned knowledge in multiple languages. However, the knowledge storage mechanism in LLMs still remains mysterious. Some researchers attempt to demystify the factual knowledge in LLMs from the perspective of knowledge neurons, and subsequently discover language-agnostic knowledge neurons that store factual knowledge in a form that transcends language barriers. However, the preliminary finding suffers from two limitations: 1) High Uncertainty in Localization Results. Existing study only uses a prompt-based probe to localize knowledge neurons for each fact, while LLMs cannot provide consistent answers for semantically equivalent queries. Thus, it leads to inaccurate localization results with high uncertainty. 2) Lack of Analysis in More Languages. The study only analyzes language-agnostic knowledge neurons on English and Chinese data, without exploring more language families and languages. Naturally, it limits the generalizability of the findings. To address aforementioned problems, we first construct a new benchmark called Rephrased Multilingual LAMA (RML-LAMA), which contains high-quality cloze-style multilingual parallel queries for each fact. Then, we propose a novel method named Multilingual Integrated Gradients with Uncertainty Estimation (MATRICE), which quantifies the uncertainty across queries and languages during knowledge localization. Extensive experiments show that our method can accurately localize language-agnostic knowledge neurons. We also further investigate the role of language-agnostic knowledge neurons in cross-lingual knowledge editing, knowledge enhancement and new knowledge injection.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通过在大规模语料库上进行自监督预训练，学习到了海量的事实知识。同时，LLM 还展现出了出色的多语言能力，能够将学习到的知识以多种语言表达出来。然而，LLM 中的知识存储机制依然是一个谜。一些研究者试图从知识神经元的角度揭开 LLM 中事实知识的神秘面纱，并发现了能够以超越语言障碍的形式存储事实知识的语言无关知识神经元。然而，这一初步发现存在两个局限性：1）定位结果不确定性高。现有研究仅使用基于提示的探测来定位每个事实的知识神经元，而 LLM 无法为语义等效的查询提供一致的答案。因此，它会导致定位结果不准确且不确定性很高。2）缺乏对更多语言的分析。该研究仅分析了英语和中文数据上的语言无关知识神经元，而没有探索更多的语系和语言。当然，这限制了研究结果的普遍性。为了解决上述问题，我们首先构建了一个新的基准，称为 Rephrased Multilingual LAMA (RML-LAMA)，它包含每个事实的高质量完形填空式多语言并行查询。然后，我们提出了一种新方法，称为带不确定性估计的多语言集成梯度 (MATRICE)，它量化了知识本地化过程中跨查询和语言的不确定性。大量实验表明，我们的方法可以准确定位与语言无关的知识神经元。我们还进一步研究了与语言无关的知识神经元在跨语言知识编辑、知识增强和新知识注入中的作用。</li>
</ul>

<h3>Title: "Stupid robot, I want to speak to a human!" User Frustration Detection in Task-Oriented Dialog Systems</h3>
<ul>
<li><strong>Authors: </strong>Mireia Hernandez Caralt, Ivan Sekulić, Filip Carević, Nghia Khau, Diana Nicoleta Popa, Bruna Guedes, Victor Guimarães, Zeyu Yang, Andre Manso, Meghana Reddy, Paolo Rosso, Roland Mathis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17437">https://arxiv.org/abs/2411.17437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17437">https://arxiv.org/pdf/2411.17437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17437]] "Stupid robot, I want to speak to a human!" User Frustration Detection in Task-Oriented Dialog Systems(https://arxiv.org/abs/2411.17437)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Detecting user frustration in modern-day task-oriented dialog (TOD) systems is imperative for maintaining overall user satisfaction, engagement, and retention. However, most recent research is focused on sentiment and emotion detection in academic settings, thus failing to fully encapsulate implications of real-world user data. To mitigate this gap, in this work, we focus on user frustration in a deployed TOD system, assessing the feasibility of out-of-the-box solutions for user frustration detection. Specifically, we compare the performance of our deployed keyword-based approach, open-source approaches to sentiment analysis, dialog breakdown detection methods, and emerging in-context learning LLM-based detection. Our analysis highlights the limitations of open-source methods for real-world frustration detection, while demonstrating the superior performance of the LLM-based approach, achieving a 16\% relative improvement in F1 score on an internal benchmark. Finally, we analyze advantages and limitations of our methods and provide an insight into user frustration detection task for industry practitioners.</li>
<li><strong>摘要：</strong>在现代面向任务的对话 (TOD) 系统中检测用户挫败感对于保持整体用户满意度、参与度和保留率至关重要。然而，最近的研究主要集中在学术环境中的情绪和情感检测，因此未能完全涵盖现实世界用户数据的含义。为了弥补这一差距，在这项工作中，我们专注于部署的 TOD 系统中的用户挫败感，评估开箱即用的用户挫败感检测解决方案的可行性。具体来说，我们比较了我们部署的基于关键字的方法、情绪分析的开源方法、对话故障检测方法和新兴的基于上下文学习 LLM 的检测的性能。我们的分析强调了开源方法在现实世界挫败感检测中的局限性，同时展示了基于 LLM 的方法的卓越性能，在内部基准上实现了 F1 分数 16% 的相对提高。最后，我们分析了我们方法的优点和局限性，并为行业从业者提供了对用户挫败感检测任务的见解。</li>
</ul>

<h3>Title: Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search</h3>
<ul>
<li><strong>Authors: </strong>Andor Diera, Lukas Galke, Ansgar Scherp</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17538">https://arxiv.org/abs/2411.17538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17538">https://arxiv.org/pdf/2411.17538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17538]] Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search(https://arxiv.org/abs/2411.17538)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Low isotropy in an embedding space impairs performance on tasks involving semantic inference. Our study investigates the impact of isotropy on semantic code search performance and explores post-processing techniques to mitigate this issue. We analyze various code language models, examine isotropy in their embedding spaces, and its influence on search effectiveness. We propose a modified ZCA whitening technique to control isotropy levels in embeddings. Our results demonstrate that Soft-ZCA whitening improves the performance of pre-trained code language models and can complement contrastive fine-tuning. The code for our experiments is available at this https URL\_isotropy</li>
<li><strong>摘要：</strong>嵌入空间中的低各向同性会损害涉及语义推理的任务的性能。我们的研究调查了各向同性对语义代码搜索性能的影响，并探索了缓解此问题的后处理技术。我们分析了各种代码语言模型，检查了其嵌入空间中的各向同性及其对搜索效果的影响。我们提出了一种改进的 ZCA 白化技术来控制嵌入中的各向同性水平。我们的结果表明，Soft-ZCA 白化可以提高预训练代码语言模型的性能，并且可以补充对比微调。我们的实验代码可从此 https URL\_isotropy 获得</li>
</ul>

<h3>Title: Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Kuang, Jingyou Xie, Haohao Luo, Ronghao Li, Zhe Xu, Xianfeng Cheng, Yinghui Li, Xika Lin, Ying Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17558">https://arxiv.org/abs/2411.17558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17558">https://arxiv.org/pdf/2411.17558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17558]] Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey(https://arxiv.org/abs/2411.17558)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Visual Question Answering (VQA) is a challenge task that combines natural language processing and computer vision techniques and gradually becomes a benchmark test task in multimodal large language models (MLLMs). The goal of our survey is to provide an overview of the development of VQA and a detailed description of the latest models with high timeliness. This survey gives an up-to-date synthesis of natural language understanding of images and text, as well as the knowledge reasoning module based on image-question information on the core VQA tasks. In addition, we elaborate on recent advances in extracting and fusing modal information with vision-language pretraining models and multimodal large language models in VQA. We also exhaustively review the progress of knowledge reasoning in VQA by detailing the extraction of internal knowledge and the introduction of external knowledge. Finally, we present the datasets of VQA and different evaluation metrics and discuss possible directions for future work.</li>
<li><strong>摘要：</strong>视觉问答 (VQA) 是一项结合自然语言处理和计算机视觉技术的挑战任务，并逐渐成为多模态大型语言模型 (MLLM) 的基准测试任务。我们的调查旨在概述 VQA 的发展情况，并详细描述最新且时效性的模型。本调查对核心 VQA 任务中图像和文本的自然语言理解以及基于图像问题信息的知识推理模块进行了最新的综合。此外，我们详细阐述了在 VQA 中使用视觉语言预训练模型和多模态大型语言模型提取和融合模态信息的最新进展。我们还通过详细介绍内部知识的提取和外部知识的引入，详尽回顾了 VQA 中知识推理的进展。最后，我们展示了 VQA 的数据集和不同的评估指标，并讨论了未来工作的可能方向。</li>
</ul>

<h3>Title: Scaling Speech-Text Pre-training with Synthetic Interleaved Data</h3>
<ul>
<li><strong>Authors: </strong>Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17607">https://arxiv.org/abs/2411.17607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17607">https://arxiv.org/pdf/2411.17607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17607]] Scaling Speech-Text Pre-training with Synthetic Interleaved Data(https://arxiv.org/abs/2411.17607)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower sampling rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.</li>
<li><strong>摘要：</strong>语音语言模型 (SpeechLM) 接受语音输入并产生语音输出，与基于文本的大型语言模型 (LLM) 相比，它允许更自然的人机交互。开发 SpeechLM 的传统方法受到无监督语音数据和并行语音文本数据有限可用性的限制，这些数据比文本预训练数据少得多，从而限制了它们作为 LLM 的可扩展性。我们提出了一种扩展语音文本预训练的新方法，即利用从文本语料库中派生的大规模合成交错数据，从而无需并行语音文本数据集。我们的方法通过从现有文本语料库中采样文本跨度并使用文本到标记模型合成相应的语音跨度来有效地构建语音文本交错数据，从而无需生成实际语音。我们还采用了从自动语音识别 (ASR) 模型派生的监督语音标记器，方法是将矢量量化瓶颈合并到编码器中。这种监督训练方法即使在较低的采样率（例如 12.5Hz）下也能产生具有强大语义保留的离散语音标记，同时仍能保持语音重建质量。从预训练的语言模型开始，将预训练扩展到 1 万亿个标记（使用 600B 合成交错语音文本数据），我们在语音语言建模和口头问答方面取得了最先进的性能，将口头问题任务的性能从之前的 SOTA 13%（Moshi）提高到 31%。我们进一步证明，通过使用语音对话数据对预训练模型进行微调，我们可以开发一个端到端的口语聊天机器人，即使在语音领域单独运行，其在对话能力和语音质量方面也能实现与现有基线相当的竞争性能。</li>
</ul>

<h3>Title: On Limitations of LLM as Annotator for Low Resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Suramya Jadhav, Abhay Shanbhag, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17637">https://arxiv.org/abs/2411.17637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17637">https://arxiv.org/pdf/2411.17637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17637]] On Limitations of LLM as Annotator for Low Resource Languages(https://arxiv.org/abs/2411.17637)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Low-resource languages face significant challenges due to the lack of sufficient linguistic data, resources, and tools for tasks such as supervised learning, annotation, and classification. This shortage hinders the development of accurate models and datasets, making it difficult to perform critical NLP tasks like sentiment analysis or hate speech detection. To bridge this gap, Large Language Models (LLMs) present an opportunity for potential annotators, capable of generating datasets and resources for these underrepresented languages. In this paper, we focus on Marathi, a low-resource language, and evaluate the performance of both closed-source and open-source LLMs as annotators. We assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama 3.1 (8B) on classification tasks including sentiment analysis, news classification, and hate speech detection. Our findings reveal that while LLMs excel in annotation tasks for high-resource languages like English, they still fall short when applied to Marathi. Even advanced closed models like Gemini and GPT underperform in comparison to BERT-based baselines, highlighting the limitations of LLMs as annotators for low-resource languages.</li>
<li><strong>摘要：</strong>由于缺乏足够的语言数据、资源和工具来完成监督学习、注释和分类等任务，低资源语言面临着巨大的挑战。这种短缺阻碍了精确模型和数据集的开发，使得执行情绪分析或仇恨言论检测等关键 NLP 任务变得困难。为了弥补这一差距，大型语言模型 (LLM) 为潜在的注释者提供了机会，能够为这些代表性不足的语言生成数据集和资源。在本文中，我们专注于资源匮乏的语言马拉地语，并评估闭源和开源 LLM 作为注释者的表现。我们评估了 GPT-4o 和 Gemini 1.0 Pro、Gemma 2（2B 和 9B）和 Llama 3.1（8B）等模型在情绪分析、新闻分类和仇恨言论检测等分类任务上的表现。我们的研究结果表明，虽然 LLM 在英语等高资源语言的注释任务上表现出色，但在应用于马拉地语时仍然存在不足。与基于 BERT 的基线相比，即使是 Gemini 和 GPT 等先进的封闭模型的表现也不佳，这凸显了 LLM 作为低资源语言注释器的局限性。</li>
</ul>

<h3>Title: Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting</h3>
<ul>
<li><strong>Authors: </strong>Liyun Zhang, Dian Ding, Yu Lu, Yi-Chao Chen, Guangtao Xue</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17674">https://arxiv.org/abs/2411.17674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17674">https://arxiv.org/pdf/2411.17674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17674]] Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting(https://arxiv.org/abs/2411.17674)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Understanding the emotions in a dialogue usually requires external knowledge to accurately understand the contents. As the LLMs become more and more powerful, we do not want to settle on the limited ability of the pre-trained language model. However, the LLMs either can only process text modality or are too expensive to process the multimedia in- formation. We aim to utilize both the power of LLMs and the supplementary features from the multimedia modalities. In this paper, we present a framework, Lantern, that can improve the performance of a certain vanilla model by prompting large language models with receptive-field-aware attention weighting. This framework trained a multi-task vanilla model to produce probabilities of emotion classes and dimension scores. These predictions are fed into the LLMs as references to adjust the predicted probabilities of each emotion class with its external knowledge and contextual understanding. We slice the dialogue into different receptive fields, and each sample is included in exactly t receptive fields. Finally, the predictions of LLMs are merged with a receptive-field-aware attention-driven weighting module. In the experiments, vanilla models CORECT and SDT are deployed in Lantern with GPT-4 or Llama-3.1-405B. The experiments in IEMO- CAP with 4-way and 6-way settings demonstrated that the Lantern can significantly improve the performance of current vanilla models by up to 1.23% and 1.80%.</li>
<li><strong>摘要：</strong>理解对话中的情绪通常需要外部知识才能准确理解内容。随着 LLM 变得越来越强大，我们不想满足于预训练语言模型的有限能力。然而，LLM 要么只能处理文本模态，要么处理多媒体信息的成本太高。我们的目标是同时利用 LLM 的功能和多媒体模态的补充功能。在本文中，我们提出了一个框架 Lantern，它可以通过用感受野感知注意力加权提示大型语言模型来提高某个 vanilla 模型的性能。该框架训练了一个多任务 vanilla 模型来产生情绪类别和维度分数的概率。这些预测被输入到 LLM 中作为参考，以利用其外部知识和上下文理解来调整每个情绪类别的预测概率。我们将对话分成不同的感受野，每个样本都包含在恰好 t 个感受野中。最后，LLM 的预测与感受野感知注意力驱动的加权模块合并。实验中，将原始模型 CORECT 和 SDT 部署在 Lantern 中，并使用 GPT-4 或 Llama-3.1-405B 进行测试。在 IEMO-CAP 中进行的 4 路和 6 路设置实验表明，Lantern 可以显著提高当前原始模型的性能，最高提升幅度达 1.23% 和 1.80%。</li>
</ul>

<h3>Title: Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhu Xu, Zhiqiang Zhao, Zihan Zhang, Yuchi Liu, Quanwei Shen, Fei Liu, Yu Kuang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17679">https://arxiv.org/abs/2411.17679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17679">https://arxiv.org/pdf/2411.17679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17679]] Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning(https://arxiv.org/abs/2411.17679)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE (BBPE) have significantly improved the computational efficiency and vocabulary representation stability of large language models (LLMs) by segmenting text into tokens. However, this segmentation often obscures the internal character structures and sequences within tokens, preventing models from fully learning these intricate details during training. Consequently, LLMs struggle to comprehend the character compositions and positional relationships within tokens, especially when fine-tuned on downstream tasks with limited data. In this paper, we introduce Token Internal Position Awareness (TIPA), a novel approach that enhances LLMs' understanding of internal token structures by training them on reverse character prediction tasks using the tokenizer's own vocabulary. This method enables models to effectively learn and generalize character positions and internal structures. Experimental results demonstrate that LLMs trained with TIPA outperform baseline models in predicting character positions at the token level. Furthermore, when applied to the downstream task of Chinese Spelling Correction (CSC), TIPA not only accelerates model convergence but also significantly improves task performance.</li>
<li><strong>摘要：</strong>通过将文本分割成 token，字节对编码 (BPE) 和字节级 BPE (BBPE) 等 token 化技术显著提高了大型语言模型 (LLM) 的计算效率和词汇表征稳定性。然而，这种分割通常会掩盖 token 内的内部字符结构和序列，从而阻止模型在训练期间充分学习这些复杂的细节。因此，LLM 很难理解 token 内的字符组成和位置关系，尤其是在使用有限数据对下游任务进行微调时。在本文中，我们介绍了 token 内部位置感知 (TIPA)，这是一种新方法，它通过使用 token 化器自己的词汇对 LLM 进行反向字符预测任务训练，增强了 LLM 对内部 token 结构的理解。这种方法使模型能够有效地学习和概括字符位置和内部结构。实验结果表明，使用 TIPA 训练的 LLM 在预测 token 级别的字符位置方面优于基线模型。此外，当应用于下游的中文拼写纠正（CSC）任务时，TIPA 不仅加速了模型收敛，而且显著提高了任务性能。</li>
</ul>

<h3>Title: Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Wen, Vivek Hebbar, Caleb Larson, Aryan Bhatt, Ansh Radhakrishnan, Mrinank Sharma, Henry Sleight, Shi Feng, He He, Ethan Perez, Buck Shlegeris, Akbir Khan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2411.17693">https://arxiv.org/abs/2411.17693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2411.17693">https://arxiv.org/pdf/2411.17693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2411.17693]] Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats(https://arxiv.org/abs/2411.17693)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly capable, it is prudent to assess whether safety measures remain effective even if LLMs intentionally try to bypass them. Previous work introduced control evaluations, an adversarial framework for testing deployment strategies of untrusted models (i.e., models which might be trying to bypass safety measures). While prior work treats a single failure as unacceptable, we perform control evaluations in a "distributed threat setting" -- a setting where no single action is catastrophic and no single action provides overwhelming evidence of misalignment. We approach this problem with a two-level deployment framework that uses an adaptive macro-protocol to choose between micro-protocols. Micro-protocols operate on a single task, using a less capable, but extensively tested (trusted) model to harness and monitor the untrusted model. Meanwhile, the macro-protocol maintains an adaptive credence on the untrusted model's alignment based on its past actions, using it to pick between safer and riskier micro-protocols. We evaluate our method in a code generation testbed where a red team attempts to generate subtly backdoored code with an LLM whose deployment is safeguarded by a blue team. We plot Pareto frontiers of safety (# of non-backdoored solutions) and usefulness (# of correct solutions). At a given level of usefulness, our adaptive deployment strategy reduces the number of backdoors by 80% compared to non-adaptive baselines.</li>
<li><strong>摘要：</strong>随着大型语言模型 (LLM) 的功能越来越强大，明智的做法是评估安全措施是否仍然有效，即使 LLM 有意尝试绕过它们。先前的研究引入了控制评估，这是一种对抗性框架，用于测试不受信任模型（即可能试图绕过安全措施的模型）的部署策略。虽然先前的研究将单一故障视为不可接受，但我们在“分布式威胁设置”中执行控制评估——在这种设置中，没有任何单一操作是灾难性的，也没有任何单一操作提供压倒性的错位证据。我们使用两级部署框架来解决这个问题，该框架使用自适应宏协议在微协议之间进行选择。微协议在单个任务上运行，使用功能较弱但经过广泛测试（受信任）的模型来利用和监控不受信任的模型。同时，宏协议根据不受信任模型过去的行为保持对其对齐的自适应信任，使用它在更安全和更危险的微协议之间进行选择。我们在代码生成测试平台中评估了我们的方法，其中红队尝试使用 LLM 生成隐蔽后门代码，而 LLM 的部署由蓝队负责保障。我们绘制了安全性（无后门解决方案的数量）和实用性（正确解决方案的数量）的帕累托边界。在给定的实用性水平下，与非自适应基线相比，我们的自适应部署策略可将后门数量减少 80%。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
