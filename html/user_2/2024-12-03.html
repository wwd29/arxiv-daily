<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-03</h1>
<h3>Title: Safe to Serve: Aligning Instruction-Tuned Models for Safety and Helpfulness</h3>
<ul>
<li><strong>Authors: </strong>Avinash Amballa, Durga Sandeep Saluru, Gayathri Akkinapalli, Abhishek Sureddy, Akshay Kumar Sureddy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00074">https://arxiv.org/abs/2412.00074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00074">https://arxiv.org/pdf/2412.00074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00074]] Safe to Serve: Aligning Instruction-Tuned Models for Safety and Helpfulness(https://arxiv.org/abs/2412.00074)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning and text generation. However, these models can inadvertently generate unsafe or biased responses when prompted with problematic inputs, raising significant ethical and practical concerns for real-world deployment. This research addresses the critical challenge of developing language models that generate both helpful and harmless content, navigating the delicate balance between model performance and safety. We demonstrate that incorporating safety-related instructions during the instruction-tuning of pre-trained models significantly reduces toxic responses to unsafe prompts without compromising performance on helpfulness datasets. We found Direct Preference Optimization (DPO) to be particularly effective, outperforming both SIT and RAFT by leveraging both chosen and rejected responses for learning. Our approach increased safe responses from 40$\%$ to over 90$\%$ across various harmfulness benchmarks. In addition, we discuss a rigorous evaluation framework encompassing specialized metrics and diverse datasets for safety and helpfulness tasks ensuring a comprehensive assessment of the model's capabilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在复杂推理和文本生成方面表现出了卓越的能力。然而，当提示有问题的输入时，这些模型可能会无意中生成不安全或有偏见的响应，从而对实际部署提出了重大的道德和实际问题。这项研究解决了开发语言模型的关键挑战，这些模型可以生成有用和无害的内容，在模型性能和安全性之间找到微妙的平衡。我们证明，在预训练模型的指令调整过程中加入与安全相关的指令可以显著减少对不安全提示的毒性反应，而不会影响有用性数据集的性能。我们发现直接偏好优化 (DPO) 特别有效，通过利用选择和拒绝的响应进行学习，其表现优于 SIT 和 RAFT。我们的方法将各种有害性基准中的安全响应从 40% 提高到 90% 以上。此外，我们讨论了一个严格的评估框架，该框架包含专门的指标和针对安全和有用性任务的多样化数据集，确保对模型的能力进行全面评估。</li>
</ul>

<h3>Title: Fine-Tuning Large Language Models for Scientific Text Classification: A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Zhyar Rzgar K Rostam, Gábor Kertész</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00098">https://arxiv.org/abs/2412.00098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00098">https://arxiv.org/pdf/2412.00098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00098]] Fine-Tuning Large Language Models for Scientific Text Classification: A Comparative Study(https://arxiv.org/abs/2412.00098)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The exponential growth of online textual content across diverse domains has necessitated advanced methods for automated text classification. Large Language Models (LLMs) based on transformer architectures have shown significant success in this area, particularly in natural language processing (NLP) tasks. However, general-purpose LLMs often struggle with domain-specific content, such as scientific texts, due to unique challenges like specialized vocabulary and imbalanced data. In this study, we fine-tune four state-of-the-art LLMs BERT, SciBERT, BioBERT, and BlueBERT on three datasets derived from the WoS-46985 dataset to evaluate their performance in scientific text classification. Our experiments reveal that domain-specific models, particularly SciBERT, consistently outperform general-purpose models in both abstract-based and keyword-based classification tasks. Additionally, we compare our achieved results with those reported in the literature for deep learning models, further highlighting the advantages of LLMs, especially when utilized in specific domains. The findings emphasize the importance of domain-specific adaptations for LLMs to enhance their effectiveness in specialized text classification tasks.</li>
<li><strong>摘要：</strong>跨不同领域的在线文本内容呈指数级增长，因此需要先进的自动文本分类方法。基于 Transformer 架构的大型语言模型 (LLM) 已在此领域取得了显著成功，尤其是在自然语言处理 (NLP) 任务中。然而，由于专业词汇和数据不平衡等独特挑战，通用 LLM 通常在处理特定领域的内容（例如科学文本）时会遇到困难。在本研究中，我们在来自 WoS-46985 数据集的三个数据集上对四个最先进的 LLM BERT、SciBERT、BioBERT 和 BlueBERT 进行了微调，以评估它们在科学文本分类中的表现。我们的实验表明，领域特定模型（尤其是 SciBERT）在基于摘要和基于关键字的分类任务中始终优于通用模型。此外，我们将我们取得的结果与文献中报道的深度学习模型的结果进行了比较，进一步突出了 LLM 的优势，尤其是在特定领域使用时。研究结果强调了特定领域适应性对于 LLM 的重要性，以增强其在专门文本分类任务中的有效性。</li>
</ul>

<h3>Title: Efficient Learning Content Retrieval with Knowledge Injection</h3>
<ul>
<li><strong>Authors: </strong>Batuhan Sariturk, Rabia Bayraktar, Merve Elmas Erdem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00125">https://arxiv.org/abs/2412.00125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00125">https://arxiv.org/pdf/2412.00125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00125]] Efficient Learning Content Retrieval with Knowledge Injection(https://arxiv.org/abs/2412.00125)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>With the rise of online education platforms, there is a growing abundance of educational content across various domain. It can be difficult to navigate the numerous available resources to find the most suitable training, especially in domains that include many interconnected areas, such as ICT. In this study, we propose a domain-specific chatbot application that requires limited resources, utilizing versions of the Phi language model to help learners with educational content. In the proposed method, Phi-2 and Phi-3 models were fine-tuned using QLoRA. The data required for fine-tuning was obtained from the Huawei Talent Platform, where courses are available at different levels of expertise in the field of computer science. RAG system was used to support the model, which was fine-tuned by 500 Q&A pairs. Additionally, a total of 420 Q&A pairs of content were extracted from different formats such as JSON, PPT, and DOC to create a vector database to be used in the RAG system. By using the fine-tuned model and RAG approach together, chatbots with different competencies were obtained. The questions and answers asked to the generated chatbots were saved separately and evaluated using ROUGE, BERTScore, METEOR, and BLEU metrics. The precision value of the Phi-2 model supported by RAG was 0.84 and the F1 score was 0.82. In addition to a total of 13 different evaluation metrics in 4 different categories, the answers of each model were compared with the created content and the most appropriate method was selected for real-life applications.</li>
<li><strong>摘要：</strong>随着在线教育平台的兴起，各个领域的教育内容日益丰富。在众多可用资源中找到最合适的培训可能很困难，尤其是在包含许多相互关联的领域，例如 ICT。在本研究中，我们提出了一个领域特定的聊天机器人应用程序，它需要有限的资源，利用 Phi 语言模型的版本来帮助学习者提供教育内容。在提出的方法中，使用 QLoRA 对 Phi-2 和 Phi-3 模型进行了微调。微调所需的数据来自华为人才平台，该平台提供计算机科学领域不同专业水平的课程。RAG 系统用于支持该模型，该模型通过 500 个问答对进行了微调。此外，从 JSON、PPT 和 DOC 等不同格式中提取了总共 420 个问答对内容，以创建用于 RAG 系统的矢量数据库。通过结合使用微调模型和 RAG 方法，获得了具有不同能力的聊天机器人。生成的聊天机器人的问题和答案被分别保存，并使用 ROUGE、BERTScore、METEOR 和 BLEU 指标进行评估。RAG 支持的 Phi-2 模型的精度值为 0.84，F1 分数为 0.82。除了 4 个不同类别的总共 13 个不同的评估指标外，还将每个模型的答案与创建的内容进行比较，并选择最合适的方法用于实际应用。</li>
</ul>

<h3>Title: To Ensemble or Not: Assessing Majority Voting Strategies for Phishing Detection with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fouad Trad, Ali Chehab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00166">https://arxiv.org/abs/2412.00166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00166">https://arxiv.org/pdf/2412.00166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00166]] To Ensemble or Not: Assessing Majority Voting Strategies for Phishing Detection with Large Language Models(https://arxiv.org/abs/2412.00166)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>The effectiveness of Large Language Models (LLMs) significantly relies on the quality of the prompts they receive. However, even when processing identical prompts, LLMs can yield varying outcomes due to differences in their training processes. To leverage the collective intelligence of multiple LLMs and enhance their performance, this study investigates three majority voting strategies for text classification, focusing on phishing URL detection. The strategies are: (1) a prompt-based ensemble, which utilizes majority voting across the responses generated by a single LLM to various prompts; (2) a model-based ensemble, which entails aggregating responses from multiple LLMs to a single prompt; and (3) a hybrid ensemble, which combines the two methods by sending different prompts to multiple LLMs and then aggregating their responses. Our analysis shows that ensemble strategies are most suited in cases where individual components exhibit equivalent performance levels. However, when there is a significant discrepancy in individual performance, the effectiveness of the ensemble method may not exceed that of the highest-performing single LLM or prompt. In such instances, opting for ensemble techniques is not recommended.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的有效性在很大程度上取决于它们收到的提示的质量。然而，即使在处理相同的提示时，由于训练过程的差异，LLM 也会产生不同的结果。为了利用多个 LLM 的集体智慧并提高其性能，本研究调查了三种用于文本分类的多数投票策略，重点是网络钓鱼 URL 检测。这些策略是：(1) 基于提示的集成，它利用单个 LLM 对各种提示生成的响应的多数投票；(2) 基于模型的集成，它需要将多个 LLM 的响应聚合到一个提示；(3) 混合集成，它通过将不同的提示发送到多个 LLM 然后聚合它们的响应来结合这两种方法。我们的分析表明，集成策略最适合单个组件表现出同等性能水平的情况。然而，当个体表现存在显著差异时，集成方法的有效性可能不会超过表现最好的单个 LLM 或提示。在这种情况下，不建议选择集成技术。</li>
</ul>

<h3>Title: N\"ushuRescue: Revitalization of the endangered N\"ushu Language with AI</h3>
<ul>
<li><strong>Authors: </strong>Ivory Yang, Weicheng Ma, Soroush Vosoughi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00218">https://arxiv.org/abs/2412.00218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00218">https://arxiv.org/pdf/2412.00218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00218]] N\"ushuRescue: Revitalization of the endangered N\"ushu Language with AI(https://arxiv.org/abs/2412.00218)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>The preservation and revitalization of endangered and extinct languages is a meaningful endeavor, conserving cultural heritage while enriching fields like linguistics and anthropology. However, these languages are typically low-resource, making their reconstruction labor-intensive and costly. This challenge is exemplified by Nüshu, a rare script historically used by Yao women in China for self-expression within a patriarchal society. To address this challenge, we introduce NüshuRescue, an AI-driven framework designed to train large language models (LLMs) on endangered languages with minimal data. NüshuRescue automates evaluation and expands target corpora to accelerate linguistic revitalization. As a foundational component, we developed NCGold, a 500-sentence Nüshu-Chinese parallel corpus, the first publicly available dataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to Nüshu and only 35 short examples from NCGold, NüshuRescue achieved 48.69\% translation accuracy on 50 withheld sentences and generated NCSilver, a set of 98 newly translated modern Chinese sentences of varying lengths. A sample of both NCGold and NCSilver is included in the Supplementary Materials. Additionally, we developed FastText-based and Seq2Seq models to further support research on Nüshu. NüshuRescue provides a versatile and scalable tool for the revitalization of endangered languages, minimizing the need for extensive human input.</li>
<li><strong>摘要：</strong>保护和振兴濒危和灭绝语言是一项有意义的事业，既能保护文化遗产，又能丰富语言学和人类学等领域。然而，这些语言通常资源匮乏，因此重建起来既费力又费钱。女书就是这一挑战的典型例子，女书是一种罕见的文字，历史上是中国瑶族妇女在父权社会中用来表达自我的文字。为了应对这一挑战，我们推出了 NüshuRescue，这是一个人工智能驱动的框架，旨在用最少的数据训练濒危语言的大型语言模型 (LLM)。NüshuRescue 可自动评估并扩展目标语料库，以加速语言复兴。作为基础组件，我们开发了 NCGold，这是一个包含 500 句的女书-中文平行语料库，这是同类中第一个公开可用的数据集。利用 GPT-4-Turbo，在之前没有接触过女书的情况下，NüshuRescue 仅使用了 NCGold 中的 35 个简短示例，在 50 个保留句子上实现了 48.69% 的翻译准确率，并生成了 NCSilver，这是一组 98 个新翻译的现代汉语句子，长度不一。补充材料中包含了 NCGold 和 NCSilver 的样本。此外，我们还开发了基于 FastText 和 Seq2Seq 模型，以进一步支持对女书的研究。NüshuRescue 为濒危语言的复兴提供了一种多功能且可扩展的工具，最大限度地减少了对大量人工输入的需求。</li>
</ul>

<h3>Title: Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments</h3>
<ul>
<li><strong>Authors: </strong>Yasuaki Sumita, Koh Takeuchi, Hisashi Kashima</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00323">https://arxiv.org/abs/2412.00323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00323">https://arxiv.org/pdf/2412.00323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00323]] Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments(https://arxiv.org/abs/2412.00323)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are trained on large corpora written by humans and demonstrate high performance on various tasks. However, as humans are susceptible to cognitive biases, which can result in irrational judgments, LLMs can also be influenced by these biases, leading to irrational decision-making. For example, changing the order of options in multiple-choice questions affects the performance of LLMs due to order bias. In our research, we first conducted an extensive survey of existing studies examining LLMs' cognitive biases and their mitigation. The mitigation techniques in LLMs have the disadvantage that they are limited in the type of biases they can apply or require lengthy inputs or outputs. We then examined the effectiveness of two mitigation methods for humans, SoPro and AwaRe, when applied to LLMs, inspired by studies in crowdsourcing. To test the effectiveness of these methods, we conducted experiments on GPT-3.5 and GPT-4 to evaluate the influence of six biases on the outputs before and after applying these methods. The results demonstrate that while SoPro has little effect, AwaRe enables LLMs to mitigate the effect of these biases and make more rational responses.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在人类编写的大型语料库上进行训练，并在各种任务上表现出色。然而，由于人类容易受到认知偏见的影响，从而导致非理性判断，LLM 也会受到这些偏见的影响，从而导致非理性决策。例如，由于顺序偏见，改变多项选择题中选项的顺序会影响 LLM 的性能。在我们的研究中，我们首先对现有的研究 LLM 的认知偏见及其缓解方法进行了广泛的调查。LLM 中的缓解技术的缺点是它们可以应用的偏见类型有限，或者需要冗长的输入或输出。然后，我们研究了两种针对人类的缓解方法 SoPro 和 AwaRe 在应用于 LLM 时的有效性，这些方法受到众包研究的启发。为了测试这些方法的有效性，我们在 GPT-3.5 和 GPT-4 上进行了实验，以评估应用这些方法之前和之后六种偏见对输出的影响。结果表明，虽然SoPro 效果不大，但 AwaRe 可以让法学硕士减轻这些偏见的影响，并做出更理性的反应。</li>
</ul>

<h3>Title: Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided Strategy Selection</h3>
<ul>
<li><strong>Authors: </strong>Shanu Kumar, Saish Mendke, Karody Lubna Abdul Rahman, Santosh Kurasa, Parag Agrawal, Sandipan Dandapat</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00353">https://arxiv.org/abs/2412.00353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00353">https://arxiv.org/pdf/2412.00353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00353]] Enhancing Zero-shot Chain of Thought Prompting via Uncertainty-Guided Strategy Selection(https://arxiv.org/abs/2412.00353)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Chain-of-thought (CoT) prompting has significantly enhanced the capability of large language models (LLMs) by structuring their reasoning processes. However, existing methods face critical limitations: handcrafted demonstrations require extensive human expertise, while trigger phrases are prone to inaccuracies. In this paper, we propose the Zero-shot Uncertainty-based Selection (ZEUS) method, a novel approach that improves CoT prompting by utilizing uncertainty estimates to select effective demonstrations without needing access to model parameters. Unlike traditional methods, ZEUS offers high sensitivity in distinguishing between helpful and ineffective questions, ensuring more precise and reliable selection. Our extensive evaluation shows that ZEUS consistently outperforms existing CoT strategies across four challenging reasoning benchmarks, demonstrating its robustness and scalability.</li>
<li><strong>摘要：</strong>思路链 (CoT) 提示通过构建推理过程显著增强了大型语言模型 (LLM) 的能力。然而，现有方法面临严重的局限性：手工制作的演示需要大量的人类专业知识，而触发短语容易出现错误。在本文中，我们提出了零样本不确定性选择 (ZEUS) 方法，这是一种新颖的方法，它利用不确定性估计来选择有效的演示，而无需访问模型参数，从而改进了 CoT 提示。与传统方法不同，ZEUS 在区分有用和无效问题方面具有很高的灵敏度，从而确保了更精确和可靠的选择。我们广泛的评估表明，ZEUS 在四个具有挑战性的推理基准上始终优于现有的 CoT 策略，证明了其稳健性和可扩展性。</li>
</ul>

<h3>Title: Non-native speakers of English or ChatGPT: Who thinks better?</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Q. Shormani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00457">https://arxiv.org/abs/2412.00457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00457">https://arxiv.org/pdf/2412.00457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00457]] Non-native speakers of English or ChatGPT: Who thinks better?(https://arxiv.org/abs/2412.00457)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, chat</a></li>
<li><strong>Abstract: </strong>This study sets out to answer one major question: Who thinks better, non-native speakers of English or ChatGPT?, providing evidence from processing and interpreting center-embedding English constructions that human brain surpasses ChatGPT, and that ChatGPT cannot be regarded as a theory of language. Fifteen non-native speakers of English were recruited as participants of the study. A center-embedding English sentence was presented to both the study participants and ChatGPT. The study findings unveil that human brain is still far ahead of Large Language Models, specifically ChatGPT, even in the case of non-native speakers of an L2, here English. The study concludes that human brain's ability to process and interpret natural language data is unique and that ChatGPT still lags behind this human unique ability.</li>
<li><strong>摘要：</strong>本研究旨在回答一个主要问题：非英语母语人士和 ChatGPT 谁的思维更敏捷？研究通过处理和解释中心嵌入的英语结构提供证据表明，人类大脑超越了 ChatGPT，并且 ChatGPT 不能被视为一种语言理论。研究招募了 15 名非英语母语人士作为参与者。向研究参与者和 ChatGPT 展示了一个中心嵌入的英语句子。研究结果表明，即使对于非 L2（这里是英语）母语人士来说，人类大脑仍然远远领先于大型语言模型，特别是 ChatGPT。研究得出结论，人类大脑处理和解释自然语言数据的能力是独一无二的，而 ChatGPT 仍然落后于人类这种独特的能力。</li>
</ul>

<h3>Title: GloCOM: A Short Text Neural Topic Model via Global Clustering Context</h3>
<ul>
<li><strong>Authors: </strong>Quang Duc Nguyen, Tung Nguyen, Duc Anh Nguyen, Linh Ngo Van, Sang Dinh, Thien Huu Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00525">https://arxiv.org/abs/2412.00525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00525">https://arxiv.org/pdf/2412.00525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00525]] GloCOM: A Short Text Neural Topic Model via Global Clustering Context(https://arxiv.org/abs/2412.00525)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Uncovering hidden topics from short texts is challenging for traditional and neural models due to data sparsity, which limits word co-occurrence patterns, and label sparsity, stemming from incomplete reconstruction targets. Although data aggregation offers a potential solution, existing neural topic models often overlook it due to time complexity, poor aggregation quality, and difficulty in inferring topic proportions for individual documents. In this paper, we propose a novel model, GloCOM (Global Clustering COntexts for Topic Models), which addresses these challenges by constructing aggregated global clustering contexts for short documents, leveraging text embeddings from pre-trained language models. GloCOM can infer both global topic distributions for clustering contexts and local distributions for individual short texts. Additionally, the model incorporates these global contexts to augment the reconstruction loss, effectively handling the label sparsity issue. Extensive experiments on short text datasets show that our approach outperforms other state-of-the-art models in both topic quality and document representations.</li>
<li><strong>摘要：</strong>对于传统和神经模型来说，从短文本中发现隐藏的主题是一项挑战，因为数据稀疏性会限制单词共现模式，而标签稀疏性则源于不完整的重建目标。尽管数据聚合提供了一种潜在的解决方案，但现有的神经主题模型通常会忽略它，因为时间复杂度、聚合质量差以及难以推断单个文档的主题比例。在本文中，我们提出了一个新模型 GloCOM（主题模型的全局聚类上下文），该模型通过构建短文档的聚合全局聚类上下文来解决这些挑战，利用来自预训练语言模型的文本嵌入。GloCOM 可以推断聚类上下文的全局主题分布和单个短文本的局部分布。此外，该模型结合了这些全局上下文来增加重建损失，有效地处理了标签稀疏性问题。在短文本数据集上进行的大量实验表明，我们的方法在主题质量和文档表示方面都优于其他最先进的模型。</li>
</ul>

<h3>Title: ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance & Efficiency on a Specific Domain</h3>
<ul>
<li><strong>Authors: </strong>Ali Shiraee Kasmaee, Mohammad Khodadad, Mohammad Arshi Saloot, Nick Sherck, Stephen Dokas, Hamidreza Mahyar, Soheila Samiee</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00532">https://arxiv.org/abs/2412.00532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00532">https://arxiv.org/pdf/2412.00532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00532]] ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance & Efficiency on a Specific Domain(https://arxiv.org/abs/2412.00532)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in language models have started a new era of superior information retrieval and content generation, with embedding models playing an important role in optimizing data representation efficiency and performance. While benchmarks like the Massive Text Embedding Benchmark (MTEB) have standardized the evaluation of general domain embedding models, a gap remains in specialized fields such as chemistry, which require tailored approaches due to domain-specific challenges. This paper introduces a novel benchmark, the Chemical Text Embedding Benchmark (ChemTEB), designed specifically for the chemical sciences. ChemTEB addresses the unique linguistic and semantic complexities of chemical literature and data, offering a comprehensive suite of tasks on chemical domain data. Through the evaluation of 34 open-source and proprietary models using this benchmark, we illuminate the strengths and weaknesses of current methodologies in processing and understanding chemical information. Our work aims to equip the research community with a standardized, domain-specific evaluation framework, promoting the development of more precise and efficient NLP models for chemistry-related applications. Furthermore, it provides insights into the performance of generic models in a domain-specific context. ChemTEB comes with open-source code and data, contributing further to its accessibility and utility.</li>
<li><strong>摘要：</strong>语言模型的最新进展开启了卓越信息检索和内容生成的新时代，嵌入模型在优化数据表示效率和性能方面发挥着重要作用。虽然海量文本嵌入基准 (MTEB) 等基准已经标准化了通用领域嵌入模型的评估，但在化学等专业领域仍然存在差距，由于特定领域的挑战，这些领域需要量身定制的方法。本文介绍了一种专为化学科学设计的新基准——化学文本嵌入基准 (ChemTEB)。ChemTEB 解决了化学文献和数据独特的语言和语义复杂性，提供了一套全面的化学领域数据任务。通过使用此基准对 34 个开源和专有模型进行评估，我们阐明了当前方法在处理和理解化学信息方面的优缺点。我们的工作旨在为研究界提供一个标准化的、特定领域的评估框架，促进为化学相关应用开发更精确、更高效的 NLP 模型。此外，它还提供了对通用模型在特定领域环境中性能的洞察。ChemTEB 附带开源代码和数据，进一步提高了其可访问性和实用性。</li>
</ul>

<h3>Title: TextClass Benchmark: A Continuous Elo Rating of LLMs in Social Sciences</h3>
<ul>
<li><strong>Authors: </strong>Bastián González-Bustamante</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00539">https://arxiv.org/abs/2412.00539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00539">https://arxiv.org/pdf/2412.00539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00539]] TextClass Benchmark: A Continuous Elo Rating of LLMs in Social Sciences(https://arxiv.org/abs/2412.00539)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>The TextClass Benchmark project is an ongoing, continuous benchmarking process that aims to provide a comprehensive, fair, and dynamic evaluation of LLMs and transformers for text classification tasks. This evaluation spans various domains and languages in social sciences disciplines engaged in NLP and text-as-data approach. The leaderboards present performance metrics and relative ranking using a tailored Elo rating system. With each leaderboard cycle, novel models are added, fixed test sets can be replaced for unseen, equivalent data to test generalisation power, ratings are updated, and a Meta-Elo leaderboard combines and weights domain-specific leaderboards. This article presents the rationale and motivation behind the project, explains the Elo rating system in detail, and estimates Meta-Elo across different classification tasks in social science disciplines. We also present a snapshot of the first cycle of classification tasks on incivility data in Chinese, English, German and Russian. This ongoing benchmarking process includes not only additional languages such as Arabic, Hindi, and Spanish but also a classification of policy agenda topics, misinformation, among others.</li>
<li><strong>摘要：</strong>TextClass Benchmark 项目是一个持续不断的基准测试过程，旨在为文本分类任务提供 LLM 和转换器的全面、公平和动态评估。该评估涵盖了从事 NLP 和文本即数据方法的社会科学学科中的各个领域和语言。排行榜使用量身定制的 Elo 评分系统显示性能指标和相对排名。在每个排行榜周期中，都会添加新模型，可以用看不见的等效数据替换固定测试集​​以测试泛化能力，更新评级，并且 Meta-Elo 排行榜会结合和加权特定领域的排行榜。本文介绍了该项目背后的原理和动机，详细解释了 Elo 评分系统，并估计了社会科学学科中不同分类任务的 Meta-Elo。我们还展示了第一轮对中文、英文、德文和俄文不文明数据进行分类任务的快照。这个持续的基准测试过程不仅包括阿拉伯语、印地语和西班牙语等其他语言，还包括政策议程主题、错误信息等的分类。</li>
</ul>

<h3>Title: Evaluating the Consistency of LLM Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Noah Lee, Jiwoo Hong, James Thorne</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00543">https://arxiv.org/abs/2412.00543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00543">https://arxiv.org/pdf/2412.00543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00543]] Evaluating the Consistency of LLM Evaluators(https://arxiv.org/abs/2412.00543)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown potential as general evaluators along with the evident benefits of speed and cost. While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators. In this paper, we conduct extensive studies on the two aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-scale Consistency (IC), on different scoring scales and criterion granularity with open-source and proprietary models. Our comprehensive analysis demonstrates that strong proprietary models are not necessarily consistent evaluators, highlighting the importance of considering consistency in assessing the capability of LLM evaluators.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已显示出作为通用评估器的潜力，同时具有速度和成本方面的明显优势。虽然它们与人工注释者的相关性已被广泛研究，但作为评估器的一致性仍未得到充分研究，这引发了人们对 LLM 评估器可靠性的担忧。在本文中，我们使用开源和专有模型对 LLM 评估中一致性的两个方面，即自一致性 (SC) 和尺度间一致性 (IC) 进行了广泛的研究，研究范围涵盖不同的评分尺度和标准粒度。我们的综合分析表明，强大的专有模型不一定是一致的评估器，这凸显了在评估 LLM 评估器能力时考虑一致性的重要性。</li>
</ul>

<h3>Title: SeQwen at the Financial Misinformation Detection Challenge Task: Sequential Learning for Claim Verification and Explanation Generation in Financial Domains</h3>
<ul>
<li><strong>Authors: </strong>Jebish Purbey, Siddhant Gupta, Nikhil Manali, Siddartha Pullakhandam, Drishti Sharma, Ashay Srivastava, Ram Mohan Rao Kadiyala</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CE, cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00549">https://arxiv.org/abs/2412.00549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00549">https://arxiv.org/pdf/2412.00549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00549]] SeQwen at the Financial Misinformation Detection Challenge Task: Sequential Learning for Claim Verification and Explanation Generation in Financial Domains(https://arxiv.org/abs/2412.00549)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This paper presents the system description of our entry for the COLING 2025 FMD challenge, focusing on misinformation detection in financial domains. We experimented with a combination of large language models, including Qwen, Mistral, and Gemma-2, and leveraged pre-processing and sequential learning for not only identifying fraudulent financial content but also generating coherent, and concise explanations that clarify the rationale behind the classifications. Our approach achieved competitive results with an F1-score of 0.8283 for classification, and ROUGE-1 of 0.7253 for explanations. This work highlights the transformative potential of LLMs in financial applications, offering insights into their capabilities for combating misinformation and enhancing transparency while identifying areas for future improvement in robustness and domain adaptation.</li>
<li><strong>摘要：</strong>本文介绍了我们参加 COLING 2025 FMD 挑战赛的系统描述，重点关注金融领域的错误信息检测。我们尝试了多种大型语言模型，包括 Qwen、Mistral 和 Gemma-2，并利用预处理和顺序学习不仅可以识别欺诈性金融内容，还可以生成连贯而简洁的解释，阐明分类背后的原理。我们的方法取得了有竞争力的结果，分类的 F1 得分为 0.8283，解释的 ROUGE-1 得分为 0.7253。这项工作突出了 LLM 在金融应用中的变革潜力，深入了解了它们打击错误信息和提高透明度的能力，同时确定了未来在稳健性和领域适应性方面需要改进的领域。</li>
</ul>

<h3>Title: Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yue Zhou, Barbara Di Eugenio, Lu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00554">https://arxiv.org/abs/2412.00554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00554">https://arxiv.org/pdf/2412.00554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00554]] Unveiling Performance Challenges of Large Language Models in Low-Resource Healthcare: A Demographic Fairness Perspective(https://arxiv.org/abs/2412.00554)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>This paper studies the performance of large language models (LLMs), particularly regarding demographic fairness, in solving real-world healthcare tasks. We evaluate state-of-the-art LLMs with three prevalent learning frameworks across six diverse healthcare tasks and find significant challenges in applying LLMs to real-world healthcare tasks and persistent fairness issues across demographic groups. We also find that explicitly providing demographic information yields mixed results, while LLM's ability to infer such details raises concerns about biased health predictions. Utilizing LLMs as autonomous agents with access to up-to-date guidelines does not guarantee performance improvement. We believe these findings reveal the critical limitations of LLMs in healthcare fairness and the urgent need for specialized research in this area.</li>
<li><strong>摘要：</strong>本文研究了大型语言模型 (LLM) 在解决现实世界医疗任务方面的表现，特别是在人口公平性方面。我们评估了具有三种流行学习框架的最先进的 LLM，涉及六种不同的医疗任务，发现将 LLM 应用于现实世界的医疗任务存在重大挑战，并且跨人口群体的公平性问题持续存在。我们还发现，明确提供人口统计信息会产生不同的结果，而 LLM 推断此类细节的能力引发了人们对有偏见的健康预测的担忧。将 LLM 用作可以访问最新指南的自主代理并不能保证性能的提高。我们认为这些发现揭示了 LLM 在医疗公平性方面的关键局限性以及迫切需要在该领域进行专门研究。</li>
</ul>

<h3>Title: Polish Medical Exams: A new dataset for cross-lingual medical knowledge transfer assessment</h3>
<ul>
<li><strong>Authors: </strong>Łukasz Grzybowski, Jakub Pokrywka, Michał Ciesiółka, Jeremi I. Kaczmarek, Marek Kubis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00559">https://arxiv.org/abs/2412.00559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00559">https://arxiv.org/pdf/2412.00559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00559]] Polish Medical Exams: A new dataset for cross-lingual medical knowledge transfer assessment(https://arxiv.org/abs/2412.00559)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated significant potential in handling specialized tasks, including medical problem-solving. However, most studies predominantly focus on English-language contexts. This study introduces a novel benchmark dataset based on Polish medical licensing and specialization exams (LEK, LDEK, PES) taken by medical doctor candidates and practicing doctors pursuing specialization. The dataset was web-scraped from publicly available resources provided by the Medical Examination Center and the Chief Medical Chamber. It comprises over 24,000 exam questions, including a subset of parallel Polish-English corpora, where the English portion was professionally translated by the examination center for foreign candidates. By creating a structured benchmark from these existing exam questions, we systematically evaluate state-of-the-art LLMs, including general-purpose, domain-specific, and Polish-specific models, and compare their performance against human medical students. Our analysis reveals that while models like GPT-4o achieve near-human performance, significant challenges persist in cross-lingual translation and domain-specific understanding. These findings underscore disparities in model performance across languages and medical specialties, highlighting the limitations and ethical considerations of deploying LLMs in clinical practice.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在处理专业任务（包括解决医学问题）方面表现出了巨大的潜力。然而，大多数研究主要集中在英语环境中。这项研究引入了一个基于波兰医学执照和专业考试（LEK、LDEK、PES）的新型基准数据集，这些考试由医学博士候选人和追求专业化的执业医生参加。该数据集是从医学考试中心和首席医学委员会提供的公开资源中抓取的。它包含 24,000 多道考试题目，包括波兰语-英语平行语料库的子集，其中英语部分由考试中心为外国考生进行专业翻译。通过从这些现有考试题目中创建结构化基准，我们系统地评估了最先进的 LLM，包括通用、领域特定和波兰特定模型，并将它们的表现与人类医学生进行比较。我们的分析表明，虽然像 GPT-4o 这样的模型实现了接近人类的表现，但在跨语言翻译和领域特定理解方面仍然存在重大挑战。这些发现强调了不同语言和医学专业的模型表现的差异，凸显了在临床实践中部署 LLM 的局限性和道德考虑。</li>
</ul>

<h3>Title: DynRank: Improving Passage Retrieval with Dynamic Zero-Shot Prompting Based on Question Classification</h3>
<ul>
<li><strong>Authors: </strong>Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Mohammed M.Abdelgwad, Adam Jatowt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00600">https://arxiv.org/abs/2412.00600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00600">https://arxiv.org/pdf/2412.00600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00600]] DynRank: Improving Passage Retrieval with Dynamic Zero-Shot Prompting Based on Question Classification(https://arxiv.org/abs/2412.00600)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>This paper presents DynRank, a novel framework for enhancing passage retrieval in open-domain question-answering systems through dynamic zero-shot question classification. Traditional approaches rely on static prompts and pre-defined templates, which may limit model adaptability across different questions and contexts. In contrast, DynRank introduces a dynamic prompting mechanism, leveraging a pre-trained question classification model that categorizes questions into fine-grained types. Based on these classifications, contextually relevant prompts are generated, enabling more effective passage retrieval. We integrate DynRank into existing retrieval frameworks and conduct extensive experiments on multiple QA benchmark datasets.</li>
<li><strong>摘要：</strong>本文介绍了 DynRank，这是一种通过动态零样本问题分类来增强开放域问答系统中段落检索的新框架。传统方法依赖于静态提示和预定义模板，这可能会限制模型在不同问题和上下文中的适应性。相比之下，DynRank 引入了一种动态提示机制，利用预先训练的问题分类模型将问题分为细粒度类型。基于这些分类，生成与上下文相关的提示，从而实现更有效的段落检索。我们将 DynRank 集成到现有的检索框架中，并在多个 QA 基准数据集上进行了广泛的实验。</li>
</ul>

<h3>Title: Multi-Agent Collaboration in Incident Response with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zefang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00652">https://arxiv.org/abs/2412.00652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00652">https://arxiv.org/pdf/2412.00652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00652]] Multi-Agent Collaboration in Incident Response with Large Language Models(https://arxiv.org/abs/2412.00652)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Incident response (IR) is a critical aspect of cybersecurity, requiring rapid decision-making and coordinated efforts to address cyberattacks effectively. Leveraging large language models (LLMs) as intelligent agents offers a novel approach to enhancing collaboration and efficiency in IR scenarios. This paper explores the application of LLM-based multi-agent collaboration using the Backdoors & Breaches framework, a tabletop game designed for cybersecurity training. We simulate real-world IR dynamics through various team structures, including centralized, decentralized, and hybrid configurations. By analyzing agent interactions and performance across these setups, we provide insights into optimizing multi-agent collaboration for incident response. Our findings highlight the potential of LLMs to enhance decision-making, improve adaptability, and streamline IR processes, paving the way for more effective and coordinated responses to cyber threats.</li>
<li><strong>摘要：</strong>事件响应 (IR) 是网络安全的一个关键方面，需要快速决策和协调努力才能有效应对网络攻击。利用大型语言模型 (LLM) 作为智能代理提供了一种在 IR 场景中增强协作和效率的新方法。本文探讨了使用 Backdoors & Breaches 框架（一种专为网络安全培训设计的桌面游戏）的基于 LLM 的多智能体协作的应用。我们通过各种团队结构模拟现实世界的 IR 动态，包括集中式、分散式和混合配置。通过分析这些设置中的代理交互和性能，我们提供了优化多智能体协作以进行事件响应的见解。我们的研究结果强调了 LLM 在增强决策、提高适应性和简化 IR 流程方面的潜力，为更有效和协调地应对网络威胁铺平了道路。</li>
</ul>

<h3>Title: Towards Adaptive Mechanism Activation in Language Agent</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Huang, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00722">https://arxiv.org/abs/2412.00722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00722">https://arxiv.org/pdf/2412.00722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00722]] Towards Adaptive Mechanism Activation in Language Agent(https://arxiv.org/abs/2412.00722)</code><input type="text"></li>
<li><strong>Keywords: </strong>agent</a></li>
<li><strong>Abstract: </strong>Language Agent could be endowed with different mechanisms for autonomous task accomplishment. Current agents typically rely on fixed mechanisms or a set of mechanisms activated in a predefined order, limiting their adaptation to varied potential task solution structures. To this end, this paper proposes \textbf{A}daptive \textbf{L}anguage \textbf{A}gent \textbf{M}echanism \textbf{A}ctivation Learning with Self-Exploration (\textbf{ALAMA}), which focuses on optimizing mechanism activation adaptability without reliance on expert models. Initially, it builds a harmonized agent framework (\textbf{UniAct}) to \textbf{Uni}fy different mechanisms via \textbf{Act}ions. Then it leverages a training-efficient optimization method based on self-exploration to enable the UniAct to adaptively activate the appropriate mechanisms according to the potential characteristics of the task. Experimental results demonstrate significant improvements in downstream agent tasks, affirming the effectiveness of our approach in facilitating more dynamic and context-sensitive mechanism activation.</li>
<li><strong>摘要：</strong>语言代理可以配备不同的机制来自主完成任务。当前的代理通常依赖于固定机制或一组按预定义顺序激活的机制，从而限制了它们对各种潜在任务解决方案结构的适应性。为此，本文提出了\textbf{A}自适应\textbf{语言\textbf{A}代理\textbf{机制\textbf{A}激活学习与自我探索（\textbf{ALAMA}），其重点是在不依赖专家模型的情况下优化机制激活适应性。首先，它构建了一个协调的代理框架（\textbf{UniAct}），通过\textbf{Act}ions\textbf{统一}不同的机制。然后，它利用基于自我探索的训练效率优化方法，使UniAct能够根据任务的潜在特征自适应地激活适当的机制。实验结果表明下游代理任务有显著改善，证明了我们的方法在促进更动态和上下文敏感的机制激活方面的有效性。</li>
</ul>

<h3>Title: PGSO: Prompt-based Generative Sequence Optimization Network for Aspect-based Sentiment Analysis</h3>
<ul>
<li><strong>Authors: </strong>Hao Dong, Wei Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00763">https://arxiv.org/abs/2412.00763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00763">https://arxiv.org/pdf/2412.00763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00763]] PGSO: Prompt-based Generative Sequence Optimization Network for Aspect-based Sentiment Analysis(https://arxiv.org/abs/2412.00763)</code><input type="text"></li>
<li><strong>Keywords: </strong>prompt</a></li>
<li><strong>Abstract: </strong>Recently, generative pre-training based models have demonstrated remarkable results on Aspect-based Sentiment Analysis (ABSA) task. However, previous works overemphasize crafting various templates to paraphrase training targets for enhanced decoding, ignoring the internal optimizations on generative models. Despite notable results achieved by these target-oriented optimization methods, they struggle with the complicated long texts since the implicit long-distance relation, e.g., aspect-opinion relation, is difficult to extract under the position embedding mechanism in generative models. Thus, in this paper, we first clarify the causes of the problem and introduce two sequence optimization strategies: the rule-based static optimization and the score-based dynamic optimization. The rule-based approach relies on handcraft priority of dependency relation to reorder the context, while the score-based algorithm dynamically regulates the contextual sequence by calculating word position scores using neural network. Based on the dynamic optimization structure, we further propose a unified Prompt-based Generative Sequence Optimization network (named PGSO), which jointly optimizes the training target as well as the generative model. Specifically, PGSO contains two components, namely, prompt construction and sequence regulator. The former constructs a task-specific prompt based on unsupervised training objects to fully utilize the pre-trained model. The latter jointly leverages semantic, syntactic and original-sequence information to dynamically regulate contextual sequence. Our experiments conducted on four ABSA tasks across multiple benchmarks indicate that PGSO outperforms state-of-the-art methods, with an average improvement of 3.52% in F1 score.</li>
<li><strong>摘要：</strong>近期，基于生成式预训练的模型在基于方面的情感分析 (ABSA) 任务上取得了显著效果。然而，先前的研究过分强调制作各种模板来解释训练目标以增强解码，而忽略了生成模型的内部优化。尽管这些面向目标的优化方法取得了显著的效果，但它们在处理复杂的长文本时会遇到困难，因为在生成模型的位置嵌入机制下，隐含的长距离关系（例如方面-观点关系）难以提取。因此，在本文中，我们首先阐明问题的原因并介绍两种序列优化策略：基于规则的静态优化和基于分数的动态优化。基于规则的方法依靠手工设置依赖关系的优先级来重新排序上下文，而基于分数的算法通过使用神经网络计算单词位置分数来动态调节上下文序列。基于动态优化结构，我们进一步提出了一个统一的基于提示的生成序列优化网络（PGSO），该网络联合优化训练目标和生成模型。具体来说，PGSO 包含两个组件，即提示构建和序列调节器。前者基于无监督的训练对象构建特定于任务的提示，以充分利用预训练模型。后者联合利用语义、句法和原始序列信息来动态调节上下文序列。我们在多个基准上对四个 ABSA 任务进行的实验表明，PGSO 的表现优于最先进的方法，F1 分数平均提高了 3.52%。</li>
</ul>

<h3>Title: SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts</h3>
<ul>
<li><strong>Authors: </strong>Aihua Pei, Zehua Yang, Shunan Zhu, Ruoxi Cheng, Ju Jia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00765">https://arxiv.org/abs/2412.00765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00765">https://arxiv.org/pdf/2412.00765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00765]] SelfPrompt: Autonomously Evaluating LLM Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts(https://arxiv.org/abs/2412.00765)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Traditional methods for evaluating the robustness of large language models (LLMs) often rely on standardized benchmarks, which can escalate costs and limit evaluations across varied domains. This paper introduces a novel framework designed to autonomously evaluate the robustness of LLMs by incorporating refined adversarial prompts and domain-constrained knowledge guidelines in the form of knowledge graphs. Our method systematically generates descriptive sentences from domain-constrained knowledge graph triplets to formulate adversarial prompts, enhancing the relevance and challenge of the evaluation. These prompts, generated by the LLM itself and tailored to evaluate its own robustness, undergo a rigorous filtering and refinement process, ensuring that only those with high textual fluency and semantic fidelity are used. This self-evaluation mechanism allows the LLM to evaluate its robustness without the need for external benchmarks. We assess the effectiveness of our framework through extensive testing on both proprietary models like ChatGPT and open-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that our approach not only reduces dependency on conventional data but also provides a targeted and efficient means of evaluating LLM robustness in constrained domains.</li>
<li><strong>摘要：</strong>评估大型语言模型 (LLM) 稳健性的传统方法通常依赖于标准化基准，这会增加成本并限制跨不同领域的评估。本文介绍了一种新颖的框架，旨在通过以知识图谱的形式结合精炼的对抗性提示和领域约束知识指南来自主评估 LLM 的稳健性。我们的方法系统地从领域约束的知识图谱三元组生成描述性句子以制定对抗性提示，从而增强评估的相关性和挑战性。这些提示由 LLM 本身生成并经过定制以评估其自身的稳健性，经过严格的过滤和细化过程，确保仅使用具有高文本流畅性和语义保真度的提示。这种自我评估机制允许 LLM 评估其稳健性而无需外部基准。我们通过对专有模型（如 ChatGPT）和开源模型（如 Llama-3.1、Phi-3 和 Mistral）进行大量测试来评估我们框架的有效性。结果证实，我们的方法不仅减少了对传统数据的依赖，而且还提供了在受限域中评估 LLM 稳健性的有针对性且有效的方法。</li>
</ul>

<h3>Title: Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting</h3>
<ul>
<li><strong>Authors: </strong>Thilini Wijesiriwardene, Ruwan Wickramarachchi, Sreeram Vennam, Vinija Jain, Aman Chadha, Amitava Das, Ponnurangam Kumaraguru, Amit Sheth</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00869">https://arxiv.org/abs/2412.00869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00869">https://arxiv.org/pdf/2412.00869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00869]] Exploring the Abilities of Large Language Models to Solve Proportional Analogies via Knowledge-Enhanced Prompting(https://arxiv.org/abs/2412.00869)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Making analogies is fundamental to cognition. Proportional analogies, which consist of four terms, are often used to assess linguistic and cognitive abilities. For instance, completing analogies like "Oxygen is to Gas as <blank> is to <blank>" requires identifying the semantic relationship (e.g., "type of") between the first pair of terms ("Oxygen" and "Gas") and finding a second pair that shares the same relationship (e.g., "Aluminum" and "Metal"). In this work, we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for proportional analogy completion and evaluate the performance of contemporary Large Language Models (LLMs) in various knowledge-enhanced prompt settings. Specifically, we augment prompts with three types of knowledge: exemplar, structured, and targeted. Our results show that despite extensive training data, solving proportional analogies remains challenging for current LLMs, with the best model achieving an accuracy of 55%. Notably, we find that providing targeted knowledge can better assist models in completing proportional analogies compared to providing exemplars or collections of structured knowledge.</li>
<li><strong>摘要：</strong>进行类比是认知的基础。比例类比由四个术语组成，通常用于评估语言和认知能力。例如，完成“氧气与气体的关系相当于 <blank> 与 <blank>”这样的类比需要识别第一对术语（“氧气”和“气体”）之间的语义关系（例如“类型”），并找到具有相同关系的第二对术语（例如“铝”和“金属”）。在这项工作中，我们引入了一个 15K 多项选择题问答 (MCQA) 数据集用于比例类比完成，并评估当代大型语言模型 (LLM) 在各种知识增强提示设置中的表现。具体来说，我们用三种类型的知识来增强提示：示例、结构化和有针对性。我们的结果表明，尽管有大量的训练数据，但解决比例类比对于当前的 LLM 来说仍然具有挑战性，最好的模型实现了 55% 的准确率。值得注意的是，我们发现，与提供样例或结构化知识集合相比，提供有针对性的知识可以更好地帮助模型完成比例类比。</li>
</ul>

<h3>Title: VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information</h3>
<ul>
<li><strong>Authors: </strong>Ryo Kamoi, Yusen Zhang, Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00947">https://arxiv.org/abs/2412.00947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00947">https://arxiv.org/pdf/2412.00947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00947]] VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information(https://arxiv.org/abs/2412.00947)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Errors in understanding visual information in images (i.e., visual perception errors) remain a major source of mistakes in Large Vision Language Models (LVLMs). While further analysis is essential, there is a deficiency in datasets for evaluating the visual perception of LVLMs. In this work, we introduce VisOnlyQA, a new dataset designed to directly evaluate the visual perception capabilities of LVLMs on questions about geometric and numerical information in scientific figures. Our dataset enables us to analyze the visual perception of LVLMs for fine-grained visual information, independent of other capabilities such as reasoning. The evaluation set of VisOnlyQA includes 1,200 multiple-choice questions in 12 tasks on four categories of figures. We also provide synthetic training data consisting of 70k instances. Our experiments on VisOnlyQA highlight the following findings: (i) 20 LVLMs we evaluate, including GPT-4o and Gemini 1.5 Pro, work poorly on the visual perception tasks in VisOnlyQA, while human performance is nearly perfect. (ii) Fine-tuning on synthetic training data demonstrates the potential for enhancing the visual perception of LVLMs, but observed improvements are limited to certain tasks and specific models. (iii) Stronger language models improve the visual perception of LVLMs. In summary, our experiments suggest that both training data and model architectures should be improved to enhance the visual perception capabilities of LVLMs. The datasets, code, and model responses are provided at this https URL.</li>
<li><strong>摘要：</strong>理解图像中的视觉信息时的错误（即视觉感知错误）仍然是大型视觉语言模型 (LVLM) 中错误的主要来源。虽然进一步分析必不可少，但用于评估 LVLM 视觉感知的数据集仍然不足。在这项工作中，我们引入了 VisOnlyQA，这是一个新的数据集，旨在直接评估 LVLM 在科学图形中的几何和数字信息问题上的视觉感知能力。我们的数据集使我们能够分析 LVLM 对细粒度视觉信息的视觉感知，而不依赖于推理等其他功能。VisOnlyQA 的评估集包括四类图形的 12 个任务中的 1,200 个多项选择题。我们还提供了由 70k 个实例组成的合成训练数据。我们在 VisOnlyQA 上的实验突出了以下发现：(i) 我们评估的 20 个 LVLM（包括 GPT-4o 和 Gemini 1.5 Pro）在 VisOnlyQA 中的视觉感知任务上表现不佳，而人类的表现几乎完美。 (ii) 对合成训练数据进行微调表明，LVLM 具有增强视觉感知的潜力，但观察到的改进仅限于某些任务和特定模型。 (iii) 更强大的语言模型可以改善 LVLM 的视觉感知。总之，我们的实验表明，应该改进训练数据和模型架构，以增强 LVLM 的视觉感知能力。数据集、代码和模型响应在此 https URL 中提供。</li>
</ul>

<h3>Title: Uhura: A Benchmark for Evaluating Scientific Question Answering and Truthfulness in Low-Resource African Languages</h3>
<ul>
<li><strong>Authors: </strong>Edward Bayes, Israel Abebe Azime, Jesujoba O. Alabi, Jonas Kgomo, Tyna Eloundou, Elizabeth Proehl, Kai Chen, Imaan Khadir, Naome A. Etori, Shamsuddeen Hassan Muhammad, Choice Mpanza, Igneciah Pocia Thete, Dietrich Klakow, David Ifeoluwa Adelani</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.00948">https://arxiv.org/abs/2412.00948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.00948">https://arxiv.org/pdf/2412.00948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.00948]] Uhura: A Benchmark for Evaluating Scientific Question Answering and Truthfulness in Low-Resource African Languages(https://arxiv.org/abs/2412.00948)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Evaluations of Large Language Models (LLMs) on knowledge-intensive tasks and factual accuracy often focus on high-resource languages primarily because datasets for low-resource languages (LRLs) are scarce. In this paper, we present Uhura -- a new benchmark that focuses on two tasks in six typologically-diverse African languages, created via human translation of existing English benchmarks. The first dataset, Uhura-ARC-Easy, is composed of multiple-choice science questions. The second, Uhura-TruthfulQA, is a safety benchmark testing the truthfulness of models on topics including health, law, finance, and politics. We highlight the challenges creating benchmarks with highly technical content for LRLs and outline mitigation strategies. Our evaluation reveals a significant performance gap between proprietary models such as GPT-4o and o1-preview, and Claude models, and open-source models like Meta's LLaMA and Google's Gemma. Additionally, all models perform better in English than in African languages. These results indicate that LMs struggle with answering scientific questions and are more prone to generating false claims in low-resource African languages. Our findings underscore the necessity for continuous improvement of multilingual LM capabilities in LRL settings to ensure safe and reliable use in real-world contexts. We open-source the Uhura Benchmark and Uhura Platform to foster further research and development in NLP for LRLs.</li>
<li><strong>摘要：</strong>对大型语言模型 (LLM) 的知识密集型任务和事实准确性的评估通常侧重于高资源语言，这主要是因为低资源语言 (LRL) 的数据集稀缺。在本文中，我们介绍了 Uhura——一种新的基准，它侧重于六种类型多样的非洲语言中的两项任务，这些任务是通过人工翻译现有的英语基准创建的。第一个数据集 Uhura-ARC-Easy 由多项选择科学问题组成。第二个数据集 Uhura-TruthfulQA 是一个安全基准，用于测试模型在健康、法律、金融和政治等主题上的真实性。我们强调了为 LRL 创建具有高度技术内容的基准的挑战，并概述了缓解策略。我们的评估显示，专有模型（例如 GPT-4o 和 o1-preview）与 Claude 模型以及开源模型（例如 Meta 的 LLaMA 和 Google 的 Gemma）之间存在显着的性能差距。此外，所有模型在英语中的表现都优于非洲语言。这些结果表明，语言模型难以回答科学问题，而且在使用资源匮乏的非洲语言时更容易产生虚假陈述。我们的研究结果强调，有必要在 LRL 环境中不断改进多语言语言模型能力，以确保在现实环境中安全可靠地使用。我们开源了 Uhura Benchmark 和 Uhura Platform，以促进 LRL 的 NLP 的进一步研究和开发。</li>
</ul>

<h3>Title: SAUP: Situation Awareness Uncertainty Propagation on LLM Agent</h3>
<ul>
<li><strong>Authors: </strong>Qiwei Zhao, Xujiang Zhao, Yanchi Liu, Wei Cheng, Yiyou Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Huaxiu Yao, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01033">https://arxiv.org/abs/2412.01033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01033">https://arxiv.org/pdf/2412.01033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01033]] SAUP: Situation Awareness Uncertainty Propagation on LLM Agent(https://arxiv.org/abs/2412.01033)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) integrated into multistep agent systems enable complex decision-making processes across various applications. However, their outputs often lack reliability, making uncertainty estimation crucial. Existing uncertainty estimation methods primarily focus on final-step outputs, which fail to account for cumulative uncertainty over the multistep decision-making process and the dynamic interactions between agents and their environments. To address these limitations, we propose SAUP (Situation Awareness Uncertainty Propagation), a novel framework that propagates uncertainty through each step of an LLM-based agent's reasoning process. SAUP incorporates situational awareness by assigning situational weights to each step's uncertainty during the propagation. Our method, compatible with various one-step uncertainty estimation techniques, provides a comprehensive and accurate uncertainty measure. Extensive experiments on benchmark datasets demonstrate that SAUP significantly outperforms existing state-of-the-art methods, achieving up to 20% improvement in AUROC.</li>
<li><strong>摘要：</strong>集成到多步骤代理系统中的大型语言模型 (LLM) 可实现各种应用中的复杂决策过程。然而，它们的输出通常缺乏可靠性，因此不确定性估计至关重要。现有的不确定性估计方法主要关注最终步骤的输出，这些输出无法解释多步骤决策过程以及代理与其环境之间的动态交互的累积不确定性。为了解决这些限制，我们提出了 SAUP（情境意识不确定性传播），这是一种新颖的框架，可在基于 LLM 的代理推理过程的每个步骤中传播不确定性。SAUP 通过在传播过程中为每个步骤的不确定性分配情境权重来结合情境意识。我们的方法与各种单步不确定性估计技术兼容，可提供全面而准确的不确定性测量。在基准数据集上进行的大量实验表明，SAUP 明显优于现有的最先进方法，AUROC 可提高高达 20%。</li>
</ul>

<h3>Title: Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data</h3>
<ul>
<li><strong>Authors: </strong>Shuaijiang Zhao, Tingwei Guo, Bajian Xiang, Tongtang Wan, Qiang Niu, Wei Zou, Xiangang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01078">https://arxiv.org/abs/2412.01078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01078">https://arxiv.org/pdf/2412.01078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01078]] Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data(https://arxiv.org/abs/2412.01078)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The model, dataset, code and demo can be accessed at \url{this https URL}.</li>
<li><strong>摘要：</strong>GPT-4o 代表了通过语音实现与大型语言模型 (LLM) 实时交互的重要里程碑，其出色的低延迟和高流畅度不仅吸引了人们的注意，还激发了该领域的研究兴趣。这种实时语音交互在需要快速反馈和立即响应的场景中尤其有价值，可大大提升用户体验。然而，对实时大型语音语言模型的研究明显不足，尤其是针对中文。在本文中，我们提出了 KE-Omni，这是一个基于 Ke-SpeechChat 构建的无缝大型语音语言模型，Ke-SpeechChat 是一个大规模高质量合成语音交互数据集，包含 700 万条中英文对话，有 42,002 名说话者参与，总计超过 60,000 小时，这对该领域的研究和开发进步做出了重大贡献。模型、数据集、代码和演示可以在 \url{this https URL} 处访问。</li>
</ul>

<h3>Title: Automated Extraction of Acronym-Expansion Pairs from Scientific Papers</h3>
<ul>
<li><strong>Authors: </strong>Izhar Ali, Million Haileyesus, Serhiy Hnatyshyn, Jan-Lucas Ott, Vasil Hnatyshin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01093">https://arxiv.org/abs/2412.01093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01093">https://arxiv.org/pdf/2412.01093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01093]] Automated Extraction of Acronym-Expansion Pairs from Scientific Papers(https://arxiv.org/abs/2412.01093)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>This project addresses challenges posed by the widespread use of abbreviations and acronyms in digital texts. We propose a novel method that combines document preprocessing, regular expressions, and a large language model to identify abbreviations and map them to their corresponding expansions. The regular expressions alone are often insufficient to extract expansions, at which point our approach leverages GPT-4 to analyze the text surrounding the acronyms. By limiting the analysis to only a small portion of the surrounding text, we mitigate the risk of obtaining incorrect or multiple expansions for an acronym. There are several known challenges in processing text with acronyms, including polysemous acronyms, non-local and ambiguous acronyms. Our approach enhances the precision and efficiency of NLP techniques by addressing these issues with automated acronym identification and disambiguation. This study highlights the challenges of working with PDF files and the importance of document preprocessing. Furthermore, the results of this work show that neither regular expressions nor GPT-4 alone can perform well. Regular expressions are suitable for identifying acronyms but have limitations in finding their expansions within the paper due to a variety of formats used for expressing acronym-expansion pairs and the tendency of authors to omit expansions within the text. GPT-4, on the other hand, is an excellent tool for obtaining expansions but struggles with correctly identifying all relevant acronyms. Additionally, GPT-4 poses challenges due to its probabilistic nature, which may lead to slightly different results for the same input. Our algorithm employs preprocessing to eliminate irrelevant information from the text, regular expressions for identifying acronyms, and a large language model to help find acronym expansions to provide the most accurate and consistent results.</li>
<li><strong>摘要：</strong>该项目解决了数字文本中广泛使用缩写和首字母缩略词所带来的挑战。我们提出了一种新方法，该方法结合了文档预处理、正则表达式和大型语言模型来识别缩写并将其映射到相应的扩展。仅使用正则表达式通常不足以提取扩展，此时我们的方法利用 GPT-4 来分析首字母缩略词周围的文本。通过将分析限制在周围文本的一小部分，我们降低了获得首字母缩略词的错误或多个扩展的风险。处理带有首字母缩略词的文本存在几个已知挑战，包括多义首字母缩略词、非局部和模糊首字母缩略词。我们的方法通过自动首字母缩略词识别和消歧来解决这些问题，从而提高了 NLP 技术的精度和效率。这项研究强调了处理 PDF 文件的挑战以及文档预处理的重要性。此外，这项工作的结果表明，无论是正则表达式还是 GPT-4 本身都无法表现良好。正则表达式适合识别首字母缩略词，但由于用于表达首字母缩略词-扩展对的格式多种多样，并且作者倾向于在文本中省略扩展，因此正则表达式在查找论文中的首字母缩略词扩展方面存在局限性。另一方面，GPT-4 是获取扩展的绝佳工具，但难以正确识别所有相关的首字母缩略词。此外，GPT-4 的概率性质也带来了挑战，这可能会导致相同输入的结果略有不同。我们的算法采用预处理来消除文本中的不相关信息，采用正则表达式来识别首字母缩略词，并采用大型语言模型来帮助查找首字母缩略词扩展，以提供最准确和一致的结果。</li>
</ul>

<h3>Title: Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01113">https://arxiv.org/abs/2412.01113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01113">https://arxiv.org/pdf/2412.01113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01113]] Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Reasoning(https://arxiv.org/abs/2412.01113)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>This study investigates the internal reasoning mechanism of language models during symbolic multi-step reasoning, motivated by the question of whether chain-of-thought (CoT) outputs are faithful to the model's internals. Specifically, we inspect when they internally determine their answers, particularly before or after CoT begins, to determine whether models follow a post-hoc "think-to-talk" mode or a step-by-step "talk-to-think" mode of explanation. Through causal probing experiments in controlled arithmetic reasoning tasks, we found systematic internal reasoning patterns across models; for example, simple subproblems are solved before CoT begins, and more complicated multi-hop calculations are performed during CoT.</li>
<li><strong>摘要：</strong>本研究调查了语言模型在符号多步推理过程中的内部推理机制，其动机是思考思路链 (CoT) 输出是否忠实于模型的内部结构。具体来说，我们检查它们何时在内部确定答案，特别是在 CoT 开始之前或之后，以确定模型是否遵循事后“思考到交谈”模式或逐步“交谈到思考”解释模式。通过受控算术推理任务中的因果探测实验，我们发现了跨模型的系统内部推理模式；例如，在 CoT 开始之前解决简单的子问题，而在 CoT 期间执行更复杂的多跳计算。</li>
</ul>

<h3>Title: Enhancing Function-Calling Capabilities in LLMs: Strategies for Prompt Formats, Data Integration, and Multilingual Translation</h3>
<ul>
<li><strong>Authors: </strong>Yi-Chang Chen, Po-Chun Hsu, Chan-Jan Hsu, Da-shan Shiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01130">https://arxiv.org/abs/2412.01130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01130">https://arxiv.org/pdf/2412.01130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01130]] Enhancing Function-Calling Capabilities in LLMs: Strategies for Prompt Formats, Data Integration, and Multilingual Translation(https://arxiv.org/abs/2412.01130)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have significantly advanced autonomous agents, particularly in zero-shot tool usage, also known as function calling. This research delves into enhancing the function-calling capabilities of LLMs by exploring different approaches, including prompt formats for integrating function descriptions, blending function-calling and instruction-following data, introducing a novel Decision Token for conditional prompts, leveraging chain-of-thought reasoning, and overcoming multilingual challenges with a translation pipeline. Our key findings and contributions are as follows: (1) Instruction-following data improves both function-calling accuracy and relevance detection. (2) The use of the newly proposed Decision Token, combined with synthetic non-function-call data, enhances relevance detection. (3) A tailored translation pipeline effectively overcomes multilingual limitations, demonstrating significant improvements in Traditional Chinese. These insights highlight the potential for improved function-calling capabilities and multilingual applications in LLMs.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 显著提高了自主代理的性能，特别是在零样本工具使用（也称为函数调用）方面。本研究通过探索不同的方法来增强 LLM 的函数调用能力，包括集成函数描述的提示格式、混合函数调用和指令跟踪数据、为条件提示引入新的决策标记、利用思路链推理以及通过翻译管道克服多语言挑战。我们的主要发现和贡献如下：（1）指令跟踪数据可提高函数调用准确性和相关性检测。（2）使用新提出的决策标记，结合合成的非函数调用数据，可增强相关性检测。（3）量身定制的翻译管道有效克服了多语言限制，在繁体中文方面取得了显着进步。这些见解凸显了 LLM 中改进函数调用能力和多语言应用的潜力。</li>
</ul>

<h3>Title: A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans</h3>
<ul>
<li><strong>Authors: </strong>Zhihan Cao, Hiroaki Yamada, Simone Teufel, Takenobu Tokunaga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01131">https://arxiv.org/abs/2412.01131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01131">https://arxiv.org/pdf/2412.01131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01131]] A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans(https://arxiv.org/abs/2412.01131)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Recently, much work has concerned itself with the enigma of what exactly PLMs (pretrained language models) learn about different aspects of language, and how they learn it. One stream of this type of research investigates the knowledge that PLMs have about semantic relations. However, many aspects of semantic relations were left unexplored. Only one relation was considered, namely hypernymy. Furthermore, previous work did not measure humans' performance on the same task as that solved by the PLMs. This means that at this point in time, there is only an incomplete view of models' semantic relation knowledge. To address this gap, we introduce a comprehensive evaluation framework covering five relations beyond hypernymy, namely hyponymy, holonymy, meronymy, antonymy, and synonymy. We use six metrics (two newly introduced here) for recently untreated aspects of semantic relation knowledge, namely soundness, completeness, symmetry, asymmetry, prototypicality, and distinguishability and fairly compare humans and models on the same task. Our extensive experiments involve 16 PLMs, eight masked and eight causal language models. Up to now only masked language models had been tested although causal and masked language models treat context differently. Our results reveal a significant knowledge gap between humans and models for almost all semantic relations. Antonymy is the outlier relation where all models perform reasonably well. In general, masked language models perform significantly better than causal language models. Nonetheless, both masked and causal language models are likely to confuse non-antonymy relations with antonymy.</li>
<li><strong>摘要：</strong>最近，许多研究都关注 PLM（预训练语言模型）究竟从语言的不同方面学到了什么，以及它们是如何学习的。这类研究的一个分支研究了 PLM 对语义关系的了解。然而，语义关系的许多方面都尚未得到探索。只考虑了一种关系，即上位关系。此外，以前的研究并没有衡量人类在与 PLM 解决的任务相同的任务上的表现。这意味着目前，对模型的语义关系知识的了解并不完整。为了解决这一差距，我们引入了一个全面的评估框架，涵盖了上位关系以外的五种关系，即下位关系、全义关系、部分义关系、反义关系和同义词。我们使用六个指标（这里新引入了两个）来衡量语义关系知识中最近未处理的方面，即健全性、完整性、对称性、不对称性、原型性和可区分性，并公平地比较人类和模型在同一任务上的表现。我们进行了广泛的实验，涉及 16 个 PLM、八个掩蔽语言模型和八个因果语言模型。到目前为止，只测试了掩蔽语言模型，尽管因果语言模型和掩蔽语言模型对上下文的处理方式不同。我们的结果表明，对于几乎所有的语义关系，人类和模型之间存在显著的知识差距。反义关系是所有模型都表现相当好的异常关系。一般来说，掩蔽语言模型的表现明显优于因果语言模型。尽管如此，掩蔽语言模型和因果语言模型都可能将非反义关系与反义关系混淆。</li>
</ul>

<h3>Title: SailCompass: Towards Reproducible and Robust Evaluation for Southeast Asian Languages</h3>
<ul>
<li><strong>Authors: </strong>Jia Guo, Longxu Dou, Guangtao Zeng, Stanley Kok, Wei Lu, Qian Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01186">https://arxiv.org/abs/2412.01186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01186">https://arxiv.org/pdf/2412.01186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01186]] SailCompass: Towards Reproducible and Robust Evaluation for Southeast Asian Languages(https://arxiv.org/abs/2412.01186)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce SailCompass, a reproducible and robust evaluation benchmark for assessing Large Language Models (LLMs) on Southeast Asian Languages (SEA). SailCompass encompasses three main SEA languages, eight primary tasks including 14 datasets covering three task types (generation, multiple-choice questions, and classification). To improve the robustness of the evaluation approach, we explore different prompt configurations for multiple-choice questions and leverage calibrations to improve the faithfulness of classification tasks. With SailCompass, we derive the following findings: (1) SEA-specialized LLMs still outperform general LLMs, although the gap has narrowed; (2) A balanced language distribution is important for developing better SEA-specialized LLMs; (3) Advanced prompting techniques (e.g., calibration, perplexity-based ranking) are necessary to better utilize LLMs. All datasets and evaluation scripts are public.</li>
<li><strong>摘要：</strong>在本文中，我们介绍了 SailCompass，这是一个可重复且稳健的评估基准，用于评估东南亚语言 (SEA) 的大型语言模型 (LLM)。SailCompass 涵盖三种主要的 SEA 语言，八个主要任务，包括 14 个数据集，涵盖三种任务类型（生成、多项选择题和分类）。为了提高评估方法的稳健性，我们探索了多项选择题的不同提示配置，并利用校准来提高分类任务的忠实度。通过 SailCompass，我们得出以下发现：（1）尽管差距已经缩小，但 SEA 专用 LLM 的表现仍然优于一般 LLM；（2）平衡的语言分布对于开发更好的 SEA 专用 LLM 很重要；（3）高级提示技术（例如校准、基于困惑度的排名）对于更好地利用 LLM 是必不可少的。所有数据集和评估脚本都是公开的。</li>
</ul>

<h3>Title: MiningGPT -- A Domain-Specific Large Language Model for the Mining Industry</h3>
<ul>
<li><strong>Authors: </strong>Kurukulasooriya Fernando ana Gianluca Demartini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01189">https://arxiv.org/abs/2412.01189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01189">https://arxiv.org/pdf/2412.01189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01189]] MiningGPT -- A Domain-Specific Large Language Model for the Mining Industry(https://arxiv.org/abs/2412.01189)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Recent advancements of generative LLMs (Large Language Models) have exhibited human-like language capabilities but have shown a lack of domain-specific understanding. Therefore, the research community has started the development of domain-specific LLMs for many domains. In this work we focus on discussing how to build mining domain-specific LLMs, as the global mining industry contributes significantly to the worldwide economy. We report on MiningGPT, a mining domain-specific instruction-following 7B parameter LLM model which showed a 14\% higher mining domain knowledge test score as compared to its parent model Mistral 7B instruct.</li>
<li><strong>摘要：</strong>生成式 LLM（大型语言模型）的最新进展已展现出类似人类的语言能力，但缺乏特定领域的理解。因此，研究界已开始为许多领域开发特定领域的 LLM。在这项工作中，我们重点讨论如何构建采矿领域特定的 LLM，因为全球采矿业对全球经济做出了重大贡献。我们报告了 MiningGPT，这是一个采矿领域特定的指令跟随 7B 参数 LLM 模型，与其父模型 Mistral 7B instruct 相比，其采矿领域知识测试得分高出 14\%。</li>
</ul>

<h3>Title: GraphOTTER: Evolving LLM-based Graph Reasoning for Complex Table Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Qianlong Li, Chen Huang, Shuai Li, Yuanxin Xiang, Deng Xiong, Wenqiang Lei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01230">https://arxiv.org/abs/2412.01230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01230">https://arxiv.org/pdf/2412.01230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01230]] GraphOTTER: Evolving LLM-based Graph Reasoning for Complex Table Question Answering(https://arxiv.org/abs/2412.01230)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>Complex Table Question Answering involves providing accurate answers to specific questions based on intricate tables that exhibit complex layouts and flexible header locations. Despite considerable progress having been made in the LLM era, the reasoning processes of existing methods are often implicit, feeding the entire table into prompts, making it difficult to effectively filter out irrelevant information in the table. To this end, we propose GraphOTTER that explicitly establishes the reasoning process to pinpoint the correct answers. In particular, GraphOTTER leverages a graph-based representation, transforming the complex table into an undirected graph. It then conducts step-by-step reasoning on the graph, with each step guided by a set of pre-defined intermediate reasoning actions. As such, it constructs a clear reasoning path and effectively identifies the answer to a given question. Comprehensive experiments on two benchmark datasets and two LLM backbones demonstrate the effectiveness of GraphOTTER. Further analysis indicates that its success may be attributed to the ability to efficiently filter out irrelevant information, thereby focusing the reasoning process on the most pertinent data. Our code and experimental datasets are available at \url{this https URL}.</li>
<li><strong>摘要：</strong>复杂表格问答系统需要根据布局复杂、标题位置灵活的复杂表格，为特定问题提供准确的答案。尽管在 LLM 时代取得了长足的进步，但现有方法的推理过程往往是隐式的，将整个表格输入到提示中，因此很难有效地过滤掉表格中的不相关信息。为此，我们提出了 GraphOTTER，它明确建立了推理过程以找出正确的答案。具体来说，GraphOTTER 利用基于图形的表示，将复杂表格转换为无向图。然后，它对图形进行逐步推理，每一步都由一组预定义的中间推理动作引导。这样，它构建了一条清晰的推理路径，并有效地识别了给定问题的答案。在两个基准数据集和两个 LLM 主干上进行的全面实验证明了 GraphOTTER 的有效性。进一步的分析表明，它的成功可能归因于能够有效地过滤掉不相关的信息，从而将推理过程集中在最相关的数据上。我们的代码和实验数据集可在 \url{此 https URL} 处找到。</li>
</ul>

<h3>Title: Yi-Lightning Technical Report</h3>
<ul>
<li><strong>Authors: </strong>01.AI: Alan Wake, Albert Wang, Bei Chen, C.X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Ethan Dai, Fan Zhou, Feng Hu, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qichen Hu, Shawn Wang, Shijun Zhou, Shiyong Li, Tianhang Zhu, Wen Xie, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01253">https://arxiv.org/abs/2412.01253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01253">https://arxiv.org/pdf/2412.01253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01253]] Yi-Lightning Technical Report(https://arxiv.org/abs/2412.01253)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at this https URL.</li>
<li><strong>摘要：</strong>本技术报告介绍了我们最新的旗舰大型语言模型 (LLM) Yi-Lightning。它取得了卓越的表现，在 Chatbot Arena 上排名第 6，在中文、数学、编码和硬提示等专业类别中取得了特别强劲的成绩（第 2 至第 4 名）。Yi-Lightning 利用增强的混合专家 (MoE) 架构，具有先进的专家细分和路由机制以及优化的 KV 缓存技术。我们的开发过程包括全面的预训练、监督微调 (SFT) 和从人类反馈中强化学习 (RLHF)，我们为多阶段训练、合成数据构建和奖励建模制定了深思熟虑的策略。此外，我们还实施了 RAISE（负责任的 AI 安全引擎），这是一个四组件框架，用于解决预训练、后训练和服务阶段的安全问题。在我们可扩展的超级计算基础设施的支持下，所有这些创新都大大降低了培训、部署和推理成本，同时保持了高性能标准。通过对公共学术基准的进一步评估，Yi-Lightning 表现出与顶级 LLM 相当的竞争力，同时我们观察到传统静态基准测试结果与现实世界动态人类偏好之间存在显著差异。这一观察结果促使我们重新评估传统基准测试在指导开发更智能、更强大的实际应用 AI 系统方面的效用。Yi-Lightning 现在可通过我们的开发者平台（https URL）获取。</li>
</ul>

<h3>Title: Do Large Language Models with Reasoning and Acting Meet the Needs of Task-Oriented Dialogue?</h3>
<ul>
<li><strong>Authors: </strong>Michelle Elizabeth, Morgan Veyret, Miguel Couceiro, Ondrej Dusek, Lina M. Rojas-Barahona</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01262">https://arxiv.org/abs/2412.01262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01262">https://arxiv.org/pdf/2412.01262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01262]] Do Large Language Models with Reasoning and Acting Meet the Needs of Task-Oriented Dialogue?(https://arxiv.org/abs/2412.01262)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) gained immense popularity due to their impressive capabilities in unstructured conversations. However, they underperform compared to previous approaches in task-oriented dialogue (TOD), wherein reasoning and accessing external information are crucial. Empowering LLMs with advanced prompting strategies such as reasoning and acting (ReAct) has shown promise in solving complex tasks traditionally requiring reinforcement learning. In this work, we apply the ReAct strategy to guide LLMs performing TOD. We evaluate ReAct-based LLMs (ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs seem to underperform state-of-the-art approaches in simulation, human evaluation indicates higher user satisfaction rate compared to handcrafted systems despite having a lower success rate.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 因其在非结构化对话中的出色能力而广受欢迎。然而，它们在面向任务的对话 (TOD) 中的表现不如以前的方法，因为在面向任务的对话中，推理和访问外部信息至关重要。使用推理和表演 (ReAct) 等高级提示策略为 LLM 赋能，在解决传统上需要强化学习的复杂任务方面已显示出良好的前景。在这项工作中，我们应用 ReAct 策略来指导执行 TOD 的 LLM。我们在模拟和真实用户中评估基于 ReAct 的 LLM (ReAct-LLM)。虽然 ReAct-LLM 在模拟中的表现似乎不如最先进的方法，但人工评估表明，尽管成功率较低，但与手工制作的系统相比，用户满意度更高。</li>
</ul>

<h3>Title: Shadow of the (Hierarchical) Tree: Reconciling Symbolic and Predictive Components of the Neural Code for Syntax</h3>
<ul>
<li><strong>Authors: </strong>Elliot Murphy</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01276">https://arxiv.org/abs/2412.01276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01276">https://arxiv.org/pdf/2412.01276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01276]] Shadow of the (Hierarchical) Tree: Reconciling Symbolic and Predictive Components of the Neural Code for Syntax(https://arxiv.org/abs/2412.01276)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Natural language syntax can serve as a major test for how to integrate two infamously distinct frameworks: symbolic representations and connectionist neural networks. Building on a recent neurocomputational architecture for syntax (ROSE), I discuss the prospects of reconciling the neural code for hierarchical 'vertical' syntax with linear and predictive 'horizontal' processes via a hybrid neurosymbolic model. I argue that the former can be accounted for via the higher levels of ROSE in terms of vertical phrase structure representations, while the latter can explain horizontal forms of linguistic information via the tuning of the lower levels to statistical and perceptual inferences. One prediction of this is that artificial language models will contribute to the cognitive neuroscience of horizontal morphosyntax, but much less so to hierarchically compositional structures. I claim that this perspective helps resolve many current tensions in the literature. Options for integrating these two neural codes are discussed, with particular emphasis on how predictive coding mechanisms can serve as interfaces between symbolic oscillatory phase codes and population codes for the statistics of linearized aspects of syntax. Lastly, I provide a neurosymbolic mathematical model for how to inject symbolic representations into a neural regime encoding lexico-semantic statistical features.</li>
<li><strong>摘要：</strong>自然语言句法可以作为如何整合两个臭名昭著的不同框架的主要测试：符号表示和联结神经网络。基于最近的语法神经计算架构 (ROSE)，我讨论了通过混合神经符号模型将分层“垂直”句法的神经代码与线性和预测“水平”过程相协调的前景。我认为前者可以通过 ROSE 的较高级别以垂直短语结构表示来解释，而后者可以通过将较低级别调整为统计和感知推断来解释水平形式的语言信息。对此的一个预测是，人工语言模型将为水平形态句法的认知神经科学做出贡献，但对分层组合结构的贡献则要小得多。我认为这种观点有助于解决文献中许多当前的紧张关系。本文讨论了整合这两种神经代码的选项，特别强调了预测编码机制如何充当符号振荡相位代码和语法线性化方面统计的群体代码之间的接口。最后，我提供了一个神经符号数学模型，用于说明如何将符号表示注入编码词汇语义统计特征的神经机制。</li>
</ul>

<h3>Title: SiTSE: Sinhala Text Simplification Dataset and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Surangika Ranathunga, Rumesh Sirithunga, Himashi Rathnayake, Lahiru De Silva, Thamindu Aluthwala, Saman Peramuna, Ravi Shekhar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01293">https://arxiv.org/abs/2412.01293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01293">https://arxiv.org/pdf/2412.01293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01293]] SiTSE: Sinhala Text Simplification Dataset and Evaluation(https://arxiv.org/abs/2412.01293)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Text Simplification is a task that has been minimally explored for low-resource languages. Consequently, there are only a few manually curated datasets. In this paper, we present a human curated sentence-level text simplification dataset for the Sinhala language. Our evaluation dataset contains 1,000 complex sentences and corresponding 3,000 simplified sentences produced by three different human annotators. We model the text simplification task as a zero-shot and zero resource sequence-to-sequence (seq-seq) task on the multilingual language models mT5 and mBART. We exploit auxiliary data from related seq-seq tasks and explore the possibility of using intermediate task transfer learning (ITTL). Our analysis shows that ITTL outperforms the previously proposed zero-resource methods for text simplification. Our findings also highlight the challenges in evaluating text simplification systems, and support the calls for improved metrics for measuring the quality of automated text simplification systems that would suit low-resource languages as well. Our code and data are publicly available: this https URL</li>
<li><strong>摘要：</strong>文本简化是一项针对低资源语言的探索很少的任务。因此，只有少数手动整理的数据集。在本文中，我们为僧伽罗语提供了一个人工整理的句子级文本简化数据集。我们的评估数据集包含由三个不同的人工注释者生成的 1,000 个复杂句子和相应的 3,000 个简化句子。我们将文本简化任务建模为多语言语言模型 mT5 和 mBART 上的零样本和零资源序列到序列 (seq-seq) 任务。我们利用来自相关 seq-seq 任务的辅助数据，并探索使用中间任务迁移学习 (ITTL) 的可能性。我们的分析表明，ITTL 优于之前提出的零资源文本简化方法。我们的研究结果还强调了评估文本简化系统的挑战，并支持改进衡量自动文本简化系统质量的指标的呼吁，这些指标也适用于低资源语言。我们的代码和数据是公开的：此 https URL</li>
</ul>

<h3>Title: The "LLM World of Words" English free association norms generated by large language models</h3>
<ul>
<li><strong>Authors: </strong>Katherine Abramski, Riccardo Improta, Giulio Rossetti, Massimo Stella</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01330">https://arxiv.org/abs/2412.01330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01330">https://arxiv.org/pdf/2412.01330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01330]] The "LLM World of Words" English free association norms generated by large language models(https://arxiv.org/abs/2412.01330)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Free associations have been extensively used in cognitive psychology and linguistics for studying how conceptual knowledge is organized. Recently, the potential of applying a similar approach for investigating the knowledge encoded in LLMs has emerged, specifically as a method for investigating LLM biases. However, the absence of large-scale LLM-generated free association norms that are comparable with human-generated norms is an obstacle to this new research direction. To address this limitation, we create a new dataset of LLM-generated free association norms modeled after the "Small World of Words" (SWOW) human-generated norms consisting of approximately 12,000 cue words. We prompt three LLMs, namely Mistral, Llama3, and Haiku, with the same cues as those in the SWOW norms to generate three novel comparable datasets, the "LLM World of Words" (LWOW). Using both SWOW and LWOW norms, we construct cognitive network models of semantic memory that represent the conceptual knowledge possessed by humans and LLMs. We demonstrate how these datasets can be used for investigating implicit biases in humans and LLMs, such as the harmful gender stereotypes that are prevalent both in society and LLM outputs.</li>
<li><strong>摘要：</strong>自由联想在认知心理学和语言学中被广泛用于研究概念知识的组织方式。最近，应用类似方法来研究 LLM 中编码的知识的潜力已经出现，特别是作为研究 LLM 偏见的方法。然而，缺乏与人类生成的规范相当的大规模 LLM 生成的自由联想规范，这对这一新的研究方向是一个障碍。为了解决这一限制，我们创建了一个由 LLM 生成的自由联想规范的新数据集，该数据集以“词语小世界”（SWOW）人类生成的规范为模型，包含大约 12,000 个提示词。我们用与 SWOW 规范中相同的提示提示三个 LLM，即 Mistral、Llama3 和 Haiku，以生成三个新的可比较数据集，即“LLM 词语世界”（LWOW）。使用 SWOW 和 LWOW 规范，我们构建了语义记忆的认知网络模型，这些模型代表了人类和 LLM 所拥有的概念知识。我们展示了如何使用这些数据集来调查人类和法学硕士中的隐性偏见，例如社会和法学硕士输出中普遍存在的有害性别刻板印象。</li>
</ul>

<h3>Title: A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls</h3>
<ul>
<li><strong>Authors: </strong>Sheikh Shafayat, Dongkeun Yoon, Woori Jang, Jiwoo Choi, Alice Oh, Seohyon Jung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01340">https://arxiv.org/abs/2412.01340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01340">https://arxiv.org/pdf/2412.01340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01340]] A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls(https://arxiv.org/abs/2412.01340)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>In this work, we propose and evaluate the feasibility of a two-stage pipeline to evaluate literary machine translation, in a fine-grained manner, from English to Korean. The results show that our framework provides fine-grained, interpretable metrics suited for literary translation and obtains a higher correlation with human judgment than traditional machine translation metrics. Nonetheless, it still fails to match inter-human agreement, especially in metrics like Korean Honorifics. We also observe that LLMs tend to favor translations generated by other LLMs, and we highlight the necessity of developing more sophisticated evaluation methods to ensure accurate and culturally sensitive machine translation of literary works.</li>
<li><strong>摘要：</strong>在这项研究中，我们提出并评估了两阶段流程的可行性，该流程以细粒度的方式评估从英语到韩语的文学机器翻译。结果表明，我们的框架提供了适合文学翻译的细粒度、可解释的指标，并且与传统的机器翻译指标相比，与人类判断的相关性更高。尽管如此，它仍然无法达到人与人之间的一致性，尤其是在韩语敬语等指标方面。我们还观察到，法学硕士倾向于青睐其他法学硕士生成的翻译，我们强调开发更复杂的评估方法的必要性，以确保文学作品的机器翻译准确且具有文化敏感性。</li>
</ul>

<h3>Title: Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Yuhe Ji, Yilun Liu, Feiyu Yao, Minggui He, Shimin Tao, Xiaofeng Zhao, Su Chang, Xinhua Yang, Weibin Meng, Yuming Xie, Boxing Chen, Hao Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01377">https://arxiv.org/abs/2412.01377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01377">https://arxiv.org/pdf/2412.01377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01377]] Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge(https://arxiv.org/abs/2412.01377)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The increasing complexity of computer systems necessitates innovative approaches to fault and error management, going beyond traditional manual log analysis. While existing solutions using large language models (LLMs) show promise, they are limited by a gap between natural and domain-specific languages, which restricts their effectiveness in real-world applications. Our approach addresses these limitations by integrating interpretable domain knowledge into open-source LLMs through continual pre-training (CPT), enhancing performance on log tasks while retaining natural language processing capabilities. We created a comprehensive dataset, NLPLog, with over 250,000 question-answer pairs to facilitate this integration. Our model, SuperLog, trained with this dataset, achieves the best performance across four log analysis tasks, surpassing the second-best model by an average of 12.01%. Our contributions include a novel CPT paradigm that significantly improves model performance, the development of SuperLog with state-of-the-art results, and the release of a large-scale dataset to support further research in this domain.</li>
<li><strong>摘要：</strong>计算机系统日益复杂，因此需要创新的故障和错误管理方法，超越传统的手动日志分析。虽然使用大型语言模型 (LLM) 的现有解决方案很有前景，但它们受到自然语言和领域特定语言之间差距的限制，这限制了它们在实际应用中的有效性。我们的方法通过持续预训练 (CPT) 将可解释的领域知识集成到开源 LLM 中，从而解决了这些限制，在保留自然语言处理能力的同时提高了日志任务的性能。我们创建了一个全面的数据集 NLPLog，其中包含超过 250,000 个问答对，以促进这种集成。我们的模型 SuperLog 使用此数据集进行训练，在四个日志分析任务中取得了最佳性能，平均比第二好的模型高出 12.01%。我们的贡献包括一种可显着提高模型性能的新型 CPT 范式、开发具有最先进结果的 SuperLog 以及发布大规模数据集以支持该领域的进一步研究。</li>
</ul>

<h3>Title: Impromptu Cybercrime Euphemism Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Yucheng Zhou, Laiping Zhao, Jing Li, Fangming Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01413">https://arxiv.org/abs/2412.01413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01413">https://arxiv.org/pdf/2412.01413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01413]] Impromptu Cybercrime Euphemism Detection(https://arxiv.org/abs/2412.01413)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Detecting euphemisms is essential for content security on various social media platforms, but existing methods designed for detecting euphemisms are ineffective in impromptu euphemisms. In this work, we make a first attempt to an exploration of impromptu euphemism detection and introduce the Impromptu Cybercrime Euphemisms Detection (ICED) dataset. Moreover, we propose a detection framework tailored to this problem, which employs context augmentation modeling and multi-round iterative training. Our detection framework mainly consists of a coarse-grained and a fine-grained classification model. The coarse-grained classification model removes most of the harmless content in the corpus to be detected. The fine-grained model, impromptu euphemisms detector, integrates context augmentation and multi-round iterations training to better predicts the actual meaning of a masked token. In addition, we leverage ChatGPT to evaluate the mode's capability. Experimental results demonstrate that our approach achieves a remarkable 76-fold improvement compared to the previous state-of-the-art euphemism detector.</li>
<li><strong>摘要：</strong>检测委婉语对于各种社交媒体平台上的内容安全至关重要，但现有的委婉语检测方法对即兴委婉语无效。在这项工作中，我们首次尝试探索即兴委婉语的检测，并介绍即兴网络犯罪委婉语检测（ICED）数据集。此外，我们提出了一个针对此问题的检测框架，该框架采用上下文增强建模和多轮迭代训练。我们的检测框架主要由粗粒度和细粒度分类模型组成。粗粒度分类模型会删除要检测的语料库中的大部分无害内容。细粒度模型即兴委婉语检测器集成了上下文增强和多轮迭代训练，可以更好地预测被掩盖的标记的实际含义。此外，我们利用 ChatGPT 来评估该模型的能力。实验结果表明，与之前最先进的委婉语检测器相比，我们的方法实现了显著的 76 倍的改进。</li>
</ul>

<h3>Title: PLD+: Accelerating LLM inference by leveraging Language Model Artifacts</h3>
<ul>
<li><strong>Authors: </strong>Shwetha Somasundaram, Anirudh Phukan, Apoorv Saxena</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01447">https://arxiv.org/abs/2412.01447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01447">https://arxiv.org/pdf/2412.01447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01447]] PLD+: Accelerating LLM inference by leveraging Language Model Artifacts(https://arxiv.org/abs/2412.01447)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>To reduce the latency associated with autoretrogressive LLM inference, speculative decoding has emerged as a novel decoding paradigm, where future tokens are drafted and verified in parallel. However, the practical deployment of speculative decoding is hindered by its requirements for additional computational resources and fine-tuning, which limits its out-of-the-box usability. To address these challenges, we present PLD+, a suite of novel algorithms developed to accelerate the inference process of LLMs, particularly for input-guided tasks. These tasks, which include code editing, text editing, summarization, etc., often feature outputs with substantial overlap with their inputs-an attribute PLD+ is designed to exploit. PLD+ also leverages the artifacts (attention and hidden states) generated during inference to accelerate inference speed. We test our approach on five input-guided tasks and through extensive experiments we find that PLD+ outperforms all tuning-free approaches. In the greedy setting, it even outperforms the state-of-the-art tuning-dependent approach EAGLE on four of the tasks. (by a margin of upto 2.31 in terms of avg. speedup). Our approach is tuning free, does not require any additional compute and can easily be used for accelerating inference of any LLM.</li>
<li><strong>摘要：</strong>为了减少与自回溯 LLM 推理相关的延迟，推测解码已成为一种新颖的解码范式，其中未来的标记是并行起草和验证的。然而，推测解码的实际部署受到其对额外计算资源和微调的要求的阻碍，这限制了其开箱即用的可用性。为了应对这些挑战，我们提出了 PLD+，这是一套新颖的算法，旨在加速 LLM 的推理过程，特别是对于输入引导任务。这些任务包括代码编辑、文本编辑、摘要等，通常具有与输入有很大重叠的输出 - PLD+ 旨在利用这一属性。PLD+ 还利用推理过程中生成的工件（注意力和隐藏状态）来加速推理速度。我们在五个输入引导任务上测试了我们的方法，通过大量实验，我们发现 PLD+ 优于所有无需调整的方法。在贪婪设置下，它甚至在四项任务中的表现优于最先进的依赖于调整的方法 EAGLE。（平均加速比高达 2.31）。我们的方法无需调整，不需要任何额外的计算，并且可以轻松用于加速任何 LLM 的推理。</li>
</ul>

<h3>Title: Early Exit Is a Natural Capability in Transformer-based Models: An Empirical Study on Early Exit without Joint Optimization</h3>
<ul>
<li><strong>Authors: </strong>Weiqiao Shan, Long Meng, Tong Zheng, Yingfeng Luo, Bei Li, junxin Wang, Tong Xiao, Jingbo Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01455">https://arxiv.org/abs/2412.01455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01455">https://arxiv.org/pdf/2412.01455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01455]] Early Exit Is a Natural Capability in Transformer-based Models: An Empirical Study on Early Exit without Joint Optimization(https://arxiv.org/abs/2412.01455)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit exceptional performance across various downstream tasks. However, they encounter limitations due to slow inference speeds stemming from their extensive parameters. The early exit (EE) is an approach that aims to accelerate auto-regressive decoding. EE generates outputs from intermediate layers instead of using the whole model, which offers a promising solution to this challenge. However, additional output layers and joint optimization used in conventional EE hinder the application of EE in LLMs. In this paper, we explore the possibility of LLMs EE without additional output layers and joint optimization. Our findings indicate that EE is a natural capability within transformer-based models. While joint optimization does not give model EE capability, it must be employed to address challenges by improving the accuracy of locating the optimal EE layer through gating functions. Additionally, our study reveals patterns in EE behavior from a sub-word perspective based on the LLaMA model and the potential possibility for EE based on sub-layers.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种下游任务中表现出色。然而，由于参数过多导致推理速度慢，它们也面临限制。早期退出 (EE) 是一种旨在加速自回归解码的方法。EE 从中间层生成输出，而不是使用整个模型，这为这一挑战提供了一个有希望的解决方案。然而，传统 EE 中使用的额外输出层和联合优化阻碍了 EE 在 LLM 中的应用。在本文中，我们探讨了没有额外输出层和联合优化的 LLM EE 的可能性。我们的研究结果表明，EE 是基于 Transformer 的模型中的一种自然能力。虽然联合优化不能赋予模型 EE 能力，但必须通过提高通过门控函数定位最佳 EE 层的准确性来应对挑战。此外，我们的研究从基于 LLaMA 模型的子词角度揭示了 EE 行为的模式，以及基于子层的 EE 的潜在可能性。</li>
</ul>

<h3>Title: Scaling Law for Language Models Training Considering Batch Size</h3>
<ul>
<li><strong>Authors: </strong>Xian Shuai, Yiding Wang, Yimeng Wu, Xin Jiang, Xiaozhe Ren</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01505">https://arxiv.org/abs/2412.01505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01505">https://arxiv.org/pdf/2412.01505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01505]] Scaling Law for Language Models Training Considering Batch Size(https://arxiv.org/abs/2412.01505)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made remarkable advances in recent years, with scaling laws playing a critical role in this rapid progress. In this paper, we empirically investigate how a critical hyper-parameter, i.e., the global batch size, influences the LLM training prdocess. We begin by training language models ranging from 125 million to 2.6 billion parameters, using up to 300 billion high-quality tokens. Through these experiments, we establish a basic scaling law on model size and training data amount. We then examine how varying batch sizes and learning rates affect the convergence and generalization of these models. Our analysis yields batch size scaling laws under two different cases: with a fixed compute budget, and with a fixed amount of training data. Extrapolation experiments on models of increasing sizes validate our predicted laws, which provides guidance for optimizing LLM training strategies under specific resource constraints.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 近年来取得了令人瞩目的进步，而缩放定律在这一快速进步中发挥着至关重要的作用。在本文中，我们通过实证研究了关键超参数（即全局批量大小）如何影响 LLM 训练过程。我们首先训练包含 1.25 亿到 26 亿个参数的语言模型，使用多达 3000 亿个高质量标记。通过这些实验，我们建立了模型大小和训练数据量的基本缩放定律。然后，我们研究不同的批量大小和学习率如何影响这些模型的收敛和泛化。我们的分析得出了两种不同情况下的批量大小缩放定律：计算预算固定和训练数据量固定。对不断增加的模型进行的外推实验验证了我们预测的定律，这为在特定资源约束下优化 LLM 训练策略提供了指导。</li>
</ul>

<h3>Title: Medchain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking</h3>
<ul>
<li><strong>Authors: </strong>Jie Liu, Wenxuan Wang, Zizhan Ma, Guolin Huang, Yihang SU, Kao-Jung Chang, Wenting Chen, Haoliang Li, Linlin Shen, Michael Lyu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01605">https://arxiv.org/abs/2412.01605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01605">https://arxiv.org/pdf/2412.01605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01605]] Medchain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking(https://arxiv.org/abs/2412.01605)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Clinical decision making (CDM) is a complex, dynamic process crucial to healthcare delivery, yet it remains a significant challenge for artificial intelligence systems. While Large Language Model (LLM)-based agents have been tested on general medical knowledge using licensing exams and knowledge question-answering tasks, their performance in the CDM in real-world scenarios is limited due to the lack of comprehensive testing datasets that mirror actual medical practice. To address this gap, we present MedChain, a dataset of 12,163 clinical cases that covers five key stages of clinical workflow. MedChain distinguishes itself from existing benchmarks with three key features of real-world clinical practice: personalization, interactivity, and sequentiality. Further, to tackle real-world CDM challenges, we also propose MedChain-Agent, an AI system that integrates a feedback mechanism and a MCase-RAG module to learn from previous cases and adapt its responses. MedChain-Agent demonstrates remarkable adaptability in gathering information dynamically and handling sequential clinical tasks, significantly outperforming existing approaches. The relevant dataset and code will be released upon acceptance of this paper.</li>
<li><strong>摘要：</strong>临床决策 (CDM) 是一个复杂、动态的过程，对医疗保健至关重要，但它仍然是人工智能系统面临的重大挑战。虽然基于大型语言模型 (LLM) 的代理已经通过执照考试和知识问答任务对一般医学知识进行了测试，但由于缺乏反映实际医疗实践的全面测试数据集，它们在现实场景中的 CDM 表现有限。为了解决这一差距，我们提出了 MedChain，这是一个包含 12,163 个临床病例的数据集，涵盖了临床工作流程的五个关键阶段。MedChain 与现有基准的不同之处在于它具有现实世界临床实践的三个关键特征：个性化、交互性和连续性。此外，为了应对现实世界的 CDM 挑战，我们还提出了 MedChain-Agent，这是一个集成了反馈机制和 MCase-RAG 模块的 AI 系统，可以从以前的案例中学习并调整其响应。MedChain-Agent 在动态收集信息和处理连续临床任务方面表现出了非凡的适应性，远远优于现有方法。相关数据集和代码将在本文被接受后发布。</li>
</ul>

<h3>Title: If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World</h3>
<ul>
<li><strong>Authors: </strong>Adrian de Wynter</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01617">https://arxiv.org/abs/2412.01617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01617">https://arxiv.org/pdf/2412.01617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01617]] If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World(https://arxiv.org/abs/2412.01617)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>Loneliness, or the lack of fulfilling relationships, significantly impacts a person's mental and physical well-being and is prevalent worldwide. Previous research suggests that large language models (LLMs) may help mitigate loneliness. However, we argue that the use of widespread LLMs like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose. To explore this, we analysed user interactions with ChatGPT, particularly those outside of its marketed use as task-oriented assistant. In dialogues classified as lonely, users frequently (37%) sought advice or validation, and received good engagement. However, ChatGPT failed in sensitive scenarios, like responding appropriately to suicidal ideation or trauma. We also observed a 35% higher incidence of toxic content, with women being 22 times more likely to be targeted than men. Our findings underscore ethical and legal questions about this technology, and note risks like radicalisation or further isolation. We conclude with recommendations for research and industry to address loneliness.</li>
<li><strong>摘要：</strong>孤独，或缺乏令人满意的人际关系，会严重影响一个人的身心健康，在世界各地都很普遍。先前的研究表明，大型语言模型 (LLM) 可能有助于缓解孤独感。然而，我们认为，像 ChatGPT 这样广泛使用的 LLM 更为普遍，也更具风险，因为它们不是为此目的而设计的。为了探索这一点，我们分析了用户与 ChatGPT 的互动，特别是那些除了其作为任务导向型助手的市场用途之外的互动。在被归类为孤独的对话中，用户经常 (37%) 寻求建议或认可，并获得良好的参与度。然而，ChatGPT 在敏感场景中失败了，比如对自杀意念或创伤做出适当的反应。我们还观察到有毒内容的发生率高出 35%，女性成为攻击目标的可能性是男性的 22 倍。我们的研究结果强调了有关这项技术的道德和法律问题，并指出了激进化或进一步孤立等风险。我们最后提出了针对研究和行业的建议，以解决孤独问题。</li>
</ul>

<h3>Title: NYT-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers</h3>
<ul>
<li><strong>Authors: </strong>Angel Yahir Loredo Lopez, Tyler McDonald, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01621">https://arxiv.org/abs/2412.01621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01621">https://arxiv.org/pdf/2412.01621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01621]] NYT-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers(https://arxiv.org/abs/2412.01621)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive performance on various benchmarks, yet their ability to engage in deliberate reasoning remains questionable. We present NYT-Connections, a collection of 358 simple word classification puzzles derived from the New York Times Connections game. This benchmark is designed to penalize quick, intuitive "System 1" thinking, isolating fundamental reasoning skills. We evaluated six recent LLMs, a simple machine learning heuristic, and humans across three configurations: single-attempt, multiple attempts without hints, and multiple attempts with contextual hints. Our findings reveal a significant performance gap: even top-performing LLMs like GPT-4 fall short of human performance by nearly 30%. Notably, advanced prompting techniques such as Chain-of-Thought and Self-Consistency show diminishing returns as task difficulty increases. NYT-Connections uniquely combines linguistic isolation, resistance to intuitive shortcuts, and regular updates to mitigate data leakage, offering a novel tool for assessing LLM reasoning capabilities.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各种基准测试中都表现出色，但它们进行深思熟虑推理的能力仍然值得怀疑。我们介绍了 NYT-Connections，这是一组 358 个简单的单词分类谜题，源自《纽约时报》Connections 游戏。该基准测试旨在惩罚快速、直观的“系统 1”思维，从而隔离基本推理技能。我们在三种配置中评估了六款最近的 LLM、一种简单的机器学习启发式方法和人类：单次尝试、多次尝试（没有提示）和多次尝试（有上下文提示）。我们的研究结果揭示了显著的性能差距：即使是表现最好的 LLM（如 GPT-4），也比人类的表现低了近 30%。值得注意的是，随着任务难度的增加，思维链和自洽等高级提示技术显示出收益递减。NYT-Connections 独特地结合了语言隔离、对直观捷径的抵抗以及定期更新以减轻数据泄漏，为评估 LLM 推理能力提供了一种新颖的工具。</li>
</ul>

<h3>Title: CHIMA: Headline-Guided Extractive Summarization for Thai News Articles</h3>
<ul>
<li><strong>Authors: </strong>Pimpitchaya Kositcharoensuk, Nakarin Sritrakool, Ploy N. Pratanwanich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01624">https://arxiv.org/abs/2412.01624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01624">https://arxiv.org/pdf/2412.01624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01624]] CHIMA: Headline-Guided Extractive Summarization for Thai News Articles(https://arxiv.org/abs/2412.01624)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Text summarization is a process of condensing lengthy texts while preserving their essential information. Previous studies have predominantly focused on high-resource languages, while low-resource languages like Thai have received less attention. Furthermore, earlier extractive summarization models for Thai texts have primarily relied on the article's body, without considering the headline. This omission can result in the exclusion of key sentences from the summary. To address these limitations, we propose CHIMA, an extractive summarization model that incorporates the contextual information of the headline for Thai news articles. Our model utilizes a pre-trained language model to capture complex language semantics and assigns a probability to each sentence to be included in the summary. By leveraging the headline to guide sentence selection, CHIMA enhances the model's ability to recover important sentences and discount irrelevant ones. Additionally, we introduce two strategies for aggregating headline-body similarities, simple average and harmonic mean, providing flexibility in sentence selection to accommodate varying writing styles. Experiments on publicly available Thai news datasets demonstrate that CHIMA outperforms baseline models across ROUGE, BLEU, and F1 scores. These results highlight the effectiveness of incorporating the headline-body similarities as model guidance. The results also indicate an enhancement in the model's ability to recall critical sentences, even those scattered throughout the middle or end of the article. With this potential, headline-guided extractive summarization offers a promising approach to improve the quality and relevance of summaries for Thai news articles.</li>
<li><strong>摘要：</strong>文本摘要是将长文本浓缩并保留其基本信息的过程。先前的研究主要集中在资源丰富的语言上，而资源匮乏的语言（如泰语）则受到的关注较少。此外，早期的泰语文本提取摘要模型主要依赖于文章正文，而不考虑标题。这种遗漏可能会导致关键句子被排除在摘要之外。为了解决这些限制，我们提出了 CHIMA，这是一种提取摘要模型，它结合了泰语新闻文章标题的上下文信息。我们的模型利用预先训练的语言模型来捕捉复杂的语言语义，并为每个句子分配一个包含在摘要中的概率。通过利用标题来指导句子选择，CHIMA 增强了模型恢复重要句子和忽略不相关句子的能力。此外，我们引入了两种聚合标题正文相似性的策略，即简单平均值和调和平均值，从而提供句子选择的灵活性以适应不同的写作风格。在公开的泰国新闻数据集上进行的实验表明，CHIMA 在 ROUGE、BLEU 和 F1 得分方面均优于基线模型。这些结果凸显了将标题与正文相似性作为模型指导的有效性。结果还表明，模型回忆关键句子的能力得到了增强，即使是那些分散在文章中间或结尾的句子。凭借这一潜力，标题引导的摘录摘要提供了一种有希望的方法来提高泰国新闻文章摘要的质量和相关性。</li>
</ul>

<h3>Title: Using Large Language Models in Automatic Hint Ranking and Generation Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jamshid Mozafari, Florian Gerhold, Adam Jatowt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01626">https://arxiv.org/abs/2412.01626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01626">https://arxiv.org/pdf/2412.01626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01626]] Using Large Language Models in Automatic Hint Ranking and Generation Tasks(https://arxiv.org/abs/2412.01626)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>The use of Large Language Models (LLMs) has increased significantly recently, with individuals frequently interacting with chatbots to receive answers to a wide range of questions. In an era where information is readily accessible, it is crucial to stimulate and preserve human cognitive abilities and maintain strong reasoning skills. This paper addresses such challenges by promoting the use of hints as an alternative or a supplement to direct answers. We first introduce a manually constructed hint dataset, WIKIHINT, which includes 5,000 hints created for 1,000 questions. We then finetune open-source LLMs such as LLaMA-3.1 for hint generation in answer-aware and answer-agnostic contexts. We assess the effectiveness of the hints with human participants who try to answer questions with and without the aid of hints. Additionally, we introduce a lightweight evaluation method, HINTRANK, to evaluate and rank hints in both answer-aware and answer-agnostic settings. Our findings show that (a) the dataset helps generate more effective hints, (b) including answer information along with questions generally improves hint quality, and (c) encoder-based models perform better than decoder-based models in hint ranking.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 的使用最近显著增加，个人经常与聊天机器人互动以获得各种问题的答案。在信息唾手可得的时代，激发和保持人类的认知能力以及保持强大的推理能力至关重要。本文通过提倡使用提示作为直接答案的替代或补充来解决这些挑战。我们首先介绍一个手动构建的提示数据集 WIKIHINT，其中包括为 1,000 个问题创建的 5,000 个提示。然后，我们对开源 LLM（例如 LLaMA-3.1）进行微调，以便在答案感知和答案不可知的环境中生成提示。我们通过尝试在有提示和没有提示帮助的情况下回答问题的人类参与者来评估提示的有效性。此外，我们引入了一种轻量级评估方法 HINTRANK，用于在答案感知和答案不可知的设置中评估和排名提示。我们的研究结果表明：(a) 数据集有助于生成更有效的提示；(b) 在问题中包含答案信息通常会提高提示质量；(c) 基于编码器的模型在提示排名方面比基于解码器的模型表现更好。</li>
</ul>

<h3>Title: Concept Based Continuous Prompts for Interpretable Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Qian Chen, Dongyang Li, Xiaofeng He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01644">https://arxiv.org/abs/2412.01644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01644">https://arxiv.org/pdf/2412.01644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01644]] Concept Based Continuous Prompts for Interpretable Text Classification(https://arxiv.org/abs/2412.01644)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt</a></li>
<li><strong>Abstract: </strong>Continuous prompts have become widely adopted for augmenting performance across a wide range of natural language tasks. However, the underlying mechanism of this enhancement remains obscure. Previous studies rely on individual words for interpreting continuous prompts, which lacks comprehensive semantic understanding. Drawing inspiration from Concept Bottleneck Models, we propose a framework for interpreting continuous prompts by decomposing them into human-readable concepts. Specifically, to ensure the feasibility of the decomposition, we demonstrate that a corresponding concept embedding matrix and a coefficient matrix can always be found to replace the prompt embedding matrix. Then, we employ GPT-4o to generate a concept pool and choose potential candidate concepts that are discriminative and representative using a novel submodular optimization algorithm. Experiments demonstrate that our framework can achieve similar results as the original P-tuning and word-based approaches using only a few concepts while providing more plausible results. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>连续提示已被广泛用于增强各种自然语言任务的性能。然而，这种增强的根本机制仍然不清楚。以前的研究依靠单个单词来解释连续提示，缺乏全面的语义理解。从概念瓶颈模型中汲取灵感，我们提出了一个框架，通过将连续提示分解为人类可读的概念来解释连续提示。具体来说，为了确保分解的可行性，我们证明总能找到相应的概念嵌入矩阵和系数矩阵来替换提示嵌入矩阵。然后，我们使用 GPT-4o 生成概念池，并使用一种新颖的子模块优化算法选择具有判别性和代表性的潜在候选概念。实验表明，我们的框架仅使用少量概念就可以实现与原始 P 调整和基于单词的方法类似的结果，同时提供更合理的结果。我们的代码可在此 https URL 上找到。</li>
</ul>

<h3>Title: Can We Afford The Perfect Prompt? Balancing Cost and Accuracy with the Economical Prompting Index</h3>
<ul>
<li><strong>Authors: </strong>Tyler McDonald, Anthony Colosimo, Yifeng Li, Ali Emami</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01690">https://arxiv.org/abs/2412.01690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01690">https://arxiv.org/pdf/2412.01690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01690]] Can We Afford The Perfect Prompt? Balancing Cost and Accuracy with the Economical Prompting Index(https://arxiv.org/abs/2412.01690)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>As prompt engineering research rapidly evolves, evaluations beyond accuracy are crucial for developing cost-effective techniques. We present the Economical Prompting Index (EPI), a novel metric that combines accuracy scores with token consumption, adjusted by a user-specified cost concern level to reflect different resource constraints. Our study examines 6 advanced prompting techniques, including Chain-of-Thought, Self-Consistency, and Tree of Thoughts, across 10 widely-used language models and 4 diverse datasets. We demonstrate that approaches such as Self-Consistency often provide statistically insignificant gains while becoming cost-prohibitive. For example, on high-performing models like Claude 3.5 Sonnet, the EPI of simpler techniques like Chain-of-Thought (0.72) surpasses more complex methods like Self-Consistency (0.64) at slight cost concern levels. Our findings suggest a reevaluation of complex prompting strategies in resource-constrained scenarios, potentially reshaping future research priorities and improving cost-effectiveness for end-users.</li>
<li><strong>摘要：</strong>随着即时工程研究的快速发展，除了准确性之外的评估对于开发具有成本效益的技术至关重要。我们提出了经济提示指数 (EPI)，这是一种将准确性得分与令牌消耗相结合的新指标，可根据用户指定的成本关注程度进行调整，以反映不同的资源限制。我们的研究考察了 10 种广泛使用的语言模型和 4 个不同的数据集中的 6 种高级提示技术，包括思维链、自洽和思维树。我们表明，诸如自洽之类的方法通常会提供统计上不显着的收益，但成本却过高。例如，在 Claude 3.5 Sonnet 等高性能模型上，在成本关注程度略高的水平上，思维链 (0.72) 等更简单的技术的 EPI 超过了自洽 (0.64) 等更复杂的方法。我们的研究结果表明，在资源受限的情况下，需要重新评估复杂的提示策略，这可能会重塑未来的研究重点并提高最终用户的成本效益。</li>
</ul>

<h3>Title: Are We There Yet? Revealing the Risks of Utilizing Large Language Models in Scholarly Peer Review</h3>
<ul>
<li><strong>Authors: </strong>Rui Ye, Xianghe Pang, Jingyi Chai, Jiaao Chen, Zhenfei Yin, Zhen Xiang, Xiaowen Dong, Jing Shao, Siheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01708">https://arxiv.org/abs/2412.01708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01708">https://arxiv.org/pdf/2412.01708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01708]] Are We There Yet? Revealing the Risks of Utilizing Large Language Models in Scholarly Peer Review(https://arxiv.org/abs/2412.01708)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Scholarly peer review is a cornerstone of scientific advancement, but the system is under strain due to increasing manuscript submissions and the labor-intensive nature of the process. Recent advancements in large language models (LLMs) have led to their integration into peer review, with promising results such as substantial overlaps between LLM- and human-generated reviews. However, the unchecked adoption of LLMs poses significant risks to the integrity of the peer review system. In this study, we comprehensively analyze the vulnerabilities of LLM-generated reviews by focusing on manipulation and inherent flaws. Our experiments show that injecting covert deliberate content into manuscripts allows authors to explicitly manipulate LLM reviews, leading to inflated ratings and reduced alignment with human reviews. In a simulation, we find that manipulating 5% of the reviews could potentially cause 12% of the papers to lose their position in the top 30% rankings. Implicit manipulation, where authors strategically highlight minor limitations in their papers, further demonstrates LLMs' susceptibility compared to human reviewers, with a 4.5 times higher consistency with disclosed limitations. Additionally, LLMs exhibit inherent flaws, such as potentially assigning higher ratings to incomplete papers compared to full papers and favoring well-known authors in single-blind review process. These findings highlight the risks of over-reliance on LLMs in peer review, underscoring that we are not yet ready for widespread adoption and emphasizing the need for robust safeguards.</li>
<li><strong>摘要：</strong>学术同行评审是科学进步的基石，但由于稿件提交量不断增加以及该过程的劳动密集性，该系统承受着巨大的压力。大型语言模型 (LLM) 的最新进展已将其整合到同行评审中，并取得了令人鼓舞的结果，例如 LLM 和人工生成的评审之间存在大量重叠。然而，不受控制地采用 LLM 对同行评审系统的完整性构成了重大风险。在本研究中，我们通过关注操纵和固有缺陷，全面分析了 LLM 生成的评审的漏洞。我们的实验表明，在稿件中注入隐性故意内容允许作者明确操纵 LLM 评审，从而导致评分虚高并降低与人工评审的一致性。在模拟中，我们发现操纵 5% 的评审可能会导致 12% 的论文失去前 30% 的排名。隐性操纵，即作者策略性地突出论文中的小缺陷，进一步证明了 LLM 与人类审阅者相比的敏感性，与公开的局限性的一致性高出 4.5 倍。此外，法学硕士 (LLM) 还存在固有缺陷，例如，与完整论文相比，不完整论文的评分可能会更高，并且在单盲评审过程中更青睐知名作者。这些发现凸显了在同行评审中过度依赖法学硕士的风险，强调我们尚未准备好广泛采用，并强调需要采取强有力的保障措施。</li>
</ul>

<h3>Title: Towards Resource Efficient and Interpretable Bias Mitigation in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Schrasing Tong, Eliott Zemour, Rawisara Lohanimit, Lalana Kagal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.01711">https://arxiv.org/abs/2412.01711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.01711">https://arxiv.org/pdf/2412.01711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.01711]] Towards Resource Efficient and Interpretable Bias Mitigation in Large Language Models(https://arxiv.org/abs/2412.01711)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Although large language models (LLMs) have demonstrated their effectiveness in a wide range of applications, they have also been observed to perpetuate unwanted biases present in the training data, potentially leading to harm for marginalized communities. In this paper, we mitigate bias by leveraging small biased and anti-biased expert models to obtain a debiasing signal that will be added to the LLM output at decoding-time. This approach combines resource efficiency with interpretability and can be optimized for mitigating specific types of bias, depending on the target use case. Experiments on mitigating gender, race, and religion biases show a reduction in bias on several local and global bias metrics while preserving language model performance.</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 已在广泛的应用中证明了其有效性，但人们也观察到它们会延续训练数据中存在的不良偏见，从而可能对边缘化社区造成伤害。在本文中，我们利用小型有偏见和反偏见专家模型来减轻偏见，以获得将在解码时添加到 LLM 输出中的去偏见信号。这种方法结合了资源效率和可解释性，并且可以根据目标用例进行优化以减轻特定类型的偏见。减轻性别、种族和宗教偏见的实验表明，在保持语言模型性能的同时，几个局部和全局偏见指标上的偏见有所减少。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
