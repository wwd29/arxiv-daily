<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-07-12</h1>
<h3>Title: FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in Domain-specific Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Yongjian Tang, Rakebul Hasan, Thomas Runkler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08035">https://arxiv.org/abs/2407.08035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08035">https://arxiv.org/pdf/2407.08035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08035]] FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in Domain-specific Scenarios(https://arxiv.org/abs/2407.08035)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have provided a new pathway for Named Entity Recognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting methods avoid the need for training, conserve substantial computational resources, and rely on minimal annotated data. Previous studies have achieved comparable performance to fully supervised BERT-based fine-tuning approaches on general NER benchmarks. However, none of the previous approaches has investigated the efficiency of LLM-based few-shot learning in domain-specific scenarios. To address this gap, we introduce FsPONER, a novel approach for optimizing few-shot prompts, and evaluate its performance on domain-specific NER datasets, with a focus on industrial manufacturing and maintenance, while using multiple LLMs -- GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna. FsPONER consists of three few-shot selection methods based on random sampling, TF-IDF vectors, and a combination of both. We compare these methods with a general-purpose GPT-NER method as the number of few-shot examples increases and evaluate their optimal NER performance against fine-tuned BERT and LLaMA 2-chat. In the considered real-world scenarios with data scarcity, FsPONER with TF-IDF surpasses fine-tuned models by approximately 10% in F1 score.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 为命名实体识别 (NER) 任务提供了一条新途径。与微调相比，基于 LLM 的提示方法无需训练，节省了大量计算资源，并且仅依赖最少的注释数据。先前的研究在通用 NER 基准上取得了与完全监督的基于 BERT 的微调方法相当的性能。但是，之前的方法都没有研究过基于 LLM 的少样本学习在特定领域场景中的效率。为了解决这一差距，我们引入了一种优化少样本提示的新方法 FsPONER，并评估了其在特定领域 NER 数据集上的性能，重点关注工业制造和维护，同时使用多个 LLM——GPT-4-32K、GPT-3.5-Turbo、LLaMA 2-chat 和 Vicuna。FsPONER 由三种基于随机抽样、TF-IDF 向量和两者结合的少样本选择方法组成。随着小样本示例数量的增加，我们将这些方法与通用 GPT-NER 方法进行比较，并评估它们与微调 BERT 和 LLaMA 2-chat 的最佳 NER 性能。在考虑数据稀缺的现实场景中，具有 TF-IDF 的 FsPONER 在 F1 得分上比微调模型高出约 10%。</li>
</ul>

<h3>Title: Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi R. Fung, Jing Li, Manling Li, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08039">https://arxiv.org/abs/2407.08039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08039">https://arxiv.org/pdf/2407.08039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08039]] Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models(https://arxiv.org/abs/2407.08039)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Hallucination is often regarded as a major impediment for using large language models (LLMs), especially for knowledge-intensive tasks. Even when the training corpus consists solely of true statements, language models still generate hallucinations in the form of amalgamations of multiple facts. We coin this phenomenon as ``knowledge overshadowing'': when we query knowledge from a language model with multiple conditions, some conditions overshadow others, leading to hallucinated outputs. This phenomenon partially stems from training data imbalance, which we verify on both pretrained models and fine-tuned models, over a wide range of LM model families and sizes.From a theoretical point of view, knowledge overshadowing can be interpreted as over-generalization of the dominant conditions (patterns). We show that the hallucination rate grows with both the imbalance ratio (between the popular and unpopular condition) and the length of dominant condition description, consistent with our derived generalization bound. Finally, we propose to utilize overshadowing conditions as a signal to catch hallucination before it is produced, along with a training-free self-contrastive decoding method to alleviate hallucination during inference. Our proposed approach showcases up to 82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control, with different models and datasets.</li>
<li><strong>摘要：</strong>幻觉通常被视为使用大型语言模型 (LLM) 的主要障碍，尤其是对于知识密集型任务。即使训练语料库仅由真实陈述组成，语言模型仍然会以多种事实融合的形式产生幻觉。我们将这种现象称为“知识遮蔽”：当我们从具有多种条件的语言模型中查询知识时，某些条件会遮蔽其他条件，从而导致幻觉输出。这种现象部分源于训练数据不平衡，我们在广泛的 LM 模型系列和大小的预训练模型和微调模型上验证了这一点。从理论的角度来看，知识遮蔽可以解释为主导条件（模式）的过度概括。我们表明，幻觉率随着不平衡率（流行和不流行条件之间）和主导条件描述长度的增加而增长，这与我们得出的泛化界限一致。最后，我们建议利用遮蔽条件作为信号，在幻觉产生之前将其捕捉到，同时使用无需训练的自对比解码方法来缓解推理过程中的幻觉。我们提出的方法在不同的模型和数据集下，幻觉预测的 F1 高达 82%，幻觉控制的 F1 为 11.2% 至 39.4%。</li>
</ul>

<h3>Title: RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization</h3>
<ul>
<li><strong>Authors: </strong>Xijie Huang, Zechun Liu, Shih-Yang Liu, Kwang-Ting Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08044">https://arxiv.org/abs/2407.08044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08044">https://arxiv.org/pdf/2407.08044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08044]] RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization(https://arxiv.org/abs/2407.08044)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient Fine-Tuning (PEFT)method, significantly enhances the training efficiency by updating only a small portion of the weights in Large Language Models (LLMs). Recently, weight-only quantization techniques have also been applied to LoRA methods to reduce the memory footprint of fine-tuning. However, applying weight-activation quantization to the LoRA pipeline is under-explored, and we observe substantial performance degradation primarily due to the presence of activation outliers. In this work, we propose RoLoRA, the first LoRA-based scheme for effective weight-activation quantization. RoLoRA utilizes rotation for outlier elimination and proposes rotation-aware fine-tuning to preserve the outlier-free characteristics in rotated LLMs. Experimental results show RoLoRA consistently improves low-bit LoRA convergence and post-training quantization robustness in weight-activation settings. We evaluate RoLoRA across LLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain of 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks compared to LoRA baseline. We further demonstrate its effectiveness on Large Multimodal Models (LLaVA-1.5-7B). Codes are available at this https URL</li>
<li><strong>摘要：</strong>低秩自适应 (LoRA) 作为一种代表性的参数高效微调 (PEFT) 方法，通过仅更新大型语言模型 (LLM) 中的一小部分权重，显著提高了训练效率。最近，仅权重量化技术也已应用于 LoRA 方法，以减少微调的内存占用。然而，将权重激活量化应用于 LoRA 管道尚未得到充分探索，我们观察到性能大幅下降，主要是由于激活异常值的存在。在这项工作中，我们提出了 RoLoRA，这是第一个基于 LoRA 的有效权重激活量化方案。RoLoRA 利用旋转来消除异常值，并提出旋转感知微调以保留旋转 LLM 中的无异常值特性。实验结果表明，RoLoRA 在权重激活设置下持续改善了低位 LoRA 收敛和训练后量化鲁棒性。我们在 LLaMA2-7B/13B、LLaMA3-8B 模型上评估了 RoLoRA，与 LoRA 基线相比，4 位权重激活量化 LLaMA2-13B 在常识推理任务上的绝对准确率提高了 29.5%。我们进一步证明了其在大型多模态模型 (LLaVA-1.5-7B) 上的有效性。代码可从此 https URL 获取</li>
</ul>

<h3>Title: Automata-based constraints for language model decoding</h3>
<ul>
<li><strong>Authors: </strong>Terry Koo, Frederick Liu, Luheng He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.FL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08103">https://arxiv.org/abs/2407.08103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08103">https://arxiv.org/pdf/2407.08103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08103]] Automata-based constraints for language model decoding(https://arxiv.org/abs/2407.08103)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>LMs are often expected to generate strings in some formal language; for example, structured data, API calls, or code snippets. Although LMs can be tuned to improve their adherence to formal syntax, this does not guarantee conformance, especially with smaller LMs suitable for large-scale deployment. In addition, tuning requires significant resources, making it impractical for uncommon or task-specific formats. To prevent downstream parsing errors we would ideally constrain the LM to only produce valid output, but this is severely complicated by tokenization, which is typically both ambiguous and misaligned with the formal grammar. We solve these issues through the application of automata theory, deriving an efficient closed-form solution for the regular languages, a broad class of formal languages with many practical applications, including API calls or schema-guided JSON and YAML. We also discuss pragmatic extensions for coping with the issue of high branching factor. Finally, we extend our techniques to deterministic context-free languages, which similarly admit an efficient closed-form solution. In spite of its flexibility and representative power, our approach only requires access to per-token decoding logits and lowers into simple calculations that are independent of LM size, making it both efficient and easy to apply to almost any LM architecture.</li>
<li><strong>摘要：</strong>人们通常希望 LM 以某种形式语言生成字符串；例如，结构化数据、API 调用或代码片段。虽然可以调整 LM 以提高其对形式语法的遵守，但这并不能保证一致性，尤其是对于适合大规模部署的小型 LM。此外，调整需要大量资源，因此对于不常见或特定于任务的格式来说，调整是不切实际的。为了防止下游解析错误，我们理想情况下会限制 LM 仅产生有效输出，但这因标记化而变得非常复杂，标记化通常既含糊又与形式语法不一致。我们通过应用自动机理论解决了这些问题，为常规语言（一种具有许多实际应用的广泛形式语言，包括 API 调用或模式引导的 JSON 和 YAML）推导出有效的闭式解决方案。我们还讨论了用于应对高分支因子问题的实用扩展。最后，我们将我们的技术扩展到确定性上下文无关语言，它们同样承认有效的闭式解决方案。尽管我们的方法具有灵活性和代表性，但它只需要访问每个标记解码逻辑，并降低为与 LM 大小无关的简单计算，从而使其既高效又易于应用于几乎任何 LM 架构。</li>
</ul>

<h3>Title: Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal Theory for Post-Purchase Intention Analysis</h3>
<ul>
<li><strong>Authors: </strong>Gerard Christopher Yeo, Shaz Furniturewala, Kokil Jaidka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08182">https://arxiv.org/abs/2407.08182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08182">https://arxiv.org/pdf/2407.08182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08182]] Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal Theory for Post-Purchase Intention Analysis(https://arxiv.org/abs/2407.08182)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Supervised machine-learning models for predicting user behavior offer a challenging classification problem with lower average prediction performance scores than other text classification tasks. This study evaluates multi-task learning frameworks grounded in Cognitive Appraisal Theory to predict user behavior as a function of users' self-expression and psychological attributes. Our experiments show that users' language and traits improve predictions above and beyond models predicting only from text. Our findings highlight the importance of integrating psychological constructs into NLP to enhance the understanding and prediction of user actions. We close with a discussion of the implications for future applications of large language models for computational psychology.</li>
<li><strong>摘要：</strong>用于预测用户行为的监督机器学习模型提出了一个具有挑战性的分类问题，其平均预测性能得分低于其他文本分类任务。本研究评估了基于认知评估理论的多任务学习框架，以根据用户的自我表达和心理属性预测用户行为。我们的实验表明，用户的语言和特征可以提高预测能力，优于仅从文本进行预测的模型。我们的研究结果强调了将心理结构整合到 NLP 中以增强对用户行为的理解和预测的重要性。最后，我们讨论了大型语言模型对计算心理学未来应用的影响。</li>
</ul>

<h3>Title: fairBERTs: Erasing Sensitive Information Through Semantic and Fairness-aware Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Jinfeng Li, Yuefeng Chen, Xiangyu Liu, Longtao Huang, Rong Zhang, Hui Xue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08189">https://arxiv.org/abs/2407.08189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08189">https://arxiv.org/pdf/2407.08189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08189]] fairBERTs: Erasing Sensitive Information Through Semantic and Fairness-aware Perturbations(https://arxiv.org/abs/2407.08189)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) have revolutionized both the natural language processing research and applications. However, stereotypical biases (e.g., gender and racial discrimination) encoded in PLMs have raised negative ethical implications for PLMs, which critically limits their broader applications. To address the aforementioned unfairness issues, we present fairBERTs, a general framework for learning fair fine-tuned BERT series models by erasing the protected sensitive information via semantic and fairness-aware perturbations generated by a generative adversarial network. Through extensive qualitative and quantitative experiments on two real-world tasks, we demonstrate the great superiority of fairBERTs in mitigating unfairness while maintaining the model utility. We also verify the feasibility of transferring adversarial components in fairBERTs to other conventionally trained BERT-like models for yielding fairness improvements. Our findings may shed light on further research on building fairer fine-tuned PLMs.</li>
<li><strong>摘要：</strong>预训练语言模型 (PLM) 彻底改变了自然语言处理研究和应用。然而，PLM 中编码的刻板偏见（例如性别和种族歧视）对 PLM 产生了负面的道德影响，这严重限制了其更广泛的应用。为了解决上述不公平问题，我们提出了 fairBERTs，这是一个通过生成对抗网络生成的语义和公平感知扰动来擦除受保护的敏感信息，从而学习公平微调 BERT 系列模型的通用框架。通过对两个真实任务进行大量定性和定量实验，我们证明了 fairBERTs 在减轻不公平性的同时保持模型效用的巨大优势。我们还验证了将 fairBERTs 中的对抗性组件转移到其他传统训练的 BERT 类模型以实现公平性改进的可行性。我们的研究结果可能有助于进一步研究构建更公平的微调 PLM。</li>
</ul>

<h3>Title: Generating Contextually-Relevant Navigation Instructions for Blind and Low Vision People</h3>
<ul>
<li><strong>Authors: </strong>Zain Merchant, Abrar Anwar, Emily Wang, Souti Chattopadhyay, Jesse Thomason</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08219">https://arxiv.org/abs/2407.08219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08219">https://arxiv.org/pdf/2407.08219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08219]] Generating Contextually-Relevant Navigation Instructions for Blind and Low Vision People(https://arxiv.org/abs/2407.08219)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Navigating unfamiliar environments presents significant challenges for blind and low-vision (BLV) individuals. In this work, we construct a dataset of images and goals across different scenarios such as searching through kitchens or navigating outdoors. We then investigate how grounded instruction generation methods can provide contextually-relevant navigational guidance to users in these instances. Through a sighted user study, we demonstrate that large pretrained language models can produce correct and useful instructions perceived as beneficial for BLV users. We also conduct a survey and interview with 4 BLV users and observe useful insights on preferences for different instructions based on the scenario.</li>
<li><strong>摘要：</strong>对于盲人和低视力 (BLV) 人士来说，在陌生的环境中导航是一项重大挑战。在这项工作中，我们构建了一个包含不同场景（例如在厨房中搜索或在户外导航）的图像和目标的数据集。然后，我们研究了扎实的指令生成方法如何在这些情况下为用户提供与上下文相关的导航指导。通过一项视力正常的用户研究，我们证明大型预训练语言模型可以生成正确且有用的指令，这些指令被认为对 BLV 用户有益。我们还对 4 名 BLV 用户进行了调查和采访，并根据场景观察了对不同指令偏好的有用见解。</li>
</ul>

<h3>Title: Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting</h3>
<ul>
<li><strong>Authors: </strong>Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, Chen-Yu Lee, Tomas Pfister</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08223">https://arxiv.org/abs/2407.08223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08223">https://arxiv.org/pdf/2407.08223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08223]] Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting(https://arxiv.org/abs/2407.08223)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, long context, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs. In this work, we introduce Speculative RAG - a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long context. Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. Extensive experiments demonstrate that Speculative RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy by up to 12.97% while reducing latency by 51% compared to conventional RAG systems on PubHealth.</li>
<li><strong>摘要：</strong>检索增强生成 (RAG) 将大型语言模型 (LLM) 的生成能力与外部知识源相结合，以提供更准确、最新的响应。RAG 的最新进展侧重于通过迭代 LLM 细化或通过对 LLM 进行额外指令调整获得的自我批评能力来改善检索结果。在这项工作中，我们引入了 Speculative RAG - 一个利用较大的通用 LM 来有效验证由较小的、精炼的专业 LM 并行生成的多个 RAG 草稿的框架。每个草稿都是从检索到的文档的不同子集生成的，提供对证据的不同视角，同时减少每个草稿的输入标记数。这种方法增强了对每个子集的理解，并减轻了长上下文中潜在的位置偏差。我们的方法通过将起草委托给较小的专业 LM 来加速 RAG，较大的通用 LM 对草稿执行一次验证。大量实验表明，Speculative RAG 在 TriviaQA、MuSiQue、PubHealth 和 ARC-Challenge 基准测试中实现了最佳性能，同时降低了延迟。与 PubHealth 上的传统 RAG 系统相比，它的准确率显著提高了 12.97%，同时将延迟降低了 51%。</li>
</ul>

<h3>Title: LLMs' morphological analyses of complex FST-generated Finnish words</h3>
<ul>
<li><strong>Authors: </strong>Anssi Moisio, Mathias Creutz, Mikko Kurimo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08269">https://arxiv.org/abs/2407.08269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08269">https://arxiv.org/pdf/2407.08269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08269]] LLMs' morphological analyses of complex FST-generated Finnish words(https://arxiv.org/abs/2407.08269)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>Rule-based language processing systems have been overshadowed by neural systems in terms of utility, but it remains unclear whether neural NLP systems, in practice, learn the grammar rules that humans use. This work aims to shed light on the issue by evaluating state-of-the-art LLMs in a task of morphological analysis of complex Finnish noun forms. We generate the forms using an FST tool, and they are unlikely to have occurred in the training sets of the LLMs, therefore requiring morphological generalisation capacity. We find that GPT-4-turbo has some difficulties in the task while GPT-3.5-turbo struggles and smaller models Llama2-70B and Poro-34B fail nearly completely.</li>
<li><strong>摘要：</strong>基于规则的语言处理系统在实用性方面一直被神经系统所掩盖，但神经 NLP 系统在实践中是否学习人类使用的语法规则仍不清楚。这项工作旨在通过评估最先进的 LLM 在复杂芬兰名词形式的形态分析任务中的表现来阐明这一问题。我们使用 FST 工具生成形式，它们不太可能出现在 LLM 的训练集中，因此需要形态泛化能力。我们发现 GPT-4-turbo 在这项任务中遇到了一些困难，而 GPT-3.5-turbo 则举步维艰，较小的模型 Llama2-70B 和 Poro-34B 几乎完全失败。</li>
</ul>

<h3>Title: RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Zhenhe Wu, Zhongqiu Li, Jie Zhang, Mengxiang Li, Yu Zhao, Ruiyu Fang, Zhongjiang He, Xuelong Li, Zhoujun Li, Shuangyong Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/">https://arxiv.org/abs/</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/">https://arxiv.org/pdf/</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[]] RB-SQL: A Retrieval-based LLM Framework for Text-to-SQL(https://arxiv.org/abs/)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) with in-context learning have significantly improved the performance of text-to-SQL task. Previous works generally focus on using exclusive SQL generation prompt to improve the LLMs' reasoning ability. However, they are mostly hard to handle large databases with numerous tables and columns, and usually ignore the significance of pre-processing database and extracting valuable information for more efficient prompt engineering. Based on above analysis, we propose RB-SQL, a novel retrieval-based LLM framework for in-context prompt engineering, which consists of three modules that retrieve concise tables and columns as schema, and targeted examples for in-context learning. Experiment results demonstrate that our model achieves better performance than several competitive baselines on public datasets BIRD and Spider.</li>
<li><strong>摘要：</strong>具有上下文学习功能的大型语言模型 (LLM) 显著提高了文本到 SQL 任务的性能。以前的研究通常侧重于使用独有的 SQL 生成提示来提高 LLM 的推理能力。然而，它们大多难以处理具有大量表和列的大型数据库，并且通常忽略了预处理数据库和提取有价值信息对于更高效的提示工程的重要性。基于以上分析，我们提出了一种基于检索的上下文提示工程 LLM 框架 RB-SQL，它由三个模块组成，用于检索简洁的表和列作为模式，以及用于上下文学习的目标示例。实验结果表明，我们的模型在公共数据集 BIRD 和 Spider 上取得了比几个竞争基线更好的性能。</li>
</ul>

<h3>Title: AutoBencher: Creating Salient, Novel, Difficult Datasets for Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiang Lisa Li, Evan Zheran Liu, Percy Liang, Tatsunori Hashimoto</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08351">https://arxiv.org/abs/2407.08351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08351">https://arxiv.org/pdf/2407.08351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08351]] AutoBencher: Creating Salient, Novel, Difficult Datasets for Language Models(https://arxiv.org/abs/2407.08351)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Evaluation is critical for assessing capabilities, tracking scientific progress, and informing model selection. In this paper, we present three desiderata for a good benchmark for language models: (i) salience (e.g., knowledge about World War II is more salient than a random day in history), (ii) novelty (i.e., the benchmark reveals new trends in model rankings not shown by previous benchmarks), and (iii) difficulty (i.e., the benchmark should be difficult for existing models, leaving headroom for future improvement). We operationalize these three desiderata and cast benchmark creation as a search problem, that of finding benchmarks that that satisfy all three desiderata. To tackle this search problem, we present AutoBencher, which uses a language model to automatically search for datasets that meet the three desiderata. AutoBencher uses privileged information (e.g. relevant documents) to construct reliable datasets, and adaptivity with reranking to optimize for the search objective. We use AutoBencher to create datasets for math, multilingual, and knowledge-intensive question answering. The scalability of AutoBencher allows it to test fine-grained categories and tail knowledge, creating datasets that are on average 27% more novel and 22% more difficult than existing benchmarks. A closer investigation of our constructed datasets shows that we can identify specific gaps in LM knowledge in language models that are not captured by existing benchmarks, such as Gemini Pro performing much worse on question answering about the Permian Extinction and Fordism, while OpenAGI-7B performing surprisingly well on QA about COVID-19.</li>
<li><strong>摘要：</strong>评估对于评估能力、跟踪科学进展和指导模型选择至关重要。在本文中，我们提出了语言模型良好基准的三个要求：（i）显著性（例如，有关第二次世界大战的知识比历史上的某一天更显著），（ii）新颖性（即基准揭示了以前基准未显示的模型排名的新趋势），以及（iii）难度（即基准应该对现有模型来说很难，为未来的改进留出空间）。我们将这三个要求付诸实践，并将基准创建视为一个搜索问题，即找到满足所有三个要求的基准。为了解决这个搜索问题，我们提出了 AutoBencher，它使用语言模型自动搜索满足这三个要求的数据集。AutoBencher 使用特权信息（例如相关文档）来构建可靠的数据集，并使用重新排名的自适应性来优化搜索目标。我们使用 AutoBencher 创建数学、多语言和知识密集型问答数据集。AutoBencher 的可扩展性使其能够测试细粒度类别和尾部知识，从而创建比现有基准平均新颖 27%、难度增加 22% 的数据集。仔细调查我们构建的数据集后发现，我们可以确定语言模型中 LM 知识的具体差距，而这些差距未被现有基准所捕捉到，例如 Gemini Pro 在回答有关二叠纪灭绝和福特主义的问题时表现要差得多，而 OpenAGI-7B 在有关 COVID-19 的问答方面表现出人意料地好。</li>
</ul>

<h3>Title: Self-training Language Models for Arithmetic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Marek Kadlčík, Michal Štefánik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08400">https://arxiv.org/abs/2407.08400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08400">https://arxiv.org/pdf/2407.08400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08400]] Self-training Language Models for Arithmetic Reasoning(https://arxiv.org/abs/2407.08400)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Language models achieve impressive results in tasks involving complex multistep reasoning, but scaling these capabilities further traditionally requires expensive collection of more annotated data. In this work, we explore the potential of improving the capabilities of language models without new data, merely using automated feedback to the validity of their predictions in arithmetic reasoning (self-training). We find that models can substantially improve in both single-round (offline) and online self-training. In the offline setting, supervised methods are able to deliver gains comparable to preference optimization, but in online self-training, preference optimization shows to largely outperform supervised training thanks to superior stability and robustness on unseen types of problems.</li>
<li><strong>摘要：</strong>语言模型在涉及复杂多步推理的任务中取得了令人印象深刻的成果，但进一步扩展这些能力传统上需要花费大量金钱收集更多带注释的数据。在这项工作中，我们探索了在没有新数据的情况下提高语言模型能力的潜力，仅仅使用自动反馈来验证算术推理（自我训练）中预测的有效性。我们发现模型可以在单轮（离线）和在线自我训练中得到显着改进。在离线设置中，监督方法能够提供与偏好优化相当的收益，但在在线自我训练中，由于对未知类型的问题具有出色的稳定性和鲁棒性，偏好优化显示出远远优于监督训练。</li>
</ul>

<h3>Title: Beyond Instruction Following: Evaluating Rule Following of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wangtao Sun, Chenxiang Zhang, Xueyou Zhang, Ziyang Huang, Haotian Xu, Pei Chen, Shizhu He, Jun Zhao, Kang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08440">https://arxiv.org/abs/2407.08440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08440">https://arxiv.org/pdf/2407.08440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08440]] Beyond Instruction Following: Evaluating Rule Following of Large Language Models(https://arxiv.org/abs/2407.08440)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Although Large Language Models (LLMs) have demonstrated strong instruction-following ability to be helpful, they are further supposed to be controlled and guided by rules in real-world scenarios to be safe, and accurate in responses. This demands the possession of rule-following capability of LLMs. However, few works have made a clear evaluation of the rule-following capability of LLMs. Previous studies that try to evaluate the rule-following capability of LLMs fail to distinguish the rule-following scenarios from the instruction-following scenarios. Therefore, this paper first makes a clarification of the concept of rule-following, and curates a comprehensive benchmark, RuleBench, to evaluate a diversified range of rule-following abilities. Our experimental results on a variety of LLMs show that they are still limited in following rules. Our further analysis provides insights into the improvements for LLMs toward a better rule-following intelligent agent. The data and code can be found at: https://anonymous.4open.science/r/llm-rule-following-B3E3/</li>
<li><strong>摘要：</strong>尽管大型语言模型 (LLM) 已经展现出强大的指令遵循能力，但它们还需要在现实场景中受规则控制和引导，以确保安全性和响应准确性。这要求 LLM 具备规则遵循能力。然而，很少有研究对 LLM 的规则遵循能力做出明确的评估。先前试图评估 LLM 规则遵循能力的研究未能区分规则遵循场景和指令遵循场景。因此，本文首先澄清规则遵循的概念，并制定了一个全面的基准 RuleBench，以评估各种规则遵循能力。我们对各种 LLM 的实验结果表明，它们在遵循规则方面仍然有限。我们的进一步分析为改进 LLM 以成为更好的规则遵循智能代理提供了见解。数据和代码可以在以下网址找到：https://anonymous.4open.science/r/llm-rule-following-B3E3/</li>
</ul>

<h3>Title: Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Cantini, Giada Cosenza, Alessio Orsino, Domenico Talia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08441">https://arxiv.org/abs/2407.08441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08441">https://arxiv.org/pdf/2407.08441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08441]] Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation(https://arxiv.org/abs/2407.08441)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized artificial intelligence, demonstrating remarkable computational power and linguistic capabilities. However, these models are inherently prone to various biases stemming from their training data. These include selection, linguistic, and confirmation biases, along with common stereotypes related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age. This study explores the presence of these biases within the responses given by the most recent LLMs, analyzing the impact on their fairness and reliability. We also investigate how known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation. Extensive experiments are conducted using the most widespread LLMs at different scales, confirming that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes. Our findings underscore the importance of enhancing mitigation techniques to address these safety issues, toward a more sustainable and inclusive artificial intelligence.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 彻底改变了人工智能，展示了非凡的计算能力和语言能力。然而，这些模型本质上容易受到源自其训练数据的各种偏见的影响。这些偏见包括选择、语言和确认偏见，以及与性别、种族、性取向、宗教、社会经济地位、残疾和年龄相关的常见刻板印象。本研究探讨了这些偏见在最新 LLM 给出的回答中的存在，分析了它们对其公平性和可靠性的影响。我们还研究了如何利用已知的提示工程技术来有效揭示 LLM 的隐藏偏见，测试它们对专门为偏见引出而制作的越狱提示的对抗鲁棒性。使用不同规模的最广泛的 LLM 进行了广泛的实验，证实尽管 LLM 具有先进的功能和复杂的对齐过程，但仍可以被操纵以产生有偏见或不适当的响应。我们的研究结果强调了增强缓解技术以解决这些安全问题的重要性，以实现更可持续和更具包容性的人工智能。</li>
</ul>

<h3>Title: Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zheng Wang, Boxiao Jin, Zhongzhi Yu, Minjia Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08454">https://arxiv.org/abs/2407.08454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08454">https://arxiv.org/pdf/2407.08454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08454]] Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks(https://arxiv.org/abs/2407.08454)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chat</a></li>
<li><strong>Abstract: </strong>How to efficiently serve Large Language Models (LLMs) has become a pressing issue because of their huge computational cost in their autoregressive generation process. To mitigate computational costs, LLMs often employ the KV Cache technique to improve the generation speed. While improving the computational efficiency, the storage requirements of the KV cache are substantial, particularly in long-context scenarios, leading to significant memory consumption. Existing KV cache eviction methods often degrade the performance of LLMs in long-context scenarios due to the information loss introduced by eviction. In this paper, we propose a novel KV cache merging approach, called KVMerger, to achieve adaptive KV cache compression for long-context tasks without significant performance degradation under constrained memory budgets. Our approach is inspired by the intriguing observation that key states exhibit high similarity at the token level within a single sequence. To facilitate merging, we develop an effective yet straightforward merging set identification algorithm to identify suitable KV states for merging. Our merging set identification algorithm stimulates the second observation that KV cache sparsity, from similarity perspective, is independent of the dataset and remains persistent at the model level. Subsequently, we propose a Gaussian kernel weighted merging algorithm to selectively merge all states within each merging set. We conduct extensive experiments to demonstrate the effectiveness of KVMerger for long-context tasks under constrained memory budgets, applying it to models including Llama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll benchmarks, we compare our method with other KV cache compression techniques, including H2O and CaM, showing that our method achieves superior performance across tasks with both 50% and 35% KV cache budgets.</li>
<li><strong>摘要：</strong>如何有效地服务于大型语言模型 (LLM) 已成为一个紧迫的问题，因为它们在自回归生成过程中具有巨大的计算成本。为了降低计算成本，LLM 通常采用 KV 缓存技术来提高生成速度。在提高计算效率的同时，KV 缓存的存储要求也很高，尤其是在长上下文场景中，这会导致大量的内存消耗。现有的 KV 缓存驱逐方法通常会降低 LLM 在长上下文场景下的性能，因为驱逐会导致信息丢失。在本文中，我们提出了一种新颖的 KV 缓存合并方法，称为 KVMerger，以在受限的内存预算下实现自适应的长上下文任务的 KV 缓存压缩，而不会显著降低性能。我们的方法受到一个有趣的观察的启发，即关键状态在单个序列中的 token 级别表现出高度相似性。为了促进合并，我们开发了一种有效而直接的合并集识别算法来识别适合合并的 KV 状态。我们的合并集识别算法激发了第二个观察结果，即从相似性角度来看，KV 缓存稀疏性与数据集无关，并在模型级别保持不变。随后，我们提出了一种高斯核加权合并算法，以选择性地合并每个合并集内的所有状态。我们进行了大量实验，以证明 KVMerger 在内存预算受限的情况下对长上下文任务的有效性，并将其应用于包括 Llama2-7B-chat 和 Llama2-13B-chat 在内的模型。使用 LongBench 和 ZeroScroll 基准测试，我们将我们的方法与其他 KV 缓存压缩技术（包括 H2O 和 CaM）进行了比较，结果表明我们的方法在 KV 缓存预算为 50% 和 35% 的任务中均实现了卓越的性能。</li>
</ul>

<h3>Title: Investigating Public Fine-Tuning Datasets: A Complex Review of Current Practices from a Construction Perspective</h3>
<ul>
<li><strong>Authors: </strong>Runyuan Ma, Wei Li, Fukai Shang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08475">https://arxiv.org/abs/2407.08475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08475">https://arxiv.org/pdf/2407.08475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08475]] Investigating Public Fine-Tuning Datasets: A Complex Review of Current Practices from a Construction Perspective(https://arxiv.org/abs/2407.08475)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>With the rapid development of the large model domain, research related to fine-tuning has concurrently seen significant advancement, given that fine-tuning is a constituent part of the training process for large-scale models. Data engineering plays a fundamental role in the training process of models, which includes data infrastructure, data processing, etc. Data during fine-tuning likewise forms the base for large models. In order to embrace the power and explore new possibilities of fine-tuning datasets, this paper reviews current public fine-tuning datasets from the perspective of data construction. An overview of public fine-tuning datasets from two sides: evolution and taxonomy, is provided in this review, aiming to chart the development trajectory. Construction techniques and methods for public fine-tuning datasets of Large Language Models (LLMs), including data generation and data augmentation among others, are detailed. This elaboration follows the aforementioned taxonomy, specifically across demonstration, comparison, and generalist categories. Additionally, a category tree of data generation techniques has been abstracted in our review to assist researchers in gaining a deeper understanding of fine-tuning datasets from the construction dimension. Our review also summarizes the construction features in different data preparation phases of current practices in this field, aiming to provide a comprehensive overview and inform future research. Fine-tuning dataset practices, encompassing various data modalities, are also discussed from a construction perspective in our review. Towards the end of the article, we offer insights and considerations regarding the future construction and developments of fine-tuning datasets.</li>
<li><strong>摘要：</strong>随着大模型领域的快速发展，微调作为大型模型训练过程的一个重要组成部分，其相关研究也取得了长足的进步。数据工程在模型的训练过程中起着基础性的作用，包括数据基础设施、数据处理等。微调过程中的数据同样是大型模型的基础。为了充分利用微调数据集的强大功能并探索其新的可能性，本文从数据构建的角度回顾了当前的公共微调数据集。本文从演化和分类两个方面对公共微调数据集进行了概述，旨在描绘出其发展轨迹。本文详细介绍了大型语言模型 (LLM) 公共微调数据集的构建技术和方法，包括数据生成和数据增强等。本文的阐述遵循上述分类法，具体涵盖了演示、比较和通才类别。此外，我们在综述中抽象出了一个数据生成技术的分类树，以帮助研究人员从构建维度更深入地了解微调数据集。我们的综述还总结了该领域当前实践中不同数据准备阶段的构建特征，旨在提供全面的概述并为未来的研究提供参考。我们的综述还从构建的角度讨论了涵盖各种数据模式的微调数据集实践。在文章的最后，我们提供了有关微调数据集未来构建和发展的见解和考虑。</li>
</ul>

<h3>Title: Investigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024</h3>
<ul>
<li><strong>Authors: </strong>Ilias Chalkidis</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08495">https://arxiv.org/abs/2407.08495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08495">https://arxiv.org/pdf/2407.08495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08495]] Investigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024(https://arxiv.org/abs/2407.08495)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Instruction-finetuned Large Language Models exhibit unprecedented Natural Language Understanding capabilities. Recent work has been exploring political biases and political reasoning capabilities in LLMs, mainly scoped in the US context. In light of the recent 2024 European Parliament elections, we are investigating if LLMs can be used as Voting Advice Applications (VAAs). We audit MISTRAL and MIXTRAL models and evaluate their accuracy in predicting the stance of political parties based on the latest "EU and I" voting assistance questionnaire. Furthermore, we explore alternatives to improve models' performance by augmenting the input context via Retrieval-Augmented Generation (RAG) relying on web search, and Self-Reflection using staged conversations that aim to re-collect relevant content from the model's internal memory. We find that MIXTRAL is highly accurate with an 82% accuracy on average. Augmenting the input context with expert-curated information can lead to a significant boost of approx. 9%, which remains an open challenge for automated approaches.</li>
<li><strong>摘要：</strong>指令微调大型语言模型展现出前所未有的自然语言理解能力。最近的研究一直在探索 LLM 中的政治偏见和政治推理能力，主要在美国背景下进行。鉴于最近的 2024 年欧洲议会选举，我们正在调查 LLM 是否可以用作投票建议应用程序 (VAA)。我们审核了 MISTRAL 和 MIXTRAL 模型，并根据最新的“欧盟和我”投票援助问卷评估了它们在预测政党立场方面的准确性。此外，我们探索了通过依赖网络搜索的检索增强生成 (RAG) 来增强输入上下文以及使用分阶段对话进行自我反思来提高模型性能的替代方案，这些对话旨在从模型的内部记忆中重新收集相关内容。我们发现 MIXTRAL 的准确率很高，平均准确率为 82%。使用专家策划的信息增强输入上下文可以显着提高约 9%，这对自动化方法来说仍然是一个悬而未决的挑战。</li>
</ul>

<h3>Title: Autoregressive Speech Synthesis without Vector Quantization</h3>
<ul>
<li><strong>Authors: </strong>Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, Helen Meng, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08551">https://arxiv.org/abs/2407.08551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08551">https://arxiv.org/pdf/2407.08551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08551]] Autoregressive Speech Synthesis without Vector Quantization(https://arxiv.org/abs/2407.08551)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present MELLE, a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens. (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language models VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling discrete codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. See this https URL for demos of our work.</li>
<li><strong>摘要：</strong>我们提出了 MELLE，一种用于文本到语音合成 (TTS) 的新型基于连续值标记的语言建模方法。MELLE 直接从文本条件自回归生成连续梅尔频谱图帧，绕过了矢量量化的需要，矢量量化最初是为音频压缩而设计的，与梅尔频谱图相比，它牺牲了保真度。具体来说，(i) 我们应用回归损失和提出的频谱图通量损失函数来模拟连续值标记的概率分布，而不是交叉熵损失。(ii) 我们在 MELLE 中加入了变分推理，以促进采样机制，从而增强输出多样性和模型鲁棒性。实验表明，与两阶段编解码器语言模型 VALL-E 及其变体相比，单阶段 MELLE 通过避免采样离散代码的固有缺陷来缓解鲁棒性问题，在多个指标上实现了卓越的性能，最重要的是，提供了更精简的范例。请参阅此 https URL 以了解我们工作的演示。</li>
</ul>

<h3>Title: On the Universal Truthfulness Hyperplane Inside LLMs</h3>
<ul>
<li><strong>Authors: </strong>Junteng Liu, Shiqi Chen, Yu Cheng, Junxian He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08582">https://arxiv.org/abs/2407.08582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08582">https://arxiv.org/pdf/2407.08582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08582]] On the Universal Truthfulness Hyperplane Inside LLMs(https://arxiv.org/abs/2407.08582)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>While large language models (LLMs) have demonstrated remarkable abilities across various fields, hallucination remains a significant challenge. Recent studies have explored hallucinations through the lens of internal representations, proposing mechanisms to decipher LLMs' adherence to facts. However, these approaches often fail to generalize to out-of-distribution data, leading to concerns about whether internal representation patterns reflect fundamental factual awareness, or only overfit spurious correlations on the specific datasets. In this work, we investigate whether a universal truthfulness hyperplane that distinguishes the model's factually correct and incorrect outputs exists within the model. To this end, we scale up the number of training datasets and conduct an extensive evaluation -- we train the truthfulness hyperplane on a diverse collection of over 40 datasets and examine its cross-task, cross-domain, and in-domain generalization. Our results indicate that increasing the diversity of the training datasets significantly enhances the performance in all scenarios, while the volume of data samples plays a less critical role. This finding supports the optimistic hypothesis that a universal truthfulness hyperplane may indeed exist within the model, offering promising directions for future research.</li>
<li><strong>摘要：</strong>虽然大型语言模型 (LLM) 已在各个领域展现出非凡的能力，但幻觉仍然是一项重大挑战。最近的研究通过内部表征的视角探索了幻觉，提出了解读 LLM 对事实的坚持的机制。然而，这些方法往往无法推广到分布外的数据，导致人们担心内部表征模式是否反映了基本的事实意识，或者只是在特定数据集上过度拟合了虚假相关性。在这项工作中，我们调查了模型中是否存在一个通用的真实性超平面，可以区分模型的事实正确和不正确的输出。为此，我们扩大了训练数据集的数量并进行了广泛的评估——我们在超过 40 个数据集的多样化集合上训练真实性超平面，并检查其跨任务、跨域和域内泛化。我们的结果表明，增加训练数据集的多样性可以显著提高所有场景下的性能，而数据样本的数量则不那么重要。这一发现支持了乐观的假设，即模型中确实可能存在一个通用的真实性超平面，为未来的研究提供了有希望的方向。</li>
</ul>

<h3>Title: Turn-Level Empathy Prediction Using Psychological Indicators</h3>
<ul>
<li><strong>Authors: </strong>Shaz Furniturewala, Kokil Jaidka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08607">https://arxiv.org/abs/2407.08607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08607">https://arxiv.org/pdf/2407.08607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08607]] Turn-Level Empathy Prediction Using Psychological Indicators(https://arxiv.org/abs/2407.08607)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>For the WASSA 2024 Empathy and Personality Prediction Shared Task, we propose a novel turn-level empathy detection method that decomposes empathy into six psychological indicators: Emotional Language, Perspective-Taking, Sympathy and Compassion, Extroversion, Openness, and Agreeableness. A pipeline of text enrichment using a Large Language Model (LLM) followed by DeBERTA fine-tuning demonstrates a significant improvement in the Pearson Correlation Coefficient and F1 scores for empathy detection, highlighting the effectiveness of our approach. Our system officially ranked 7th at the CONV-turn track.</li>
<li><strong>摘要：</strong>对于 WASSA 2024 同理心和性格预测共享任务，我们提出了一种新颖的回合级同理心检测方法，将同理心分解为六个心理指标：情感语言、观点采择、同情和同理心、外向性、开放性和亲和性。使用大型语言模型 (LLM) 进行文本丰富，然后进行 DeBERTA 微调，结果表明，同理心检测的皮尔逊相关系数和 F1 分数显著提高，凸显了我们方法的有效性。我们的系统在 CONV-turn 赛道上正式排名第 7。</li>
</ul>

<h3>Title: Tamil Language Computing: the Present and the Future</h3>
<ul>
<li><strong>Authors: </strong>Kengatharaiyer Sarveswaran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08618">https://arxiv.org/abs/2407.08618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08618">https://arxiv.org/pdf/2407.08618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08618]] Tamil Language Computing: the Present and the Future(https://arxiv.org/abs/2407.08618)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper delves into the text processing aspects of Language Computing, which enables computers to understand, interpret, and generate human language. Focusing on tasks such as speech recognition, machine translation, sentiment analysis, text summarization, and language modelling, language computing integrates disciplines including linguistics, computer science, and cognitive psychology to create meaningful human-computer interactions. Recent advancements in deep learning have made computers more accessible and capable of independent learning and adaptation. In examining the landscape of language computing, the paper emphasises foundational work like encoding, where Tamil transitioned from ASCII to Unicode, enhancing digital communication. It discusses the development of computational resources, including raw data, dictionaries, glossaries, annotated data, and computational grammars, necessary for effective language processing. The challenges of linguistic annotation, the creation of treebanks, and the training of large language models are also covered, emphasising the need for high-quality, annotated data and advanced language models. The paper underscores the importance of building practical applications for languages like Tamil to address everyday communication needs, highlighting gaps in current technology. It calls for increased research collaboration, digitization of historical texts, and fostering digital usage to ensure the comprehensive development of Tamil language processing, ultimately enhancing global communication and access to digital services.</li>
<li><strong>摘要：</strong>本文深入探讨了语言计算的文本处理方面，语言计算使计算机能够理解、解释和生成人类语言。语言计算专注于语音识别、机器翻译、情感分析、文本摘要和语言建模等任务，整合了语言学、计算机科学和认知心理学等学科，以创建有意义的人机交互。深度学习的最新进展使计算机更易于访问，并具有独立学习和适应的能力。在研究语言计算领域时，本文强调了编码等基础工作，其中泰米尔语从 ASCII 过渡到 Unicode，增强了数字通信。它讨论了有效语言处理所必需的计算资源的开发，包括原始数据、词典、词汇表、带注释的数据和计算语法。还介绍了语言注释、树库的创建和大型语言模型的训练的挑战，强调了对高质量、带注释的数据和高级语言模型的需求。论文强调了为泰米尔语等语言开发实用应用程序以满足日常交流需求的重要性，并强调了当前技术的差距。论文呼吁加强研究合作、数字化历史文本并促进数字化使用，以确保泰米尔语处理的全面发展，最终增强全球通信和数字服务的可及性。</li>
</ul>

<h3>Title: Towards Building Specialized Generalist AI with System 1 and System 2 Fusion</h3>
<ul>
<li><strong>Authors: </strong>Kaiyan Zhang, Biqing Qi, Bowen Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08642">https://arxiv.org/abs/2407.08642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08642">https://arxiv.org/pdf/2407.08642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08642]] Towards Building Specialized Generalist AI with System 1 and System 2 Fusion(https://arxiv.org/abs/2407.08642)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>In this perspective paper, we introduce the concept of Specialized Generalist Artificial Intelligence (SGAI or simply SGI) as a crucial milestone toward Artificial General Intelligence (AGI). Compared to directly scaling general abilities, SGI is defined as AI that specializes in at least one task, surpassing human experts, while also retaining general abilities. This fusion path enables SGI to rapidly achieve high-value areas. We categorize SGI into three stages based on the level of mastery over professional skills and generality performance. Additionally, we discuss the necessity of SGI in addressing issues associated with large language models, such as their insufficient generality, specialized capabilities, uncertainty in innovation, and practical applications. Furthermore, we propose a conceptual framework for developing SGI that integrates the strengths of Systems 1 and 2 cognitive processing. This framework comprises three layers and four key components, which focus on enhancing individual abilities and facilitating collaborative evolution. We conclude by summarizing the potential challenges and suggesting future directions. We hope that the proposed SGI will provide insights into further research and applications towards achieving AGI.</li>
<li><strong>摘要：</strong>在这篇前瞻性论文中，我们引入了专业通用人工智能 (SGAI 或简称 SGI) 的概念，这是迈向通用人工智能 (AGI) 的关键里程碑。与直接扩展通用能力相比，SGI 被定义为专门从事至少一项任务的人工智能，超越人类专家，同时还保留通用能力。这种融合路径使 SGI 能够快速实现高价值领域。我们根据对专业技能和通用性表现的掌握程度将 SGI 分为三个阶段。此外，我们讨论了 SGI 在解决与大型语言模型相关的问题方面的必要性，例如它们的通用性不足、专业能力、创新的不确定性和实际应用。此外，我们提出了一个开发 SGI 的概念框架，该框架整合了系统 1 和 2 认知处理的优势。该框架包括三个层次和四个关键组件，重点是增强个人能力和促进协作发展。最后，我们总结了潜在的挑战并提出了未来的方向。我们希望提出的 SGI 能够为实现 AGI 的进一步研究和应用提供见解。</li>
</ul>

<h3>Title: Uncertainty Estimation of Large Language Models in Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Wu, Yizhou Yu, Hong-Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08662">https://arxiv.org/abs/2407.08662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08662">https://arxiv.org/pdf/2407.08662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08662]] Uncertainty Estimation of Large Language Models in Medical Question Answering(https://arxiv.org/abs/2407.08662)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, chat</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) show promise for natural language generation in healthcare, but risk hallucinating factually incorrect information. Deploying LLMs for medical question answering necessitates reliable uncertainty estimation (UE) methods to detect hallucinations. In this work, we benchmark popular UE methods with different model sizes on medical question-answering datasets. Our results show that current approaches generally perform poorly in this domain, highlighting the challenge of UE for medical applications. We also observe that larger models tend to yield better results, suggesting a correlation between model size and the reliability of UE. To address these challenges, we propose Two-phase Verification, a probability-free Uncertainty Estimation approach. First, an LLM generates a step-by-step explanation alongside its initial answer, followed by formulating verification questions to check the factual claims in the explanation. The model then answers these questions twice: first independently, and then referencing the explanation. Inconsistencies between the two sets of answers measure the uncertainty in the original response. We evaluate our approach on three biomedical question-answering datasets using Llama 2 Chat models and compare it against the benchmarked baseline methods. The results show that our Two-phase Verification method achieves the best overall accuracy and stability across various datasets and model sizes, and its performance scales as the model size increases.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 有望在医疗保健领域生成自然语言，但存在产生幻觉的事实错误信息的风险。部署 LLM 进行医学问答需要可靠的不确定性估计 (UE) 方法来检测幻觉。在这项工作中，我们在医学问答数据集上对具有不同模型大小的流行 UE 方法进行了基准测试。我们的结果表明，当前方法通常在这个领域表现不佳，凸显了 UE 在医疗应用中的挑战。我们还观察到，较大的模型往往会产生更好的结果，这表明模型大小与 UE 的可靠性之间存在相关性。为了应对这些挑战，我们提出了两阶段验证，一种无概率的不确定性估计方法。首先，LLM 在其初始答案旁边生成分步解释，然后制定验证问题以检查解释中的事实主张。然后，模型回答这些问题两次：第一次独立回答，然后引用解释。两组答案之间的不一致衡量了原始响应中的不确定性。我们使用 Llama 2 Chat 模型在三个生物医学问答数据集上评估了我们的方法，并将其与基准基线方法进行了比较。结果表明，我们的两阶段验证方法在各种数据集和模型大小中实现了最佳的整体准确率和稳定性，并且其性能随着模型大小的增加而扩展。</li>
</ul>

<h3>Title: GTA: A Benchmark for General Tool Agents</h3>
<ul>
<li><strong>Authors: </strong>Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, Xinyi Le</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08713">https://arxiv.org/abs/2407.08713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08713">https://arxiv.org/pdf/2407.08713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08713]] GTA: A Benchmark for General Tool Agents(https://arxiv.org/abs/2407.08713)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, agent</a></li>
<li><strong>Abstract: </strong>Significant focus has been placed on integrating large language models (LLMs) with various tools in developing general-purpose agents. This poses a challenge to LLMs' tool-use capabilities. However, there are evident gaps between existing tool-use evaluations and real-world scenarios. Current evaluations often use AI-generated queries, single-step tasks, dummy tools, and text-only interactions, failing to reveal the agents' real-world problem-solving abilities effectively. To address this, we propose GTA, a benchmark for General Tool Agents, featuring three main aspects: (i) Real user queries: human-written queries with simple real-world objectives but implicit tool-use, requiring the LLM to reason the suitable tools and plan the solution steps. (ii) Real deployed tools: an evaluation platform equipped with tools across perception, operation, logic, and creativity categories to evaluate the agents' actual task execution performance. (iii) Real multimodal inputs: authentic image files, such as spatial scenes, web page screenshots, tables, code snippets, and printed/handwritten materials, used as the query contexts to align with real-world scenarios closely. We design 229 real-world tasks and executable tool chains to evaluate mainstream LLMs. Our findings show that real-world user queries are challenging for existing LLMs, with GPT-4 completing less than 50% of the tasks and most LLMs achieving below 25%. This evaluation reveals the bottlenecks in the tool-use capabilities of current LLMs in real-world scenarios, which provides future direction for advancing general-purpose tool agents. The code and dataset are available at this https URL.</li>
<li><strong>摘要：</strong>在开发通用代理时，人们非常重视将大型语言模型 (LLM) 与各种工具相结合。这对 LLM 的工具使用能力提出了挑战。然而，现有的工具使用评估与真实世界场景之间存在明显差距。目前的评估通常使用人工智能生成的查询、单步任务、虚拟工具和纯文本交互，无法有效揭示代理在现实世界中解决问题的能力。为了解决这个问题，我们提出了通用工具代理的基准 GTA，它具有三个主要方面：(i) 真实用户查询：人类编写的查询具有简单的真实世界目标，但隐含了工具使用，需要 LLM 推理合适的工具并规划解决步骤。(ii) 真实部署的工具：一个评估平台，配备了涵盖感知、操作、逻辑和创造力类别的工具，以评估代理的实际任务执行性能。 (iii) 真实的多模态输入：真实的图像文件（例如空间场景、网页截图、表格、代码片段和打印/手写材料）用作查询上下文，以与真实场景紧密结合。我们设计了 229 个真实世界任务和可执行工具链来评估主流 LLM。我们的研究结果表明，真实世界的用户查询对现有的 LLM 来说具有挑战性，GPT-4 完成了不到 50% 的任务，大多数 LLM 的完成率低于 25%。这次评估揭示了当前 LLM 在现实场景中工具使用能力的瓶颈，为推进通用工具代理提供了未来的方向。代码和数据集可在此 https URL 上找到。</li>
</ul>

<h3>Title: A Taxonomy for Data Contamination in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Medha Palavalli, Amanda Bertsch, Matthew R. Gormley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08716">https://arxiv.org/abs/2407.08716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08716">https://arxiv.org/pdf/2407.08716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08716]] A Taxonomy for Data Contamination in Large Language Models(https://arxiv.org/abs/2407.08716)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models pretrained on extensive web corpora demonstrate remarkable performance across a wide range of downstream tasks. However, a growing concern is data contamination, where evaluation datasets may be contained in the pretraining corpus, inflating model performance. Decontamination, the process of detecting and removing such data, is a potential solution; yet these contaminants may originate from altered versions of the test set, evading detection during decontamination. How different types of contamination impact the performance of language models on downstream tasks is not fully understood. We present a taxonomy that categorizes the various types of contamination encountered by LLMs during the pretraining phase and identify which types pose the highest risk. We analyze the impact of contamination on two key NLP tasks -- summarization and question answering -- revealing how different types of contamination influence task performance during evaluation.</li>
<li><strong>摘要：</strong>在大量网络语料库上进行预训练的大型语言模型在广泛的下游任务中表现出色。然而，数据污染问题日益令人担忧，评估数据集可能包含在预训练语料库中，从而夸大了模型性能。净化，即检测和删除此类数据的过程，是一种潜在的解决方案；但这些污染物可能源自测试集的更改版本，从而逃避净化过程中的检测。不同类型的污染如何影响语言模型在下游任务上的性能尚不完全清楚。我们提出了一个分类法，对 LLM 在预训练阶段遇到的各种类型的污染进行分类，并确定哪些类型的风险最高。我们分析了污染对两个关键 NLP 任务——总结和问答——的影响，揭示了不同类型的污染如何影响评估过程中的任务性能。</li>
</ul>

<h3>Title: Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F. Wong, Xiaowei Huang, Qiufeng Wang, Kaizhu Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2407.08733">https://arxiv.org/abs/2407.08733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2407.08733">https://arxiv.org/pdf/2407.08733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2407.08733]] Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist(https://arxiv.org/abs/2407.08733)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Exceptional mathematical reasoning ability is one of the key features that demonstrate the power of large language models (LLMs). How to comprehensively define and evaluate the mathematical abilities of LLMs, and even reflect the user experience in real-world scenarios, has emerged as a critical issue. Current benchmarks predominantly concentrate on problem-solving capabilities, which presents a substantial risk of model overfitting and fails to accurately represent genuine mathematical reasoning abilities. In this paper, we argue that if a model really understands a problem, it should be robustly and readily applied across a diverse array of tasks. Motivated by this, we introduce MATHCHECK, a well-designed checklist for testing task generalization and reasoning robustness, as well as an automatic tool to generate checklists efficiently. MATHCHECK includes multiple mathematical reasoning tasks and robustness test types to facilitate a comprehensive evaluation of both mathematical reasoning ability and behavior testing. Utilizing MATHCHECK, we develop MATHCHECK-GSM and MATHCHECK-GEO to assess mathematical textual reasoning and multi-modal reasoning capabilities, respectively, serving as upgraded versions of benchmarks including GSM8k, GeoQA, UniGeo, and Geometry3K. We adopt MATHCHECK-GSM and MATHCHECK-GEO to evaluate over 20 LLMs and 11 MLLMs, assessing their comprehensive mathematical reasoning abilities. Our results demonstrate that while frontier LLMs like GPT-4o continue to excel in various abilities on the checklist, many other model families exhibit a significant decline. Further experiments indicate that, compared to traditional math benchmarks, MATHCHECK better reflects true mathematical abilities and represents mathematical intelligence more linearly, thereby supporting our design. On our MATHCHECK, we can easily conduct detailed behavior analysis to deeply investigate models.</li>
<li><strong>摘要：</strong>卓越的数学推理能力是大型语言模型（LLM）的关键特征之一。如何全面定义和评估LLM的数学能力，甚至反映其在现实场景中的用户体验，已成为一个关键问题。当前的基准测试主要集中在解决问题的能力上，这会带来很大的模型过拟合风险，并且无法准确表示真正的数学推理能力。在本文中，我们认为，如果一个模型真正理解了一个问题，那么它应该能够稳健且轻松地应用于各种各样的任务。受此启发，我们推出了MATHCHECK，这是一个精心设计的用于测试任务泛化和推理稳健性的检查表，也是一个高效生成检查表的自动工具。MATHCHECK包括多个数学推理任务和稳健性测试类型，以便于对数学推理能力和行为测试进行全面评估。利用 MATHCHECK，我们开发了 MATHCHECK-GSM 和 MATHCHECK-GEO，分别用于评估数学文本推理和多模态推理能力，作为 GSM8k、GeoQA、UniGeo 和 Geometry3K 等基准的升级版。我们采用 MATHCHECK-GSM 和 MATHCHECK-GEO 评估了 20 多个 LLM 和 11 个 MLLM，评估了它们的综合数学推理能力。我们的结果表明，虽然 GPT-4o 等前沿 LLM 在清单上的各项能力上继续表现出色，但许多其他模型系列却出现了显著的下滑。进一步的实验表明，与传统的数学基准相比，MATHCHECK 更能反映真实的数学能力，更线性地表示数学智能，从而支持我们的设计。在我们的 MATHCHECK 上，我们可以轻松地进行详细的行为分析，以深入研究模型。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
