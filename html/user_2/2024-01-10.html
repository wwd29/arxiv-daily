<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-10</h1>
<h2>language model</h2>
<h3>Title: Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning. (arXiv:2401.04361v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04361">http://arxiv.org/abs/2401.04361</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04361]] Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning(http://arxiv.org/abs/2401.04361)</code></li>
<li>Summary: <p>Knowledge-grounded dialogue (KGD) learns to generate an informative response
based on a given dialogue context and external knowledge (\emph{e.g.},
knowledge graphs; KGs). Recently, the emergence of large language models (LLMs)
and pre-training techniques has brought great success to knowledge-grounded
dialogue. However, when building KGD systems in real applications, there are
various real-world noises that are inevitable to face. For example, the
dialogue context might involve perturbations such as misspellings and
abbreviations. In addition, KGs typically suffer from incompletion and also
might contain erroneous and outdated facts. Such real-world noises pose a
challenge to the robustness of KGD systems and hinder their applications in the
real world. In this paper, we propose an entity-based contrastive learning
framework for improving the robustness of KGD. Specifically, we make use of the
entity information in a KGD sample to create both its positive and negative
samples which involve semantic-irrelevant and semantic-relevant perturbations,
respectively. The contrastive learning framework ensures the KGD model is aware
of these two types of perturbations, thus generating informative responses with
the potentially noisy inputs in real applications. Experimental results on
three benchmark datasets show that our method achieves new state-of-the-art
performance in terms of automatic evaluation scores, verifying its
effectiveness and potentiality. Furthermore, we show that our method can
generate better responses than comparison models in both the noisy and the
few-shot settings.
</p></li>
</ul>

<h3>Title: TechGPT-2.0: A large language model project to solve the task of knowledge graph construction. (arXiv:2401.04507v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04507">http://arxiv.org/abs/2401.04507</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04507]] TechGPT-2(http://arxiv.org/abs/2401.04507)</code></li>
<li>Summary: <p>Large language models have exhibited robust performance across diverse
natural language processing tasks. This report introduces TechGPT-2.0, a
project designed to enhance the capabilities of large language models
specifically in knowledge graph construction tasks, including named entity
recognition (NER) and relationship triple extraction (RTE) tasks in NLP
applications. Additionally, it serves as a LLM accessible for research within
the Chinese open-source model community. We offer two 7B large language model
weights and a QLoRA weight specialized for processing lengthy texts.Notably,
TechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all
functionalities from TechGPT-1.0, it exhibits robust text processing
capabilities, particularly in the domains of medicine and law. Furthermore, we
introduce new capabilities to the model, enabling it to process texts in
various domains such as geographical areas, transportation, organizations,
literary works, biology, natural sciences, astronomical objects, and
architecture. These enhancements also fortified the model's adeptness in
handling hallucinations, unanswerable queries, and lengthy texts. This report
provides a comprehensive and detailed introduction to the full fine-tuning
process on Huawei's Ascend servers, encompassing experiences in Ascend server
debugging, instruction fine-tuning data processing, and model training. Our
code is available at https://github.com/neukg/TechGPT-2.0
</p></li>
</ul>

<h3>Title: Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models. (arXiv:2401.04515v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04515">http://arxiv.org/abs/2401.04515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04515]] Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models(http://arxiv.org/abs/2401.04515)</code></li>
<li>Summary: <p>This article investigates a zero-shot approach to hypernymy prediction using
large language models (LLMs). The study employs a method based on text
probability calculation, applying it to various generated prompts. The
experiments demonstrate a strong correlation between the effectiveness of
language model prompts and classic patterns, indicating that preliminary prompt
selection can be carried out using smaller models before moving to larger ones.
We also explore prompts for predicting co-hyponyms and improving hypernymy
predictions by augmenting prompts with additional information through
automatically identified co-hyponyms. An iterative approach is developed for
predicting higher-level concepts, which further improves the quality on the
BLESS dataset (MAP = 0.8).
</p></li>
</ul>

<h3>Title: The Critique of Critique. (arXiv:2401.04518v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04518">http://arxiv.org/abs/2401.04518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04518]] The Critique of Critique(http://arxiv.org/abs/2401.04518)</code></li>
<li>Summary: <p>Critique, as a natural language description for assessing the quality of
model-generated content, has been proven to play an essential role in the
training, evaluation, and refinement of Large Language Models (LLMs). However,
there is a lack of principled understanding in evaluating the quality of the
critique itself. In this paper, we pioneer the critique of critique, termed
MetaCritique, which is a framework to evaluate the critique from two aspects,
i.e., factuality as precision score and comprehensiveness as recall score. We
calculate the harmonic mean of precision and recall as the overall rating
called F1 score. To obtain a reliable evaluation outcome, we propose Atomic
Information Units (AIUs), which describe the critique in a more fine-grained
manner. MetaCritique takes each AIU into account and aggregates each AIU's
judgment for the overall score. Moreover, given the evaluation process involves
intricate reasoning, our MetaCritique provides a natural language rationale to
support each judgment. We construct a meta-evaluation dataset containing 300
critiques (2653 AIUs) across four tasks (question answering, reasoning,
entailment, and summarization), and we conduct a comparative study to
demonstrate the feasibility and effectiveness. Experiments also show superior
critique judged by MetaCritique leads to better refinement, indicating
generative artificial intelligence indeed has the potential to be significantly
advanced with our MetaCritique. We will release relevant code and
meta-evaluation datasets at https://github.com/GAIR-NLP/MetaCritique.
</p></li>
</ul>

<h3>Title: Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04536">http://arxiv.org/abs/2401.04536</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04536]] Evaluating Language Model Agency through Negotiations(http://arxiv.org/abs/2401.04536)</code></li>
<li>Summary: <p>Companies, organizations, and governments increasingly exploit Language
Models' (LM) remarkable capability to display agent-like behavior. As LMs are
adopted to perform tasks with growing autonomy, there exists an urgent need for
reliable and scalable evaluation benchmarks. Current, predominantly static LM
benchmarks are ill-suited to evaluate such dynamic applications. Thus, we
propose jointly evaluating LM performance and alignment through the lenses of
negotiation games. We argue that this common task better reflects real-world
deployment conditions while offering insights into LMs' decision-making
processes. Crucially, negotiation games allow us to study multi-turn, and
cross-model interactions, modulate complexity, and side-step accidental data
leakage in evaluation. We report results for six publicly accessible LMs from
several major providers on a variety of negotiation games, evaluating both
self-play and cross-play performance. Noteworthy findings include: (i)
open-source models are currently unable to complete these tasks; (ii)
cooperative bargaining games prove challenging; and (iii) the most powerful
models do not always "win".
</p></li>
</ul>

<h3>Title: Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. (arXiv:2401.04658v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04658">http://arxiv.org/abs/2401.04658</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04658]] Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models(http://arxiv.org/abs/2401.04658)</code></li>
<li>Summary: <p>Linear attention is an efficient attention mechanism that has recently
emerged as a promising alternative to conventional softmax attention. With its
ability to process tokens in linear computational complexities, linear
attention, in theory, can handle sequences of unlimited length without
sacrificing speed, i.e., maintaining a constant training speed for various
sequence lengths with a fixed memory consumption. However, due to the issue
with cumulative summation (cumsum), current linear attention algorithms cannot
demonstrate their theoretical advantage in a causal setting. In this paper, we
present Lightning Attention-2, the first linear attention implementation that
enables linear attention to realize its theoretical computational benefits. To
achieve this, we leverage the thought of tiling, separately handling the
intra-block and inter-block components in linear attention calculation.
Specifically, we utilize the conventional attention computation mechanism for
the intra-blocks and apply linear attention kernel tricks for the inter-blocks.
A tiling technique is adopted through both forward and backward procedures to
take full advantage of the GPU hardware. We implement our algorithm in Triton
to make it IO-aware and hardware-friendly. Various experiments are conducted on
different model sizes and sequence lengths. Lightning Attention-2 retains
consistent training and inference speed regardless of input sequence length and
is significantly faster than other attention mechanisms. The source code is
available at https://github.com/OpenNLPLab/lightning-attention.
</p></li>
</ul>

<h3>Title: RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04679">http://arxiv.org/abs/2401.04679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04679]] RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation(http://arxiv.org/abs/2401.04679)</code></li>
<li>Summary: <p>We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
https://github.com/IST-DASLab/RoSA}{\texttt{https://github.com/IST-DASLab/RoSA
</p></li>
</ul>

<h3>Title: Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning. (arXiv:2401.04151v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04151">http://arxiv.org/abs/2401.04151</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04151]] Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning(http://arxiv.org/abs/2401.04151)</code></li>
<li>Summary: <p>Fine-tuning is the primary methodology for tailoring pre-trained large
language models to specific tasks. As the model's scale and the diversity of
tasks expand, parameter-efficient fine-tuning methods are of paramount
importance. One of the most widely used family of methods is low-rank
adaptation (LoRA) and its variants. LoRA encodes weight update as the product
of two low-rank matrices. Despite its advantages, LoRA falls short of
full-parameter fine-tuning in terms of generalization error for certain tasks.
</p>
<p>We introduce Chain of LoRA (COLA), an iterative optimization framework
inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full
parameter fine-tuning, without incurring additional computational costs or
memory overheads. COLA employs a residual learning procedure where it merges
learned LoRA modules into the pre-trained language model parameters and
re-initilize optimization for new born LoRA modules. We provide theoretical
convergence guarantees as well as empirical results to validate the
effectiveness of our algorithm. Across various models (OPT and llama-2) and
seven benchmarking tasks, we demonstrate that COLA can consistently outperform
LoRA without additional computational or memory costs.
</p></li>
</ul>

<h3>Title: Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?. (arXiv:2401.04218v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04218">http://arxiv.org/abs/2401.04218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04218]] Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?(http://arxiv.org/abs/2401.04218)</code></li>
<li>Summary: <p>We present a benchmark for assessing the capability of Large Language Models
(LLMs) to discern intercardinal directions between geographic locations and
apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark
specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar
to humans, where judgments about individual locations' spatial relationships
are influenced by the perceived relationships of the larger groups that contain
them. To investigate this, we formulated 14 questions focusing on well-known
American cities. Seven questions were designed to challenge the LLMs with
scenarios potentially influenced by the orientation of larger geographical
units, such as states or countries, while the remaining seven targeted
locations less susceptible to such hierarchical categorization. Among the
tested models, GPT-4 exhibited superior performance with 55.3% accuracy,
followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed
significantly reduced accuracy on tasks with suspected hierarchical bias. For
example, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on
others. Despite these inaccuracies, the models identified the nearest cardinal
direction in most cases, suggesting associative learning, embodying human-like
misconceptions. We discuss the potential of text-based data representing
geographic relationships directly to improve the spatial reasoning capabilities
of LLMs.
</p></li>
</ul>

<h3>Title: Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04343">http://arxiv.org/abs/2401.04343</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04343]] Private Fine-tuning of Large Language Models with Zeroth-order Optimization(http://arxiv.org/abs/2401.04343)</code></li>
<li>Summary: <p>Fine-tuning large pretrained models on private datasets may run the risk of
violating privacy. Differential privacy is a framework for mitigating privacy
risks by enforcing algorithmic stability. DP-SGD enables training models with
private data in a privacy-preserving manner, but raises new obstacles in the
form of performance loss and significant engineering challenges. We introduce
DP-ZO, a new method for fine-tuning large language models that preserves the
privacy of training data by privatizing zeroth-order optimization. A key
insight into the design of our method is that the direction of the gradient in
SPSA, the zeroth-order algorithm we use, is always random and the only
information that depends on private data is the step size, i.e., a scalar.
Therefore, we only need to privatize the scalar step size, which is
memory-efficient. DP-ZO, which can be instantiated with either Laplace or
Gaussian noise, provides a strong privacy-utility trade-off across different
tasks, and model sizes, under conservative privacy budgets. One noteworthy
result is that DP-ZO exhibits just $1.86\%$ performance degradation due to
privacy at $(1,10^{-5})$-DP when fine-tuning OPT-66B on 1000 training samples
from SQuAD.
</p></li>
</ul>

<h3>Title: Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding. (arXiv:2401.04398v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04398">http://arxiv.org/abs/2401.04398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04398]] Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding(http://arxiv.org/abs/2401.04398)</code></li>
<li>Summary: <p>Table-based reasoning with large language models (LLMs) is a promising
direction to tackle many table understanding tasks, such as table-based
question answering and fact verification. Compared with generic reasoning,
table-based reasoning requires the extraction of underlying semantics from both
free-form questions and semi-structured tabular data. Chain-of-Thought and its
similar approaches incorporate the reasoning chain in the form of textual
context, but it is still an open question how to effectively leverage tabular
data in the reasoning chain. We propose the Chain-of-Table framework, where
tabular data is explicitly used in the reasoning chain as a proxy for
intermediate thoughts. Specifically, we guide LLMs using in-context learning to
iteratively generate operations and update the table to represent a tabular
reasoning chain. LLMs can therefore dynamically plan the next operation based
on the results of the previous ones. This continuous evolution of the table
forms a chain, showing the reasoning process for a given tabular problem. The
chain carries structured information of the intermediate results, enabling more
accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art
performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM
choices.
</p></li>
</ul>

<h3>Title: TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models. (arXiv:2401.04471v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04471">http://arxiv.org/abs/2401.04471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04471]] TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models(http://arxiv.org/abs/2401.04471)</code></li>
<li>Summary: <p>Large language models (LLMs) and multimodal large language models (MLLMs)
have shown excellent general capabilities, even exhibiting adaptability in many
professional domains such as law, economics, transportation, and medicine.
Currently, many domain-specific benchmarks have been proposed to verify the
performance of (M)LLMs in specific fields. Among various domains,
transportation plays a crucial role in modern society as it impacts the
economy, the environment, and the quality of life for billions of people.
However, it is unclear how much traffic knowledge (M)LLMs possess and whether
they can reliably perform transportation-related tasks. To address this gap, we
propose TransportationGames, a carefully designed and thorough evaluation
benchmark for assessing (M)LLMs in the transportation domain. By
comprehensively considering the applications in real-world scenarios and
referring to the first three levels in Bloom's Taxonomy, we test the
performance of various (M)LLMs in memorizing, understanding, and applying
transportation knowledge by the selected tasks. The experimental results show
that although some models perform well in some tasks, there is still much room
for improvement overall. We hope the release of TransportationGames can serve
as a foundation for future research, thereby accelerating the implementation
and application of (M)LLMs in the transportation domain.
</p></li>
</ul>

<h3>Title: LUNA: A Framework for Language Understanding and Naturalness Assessment. (arXiv:2401.04522v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04522">http://arxiv.org/abs/2401.04522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04522]] LUNA: A Framework for Language Understanding and Naturalness Assessment(http://arxiv.org/abs/2401.04522)</code></li>
<li>Summary: <p>The evaluation of Natural Language Generation (NLG) models has gained
increased attention, urging the development of metrics that evaluate various
aspects of generated text. LUNA addresses this challenge by introducing a
unified interface for 20 NLG evaluation metrics. These metrics are categorized
based on their reference-dependence and the type of text representation they
employ, from string-based n-gram overlap to the utilization of static
embeddings and pre-trained language models.
</p>
<p>The straightforward design of LUNA allows for easy extension with novel
metrics, requiring just a few lines of code. LUNA offers a user-friendly tool
for evaluating generated texts.
</p></li>
</ul>

<h3>Title: An Assessment on Comprehending Mental Health through Large Language Models. (arXiv:2401.04592v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04592">http://arxiv.org/abs/2401.04592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04592]] An Assessment on Comprehending Mental Health through Large Language Models(http://arxiv.org/abs/2401.04592)</code></li>
<li>Summary: <p>Mental health challenges pose considerable global burdens on individuals and
communities. Recent data indicates that more than 20% of adults may encounter
at least one mental disorder in their lifetime. On the one hand, the
advancements in large language models have facilitated diverse applications,
yet a significant research gap persists in understanding and enhancing the
potential of large language models within the domain of mental health. On the
other hand, across various applications, an outstanding question involves the
capacity of large language models to comprehend expressions of human mental
health conditions in natural language. This study presents an initial
evaluation of large language models in addressing this gap. Due to this, we
compare the performance of Llama-2 and ChatGPT with classical Machine as well
as Deep learning models. Our results on the DAIC-WOZ dataset show that
transformer-based models, like BERT or XLNet, outperform the large language
models.
</p></li>
</ul>

<h3>Title: Language Detection for Transliterated Content. (arXiv:2401.04619v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04619">http://arxiv.org/abs/2401.04619</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04619]] Language Detection for Transliterated Content(http://arxiv.org/abs/2401.04619)</code></li>
<li>Summary: <p>In the contemporary digital era, the Internet functions as an unparalleled
catalyst, dismantling geographical and linguistic barriers particularly evident
in texting. This evolution facilitates global communication, transcending
physical distances and fostering dynamic cultural exchange. A notable trend is
the widespread use of transliteration, where the English alphabet is employed
to convey messages in native languages, posing a unique challenge for language
technology in accurately detecting the source language. This paper addresses
this challenge through a dataset of phone text messages in Hindi and Russian
transliterated into English utilizing BERT for language classification and
Google Translate API for transliteration conversion. The research pioneers
innovative approaches to identify and convert transliterated text, navigating
challenges in the diverse linguistic landscape of digital communication.
Emphasizing the pivotal role of comprehensive datasets for training Large
Language Models LLMs like BERT, our model showcases exceptional proficiency in
accurately identifying and classifying languages from transliterated text. With
a validation accuracy of 99% our models robust performance underscores its
reliability. The comprehensive exploration of transliteration dynamics
supported by innovative approaches and cutting edge technologies like BERT,
positions our research at the forefront of addressing unique challenges in the
linguistic landscape of digital communication. Beyond contributing to language
identification and transliteration capabilities this work holds promise for
applications in content moderation, analytics and fostering a globally
connected community engaged in meaningful dialogue.
</p></li>
</ul>

<h3>Title: Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers. (arXiv:2401.04695v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04695">http://arxiv.org/abs/2401.04695</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04695]] Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers(http://arxiv.org/abs/2401.04695)</code></li>
<li>Summary: <p>Factual questions typically can be answered correctly at different levels of
granularity. For example, both ``August 4, 1961'' and ``1961'' are correct
answers to the question ``When was Barack Obama born?''. Standard question
answering (QA) evaluation protocols, however, do not explicitly take this into
account and compare a predicted answer against answers of a single granularity
level. In this work, we propose GRANOLA QA, a novel evaluation setting where a
predicted answer is evaluated in terms of accuracy and informativeness against
a set of multi-granularity answers. We present a simple methodology for
enriching existing datasets with multi-granularity answers, and create
GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We
evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm,
called Decoding with Response Aggregation (DRAG), that is geared towards
aligning the response granularity with the model's uncertainty. Our experiments
show that large language models with standard decoding tend to generate
specific answers, which are often incorrect. In contrast, when evaluated on
multi-granularity answers, DRAG yields a nearly 20 point increase in accuracy
on average, which further increases for rare entities. Overall, this reveals
that standard evaluation and decoding schemes may significantly underestimate
the knowledge encapsulated in LMs.
</p></li>
</ul>

<h3>Title: Model Editing Can Hurt General Abilities of Large Language Models. (arXiv:2401.04700v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04700">http://arxiv.org/abs/2401.04700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04700]] Model Editing Can Hurt General Abilities of Large Language Models(http://arxiv.org/abs/2401.04700)</code></li>
<li>Summary: <p>Recent advances in large language models (LLMs) have opened up new paradigms
for accessing the knowledge stored in their parameters. One critical challenge
that has emerged is the presence of hallucinations in LLM outputs due to false
or outdated knowledge. Since retraining LLMs with updated information is
resource-intensive, there has been a growing interest in model editing.
However, many model editing methods, while effective in various scenarios, tend
to overemphasize aspects such as efficacy, generalization, and locality in
editing performance, often overlooking potential side effects on the general
abilities of LLMs. In this paper, we raise concerns that the improvement of
model factuality may come at the cost of a significant degradation of these
general abilities, which is not conducive to the sustainable development of
LLMs. Systematically, we analyze side effects by evaluating four popular
editing methods on two LLMs across eight representative task categories.
Extensive empirical research reveals that model editing does improve model
factuality but at the expense of substantially impairing general abilities.
Therefore, we advocate for more research efforts to minimize the loss of
general abilities acquired during LLM pre-training and to ultimately preserve
them during model editing.
</p></li>
</ul>

<h2>gpt</h2>
<h3>Title: DepressionEmo: A novel dataset for multilabel classification of depression emotions. (arXiv:2401.04655v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04655">http://arxiv.org/abs/2401.04655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04655]] DepressionEmo: A novel dataset for multilabel classification of depression emotions(http://arxiv.org/abs/2401.04655)</code></li>
<li>Summary: <p>Emotions are integral to human social interactions, with diverse responses
elicited by various situational contexts. Particularly, the prevalence of
negative emotional states has been correlated with negative outcomes for mental
health, necessitating a comprehensive analysis of their occurrence and impact
on individuals. In this paper, we introduce a novel dataset named DepressionEmo
designed to detect 8 emotions associated with depression by 6037 examples of
long Reddit user posts. This dataset was created through a majority vote over
inputs by zero-shot classifications from pre-trained models and validating the
quality by annotators and ChatGPT, exhibiting an acceptable level of interrater
reliability between annotators. The correlation between emotions, their
distribution over time, and linguistic analysis are conducted on DepressionEmo.
Besides, we provide several text classification methods classified into two
groups: machine learning methods such as SVM, XGBoost, and Light GBM; and deep
learning methods such as BERT, GAN-BERT, and BART. The pretrained BART model,
bart-base allows us to obtain the highest F1- Macro of 0.76, showing its
outperformance compared to other methods evaluated in our analysis. Across all
emotions, the highest F1-Macro value is achieved by suicide intent, indicating
a certain value of our dataset in identifying emotions in individuals with
depression symptoms through text analysis. The curated dataset is publicly
available at: https://github.com/abuBakarSiddiqurRahman/DepressionEmo.
</p></li>
</ul>

<h2>llm</h2>
<h3>Title: Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04319">http://arxiv.org/abs/2401.04319</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04319]] Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs(http://arxiv.org/abs/2401.04319)</code></li>
<li>Summary: <p>In this paper, we explore a new way for user targeting, where non-expert
marketers could select their target users solely given demands in natural
language form. The key to this issue is how to transform natural languages into
practical structured logical languages, i.e., the structured understanding of
marketer demands. Considering the impressive natural language processing
ability of large language models (LLMs), we try to leverage LLMs to solve this
issue. Past research indicates that the reasoning ability of LLMs can be
effectively enhanced through chain-of-thought (CoT) prompting. But existing
methods still have some limitations: (1) Previous methods either use simple
"Let's think step by step" spells or provide fixed examples in demonstrations
without considering compatibility between prompts and questions, making LLMs
ineffective in some complex reasoning tasks such as structured language
transformation. (2) Previous methods are often implemented in closed-source
models or excessively large models, which is not suitable in industrial
practical scenarios. Based on these, we propose ARALLM (i.e., Analogical
Reasoning Augmented Large Language Models) consisting of two modules:
Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model
Distillation.
</p></li>
</ul>

<h3>Title: MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04531">http://arxiv.org/abs/2401.04531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04531]] MERA: A Comprehensive LLM Evaluation in Russian(http://arxiv.org/abs/2401.04531)</code></li>
<li>Summary: <p>Over the past few years, one of the most notable advancements in AI research
has been in foundation models (FMs), headlined by the rise of language models
(LMs). As the models' size increases, LMs demonstrate enhancements in
measurable aspects and the development of new qualitative features. However,
despite researchers' attention and the rapid growth in LM application, the
capabilities, limitations, and associated risks still need to be better
understood. To address these issues, we introduce an open Multimodal Evaluation
of Russian-language Architectures (MERA), a new instruction benchmark for
evaluating foundation models oriented towards the Russian language. The
benchmark encompasses 21 evaluation tasks for generative models in 11 skill
domains and is designed as a black-box test to ensure the exclusion of data
leakage. The paper introduces a methodology to evaluate FMs and LMs in zero-
and few-shot fixed instruction settings that can be extended to other
modalities. We propose an evaluation methodology, an open-source code base for
the MERA assessment, and a leaderboard with a submission system. We evaluate
open LMs as baselines and find that they are still far behind the human level.
We publicly release MERA to guide forthcoming research, anticipate
groundbreaking model features, standardize the evaluation procedure, and
address potential societal drawbacks.
</p></li>
</ul>

<h2>long context</h2>
<h2>lora</h2>
<h3>Title: Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04130">http://arxiv.org/abs/2401.04130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04130]] Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules(http://arxiv.org/abs/2401.04130)</code></li>
<li>Summary: <p>Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual
Prompt Tuning (VPT) have found success in enabling adaptation to new domains by
tuning small modules within a transformer model. However, the number of domains
encountered during test time can be very large, and the data is usually
unlabeled. Thus, adaptation to new domains is challenging; it is also
impractical to generate customized tuned modules for each such domain. Toward
addressing these challenges, this work introduces PLUTO: a Plug-and-pLay
modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of
modules, each specialized for different source domains, effectively creating a
``module store''. Given a target domain with few-shot unlabeled data, we
introduce an unsupervised test-time adaptation (TTA) method to (1) select a
sparse subset of relevant modules from this store and (2) create a weighted
combination of selected modules without tuning their weights. This
plug-and-play nature enables us to harness multiple most-relevant source
domains in a single inference call. Comprehensive evaluations demonstrate that
PLUTO uniformly outperforms alternative TTA methods and that selecting $\leq$5
modules suffice to extract most of the benefit. At a high level, our method
equips pre-trained transformers with the capability to dynamically adapt to new
domains, motivating a new paradigm for efficient and scalable domain
adaptation.
</p></li>
</ul>

<h3>Title: Curiosity & Entropy Driven Unsupervised RL in Multiple Environments. (arXiv:2401.04198v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04198">http://arxiv.org/abs/2401.04198</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04198]] Curiosity & Entropy Driven Unsupervised RL in Multiple Environments(http://arxiv.org/abs/2401.04198)</code></li>
<li>Summary: <p>The authors of 'Unsupervised Reinforcement Learning in Multiple environments'
propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple
environments. They pre-train a task-agnostic exploration policy using
interactions from an entire environment class and then fine-tune this policy
for various tasks using supervision. We expanded upon this work, with the goal
of improving performance. We primarily propose and experiment with five new
modifications to the original work: sampling trajectories using an
entropy-based probability distribution, dynamic alpha, higher KL Divergence
threshold, curiosity-driven exploration, and alpha-percentile sampling on
curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a
significant improvement over the baseline from the earlier work. PDF-sampling
failed to provide any improvement due to it being approximately equivalent to
the baseline method when the sample space is small. In high-dimensional
environments, the addition of curiosity-driven exploration enhances learning by
encouraging the agent to seek diverse experiences and explore the unknown more.
However, its benefits are limited in low-dimensional and simpler environments
where exploration possibilities are constrained and there is little that is
truly unknown to the agent. Overall, some of our experiments did boost
performance over the baseline and there are a few directions that seem
promising for further research.
</p></li>
</ul>

<h2>hallucination</h2>
<h2>prompt</h2>
<h3>Title: Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset. (arXiv:2401.04481v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04481">http://arxiv.org/abs/2401.04481</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04481]] Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset(http://arxiv.org/abs/2401.04481)</code></li>
<li>Summary: <p>The recent success in language generation capabilities of large language
models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns
about their possible misuse in inducing mass agitation and communal hatred via
generating fake news and spreading misinformation. Traditional means of
developing a misinformation ground-truth dataset does not scale well because of
the extensive manual effort required to annotate the data. In this paper, we
propose an LLM-based approach of creating silver-standard ground-truth datasets
for identifying misinformation. Specifically speaking, given a trusted news
article, our proposed approach involves prompting LLMs to automatically
generate a summarised version of the original article. The prompts in our
proposed approach act as a controlling mechanism to generate specific types of
factual incorrectness in the generated summaries, e.g., incorrect quantities,
false attributions etc. To investigate the usefulness of this dataset, we
conduct a set of experiments where we train a range of supervised models for
the task of misinformation detection.
</p></li>
</ul>

<h2>code</h2>
<h3>Title: On The Potential of The Fractal Geometry and The CNNs Ability to Encode it. (arXiv:2401.04141v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04141">http://arxiv.org/abs/2401.04141</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04141]] On The Potential of The Fractal Geometry and The CNNs Ability to Encode it(http://arxiv.org/abs/2401.04141)</code></li>
<li>Summary: <p>The fractal dimension provides a statistical index of object complexity by
studying how the pattern changes with the measuring scale. Although useful in
several classification tasks, the fractal dimension is under-explored in deep
learning applications. In this work, we investigate the features that are
learned by deep models and we study whether these deep networks are able to
encode features as complex and high-level as the fractal dimensions.
Specifically, we conduct a correlation analysis experiment to show that deep
networks are not able to extract such a feature in none of their layers. We
combine our analytical study with a human evaluation to investigate the
differences between deep learning networks and models that operate on the
fractal feature solely. Moreover, we show the effectiveness of fractal features
in applications where the object structure is crucial for the classification
task. We empirically show that training a shallow network on fractal features
achieves performance comparable, even superior in specific cases, to that of
deep networks trained on raw data while requiring less computational resources.
Fractals improved the accuracy of the classification by 30% on average while
requiring up to 84% less time to train. We couple our empirical study with a
complexity analysis of the computational cost of extracting the proposed
fractal features, and we study its limitation.
</p></li>
</ul>

<h3>Title: Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting. (arXiv:2401.04148v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04148">http://arxiv.org/abs/2401.04148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04148]] Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting(http://arxiv.org/abs/2401.04148)</code></li>
<li>Summary: <p>Accurate spatial-temporal traffic flow forecasting is crucial in aiding
traffic managers in implementing control measures and assisting drivers in
selecting optimal travel routes. Traditional deep-learning based methods for
traffic flow forecasting typically rely on historical data to train their
models, which are then used to make predictions on future data. However, the
performance of the trained model usually degrades due to the temporal drift
between the historical and future data. To make the model trained on historical
data better adapt to future data in a fully online manner, this paper conducts
the first study of the online test-time adaptation techniques for
spatial-temporal traffic flow forecasting problems. To this end, we propose an
Adaptive Double Correction by Series Decomposition (ADCSD) method, which first
decomposes the output of the trained model into seasonal and trend-cyclical
parts and then corrects them by two separate modules during the testing phase
using the latest observed data entry by entry. In the proposed ADCSD method,
instead of fine-tuning the whole trained model during the testing phase, a lite
network is attached after the trained model, and only the lite network is
fine-tuned in the testing process each time a data entry is observed. Moreover,
to satisfy that different time series variables may have different levels of
temporal drift, two adaptive vectors are adopted to provide different weights
for different time series variables. Extensive experiments on four real-world
traffic flow forecasting datasets demonstrate the effectiveness of the proposed
ADCSD method. The code is available at https://github.com/Pengxin-Guo/ADCSD.
</p></li>
</ul>

<h3>Title: IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records. (arXiv:2401.04402v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04402">http://arxiv.org/abs/2401.04402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04402]] IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records(http://arxiv.org/abs/2401.04402)</code></li>
<li>Summary: <p>Electronic Health Records present a valuable modality for driving
personalized medicine, where treatment is tailored to fit individual-level
differences. For this purpose, many data-driven machine learning and
statistical models rely on the wealth of longitudinal EHRs to study patients'
physiological and treatment effects. However, longitudinal EHRs tend to be
sparse and highly missing, where missingness could also be informative and
reflect the underlying patient's health status. Therefore, the success of
data-driven models for personalized medicine highly depends on how the EHR data
is represented from physiological data, treatments, and the missing values in
the data. To this end, we propose a novel deep-learning model that learns the
underlying patient dynamics over time across multivariate data to generate
personalized realistic values conditioning on an individual's demographic
characteristics and treatments. Our proposed model, IGNITE (Individualized
GeNeration of Imputations in Time-series Electronic health records), utilises a
conditional dual-variational autoencoder augmented with dual-stage attention to
generate missing values for an individual. In IGNITE, we further propose a
novel individualized missingness mask (IMM), which helps our model generate
values based on the individual's observed data and missingness patterns. We
further extend the use of IGNITE from imputing missingness to a personalized
data synthesizer, where it generates missing EHRs that were never observed
prior or even generates new patients for various applications. We validate our
model on three large publicly available datasets and show that IGNITE
outperforms state-of-the-art approaches in missing data reconstruction and task
prediction.
</p></li>
</ul>

<h3>Title: LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training. (arXiv:2401.04348v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04348">http://arxiv.org/abs/2401.04348</a></li>
<li>Code URL: <a href="https://github.com/phkhanhtrinh23/LAMPAT">https://github.com/phkhanhtrinh23/LAMPAT</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04348]] LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training(http://arxiv.org/abs/2401.04348)</code></li>
<li>Summary: <p>Paraphrases are texts that convey the same meaning while using different
words or sentence structures. It can be used as an automatic data augmentation
tool for many Natural Language Processing tasks, especially when dealing with
low-resource languages, where data shortage is a significant problem. To
generate a paraphrase in multilingual settings, previous studies have leveraged
the knowledge from the machine translation field, i.e., forming a paraphrase
through zero-shot machine translation in the same language. Despite good
performance on human evaluation, those methods still require parallel
translation datasets, thus making them inapplicable to languages that do not
have parallel corpora. To mitigate that problem, we proposed the first
unsupervised multilingual paraphrasing model, LAMPAT ($\textbf{L}$ow-rank
$\textbf{A}$daptation for $\textbf{M}$ultilingual $\textbf{P}$araphrasing using
$\textbf{A}$dversarial $\textbf{T}$raining), by which monolingual dataset is
sufficient enough to generate a human-like and diverse sentence. Throughout the
experiments, we found out that our method not only works well for English but
can generalize on unseen languages as well. Data and code are available at
https://github.com/phkhanhtrinh23/LAMPAT.
</p></li>
</ul>

<h3>Title: CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets. (arXiv:2401.04139v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04139">http://arxiv.org/abs/2401.04139</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04139]] CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets(http://arxiv.org/abs/2401.04139)</code></li>
<li>Summary: <p>This study introduces CCNETS (Causal Learning with Causal Cooperative Nets),
a novel generative model-based classifier designed to tackle the challenge of
generating data for imbalanced datasets in pattern recognition. CCNETS is
uniquely crafted to emulate brain-like information processing and comprises
three main components: Explainer, Producer, and Reasoner. Each component is
designed to mimic specific brain functions, which aids in generating
high-quality datasets and enhancing classification performance.
</p>
<p>The model is particularly focused on addressing the common and significant
challenge of handling imbalanced datasets in machine learning. CCNETS's
effectiveness is demonstrated through its application to a "fraud dataset,"
where normal transactions significantly outnumber fraudulent ones (99.83% vs.
0.17%). Traditional methods often struggle with such imbalances, leading to
skewed performance metrics. However, CCNETS exhibits superior classification
ability, as evidenced by its performance metrics. Specifically, it achieved an
F1-score of 0.7992, outperforming traditional models like Autoencoders and
Multi-layer Perceptrons (MLP) in the same context. This performance indicates
CCNETS's proficiency in more accurately distinguishing between normal and
fraudulent patterns.
</p>
<p>The innovative structure of CCNETS enhances the coherence between generative
and classification models, helping to overcome the limitations of pattern
recognition that rely solely on generative models. This study emphasizes
CCNETS's potential in diverse applications, especially where quality data
generation and pattern recognition are key. It proves effective in machine
learning, particularly for imbalanced datasets. CCNETS overcomes current
challenges in these datasets and advances machine learning with brain-inspired
approaches.
</p></li>
</ul>

<h3>Title: Robust Imitation Learning for Automated Game Testing. (arXiv:2401.04572v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04572">http://arxiv.org/abs/2401.04572</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04572]] Robust Imitation Learning for Automated Game Testing(http://arxiv.org/abs/2401.04572)</code></li>
<li>Summary: <p>Game development is a long process that involves many stages before a product
is ready for the market. Human play testing is among the most time consuming,
as testers are required to repeatedly perform tasks in the search for errors in
the code. Therefore, automated testing is seen as a key technology for the
gaming industry, as it would dramatically improve development costs and
efficiency. Toward this end, we propose EVOLUTE, a novel imitation
learning-based architecture that combines behavioural cloning (BC) with energy
based models (EBMs). EVOLUTE is a two-stream ensemble model that splits the
action space of autonomous agents into continuous and discrete tasks. The EBM
stream handles the continuous tasks, to have a more refined and adaptive
control, while the BC stream handles discrete actions, to ease training. We
evaluate the performance of EVOLUTE in a shooting-and-driving game, where the
agent is required to navigate and continuously identify targets to attack. The
proposed model has higher generalisation capabilities than standard BC
approaches, showing a wider range of behaviours and higher performances. Also,
EVOLUTE is easier to train than a pure end-to-end EBM model, as discrete tasks
can be quite sparse in the dataset and cause model training to explore a much
wider set of possible actions while training.
</p></li>
</ul>

<h3>Title: Mixture of multilayer stochastic block models for multiview clustering. (arXiv:2401.04682v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04682">http://arxiv.org/abs/2401.04682</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04682]] Mixture of multilayer stochastic block models for multiview clustering(http://arxiv.org/abs/2401.04682)</code></li>
<li>Summary: <p>In this work, we propose an original method for aggregating multiple
clustering coming from different sources of information. Each partition is
encoded by a co-membership matrix between observations. Our approach uses a
mixture of multilayer Stochastic Block Models (SBM) to group co-membership
matrices with similar information into components and to partition observations
into different clusters, taking into account their specificities within the
components. The identifiability of the model parameters is established and a
variational Bayesian EM algorithm is proposed for the estimation of these
parameters. The Bayesian framework allows for selecting an optimal number of
clusters and components. The proposed approach is compared using synthetic data
with consensus clustering and tensor-based algorithms for community detection
in large-scale complex networks. Finally, the method is utilized to analyze
global food trading networks, leading to structures of interest.
</p></li>
</ul>

<h2>chat</h2>
<h2>retrieval augmented generation</h2>
<h2>retrieval-augmented generation</h2>
<h2>rag</h2>
<h3>Title: Robust Calibration For Improved Weather Prediction Under Distributional Shift. (arXiv:2401.04144v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04144">http://arxiv.org/abs/2401.04144</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04144]] Robust Calibration For Improved Weather Prediction Under Distributional Shift(http://arxiv.org/abs/2401.04144)</code></li>
<li>Summary: <p>In this paper, we present results on improving out-of-domain weather
prediction and uncertainty estimation as part of the \texttt{Shifts Challenge
on Robustness and Uncertainty under Real-World Distributional Shift} challenge.
We find that by leveraging a mixture of experts in conjunction with an advanced
data augmentation technique borrowed from the computer vision domain, in
conjunction with robust \textit{post-hoc} calibration of predictive
uncertainties, we can potentially achieve more accurate and better-calibrated
results with deep neural networks than with boosted tree models for tabular
data. We quantify our predictions using several metrics and propose several
future lines of inquiry and experimentation to boost performance.
</p></li>
</ul>

<h3>Title: Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04336">http://arxiv.org/abs/2401.04336</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04336]] Deep Efficient Private Neighbor Generation for Subgraph Federated Learning(http://arxiv.org/abs/2401.04336)</code></li>
<li>Summary: <p>Behemoth graphs are often fragmented and separately stored by multiple data
owners as distributed subgraphs in many realistic applications. Without harming
data privacy, it is natural to consider the subgraph federated learning
(subgraph FL) scenario, where each local client holds a subgraph of the entire
global graph, to obtain globally generalized graph mining models. To overcome
the unique challenge of incomplete information propagation on local subgraphs
due to missing cross-subgraph neighbors, previous works resort to the
augmentation of local neighborhoods through the joint FL of missing neighbor
generators and GNNs. Yet their technical designs have profound limitations
regarding the utility, efficiency, and privacy goals of FL. In this work, we
propose FedDEP to comprehensively tackle these challenges in subgraph FL.
FedDEP consists of a series of novel technical designs: (1) Deep neighbor
generation through leveraging the GNN embeddings of potential missing
neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding
prototyping; and (3) Privacy protection through noise-less
edge-local-differential-privacy.
</p>
<p>We analyze the correctness and efficiency of FedDEP, and provide theoretical
guarantees on its privacy.
</p>
<p>Empirical results on four real-world datasets justify the clear benefits of
proposed techniques.
</p></li>
</ul>

<h3>Title: Enhancing Acute Kidney Injury Prediction through Integration of Drug Features in Intensive Care Units. (arXiv:2401.04368v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04368">http://arxiv.org/abs/2401.04368</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04368]] Enhancing Acute Kidney Injury Prediction through Integration of Drug Features in Intensive Care Units(http://arxiv.org/abs/2401.04368)</code></li>
<li>Summary: <p>The relationship between acute kidney injury (AKI) prediction and nephrotoxic
drugs, or drugs that adversely affect kidney function, is one that has yet to
be explored in the critical care setting. One contributing factor to this gap
in research is the limited investigation of drug modalities in the intensive
care unit (ICU) context, due to the challenges of processing prescription data
into the corresponding drug representations and a lack in the comprehensive
understanding of these drug representations. This study addresses this gap by
proposing a novel approach that leverages patient prescription data as a
modality to improve existing models for AKI prediction. We base our research on
Electronic Health Record (EHR) data, extracting the relevant patient
prescription information and converting it into the selected drug
representation for our research, the extended-connectivity fingerprint (ECFP).
Furthermore, we adopt a unique multimodal approach, developing machine learning
models and 1D Convolutional Neural Networks (CNN) applied to clinical drug
representations, establishing a procedure which has not been used by any
previous studies predicting AKI. The findings showcase a notable improvement in
AKI prediction through the integration of drug embeddings and other patient
cohort features. By using drug features represented as ECFP molecular
fingerprints along with common cohort features such as demographics and lab
test values, we achieved a considerable improvement in model performance for
the AKI prediction task over the baseline model which does not include the drug
representations as features, indicating that our distinct approach enhances
existing baseline techniques and highlights the relevance of drug data in
predicting AKI in the ICU setting
</p></li>
</ul>

<h3>Title: Air Quality Forecasting Using Machine Learning: A Global perspective with Relevance to Low-Resource Settings. (arXiv:2401.04369v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04369">http://arxiv.org/abs/2401.04369</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04369]] Air Quality Forecasting Using Machine Learning: A Global perspective with Relevance to Low-Resource Settings(http://arxiv.org/abs/2401.04369)</code></li>
<li>Summary: <p>Air pollution stands as the fourth leading cause of death globally. While
extensive research has been conducted in this domain, most approaches rely on
large datasets when it comes to prediction. This limits their applicability in
low-resource settings though more vulnerable. This study addresses this gap by
proposing a novel machine learning approach for accurate air quality prediction
using two months of air quality data. By leveraging the World Weather
Repository, the meteorological, air pollutant, and Air Quality Index features
from 197 capital cities were considered to predict air quality for the next
day. The evaluation of several machine learning models demonstrates the
effectiveness of the Random Forest algorithm in generating reliable
predictions, particularly when applied to classification rather than
regression, approach which enhances the model's generalizability by 42%,
achieving a cross-validation score of 0.38 for regression and 0.89 for
classification. To instill confidence in the predictions, interpretable machine
learning was considered. Finally, a cost estimation comparing the
implementation of this solution in high-resource and low-resource settings is
presented including a tentative of technology licensing business model. This
research highlights the potential for resource-limited countries to
independently predict air quality while awaiting larger datasets to further
refine their predictions.
</p></li>
</ul>

<h3>Title: AI Competitions and Benchmarks, Practical issues: Proposals, grant money, sponsors, prizes, dissemination, publicity. (arXiv:2401.04452v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04452">http://arxiv.org/abs/2401.04452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04452]] AI Competitions and Benchmarks, Practical issues: Proposals, grant money, sponsors, prizes, dissemination, publicity(http://arxiv.org/abs/2401.04452)</code></li>
<li>Summary: <p>This chapter provides a comprehensive overview of the pragmatic aspects
involved in organizing AI competitions. We begin by discussing strategies to
incentivize participation, touching upon effective communication techniques,
aligning with trending topics in the field, structuring awards, potential
recruitment opportunities, and more. We then shift to the essence of community
engagement, and into organizational best practices and effective means of
disseminating challenge outputs. Lastly, the chapter addresses the logistics,
exposing on costs, required manpower, and resource allocation for effectively
managing and executing a challenge. By examining these practical problems,
readers will gain actionable insights to navigate the multifaceted landscape of
AI competition organization, from inception to completion.
</p></li>
</ul>

<h3>Title: Distribution-Free Conformal Joint Prediction Regions for Neural Marked Temporal Point Processes. (arXiv:2401.04612v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04612">http://arxiv.org/abs/2401.04612</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04612]] Distribution-Free Conformal Joint Prediction Regions for Neural Marked Temporal Point Processes(http://arxiv.org/abs/2401.04612)</code></li>
<li>Summary: <p>Sequences of labeled events observed at irregular intervals in continuous
time are ubiquitous across various fields. Temporal Point Processes (TPPs)
provide a mathematical framework for modeling these sequences, enabling
inferences such as predicting the arrival time of future events and their
associated label, called mark. However, due to model misspecification or lack
of training data, these probabilistic models may provide a poor approximation
of the true, unknown underlying process, with prediction regions extracted from
them being unreliable estimates of the underlying uncertainty. This paper
develops more reliable methods for uncertainty quantification in neural TPP
models via the framework of conformal prediction. A primary objective is to
generate a distribution-free joint prediction region for the arrival time and
mark, with a finite-sample marginal coverage guarantee. A key challenge is to
handle both a strictly positive, continuous response and a categorical
response, without distributional assumptions. We first consider a simple but
overly conservative approach that combines individual prediction regions for
the event arrival time and mark. Then, we introduce a more effective method
based on bivariate highest density regions derived from the joint predictive
density of event arrival time and mark. By leveraging the dependencies between
these two variables, this method exclude unlikely combinations of the two,
resulting in sharper prediction regions while still attaining the pre-specified
coverage level. We also explore the generation of individual univariate
prediction regions for arrival times and marks through conformal regression and
classification techniques. Moreover, we investigate the stronger notion of
conditional coverage. Finally, through extensive experimentation on both
simulated and real-world datasets, we assess the validity and efficiency of
these methods.
</p></li>
</ul>

<h3>Title: Transfer-Learning-Based Autotuning Using Gaussian Copula. (arXiv:2401.04669v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04669">http://arxiv.org/abs/2401.04669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04669]] Transfer-Learning-Based Autotuning Using Gaussian Copula(http://arxiv.org/abs/2401.04669)</code></li>
<li>Summary: <p>As diverse high-performance computing (HPC) systems are built, many
opportunities arise for applications to solve larger problems than ever before.
Given the significantly increased complexity of these HPC systems and
application tuning, empirical performance tuning, such as autotuning, has
emerged as a promising approach in recent years. Despite its effectiveness,
autotuning is often a computationally expensive approach. Transfer learning
(TL)-based autotuning seeks to address this issue by leveraging the data from
prior tuning. Current TL methods for autotuning spend significant time modeling
the relationship between parameter configurations and performance, which is
ineffective for few-shot (that is, few empirical evaluations) tuning on new
tasks. We introduce the first generative TL-based autotuning approach based on
the Gaussian copula (GC) to model the high-performing regions of the search
space from prior data and then generate high-performing configurations for new
tasks. This allows a sampling-based approach that maximizes few-shot
performance and provides the first probabilistic estimation of the few-shot
budget for effective TL-based autotuning. We compare our generative TL approach
with state-of-the-art autotuning techniques on several benchmarks. We find that
the GC is capable of achieving 64.37% of peak few-shot performance in its first
evaluation. Furthermore, the GC model can determine a few-shot transfer budget
that yields up to 33.39$\times$ speedup, a dramatic improvement over the
20.58$\times$ speedup using prior techniques.
</p></li>
</ul>

<h2>multi-run</h2>
<h2>chain-of-thought</h2>
<h2>tree-of-thought</h2>
<h2>agent</h2>
<h3>Title: i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance. (arXiv:2401.04429v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04429">http://arxiv.org/abs/2401.04429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04429]] i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance(http://arxiv.org/abs/2401.04429)</code></li>
<li>Summary: <p>Ride-hailing platforms have been facing the challenge of balancing demand and
supply. Existing vehicle reposition techniques often treat drivers as
homogeneous agents and relocate them deterministically, assuming compliance
with the reposition. In this paper, we consider a more realistic and
driver-centric scenario where drivers have unique cruising preferences and can
decide whether to take the recommendation or not on their own. We propose
i-Rebalance, a personalized vehicle reposition technique with deep
reinforcement learning (DRL). i-Rebalance estimates drivers' decisions on
accepting reposition recommendations through an on-field user study involving
99 real drivers. To optimize supply-demand balance and enhance preference
satisfaction simultaneously, i-Rebalance has a sequential reposition strategy
with dual DRL agents: Grid Agent to determine the reposition order of idle
vehicles, and Vehicle Agent to provide personalized recommendations to each
vehicle in the pre-defined order. This sequential learning strategy facilitates
more effective policy training within a smaller action space compared to
traditional joint-action methods. Evaluation of real-world trajectory data
shows that i-Rebalance improves driver acceptance rate by 38.07% and total
driver income by 9.97%.
</p></li>
</ul>

<h3>Title: Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04620">http://arxiv.org/abs/2401.04620</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04620]] Agent Alignment in Evolving Social Norms(http://arxiv.org/abs/2401.04620)</code></li>
<li>Summary: <p>Agents based on Large Language Models (LLMs) are increasingly permeating
various domains of human production and life, highlighting the importance of
aligning them with human values. The current alignment of AI systems primarily
focuses on passively aligning LLMs through human intervention. However, agents
possess characteristics like receiving environmental feedback and
self-evolution, rendering the LLM alignment methods inadequate. In response, we
propose an evolutionary framework for agent evolution and alignment, named
EvolutionaryAgent, which transforms agent alignment into a process of evolution
and selection under the principle of survival of the fittest. In an environment
where social norms continuously evolve, agents better adapted to the current
social norms will have a higher probability of survival and proliferation,
while those inadequately aligned dwindle over time. Experimental results
assessing the agents from multiple perspectives in aligning with social norms
demonstrate that EvolutionaryAgent possesses the capability to align
progressively better with the evolving social norms while maintaining its
proficiency in general tasks. Effectiveness tests conducted on various open and
closed-source LLMs as the foundation for agents also prove the applicability of
our approach.
</p></li>
</ul>

<h3>Title: Deep Reinforcement Multi-agent Learning framework for Information Gathering with Local Gaussian Processes for Water Monitoring. (arXiv:2401.04631v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04631">http://arxiv.org/abs/2401.04631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04631]] Deep Reinforcement Multi-agent Learning framework for Information Gathering with Local Gaussian Processes for Water Monitoring(http://arxiv.org/abs/2401.04631)</code></li>
<li>Summary: <p>The conservation of hydrological resources involves continuously monitoring
their contamination. A multi-agent system composed of autonomous surface
vehicles is proposed in this paper to efficiently monitor the water quality. To
achieve a safe control of the fleet, the fleet policy should be able to act
based on measurements and to the the fleet state. It is proposed to use Local
Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective
monitoring policies. Local Gaussian processes, unlike classical global Gaussian
processes, can accurately model the information in a dissimilar spatial
correlation which captures more accurately the water quality information. A
Deep convolutional policy is proposed, that bases the decisions on the
observation on the mean and variance of this model, by means of an information
gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to
minimize the estimation error in a safe manner thanks to a Consensus-based
heuristic. Simulation results indicate an improvement of up to 24% in terms of
the mean absolute error with the proposed models. Also, training results with
1-3 agents indicate that our proposed approach returns 20% and 24% smaller
average estimation errors for, respectively, monitoring water quality variables
and monitoring algae blooms, as compared to state-of-the-art approaches
</p></li>
</ul>

<h3>Title: MARG: Multi-Agent Review Generation for Scientific Papers. (arXiv:2401.04259v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04259">http://arxiv.org/abs/2401.04259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04259]] MARG: Multi-Agent Review Generation for Scientific Papers(http://arxiv.org/abs/2401.04259)</code></li>
<li>Summary: <p>We study the ability of LLMs to generate feedback for scientific papers and
develop MARG, a feedback generation approach using multiple LLM instances that
engage in internal discussion. By distributing paper text across agents, MARG
can consume the full text of papers beyond the input length limitations of the
base LLM, and by specializing agents and incorporating sub-tasks tailored to
different comment types (experiments, clarity, impact) it improves the
helpfulness and specificity of feedback. In a user study, baseline methods
using GPT-4 were rated as producing generic or very generic comments more than
half the time, and only 1.7 comments per paper were rated as good overall in
the best baseline. Our system substantially improves the ability of GPT-4 to
generate specific and helpful feedback, reducing the rate of generic comments
from 60% to 29% and generating 3.7 good comments per paper (a 2.2x
improvement).
</p></li>
</ul>

<h3>Title: The Role of Higher-Order Cognitive Models in Active Learning. (arXiv:2401.04397v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.04397">http://arxiv.org/abs/2401.04397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.04397]] The Role of Higher-Order Cognitive Models in Active Learning(http://arxiv.org/abs/2401.04397)</code></li>
<li>Summary: <p>Building machines capable of efficiently collaborating with humans has been a
longstanding goal in artificial intelligence. Especially in the presence of
uncertainties, optimal cooperation often requires that humans and artificial
agents model each other's behavior and use these models to infer underlying
goals, beliefs or intentions, potentially involving multiple levels of
recursion. Empirical evidence for such higher-order cognition in human behavior
is also provided by previous works in cognitive science, linguistics, and
robotics. We advocate for a new paradigm for active learning for human feedback
that utilises humans as active data sources while accounting for their higher
levels of agency. In particular, we discuss how increasing level of agency
results in qualitatively different forms of rational communication between an
active learning system and a teacher. Additionally, we provide a practical
example of active learning using a higher-order cognitive model. This is
accompanied by a computational study that underscores the unique behaviors that
this model produces.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
