<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-05-03</h1>
<h3>Title: A Survey on the Real Power of ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Ming Liu, Ran Liu, Hua Wang, Wray Buntine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00704">https://arxiv.org/abs/2405.00704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00704">https://arxiv.org/pdf/2405.00704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00704]] A Survey on the Real Power of ChatGPT(https://arxiv.org/abs/2405.00704)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, chat</a></li>
<li><strong>Abstract: </strong>ChatGPT has changed the AI community and an active research line is the performance evaluation of ChatGPT. A key challenge for the evaluation is that ChatGPT is still closed-source and traditional benchmark datasets may have been used by ChatGPT as the training data. In this paper, (i) we survey recent studies which uncover the real performance levels of ChatGPT in seven categories of NLP tasks, (ii) review the social implications and safety issues of ChatGPT, and (iii) emphasize key challenges and opportunities for its evaluation. We hope our survey can shed some light on its blackbox manner, so that researchers are not misleaded by its surface generation.</li>
<li><strong>摘要：</strong>ChatGPT 改变了 AI 社区，一个活跃的研究方向是 ChatGPT 的性能评估。评估的一个关键挑战是 ChatGPT 仍然是闭源的，ChatGPT 可能已使用传统基准数据集作为训练数据。在本文中，（i）我们调查了最近的研究，这些研究揭示了 ChatGPT 在七类 NLP 任务中的真实性能水平，（ii）回顾了 ChatGPT 的社会影响和安全问题，以及（iii）强调了其面临的主要挑战和机遇评估。我们希望我们的调查能够揭示其黑箱方式，以便研究人员不会被其表面生成所误导。</li>
</ul>

<h3>Title: SHED: Shapley-Based Automated Dataset Refinement for Instruction  Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yexiao He, Ziyao Wang, Zheyu Shen, Guoheng Sun, Yucong Dai, Yongkai Wu, Hongyi Wang, Ang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00705">https://arxiv.org/abs/2405.00705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00705">https://arxiv.org/pdf/2405.00705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00705]] SHED: Shapley-Based Automated Dataset Refinement for Instruction  Fine-Tuning(https://arxiv.org/abs/2405.00705)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The pre-trained Large Language Models (LLMs) can be adapted for many downstream tasks and tailored to align with human preferences through fine-tuning. Recent studies have discovered that LLMs can achieve desirable performance with only a small amount of high-quality data, suggesting that a large amount of the data in these extensive datasets is redundant or even harmful. Identifying high-quality data from vast datasets to curate small yet effective datasets has emerged as a critical challenge. In this paper, we introduce SHED, an automated dataset refinement framework based on Shapley value for instruction fine-tuning. SHED eliminates the need for human intervention or the use of commercial LLMs. Moreover, the datasets curated through SHED exhibit transferability, indicating they can be reused across different LLMs with consistently high performance. We conduct extensive experiments to evaluate the datasets curated by SHED. The results demonstrate SHED's superiority over state-of-the-art methods across various tasks and LLMs; notably, datasets comprising only 10% of the original data selected by SHED achieve performance comparable to or surpassing that of the full datasets.</li>
<li><strong>摘要：</strong>预先训练的大型语言模型（LLM）可以适应许多下游任务，并通过微调来适应人类的喜好。最近的研究发现，法学硕士只需少量高质量数据即可实现理想的性能，这表明这些广泛数据集中的大量数据是冗余的，甚至是有害的。从大量数据集中识别高质量数据以整理小而有效的数据集已成为一项关键挑战。在本文中，我们介绍 SHED，一种基于 Shapley 值进行指令微调的自动化数据集细化框架。 SHED 消除了人为干预或使用商业法学硕士的需要。此外，通过 SHED 整理的数据集具有可转移性，这表明它们可以在不同的法学硕士之间重复使用，并始终保持高性能。我们进行了大量的实验来评估 SHED 整理的数据集。结果证明 SHED 在各种任务和法学硕士方面优于最先进的方法；值得注意的是，SHED 选择的仅包含原始数据 10% 的数据集的性能可与完整数据集相当或超过。</li>
</ul>

<h3>Title: Science Written by Generative AI is Perceived as Less Intelligent, but  More Credible and Trustworthy than Science Written by Humans</h3>
<ul>
<li><strong>Authors: </strong>David M. Markowitz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00706">https://arxiv.org/abs/2405.00706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00706">https://arxiv.org/pdf/2405.00706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00706]] Science Written by Generative AI is Perceived as Less Intelligent, but  More Credible and Trustworthy than Science Written by Humans(https://arxiv.org/abs/2405.00706)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt</a></li>
<li><strong>Abstract: </strong>This paper evaluated the effectiveness of using generative AI to simplify science communication and enhance public trust in science. By comparing lay summaries of journal articles from PNAS, yoked to those generated by AI, this work assessed linguistic simplicity across such summaries and public perceptions. Study 1a analyzed simplicity features of PNAS abstracts (scientific summaries) and significance statements (lay summaries), observing that lay summaries were indeed linguistically simpler, but effect size differences were small. Study 1b used GPT-4 to create significance statements based on paper abstracts and this more than doubled the average effect size without fine-tuning. Finally, Study 2 experimentally demonstrated that simply-written GPT summaries facilitated more favorable public perceptions of scientists (their credibility, trustworthiness) than more complexly-written human PNAS summaries. AI has the potential to engage scientific communities and the public via a simple language heuristic, advocating for its integration into scientific dissemination for a more informed society.</li>
<li><strong>摘要：</strong>本文评估了使用生成式人工智能简化科学传播和增强公众对科学的信任的有效性。通过比较 PNAS 期刊文章的简明摘要与人工智能生成的摘要，这项工作评估了此类摘要和公众看法的语言简单性。研究 1a 分析了 PNAS 摘要（科学摘要）和重要性陈述（非专业摘要）的简单性特征，观察到非专业摘要在语言上确实更简单，但效应大小差异很小。研究 1b 使用 GPT-4 根据论文摘要创建重要性陈述，无需微调，平均效果大小就增加了一倍多。最后，研究 2 通过实验证明，与写得复杂的人类 PNAS 摘要相比，写得简单的 GPT 摘要更能促进公众对科学家的更有利的看法（他们的可信度、可信度）。人工智能有潜力通过简单的语言启发式吸引科学界和公众，倡导将其融入科学传播，打造一个更加知情的社会。</li>
</ul>

<h3>Title: Interactive Analysis of LLMs using Meaningful Counterfactuals</h3>
<ul>
<li><strong>Authors: </strong>Furui Cheng, Vilém Zouhar, Robin Shing Moon Chan, Daniel Fürst, Hendrik Strobelt, Mennatallah El-Assady</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00708">https://arxiv.org/abs/2405.00708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00708">https://arxiv.org/pdf/2405.00708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00708]] Interactive Analysis of LLMs using Meaningful Counterfactuals(https://arxiv.org/abs/2405.00708)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Counterfactual examples are useful for exploring the decision boundaries of machine learning models and determining feature attributions. How can we apply counterfactual-based methods to analyze and explain LLMs? We identify the following key challenges. First, the generated textual counterfactuals should be meaningful and readable to users and thus can be mentally compared to draw conclusions. Second, to make the solution scalable to long-form text, users should be equipped with tools to create batches of counterfactuals from perturbations at various granularity levels and interactively analyze the results. In this paper, we tackle the above challenges and contribute 1) a novel algorithm for generating batches of complete and meaningful textual counterfactuals by removing and replacing text segments in different granularities, and 2) LLM Analyzer, an interactive visualization tool to help users understand an LLM's behaviors by interactively inspecting and aggregating meaningful counterfactuals. We evaluate the proposed algorithm by the grammatical correctness of its generated counterfactuals using 1,000 samples from medical, legal, finance, education, and news datasets. In our experiments, 97.2% of the counterfactuals are grammatically correct. Through a use case, user studies, and feedback from experts, we demonstrate the usefulness and usability of the proposed interactive visualization tool.</li>
<li><strong>摘要：</strong>反事实示例对于探索机器学习模型的决策边界和确定特征属性很有用。我们如何应用基于反事实的方法来分析和解释法学硕士？我们确定了以下主要挑战。首先，生成的文本反事实应该对用户有意义且可读，因此可以在心理上进行比较以得出结论。其次，为了使解决方案能够扩展到长文本，用户应该配备工具来根据不同粒度级别的扰动创建批量反事实，并交互式地分析结果。在本文中，我们解决了上述挑战，并贡献了 1) 一种新颖的算法，通过删除和替换不同粒度的文本段来生成批量完整且有意义的文本反事实，以及 2) LLM 分析器，一种交互式可视化工具，可帮助用户理解LLM 的行为通过交互式检查和聚合有意义的反事实来实现。我们使用来自医学、法律、金融、教育和新闻数据集的 1,000 个样本，通过生成的反事实的语法正确性来评估所提出的算法。在我们的实验中，97.2% 的反事实在语法上是正确的。通过用例、用户研究和专家的反馈，我们展示了所提出的交互式可视化工具的实用性和可用性。</li>
</ul>

<h3>Title: Evaluating Tool-Augmented Agents in Remote Sensing Platforms</h3>
<ul>
<li><strong>Authors: </strong>Simranjit Singh, Michael Fore, Dimitrios Stamoulis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00709">https://arxiv.org/abs/2405.00709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00709">https://arxiv.org/pdf/2405.00709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00709]] Evaluating Tool-Augmented Agents in Remote Sensing Platforms(https://arxiv.org/abs/2405.00709)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Tool-augmented Large Language Models (LLMs) have shown impressive capabilities in remote sensing (RS) applications. However, existing benchmarks assume question-answering input templates over predefined image-text data pairs. These standalone instructions neglect the intricacies of realistic user-grounded tasks. Consider a geospatial analyst: they zoom in a map area, they draw a region over which to collect satellite imagery, and they succinctly ask "Detect all objects here". Where is `here`, if it is not explicitly hardcoded in the image-text template, but instead is implied by the system state, e.g., the live map positioning? To bridge this gap, we present GeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual, and click-based actions on a real UI platform. Through in-depth evaluation of state-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights towards stronger agents for RS applications.</li>
<li><strong>摘要：</strong>工具增强的大型语言模型 (LLM) 在遥感 (RS) 应用中表现出了令人印象深刻的功能。然而，现有的基准测试假设问答输入模板基于预定义的图像文本数据对。这些独立的指令忽略了现实的基于用户的任务的复杂性。考虑一个地理空间分析师：他们放大地图区域，绘制一个用于收集卫星图像的区域，然后简洁地询问“检测此处的所有物体”。如果“这里”没有明确硬编码在图像文本模板中，而是由系统状态（例如实时地图定位）暗示，那么它在哪里？为了弥补这一差距，我们推出了 GeoLLM-QA，这是一个基准测试，旨在捕获真实 UI 平台上的长序列口头、视觉和基于点击的操作。通过对 1,000 项不同任务中最先进的法学硕士进行深入评估，我们为 RS 应用程序提供更强大的代理的见解。</li>
</ul>

<h3>Title: Homonym Sense Disambiguation in the Georgian Language</h3>
<ul>
<li><strong>Authors: </strong>Davit Melikidze, Alexander Gamkrelidze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00710">https://arxiv.org/abs/2405.00710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00710">https://arxiv.org/pdf/2405.00710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00710]] Homonym Sense Disambiguation in the Georgian Language(https://arxiv.org/abs/2405.00710)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>This research proposes a novel approach to the Word Sense Disambiguation (WSD) task in the Georgian language, based on supervised fine-tuning of a pre-trained Large Language Model (LLM) on a dataset formed by filtering the Georgian Common Crawls corpus. The dataset is used to train a classifier for words with multiple senses. Additionally, we present experimental results of using LSTM for WSD. Accurately disambiguating homonyms is crucial in natural language processing. Georgian, an agglutinative language belonging to the Kartvelian language family, presents unique challenges in this context. The aim of this paper is to highlight the specific problems concerning homonym disambiguation in the Georgian language and to present our approach to solving them. The techniques discussed in the article achieve 95% accuracy for predicting lexical meanings of homonyms using a hand-classified dataset of over 7500 sentences.</li>
<li><strong>摘要：</strong>这项研究提出了一种格鲁吉亚语词义消歧（WSD）任务的新方法，该方法基于对通过过滤格鲁吉亚通用爬行语料库形成的数据集上的预训练大型语言模型（LLM）进行监督微调。该数据集用于训练具有多种含义的单词的分类器。此外，我们还展示了使用 LSTM 进行 WSD 的实验结果。准确消除同音异义词歧义在自然语言处理中至关重要。格鲁吉亚语是一种属于卡特维利语系的粘着语言，在这方面提出了独特的挑战。本文的目的是强调格鲁吉亚语同音异义消歧的具体问题，并提出我们解决这些问题的方法。本文讨论的技术使用包含 7500 多个句子的手工分类数据集来预测同音异义词的词汇含义，准确率达到 95%。</li>
</ul>

<h3>Title: Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of  Theories, Detection Methods, and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Xiaomin Yu, Yezhaohui Wang, Yanfang Chen, Zhen Tao, Dinghao Xi, Shichao Song, Simin Niu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00711">https://arxiv.org/abs/2405.00711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00711">https://arxiv.org/pdf/2405.00711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00711]] Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of  Theories, Detection Methods, and Opportunities(https://arxiv.org/abs/2405.00711)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>In recent years, generative artificial intelligence models, represented by Large Language Models (LLMs) and Diffusion Models (DMs), have revolutionized content production methods. These artificial intelligence-generated content (AIGC) have become deeply embedded in various aspects of daily life and work, spanning texts, images, videos, and audio. The authenticity of AI-generated content is progressively enhancing, approaching human-level creative standards. However, these technologies have also led to the emergence of Fake Artificial Intelligence Generated Content (FAIGC), posing new challenges in distinguishing genuine information. It is crucial to recognize that AIGC technology is akin to a double-edged sword; its potent generative capabilities, while beneficial, also pose risks for the creation and dissemination of FAIGC. In this survey, We propose a new taxonomy that provides a more comprehensive breakdown of the space of FAIGC methods today. Next, we explore the modalities and generative technologies of FAIGC, categorized under AI-generated disinformation and AI-generated misinformation. From various perspectives, we then introduce FAIGC detection methods, including Deceptive FAIGC Detection, Deepfake Detection, and Hallucination-based FAIGC Detection. Finally, we discuss outstanding challenges and promising areas for future research.</li>
<li><strong>摘要：</strong>近年来，以大语言模型（LLM）和扩散模型（DM）为代表的生成式人工智能模型彻底改变了内容生产方式。这些人工智能生成的内容（AIGC）已深深嵌入日常生活和工作的各个方面，涵盖文本、图像、视频和音频。人工智能生成内容的真实性正在逐步增强，接近人类水平的创作标准。然而，这些技术也导致了虚假人工智能生成内容（FAIGC）的出现，给区分真实信息带来了新的挑战。认识到 AIGC 技术就像一把双刃剑至关重要。其强大的生成能力虽然有益，但也给 FAIGC 的创建和传播带来了风险。在这项调查中，我们提出了一种新的分类法，对当今 FAIGC 方法的空间进行了更全面的细分。接下来，我们探讨 FAIGC 的模式和生成技术，分为人工智能生成的虚假信息和人工智能生成的错误信息。然后我们从不同的角度介绍FAIGC检测方法，包括欺骗性FAIGC检测、Deepfake检测和基于幻觉的FAIGC检测。最后，我们讨论了突出的挑战和未来研究的有前景的领域。</li>
</ul>

<h3>Title: Towards Adapting Open-Source Large Language Models for Expert-Level  Clinical Note Generation</h3>
<ul>
<li><strong>Authors: </strong>Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00715">https://arxiv.org/abs/2405.00715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00715">https://arxiv.org/pdf/2405.00715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00715]] Towards Adapting Open-Source Large Language Models for Expert-Level  Clinical Note Generation(https://arxiv.org/abs/2405.00715)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown promising capabilities in handling clinical text summarization tasks. In this study, we demonstrate that a small open-source LLM can be effectively trained to generate high-quality clinical notes from outpatient patient-doctor dialogues. We achieve this through a comprehensive domain- and task-specific adaptation process for the LLaMA-2 13 billion parameter model. This process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced an enhanced approach, termed DistillDirect, for performing on-policy reinforcement learning with Gemini Pro serving as the teacher model. Our resulting model, LLaMA-Clinic, is capable of generating clinical notes that are comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher across all three criteria: real-world readiness, completeness, and accuracy. Notably, in the more challenging "Assessment and Plan" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness compared to physician-authored notes (4.1/5). Additionally, we identified caveats in public clinical note datasets, such as ACI-BENCH. We highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format. Overall, our research demonstrates the potential and feasibility of training smaller, open-source LLMs to assist with clinical documentation, capitalizing on healthcare institutions' access to patient records and domain expertise. We have made our newly created synthetic clinic dialogue-note dataset and the physician feedback dataset publicly available to foster future research in this field.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在处理临床文本摘要任务方面表现出了良好的能力。在这项研究中，我们证明了小型开源法学硕士可以经过有效培训，从门诊医患对话中生成高质量的临床记录。我们通过 LLaMA-2 130 亿参数模型的全面的特定领域和特定任务的适应过程来实现这一目标。这个过程包括持续的预训练、监督微调以及来自人工智能和人类反馈的强化学习。我们引入了一种称为 DistillDirect 的增强方法，用于以 Gemini Pro 作为教师模型来执行策略强化学习。我们生成的模型 LLaMA-Clinic 能够生成质量与医生撰写的临床记录相当的临床记录。在一项盲法医师读者研究中，大多数 (90.4%) 的个人评估将 LLaMA-Clinic 生成的笔记在所有三个标准上评为“可接受”或更高：现实世界的准备情况、完整性和准确性。值得注意的是，在更具挑战性的“评估和计划”部分，与医生撰写的笔记 (4.1/5) 相比，LLaMA-Clinic 在现实世界准备度方面得分更高 (4.2/5)。此外，我们还发现了公共临床记录数据集（例如 ACI-BENCH）中的警告。我们强调未来临床记录生成任务的关键考虑因素，强调预先定义最佳实践记录格式的重要性。总体而言，我们的研究证明了培训小型开源法学硕士以协助临床记录、利用医疗机构对患者记录和领域专业知识的访问的潜力和可行性。我们公开了新创建的综合临床对话记录数据集和医生反馈数据集，以促进该领域的未来研究。</li>
</ul>

<h3>Title: Large Language Models in Healthcare: A Comprehensive Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Lei Clifton, David A. Clifton</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00716">https://arxiv.org/abs/2405.00716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00716">https://arxiv.org/pdf/2405.00716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00716]] Large Language Models in Healthcare: A Comprehensive Benchmark(https://arxiv.org/abs/2405.00716)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering task with answer options for evaluation. However, in real clinical settings, many clinical decisions, such as treatment recommendations, involve answering open-ended questions without pre-set options. Meanwhile, existing studies mainly use accuracy to assess model performance. In this paper, we comprehensively benchmark diverse LLMs in healthcare, to clearly understand their strengths and weaknesses. Our benchmark contains seven tasks and thirteen datasets across medical language generation, understanding, and reasoning. We conduct a detailed evaluation of the existing sixteen LLMs in healthcare under both zero-shot and few-shot (i.e., 1,3,5-shot) learning settings. We report the results on five metrics (i.e. matching, faithfulness, comprehensiveness, generalizability, and robustness) that are critical in achieving trust from clinical users. We further invite medical experts to conduct human evaluation.</li>
<li><strong>摘要：</strong>采用大语言模型（LLM）来帮助临床医生引起了广泛关注。现有作品主要采用带有答案选项的封闭式问答任务进行评估。然而，在真实的临床环境中，许多临床决策（例如治疗建议）涉及回答没有预设选项的开放式问题。同时，现有研究主要使用准确性来评估模型性能。在本文中，我们对医疗保健领域的不同法学硕士进行了全面的基准测试，以清楚地了解他们的优势和劣势。我们的基准测试包含涵盖医学语言生成、理解和推理的七个任务和十三个数据集。我们在零样本和少样本（即 1、3、5 样本）学习设置下对医疗保健领域现有的 16 个法学硕士进行了详细评估。我们报告了五个指标（即匹配性、忠实性、全面性、普遍性和稳健性）的结果，这对于获得临床用户的信任至关重要。我们进一步邀请医学专家进行人体评估。</li>
</ul>

<h3>Title: Can't say cant? Measuring and Reasoning of Dark Jargons in Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xu Ji, Jianyi Zhang, Ziyin Zhou, Zhangchi Zhao, Qianqian Qiao, Kaiying Han, Md Imran Hossen, Xiali Hei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00718">https://arxiv.org/abs/2405.00718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00718">https://arxiv.org/pdf/2405.00718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00718]] Can't say cant? Measuring and Reasoning of Dark Jargons in Large  Language Models(https://arxiv.org/abs/2405.00718)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat</a></li>
<li><strong>Abstract: </strong>Ensuring the resilience of Large Language Models (LLMs) against malicious exploitation is paramount, with recent focus on mitigating offensive responses. Yet, the understanding of cant or dark jargon remains unexplored. This paper introduces a domain-specific Cant dataset and CantCounter evaluation framework, employing Fine-Tuning, Co-Tuning, Data-Diffusion, and Data-Analysis stages. Experiments reveal LLMs, including ChatGPT, are susceptible to cant bypassing filters, with varying recognition accuracy influenced by question types, setups, and prompt clues. Updated models exhibit higher acceptance rates for cant queries. Moreover, LLM reactions differ across domains, e.g., reluctance to engage in racism versus LGBT topics. These findings underscore LLMs' understanding of cant and reflect training data characteristics and vendor approaches to sensitive topics. Additionally, we assess LLMs' ability to demonstrate reasoning capabilities. Access to our datasets and code is available at https://github.com/cistineup/CantCounter.</li>
<li><strong>摘要：</strong>确保大型语言模型 (LLM) 抵御恶意利用的弹性至关重要，最近的重点是减轻攻击性响应。然而，对黑话或黑暗行话的理解仍有待探索。本文介绍了特定领域的 Cant 数据集和 CantCounter 评估框架，采用微调、协同调整、数据扩散和数据分析阶段。实验表明，包括 ChatGPT 在内的法学硕士很容易受到无法绕过过滤器的影响，其识别准确度会受到问题类型、设置和提示线索的影响。更新后的模型对黑线查询表现出更高的接受率。此外，法学硕士的反应在不同领域也有所不同，例如，不愿意参与种族主义和 LGBT 话题。这些发现强调了法学硕士对黑话的理解，并反映了培训数据特征和供应商对敏感主题的方法。此外，我们还评估法学硕士展示推理能力的能力。您可以通过 https://github.com/cistineup/CantCounter 访问我们的数据集和代码。</li>
</ul>

<h3>Title: LLMs for Generating and Evaluating Counterfactuals: A Comprehensive  Study</h3>
<ul>
<li><strong>Authors: </strong>Van Bach Nguyen, Paul Youssef, Jörg Schlötterer, Christin Seifert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00722">https://arxiv.org/abs/2405.00722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00722">https://arxiv.org/pdf/2405.00722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00722]] LLMs for Generating and Evaluating Counterfactuals: A Comprehensive  Study(https://arxiv.org/abs/2405.00722)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>As NLP models become more complex, understanding their decisions becomes more crucial. Counterfactuals (CFs), where minimal changes to inputs flip a model's prediction, offer a way to explain these models. While Large Language Models (LLMs) have shown remarkable performance in NLP tasks, their efficacy in generating high-quality CFs remains uncertain. This work fills this gap by investigating how well LLMs generate CFs for two NLU tasks. We conduct a comprehensive comparison of several common LLMs, and evaluate their CFs, assessing both intrinsic metrics, and the impact of these CFs on data augmentation. Moreover, we analyze differences between human and LLM-generated CFs, providing insights for future research directions. Our results show that LLMs generate fluent CFs, but struggle to keep the induced changes minimal. Generating CFs for Sentiment Analysis (SA) is less challenging than NLI where LLMs show weaknesses in generating CFs that flip the original label. This also reflects on the data augmentation performance, where we observe a large gap between augmenting with human and LLMs CFs. Furthermore, we evaluate LLMs' ability to assess CFs in a mislabelled data setting, and show that they have a strong bias towards agreeing with the provided labels. GPT4 is more robust against this bias and its scores correlate well with automatic metrics. Our findings reveal several limitations and point to potential future work directions.</li>
<li><strong>摘要：</strong>随着 NLP 模型变得越来越复杂，理解他们的决策变得更加重要。反事实（CF），即输入的最小变化会翻转模型的预测，提供了一种解释这些模型的方法。虽然大型语言模型 (LLM) 在 NLP 任务中表现出了卓越的性能，但它们在生成高质量 CF 方面的功效仍然不确定。这项工作通过调查法学硕士为两个 NLU 任务生成 CF 的效果来填补这一空白。我们对几种常见的法学硕士进行了全面比较，并评估了它们的 CF，评估了内在指标以及这些 CF 对数据增强的影响。此外，我们分析了人类和法学硕士生成的 CF 之间的差异，为未来的研究方向提供了见解。我们的结果表明，法学硕士可以生成流畅的 CF，但很难将引起的变化保持在最低限度。生成用于情感分析 (SA) 的 CF 比 NLI 更具挑战性，其中法学硕士在生成翻转原始标签的 CF 方面表现出弱点。这也反映在数据增强性能上，我们观察到人类 CF 和法学硕士 CF 的增强之间存在很大差距。此外，我们评估了法学硕士在错误标签的数据设置中评估 CF 的能力，并表明他们对同意所提供的标签有强烈的偏见。 GPT4 对于这种偏差更加稳健，并且其分数与自动指标密切相关。我们的研究结果揭示了一些局限性，并指出了未来潜在的工作方向。</li>
</ul>

<h3>Title: Evaluating the Application of ChatGPT in Outpatient Triage Guidance: A  Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Dou Liu, Ying Han, Xiandi Wang, Xiaomei Tan, Di Liu, Guangwu Qian, Kang Li, Dan Pu, Rong Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00728">https://arxiv.org/abs/2405.00728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00728">https://arxiv.org/pdf/2405.00728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00728]] Evaluating the Application of ChatGPT in Outpatient Triage Guidance: A  Comparative Study(https://arxiv.org/abs/2405.00728)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, chat</a></li>
<li><strong>Abstract: </strong>The integration of Artificial Intelligence (AI) in healthcare presents a transformative potential for enhancing operational efficiency and health outcomes. Large Language Models (LLMs), such as ChatGPT, have shown their capabilities in supporting medical decision-making. Embedding LLMs in medical systems is becoming a promising trend in healthcare development. The potential of ChatGPT to address the triage problem in emergency departments has been examined, while few studies have explored its application in outpatient departments. With a focus on streamlining workflows and enhancing efficiency for outpatient triage, this study specifically aims to evaluate the consistency of responses provided by ChatGPT in outpatient guidance, including both within-version response analysis and between-version comparisons. For within-version, the results indicate that the internal response consistency for ChatGPT-4.0 is significantly higher than ChatGPT-3.5 (p=0.03) and both have a moderate consistency (71.2% for 4.0 and 59.6% for 3.5) in their top recommendation. However, the between-version consistency is relatively low (mean consistency score=1.43/3, median=1), indicating few recommendations match between the two versions. Also, only 50% top recommendations match perfectly in the comparisons. Interestingly, ChatGPT-3.5 responses are more likely to be complete than those from ChatGPT-4.0 (p=0.02), suggesting possible differences in information processing and response generation between the two versions. The findings offer insights into AI-assisted outpatient operations, while also facilitating the exploration of potentials and limitations of LLMs in healthcare utilization. Future research may focus on carefully optimizing LLMs and AI integration in healthcare systems based on ergonomic and human factors principles, precisely aligning with the specific needs of effective outpatient triage.</li>
<li><strong>摘要：</strong>人工智能 (AI) 在医疗保健领域的集成为提高运营效率和健康结果带来了变革潜力。 ChatGPT 等大型语言模型 (LLM) 已显示出其支持医疗决策的能力。将法学硕士嵌入医疗系统正在成为医疗保健发展的一个有前景的趋势。 ChatGPT 解决急诊科分诊问题的潜力已经得到检验，但很少有研究探讨其在门诊部的应用。本研究的重点是简化工作流程和提高门诊分诊效率，特别旨在评估 ChatGPT 在门诊指导中提供的响应的一致性，包括版本内响应分析和版本间比较。对于版本内，结果表明 ChatGPT-4.0 的内部响应一致性显着高于 ChatGPT-3.5 (p=0.03)，并且在其最高推荐中均具有中等一致性（4.0 为 71.2%，3.5 为 59.6%） 。然而，版本间一致性相对较低（平均一致性得分=1.43/3，中位数=1），表明两个版本之间很少有推荐匹配。此外，只有 50% 的热门推荐在比较中完全匹配。有趣的是，ChatGPT-3.5 的响应比 ChatGPT-4.0 的响应更有可能完整（p=0.02），这表明两个版本之间的信息处理和响应生成可能存在差异。这些发现提供了对人工智能辅助门诊手术的见解，同时也有助于探索法学硕士在医疗保健利用方面的潜力和局限性。未来的研究可能会侧重于基于人体工程学和人为因素原则，仔细优化法学硕士和人工智能在医疗保健系统中的集成，精确地满足有效门诊分诊的具体需求。</li>
</ul>

<h3>Title: LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Justin Zhao, Timothy Wang, Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky, Piero Molino, Travis Addair, Devvret Rishi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00732">https://arxiv.org/abs/2405.00732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00732">https://arxiv.org/pdf/2405.00732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00732]] LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report(https://arxiv.org/abs/2405.00732)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, we measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.</li>
<li><strong>摘要：</strong>低秩适应 (LoRA) 已成为大型语言模型 (LLM) 参数高效微调 (PEFT) 最广泛采用的方法之一。 LoRA 减少了可训练参数的数量和内存使用量，同时实现了与完全微调相当的性能。我们的目标是评估在现实应用中使用 LoRA 进行微调的法学硕士培训和服务的可行性。首先，我们测量了在 10 个基本模型和 31 个任务（总共 310 个模型）中使用量化低阶适配器进行微调的 LLM 的质量。我们发现 4 位 LoRA 微调模型平均比基础模型高 34 个点，比 GPT-4 平均高 10 个点。其次，我们研究了最有效的微调基础模型，并评估了任务复杂性启发法在预测微调结果方面的相关性和预测能力。最后，我们评估了 LoRAX 的延迟和并发能力，LoRAX 是一款开源多 LoRA 推理服务器，它有助于使用共享基础模型权重和动态适配器加载在单个 GPU 上部署多个 LoRA 微调模型。 LoRAX 为 LoRA Land 提供支持，LoRA Land 是一个 Web 应用程序，在具有 80GB 内存的单个 NVIDIA A100 GPU 上托管 25 个经过 LoRA 微调的 Mistral-7B LLM。 LoRA Land 强调聘用多个专业法学硕士相对于单一通用法学硕士的质量和成本效益。</li>
</ul>

<h3>Title: "Ask Me Anything": How Comcast Uses LLMs to Assist Agents in Real Time</h3>
<ul>
<li><strong>Authors: </strong>Scott Rome, Tianwen Chen, Raphael Tang, Luwei Zhou, Ferhan Ture</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00801">https://arxiv.org/abs/2405.00801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00801">https://arxiv.org/pdf/2405.00801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00801]] "Ask Me Anything": How Comcast Uses LLMs to Assist Agents in Real Time(https://arxiv.org/abs/2405.00801)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chat, agent</a></li>
<li><strong>Abstract: </strong>Customer service is how companies interface with their customers. It can contribute heavily towards the overall customer satisfaction. However, high-quality service can become expensive, creating an incentive to make it as cost efficient as possible and prompting most companies to utilize AI-powered assistants, or "chat bots". On the other hand, human-to-human interaction is still desired by customers, especially when it comes to complex scenarios such as disputes and sensitive topics like bill payment. This raises the bar for customer service agents. They need to accurately understand the customer's question or concern, identify a solution that is acceptable yet feasible (and within the company's policy), all while handling multiple conversations at once. In this work, we introduce "Ask Me Anything" (AMA) as an add-on feature to an agent-facing customer service interface. AMA allows agents to ask questions to a large language model (LLM) on demand, as they are handling customer conversations -- the LLM provides accurate responses in real-time, reducing the amount of context switching the agent needs. In our internal experiments, we find that agents using AMA versus a traditional search experience spend approximately 10% fewer seconds per conversation containing a search, translating to millions of dollars of savings annually. Agents that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating its usefulness as an AI-assisted feature for customer care.</li>
<li><strong>摘要：</strong>客户服务是公司与客户互动的方式。它可以极大地提高客户的整体满意度。然而，高质量的服务可能会变得昂贵，从而激励人们尽可能提高成本效益，并促使大多数公司使用人工智能助理或“聊天机器人”。另一方面，人与人之间的互动仍然是客户所期望的，尤其是在涉及纠纷等复杂场景以及账单支付等敏感话题时。这提高了客户服务代理的门槛。他们需要准确理解客户的问题或疑虑，确定可接受但可行的解决方案（并且在公司政策范围内），同时同时处理多个对话。在这项工作中，我们引入了“Ask Me Anything”(AMA) 作为面向代理的客户服务界面的附加功能。 AMA 允许客服人员在处理客户对话时按需向大型语言模型 (LLM) 提问 - LLM 可以实时提供准确的响应，从而减少客服人员所需的上下文切换量。在我们的内部实验中，我们发现与传统搜索体验相比，使用 AMA 的代理在每次包含搜索的对话中花费的时间大约减少了 10%，这意味着每年可以节省数百万美元。使用 AMA 功能的客服人员在近 80% 的时间内提供了积极反馈，证明了其作为人工智能辅助客户服务功能的实用性。</li>
</ul>

<h3>Title: WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace  Setting</h3>
<ul>
<li><strong>Authors: </strong>Olly Styles, Sam Miller, Patricio Cerda-Mardini, Tanaya Guha, Victor Sanchez, Bertie Vidgen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00823">https://arxiv.org/abs/2405.00823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00823">https://arxiv.org/pdf/2405.00823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00823]] WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace  Setting(https://arxiv.org/abs/2405.00823)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, agent</a></li>
<li><strong>Abstract: </strong>We introduce WorkBench: a benchmark dataset for evaluating agents' ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents' errors can result in the wrong action being taken, such as an email being sent to the wrong person. WorkBench reveals weaknesses in agents' ability to undertake common business activities, raising questions about their use in high-stakes workplace settings. WorkBench is publicly available as a free resource at https://github.com/olly-styles/WorkBench.</li>
<li><strong>摘要：</strong>我们引入了 WorkBench：一个用于评估代理在工作场所环境中执行任务的能力的基准数据集。WorkBench 包含一个沙盒环境，其中有五个数据库、26 个工具和 690 个任务。这些任务代表常见的业务活动，例如发送电子邮件和安排会议。WorkBench 中的任务具有挑战性，因为它们需要规划、选择工具，并且通常需要执行多项操作。如果成功执行了一项任务，则数据库值中的一个（或多个）可能会发生变化。每个任务的正确结果都是唯一且明确的，这允许进行稳健的自动评估。我们称此为关键贡献以结果为中心的评估。我们在 WorkBench 上评估了五个现有的 ReAct 代理，发现他们成功完成的任务只有 3%（Llama2-70B），表现最佳的代理（GPT-4）仅完成了 43%。我们进一步发现，代理的错误可能会导致采取错误的行动，例如将电子邮件发送给错误的人。WorkBench 揭示了代理在开展常见业务活动方面的能力存在弱点，这引发了人们对它们在高风险工作场所环境中的使用产生质疑。 WorkBench 作为免费资源公开提供，网址为 https://github.com/olly-styles/WorkBench。</li>
</ul>

<h3>Title: WIBA: What Is Being Argued? A Comprehensive Approach to Argument Mining</h3>
<ul>
<li><strong>Authors: </strong>Arman Irani, Ju Yeon Park, Kevin Esterling, Michalis Faloutsos</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00828">https://arxiv.org/abs/2405.00828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00828">https://arxiv.org/pdf/2405.00828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00828]] WIBA: What Is Being Argued? A Comprehensive Approach to Argument Mining(https://arxiv.org/abs/2405.00828)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>We propose WIBA, a novel framework and suite of methods that enable the comprehensive understanding of "What Is Being Argued" across contexts. Our approach develops a comprehensive framework that detects: (a) the existence, (b) the topic, and (c) the stance of an argument, correctly accounting for the logical dependence among the three tasks. Our algorithm leverages the fine-tuning and prompt-engineering of Large Language Models. We evaluate our approach and show that it performs well in all the three capabilities. First, we develop and release an Argument Detection model that can classify a piece of text as an argument with an F1 score between 79% and 86% on three different benchmark datasets. Second, we release a language model that can identify the topic being argued in a sentence, be it implicit or explicit, with an average similarity score of 71%, outperforming current naive methods by nearly 40%. Finally, we develop a method for Argument Stance Classification, and evaluate the capability of our approach, showing it achieves a classification F1 score between 71% and 78% across three diverse benchmark datasets. Our evaluation demonstrates that WIBA allows the comprehensive understanding of What Is Being Argued in large corpora across diverse contexts, which is of core interest to many applications in linguistics, communication, and social and computer science. To facilitate accessibility to the advancements outlined in this work, we release WIBA as a free open access platform (wiba.dev).</li>
<li><strong>摘要：</strong>我们提出了 WIBA，这是一种新颖的框架和方法套件，可以跨上下文全面理解“正在争论的内容”。我们的方法开发了一个全面的框架，可以检测：（a）存在性，（b）主题，以及（c）论证的立场，正确解释这三个任务之间的逻辑依赖性。我们的算法利用大型语言模型的微调和快速工程。我们评估了我们的方法，并表明它在所有三个功能方面都表现良好。首先，我们开发并发布了一个参数检测模型，该模型可以将一段文本分类为参数，在三个不同的基准数据集上 F1 分数在 79% 到 86% 之间。其次，我们发布了一个语言模型，可以识别句子中争论的主题，无论是隐式的还是显式的，平均相似度得分为 71%，比当前的简单方法高出近 40%。最后，我们开发了一种论证立场分类方法，并评估了我们方法的能力，表明它在三个不同的基准数据集上实现了 71% 到 78% 之间的分类 F1 分数。我们的评估表明，WIBA 可以全面理解不同背景下大型语料库中正在争论的内容，这对于语言学、传播学以及社会和计算机科学领域的许多应用来说具有核心意义。为了方便获得这项工作中概述的进步，我们将 WIBA 作为免费的开放访问平台 (wiba.dev) 发布。</li>
</ul>

<h3>Title: Math Multiple Choice Question Generation via Human-Large Language Model  Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Jaewook Lee, Digory Smith, Simon Woodhead, Andrew Lan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00864">https://arxiv.org/abs/2405.00864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00864">https://arxiv.org/pdf/2405.00864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00864]] Math Multiple Choice Question Generation via Human-Large Language Model  Collaboration(https://arxiv.org/abs/2405.00864)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Multiple choice questions (MCQs) are a popular method for evaluating students' knowledge due to their efficiency in administration and grading. Crafting high-quality math MCQs is a labor-intensive process that requires educators to formulate precise stems and plausible distractors. Recent advances in large language models (LLMs) have sparked interest in automating MCQ creation, but challenges persist in ensuring mathematical accuracy and addressing student errors. This paper introduces a prototype tool designed to facilitate collaboration between LLMs and educators for streamlining the math MCQ generation process. We conduct a pilot study involving math educators to investigate how the tool can help them simplify the process of crafting high-quality math MCQs. We found that while LLMs can generate well-formulated question stems, their ability to generate distractors that capture common student errors and misconceptions is limited. Nevertheless, a human-AI collaboration has the potential to enhance the efficiency and effectiveness of MCQ generation.</li>
<li><strong>摘要：</strong>由于管理和评分的效率，多项选择题（MCQ）是评估学生知识的流行方法。制作高质量的数学 MCQ 是一个劳动密集型的过程，需要教育工作者制定精确的主干和合理的干扰项。大型语言模型 (LLM) 的最新进展激发了人们对自动创建 MCQ 的兴趣，但在确保数学准确性和解决学生错误方面仍然存在挑战。本文介绍了一种原型工具，旨在促进法学硕士和教育工作者之间的协作，以简化数学 MCQ 生成过程。我们开展了一项涉及数学教育工作者的试点研究，以调查该工具如何帮助他们简化制作高质量数学 MCQ 的过程。我们发现，虽然法学硕士可以生成精心设计的问题主干，但他们生成捕捉常见学生错误和误解的干扰因素的能力有限。尽管如此，人类与人工智能的协作有潜力提高 MCQ 生成的效率和有效性。</li>
</ul>

<h3>Title: DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token  Sampling</h3>
<ul>
<li><strong>Authors: </strong>Shikhar Tuli, Chi-Heng Lin, Yen-Chang Hsu, Niraj K. Jha, Yilin Shen, Hongxia Jin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00888">https://arxiv.org/abs/2405.00888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00888">https://arxiv.org/pdf/2405.00888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00888]] DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token  Sampling(https://arxiv.org/abs/2405.00888)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Traditional language models operate autoregressively, i.e., they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that reduce net inference times. Our models $\textit{dynamically}$ predict multiple tokens based on their confidence in the predicted joint probability distribution. We propose a lightweight technique to train these models, leveraging the weights of traditional autoregressive counterparts. Moreover, we propose novel ways to enhance the estimated joint probability to improve text generation quality, namely co-occurrence weighted masking and adaptive thresholding. We also propose systematic qualitative and quantitative methods to rigorously test the quality of generated text for non-autoregressive generation. One of the models in our suite, DynaMo-7.3B-T3, achieves same-quality generated text as the baseline (Pythia-6.9B) while achieving 2.57$\times$ speed-up with only 5.87% and 2.67% parameter and training time overheads, respectively.</li>
<li><strong>摘要：</strong>传统语言模型以自回归方式运行，即它们一次预测一个标记。模型大小的快速爆炸导致推理时间变长。在这项工作中，我们提出了 DynaMo，这是一套可以减少网络推理时间的多标记预测语言模型。我们的模型$\textit{动态}$根据对预测联合概率分布的置信度来预测多个标记。我们提出了一种轻量级技术来训练这些模型，利用传统自回归模型的权重。此外，我们提出了增强估计联合概率以提高文本生成质量的新方法，即共现加权掩码和自适应阈值。我们还提出了系统的定性和定量方法来严格测试非自回归生成的生成文本的质量。我们套件中的模型之一 DynaMo-7.3B-T3 实现了与基线 (Pythia-6.9B) 相同质量的生成文本，同时仅用 5.87% 和 2.67% 的参数和训练就实现了 2.57$\times$ 的加速分别是时间开销。</li>
</ul>

<h3>Title: How Can I Get It Right? Using GPT to Rephrase Incorrect Trainee  Responses</h3>
<ul>
<li><strong>Authors: </strong>Jionghao Lin, Zifei Han, Danielle R. Thomas, Ashish Gurung, Shivang Gupta, Vincent Aleven, Kenneth R. Koedinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00970">https://arxiv.org/abs/2405.00970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00970">https://arxiv.org/pdf/2405.00970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00970]] How Can I Get It Right? Using GPT to Rephrase Incorrect Trainee  Responses(https://arxiv.org/abs/2405.00970)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm</a></li>
<li><strong>Abstract: </strong>One-on-one tutoring is widely acknowledged as an effective instructional method, conditioned on qualified tutors. However, the high demand for qualified tutors remains a challenge, often necessitating the training of novice tutors (i.e., trainees) to ensure effective tutoring. Research suggests that providing timely explanatory feedback can facilitate the training process for trainees. However, it presents challenges due to the time-consuming nature of assessing trainee performance by human experts. Inspired by the recent advancements of large language models (LLMs), our study employed the GPT-4 model to build an explanatory feedback system. This system identifies trainees' responses in binary form (i.e., correct/incorrect) and automatically provides template-based feedback with responses appropriately rephrased by the GPT-4 model. We conducted our study on 410 responses from trainees across three training lessons: Giving Effective Praise, Reacting to Errors, and Determining What Students Know. Our findings indicate that: 1) using a few-shot approach, the GPT-4 model effectively identifies correct/incorrect trainees' responses from three training lessons with an average F1 score of 0.84 and an AUC score of 0.85; and 2) using the few-shot approach, the GPT-4 model adeptly rephrases incorrect trainees' responses into desired responses, achieving performance comparable to that of human experts.</li>
<li><strong>摘要：</strong>一对一辅导被广泛认为是一种有效的教学方法，但前提是有合格的导师。然而，对合格导师的高需求仍然是一个挑战，通常需要培训新手导师（即实习生）以确保有效的辅导。研究表明，提供及时的解释性反馈可以促进受训者的培训过程。然而，由于人类专家评估学员表现非常耗时，因此它带来了挑战。受大型语言模型 (LLM) 最新进展的启发，我们的研究采用 GPT-4 模型来构建解释性反馈系统。该系统以二进制形式识别学员的回答（即正确/不正确），并自动提供基于模板的反馈以及由 GPT-4 模型适当改写的回答。我们对学员在三个培训课程中的 410 条回应进行了研究：给予有效表扬、对错误做出反应以及确定学生所知道的内容。我们的研究结果表明：1）使用少量样本方法，GPT-4 模型有效地识别了三个培训课程中学员的正确/错误反应，平均 F1 分数为 0.84，AUC 分数为 0.85； 2）使用少样本方法，GPT-4 模型巧妙地将受训者的错误反应改写为所需的反应，达到与人类专家相当的性能。</li>
</ul>

<h3>Title: CACTUS: Chemistry Agent Connecting Tool-Usage to Science</h3>
<ul>
<li><strong>Authors: </strong>Andrew D. McNaughton, Gautham Ramalaxmi, Agustin Kruel, Carter R. Knutson, Rohith A. Varikoti, Neeraj Kumar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, physics.chem-ph, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00972">https://arxiv.org/abs/2405.00972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00972">https://arxiv.org/pdf/2405.00972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00972]] CACTUS: Chemistry Agent Connecting Tool-Usage to Science(https://arxiv.org/abs/2405.00972)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable potential in various domains, but they often lack the ability to access and reason over domain-specific knowledge and tools. In this paper, we introduced CACTUS (Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that integrates cheminformatics tools to enable advanced reasoning and problem-solving in chemistry and molecular discovery. We evaluate the performance of CACTUS using a diverse set of open-source LLMs, including Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of thousands of chemistry questions. Our results demonstrate that CACTUS significantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b models achieving the highest accuracy regardless of the prompting strategy used. Moreover, we explore the impact of domain-specific prompting and hardware configurations on model performance, highlighting the importance of prompt engineering and the potential for deploying smaller models on consumer-grade hardware without significant loss in accuracy. By combining the cognitive capabilities of open-source LLMs with domain-specific tools, CACTUS can assist researchers in tasks such as molecular property prediction, similarity searching, and drug-likeness assessment. Furthermore, CACTUS represents a significant milestone in the field of cheminformatics, offering an adaptable tool for researchers engaged in chemistry and molecular discovery. By integrating the strengths of open-source LLMs with domain-specific tools, CACTUS has the potential to accelerate scientific advancement and unlock new frontiers in the exploration of novel, effective, and safe therapeutic candidates, catalysts, and materials. Moreover, CACTUS's ability to integrate with automated experimentation platforms and make data-driven decisions in real time opens up new possibilities for autonomous discovery.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在各个领域都显示出巨大的潜力，但它们往往缺乏访问和推理特定领域知识和工具的能力。在本文中，我们介绍了 CACTUS（化学代理连接工具 - 使用科学），这是一种基于法学硕士的代理，它集成了化学信息学工具，以实现化学和分子发现中的高级推理和问题解决。我们使用多种开源 LLM（包括 Gemma-7b、Falcon-7b、MPT-7b、Llama2-7b 和 Mistral-7b）以数千个化学问题为基准，评估 CACTUS 的性能。我们的结果表明，CACTUS 的性能显着优于基线 LLM，无论使用何种提示策略，Gemma-7b 和 Mistral-7b 模型都实现了最高的准确度。此外，我们探讨了特定领域的提示和硬件配置对模型性能的影响，强调了提示工程的重要性以及在消费级硬件上部署较小模型而不显着损失准确性的潜力。通过将开源法学硕士的认知能力与特定领域的工具相结合，CACTUS 可以协助研究人员完成分子特性预测、相似性搜索和药物相似性评估等任务。此外，CACTUS 代表了化学信息学领域的一个重要里程碑，为从事化学和分子发现的研究人员提供了一个适应性强的工具。通过将开源法学硕士的优势与特定领域的工具相结合，CACTUS 有潜力加速科学进步，并在探索新颖、有效和安全的候选治疗药物、催化剂和材料方面开辟新领域。此外，CACTUS 能够与自动化实验平台集成并实时做出数据驱动的决策，为自主发现开辟了新的可能性。</li>
</ul>

<h3>Title: On the Evaluation of Machine-Generated Reports</h3>
<ul>
<li><strong>Authors: </strong>James Mayfield, Eugene Yang, Dawn Lawrie, Sean MacAvaney, Paul McNamee, Douglas W. Oard, Luca Soldaini, Ian Soboroff, Orion Weller, Efsun Kayi, Kate Sanders, Marc Mason, Noah Hibbler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00982">https://arxiv.org/abs/2405.00982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00982">https://arxiv.org/pdf/2405.00982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00982]] On the Evaluation of Machine-Generated Reports(https://arxiv.org/abs/2405.00982)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have enabled new ways to satisfy information needs. Although great strides have been made in applying them to settings like document ranking and short-form text generation, they still struggle to compose complete, accurate, and verifiable long-form reports. Reports with these qualities are necessary to satisfy the complex, nuanced, or multi-faceted information needs of users. In this perspective paper, we draw together opinions from industry and academia, and from a variety of related research areas, to present our vision for automatic report generation, and -- critically -- a flexible framework by which such reports can be evaluated. In contrast with other summarization tasks, automatic report generation starts with a detailed description of an information need, stating the necessary background, requirements, and scope of the report. Further, the generated reports should be complete, accurate, and verifiable. These qualities, which are desirable -- if not required -- in many analytic report-writing settings, require rethinking how to build and evaluate systems that exhibit these qualities. To foster new efforts in building these systems, we present an evaluation framework that draws on ideas found in various evaluations. To test completeness and accuracy, the framework uses nuggets of information, expressed as questions and answers, that need to be part of any high-quality generated report. Additionally, evaluation of citations that map claims made in the report to their source documents ensures verifiability.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 提供了满足信息需求的新方法。尽管在将它们应用于文档排名和短格式文本生成等设置方面已经取得了巨大进步，但它们仍然难以撰写完整、准确且可验证的长格式报告。具有这些品质的报告对于满足用户复杂、细致或多方面的信息需求是必要的。在这篇展望论文中，我们汇集了来自工业界和学术界以及各个相关研究领域的意见，提出了我们对自动报告生成的愿景，以及至关重要的一个可以评估此​​类报告的灵活框架。与其他摘要任务相比，自动报告生成从信息需求的详细描述开始，说明报告的必要背景、要求和范围。此外，生成的报告应该完整、准确且可验证。这些品质在许多分析报告撰写环境中是可取的（如果不是必需的），需要重新思考如何构建和评估展现这些品质的系统。为了促进构建这些系统的新努力，我们提出了一个评估框架，该框架借鉴了各种评估中的想法。为了测试完整性和准确性，该框架使用以问题和答案表示的信息块，这些信息需要成为任何高质量生成报告的一部分。此外，对将报告中的主张映射到其源文件的引用进行评估可确保可验证性。</li>
</ul>

<h3>Title: Context-Aware Clustering using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sindhu Tipirneni, Ravinarayana Adkathimar, Nurendra Choudhary, Gaurush Hiranandani, Rana Ali Amjad, Vassilis N. Ioannidis, Changhe Yuan, Chandan K. Reddy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.00988">https://arxiv.org/abs/2405.00988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.00988">https://arxiv.org/pdf/2405.00988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.00988]] Context-Aware Clustering using Large Language Models(https://arxiv.org/abs/2405.00988)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of Large Language Models (LLMs) in text understanding and generation, their potential for text clustering tasks remains underexplored. We observed that powerful closed-source LLMs provide good quality clusterings of entity sets but are not scalable due to the massive compute power required and the associated costs. Thus, we propose CACTUS (Context-Aware ClusTering with aUgmented triplet losS), a systematic approach that leverages open-source LLMs for efficient and effective supervised clustering of entity subsets, particularly focusing on text-based entities. Existing text clustering methods fail to effectively capture the context provided by the entity subset. Moreover, though there are several language modeling based approaches for clustering, very few are designed for the task of supervised clustering. This paper introduces a novel approach towards clustering entity subsets using LLMs by capturing context via a scalable inter-entity attention mechanism. We propose a novel augmented triplet loss function tailored for supervised clustering, which addresses the inherent challenges of directly applying the triplet loss to this problem. Furthermore, we introduce a self-supervised clustering task based on text augmentation techniques to improve the generalization of our model. For evaluation, we collect ground truth clusterings from a closed-source LLM and transfer this knowledge to an open-source LLM under the supervised clustering framework, allowing a faster and cheaper open-source model to perform the same task. Experiments on various e-commerce query and product clustering datasets demonstrate that our proposed approach significantly outperforms existing unsupervised and supervised baselines under various external clustering evaluation metrics.</li>
<li><strong>摘要：</strong>尽管大型语言模型（LLM）在文本理解和生成方面取得了显着的成功，但它们在文本聚类任务中的潜力仍未得到充分开发。我们观察到，强大的闭源法学硕士可以提供高质量的实体集聚类，但由于需要大量的计算能力和相关成本，因此无法扩展。因此，我们提出了 CACTUS（具有增强的三元组损失的上下文感知聚类），这是一种利用开源法学硕士对实体子集进行高效且有效的监督聚类的系统方法，特别关注基于文本的实体。现有的文本聚类方法无法有效捕获实体子集提供的上下文。此外，尽管有几种基于语言建模的聚类方法，但很少有是为监督聚类任务而设计的。本文介绍了一种使用 LLM 聚类实体子集的新方法，通过可扩展的实体间注意机制捕获上下文。我们提出了一种专为监督聚类量身定制的新型增强三元组损失函数，它解决了直接将三元组损失应用于此问题的固有挑战。此外，我们引入了基于文本增强技术的自监督聚类任务，以提高模型的泛化能力。为了进行评估，我们从闭源 LLM 收集真实聚类，并将这些知识转移到监督聚类框架下的开源 LLM，从而允许更快、更便宜的开源模型来执行相同的任务。对各种电子商务查询和产品聚类数据集的实验表明，我们提出的方法在各种外部聚类评估指标下显着优于现有的无监督和监督基线。</li>
</ul>

<h3>Title: UniGen: Universal Domain Generalization for Sentiment Classification via  Zero-shot Dataset Generation</h3>
<ul>
<li><strong>Authors: </strong>Juhwan Choi, Yeonghwa Kim, Seunguk Yu, JungMin Yun, YoungBin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01022">https://arxiv.org/abs/2405.01022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01022">https://arxiv.org/pdf/2405.01022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01022]] UniGen: Universal Domain Generalization for Sentiment Classification via  Zero-shot Dataset Generation(https://arxiv.org/abs/2405.01022)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Although pre-trained language models have exhibited great flexibility and versatility with prompt-based few-shot learning, they suffer from the extensive parameter size and limited applicability for inference. Recent studies have suggested that PLMs be used as dataset generators and a tiny task-specific model be trained to achieve efficient inference. However, their applicability to various domains is limited because they tend to generate domain-specific datasets. In this work, we propose a novel approach to universal domain generalization that generates a dataset regardless of the target domain. This allows for generalization of the tiny task model to any domain that shares the label space, thus enhancing the real-world applicability of the dataset generation paradigm. Our experiments indicate that the proposed method accomplishes generalizability across various domains while using a parameter set that is orders of magnitude smaller than PLMs.</li>
<li><strong>摘要：</strong>尽管预训练的语言模型通过基于提示的小样本学习表现出了极大的灵活性和多功能性，但它们受到参数大小广泛和推理适用性有限的困扰。最近的研究表明，PLM 可用作数据集生成器，并训练一个微型特定任务模型以实现高效推理。然而，它们对各个领域的适用性受到限制，因为它们倾向于生成特定于领域的数据集。在这项工作中，我们提出了一种通用域泛化的新方法，无论目标域如何，都可以生成数据集。这允许将微小任务模型推广到共享标签空间的任何域，从而增强数据集生成范例的实际适用性。我们的实验表明，所提出的方法在使用比 PLM 小几个数量级的参数集时实现了跨不同领域的通用性。</li>
</ul>

<h3>Title: Efficient Data Generation for Source-grounded Information-seeking  Dialogs: A Use Case for Meeting Transcripts</h3>
<ul>
<li><strong>Authors: </strong>Lotem Golany, Filippo Galgani, Maya Mamo, Nimrod Parasol, Omer Vandsburger, Nadav Bar, Ido Dagan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01121">https://arxiv.org/abs/2405.01121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01121">https://arxiv.org/pdf/2405.01121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01121]] Efficient Data Generation for Source-grounded Information-seeking  Dialogs: A Use Case for Meeting Transcripts(https://arxiv.org/abs/2405.01121)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, agent</a></li>
<li><strong>Abstract: </strong>Existing methods for creating source-grounded information-seeking dialog datasets are often costly and hard to implement due to their sole reliance on human annotators. We propose combining large language models (LLMs) prompting with human expertise for more efficient and reliable data generation. Instead of the labor-intensive Wizard-of-Oz (WOZ) method, where two annotators generate a dialog from scratch, role-playing agent and user, we use LLM generation to simulate the two roles. Annotators then verify the output and augment it with attribution data. We demonstrate our method by constructing MISeD -- Meeting Information Seeking Dialogs dataset -- the first information-seeking dialog dataset focused on meeting transcripts. Models finetuned with MISeD demonstrate superior performance on our test set, as well as on a novel fully-manual WOZ test set and an existing query-based summarization benchmark, suggesting the utility of our approach.</li>
<li><strong>摘要：</strong>用于创建基于源的信息搜索对话数据集的现有方法通常成本高昂且难以实现，因为它们仅依赖于人类注释者。我们建议将大型语言模型 (LLM) 提示与人类专业知识相结合，以实现更高效、更可靠的数据生成。我们使用 LLM 生成来模拟这两个角色，而不是劳动密集型的 Wizard-of-Oz (WOZ) 方法，即两个注释者从头开始生成对话、角色扮演代理和用户。然后，注释者验证输出并使用归因数据对其进行扩充。我们通过构建 MISeD（会议信息寻求对话数据集）来演示我们的方法，这是第一个专注于会议记录的信息寻求对话数据集。使用 MISeD 微调的模型在我们的测试集以及新颖的全手动 WOZ 测试集和现有的基于查询的摘要基准上展示了卓越的性能，表明我们的方法的实用性。</li>
</ul>

<h3>Title: TartuNLP at EvaLatin 2024: Emotion Polarity Detection</h3>
<ul>
<li><strong>Authors: </strong>Aleksei Dorkin, Kairit Sirts</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01159">https://arxiv.org/abs/2405.01159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01159">https://arxiv.org/pdf/2405.01159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01159]] TartuNLP at EvaLatin 2024: Emotion Polarity Detection(https://arxiv.org/abs/2405.01159)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm</a></li>
<li><strong>Abstract: </strong>This paper presents the TartuNLP team submission to EvaLatin 2024 shared task of the emotion polarity detection for historical Latin texts. Our system relies on two distinct approaches to annotating training data for supervised learning: 1) creating heuristics-based labels by adopting the polarity lexicon provided by the organizers and 2) generating labels with GPT4. We employed parameter efficient fine-tuning using the adapters framework and experimented with both monolingual and cross-lingual knowledge transfer for training language and task adapters. Our submission with the LLM-generated labels achieved the overall first place in the emotion polarity detection task. Our results show that LLM-based annotations show promising results on texts in Latin.</li>
<li><strong>摘要：</strong>本文介绍了 TartuNLP 团队提交给 EvaLatin 2024 的共享任务：历史拉丁语文本的情感极性检测。我们的系统依赖于两种不同的方法来注释监督学习的训练数据：1）通过采用组织者提供的极性词典创建基于启发式的标签，2）使用 GPT4 生成标签。我们使用适配器框架进行参数高效微调，并尝试使用单语言和跨语言知识迁移来训练语言和任务适配器。我们提交的法学硕士生成的标签在情绪极性检测任务中获得了总体第一名。我们的结果表明，基于法学硕士的注释在拉丁语文本上显示出有希望的结果。</li>
</ul>

<h3>Title: Prompt engineering paradigms for medical applications: scoping review  and recommendations for better practices</h3>
<ul>
<li><strong>Authors: </strong>Jamil Zaghir, Marco Naguib, Mina Bjelogrlic, Aurélie Névéol, Xavier Tannier, Christian Lovis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01249">https://arxiv.org/abs/2405.01249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01249">https://arxiv.org/pdf/2405.01249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01249]] Prompt engineering paradigms for medical applications: scoping review  and recommendations for better practices(https://arxiv.org/abs/2405.01249)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt, chat, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Prompt engineering is crucial for harnessing the potential of large language models (LLMs), especially in the medical domain where specialized terminology and phrasing is used. However, the efficacy of prompt engineering in the medical domain remains to be explored. In this work, 114 recent studies (2022-2024) applying prompt engineering in medicine, covering prompt learning (PL), prompt tuning (PT), and prompt design (PD) are reviewed. PD is the most prevalent (78 articles). In 12 papers, PD, PL, and PT terms were used interchangeably. ChatGPT is the most commonly used LLM, with seven papers using it for processing sensitive clinical data. Chain-of-Thought emerges as the most common prompt engineering technique. While PL and PT articles typically provide a baseline for evaluating prompt-based approaches, 64% of PD studies lack non-prompt-related baselines. We provide tables and figures summarizing existing work, and reporting recommendations to guide future research contributions.</li>
<li><strong>摘要：</strong>快速工程对于利用大语言模型 (LLM) 的潜力至关重要，尤其是在使用专业术语和措辞的医学领域。然而，即时工程在医学领域的功效仍有待探索。在这项工作中，回顾了 114 项最近（2022-2024）在医学中应用即时工程的研究，涵盖即时学习（PL）、即时调整（PT）和即时设计（PD）。 PD 最为普遍（78 篇文章）。在 12 篇论文中，PD、PL 和 PT 术语可以互换使用。 ChatGPT 是最常用的法学硕士，有七篇论文使用它来处理敏感的临床数据。思想链成为最常见的即时工程技术。虽然 PL 和 PT 文章通常为评估基于提示的方法提供基线，但 64% 的 PD 研究缺乏非提示相关的基线。我们提供总结现有工作的表格和数字，并报告建议以指导未来的研究贡献。</li>
</ul>

<h3>Title: Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine  Translation</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Tetsuro Morimura, Ukyo Honda, Daisuke Kawahara</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01280">https://arxiv.org/abs/2405.01280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01280">https://arxiv.org/pdf/2405.01280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01280]] Reinforcement Learning for Edit-Based Non-Autoregressive Neural Machine  Translation(https://arxiv.org/abs/2405.01280)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Non-autoregressive (NAR) language models are known for their low latency in neural machine translation (NMT). However, a performance gap exists between NAR and autoregressive models due to the large decoding space and difficulty in capturing dependency between target words accurately. Compounding this, preparing appropriate training data for NAR models is a non-trivial task, often exacerbating exposure bias. To address these challenges, we apply reinforcement learning (RL) to Levenshtein Transformer, a representative edit-based NAR model, demonstrating that RL with self-generated data can enhance the performance of edit-based NAR models. We explore two RL approaches: stepwise reward maximization and episodic reward maximization. We discuss the respective pros and cons of these two approaches and empirically verify them. Moreover, we experimentally investigate the impact of temperature setting on performance, confirming the importance of proper temperature setting for NAR models' training.</li>
<li><strong>摘要：</strong>非自回归 (NAR) 语言模型以其在神经机器翻译 (NMT) 中的低延迟而闻名。然而，由于解码空间较大且难以准确捕获目标词之间的依赖关系，NAR 和自回归模型之间存在性能差距。更复杂的是，为 NAR 模型准备适当的训练数据是一项艰巨的任务，通常会加剧暴露偏差。为了应对这些挑战，我们将强化学习（RL）应用于具有代表性的基于编辑的 NAR 模型 Levenshtein Transformer，证明具有自我生成数据的 RL 可以增强基于编辑的 NAR 模型的性能。我们探索两种强化学习方法：逐步奖励最大化和情景奖励最大化。我们讨论这两种方法各自的优缺点并进行实证验证。此外，我们通过实验研究了温度设置对性能的影响，证实了适当的温度设置对于 NAR 模型训练的重要性。</li>
</ul>

<h3>Title: Low-resource speech recognition and dialect identification of Irish in a  multi-task framework</h3>
<ul>
<li><strong>Authors: </strong>Liam Lonergan, Mengjie Qian, Neasa Ní Chiaráin, Christer Gobl, Ailbhe Ní Chasaide</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01293">https://arxiv.org/abs/2405.01293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01293">https://arxiv.org/pdf/2405.01293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01293]] Low-resource speech recognition and dialect identification of Irish in a  multi-task framework(https://arxiv.org/abs/2405.01293)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>This paper explores the use of Hybrid CTC/Attention encoder-decoder models trained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech recognition (ASR) and dialect identification (DID). Results are compared to the current best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN). An optimal InterCTC setting is initially established using a Conformer encoder. This setting is then used to train a model with an E-branchformer encoder and the performance of both architectures are compared. A multi-task fine-tuning approach is adopted for language model (LM) shallow fusion. The experiments yielded an improvement in DID accuracy of 10.8% relative to a baseline ECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task approach emerges as a promising strategy for Irish low-resource ASR and DID.</li>
<li><strong>摘要：</strong>本文探讨了使用中级 CTC (InterCTC) 训练的混合 CTC/Attention 编码器-解码器模型进行爱尔兰语（盖尔语）低资源语音识别 (ASR) 和方言识别 (DID)。将结果与当前针对 ASR (TDNN-HMM) 和 DID (ECAPA-TDNN) 训练的最佳性能模型进行比较。最佳 InterCTC 设置最初是使用 Conformer 编码器建立的。然后使用此设置来训练带有 E-branchformer 编码器的模型，并比较两种架构的性能。语言模型（LM）浅层融合采用多任务微调方法。相对于基线 ECAPA-TDNN，实验的 DID 准确率提高了 10.8%，WER 性能接近 TDNN-HMM 模型。这种多任务方法对于爱尔兰资源匮乏的 ASR 和 DID 来说是一种很有前途的策略。</li>
</ul>

<h3>Title: The Effectiveness of LLMs as Annotators: A Comparative Overview and  Empirical Analysis of Direct Representation</h3>
<ul>
<li><strong>Authors: </strong>Maja Pavlovic, Massimo Poesio</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01299">https://arxiv.org/abs/2405.01299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01299">https://arxiv.org/pdf/2405.01299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01299]] The Effectiveness of LLMs as Annotators: A Comparative Overview and  Empirical Analysis of Direct Representation(https://arxiv.org/abs/2405.01299)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful support tools across various natural language tasks and a range of application domains. Recent studies focus on exploring their capabilities for data annotation. This paper provides a comparative overview of twelve studies investigating the potential of LLMs in labelling data. While the models demonstrate promising cost and time-saving benefits, there exist considerable limitations, such as representativeness, bias, sensitivity to prompt variations and English language preference. Leveraging insights from these studies, our empirical analysis further examines the alignment between human and GPT-generated opinion distributions across four subjective datasets. In contrast to the studies examining representation, our methodology directly obtains the opinion distribution from GPT. Our analysis thereby supports the minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 已成为跨各种自然语言任务和一系列应用领域的强大支持工具。最近的研究重点是探索它们的数据注释能力。本文对 12 项研究法学硕士在标记数据方面的潜力进行了比较概述。虽然这些模型显示出有希望的成本和时间节省优势，但仍存在相当大的局限性，例如代表性、偏差、对提示变化的敏感性和英语语言偏好。利用这些研究的见解，我们的实证分析进一步检验了四个主观数据集中人类和 GPT 生成的意见分布之间的一致性。与考察代表性的研究相反，我们的方法直接从 GPT 获取意见分布。因此，我们的分析支持了在评估数据注释任务时考虑不同观点的少数研究，并强调了在这个方向上进一步研究的必要性。</li>
</ul>

<h3>Title: The Power of Question Translation Training in Multilingual Reasoning:  Broadened Scope and Deepened Insights</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Zhu, Shujian Huang, Fei Yuan, Cheng Chen, Jiajun Chen, Alexandra Birch</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01345">https://arxiv.org/abs/2405.01345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01345">https://arxiv.org/pdf/2405.01345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01345]] The Power of Question Translation Training in Multilingual Reasoning:  Broadened Scope and Deepened Insights(https://arxiv.org/abs/2405.01345)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Bridging the significant gap between large language model's English and non-English performance presents a great challenge. While some previous studies attempt to mitigate this gap with translated training data, the recently proposed question alignment approach leverages the model's English expertise to improve multilingual performance with minimum usage of expensive, error-prone translation. In this paper, we explore how broadly this method can be applied by examining its effects in reasoning with executable code and reasoning with common sense. We also explore how to apply this approach efficiently to extremely large language models using proxy-tuning. Experiment results on multilingual reasoning benchmarks mGSM, mSVAMP and xCSQA demonstrate that the question alignment approach can be used to boost multilingual performance across diverse reasoning scenarios, model families, and sizes. For instance, when applied to the LLaMA2 models, our method brings an average accuracy improvements of 12.2% on mGSM even with the 70B model. To understand the mechanism of its success, we analyze representation space, chain-of-thought and translation data scales, which reveals how question translation training strengthens language alignment within LLMs and shapes their working patterns.</li>
<li><strong>摘要：</strong>缩小大型语言模型的英语和非英语性能之间的巨大差距是一个巨大的挑战。虽然之前的一些研究试图通过翻译的训练数据来缩小这一差距，但最近提出的问题对齐方法利用模型的英语专业知识来提高多语言性能，同时最大限度地减少昂贵且容易出错的翻译的使用。在本文中，我们通过检查其在可执行代码推理和常识推理中的效果，探讨了该方法的应用范围。我们还探索如何使用代理调优将此方法有效地应用于超大型语言模型。多语言推理基准 mGSM、mSVAMP 和 xCSQA 上的实验结果表明，问题对齐方法可用于提高跨不同推理场景、模型系列和规模的多语言性能。例如，当应用于 LLaMA2 模型时，即使使用 70B 模型，我们的方法在 mGSM 上的平均准确度也提高了 12.2%。为了理解其成功的机制，我们分析了表征空间、思维链和翻译数据规模，揭示了问题翻译训练如何加强法学硕士内的语言一致性并塑造其工作模式。</li>
</ul>

<h3>Title: GAIA: A General AI Assistant for Intelligent Accelerator Operations</h3>
<ul>
<li><strong>Authors: </strong>Frank Mayet</a></li>
<li><strong>Subjects: </strong>cs.CL, physics.acc-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01359">https://arxiv.org/abs/2405.01359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01359">https://arxiv.org/pdf/2405.01359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01359]] GAIA: A General AI Assistant for Intelligent Accelerator Operations(https://arxiv.org/abs/2405.01359)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, retrieval augmented generation</a></li>
<li><strong>Abstract: </strong>Large-scale machines like particle accelerators are usually run by a team of experienced operators. In case of a particle accelerator, these operators possess suitable background knowledge on both accelerator physics and the technology comprising the machine. Due to the complexity of the machine, particular subsystems of the machine are taken care of by experts, who the operators can turn to. In this work the reasoning and action (ReAct) prompting paradigm is used to couple an open-weights large language model (LLM) with a high-level machine control system framework and other tools, e.g. the electronic logbook or machine design documentation. By doing so, a multi-expert retrieval augmented generation (RAG) system is implemented, which assists operators in knowledge retrieval tasks, interacts with the machine directly if needed, or writes high level control system scripts. This consolidation of expert knowledge and machine interaction can simplify and speed up machine operation tasks for both new and experienced human operators.</li>
<li><strong>摘要：</strong>像粒子加速器这样的大型机器通常由经验丰富的操作员团队运行。对于粒子加速器，这些操作员拥有加速器物理和机器组成技术方面的适当背景知识。由于机器的复杂性，机器的特定子系统由专家负责，操作员可以向专家求助。在这项工作中，推理和行动（ReAct）提示范式用于将开放权重大型语言模型（LLM）与高级机器控制系统框架和其他工具（例如电子日志或机器设计文档。通过这样做，实现了多专家检索增强生成（RAG）系统，该系统可帮助操作员执行知识检索任务，在需要时直接与机器交互，或编写高级控制系统脚本。这种专家知识和机器交互的整合可以简化和加快新的和经验丰富的操作员的机器操作任务。</li>
</ul>

<h3>Title: Verification and Refinement of Natural Language Explanations through  LLM-Symbolic Theorem Proving</h3>
<ul>
<li><strong>Authors: </strong>Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01379">https://arxiv.org/abs/2405.01379</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01379">https://arxiv.org/pdf/2405.01379</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01379]] Verification and Refinement of Natural Language Explanations through  LLM-Symbolic Theorem Proving(https://arxiv.org/abs/2405.01379)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Natural language explanations have become a proxy for evaluating explainable and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that augments a TP with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of human-annotated explanations of variable complexity in different domains.</li>
<li><strong>摘要：</strong>自然语言解释已成为评估可解释的多步骤自然语言推理（NLI）模型的代理。然而，评估 NLI 解释的有效性具有挑战性，因为它通常涉及适当数据集的众包，这是一个耗时且容易出现逻辑错误的过程。为了解决现有的局限性，本文通过集成大型语言模型（LLM）和定理证明器（TP）来研究自然语言解释的验证和细化。具体来说，我们提出了一个名为 Explanation-Refiner 的神经符号框架，它通过 LLM 增强 TP，以生成和形式化解释性句子，并为 NLI 提出潜在的推理策略。反过来，TP 用于为解释的逻辑有效性提供正式保证，并为后续改进生成反馈。我们演示了如何联合使用 Explanation-Refiner 来评估最先进的法学硕士的解释推理、自动形式化和纠错机制，以及如何自动提高不同领域中变量复杂性的人工注释解释的质量。</li>
</ul>

<h3>Title: WildChat: 1M ChatGPT Interaction Logs in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, Yuntian Deng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01470">https://arxiv.org/abs/2405.01470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01470">https://arxiv.org/pdf/2405.01470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01470]] WildChat: 1M ChatGPT Interaction Logs in the Wild(https://arxiv.org/abs/2405.01470)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, prompt, chat</a></li>
<li><strong>Abstract: </strong>Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to ChatGPT for online users in exchange for their affirmative, consensual opt-in to anonymously collect their chat transcripts and request headers. From this, we compiled WildChat, a corpus of 1 million user-ChatGPT conversations, which consists of over 2.5 million interaction turns. We compare WildChat with other popular user-chatbot interaction datasets, and find that our dataset offers the most diverse user prompts, contains the largest number of languages, and presents the richest variety of potentially toxic use-cases for researchers to study. In addition to timestamped chat transcripts, we enrich the dataset with demographic data, including state, country, and hashed IP addresses, alongside request headers. This augmentation allows for more detailed analysis of user behaviors across different geographical regions and temporal dimensions. Finally, because it captures a broad range of use cases, we demonstrate the dataset's potential utility in fine-tuning instruction-following models. WildChat is released at https://wildchat.allen.ai under AI2 ImpACT Licenses.</li>
<li><strong>摘要：</strong>GPT-4 和 ChatGPT 等聊天机器人现在正在为数百万用户提供服务。尽管它们被广泛使用，但仍然缺乏公共数据集来展示这些工具在实践中如何被用户群体使用。为了弥补这一差距，我们为在线用户提供了免费访问 ChatGPT 的机会，以换取他们肯定、同意的选择，以匿名方式收集他们的聊天记录和请求标头。据此，我们编译了 WildChat，这是一个包含 100 万个用户 ChatGPT 对话的语料库，其中包含超过 250 万个交互回合。我们将 WildChat 与其他流行的用户聊天机器人交互数据集进行比较，发现我们的数据集提供了最多样化的用户提示，包含最多数量的语言，并提供了最丰富的潜在有毒用例供研究人员研究。除了带时间戳的聊天记录之外，我们还使用人口统计数据丰富了数据集，包括州、国家/地区和散列 IP 地址以及请求标头。这种增强可以对不同地理区域和时间维度的用户行为进行更详细的分析。最后，由于它捕获了广泛的用例，我们展示了该数据集在微调指令跟踪模型方面的潜在效用。 WildChat 在 AI2 ImpACT 许可证下于 https://wildchat.allen.ai 发布。</li>
</ul>

<h3>Title: V-FLUTE: Visual Figurative Language Understanding with Textual  Explanations</h3>
<ul>
<li><strong>Authors: </strong>Arkadiy Saakyan, Shreyas Kulkarni, Tuhin Chakrabarty, Smaranda Muresan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01474">https://arxiv.org/abs/2405.01474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01474">https://arxiv.org/pdf/2405.01474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01474]] V-FLUTE: Visual Figurative Language Understanding with Textual  Explanations(https://arxiv.org/abs/2405.01474)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Large Vision-Language models (VLMs) have demonstrated strong reasoning capabilities in tasks requiring a fine-grained understanding of literal images and text, such as visual question-answering or visual entailment. However, there has been little exploration of these models' capabilities when presented with images and captions containing figurative phenomena such as metaphors or humor, the meaning of which is often implicit. To close this gap, we propose a new task and a high-quality dataset: Visual Figurative Language Understanding with Textual Explanations (V-FLUTE). We frame the visual figurative language understanding problem as an explainable visual entailment task, where the model has to predict whether the image (premise) entails a claim (hypothesis) and justify the predicted label with a textual explanation. Using a human-AI collaboration framework, we build a high-quality dataset, V-FLUTE, that contains 6,027 <image, claim, label, explanation> instances spanning five diverse multimodal figurative phenomena: metaphors, similes, idioms, sarcasm, and humor. The figurative phenomena can be present either in the image, the caption, or both. We further conduct both automatic and human evaluations to assess current VLMs' capabilities in understanding figurative phenomena.</li>
<li><strong>摘要：</strong>大型视觉语言模型（VLM）在需要对文字图像和文本进行细粒度理解的任务中表现出了强大的推理能力，例如视觉问答或视觉蕴涵。然而，当呈现包含隐喻或幽默等比喻现象的图像和标题时，很少有人探索这些模型的能力，而其含义通常是隐含的。为了弥补这一差距，我们提出了一项新任务和高质量数据集：带有文本解释的视觉形象语言理解（V-FLUTE）。我们将视觉比喻语言理解问题构建为可解释的视觉蕴含任务，其中模型必须预测图像（前提）是否蕴涵主张（假设），并用文本解释证明预测标签的合理性。使用人类-AI 协作框架，我们构建了一个高质量的数据集 V-FLUTE，其中包含 6,027 个<图像、主张、标签、解释>实例，涵盖五种不同的多模态比喻现象：隐喻、明喻、习语、讽刺和幽默。比喻现象可以出现在图像、标题或两者中。我们进一步进行自动和人工评估，以评估当前 VLM 理解比喻现象的能力。</li>
</ul>

<h3>Title: NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin, Ashwath Aithal, Oleksii Kuchaiev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01481">https://arxiv.org/abs/2405.01481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01481">https://arxiv.org/pdf/2405.01481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01481]] NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment(https://arxiv.org/abs/2405.01481)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Aligning Large Language Models (LLMs) with human values and preferences is essential for making them helpful and safe. However, building efficient tools to perform alignment can be challenging, especially for the largest and most competent LLMs which often contain tens or hundreds of billions of parameters. We create NeMo-Aligner, a toolkit for model alignment that can efficiently scale to using hundreds of GPUs for training. NeMo-Aligner comes with highly optimized and scalable implementations for major paradigms of model alignment such as: Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), SteerLM, and Self-Play Fine-Tuning (SPIN). Additionally, our toolkit supports running most of the alignment techniques in a Parameter Efficient Fine-Tuning (PEFT) setting. NeMo-Aligner is designed for extensibility, allowing support for other alignment techniques with minimal effort. It is open-sourced with Apache 2.0 License and we invite community contributions at https://github.com/NVIDIA/NeMo-Aligner</li>
<li><strong>摘要：</strong>让大型语言模型 (LLM) 与人类价值观和偏好保持一致对于使其有用且安全至关重要。然而，构建高效的工具来执行对齐可能具有挑战性，特别是对于通常包含数百或数千亿参数的最大和最有能力的法学硕士而言。我们创建了 NeMo-Aligner，这是一个用于模型对齐的工具包，可以有效地扩展到使用数百个 GPU 进行训练。 NeMo-Aligner 为模型对齐的主要范例提供了高度优化和可扩展的实现，例如：来自人类反馈的强化学习 (RLHF)、直接偏好优化 (DPO)、SteerLM 和自玩微调 (SPIN)。此外，我们的工具包支持在参数高效微调 (PEFT) 设置中运行大多数对齐技术。 NeMo-Aligner 专为可扩展性而设计，可以轻松支持其他对齐技术。它是开源的，具有 Apache 2.0 许可证，我们邀请社区贡献 https://github.com/NVIDIA/NeMo-Aligner</li>
</ul>

<h3>Title: Controllable Text Generation in the Instruction-Tuning Era</h3>
<ul>
<li><strong>Authors: </strong>Dhananjay Ashok, Barnabas Poczos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01490">https://arxiv.org/abs/2405.01490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01490">https://arxiv.org/pdf/2405.01490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01490]] Controllable Text Generation in the Instruction-Tuning Era(https://arxiv.org/abs/2405.01490)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>While most research on controllable text generation has focused on steering base Language Models, the emerging instruction-tuning and prompting paradigm offers an alternate approach to controllability. We compile and release ConGenBench, a testbed of 17 different controllable generation tasks, using a subset of it to benchmark the performance of 9 different baselines and methods on Instruction-tuned Language Models. To our surprise, we find that prompting-based approaches outperform controllable text generation methods on most datasets and tasks, highlighting a need for research on controllable text generation with Instruction-tuned Language Models in specific. Prompt-based approaches match human performance on most stylistic tasks while lagging on structural tasks, foregrounding a need to study more varied constraints and more challenging stylistic tasks. To facilitate such research, we provide an algorithm that uses only a task dataset and a Large Language Model with in-context capabilities to automatically generate a constraint dataset. This method eliminates the fields dependence on pre-curated constraint datasets, hence vastly expanding the range of constraints that can be studied in the future.</li>
<li><strong>摘要：</strong>虽然大多数关于可控文本生成的研究都集中在引导基础语言模型上，但新兴的指令调整和提示范式提供了另一种可控性方法。我们编译并发布了 ConGenBench，这是 17 种不同的可控生成任务的测试平台，使用它的一个子集来对指令调整语言模型上 9 种不同基线和方法的性能进行基准测试。令我们惊讶的是，我们发现基于提示的方法在大多数数据集和任务上都优于可控文本生成方法，这突出表明需要研究特定于指令调整语言模型的可控文本生成。基于提示的方法与人类在大多数风格任务上的表现相匹配，但在结构任务上却滞后，这凸显了研究更多样的约束和更具挑战性的风格任务的需要。为了促进此类研究，我们提供了一种算法，该算法仅使用任务数据集和具有上下文功能的大型语言模型来自动生成约束数据集。该方法消除了字段对预先策划的约束数据集的依赖，从而极大地扩展了未来可以研究的约束范围。</li>
</ul>

<h3>Title: Analyzing the Role of Semantic Representations in the Era of Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhijing Jin, Yuen Chen, Fernando Gonzalez, Jiarui Liu, Jiayi Zhang, Julian Michael, Bernhard Schölkopf, Mona Diab</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01502">https://arxiv.org/abs/2405.01502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01502">https://arxiv.org/pdf/2405.01502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01502]] Analyzing the Role of Semantic Representations in the Era of Large  Language Models(https://arxiv.org/abs/2405.01502)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt, chain-of-thought</a></li>
<li><strong>Abstract: </strong>Traditionally, natural language processing (NLP) models often use a rich set of features created by linguistic expertise, such as semantic representations. However, in the era of large language models (LLMs), more and more tasks are turned into generic, end-to-end sequence generation problems. In this paper, we investigate the question: what is the role of semantic representations in the era of LLMs? Specifically, we investigate the effect of Abstract Meaning Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven chain-of-thought prompting method, which we call AMRCoT, and find that it generally hurts performance more than it helps. To investigate what AMR may have to offer on these tasks, we conduct a series of analysis experiments. We find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step where the LLM must connect its reasoning over the AMR to its prediction. We recommend focusing on these areas for future work in semantic representations for LLMs. Our code: https://github.com/causalNLP/amr_llm.</li>
<li><strong>摘要：</strong>传统上，自然语言处理 (NLP) 模型通常使用由语言专业知识创建的一组丰富的特征，例如语义表示。然而，在大型语言模型（LLM）时代，越来越多的任务变成了通用的、端到端的序列生成问题。在本文中，我们研究了这样一个问题：语义表示在法学硕士时代的作用是什么？具体来说，我们研究了抽象意义表示 (AMR) 在五种不同的 NLP 任务中的效果。我们提出了一种 AMR 驱动的思想链提示方法，我们称之为 AMRCoT，我们发现它通常对性能的损害大于它的帮助。为了研究 AMR 在这些任务中可能提供的功能，我们进行了一系列分析实验。我们发现很难预测 AMR 可能对哪些输入示例有所帮助或有害，但多词表达式、命名实体以及在最后的推理步骤中往往会出现错误，法学硕士必须将其对 AMR 的推理与其自身的推理联系起来。预言。我们建议未来法学硕士语义表示的工作重点关注这些领域。我们的代码：https://github.com/causalNLP/amr_llm。</li>
</ul>

<h3>Title: D2PO: Discriminator-Guided DPO with Response Evaluation Models</h3>
<ul>
<li><strong>Authors: </strong>Prasann Singhal, Nathan Lambert, Scott Niekum, Tanya Goyal, Greg Durrett</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01511">https://arxiv.org/abs/2405.01511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01511">https://arxiv.org/pdf/2405.01511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01511]] D2PO: Discriminator-Guided DPO with Response Evaluation Models(https://arxiv.org/abs/2405.01511)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, chat</a></li>
<li><strong>Abstract: </strong>Varied approaches for aligning language models have been proposed, including supervised fine-tuning, RLHF, and direct optimization methods such as DPO. Although DPO has rapidly gained popularity due to its straightforward training process and competitive results, there is an open question of whether there remain practical advantages of using a discriminator, like a reward model, to evaluate responses. We propose D2PO, discriminator-guided DPO, an approach for the online setting where preferences are being collected throughout learning. As we collect gold preferences, we use these not only to train our policy, but to train a discriminative response evaluation model to silver-label even more synthetic data for policy training. We explore this approach across a set of diverse tasks, including a realistic chat setting, we find that our approach leads to higher-quality outputs compared to DPO with the same data budget, and greater efficiency in terms of preference data requirements. Furthermore, we show conditions under which silver labeling is most helpful: it is most effective when training the policy with DPO, outperforming traditional PPO, and benefits from maintaining a separate discriminator from the policy model.</li>
<li><strong>摘要：</strong>人们已经提出了多种对齐语言模型的方法，包括监督微调、RLHF 和直接优化方法（例如 DPO）。尽管 DPO 因其简单的训练过程和竞争性结果而迅速受到欢迎，但仍存在一个悬而未决的问题：使用判别器（如奖励模型）来评估响应是否仍然具有实际优势。我们提出了 D2PO，即鉴别器引导的 DPO，这是一种在线环境的方法，在整个学习过程中收集偏好。当我们收集黄金偏好时，我们不仅使用这些数据来训练我们的政策，而且还训练一个有区别的响应评估模型，为政策训练的更多合成数据贴上白银标签。我们在一系列不同的任务中探索了这种方法，包括现实的聊天设置，我们发现与相同数据预算的 DPO 相比，我们的方法可以带来更高质量的输出，并且在偏好数据要求方面效率更高。此外，我们还展示了银标签最有帮助的条件：使用 DPO 训练策略时最有效，优于传统的 PPO，并且从策略模型中维护单独的判别器会带来好处。</li>
</ul>

<h3>Title: FLAME: Factuality-Aware Alignment for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, Xilun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01525">https://arxiv.org/abs/2405.01525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01525">https://arxiv.org/pdf/2405.01525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01525]] FLAME: Factuality-Aware Alignment for Large Language Models(https://arxiv.org/abs/2405.01525)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination</a></li>
<li><strong>Abstract: </strong>Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.</li>
<li><strong>摘要：</strong>对齐是一个标准程序，用于微调预训练的大型语言模型 (LLM)，使其遵循自然语言指令并充当有用的 AI 助手。然而，我们观察到，传统的对齐过程无法提高法学硕士的事实准确性，并且常常导致产生更多虚假事实（即幻觉）。在本文中，我们研究如何使 LLM 对齐过程更加真实，首先确定两个对齐步骤中导致幻觉的因素：监督微调（SFT）和强化学习（RL）。特别是，我们发现对法学硕士进行新知识或不熟悉的文本培训可能会鼓励产生幻觉。这使得 SFT 不太真实，因为它训练的人类标记数据对于法学硕士来说可能是新颖的。此外，标准强化学习中使用的奖励函数也会鼓励产生幻觉，因为它引导法学硕士对各种指令提供更有帮助的响应，通常更喜欢更长、更详细的响应。基于这些观察，我们提出了事实感知对齐，包括通过直接偏好优化的事实感知 SFT 和事实感知 RL。实验表明，我们提出的事实意识对齐指导法学硕士输出更多的事实响应，同时保持指令跟踪能力。</li>
</ul>

<h3>Title: Prometheus 2: An Open Source Language Model Specialized in Evaluating  Other Language Models</h3>
<ul>
<li><strong>Authors: </strong>Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2405.01535">https://arxiv.org/abs/2405.01535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2405.01535">https://arxiv.org/pdf/2405.01535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2405.01535]] Prometheus 2: An Open Source Language Model Specialized in Evaluating  Other Language Models(https://arxiv.org/abs/2405.01535)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt</a></li>
<li><strong>Abstract: </strong>Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available at https://github.com/prometheus-eval/prometheus-eval.</li>
<li><strong>摘要：</strong>GPT-4 等专有 LM 通常用于评估各种 LM 响应的质量。然而，包括透明度、可控性和可负担性在内的担忧强烈推动了专门从事评估的开源语言模型的开发。另一方面，现有的开放评估器 LM 表现出严重的缺点：1）它们发出的分数与人类分配的分数明显不同，2）它们缺乏执行直接评估和成对排名（两种最流行的评估形式）的灵活性。此外，他们不具备根据自定义评估标准进行评估的能力，而是关注有用性和无害性等一般属性。为了解决这些问题，我们引入了 Prometheus 2，这是一个比其前身更强大的评估器 LM，它密切反映了人类和 GPT-4 的判断。此外，它能够处理直接评估和按用户定义的评估标准分组的成对排名格式。在四个直接评估基准和四个成对排名基准上，Prometheus 2 在所有测试的开放评估器 LM 中与人类和专有 LM 法官的相关性和一致性最高。我们的模型、代码和数据均可在 https://github.com/prometheus-eval/prometheus-eval 上公开获取。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
