<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-12-15</h1>
<h3>Title: Benchmarking Automatic Speech Recognition Models for African Languages</h3>
<ul>
<li><strong>Authors: </strong>Alvin Nahabwe, Sulaiman Kagumire, Denis Musinguzi, Bruno Beijuka, Jonah Mubuuke Kyagaba, Peter Nabende, Andrew Katumba, Joyce Nakatumba-Nabende</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10968">https://arxiv.org/abs/2512.10968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10968">https://arxiv.org/pdf/2512.10968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10968]] Benchmarking Automatic Speech Recognition Models for African Languages(https://arxiv.org/abs/2512.10968)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Automatic speech recognition (ASR) for African languages remains constrained by limited labeled data and the lack of systematic guidance on model selection, data scaling, and decoding strategies. Large pre-trained systems such as Whisper, XLS-R, MMS, and W2v-BERT have expanded access to ASR technology, but their comparative behavior in African low-resource contexts has not been studied in a unified and systematic way. In this work, we benchmark four state-of-the-art ASR models across 13 African languages, fine-tuning them on progressively larger subsets of transcribed data ranging from 1 to 400 hours. Beyond reporting error rates, we provide new insights into why models behave differently under varying conditions. We show that MMS and W2v-BERT are more data efficient in very low-resource regimes, XLS-R scales more effectively as additional data becomes available, and Whisper demonstrates advantages in mid-resource conditions. We also analyze where external language model decoding yields improvements and identify cases where it plateaus or introduces additional errors, depending on the alignment between acoustic and text resources. By highlighting the interaction between pre-training coverage, model architecture, dataset domain, and resource availability, this study offers practical and insights into the design of ASR systems for underrepresented languages.</li>
<li><strong>摘要：</strong>非洲语言的自动语音识别 (ASR) 仍然受到有限的标记数据以及缺乏模型选择、数据扩展和解码策略方面的系统指导的限制。 Whisper、XLS-R、MMS 和 W2v-BERT 等大型预训练系统已经扩大了 ASR 技术的使用范围，但它们在非洲资源匮乏环境中的比较行为尚未得到统一、系统的研究。在这项工作中，我们对 13 种非洲语言的四种最先进的 ASR 模型进行了基准测试，并根据 1 到 400 小时范围内逐渐增大的转录数据子集对它们进行微调。除了报告错误率之外，我们还提供了关于为什么模型在不同条件下表现不同的新见解。我们证明，MMS 和 W2v-BERT 在资源极少的情况下数据效率更高，XLS-R 在有更多数据可用时可以更有效地扩展，而 Whisper 在中等资源条件下展示了优势。我们还分析外部语言模型解码在哪些方面产生了改进，并根据声学和文本资源之间的对齐情况识别出停滞或引入额外错误的情况。通过强调预训练覆盖范围、模型架构、数据集域和资源可用性之间的相互作用，本研究为代表性不足的语言的 ASR 系统设计提供了实用和见解。</li>
</ul>

<h3>Title: MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA</h3>
<ul>
<li><strong>Authors: </strong>Seonok Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10996">https://arxiv.org/abs/2512.10996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10996">https://arxiv.org/pdf/2512.10996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10996]] MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA(https://arxiv.org/abs/2512.10996)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, llm, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）的最新进展显着增强了大型语言模型（LLM）执行复杂问答（QA）任务的能力。在本文中，我们介绍了 MedBioRAG，这是一种检索增强模型，旨在通过语义和词汇搜索、文档检索和监督微调的组合来提高生物医学 QA 性能。 MedBioRAG 有效检索相关生物医学文档并对其进行排名，从而生成精确且上下文感知的响应。我们使用 NFCorpus、TREC-COVID、MedQA、PubMedQA 和 BioASQ 等基准数据集跨文本检索、封闭式 QA 和长格式 QA 任务评估 MedBioRAG。实验结果表明，MedBioRAG 在所有评估任务中均优于先前最先进的 (SoTA) 模型和 GPT-4o 基础模型。值得注意的是，我们的方法提高了文档检索的 NDCG 和 MRR 分数，同时在封闭式 QA 中实现了更高的准确性，在长格式 QA 中实现了 ROUGE 分数。我们的研究结果强调了基于语义搜索的检索和法学硕士微调在生物医学应用中的有效性。</li>
</ul>

<h3>Title: KBQA-R1: Reinforcing Large Language Models for Knowledge Base Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Xin Sun, Zhongqi Chen, Xing Zheng, Qiang Liu, Shu Wu, Bowen Song, Zilei Wang, Weiqiang Wang, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.10999">https://arxiv.org/abs/2512.10999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.10999">https://arxiv.org/pdf/2512.10999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.10999]] KBQA-R1: Reinforcing Large Language Models for Knowledge Base Question Answering(https://arxiv.org/abs/2512.10999)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Knowledge Base Question Answering (KBQA) challenges models to bridge the gap between natural language and strict knowledge graph schemas by generating executable logical forms. While Large Language Models (LLMs) have advanced this field, current approaches often struggle with a dichotomy of failure: they either generate hallucinated queries without verifying schema existence or exhibit rigid, template-based reasoning that mimics synthesized traces without true comprehension of the environment. To address these limitations, we present \textbf{KBQA-R1}, a framework that shifts the paradigm from text imitation to interaction optimization via Reinforcement Learning. Treating KBQA as a multi-turn decision process, our model learns to navigate the knowledge base using a list of actions, leveraging Group Relative Policy Optimization (GRPO) to refine its strategies based on concrete execution feedback rather than static supervision. Furthermore, we introduce \textbf{Referenced Rejection Sampling (RRS)}, a data synthesis method that resolves cold-start challenges by strictly aligning reasoning traces with ground-truth action sequences. Extensive experiments on WebQSP, GrailQA, and GraphQuestions demonstrate that KBQA-R1 achieves state-of-the-art performance, effectively grounding LLM reasoning in verifiable execution.</li>
<li><strong>摘要：</strong>知识库问答（KBQA）通过生成可执行的逻辑形式来挑战模型，以弥合自然语言和严格的知识图模式之间的差距。虽然大型语言模型 (LLM) 推动了这一领域的发展，但当前的方法常常面临失败的二分法：它们要么在没有验证模式存在的情况下生成幻觉查询，要么表现出严格的、基于模板的推理，模仿合成的痕迹，而没有真正理解环境。为了解决这些限制，我们提出了 \textbf{KBQA-R1}，这是一个通过强化学习将范式从文本模仿转变为交互优化的框架。将 KBQA 视为多轮决策过程，我们的模型学习使用一系列操作来导航知识库，利用组相对策略优化 (GRPO) 根据具体执行反馈而不是静态监督来完善其策略。此外，我们引入了 \textbf{Referenced Rejection Sampling (RRS)}，这是一种数据合成方法，通过严格地将推理轨迹与真实动作序列对齐来解决冷启动挑战。对 WebQSP、GrailQA 和 GraphQuestions 的大量实验表明，KBQA-R1 实现了最先进的性能，有效地将 LLM 推理建立在可验证的执行中。</li>
</ul>

<h3>Title: PIAST: Rapid Prompting with In-context Augmentation for Scarce Training data</h3>
<ul>
<li><strong>Authors: </strong>Pawel Batorski, Paul Swoboda</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11013">https://arxiv.org/abs/2512.11013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11013">https://arxiv.org/pdf/2512.11013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11013]] PIAST: Rapid Prompting with In-context Augmentation for Scarce Training data(https://arxiv.org/abs/2512.11013)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm, prompt</a></li>
<li><strong>Abstract: </strong>LLMs are highly sensitive to prompt design, but handcrafting effective prompts is difficult and often requires intricate crafting of few-shot examples. We propose a fast automatic prompt construction algorithm that augments human instructions by generating a small set of few shot examples. Our method iteratively replaces/drops/keeps few-shot examples using Monte Carlo Shapley estimation of example utility. For faster execution, we use aggressive subsampling and a replay buffer for faster evaluations. Our method can be run using different compute time budgets. On a limited budget, we outperform existing automatic prompting methods on text simplification and GSM8K and obtain second best results on classification and summarization. With an extended, but still modest compute budget we set a new state of the art among automatic prompting methods on classification, simplification and GSM8K. Our results show that carefully constructed examples, rather than exhaustive instruction search, are the dominant lever for fast and data efficient prompt engineering. Our code is available at this https URL.</li>
<li><strong>摘要：</strong>法学硕士对提示设计高度敏感，但手工制作有效的提示很困难，并且通常需要复杂地制作少量示例。我们提出了一种快速自动提示构建算法，通过生成一小组少数镜头示例来增强人类指令。我们的方法使用示例效用的蒙特卡罗沙普利估计来迭代地替换/删除/保留少数样本。为了更快地执行，我们使用积极的子采样和重播缓冲区来更快地进行评估。我们的方法可以使用不同的计算时间预算来运行。在有限的预算下，我们在文本简化和 GSM8K 方面优于现有的自动提示方法，并在分类和摘要方面获得第二好的结果。通过扩展但仍然适度的计算预算，我们在分类、简化和 GSM8K 的自动提示方法中建立了新的最先进技术。我们的结果表明，精心构建的示例，而不是详尽的指令搜索，是快速且数据高效的提示工程的主导杠杆。我们的代码可以在这个 https URL 上找到。</li>
</ul>

<h3>Title: Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Kamp, Roos Bakker, Dominique Blok</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11108">https://arxiv.org/abs/2512.11108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11108">https://arxiv.org/pdf/2512.11108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11108]] Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution(https://arxiv.org/abs/2512.11108)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.</li>
<li><strong>摘要：</strong>高质量的解释可以加强对语言模型和数据的理解。特征归因方法（例如集成梯度）是一种事后解释器，可以提供令牌级别的见解。然而，由于不同方法的潜在偏差，对相同输入的解释可能会有很大差异。用户可能意识到这个问题并且不信任他们的实用程序，而不知情的用户可能不充分信任他们。在这项工作中，我们深入探讨了归因方法之间表面上的不一致，通过三个评估指标的模型和方法无关的框架来构建它们的偏见。我们系统地评估两个变压器的词汇和位置偏差（输入中的内容和位置）；首先，在人工数据的受控伪随机分类任务中；然后，在自然数据上进行半受控因果关系检测任务。我们发现，在我们的模型比较中，词汇和位置偏差在结构上是不平衡的，在一种类型上得分高的模型在另一种类型上得分低。我们还发现有迹象表明，产生异常解释的方法本身更有可能存在偏见。</li>
</ul>

<h3>Title: FIBER: A Multilingual Evaluation Resource for Factual Inference Bias</h3>
<ul>
<li><strong>Authors: </strong>Evren Ayberk Munis, Deniz Yılmaz, Arianna Muti, Çağrı Toraman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11110">https://arxiv.org/abs/2512.11110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11110">https://arxiv.org/pdf/2512.11110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11110]] FIBER: A Multilingual Evaluation Resource for Factual Inference Bias(https://arxiv.org/abs/2512.11110)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, prompt</a></li>
<li><strong>Abstract: </strong>Large language models are widely used across domains, yet there are concerns about their factual reliability and biases. Factual knowledge probing offers a systematic means to evaluate these aspects. Most existing benchmarks focus on single-entity facts and monolingual data. We therefore present FIBER, a multilingual benchmark for evaluating factual knowledge in single- and multi-entity settings. The dataset includes sentence completion, question-answering, and object-count prediction tasks in English, Italian, and Turkish. Using FIBER, we examine whether the prompt language induces inference bias in entity selection and how large language models perform on multi-entity versus single-entity questions. The results indicate that the language of the prompt can influence the model's generated output, particularly for entities associated with the country corresponding to that language. However, this effect varies across different topics such that 31% of the topics exhibit factual inference bias score greater than 0.5. Moreover, the level of bias differs across languages such that Turkish prompts show higher bias compared to Italian in 83% of the topics, suggesting a language-dependent pattern. Our findings also show that models face greater difficulty when handling multi-entity questions than the single-entity questions. Model performance differs across both languages and model sizes. The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores. Larger models, including Llama-3.1-8B and Qwen-2.5-7B, show consistently better performance than smaller 3B-4B models.</li>
<li><strong>摘要：</strong>大型语言模型在各个领域广泛使用，但人们对其事实可靠性和偏见感到担忧。事实知识探究提供了评估这些方面的系统方法。大多数现有基准都侧重于单一实体事实和单语言数据。因此，我们推出了 FIBER，一种用于评估单实体和多实体环境中的事实知识的多语言基准。该数据集包括英语、意大利语和土耳其语的句子完成、问答和对象计数预测任务。使用 FIBER，我们检查提示语言是否会在实体选择中引起推理偏差，以及大型语言模型在多实体与单实体问题上的表现如何。结果表明，提示的语言可以影响模型生成的输出，特别是对于与该语言对应的国家/地区关联的实体。然而，这种影响因不同主题而异，31% 的主题表现出大于 0.5 的事实推理偏差分数。此外，不同语言的偏见程度有所不同，在 83% 的主题中，土耳其语提示显示出比意大利语更高的偏见，这表明存在一种依赖于语言的模式。我们的研究结果还表明，模型在处理多实体问题时比处理单实体问题时面临更大的困难。模型性能因语言和模型大小而异。英语的平均准确率最高，而土耳其语和意大利语的得分明显较低。较大的型号（包括 Llama-3.1-8B 和 Qwen-2.5-7B）始终表现出比较小的 3B-4B 型号更好的性能。</li>
</ul>

<h3>Title: SciLaD: A Large-Scale, Transparent, Reproducible Dataset for Natural Scientific Language Processing</h3>
<ul>
<li><strong>Authors: </strong>Luca Foppiano, Sotaro Takeshita, Pedro Ortiz Suarez, Ekaterina Borisova, Raia Abu Ahmad, Malte Ostendorff, Fabio Barth, Julian Moreno-Schneider, Georg Rehm</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11192">https://arxiv.org/abs/2512.11192</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11192">https://arxiv.org/pdf/2512.11192</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11192]] SciLaD: A Large-Scale, Transparent, Reproducible Dataset for Natural Scientific Language Processing(https://arxiv.org/abs/2512.11192)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>SciLaD is a novel, large-scale dataset of scientific language constructed entirely using open-source frameworks and publicly available data sources. It comprises a curated English split containing over 10 million scientific publications and a multilingual, unfiltered TEI XML split including more than 35 million publications. We also publish the extensible pipeline for generating SciLaD. The dataset construction and processing workflow demonstrates how open-source tools can enable large-scale, scientific data curation while maintaining high data quality. Finally, we pre-train a RoBERTa model on our dataset and evaluate it across a comprehensive set of benchmarks, achieving performance comparable to other scientific language models of similar size, validating the quality and utility of SciLaD. We publish the dataset and evaluation pipeline to promote reproducibility, transparency, and further research in natural scientific language processing and understanding including scholarly document processing.</li>
<li><strong>摘要：</strong>SciLaD 是一个新颖的大规模科学语言数据集，完全使用开源框架和公开数据源构建。它包括一个包含超过 1000 万份科学出版物的精心策划的英语拆分和一个包含超过 3500 万份出版物的多语言、未经过滤的 TEI XML 拆分。我们还发布了用于生成 SciLaD 的可扩展管道。数据集构建和处理工作流程展示了开源工具如何在保持高数据质量的同时实现大规模、科学的数据管理。最后，我们在数据集上预训练 RoBERTa 模型，并通过一组全面的基准对其进行评估，实现与其他类似规模的科学语言模型相当的性能，验证了 SciLaD 的质量和实用性。我们发布数据集和评估流程，以促进自然科学语言处理和理解（包括学术文档处理）的可重复性、透明度和进一步研究。</li>
</ul>

<h3>Title: Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Yun-Chung Liu, Rui Yang, Jonathan Chong Kai Liew, Ziran Yin, Henry Foote, Christopher J. Lindsell, Chuan Hong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11261">https://arxiv.org/abs/2512.11261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11261">https://arxiv.org/pdf/2512.11261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11261]] Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach(https://arxiv.org/abs/2512.11261)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Systematic reviews are a key component of evidence-based medicine, playing a critical role in synthesizing existing research evidence and guiding clinical decisions. However, with the rapid growth of research publications, conducting systematic reviews has become increasingly burdensome, with title and abstract screening being one of the most time-consuming and resource-intensive steps. To mitigate this issue, we designed a two-stage dynamic few-shot learning (DFSL) approach aimed at improving the efficiency and performance of large language models (LLMs) in the title and abstract screening task. Specifically, this approach first uses a low-cost LLM for initial screening, then re-evaluates low-confidence instances using a high-performance LLM, thereby enhancing screening performance while controlling computational costs. We evaluated this approach across 10 systematic reviews, and the results demonstrate its strong generalizability and cost-effectiveness, with potential to reduce manual screening burden and accelerate the systematic review process in practical applications.</li>
<li><strong>摘要：</strong>系统评价是循证医学的关键组成部分，在综合现有研究证据和指导临床决策方面发挥着关键作用。然而，随着研究出版物的快速增长，进行系统评价变得越来越繁重，标题和摘要筛选是最耗时和资源密集的步骤之一。为了缓解这个问题，我们设计了一种两阶段动态少样本学习（DFSL）方法，旨在提高标题和摘要筛选任务中大型语言模型（LLM）的效率和性能。具体来说，该方法首先使用低成本的LLM进行初步筛选，然后使用高性能的LLM重新评估低置信度实例，从而在控制计算成本的同时增强筛选性能。我们通过 10 次系统评价对这种方法进行了评估，结果证明了其强大的通用性和成本效益，有可能减少人工筛选负担并加速实际应用中的系统评价过程。</li>
</ul>

<h3>Title: When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents</h3>
<ul>
<li><strong>Authors: </strong>Mrinal Rawat, Arkajyoti Chakraborty, Neha Gupta, Roberto Pieraccini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11277">https://arxiv.org/abs/2512.11277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11277">https://arxiv.org/pdf/2512.11277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11277]] When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents(https://arxiv.org/abs/2512.11277)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, agent</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.</li>
<li><strong>摘要：</strong>有监督微调（SFT）已成为提高下游任务中大型语言模型（LLM）性能的最有效方法之一。然而，当底层数据分布发生变化时，SFT 可能难以泛化，即使新数据没有完全落在训练域之外。最近以推理为重点的模型（例如 o1 和 R1）与非推理模型相比表现出一致的增益，凸显了推理对于提高泛化性和可靠性的重要性。然而，为 SFT 收集高质量的推理轨迹仍然具有挑战性——注释成本高昂、主观且难以扩展。为了解决这一限制，我们利用强化学习 (RL) 使模型能够直接从任务结果中学习推理策略。我们提出了一个管道，其中法学硕士生成推理步骤，指导工具的调用（例如函数调用）和对话代理的最终答案生成。我们的方法采用组相对策略优化（GRPO），并围绕工具准确性和答案正确性设计奖励，使模型能够迭代地完善其推理和操作。实验结果表明，我们的方法提高了推理质量和工具调用的精度，比 SFT 模型（没有显式思维训练）实现了 1.5% 的相对改进，与普通 Qwen3-1.7B 模型的基础相比实现了 40% 的增益。这些发现证明了通过强化学习统一推理和行动学习以构建能力更强、更通用的对话代理的前景。</li>
</ul>

<h3>Title: AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11280">https://arxiv.org/abs/2512.11280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11280">https://arxiv.org/pdf/2512.11280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11280]] AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference(https://arxiv.org/abs/2512.11280)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance across a wide range of tasks, but their increasing parameter sizes significantly slow down inference. Speculative decoding mitigates this issue by leveraging a smaller draft model to predict candidate tokens, which are then verified by a larger target model. However, existing approaches often require additional training, extensive hyperparameter tuning, or prior analysis of models and tasks before deployment. In this paper, we propose Adaptive Speculative Decoding (AdaSD), a hyperparameter-free decoding scheme that dynamically adjusts generation length and acceptance criteria during inference. AdaSD introduces two adaptive thresholds: one to determine when to stop candidate token generation and another to decide token acceptance, both updated in real time based on token entropy and Jensen-Shannon distance. This approach eliminates the need for pre-analysis or fine-tuning and is compatible with off-the-shelf models. Experiments on benchmark datasets demonstrate that AdaSD achieves up to 49\% speedup over standard speculative decoding while limiting accuracy degradation to under 2\%, making it a practical solution for efficient and adaptive LLM inference.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在广泛的任务中取得了卓越的性能，但其不断增加的参数大小显着减慢了推理速度。推测性解码通过利用较小的草稿模型来预测候选标记，然后由较大的目标模型进行验证，从而缓解了这个问题。然而，现有方法通常需要额外的训练、广泛的超参数调整或在部署之前对模型和任务进行预先分析。在本文中，我们提出了自适应推测解码（AdaSD），这是一种无超参数解码方案，可在推理过程中动态调整生成长度和接受标准。 AdaSD 引入了两个自适应阈值：一个用于确定何时停止候选令牌生成，另一个用于决定令牌接受，两者都根据令牌熵和 Jensen-Shannon 距离实时更新。这种方法无需预先分析或微调，并且与现成的模型兼容。基准数据集上的实验表明，AdaSD 比标准推测解码实现了高达 49% 的加速，同时将精度下降限制在 2% 以下，使其成为高效、自适应 LLM 推理的实用解决方案。</li>
</ul>

<h3>Title: CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise</h3>
<ul>
<li><strong>Authors: </strong>Qingsen Ma, Dianyun Wang, Ran Jing, Yujun Sun, Zhenbo Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11282">https://arxiv.org/abs/2512.11282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11282">https://arxiv.org/pdf/2512.11282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11282]] CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise(https://arxiv.org/abs/2512.11282)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, gpt, hallucination, prompt</a></li>
<li><strong>Abstract: </strong>Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.</li>
<li><strong>摘要：</strong>大型语言模型在处理长且嘈杂的检索上下文时通常会产生幻觉，因为它们依赖于虚假的相关性而不是真正的因果关系。我们提出了 CIP，一种轻量级、即插即用的因果提示框架，可以减轻输入阶段的幻觉。 CIP 在实体、动作和事件之间构建因果关系序列，并将其注入提示中，以指导推理到因果相关的证据。通过因果干预和反事实推理，CIP 抑制非因果推理路径，提高事实基础和可解释性。在GPT-4o、Gemini 2.0 Flash、Llama 3.1等7种主流语言模型上的实验表明，CIP持续提升推理质量和可靠性，归因率提升2.6分，因果一致性得分提升0.38，有效信息密度提升4倍。 API 级别分析进一步表明，CIP 可以加速上下文理解，并将端到端响应延迟减少高达 55.1%。这些结果表明因果推理可能成为提高大型语言模型的可解释性、稳定性和效率的有前途的范例。</li>
</ul>

<h3>Title: LegalRikai: Open Benchmark -- A Benchmark for Complex Japanese Corporate Legal Tasks</h3>
<ul>
<li><strong>Authors: </strong>Shogo Fujita, Yuji Naraki, Yiqing Zhu, Shinsuke Mori</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11297">https://arxiv.org/abs/2512.11297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11297">https://arxiv.org/pdf/2512.11297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11297]] LegalRikai: Open Benchmark -- A Benchmark for Complex Japanese Corporate Legal Tasks(https://arxiv.org/abs/2512.11297)</code><input type="text"></li>
<li><strong>Keywords: </strong>gpt, llm, prompt</a></li>
<li><strong>Abstract: </strong>This paper introduces LegalRikai: Open Benchmark, a new benchmark comprising four complex tasks that emulate Japanese corporate legal practices. The benchmark was created by legal professionals under the supervision of an attorney. This benchmark has 100 samples that require long-form, structured outputs, and we evaluated them against multiple practical criteria. We conducted both human and automated evaluations using leading LLMs, including GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1. Our human evaluation revealed that abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks. Furthermore, our analysis reveals that automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge. The result demonstrates the utility of automated evaluation as a screening tool when expert availability is limited. We propose a dataset evaluation framework to promote more practice-oriented research in the legal domain.</li>
<li><strong>摘要：</strong>本文介绍了 LegalRikai：开放基准，这是一个新基准，包含四个模拟日本公司法律实践的复杂任务。该基准是由法律专业人士在律师的监督下创建的。该基准测试有 100 个需要长格式、结构化输出的样本，我们根据多个实际标准对它们进行了评估。我们使用领先的法学硕士（包括 GPT-5、Gemini 2.5 Pro 和 Claude Opus 4.1）进行了人工和自动评估。我们的人工评估显示，抽象指令会导致不必要的修改，突出了传统短文本任务所忽略的文档级编辑中的模型弱点。此外，我们的分析表明，自动评估与人类对具有明确语言基础的标准的判断非常一致，并且评估结构一致性仍然是一个挑战。结果表明，当专家数量有限时，自动评估作为筛选工具的实用性。我们提出了一个数据集评估框架，以促进法律领域更多以实践为导向的研究。</li>
</ul>

<h3>Title: Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture</h3>
<ul>
<li><strong>Authors: </strong>Jiarun Liu, Shiyue Xu, Yang Li, Shangkun Liu, Yongli Yu, Peng Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11303">https://arxiv.org/abs/2512.11303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11303">https://arxiv.org/pdf/2512.11303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11303]] Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture(https://arxiv.org/abs/2512.11303)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, agent</a></li>
<li><strong>Abstract: </strong>Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2%) and Memento (70.9%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.</li>
<li><strong>摘要：</strong>由于工具可用性和经验重用的限制，大型语言模型代理在适应新任务方面面临着根本挑战。现有方法要么依赖覆盖范围有限的预定义工具，要么从头开始构建工具而不利用过去的经验，从而导致探索效率低下和性能不佳。我们引入了 SMITH（共享内存集成工具中心），这是一种统一的认知架构，通过分层内存组织将动态工具创建与跨任务经验共享无缝集成。 SMITH 将代理记忆组织为程序、语义和情景组件，从而实现系统功能扩展，同时保留成功的执行模式。我们的方法将工具创建形式化为受控沙箱环境中的迭代代码生成，并通过具有语义相似性匹配的情景记忆检索来共享经验。我们进一步提出了一种基于智能体集成难度重估的课程学习策略。 GAIA 基准的大量实验证明了 SMITH 的有效性，达到了 81.8% 的 Pass@1 准确率，并超越了最先进的基准，包括 Alita (75.2%) 和 Memento (70.9%)。我们的工作为构建真正的自适应代理奠定了基础，这些代理通过工具创建和经验积累的原则性整合不断发展其能力。</li>
</ul>

<h3>Title: qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shreya Shukla, Aditya Sriram, Milinda Kuppur Narayanaswamy, Hiteshi Jain</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11366">https://arxiv.org/abs/2512.11366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11366">https://arxiv.org/pdf/2512.11366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11366]] qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs(https://arxiv.org/abs/2512.11366)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>The deployment of large language models for specialized tasks often requires domain-specific parameter-efficient finetuning through Low-Rank Adaptation (LoRA) modules. However, effectively fusing these adapters to handle complex, multi-domain composite queries remains a critical challenge. Existing LoRA fusion approaches either use static weights, which assign equal relevance to each participating LoRA, or require data-intensive supervised training for every possible LoRA combination to obtain respective optimal fusion weights. We propose qa-FLoRA, a novel query-adaptive data-and-training-free method for LoRA fusion that dynamically computes layer-level fusion weights by measuring distributional divergence between the base model and respective adapters. Our approach eliminates the need for composite training data or domain-representative samples, making it readily applicable to existing adapter collections. Extensive experiments across nine multilingual composite tasks spanning mathematics, coding, and medical domains, show that qa-FLoRA outperforms static fusion by ~5% with LLaMA-2 and ~6% with LLaMA-3, and the training-free baselines by ~7% with LLaMA-2 and ~10% with LLaMA-3, while significantly closing the gap with supervised baselines. Further, layer-level analysis of our fusion weights reveals interpretable fusion patterns, demonstrating the effectiveness of our approach for robust multi-domain adaptation.</li>
<li><strong>摘要：</strong>用于专门任务的大型语言模型的部署通常需要通过低秩适应（LoRA）模块进行特定领域的参数高效微调。然而，有效地融合这些适配器来处理复杂的多域复合查询仍然是一个严峻的挑战。现有的 LoRA 融合方法要么使用静态权重，为每个参与的 LoRA 分配相同的相关性，要么需要对每个可能的 LoRA 组合进行数据密集型监督训练以获得各自的最佳融合权重。我们提出了 qa-FLoRA，一种用于 LoRA 融合的新型查询自适应数据和免训练方法，它通过测量基础模型和相应适配器之间的分布散度来动态计算层级融合权重。我们的方法消除了对复合训练数据或领域代表性样本的需求，使其易于适用于现有的适配器集合。涵盖数学、编码和医学领域的九个多语言复合任务的广泛实验表明，qa-FLoRA 的性能优于静态融合，LLaMA-2 优于静态融合约 5%，LLaMA-3 优于静态融合约 6%，LLaMA-2 优于静态融合约 7%，LLaMA-3 优于无训练基线约 7%，LLaMA-3 优于静态融合约 10%，同时显着缩小了与监督基线的差距。此外，我们的融合权重的层级分析揭示了可解释的融合模式，证明了我们的方法对于鲁棒多域适应的有效性。</li>
</ul>

<h3>Title: Mining Legal Arguments to Study Judicial Formalism</h3>
<ul>
<li><strong>Authors: </strong>Tomáš Koref, Lena Held, Mahammad Namazov, Harun Kumru, Yassine Thlija, Christoph Burchard, Ivan Habernal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11374">https://arxiv.org/abs/2512.11374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11374">https://arxiv.org/pdf/2512.11374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11374]] Mining Legal Arguments to Study Judicial Formalism(https://arxiv.org/abs/2512.11374)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Courts must justify their decisions, but systematically analyzing judicial reasoning at scale remains difficult. This study refutes claims about formalistic judging in Central and Eastern Europe (CEE) by developing automated methods to detect and classify judicial reasoning in Czech Supreme Courts' decisions using state-of-the-art natural language processing methods. We create the MADON dataset of 272 decisions from two Czech Supreme Courts with expert annotations of 9,183 paragraphs with eight argument types and holistic formalism labels for supervised training and evaluation. Using a corpus of 300k Czech court decisions, we adapt transformer LLMs for Czech legal domain by continued pretraining and experiment with methods to address dataset imbalance including asymmetric loss and class weighting. The best models successfully detect argumentative paragraphs (82.6\% macro-F1), classify traditional types of legal argument (77.5\% macro-F1), and classify decisions as formalistic/non-formalistic (83.2\% macro-F1). Our three-stage pipeline combining ModernBERT, Llama 3.1, and traditional feature-based machine learning achieves promising results for decision classification while reducing computational costs and increasing explainability. Empirically, we challenge prevailing narratives about CEE formalism. This work shows that legal argument mining enables reliable judicial philosophy classification and shows the potential of legal argument mining for other important tasks in computational legal studies. Our methodology is easily replicable across jurisdictions, and our entire pipeline, datasets, guidelines, models, and source codes are available at this https URL.</li>
<li><strong>摘要：</strong>法院必须证明其判决的合理性，但大规模系统地分析司法推理仍然很困难。这项研究通过开发自动化方法，使用最先进的自然语言处理方法来检测和分类捷克最高法院判决中的司法推理，从而驳斥了有关中欧和东欧 (CEE) 形式主义判断的说法。我们创建了包含来自两个捷克最高法院的 272 项判决的 MADON 数据集，其中包含 9,183 个段落的专家注释、八种论证类型和用于监督训练和评估的整体形式主义标签。使用包含 30 万个捷克法院判决的语料库，我们通过持续预训练和试验解决数据集不平衡（包括不对称损失和类别加权）的方法，使 Transformer LLM 适应捷克法律领域。最好的模型成功地检测论证段落（82.6\% 宏观-F1），对传统类型的法律论证进行分类（77.5\% 宏观-F1），并将决策分类为形式主义/非形式主义（83.2\% 宏观-F1）。我们的三阶段管道结合了 ModernBERT、Llama 3.1 和传统的基于特征的机器学习，在决策分类方面取得了有希望的结果，同时降低了计算成本并提高了可解释性。根据经验，我们挑战有关中东欧形式主义的流行叙述。这项工作表明，法律论证挖掘能够实现可靠的司法哲学分类，并展示了法律论证挖掘在计算法律研究中其他重要任务中的潜力。我们的方法可以轻松地跨司法管辖区复制，并且我们的整个管道、数据集、指南、模型和源代码都可以在此 https URL 上找到。</li>
</ul>

<h3>Title: Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis</h3>
<ul>
<li><strong>Authors: </strong>Felipe Ribeiro Fujita de Mello, Hideyuki Takada</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11388">https://arxiv.org/abs/2512.11388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11388">https://arxiv.org/pdf/2512.11388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11388]] Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis(https://arxiv.org/abs/2512.11388)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>We investigated the impact of data selection on machine translation fine-tuning for open LLMs. Using Japanese-English corpora, we compare five selectors: TF-IDF, COMET Kiwi, QuRate, FD-Score, and random selection, under controlled training conditions. We observed that semantic selectors consistently outperform lexical and geometry-based heuristics, and that even when the selected data differ by less than 3%, the impact on model performance is substantial, underscoring the sensitivity of fine-tuning to data quality.</li>
<li><strong>摘要：</strong>我们研究了数据选择对开放法学硕士机器翻译微调的影响。使用日语-英语语料库，我们在受控训练条件下比较了五个选择器：TF-IDF、COMET Kiwi、QuRate、FD-Score 和随机选择。我们观察到，语义选择器的性能始终优于基于词汇和几何的启发式方法，即使所选数据的差异小于 3%，对模型性能的影响也是巨大的，这强调了微调对数据质量的敏感性。</li>
</ul>

<h3>Title: Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction</h3>
<ul>
<li><strong>Authors: </strong>Galann Pennec, Zhengyuan Liu, Nicholas Asher, Philippe Muller, Nancy F. Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11399">https://arxiv.org/abs/2512.11399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11399">https://arxiv.org/pdf/2512.11399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11399]] Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction(https://arxiv.org/abs/2512.11399)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.</li>
<li><strong>摘要：</strong>视觉语言模型 (VLM) 能够处理越来越长的视频。然而，重要的视觉信息很容易在整个上下文中丢失，并被 VLM 错过。此外，设计能够对长视频内容进行经济有效的分析的工具也很重要。在本文中，我们提出了一种剪辑选择方法，其目标是将关键视频时刻包含在多模态摘要中。我们将视频分成短片，并使用轻量级视频字幕模型生成每个短片的紧凑视觉描述。然后将它们传递给大型语言模型 (LLM)，该模型会选择包含最相关视觉信息的 K 个剪辑以进行多模式摘要。我们在任务的参考剪辑上评估我们的方法，这些参考剪辑是自动从 MovieSum 数据集中的完整人工注释剧本和摘要中得出的。我们进一步表明，这些参考剪辑（不到电影的 6%）足以在 MovieSum 中构建电影的完整多模态摘要。使用我们的剪辑选择方法，我们实现了接近这些参考剪辑的摘要性能，同时捕获比随机剪辑选择更相关的视频信息。重要的是，我们通过依赖轻量级字幕模型来保持较低的计算成本。</li>
</ul>

<h3>Title: CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Akash Ghosh, Srivarshinee Sridhar, Raghav Kaushik Ravi, Muhsin Muhsin, Sriparna Saha, Chirag Agarwal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11437">https://arxiv.org/abs/2512.11437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11437">https://arxiv.org/pdf/2512.11437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11437]] CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare(https://arxiv.org/abs/2512.11437)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.</li>
<li><strong>摘要：</strong>将语言模型 (LM) 集成到医疗保健系统中对于改善医疗工作流程和决策有着巨大的希望。然而，在现实世界中采用它们的一个关键障碍是缺乏对其可信度的可靠评估，特别是在多语言医疗保健环境中。现有的 LM 主要接受高资源语言的培训，这使得它们无法处理中低资源语言的医疗保健查询的复杂性和多样性，这给在语言多样性至关重要的全球医疗保健环境中部署它们带来了重大挑战。在这项工作中，我们提出了 CLINIC，这是一个综合多语言基准，用于评估医疗保健中语言模型的可信度。 CLINIC 在可信度的五个关键维度上系统地对 LM 进行基准测试：真实性、公平性、安全性、稳健性和隐私性，通过 18 种不同的任务进行操作，涵盖 15 种语言（覆盖所有主要大陆），并涵盖广泛的关键医疗保健主题，如疾病状况、预防措施、诊断测试、治疗、手术和药物。我们的广泛评估表明，语言模型在事实正确性方面存在困难，在人口和语言群体中表现出偏见，并且容易受到隐私侵犯和对抗性攻击。通过强调这些缺点，CLINIC 为跨多种语言增强医疗保健领域 LM 的全球影响力和安全性奠定了基础。</li>
</ul>

<h3>Title: Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuanbo Su, Yingfang Zhang, Hao Luo, Xiaoteng Liu, Leo Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11485">https://arxiv.org/abs/2512.11485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11485">https://arxiv.org/pdf/2512.11485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11485]] Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning(https://arxiv.org/abs/2512.11485)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) adapt to tasks via gradient fine-tuning (heavy computation, catastrophic forgetting) or In-Context Learning (ICL: low robustness, poor mistake learning). To fix this, we introduce Mistake Notebook Learning (MNL), a training-free framework with a persistent knowledge base of abstracted error patterns. Unlike prior instance/single-trajectory memory methods, MNL uses batch-wise error abstraction: it extracts generalizable guidance from multiple failures, stores insights in a dynamic notebook, and retains only baseline-outperforming guidance via hold-out validation (ensuring monotonic improvement). We show MNL nearly matches Supervised Fine-Tuning (93.9% vs 94.3% on GSM8K) and outperforms training-free alternatives on GSM8K, Spider, AIME, and KaggleDBQA. On KaggleDBQA (Qwen3-8B), MNL hits 28% accuracy (47% relative gain), outperforming Memento (15.1%) and Training-Free GRPO (22.1) - proving it's a strong training-free alternative for complex reasoning.</li>
<li><strong>摘要：</strong>大型语言模型（LLM）通过梯度微调（大量计算、灾难性遗忘）或上下文学习（ICL：鲁棒性低、错误学习能力差）来适应任务。为了解决这个问题，我们引入了错误笔记本学习（MNL），这是一个免训练框架，具有抽象错误模式的持久知识库。与之前的实例/单轨迹记忆方法不同，MNL 使用批量错误抽象：它从多个故障中提取可概括的指导，将见解存储在动态笔记本中，并通过保留验证仅保留优于基线的指导（确保单调改进）。我们表明，MNL 几乎与监督微调相匹配（GSM8K 上为 93.9% vs 94.3%），并且在 GSM8K、Spider、AIME 和 KaggleDBQA 上优于免训练替代方案。在 KaggleDBQA (Qwen3-8B) 上，MNL 达到 28% 的准确率（47% 相对增益），优于 Memento (15.1%) 和无训练 GRPO (22.1) - 证明它是复杂推理的强大的免训练替代方案。</li>
</ul>

<h3>Title: Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction</h3>
<ul>
<li><strong>Authors: </strong>Kai Golan Hashiloni, Brenda Kasabe Nokai, Michal Shevach, Esthy Shemesh, Ronit Bartin, Anna Bergrin, Liran Harel, Nachum Dershowitz, Liat Nadai Arad, Kfir Bar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11502">https://arxiv.org/abs/2512.11502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11502">https://arxiv.org/pdf/2512.11502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11502]] Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction(https://arxiv.org/abs/2512.11502)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>We present a new Hebrew medical language model designed to extract structured clinical timelines from electronic health records, enabling the construction of patient journeys. Our model is based on DictaBERT 2.0 and continually pre-trained on over five million de-identified hospital records. To evaluate its effectiveness, we introduce two new datasets -- one from internal medicine and emergency departments, and another from oncology -- annotated for event temporal relations. Our results show that our model achieves strong performance on both datasets. We also find that vocabulary adaptation improves token efficiency and that de-identification does not compromise downstream performance, supporting privacy-conscious model development. The model is made available for research use under ethical restrictions.</li>
<li><strong>摘要：</strong>我们提出了一种新的希伯来语医学语言模型，旨在从电子健康记录中提取结构化的临床时间表，从而构建患者旅程。我们的模型基于 DictaBERT 2.0，并持续对超过 500 万条去识别化的医院记录进行预训练。为了评估其有效性，我们引入了两个新的数据集——一个来自内科和急诊科，另一个来自肿瘤科——对事件时间关系进行了注释。我们的结果表明，我们的模型在两个数据集上都取得了出色的性能。我们还发现词汇适应提高了令牌效率，并且去标识化不会损害下游性能，支持隐私意识模型的开发。该模型在道德限制下可供研究使用。</li>
</ul>

<h3>Title: Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mohor Banerjee, Nadya Yuki Wangsajaya, Syed Ali Redha Alsagoff, Min Sen Tan, Zachary Choy Kit Chun, Alvin Chan Guo Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11509">https://arxiv.org/abs/2512.11509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11509">https://arxiv.org/pdf/2512.11509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11509]] Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs(https://arxiv.org/abs/2512.11509)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.</li>
<li><strong>摘要：</strong>大型语言模型 (LLM) 在自然语言理解和推理方面表现出卓越的能力，但会产生幻觉：生成实际上不正确的内容。尽管已经开发出许多方法来减少幻觉，但它们对富有创造力的一代人的影响仍有待探索。这一差距对于人工智能辅助的科学发现尤为重要，因为这既需要事实的准确性，又需要创造性的假设生成。我们研究了三种减少幻觉的技术：验证链（CoVe）、对比层解码（DoLa）和检索增强生成（RAG）如何影响法学硕士的创造力。在两个创造力基准（NeoCoder 和 CS4）上以不同规模（1B - 70B 参数）评估多个模型系列（LLaMA、Qwen、Mistral），我们发现这些方法对发散创造力具有相反的影响。 CoVe 增强发散思维，DoLa 抑制发散思维，而 RAG 的影响最小。我们的研究结果为在科学应用中选择适当的减少幻觉的方法提供了指导，其中事实准确性和创造性探索之间的平衡至关重要。</li>
</ul>

<h3>Title: Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet</h3>
<ul>
<li><strong>Authors: </strong>Mevlüt Bagci, Ali Abusaleh, Daniel Baumartz, Giueseppe Abrami, Maxim Konca, Alexander Mehler</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11567">https://arxiv.org/abs/2512.11567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11567">https://arxiv.org/pdf/2512.11567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11567]] Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet(https://arxiv.org/abs/2512.11567)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model</a></li>
<li><strong>Abstract: </strong>Social media serves as a critical medium in modern politics because it both reflects politicians' ideologies and facilitates communication with younger generations. We present MultiParTweet, a multilingual tweet corpus from X that connects politicians' social media discourse with German political corpus GerParCor, thereby enabling comparative analyses between online communication and parliamentary debates. MultiParTweet contains 39 546 tweets, including 19 056 media items. Furthermore, we enriched the annotation with nine text-based models and one vision-language model (VLM) to annotate MultiParTweet with emotion, sentiment, and topic annotations. Moreover, the automated annotations are evaluated against a manually annotated subset. MultiParTweet can be reconstructed using our tool, TTLABTweetCrawler, which provides a framework for collecting data from X. To demonstrate a methodological demonstration, we examine whether the models can predict each other using the outputs of the remaining models. In summary, we provide MultiParTweet, a resource integrating automatic text and media-based annotations validated with human annotations, and TTLABTweetCrawler, a general-purpose X data collection tool. Our analysis shows that the models are mutually predictable. In addition, VLM-based annotation were preferred by human annotators, suggesting that multimodal representations align more with human interpretation.</li>
<li><strong>摘要：</strong>社交媒体是现代政治的重要媒介，因为它既反映政治家的意识形态，又促进与年轻一代的沟通。我们推出了 MultiParTweet，这是 X 的多语言推文语料库，它将政客的社交媒体话语与德国政治语料库 GerParCor 连接起来，从而实现在线交流和议会辩论之间的比较分析。 MultiParTweet 包含 39 546 条推文，其中包括 19 056 条媒体项。此外，我们使用九种基于文本的模型和一种视觉语言模型（VLM）丰富了注释，以用情感、情绪和主题注释来注释 MultiParTweet。此外，自动注释是根据手动注释的子集进行评估的。 MultiParTweet 可以使用我们的工具 TTLABTweetCrawler 进行重建，该工具提供了一个用于从 X 收集数据的框架。为了进行方法论证，我们检查模型是否可以使用其余模型的输出相互预测。总之，我们提供了 MultiParTweet（一种集成了自动文本和基于媒体的注释并经过人工注释验证的资源）和 TTLABTweetCrawler（一种通用 X 数据收集工具）。我们的分析表明，这些模型是可以相互预测的。此外，基于 VLM 的注释受到人类注释者的青睐，这表明多模态表示更符合人类的解释。</li>
</ul>

<h3>Title: Visualizing token importance for black-box language models</h3>
<ul>
<li><strong>Authors: </strong>Paulius Rauba, Qiyao Wei, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11573">https://arxiv.org/abs/2512.11573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11573">https://arxiv.org/pdf/2512.11573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11573]] Visualizing token importance for black-box language models(https://arxiv.org/abs/2512.11573)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, prompt</a></li>
<li><strong>Abstract: </strong>We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.</li>
<li><strong>摘要：</strong>我们考虑审核黑盒大型语言模型 (LLM) 的问题，以确保它们在生产环境中部署时表现可靠，特别是在法律、医疗和监管合规等高风险领域。现有的 LLM 审计方法通常侧重于模型行为的孤立方面，例如检测特定偏差或评估公平性。我们对一个更普遍的问题感兴趣——我们能否理解黑盒法学硕士的输出如何依赖于每个输入标记？在依赖于无法访问的语言模型 API 端点的实际应用程序中，迫切需要拥有此类工具。然而，这是一个非常重要的问题，因为 LLM 是随机函数（即两个输出偶然会不同），而计算提示级别梯度来近似输入灵敏度是不可行的。为了解决这个问题，我们提出了基于分布的敏感性分析（DBSA），这是一种轻量级的模型无关程序，用于评估语言模型输出对每个输入标记的敏感性，而不对 LLM 做出任何分布假设。 DBSA 是为从业者开发的实用工具，可以对依赖于特定输入标记的法学硕士进行快速、即插即用的可视化探索。通过说明性示例，我们演示了 DBSA 如何使用户能够检查 LLM 输入并找到现有 LLM 可解释性方法可能忽略的敏感性。</li>
</ul>

<h3>Title: Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols</h3>
<ul>
<li><strong>Authors: </strong>Björn Deiseroth, Max Henning Höth, Kristian Kersting, Letitia Parcalabescu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11614">https://arxiv.org/abs/2512.11614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11614">https://arxiv.org/pdf/2512.11614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11614]] Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols(https://arxiv.org/abs/2512.11614)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm, hallucination, retrieval-augmented generation</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.</li>
<li><strong>摘要：</strong>检索增强生成（RAG）模型依靠检索到的证据来指导大型语言模型（LLM）生成器，但当前系统将检索视为弱启发式而不是可验证的证据。结果，法学硕士在没有支持的情况下回答，在不完整或误导性的背景下产生幻觉，并依赖虚假证据。我们引入了一个训练框架，通过改编 Merlin-Arthur (M/A) 协议，将整个 RAG 管道（检索器和生成器）视为交互式证明系统。亚瑟（生成法学硕士）针对来源不明的问题进行训练：梅林提供了有用的证据，而莫甘娜则注入了对抗性的、误导性的背景。两者都使用线性时间 XAI 方法来识别和修改对 Arthur 最有影响的证据。因此，亚瑟学会了（i）当上下文支持答案时回答，（ii）当证据不足时拒绝，以及（iii）依赖真正为答案奠定基础的特定上下文范围。我们进一步引入了严格的评估框架，以将解释保真度与基线预测错误分开。这使我们能够引入和衡量解释信息分数（EIF），该分数将相对于模型容量和不完善基准的并购认证的互信息保证标准化。在三个 RAG 数据集和两个不同规模的模型系列中，经过 M/A 训练的法学硕士表现出更好的基础性、完整性、健全性和拒绝行为，以及减少的幻觉——无需手动注释无法回答的问题。检索器同样通过自动生成 M/A 硬阳性和阴性来提高召回率和 MRR。我们的结果表明，自主交互式证明式监督为可靠的 RAG 系统提供了一条原则性且实用的路径，该系统将检索到的文档视为可验证的证据，而不是建议。</li>
</ul>

<h3>Title: Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks</h3>
<ul>
<li><strong>Authors: </strong>Sergey Pankratov, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11718">https://arxiv.org/abs/2512.11718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11718">https://arxiv.org/pdf/2512.11718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11718]] Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks(https://arxiv.org/abs/2512.11718)</code><input type="text"></li>
<li><strong>Keywords: </strong>language model, llm</a></li>
<li><strong>Abstract: </strong>Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\mathbb{E}[X] \leq (\mu + \mu_{(2)})\log(P )/\mu^2 + O(1)$, where $P$ is the verifier's capacity, $\mu$ is the expected entropy of the verifier's output distribution, and $\mu_{(2)}$ is the expected second log-moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.</li>
<li><strong>摘要：</strong>推测生成已成为一种有前途的技术，可通过利用并行性同时验证多个草稿令牌来加速大型语言模型 (LLM) 中的推理。然而，对于可实现的加速的根本限制仍然知之甚少。在这项工作中，我们为任何确定性推测生成算法的运行时间建立了第一个“严格”下限。这是通过在令牌生成过程和分支随机游走之间进行并行来实现的，这使我们能够分析最佳草案树选择问题。我们证明，在基本假设下，每次推测迭代成功预测的令牌的预期数量限制为 $\mathbb{E}[X] \leq (\mu + \mu_{(2)})\log(P )/\mu^2 + O(1)$，其中 $P$ 是验证者的容量，$\mu$ 是验证者输出分布的预期熵，$\mu_{(2)}$ 是预期的第二个熵对数时刻。这一结果为并行令牌生成的局限性提供了新的见解，并可以指导未来推测解码系统的设计。对 Llama 模型的实证评估验证了我们的理论预测，证实了我们在实际环境中的局限性。</li>
</ul>

<h3>Title: SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support</h3>
<ul>
<li><strong>Authors: </strong>Yuming Feng, Xinrui Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2512.11755">https://arxiv.org/abs/2512.11755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2512.11755">https://arxiv.org/pdf/2512.11755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2512.11755]] SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support(https://arxiv.org/abs/2512.11755)</code><input type="text"></li>
<li><strong>Keywords: </strong>llm</a></li>
<li><strong>Abstract: </strong>Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems.</li>
<li><strong>摘要：</strong>在线产品评论包含丰富但嘈杂的信号，这些信号会让用户不知所措并阻碍有效的决策。现有的基于法学硕士的摘要仍然是通用的，无法考虑个人偏好，限制了它们的实际效用。我们提出了 SUMFORU，这是一个可引导的评论总结框架，可将输出与明确的用户角色保持一致，以支持个性化购买决策。我们的方法将基于 Amazon 2023 评论数据集构建的高质量数据管道与两阶段对齐程序集成在一起：(1) 通过不对称知识蒸馏进行人物感知监督微调 (SFT)，以及 (2) 使用偏好估计器捕获细粒度的人物相关信号的人工智能反馈强化学习 (RLAIF)。我们通过基于规则、基于法学硕士和以人为中心的指标评估模型，证明在一致性、基础性和偏好一致性方面的持续改进。我们的框架在所有评估设置中实现了最高性能，并有效地推广到了未见过的产品类别。我们的结果强调了可引导的多元协调对于构建下一代个性化决策支持系统的前景。</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
